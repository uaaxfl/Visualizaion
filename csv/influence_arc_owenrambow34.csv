1995.iwpt-1.30,J94-1004,0,0.104021,"Missing"
1997.mtsummit-workshop.12,J94-4004,0,0.436237,"ant strides towards understanding why interlingua-based analyses are successful. • There are practical advantages to tying machine translation to surface-oriented representations as much as possible: the availability of large bilingual corpora has made the exploitation of stochastic approaches a crucial element in the practical success of MT, and such approaches are by nature oriented towards the surface form. This paper is structured as follows. In Section 2, we introduce some the standard interlingua-based analysis for a well-known case of “structural divergence”, a difficult MT challenges (Dorr, 1994). In Section 3, we present our MT system and its lexico-structural transfer formalism in particular. In Section 4, we discuss the semantic analysis that underlies our approach. In Section 5, we present a simple transposition of the interlingua approach to lexico-structural transfer. In Section 6, we show how we can eliminate the most unmotivated features of this analysis by using lexical functions, a language-internal device for relating lexemes. We conclude in Section 7. 91 2 Structural Divergences A classic problem in machine translation is the translation of motion verbs from English to Fre"
1997.mtsummit-workshop.12,J87-3006,0,\N,Missing
2007.mtsummit-papers.39,W05-0909,0,0.0435449,"to characteristics of the translation input is growing more and more difficult. Problems are aggravated further in the case of speech translation (of e.g. broadcast news, talkshows, etc.), where the input to the translation module is provided by an automatic speech recognition (ASR) system whose performance also influences the quality of the final translation output. During machine translation (MT) system development, automatic evaluation criteria are commonly used to judge performance, such as the BLEU score (Papineni et al. 2002), the NIST score (Doddington 2002), or, more recently, METEOR (Banerjee & Lavie 2005). Although the use of fully automated evaluation criteria is helpful in accelerating the system development cycle, all of the above criteria have shown to be inferior to human judgments of translation performance. Moreover, they do not yield any insight into precisely which input characteristics caused particular translation errors, or which system components need to be improved in order to reach the desired performance level. Human analysis of machine translation errors, on the other hand, is costly and time-intensive and can typically not be performed on a regular basis in the course of syst"
2007.mtsummit-papers.39,E06-1005,0,0.0419776,"Missing"
2007.mtsummit-papers.39,niessen-etal-2000-evaluation,0,0.0362707,"tained from human annotations and are statistically related to measurements of the overall system performance. The various input document features are then ranked with respect to their impact on translation performance. Previous Work Most work on error analysis in statistical machine translation has made use of extensive human analysis, such as classifying unsatisfactory output into categories such as wrong word choice, missing content words, missing function words, etc. (see e.g. Koehn 2003, Och et al. 2003). Previous work on automatic or semi-automatic error analysis in SMT systems includes Niessen et al. (2000), Popovic et al. (2006a) and Popovic et al. (2006b). In Niessen et al. (2002), a graphical user interface was presented that automatically extracts various error measures for translation candidates and thus facilitates manual error analysis. In Popovic et al. (2006a) and Popovic et al. (2006b), errors in an English-Spanish statistical MT system were analyzed with respect to their syntactic and morphological origin. This was done by modifying the references and the machine translation output by eliminating morphological inflections or suspected reordered constituents, and by analyzing the resul"
2007.mtsummit-papers.39,P02-1040,0,0.0911609,"As a consequence, diagnosing problems in translation performance and relating them to characteristics of the translation input is growing more and more difficult. Problems are aggravated further in the case of speech translation (of e.g. broadcast news, talkshows, etc.), where the input to the translation module is provided by an automatic speech recognition (ASR) system whose performance also influences the quality of the final translation output. During machine translation (MT) system development, automatic evaluation criteria are commonly used to judge performance, such as the BLEU score (Papineni et al. 2002), the NIST score (Doddington 2002), or, more recently, METEOR (Banerjee & Lavie 2005). Although the use of fully automated evaluation criteria is helpful in accelerating the system development cycle, all of the above criteria have shown to be inferior to human judgments of translation performance. Moreover, they do not yield any insight into precisely which input characteristics caused particular translation errors, or which system components need to be improved in order to reach the desired performance level. Human analysis of machine translation errors, on the other hand, is costly and time-"
2007.mtsummit-papers.39,W06-3101,0,0.229094,"Missing"
2020.acl-main.701,D15-1075,0,0.0465133,"tic. Accordingly, we review existing tasks grouped by their data collection methods. We argue that each category falls short of testing a useful body of content in a satisfying way. 2.1 Manually written questions By far the most popular strategy for generating MRC questions is to have humans—usually crowd workers, but sometimes trained annotators—think of questions about each passage. The most straightforward version of this method gives annotators little to no guidance regarding what questions to ask. One early example is the TREC-8 dataset (Voorhees and Tice, 2000). In the more recent SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) entailment tasks, the only constraint on crowd workers was that they produce one entailed, one contradicted, and one neutral hypothesis for each premise sentence.2 Similarly, the workers who assembled NewsQA (Trischler et al., 2017) were told only that the questions had to be answerable with short phrases, and workers for SQuAD (Rajpurkar et al., 2016) were simply given a “good” and a “bad” example and encouraged to use original wording. 1 We will use “narrative” and “story” interchangeably, roughly following the Wikipedia definition: “A narrative or story is"
2020.acl-main.701,E06-1032,0,0.139525,"ce may be inevitable, and seem to warrant use of the pyramid method. 4.1.2 Free-text evaluation It is difficult to evaluate a system directly on an RoU or a rubric, as they are written in plain English. One option is to pose broad ToU questions (e.g., “What events happened and in what order?”) and then to automatically compare systems’ full freetext answers to annotators’. But this would require an automated comparison metric, and existing metrics such as ROUGE and BLEU are concerned only with lexical similarity. Their correlation with humans’ quality judgments is substantial but not stellar (Callison-Burch et al., 2006), and high scores do not always indicate good answers in MRC (see Yang et al., 2018a; Nema and Khapra, 2018). Superficial similarity measures may prove particularly weak given how open-ended ToU questions are. Alternatively, human evaluators could read both the RoU-derived rubric and the system output and decide whether the output adequately covers each nugget from the rubric. This is how the pyramid method is typically applied in summarization. Still a third possibility is to have human evaluators ask targeted questions about each nugget from the rubric. The evaluators could then judge whethe"
2020.acl-main.701,P16-1223,0,0.0307376,"signers start by considering what systems should in fact comprehend. In this paper we make two key contributions. First, we argue that existing approaches do not adequately define comprehension; they are too unsystematic about what content is tested. Second, we present a detailed definition of comprehension—a T EM PLATE OF U NDERSTANDING —for a widely useful class of texts, namely short narratives. We then conduct an experiment that strongly suggests existing systems are not up to the task of narrative understanding as we define it. 1 Introduction Over the past few years, neural models (e.g., Chen et al., 2016; Devlin et al., 2019; Liu et al., 2019) have begun to match or even exceed human performance on MACHINE READING COMPREHEN SION (MRC) benchmarks. In these tasks, systems demonstrate their comprehension of a passage by answering questions about it. Yet despite recent successes, MRC appears far from solved: systems continue to make basic, sometimes baffling mistakes, and they fail to generalize to new data. Such shortcomings have motivated a flurry of new MRC tasks, each designed to confront systems with questions deemed challenging for current methods. For example, tasks may ask questions requi"
2020.acl-main.701,N19-1300,0,0.012109,"e biased toward questions that humans find interesting (see Gordon and Van Durme, 2013; Misra et al., 2016; Zhang 7840 et al., 2017). They do not think to ask questions whose answers seem obvious, even when those answers are essential to comprehension. If we do not delineate such facts and evaluate systems’ ability to manipulate them, we will never be satisfied that the systems have adequately understood the text. 2.2 Naturally occurring questions A second approach is to find questions “in the wild,” then retrospectively collect documents containing the answers. This is the approach of BoolQ (Clark et al., 2019) and MS MARCO (Nguyen et al., 2016), which compile search engine queries, and of ELI5 (Fan et al., 2019), which harvests questions from Reddit’s “Explain Like I’m Five” forum. Such datasets are clearly useful for answering common queries, a valuable application class in its own right. For more complex applications, however, common queries are, if anything, less thorough than annotators at probing important elements of understanding (particularly aspects humans find obvious). The mismatch between questions and passage content is exacerbated by finding the passages retrospectively: the questions"
2020.acl-main.701,D19-5815,0,0.0204244,"ting datasets, they are closest to our proposal. 2.5 Summary: What is missing The most clear-cut way to test reading comprehension would be to select passages, describe what should be comprehended from them, and design tests for that understanding. Yet few MRC datasets have even approximated this approach. Many impose little structure on what content is tested; the 7841 rest pick some “difficult” form(s) of analysis or linguistic phenomena, but rarely consider downstream goals to determine what the questions should be about. Metrics for difficult reasoning and linguistic phenomena (see, e.g., Gardner et al., 2019) are useful, but only as tools for error analysis and mitigation; they are not top-line performance metrics. In addition, many datasets to date suffer from two other problems: 1) they select passages after the questions are asked, meaning the questions test comprehension of only small portions of the passages; and/or 2) they ask very few questions whose answers are obvious to humans. These issues of content scope also intersect with issues of format. Many tasks have adopted a span extraction format, including TREC QA, NewsQA, and (most notably) SQuAD and its successors. This format immediately"
2020.acl-main.701,P99-1042,0,0.338627,"s own right. For more complex applications, however, common queries are, if anything, less thorough than annotators at probing important elements of understanding (particularly aspects humans find obvious). The mismatch between questions and passage content is exacerbated by finding the passages retrospectively: the questions do not even attempt to test most of what each passage discusses, making them an insufficient measure of MRC. 2.3 Questions from tests designed for humans The third strategy is to pull questions from tests written for humans. Examples include the early “Deep Read” corpus (Hirschman et al., 1999); the more recent TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al., 2017) datasets, which mine collections of trivia questions; the AI2 Reasoning Challenge (ARC; Clark et al., 2018), which asks questions from standardized science tests; and RACE (Lai et al., 2017), which draws from English learning materials for Chinese school students. Our chief concern about this approach echoes our concerns from §2.1: tests designed for humans rarely bother to test content that most humans find obvious. Accordingly, they gloss over vast swaths of understanding that machines do not yet have but which"
2020.acl-main.701,D19-1243,0,0.102887,"Missing"
2020.acl-main.701,P17-1147,0,0.0266951,", common queries are, if anything, less thorough than annotators at probing important elements of understanding (particularly aspects humans find obvious). The mismatch between questions and passage content is exacerbated by finding the passages retrospectively: the questions do not even attempt to test most of what each passage discusses, making them an insufficient measure of MRC. 2.3 Questions from tests designed for humans The third strategy is to pull questions from tests written for humans. Examples include the early “Deep Read” corpus (Hirschman et al., 1999); the more recent TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al., 2017) datasets, which mine collections of trivia questions; the AI2 Reasoning Challenge (ARC; Clark et al., 2018), which asks questions from standardized science tests; and RACE (Lai et al., 2017), which draws from English learning materials for Chinese school students. Our chief concern about this approach echoes our concerns from §2.1: tests designed for humans rarely bother to test content that most humans find obvious. Accordingly, they gloss over vast swaths of understanding that machines do not yet have but which may be critical to applications. In addition, S"
2020.acl-main.701,W11-0219,0,0.0280365,"Missing"
2020.acl-main.701,P18-1156,0,0.0195785,"ly in the sense that the outputs of NLP tools (e.g., translation or information extraction systems) were recorded as correct/incorrect examples of entailment. Little attention was paid to subject matter. The problem with such an open-ended generation process is that, absent stronger guidance, people tend to write simple questions that can be answered using lexical cues. (See, e.g., the dataset analysis in Rajpurkar et al., 2016.) This makes the tasks questionable measures of comprehension. The dominant solution is to incorporate trickier twists. NarrativeQA (Koˇcisk´y et al., 2018) and DuoRC (Saha et al., 2018) reduce lexical similarity between questions and passages by showing annotators only a second passage about the same events. Other datasets emphasize reasoning presumed to be difficult, such as incorporating information from multiple parts of the text. MCTest (Richardson et al., 2013) and MultiRC (Khashabi et al., 2018) ask for questions that rely on multiple sentences; ROPES (Lin et al., 2019) has annotators apply information from one passage to write questions on a second; and HotpotQA (Yang et al., 2018b) and QASC (Khot et al., 2019) require multi-hop reasoning. Other forms of reasoning tes"
2020.acl-main.701,W17-2623,0,0.0226495,"C questions is to have humans—usually crowd workers, but sometimes trained annotators—think of questions about each passage. The most straightforward version of this method gives annotators little to no guidance regarding what questions to ask. One early example is the TREC-8 dataset (Voorhees and Tice, 2000). In the more recent SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) entailment tasks, the only constraint on crowd workers was that they produce one entailed, one contradicted, and one neutral hypothesis for each premise sentence.2 Similarly, the workers who assembled NewsQA (Trischler et al., 2017) were told only that the questions had to be answerable with short phrases, and workers for SQuAD (Rajpurkar et al., 2016) were simply given a “good” and a “bad” example and encouraged to use original wording. 1 We will use “narrative” and “story” interchangeably, roughly following the Wikipedia definition: “A narrative or story is an account of a series of related events, experiences, or the like, whether true. . . or fictitious.” 2 Parts of the original RTE datasets (Dagan et al., 2006, etc.) were generated more systematically, but only in the sense that the outputs of NLP tools (e.g., trans"
2020.acl-main.701,W18-5446,0,0.0696447,"Missing"
2020.acl-main.701,Q18-1021,0,0.137149,"exceed human performance on MACHINE READING COMPREHEN SION (MRC) benchmarks. In these tasks, systems demonstrate their comprehension of a passage by answering questions about it. Yet despite recent successes, MRC appears far from solved: systems continue to make basic, sometimes baffling mistakes, and they fail to generalize to new data. Such shortcomings have motivated a flurry of new MRC tasks, each designed to confront systems with questions deemed challenging for current methods. For example, tasks may ask questions requiring commonsense reasoning (Huang et al., 2019), multihop reasoning (Welbl et al., 2018), or inferences based on a second passage (Lin et al., 2019). This line of research assumes that ever-more“difficult” question-answering tasks will ultimately lead to more robust and useful reading comprehension. We argue that, while the question-answering * Equal contributions. format can be a fine choice for how to test comprehension, using difficulty as the basis for what to test is fundamentally flawed. To put it provocatively, the dominant MRC research paradigm is like trying to become a professional sprinter by glancing around the gym and adopting any exercises that look hard. The traini"
2020.acl-main.701,N18-1101,0,0.0391427,"sting tasks grouped by their data collection methods. We argue that each category falls short of testing a useful body of content in a satisfying way. 2.1 Manually written questions By far the most popular strategy for generating MRC questions is to have humans—usually crowd workers, but sometimes trained annotators—think of questions about each passage. The most straightforward version of this method gives annotators little to no guidance regarding what questions to ask. One early example is the TREC-8 dataset (Voorhees and Tice, 2000). In the more recent SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) entailment tasks, the only constraint on crowd workers was that they produce one entailed, one contradicted, and one neutral hypothesis for each premise sentence.2 Similarly, the workers who assembled NewsQA (Trischler et al., 2017) were told only that the questions had to be answerable with short phrases, and workers for SQuAD (Rajpurkar et al., 2016) were simply given a “good” and a “bad” example and encouraged to use original wording. 1 We will use “narrative” and “story” interchangeably, roughly following the Wikipedia definition: “A narrative or story is an account of a series of related"
2020.acl-main.701,D18-1259,0,0.131777,"tion is to incorporate trickier twists. NarrativeQA (Koˇcisk´y et al., 2018) and DuoRC (Saha et al., 2018) reduce lexical similarity between questions and passages by showing annotators only a second passage about the same events. Other datasets emphasize reasoning presumed to be difficult, such as incorporating information from multiple parts of the text. MCTest (Richardson et al., 2013) and MultiRC (Khashabi et al., 2018) ask for questions that rely on multiple sentences; ROPES (Lin et al., 2019) has annotators apply information from one passage to write questions on a second; and HotpotQA (Yang et al., 2018b) and QASC (Khot et al., 2019) require multi-hop reasoning. Other forms of reasoning tested include coreference resolution (Quoref, Dasigi et al., 2019; Winograd Schema Challange, Levesque et al., 2012), numerical reasoning (DROP, Dua et al., 2019), and commonsense reasoning (Cosmos QA, Huang et al., 2019). Tasks can also be made harder with devices such as unanswerable questions (SQuADRUn, Rajpurkar et al., 2018; NewsQA; CosmosQA) and filtering questions with an adversarial baseline (DROP; Quoref; QASC). These twists do make MRC harder. But to pursue hard questions is to overlook why easy qu"
2020.acl-main.701,S14-1015,0,0.0136712,"questions nor recognizing good answers demands nearly as much specification as stating canonical answers. Whereas the approaches in §4.1 must strive for replicability in humans’ answers, this approach seeks replicability only in humans’ judgments of answers. We suggest two ways to achieve this. First, in the absence of a rubric, we suspect that answers would best be judged via pairwise comparisons. For free-text writing, humans generally find comparative assessment easier than absolute scoring (Pollitt, 2012), and comparison is already used to evaluate natural-language generation (see, e.g., Yatskar et al., 2014). Comparisons also mitigate the difficulty of spotting errors of omission: when evaluators see an incomplete answer in isolation, they may gloss over or mentally fill in what was left unsaid. Comparing against a more complete competing answer makes it easier to notice gaps. Second, evaluators can be guided to tease apart their judgments into several desirable dimensions of explanations—e.g., accuracy, depth, and coherence—just as is often done for natural language generation. Pilot studies would be required to refine the dimensions and their specifications. 5 Current MRC systems do not compreh"
2020.acl-main.701,P02-1040,0,\N,Missing
2020.acl-main.701,N04-1019,0,\N,Missing
2020.acl-main.701,D13-1020,0,\N,Missing
2020.acl-main.701,E14-1056,0,\N,Missing
2020.acl-main.701,Q17-1027,0,\N,Missing
2020.acl-main.701,D17-1082,0,\N,Missing
2020.acl-main.701,P18-2124,0,\N,Missing
2020.acl-main.701,N18-1023,0,\N,Missing
2020.acl-main.701,N19-1246,0,\N,Missing
2020.acl-main.701,P19-1346,0,\N,Missing
2020.acl-main.701,Q18-1023,0,\N,Missing
2020.lrec-1.167,P12-2032,1,0.785215,"We have thus established that modeling the social network helps in document classification, but modeling the thread structure is also important. • We show that our approach outperforms a state-of-theart method proposed in the literature based on node embeddings, namely GraphSAGE. Because we are interested in modeling thread structure, we use datasets which maintain the integrity of the thread (i.e. all emails belong to threads and all threads have labeled emails) and which we introduced in our previous work (Alkhereyf and Rambow, 2017). The Enron dataset is based on Columbia’s Enron release (Agarwal et al., 2012). This paper adds the following research to our previous publication: • We use neural network models. • We use a strong baseline based on graph embeddings, namely, GraphSAGE (sections 5. and 6.3.). • We explicitly model email threads (subsection 6.4.). • We use word embeddings trained on our data (section 4.). Also, as part of the submission we release the annotated Enron corpus in addition to other annotations including power relations as a language resource (Agarwal et al., 2020). For Avocado, we release the annotation labels with their corresponding email ids without the email content (beca"
2020.lrec-1.167,W17-2408,1,0.886498,"oth individuals and organizations for both personal and business communications. Kiritchenko and Matwin (2011) show that a typical user daily receives 40-50 emails. And despite the massive growth of other social media over the past decade, company email is still used for personal purposes as the recent Avocado corpus shows (section 3.). 4. Unlike other text classification tasks, particularly for emails (e.g. spam filtering), email classification into business and personal has not received much attention and it remains a challenging (as shown in the human inter-annotator agreement reported in (Alkhereyf and Rambow, 2017; Jabbari et al., 2006)) and unsolved task. 5. We are interested in how people communicate in conversations, and email has real conversations. This distinguishes email from blogs and Twitter, which are readily available, but typically used for broadcasting to a large group of followers. As for any document classification task, the language used (reflecting both content and language style) is highly predictive of the class. For instance, when a student speaks with her friends, she will probably use relatively less formal language than when she speaks with her professor, and she will talk about"
2020.lrec-1.167,Q17-1010,0,0.0825347,"Missing"
2020.lrec-1.167,N15-1185,0,0.0528847,"Missing"
2020.lrec-1.167,D12-1135,0,0.0350495,"el emails as a social network in section 6.. We present the experimental study to evaluate our models in section 7., and conclude in section 8.. 2. 2.1. Related Work Incorporating Network and Language Information Many previous studies on various natural language processing tasks in the context of social networks mainly focus on textual information and ignore other information that can be extracted from the underlying social network. However, there are some studies that incorporate the social network structure to improve the performance for different tasks including: inferring user attributes (Filippova, 2012; Al Zamal et al., 2012; Perozzi and Skiena, 2015; Aletras and Chamberlain, 2018) predicting user stance (Tan et al., 2011; West et al., 2014; Gryc and Moilanen, 2014; Gui et al., 2017; Wang et al., 2018; Volkova et al., 2014), and extracting inter-personal relations (Elangovan and Eisenstein, 2015; West et al., 2014; Abu-Jbara et al., 2013; Hassan et al., 2012). Most of these studies exploiting social network information are guided by an assumption of homophily, i.e., the tendency of individuals to associate and bond with similar others (McPherson et al., 2001). Our work differs from these st"
2020.lrec-1.167,D12-1006,0,0.0309107,"e other information that can be extracted from the underlying social network. However, there are some studies that incorporate the social network structure to improve the performance for different tasks including: inferring user attributes (Filippova, 2012; Al Zamal et al., 2012; Perozzi and Skiena, 2015; Aletras and Chamberlain, 2018) predicting user stance (Tan et al., 2011; West et al., 2014; Gryc and Moilanen, 2014; Gui et al., 2017; Wang et al., 2018; Volkova et al., 2014), and extracting inter-personal relations (Elangovan and Eisenstein, 2015; West et al., 2014; Abu-Jbara et al., 2013; Hassan et al., 2012). Most of these studies exploiting social network information are guided by an assumption of homophily, i.e., the tendency of individuals to associate and bond with similar others (McPherson et al., 2001). Our work differs from these studies in that we focus on classifying a given document (i.e. email) exchanged between users, not on predicting user information, nor interpersonal relations. Note that different emails exchanged between the same set of users can belong to different classes, where in these studies, the attributes remains the same for a given set of users. Graphs are an important"
2020.lrec-1.167,P06-2053,0,0.519191,"tions for both personal and business communications. Kiritchenko and Matwin (2011) show that a typical user daily receives 40-50 emails. And despite the massive growth of other social media over the past decade, company email is still used for personal purposes as the recent Avocado corpus shows (section 3.). 4. Unlike other text classification tasks, particularly for emails (e.g. spam filtering), email classification into business and personal has not received much attention and it remains a challenging (as shown in the human inter-annotator agreement reported in (Alkhereyf and Rambow, 2017; Jabbari et al., 2006)) and unsolved task. 5. We are interested in how people communicate in conversations, and email has real conversations. This distinguishes email from blogs and Twitter, which are readily available, but typically used for broadcasting to a large group of followers. As for any document classification task, the language used (reflecting both content and language style) is highly predictive of the class. For instance, when a student speaks with her friends, she will probably use relatively less formal language than when she speaks with her professor, and she will talk about different topics. As we"
2020.lrec-1.167,D14-1162,0,0.086329,"Missing"
2020.lrec-1.167,W11-0711,0,0.0589952,"ain categories “Business” and “Personal”. Unlike our work, they do not utilize email thread structure, and many emails in the Sheffield dataset are not part of a thread and some threads are partially labeled (i.e. some emails in the thread are unlabeled). They also present a preliminary experiment for automatic classification of personal and business. We don’t use this dataset for training as we are in1337 terested in modeling threads. However, we show the performance of some of our models on this dataset in subsection 7.2.. The Sheffield dataset has been used in other studies. In particular, Peterson et al. (2011) show that the formality level in emails is affected by the interpersonal nature of email (personal or business). They use email gold labels in the Sheffield dataset to determine the email type. Mitra and Gilbert (2012) use the Sheffield dataset to study the proportion of gossip in business and personal emails. In our work, we focus on automatic classification of emails into business and personal. There has been some previous work on incorporating email communication network information for different tasks. Yoo et al. (2009) propose a semi-supervised method for personalized email prioritizatio"
2020.lrec-1.167,P14-1018,0,0.0608008,"Missing"
2020.lrec-1.167,Q14-1024,0,0.020549,"n 8.. 2. 2.1. Related Work Incorporating Network and Language Information Many previous studies on various natural language processing tasks in the context of social networks mainly focus on textual information and ignore other information that can be extracted from the underlying social network. However, there are some studies that incorporate the social network structure to improve the performance for different tasks including: inferring user attributes (Filippova, 2012; Al Zamal et al., 2012; Perozzi and Skiena, 2015; Aletras and Chamberlain, 2018) predicting user stance (Tan et al., 2011; West et al., 2014; Gryc and Moilanen, 2014; Gui et al., 2017; Wang et al., 2018; Volkova et al., 2014), and extracting inter-personal relations (Elangovan and Eisenstein, 2015; West et al., 2014; Abu-Jbara et al., 2013; Hassan et al., 2012). Most of these studies exploiting social network information are guided by an assumption of homophily, i.e., the tendency of individuals to associate and bond with similar others (McPherson et al., 2001). Our work differs from these studies in that we focus on classifying a given document (i.e. email) exchanged between users, not on predicting user information, nor interper"
2021.sigmorphon-1.23,W12-2306,1,0.668524,"be supported by the morphophonological processes present in Shupamem nominal and verbal reduplication. We then discuss an 212 Proceedings of the Seventeenth SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology,pages 212–221 August 5, 2021. ©2 alternative, in which we use the MT FST to handle both reduplication and tones. It is important to emphasize that all of the machines we discuss are deterministic, which serves as another piece of evidence that even such complex processes like full reduplication can be modelled with deterministic finite-state technology (Chandlee and Heinz, 2012; Heinz, 2018). This paper is structured as follows. First, we will briefly summarize the linguistic phenomena observed in Shupamem reduplication (Section 2). We then provide a formal description of the 2-way (Section 3) and MT FSTs (Section 4). We propose a synthesis of the 1-way, 2-way and MT FSTs in Section 5 and further illustrate them using relevant examples from Shupamem in Section 6. In Section 7 we discuss a possible alternative to the model which uses only MT FSTs. Finally, in Section 8 we show that the proposed model works for other tonal languages as well, and we conclude our contri"
2021.sigmorphon-1.23,W19-0102,1,0.90114,"Missing"
2021.sigmorphon-1.23,2020.scil-1.25,0,0.0614345,"input tape, producing a faithful copy of a string. H ndap HL L ndap ndap Pioneering work in autosegmental phonology (Leben, 1973; Williams, 1976; Goldsmith, 1976) shows tones may act independently from their tonebearing units (TBUs). Moreover, tones may exhibit behavior that is not typical for segments (Hyman, 2014; Jardine, 2016), which brings yet another strong argument for separating them from segments in their linguistic representations. Such autosegmental representations can be mimicked using finite-state machines, in particular, MultiTape Finite-State Transducers (MT FSTs) (Wiebe, 1992; Dolatian and Rawski, 2020a; Rawski and Dolatian, 2020). We note that McCarthy (1981) uses the same autosegmental representations in the linguistic representation to model templatic morphology, and this approach has been modeled for Semitic morphological processing using multitape automata (Kiraz, 2000; Habash and Rambow, 2006). This paper investigates what finite-state machinery is needed for languages which have both reduplication and tones. We first argue that we need a synthesis of the aforementioned transducers, i.e. 1-way, 2-way and MT FSTs, to model morphology in the general case. The necessity for such a formal"
2021.sigmorphon-1.23,P06-1086,1,0.718852,"(Hyman, 2014; Jardine, 2016), which brings yet another strong argument for separating them from segments in their linguistic representations. Such autosegmental representations can be mimicked using finite-state machines, in particular, MultiTape Finite-State Transducers (MT FSTs) (Wiebe, 1992; Dolatian and Rawski, 2020a; Rawski and Dolatian, 2020). We note that McCarthy (1981) uses the same autosegmental representations in the linguistic representation to model templatic morphology, and this approach has been modeled for Semitic morphological processing using multitape automata (Kiraz, 2000; Habash and Rambow, 2006). This paper investigates what finite-state machinery is needed for languages which have both reduplication and tones. We first argue that we need a synthesis of the aforementioned transducers, i.e. 1-way, 2-way and MT FSTs, to model morphology in the general case. The necessity for such a formal device will be supported by the morphophonological processes present in Shupamem nominal and verbal reduplication. We then discuss an 212 Proceedings of the Seventeenth SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology,pages 212–221 August 5, 2021. ©2 alternative, i"
2021.sigmorphon-1.23,J00-1006,0,0.342698,"for segments (Hyman, 2014; Jardine, 2016), which brings yet another strong argument for separating them from segments in their linguistic representations. Such autosegmental representations can be mimicked using finite-state machines, in particular, MultiTape Finite-State Transducers (MT FSTs) (Wiebe, 1992; Dolatian and Rawski, 2020a; Rawski and Dolatian, 2020). We note that McCarthy (1981) uses the same autosegmental representations in the linguistic representation to model templatic morphology, and this approach has been modeled for Semitic morphological processing using multitape automata (Kiraz, 2000; Habash and Rambow, 2006). This paper investigates what finite-state machinery is needed for languages which have both reduplication and tones. We first argue that we need a synthesis of the aforementioned transducers, i.e. 1-way, 2-way and MT FSTs, to model morphology in the general case. The necessity for such a formal device will be supported by the morphophonological processes present in Shupamem nominal and verbal reduplication. We then discuss an 212 Proceedings of the Seventeenth SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology,pages 212–221 August"
2021.sigmorphon-1.23,2020.scil-1.26,0,0.0253243,"hful copy of a string. H ndap HL L ndap ndap Pioneering work in autosegmental phonology (Leben, 1973; Williams, 1976; Goldsmith, 1976) shows tones may act independently from their tonebearing units (TBUs). Moreover, tones may exhibit behavior that is not typical for segments (Hyman, 2014; Jardine, 2016), which brings yet another strong argument for separating them from segments in their linguistic representations. Such autosegmental representations can be mimicked using finite-state machines, in particular, MultiTape Finite-State Transducers (MT FSTs) (Wiebe, 1992; Dolatian and Rawski, 2020a; Rawski and Dolatian, 2020). We note that McCarthy (1981) uses the same autosegmental representations in the linguistic representation to model templatic morphology, and this approach has been modeled for Semitic morphological processing using multitape automata (Kiraz, 2000; Habash and Rambow, 2006). This paper investigates what finite-state machinery is needed for languages which have both reduplication and tones. We first argue that we need a synthesis of the aforementioned transducers, i.e. 1-way, 2-way and MT FSTs, to model morphology in the general case. The necessity for such a formal device will be supported by"
A00-1009,C92-3158,0,0.0855031,"Missing"
A00-1009,A97-1039,1,0.949816,"a DSyntS only includes full meaning-bearing lexemes while a SSyntS also contains function words such as determiners, auxiliaries, and strongly governed prepositions. In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997). Figure 2 illustrates a DSyntS from a meteorological application, MeteoCogent (Kittredge and Lavoie, 1998), represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework (Lavoie and Rambow, 1997). As Figure 2 illustrates, there is a straightforward mapping between the graphical notation and the ASCII notation supported in the framework. This also applies for all the transformation rules in the framework which illustrates the declarative nature of our approach, I As mentioned in (Polgu~re, 1991), the high level of abstraction of the ConcSs makes them a suitable interlingua for multilingual NLG since they bridge the semantic discrepancies between languages, and they can be produced easily from the domain data. However, most off-the-shelf parsers available for MT produce only syntactic s"
A00-1009,A97-1037,1,0.817227,"ransduction-based approach has some important limitations. In particular, the framework requires the developer of the transformation rules to maintain them and specify the order in which the rules must be applied. For a small or a stable grammar, this does not pose a problem. However, for large or rapidly changing grammar (such as a transfer grammar in MT that may need to be adjusted when switching from one parser to another), the NLG: • Realization of English DSyntSs via SSyntS level for the domains of meteorology (MeteoCogent; Kittredge and Lavoie, 1998) and object modeling (ModelExplainer; Lavoie et al., 1997). • Generation of English text from conceptual interlingua for the meteorology domain (MeteoCogent). (The design of the 65 burden of the developer&apos;s task may be quite heavy. In practice, a considerable amount of time can be spent in testing a grammar after its revision. Another major problem is related to the maintenance of both the grammar and the lexicon. On several occasions during the development of these resources, the developer in charge of adding lexical and grammatical data must make some decisions that are domain specific. For example, in MT, writing transfer rules for terms that can"
A00-1009,A92-1006,1,0.777871,"fine the rules manually. The current framework offers no support for merging handcrafted rules with new lexical rules obtained statistically while preserving the valid handcrafted changes and deleting the invalid ones. In general, a better integration of linguistically based and statistical methods during all the development phases is greatly needed. 8 History of the Framework and Comparison with Other Systems The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al., 1992), and JOYCE (Rambow and Korelsky, 1992). The framework was originally developed for the realization of deep-syntactic structures in NLG (Lavoie and Rambow, 1997). It was later extended for generation of deep-syntactic structures from conceptual interlingua (Kittredge and Lavoie, 1998). Finally, it was applied to MT for transfer between deep-syntactic structures of different languages (Palmer et al., 1998). The current framework encompasses the full spectrum of such transformations, i.e. from the processing of conceptual structures to the processing of deep-syntactic structures, either for NLG or MT. Compared to its predecessors (Fo"
A00-1009,palmer-etal-1998-rapid,1,\N,Missing
A00-1009,1997.mtsummit-workshop.12,1,\N,Missing
A00-1009,J94-4004,0,\N,Missing
A00-1009,J85-1003,0,\N,Missing
A92-1006,P88-1020,0,0.0102945,"l of representation. There is no need for backtracking or feedback from one level of processing to an earlier one. As is argued by McDonald el al., such an architecture contributes to processing efficiency. We will now discuss the three modules of Joyce in more detail. 4 The Text Planner Prior to the design of the text planning component of Joyce, several existing approaches were studied. Since the structure of the descriptive text (Figure 2) does not mirror the structure of the domain, Paris's &quot;procedural strategy&quot; (Paris and McKeown 1987) cannot be used in general. Hovy's RST-based planner (Hovy 1988) assumes that content selection has Mready been performed, contrary to the situation in the Ulysses application; furthermore, there are efficiency problems in a pure STRIPS-like planning paradigm. We therefore found McKeown's schema-based approach (McKeown 1985) to be the most promising. However, it turned out that general rhetorical schemas cannot adequately capture the structure of the intended texts. In (Kittredge et al 1991), we argue that planning certain types of texts - such as reports and descriptions - requires domain-specific knowledge about how to communicate in that domain. T h a t"
A92-1006,W90-0112,1,0.816511,"The generated text must be of sufficiently high quality so that the user community of the underlying application accepts it as part of the documentation of software designs. • The generation must be fast enough so that the system can be used as a tool during the design process. • The system must be adaptable to new needs as they arise during further development of the underlying system, and it must be portable to completely new applications. While we were able to exploit existing research for many of the design issues, it turned out that we needed to develop our own approach to text planning (Rambow 1990). This paper will present the system and attempt to show how these design objectives led to particular design decisions. The structure of the paper is as follows. In Section 2, we will present the underlying application and give examples of the o u t p u t of the System. In Section 3, we will discuss the overall structure of Joyce. We then discuss the three main components in turn: the text planner in Section 4, the sentence planner in Section 5 and the realizer in Section 6. We will discuss the text planner in some detail since it represents a new approach to the problem. Section 7 traces the"
A92-1006,A88-1004,0,0.0157487,"ain amount of disagreement about where the line between the two is to be drawn. For example, MeKeown's T E X T (McKeown 1985) performs the tasks that Joyce classifies as sentence planning as part of the realization process, whereas Meteer's S P O K E S M A N (Meteer 1989) classifies them as part of text planning. (See (Meteer 1990, p.23sq) for a useful summary of the terminological issues 1.) In this paper, &quot;text planning&quot; will always be used in the narrow sense of &quot;content selection and organization&quot;. The architecture of Joyce is directly influenced by that of the SEMSYN system (RSsner 1987; RSsner 1988). RSsner divides the realization component into two parts, the &quot;generator kernel&quot; and the &quot;generator front end&quot;. This distinction is mirrored exactly by the distinction between sentence planning and realization in Joyce. There are two main advantages to such a tripartite architecture, one conceptual and the other practical. Conceptually, the advantage is that linguistic planning tasks are clearly separated from the actual grammar, which comprises word order and morphological rules. These rules can be stated independently of the formulation of purely semantic rules that determine lexical and sy"
A92-1006,P84-1065,0,\N,Missing
A97-1037,W96-0503,1,0.695751,"ndersen Consulting, a large systems consulting company, and the Software Engineering Laboratory at the Electronic Systems Division of Raytheon, a large Government contractor. Our design is based on initial interviews with software engineers working on a project at Raytheon, and was modified in response to feedback during iterative prototyping when these software engineers were using our system. • MoDEx output integrates tables, text generated automatically, and text entered freely by the user. Automatically generated text includes paragraphs describing the relations between classes, and paral(Lavoie et al., 1996) focuses on an earlier version of MoDEx which did not yet include customization. graphs describing examples. The human-anthored text can capture information not deducible from the model (such as high-level descriptions of purpose associated with the classes). • MoDEx lets the user customize the text plans at run-time, so that the text can reflect individual user or organizational preferences regarding the content and/or layout of the output. • MoDEx uses an interactive hypertext interface (based on standard HTML-based WWW technology) to allow users to browse through the model. • Input to MoDEx"
A97-1037,A97-1039,1,0.740384,"Missing"
altantawy-etal-2010-morphological,C96-1017,0,\N,Missing
altantawy-etal-2010-morphological,C94-1029,0,\N,Missing
altantawy-etal-2010-morphological,J00-1006,0,\N,Missing
altantawy-etal-2010-morphological,P05-1071,1,\N,Missing
altantawy-etal-2010-morphological,W05-0703,1,\N,Missing
altantawy-etal-2010-morphological,P06-1086,1,\N,Missing
bauer-etal-2012-dependency,furstenau-2008-enriching,1,\N,Missing
bauer-etal-2012-dependency,de-marneffe-etal-2006-generating,0,\N,Missing
bauer-etal-2012-dependency,W08-2121,0,\N,Missing
bauer-etal-2012-dependency,S07-1018,0,\N,Missing
bauer-etal-2012-dependency,J08-2003,0,\N,Missing
bauer-etal-2012-dependency,S07-1048,0,\N,Missing
bauer-etal-2012-dependency,C04-1186,0,\N,Missing
bauer-etal-2012-dependency,P03-1054,0,\N,Missing
bauer-etal-2012-dependency,P11-1144,0,\N,Missing
bauer-etal-2012-dependency,J02-3001,0,\N,Missing
bauer-etal-2012-dependency,J05-1004,0,\N,Missing
bhatia-etal-2010-empty,J05-1004,1,\N,Missing
bhatia-etal-2010-empty,I08-2099,1,\N,Missing
C00-1007,P98-1006,1,0.717962,"t . for our example input.  In the third experiment, as described in Section 3, we employ the supertag-based tree model whose parameters consist of whether a lexeme ld with supertag sd is a dependent of lm with supertag sm . Furthermore we use the supertag information provided by the XTAG grammar to order the dependents. This model generates There was no cost estimate for the second phase . for our example input, which is indeed the sentence found in the WSJ. As in the case of machine translation, evaluation in generation is a complex issue. We use two metrics suggested in the MT literature (Alshawi et al., 1998) based on string edit distance between the output of the generation system and the reference corpus string from the WSJ. These metrics, simple accuracy and generation accuracy, allow us to evaluate without human intervention, automatically and objectively.4 Simple accuracy is the number of insertion (I ), deletion (D) and substitutions (S ) errors between the target language strings in the test corpus and the strings produced by the generation model. The metric is summarized in Equation (1). R is the number of tokens in the target string. This metric is similar to the string distance metric us"
C00-1007,J99-2004,1,0.386226,"imate α there was α1 γ 3 no γ 1 2 cost for γ γ 2 4 phase α 1 the second γ 1 γ 5 Figure 3: Derivation tree for LTAG derivation of There was no cost estimate for the second phase 3 System Overview is composed of three modules: the Tree Chooser, the Unraveler, and the Linear Precedence (LP) Chooser. The input to the system is a dependency tree as shown in Figure 4. Note that the nodes are labeled only with lexemes, not with supertags.2 The Tree Chooser then uses a stochastic tree model to choose TAG trees for the nodes in the input structure. This step can be seen as analogous to supertagging"" (Bangalore and Joshi, 1999), except that now supertags (i.e., names of trees) must be found for words in a tree rather than for words in a linear sequence. The Unraveler then uses the XTAG grammar to produce a lattice of all possible linearizations that are compatible with the supertagged tree and the XTAG. The LP Chooser then chooses the most likely traversal of this lattice, given a language model. We discuss the three components in more detail. The Tree Chooser draws on a tree model, which is a representation of XTAG derivation for 1,000,000 words of the Wall Street Journal.3 The Tree Chooser makes the simplifying as"
C00-1007,W00-1401,1,0.513445,"ata and both models improve over the baseline LR model. Supertags incorporate richer information such as argument and adjunct distinction, and number and types of arguments. We expect to improve the performance of the supertag-based model by taking these features into account. In ongoing work, we have developed treebased metrics in addition to the string-based presented here, in order to evaluate stochastic generation models. We have also attempted to correlate these quantitative metrics with human qualitative judgements. A detailed discussion of these experiments and results is presented in (Bangalore et al., 2000). 5 Comparison with Langkilde & Knight Langkilde and Knight (1998a) use a handcrafted grammar that maps semantic representations to sequences of words with linearization constraints. A complex semantic structure is translated to a lattice, and a bigram language model then chooses among the possible surface strings encoded in the lattice. The system of Langkilde & Knight, Nitrogen, is similar to Fergus in that generation is divided into two phases, the rst of which results in a lattice from which a surface string is chosen during the second phase using a language model (in our case a trigram mo"
C00-1007,P98-1116,0,0.841501,". However, in other applications for NLG the variety of the output is much bigger, and the demands on the quality of the output somewhat less stringent. A typical example is NLG in the context of (interlingua- or transfer-based) machine translation. Another reason for relaxing the quality of the output may be that not enough time is available to develop a full grammar for a new target language in NLG. In all these cases, stochastic (empiricist"") methods provide an alternative to hand-crafted (ationalist"") approaches to NLG. To our knowledge, the rst to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and"
C00-1007,W98-1426,0,0.875342,". However, in other applications for NLG the variety of the output is much bigger, and the demands on the quality of the output somewhat less stringent. A typical example is NLG in the context of (interlingua- or transfer-based) machine translation. Another reason for relaxing the quality of the output may be that not enough time is available to develop a full grammar for a new target language in NLG. In all these cases, stochastic (empiricist"") methods provide an alternative to hand-crafted (ationalist"") approaches to NLG. To our knowledge, the rst to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and"
C00-1007,A00-2023,0,0.565882,"new target language in NLG. In all these cases, stochastic (empiricist"") methods provide an alternative to hand-crafted (ationalist"") approaches to NLG. To our knowledge, the rst to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and structure of the target text are determined to achieve the overall communicative goal. During sentence planning, linguistic means { in particular, lexical and syntactic means { are determined to convey smaller pieces of meaning. During realization, the speci cation chosen in sentence planning is transformed into a surface string, by linearizing and in ecting words in the sen"
C00-1007,P85-1012,0,0.435381,"ociated with a whole tree (rather than just a phrase-structure rule, for example), we can specify both the predicate-argument structure of the lexeme (by including nodes at which its arguments must substitute) and morphosyntactic constraints such as subject-verb agreement within the structure associated with the lexeme. This property is referred to as TAG's extended domain of locality. Note that in an LTAG, there is no distinction between lexicon and grammar. A sample grammar is shown in Figure 1. We depart from XTAG in our treatment of trees for adjuncts (such as adverbs), and instead follow McDonald and Pustejovsky (1985). While in XTAG the elementary tree for an adjunct contains phrase structure that attaches the adjunct to nodes in another tree with the stag anchored by 1 Det 2 N 3 Aux 4 Prep or 5 Adj adjoins to direction NP right N right S, VP right NP, VP left S right N right Figure 2: Adjunction table for grammar fragment speci ed label (say, VP) from the speci ed direction (say, from the left), in our system the trees for adjuncts simply express their active valency, but not how they connect to the lexical item they modify. This information is kept in the adjunction table which is associated with the gra"
C00-1007,A92-1006,1,0.670138,"the rst to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and structure of the target text are determined to achieve the overall communicative goal. During sentence planning, linguistic means { in particular, lexical and syntactic means { are determined to convey smaller pieces of meaning. During realization, the speci cation chosen in sentence planning is transformed into a surface string, by linearizing and in ecting words in the sentence (and typically, adding function words). As in the work by Langkilde and Knight, our work ignores the text planning stage, but it does address the sentence planning an"
C00-1007,A00-2026,0,0.190954,"stochastic (empiricist"") methods provide an alternative to hand-crafted (ationalist"") approaches to NLG. To our knowledge, the rst to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and structure of the target text are determined to achieve the overall communicative goal. During sentence planning, linguistic means { in particular, lexical and syntactic means { are determined to convey smaller pieces of meaning. During realization, the speci cation chosen in sentence planning is transformed into a surface string, by linearizing and in ecting words in the sentence (and typically, adding function wo"
C00-1007,W94-0319,0,0.0144855,"techniques in NLG were Langkilde and Knight (1998a) and (1998b). In this paper, we present Fergus (Flexible Empiricist/Rationalist Generation Using Syntax). follows Langkilde and Knight's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar. More recent work on aspects of stochastic generation include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000). Before we describe in more detail how we use stochastic models in NLG, we recall the basic tasks in NLG (Rambow and Korelsky, 1992; Reiter, 1994). During text planning, content and structure of the target text are determined to achieve the overall communicative goal. During sentence planning, linguistic means { in particular, lexical and syntactic means { are determined to convey smaller pieces of meaning. During realization, the speci cation chosen in sentence planning is transformed into a surface string, by linearizing and in ecting words in the sentence (and typically, adding function words). As in the work by Langkilde and Knight, our work ignores the text planning stage, but it does address the sentence planning and the realizati"
C00-1007,C98-1112,0,\N,Missing
C00-1007,C98-1006,1,\N,Missing
C02-1138,J99-2004,1,0.680382,"lar ideas can be used to port FERGUS to different domains with little manual effort. 3.1 Description of the FERGUS Surface Realizer Given an underspecified dependency tree representing one sentence as input, FERGUS outputs the best surface string according to its stochastic modeling. Each node in the input tree corresponds to a lexeme. Nodes that are related by grammatical function are linked together. Surface ordering of the lexemes remains unspecified in the tree. FERGUS consists of three models: tree chooser, unraveler, and linear precedence chooser. The tree chooser associates a supertag (Bangalore and Joshi, 1999) from a treeadjoining grammar (TAG) with each node in the underspecified dependency tree. This partially specifies the output string’s surface order; it is constrained by grammatical constraints encoded by the supertags (e.g. subcategorization constraints, voice), but remains free otherwise (e.g. ordering of modifiers). The tree chooser uses a stochastic tree model (TM) to select a supertag for each node in the tree based on local tree context. The unraveler takes the resulting semi-specified TAG derivation tree and creates a word lattice corresponding to all of the potential surface orderings"
C02-1138,C00-1007,1,0.914188,"Missing"
C02-1138,W00-1401,1,0.891425,"rom all strings like these, with duplicates, in the Communicator system by replacing the slot names with fillers according to a probability distribution. Furthermore, dependency parses are assigned to the resulting strings by hand. In the first series of experiments, we ascertain the output quality of FERGUS using the XTAG grammar on different training corpora. We vary the TM’s training corpus to be either PTB or HH. We do the same for the LM’s training corpus. Assessing the output quality of a generator is a complex issue. Here, we select as our metric understandability accuracy, defined in (Bangalore et al., 2000) as quantifying the differPTB LM HH LM PTB TM 0.30 0.37 HH TM 0.38 0.41 Table 2: Average understandability accuracies using XTAG-Based FERGUS for various kinds of training data PTB LM HH LM PTB TM 0.39 0.33 Table 3: Average understandability accuracies using automatically-extracted grammar based FERGUS for various kinds of training data ence between the generator output, in terms of both dependency tree and surface string, and the desired reference output. (Bangalore et al., 2000) finds this metric to correlate well with human judgments of understandability and quality. Understandability accur"
C02-1138,W01-0520,1,0.913667,"a trigram language model (LM), specifying the output string completely. Certain resources are required in order to train FERGUS. A TAG grammar is needed— the source of the supertags with which the semi-specified TAG derivation tree is annotated. There needs to be a treebank in order to obtain the stochastic model TM driving the tree chooser. There also needs to be a corpus of sentences in order to train the language model LM required for the LP chooser. 3.2 Labor-Minimizing Approaches to Training FERGUS The resources that are needed to train FERGUS seem quite labor intensive to develop. But (Bangalore et al., 2001) show that automatically generated version of these resources can be used by FERGUS to obtain quality output. Two kinds of TAG grammar are used in (Bangalore et al., 2001). One kind is a manually developed, broad-coverage grammar for English: the XTAG grammar (XTAG-Group, 2001). It consists of approximately 1000 tree frames. Disadvantages of using XTAG are the considerable amount of human labor expended in its development and the lack of a treebank based on XTAG—the only way to estimate parameters in the TM is to rely on a heuristic mapping of XTAG tree frames onto a pre-existing treebank (Ban"
C02-1138,A00-2018,0,0.00691146,"disadvantages of using the XTAG grammar, but they introduce potential complications— notably, an extracted grammar’s size is often much larger than that of XTAG, typically more than 2000 tree frames, potentially leading to a larger sparse data problem, and also the resulting grammar is not hand-checked. Two kinds of treebank are used in (Bangalore et al., 2001). One kind is the Penn Treebank (Marcus et al., 1993). It consists of approximately 1,000,000 words of hand-checked, bracketed text. The text consists of Wall Street Journal news articles. The other kind of treebank is the BLLIP corpus (Charniak, 2000). It consists of approximately 40,000,000 words of text that has been parsed by a broad-coverage statistical parser. The text consists of Wall Street Journal news and newswire articles. The advantage of the former is that it has been handchecked, whereas the latter has the advantage of being easily produced and hence can easily be enlarged. (Bangalore et al., 2001) experimentally determine how the quality and quantity of the resources used in training FERGUS affect the output quality of the generator. They find that while a better quality annotated corpus (Penn Treebank) results in better mode"
C02-1138,P00-1058,0,0.0261357,"ed in (Bangalore et al., 2001). One kind is a manually developed, broad-coverage grammar for English: the XTAG grammar (XTAG-Group, 2001). It consists of approximately 1000 tree frames. Disadvantages of using XTAG are the considerable amount of human labor expended in its development and the lack of a treebank based on XTAG—the only way to estimate parameters in the TM is to rely on a heuristic mapping of XTAG tree frames onto a pre-existing treebank (Bangalore and Joshi, 1999). Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). These techniques extract a linguistically motivated TAG using heuristics programmed using a modicum of human labor. They nullify the disadvantages of using the XTAG grammar, but they introduce potential complications— notably, an extracted grammar’s size is often much larger than that of XTAG, typically more than 2000 tree frames, potentially leading to a larger sparse data problem, and also the resulting grammar is not hand-checked. Two kinds of treebank are used in (Bangalore et al., 2001). One kind is the Penn Treebank (Marcus et al., 1993). It consists of approximately 1,00"
C02-1138,P95-1034,0,0.0350299,"se stochastic NLG components can be made to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period"
C02-1138,P98-1116,0,0.0580485,"ade to work together, that they can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period soft−merge Request(D−D) Imp−co"
C02-1138,J93-2004,0,0.0246778,"sing the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). These techniques extract a linguistically motivated TAG using heuristics programmed using a modicum of human labor. They nullify the disadvantages of using the XTAG grammar, but they introduce potential complications— notably, an extracted grammar’s size is often much larger than that of XTAG, typically more than 2000 tree frames, potentially leading to a larger sparse data problem, and also the resulting grammar is not hand-checked. Two kinds of treebank are used in (Bangalore et al., 2001). One kind is the Penn Treebank (Marcus et al., 1993). It consists of approximately 1,000,000 words of hand-checked, bracketed text. The text consists of Wall Street Journal news articles. The other kind of treebank is the BLLIP corpus (Charniak, 2000). It consists of approximately 40,000,000 words of text that has been parsed by a broad-coverage statistical parser. The text consists of Wall Street Journal news and newswire articles. The advantage of the former is that it has been handchecked, whereas the latter has the advantage of being easily produced and hence can easily be enlarged. (Bangalore et al., 2001) experimentally determine how the"
C02-1138,W00-0306,0,0.203755,"y can be ported to new domains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period soft−merge Request(D−D) Imp−conf(N) Imp−conf(D) FERGUS"
C02-1138,C00-2126,0,0.0197615,"mains with apparent ease, and that such NLG components can be integrated in a real-time dialog system. 1 Introduction Systems that interact with the user via natural language are in their infancy. As these systems mature and become more complex, it would be desirable for a system developer if there were automatic methods for creating natural language generation (NLG) components that can produce quality output efficiently. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. In contrast, this paper investigates how different stochastic NLG components can be made to work together effectively, whether they can easily be ported to new domains, and whether they can be integrated in a real-time dialog system. DM Dialog Manager SPoT Sentence Planner Implicit−confirm(NEWARK) Implicit−confirm(DALLAS) Request(DEPART−DATE) period soft−merge Request(D−D) Imp−conf(N) Imp−conf(D) FERGUS Surface Generator Flying"
C02-1138,N01-1003,1,0.912551,"ve goal. During sentence planning, linguistic means—in particular, lexical and syntactic means—are determined to convey smaller pieces of meaning. During realization, the specification chosen in sentence planning is transformed into a surface string by linearizing and inflecting words in the sentence (and typically, adding function words). Figure 1 shows how such components cooperate to generate text corresponding to a set of communicative goals. Our work addresses both the sentence planning stage and the realization stage. The sentence planning stage is embodied by the SPoT sentence planner (Walker et al., 2001), while the surface realization stage is embodied by the FERGUS surface realizer (Bangalore and Rambow, 2000). We extend the work of (Walker et al., 2001) and (Bangalore and Rambow, 2000) in various ways. We show that apparently each Features Used all domain-independent task-independent task-dependent of SPoT and FERGUS can be ported to different domains with little manual effort. We then show that these two components can work together effectively. Finally, we show the on-line integration of FERGUS with a dialog system. 2 Testing the Domain Independence of Sentence Planning In this section, w"
C02-1138,C98-1112,0,\N,Missing
C02-1138,C98-1114,1,\N,Missing
C02-1138,P98-1118,1,\N,Missing
C02-2026,P97-1003,0,0.0133068,"antics One type of Natural Language Understanding (NLU) application is exemplified by the database access problem: the user may type in free source language text, but the NLU component must map this text to a fixed set of actions dictated by the underlying application program. We will call such NLU applications “applicationsemantic NLU”. Other examples of applicationsemantic NLU include interfaces to commandbased applications (such as airline reservation systems), often in the guise of dialog systems. Several general-purpose off-the-shelf (OTS) parsers have become widely available (Lin, 1994; Collins, 1997). For application-semantic NLU, it is possible to use such an OTS parser in conjunction with a post-processor which transfers the output of the parser (be it phrase structure or dependency) to the domain semantics. In addition to mapping the parser output to application semantics, the post-processor often must also “correct” the output of the parser: the parser may be tailored for a particular domain (such as main presents linguistic constructions not found in the original domain (such as questions). It may also be the case that the OTS parser consistently misanalyzes certain lexemes because t"
C02-2026,C94-1079,0,0.0338234,"pecific Semantics One type of Natural Language Understanding (NLU) application is exemplified by the database access problem: the user may type in free source language text, but the NLU component must map this text to a fixed set of actions dictated by the underlying application program. We will call such NLU applications “applicationsemantic NLU”. Other examples of applicationsemantic NLU include interfaces to commandbased applications (such as airline reservation systems), often in the guise of dialog systems. Several general-purpose off-the-shelf (OTS) parsers have become widely available (Lin, 1994; Collins, 1997). For application-semantic NLU, it is possible to use such an OTS parser in conjunction with a post-processor which transfers the output of the parser (be it phrase structure or dependency) to the domain semantics. In addition to mapping the parser output to application semantics, the post-processor often must also “correct” the output of the parser: the parser may be tailored for a particular domain (such as main presents linguistic constructions not found in the original domain (such as questions). It may also be the case that the OTS parser consistently misanalyzes certain l"
C02-2026,W02-2214,1,0.870509,"Missing"
C02-2026,J00-1003,0,0.0131956,"beled by the name of an elementary tree machine by the lexicalized version of that tree machine. Of course, in each iteration, there are many more replacements than in the previous iteration. We use 5 rounds of iteration; obviously, the number of iterations restrict the syntactic complexity (but not the length) of recognized input. However, because we output brackets in the FSTs, we obtain a parse with full syntactic/lexical semantic (i.e., dependency) structure, not a “shallow parse”. This construction is in many ways similar to similar constructions proposed for CFGs, in particular that of (Nederhof, 2000). One difference is that, since we start from TAG, recursion is already factored, and we need not find cycles in the rules of the grammar. 5 Experimental Results We present results in which our classes are defined entirely with respect to syntactic behavior. This is because we do not have available an important corpus annotated with semantics. We train on the Wall Street Journal (WSJ) corpus. We evaluate by taking a list of 205 sentences which are chosen at random from entries to W ORDS E YE made by the developers (who were testing the graphical component using a different parser). Their avera"
C10-2117,J99-2004,0,0.0120921,"for the sentence Republican leader Bill Frist said the Senate was hijacked. said Frist hijacked Senate Republican leader Bill was the In the above sentence, said and hijacked are the propositions that should be tagged. Let’s look at hijacked in detail. The feature haveReportingAncestor of hijacked is ‘Y’ because it is a verb with a parent verb said. Similarly, the feature haveDaughterAux would also be ’Y’ because of daughter was, whereas whichAuxIsMyDaughter would get the value was. We also considered several other features which did not yield good results. For example, the token’s supertag (Bangalore and Joshi, 1999), the parent token’s supertag, a binary feature isRoot (Is the word the root of the parse tree?) were deemed not useful. We list the features we experimented with and decided to discard in Table 1. For finding the best performing features, we did an exhaustive search on the feature space, incrementally pruning away features that are not useful. Class Description LC LN SN Lexical features with Context Lexical and Syntactic features with Nocontext Lexical features with Context and Syntactic features with No-context Lexical and Syntactic features with Context LC SN LC SC Table 2: YAMCHA Experimen"
C10-2117,N09-2047,1,0.838087,"llum, 2002) toolkit.3 3.2 Features We divided our features into two types - Lexical and Syntactic. Lexical features are at the token level and can be extracted without any parsing with relatively high accuracy. We expect these features to be useful for our task. For example, isNumeric, which denotes whether the word is a number or alphabetic, is a lexical feature. Syntactic features of a token access its syntactic context in the dependency tree. For example, parentPOS, the POS tag of the parent word in the dependency parse tree, is a syntactic feature. We used the MICA deep dependency parser (Bangalore et al., 2009) for parsing in order to derive the syntactic features. We use MICA because we assume that the relevant information is the 3 1016 http://MALLET.cs.umass.edu/ predicate-argument structure of the verbs, which is explicit in the MICA output. While it is clear that having a perfect parse would yield useful features, current parsers perform at levels of accuracy lower than that of part-of-speech taggers, so that it is not a foregone conclusion that using automatic parser output helps in our task. The list of features we used in our experiments are summarized in Table 1. The column ’Type’ denotes th"
C10-2117,J80-3003,0,0.485333,"Missing"
C10-2117,W09-3012,1,0.488085,"e result of text processing is not a list of facts about the world, but a list of facts about different people’s cognitive states. In this paper, we limit ourselves to the writer’s beliefs, but we specifically want to determine which propositions he or she intends us to believe he or she holds as beliefs, and with what strength. The result of such processing will be a much more fine-grained representation of the information contained in written text than has been available so far. 2 Belief Annotation and Data We use a corpus of 10,000 words annotated for speaker belief of stated propositions (Diab et al., 2009). The corpus is very diverse in terms of genre, and it includes newswire text, email, instructions, and solicitations. The corpus annotates each verbal proposition (clause or small clause), by attaching one of the following tags to the head of the proposition (verbs and heads of nominal, adjectival, and prepositional predications). • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. Committed belief can also include propositions about the future: p"
C10-2117,krestel-etal-2008-minding,0,0.0199995,"Missing"
C10-2117,W00-0730,0,0.0160175,"ures Used a joint model, in which the heads are chosen and classified simultaneously, and a pipeline model, in which heads are chosen first and then classified. In this paper, we consider the joint model in detail and in Section 3.5.3, we present results of the pipeline model; they support our choice. In the joint model, we define a four-way classification task where each token is tagged as one of four classes – CB, NCB, NA, or O (nothing) – as defined in Section 2. For tagging, we experimented with Support Vector Machines (SVM) and Conditional Random Fields (CRF). For SVM, we used the YAMCHA(Kudo and Matsumoto, 2000) sequence labeling system,1 which uses the TinySVM package for classification.2 For CRF, we used the linear chain CRF implementation of 1 2 http://chasen.org/ taku/software/YAMCHA/ http://chasen.org/ taku/software/TinySVM/ the MALLET(McCallum, 2002) toolkit.3 3.2 Features We divided our features into two types - Lexical and Syntactic. Lexical features are at the token level and can be extracted without any parsing with relatively high accuracy. We expect these features to be useful for our task. For example, isNumeric, which denotes whether the word is a number or alphabetic, is a lexical feat"
C10-2117,C08-1101,0,0.0132576,"ning whether a belief in the requirement of p entails the belief in p; instead, we are only interested in whether the writer wants the reader to understand whether the writer holds a belief in the requirement that p or in p directly. This paper is also not concerned with subjectivity (Wiebe et al., 2004), the nature of the proposition p (statement about interior world or external world) is not of interest, only whether the writer wants the reader to believe the writer believes p. This paper is also not concerned with opinion and determining the polarity (or strength) of opinion (for example: (Somasundaran et al., 2008)), which corresponds to the desire dimension. Thus, this work is orthogonal to the extensive literature on opinion classification. The work of (Saur´ı and Pustejovsky, 2007; Saur´ı and Pustejovsky, 2008) is, in many respects, very similar to ours. They propose Factbank, which represents the factual interpretation as modality-polarity pairs, extracted from the basic structural elements denoting factuality encoded by Timebank. Also, they attribute the factuality to specific sources within the text. Our work 1020 is more limited in several ways: we currently only model the writer’s beliefs; we do"
C10-2117,J00-3003,0,0.0798494,"Missing"
C10-2117,J04-3002,0,0.0415265,"n dialog act tagging. This paper is not concerned with issues relating to logics for belief representation or inferencing that can be done on beliefs (for an overview, see (McArthur, 1988)), nor theories of automatic belief ascription (Wilks and Ballim, 1987). For example, this paper is not concerned with determining whether a belief in the requirement of p entails the belief in p; instead, we are only interested in whether the writer wants the reader to understand whether the writer holds a belief in the requirement that p or in p directly. This paper is also not concerned with subjectivity (Wiebe et al., 2004), the nature of the proposition p (statement about interior world or external world) is not of interest, only whether the writer wants the reader to believe the writer believes p. This paper is also not concerned with opinion and determining the polarity (or strength) of opinion (for example: (Somasundaran et al., 2008)), which corresponds to the desire dimension. Thus, this work is orthogonal to the extensive literature on opinion classification. The work of (Saur´ı and Pustejovsky, 2007; Saur´ı and Pustejovsky, 2008) is, in many respects, very similar to ours. They propose Factbank, which re"
C12-1138,P12-2032,1,0.681939,"Missing"
C12-1138,P11-1078,0,0.127936,"ogstruktur und Wörter in den Emails benutzt, um Dialogteilnehmer mit situativer Macht zu identifizieren. Keywords in German: Komputationalle Soziolinguistik, Soziale Netzwerke, Macht, Dialogakte, Dialog. Proceedings of COLING 2012: Technical Papers, pages 2259–2274, COLING 2012, Mumbai, December 2012. 2259 1 Introduction Within an interaction, there is often a power differential between the interactants. This differential is often drawn from a static source external to the interaction, such as a formal or informal power structure or hierarchy. Most computational studies (Creamer et al., 2009; Bramsen et al., 2011; Gilbert, 2012) that analyze power within interactions have used such an external power structure (namely, a corporate hierarchy) as the definition of the power differential. However, the power differential may also be dynamic and specific to the situation of the interaction. We define a person to have situational power if there is another person such that he or she has power or authority over the first person in the context of a situation or task. Such situational power may not always align with the external power structures, if one exists. For example, a Human Resources department employee"
C12-1138,W09-3953,1,0.943339,"they were quoted (Yeh and Harnly, 2006). Most emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails. Table 1 presents some statistics on participants and messages in the corpus. We define an active participant of a given thread as someone who has sent at least one email message in the thread. Apart from the thread level annotations for different types of power, the corpus also contains utterance level annotations for overt displays of power. The same corpus has been previously annotated with dialog act annotations (Hu et al., 2009). We utilize these annotations in our study 2262 Statistic Number of email threads Number of participants Ave. Participants / thread Number of active participants Ave. Active participants / thread Number of messages Ave. Messages / thread Ave. Messages / active participant Number of word tokens Situational Power (SP) Count / Mean (SD) 122 1033 8.47 (13.82) 221 1.81 (0.73) 360 2.95 (2.24) 1.45 (1.01) 20,740 81 Table 1: Corpus statistics and will describe them in more detail in the following sections. We give an example thread and corresponding situational power annotations in Table 2. The examp"
C12-1138,W11-0711,0,0.10461,"needs something that x can choose to provide or not”. They model this dependence “using the exchange-theoretic principle that the need to convince someone who disagrees with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on Situational Power (SP) and leave the other types of power for future work. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in whi"
C12-1138,prabhakaran-etal-2012-annotations,1,0.628308,"es with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on Situational Power (SP) and leave the other types of power for future work. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006). Most emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely s"
C12-1138,N12-1057,1,0.715086,"es with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on Situational Power (SP) and leave the other types of power for future work. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006). Most emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely s"
C12-1138,C10-1117,0,0.0789069,"l attributes such as power, gender, etc. They perform their study on Wikipedia discussion forums and Supreme Court hearings. They also look into situational power; however they define situational power in terms of the dependence between interactants: “x may have power over y in a given situation because y needs something that x can choose to provide or not”. They model this dependence “using the exchange-theoretic principle that the need to convince someone who disagrees with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on S"
C12-1138,C00-2137,0,0.030515,"oth gave the best performing system with an 2270 F measure of 64.4. The best performing single feature was ODPCount, which by itself gave an F measure of 60.0. The results we obtained are in line with the findings from the statistical significance study presented in Section 7. For example, DAP contained the least significant features while ODP contained the most significant feature. The tagger also performed worst when only DAP features were used and best when ODP was used. We assessed the statistical significance of F-measure improvements over baseline, using the Approximate Randomness Test (Yeh, 2000; Noreen, 1989).3 . We found the improvements to be statistically significant (p = 0.001). For the best performing feature set — DLC+ODP — we obtained a mean F-measure (macroaverage) of 64.92 with a standard deviation of 8.82 (please note that Table 4 reports micro-averaged F-measure: 64.4). The low standard deviation suggests that the model built in this setting will obtain comparable performances for new unseen data. This also means that the data from which it was trained on the different folds of the cross-validation is sufficiently consistent to learn a model with predictive power. Put dif"
C16-1043,J09-3007,0,0.0783328,"Missing"
C16-1043,W06-2920,0,0.0431974,"with nonlexical models. We discuss the annotation experiments and results in Section 6. We conclude in Section 7. 2 Related Work There have been a number of investigations into multilingual dependency parsing. For example, Nivre et al. (2007b) presents detailed results for 11 languages using the arc-eager deterministic parsing algorithm included in MaltParser. However, results are reported only for the parser trained on the full training set and would not generalize to situations where training data is limited. Likewise, the 2006 and 2007 CoNLL shared tasks of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) relied on the existence of ample training data for the languages being investigated. Our work differs in that we are interested in the performance of a dependency parser trained on very little data. Duong et al. (2015) approach dependency parsing for a low-resource language as a domain adaptation task; a treebank in a high-resource language is considered out-of-domain, and a much smaller treebank in a low-resource language is considered in-domain. They jointly train a neural network dependency parser to model the syntax of both the high-resource and the low-resource lang"
C16-1043,D15-1040,0,0.0258905,"esents detailed results for 11 languages using the arc-eager deterministic parsing algorithm included in MaltParser. However, results are reported only for the parser trained on the full training set and would not generalize to situations where training data is limited. Likewise, the 2006 and 2007 CoNLL shared tasks of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) relied on the existence of ample training data for the languages being investigated. Our work differs in that we are interested in the performance of a dependency parser trained on very little data. Duong et al. (2015) approach dependency parsing for a low-resource language as a domain adaptation task; a treebank in a high-resource language is considered out-of-domain, and a much smaller treebank in a low-resource language is considered in-domain. They jointly train a neural network dependency parser to model the syntax of both the high-resource and the low-resource language. In this paper, we focus on the alternate approach of training directly on small amounts of data. Guo et al. (2015) also investigate inducing dependency parsers for low-resource languages using training data from high-resource languages"
C16-1043,N10-1115,0,0.0139292,"arser will reduce their workload by letting them correct errors in a dependency structure rather than starting from scratch. This method of syntactic documentation does not limit the field linguist to a particular syntactic theory. We chose to use the universal labels and analyses in our corpus, but WELT users will have complete control over assignment of heads and choice of dependency labels. The only requirement is that they are consistent across sentences. In future work, we will experiment with other parsers, such as TurboParser (Martins et al., 2010), Mate (Bohnet, 2010), and Easy-First (Goldberg and Elhadad, 2010). Furthermore, we will continue to investigate methods of re-using existing parsers and dependency annotations with new languages (see Section 5); specifically, we will investigate more effective methods of adapting existing parsers to other languages. For example, we will investigate how to combine a non-lexical model with a lexical model obtained from a small number of target language sentences. We will also investigate ways for linguists to directly specify syntactic properties that can be used by the parser, similar to the way FLEx converts morphological properties specified by users into"
C16-1043,P15-1119,0,0.0142221,"ted. Our work differs in that we are interested in the performance of a dependency parser trained on very little data. Duong et al. (2015) approach dependency parsing for a low-resource language as a domain adaptation task; a treebank in a high-resource language is considered out-of-domain, and a much smaller treebank in a low-resource language is considered in-domain. They jointly train a neural network dependency parser to model the syntax of both the high-resource and the low-resource language. In this paper, we focus on the alternate approach of training directly on small amounts of data. Guo et al. (2015) also investigate inducing dependency parsers for low-resource languages using training data from high-resource languages. They focus on lexical features, which are not directly transferable among languages, and propose the use of distributed feature representations instead of discrete lexical features. Lacroix et al. (2016) describe a method for transferring dependency parsers across languages by projecting annotations across word alignments and learning from the partially annotated data. However, both of these methods rely on large amounts of (unannotated) data in the target language in orde"
C16-1043,N16-1121,0,0.0535688,"a low-resource language is considered in-domain. They jointly train a neural network dependency parser to model the syntax of both the high-resource and the low-resource language. In this paper, we focus on the alternate approach of training directly on small amounts of data. Guo et al. (2015) also investigate inducing dependency parsers for low-resource languages using training data from high-resource languages. They focus on lexical features, which are not directly transferable among languages, and propose the use of distributed feature representations instead of discrete lexical features. Lacroix et al. (2016) describe a method for transferring dependency parsers across languages by projecting annotations across word alignments and learning from the partially annotated data. However, both of these methods rely on large amounts of (unannotated) data in the target language in order to learn the word embeddings and alignments. It is unclear how well these approaches would work in the context of an endangered language where large amounts of unannotated text will not be available. Our work also differs from the above because our goal is to incorporate a parser into tools for field linguists studying end"
C16-1043,D10-1004,0,0.0179462,"tax into language documentation. The incrementally trained parser will reduce their workload by letting them correct errors in a dependency structure rather than starting from scratch. This method of syntactic documentation does not limit the field linguist to a particular syntactic theory. We chose to use the universal labels and analyses in our corpus, but WELT users will have complete control over assignment of heads and choice of dependency labels. The only requirement is that they are consistent across sentences. In future work, we will experiment with other parsers, such as TurboParser (Martins et al., 2010), Mate (Bohnet, 2010), and Easy-First (Goldberg and Elhadad, 2010). Furthermore, we will continue to investigate methods of re-using existing parsers and dependency annotations with new languages (see Section 5); specifically, we will investigate more effective methods of adapting existing parsers to other languages. For example, we will investigate how to combine a non-lexical model with a lexical model obtained from a small number of target language sentences. We will also investigate ways for linguists to directly specify syntactic properties that can be used by the parser, similar to the w"
C16-1043,H05-1066,0,0.241287,"Missing"
C16-1043,W06-2932,0,0.330188,"per makes three contributions. First, we introduce a new corpus of English, Spanish, German, and Egyptian Arabic descriptions of spatial relations and motion events, which we have annotated with dependency structures and other linguistic information. We focused on spatial relations and motion because one of the other functions of WELT will be to assist field linguists with elicitation of spatial language and documentation of spatial and motion semantics. We are making the corpus available to the public. Second, we compare the performance of two existing dependency parsing packages, MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006), using incrementally increasing amounts of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 440 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 440–449, Osaka, Japan, December 11-17 2016. this training data. Third, we show that using a parser trained on small amounts of data can assist with dependency annotation. In Section 2, we discuss related work. In Section 3, we describe the new publicly"
C16-1043,nivre-etal-2006-maltparser,0,0.342647,"e introduce a new corpus of English, Spanish, German, and Egyptian Arabic descriptions of spatial relations and motion events, which we have annotated with dependency structures and other linguistic information. We focused on spatial relations and motion because one of the other functions of WELT will be to assist field linguists with elicitation of spatial language and documentation of spatial and motion semantics. We are making the corpus available to the public. Second, we compare the performance of two existing dependency parsing packages, MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006), using incrementally increasing amounts of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 440 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 440–449, Osaka, Japan, December 11-17 2016. this training data. Third, we show that using a parser trained on small amounts of data can assist with dependency annotation. In Section 2, we discuss related work. In Section 3, we describe the new publicly available corpus. In Section 4, we"
C16-1043,C08-1085,0,0.0520103,"Missing"
C16-1043,W14-2202,1,0.900449,"Missing"
C16-1043,P14-5009,1,0.904061,"Missing"
C16-1043,L16-1262,0,\N,Missing
C16-1086,W02-0603,0,0.181756,"single AG of (Sirts and Goldwater, 2013). LIMS is our main contribution, and its performance on unseen languages in the cross-validation is our main result. 2 Related Work Early research on unsupervised morphological segmentation was performed by extensive manual rule engineering, which was very expensive. With the advance of machine learning, minimum description length (MDL) based unsupervised approaches were applied for morphological segmentation in several languages (Goldsmith, 2001). However, this required extensive manual work, which was then replaced by maximum likelihood optimization (Creutz and Lagus, 2002). Morfessor (Creutz and Lagus, 2007) is a commonly used system for unsupervised morphological segmentation. It is based on a generative probabilistic model. Because of its broad use, we use Morfessor as a reference system in this paper. Another system was developed by Poon et al. (2009), who apply classic log-linear models that use contextual and global features. Nonparametric Bayesian methods with probabilistic grammars, such as Dirichlet process mixture models (see Antoniak (1974), Pitman (2002)) are widely used in unsupervised learning for NLP. Johnson et al. (2007) introduce nonparametric"
C16-1086,J01-2001,0,0.108328,"cal segmentation system, and show that it always outperforms Morfessor and on five out of six languages outperforms the best single AG of (Sirts and Goldwater, 2013). LIMS is our main contribution, and its performance on unseen languages in the cross-validation is our main result. 2 Related Work Early research on unsupervised morphological segmentation was performed by extensive manual rule engineering, which was very expensive. With the advance of machine learning, minimum description length (MDL) based unsupervised approaches were applied for morphological segmentation in several languages (Goldsmith, 2001). However, this required extensive manual work, which was then replaced by maximum likelihood optimization (Creutz and Lagus, 2002). Morfessor (Creutz and Lagus, 2007) is a commonly used system for unsupervised morphological segmentation. It is based on a generative probabilistic model. Because of its broad use, we use Morfessor as a reference system in this paper. Another system was developed by Poon et al. (2009), who apply classic log-linear models that use contextual and global features. Nonparametric Bayesian methods with probabilistic grammars, such as Dirichlet process mixture models (s"
C16-1086,W08-0704,0,0.547438,"t in NLP for lowresource languages, unsupervised morphological segmentation becomes a crucial pre-processing step to reduce data sparseness: instead of working on a large vocabulary of plausible words, a smaller set of smaller word units is processed. A well-known toolkit used for unsupervised segmentation is Morfessor (Creutz and Lagus, 2007), which is a generative probabilistic model. In competition, Adaptor Grammars (AGs) (Johnson et al., 2007) represent a framework for specifying compositional nonparametric Bayesian models and are applied in unsupervised segmentation with notable success (Johnson, 2008). AGs generalize probabilistic context-free grammars by allowing some nonterminals to be “adapted, which allows for dependencies between applications of these rules. Sirts and Goldwater (2013) present an in-depth investigation of the use of AGs. They make two important contributions. First, they discuss the effect of the underlying grammar on the results of unsupervised morphological segmentation. Second, they investigate two ways of using a small amount of annotated data during training. They show that while for English, Morfessor remains the top performing system, on three other languages th"
C16-1086,N09-1024,0,0.205729,"very expensive. With the advance of machine learning, minimum description length (MDL) based unsupervised approaches were applied for morphological segmentation in several languages (Goldsmith, 2001). However, this required extensive manual work, which was then replaced by maximum likelihood optimization (Creutz and Lagus, 2002). Morfessor (Creutz and Lagus, 2007) is a commonly used system for unsupervised morphological segmentation. It is based on a generative probabilistic model. Because of its broad use, we use Morfessor as a reference system in this paper. Another system was developed by Poon et al. (2009), who apply classic log-linear models that use contextual and global features. Nonparametric Bayesian methods with probabilistic grammars, such as Dirichlet process mixture models (see Antoniak (1974), Pitman (2002)) are widely used in unsupervised learning for NLP. Johnson et al. (2007) introduce nonparametric Bayesian models on whole tree structures, namely; Adaptor Grammars (AGs). AGs provide a flexible distribution over parse trees and are successfully applied in unsupervised segmentation (Johnson, 2008). Sirts and Goldwater (2013) explore the use of AGs for minimally supervised morphologi"
C16-1086,Q13-1021,0,0.139039,"y of plausible words, a smaller set of smaller word units is processed. A well-known toolkit used for unsupervised segmentation is Morfessor (Creutz and Lagus, 2007), which is a generative probabilistic model. In competition, Adaptor Grammars (AGs) (Johnson et al., 2007) represent a framework for specifying compositional nonparametric Bayesian models and are applied in unsupervised segmentation with notable success (Johnson, 2008). AGs generalize probabilistic context-free grammars by allowing some nonterminals to be “adapted, which allows for dependencies between applications of these rules. Sirts and Goldwater (2013) present an in-depth investigation of the use of AGs. They make two important contributions. First, they discuss the effect of the underlying grammar on the results of unsupervised morphological segmentation. Second, they investigate two ways of using a small amount of annotated data during training. They show that while for English, Morfessor remains the top performing system, on three other languages their approach can beat the high Morfessor baseline. A typical application for unsupervised morphological segmentation involves situations in which we are confronted with a low-resource language"
C16-1086,P08-1084,0,0.0502158,". (2007) introduce nonparametric Bayesian models on whole tree structures, namely; Adaptor Grammars (AGs). AGs provide a flexible distribution over parse trees and are successfully applied in unsupervised segmentation (Johnson, 2008). Sirts and Goldwater (2013) explore the use of AGs for minimally supervised morphological segmentation. They also compare the performance of different grammar trees. In this paper, we explore a much larger set of grammars, and simulate the performance of doing supervised morphological segmentation without the use of any annotated data or scholar-seeded knowledge. Snyder and Barzilay (2008) propose a discriminative model for unsupervised morphological segmentation by using morphological chains to model the word formation process. A main drawback in their system is the slow learning curve, which requires a vast amount of data to learn from. Wang et al. (2016) propose novel neural network architectures that learn the structure of input sequences directly from raw input words and are subsequently able to predict morphological boundaries. The architectures rely on Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997). 3 Problem Definition, Data, and Experimental Setup The"
C16-1086,C10-1116,0,0.802679,"sampling iterations Zulu 50,597 1,000 instead of 1,000 iterations; early experiments showed that the reEstonian 50,374 1,497 sults of 500 iterations are nearly as good as 1,000 iterations, while fewer iterations decreased performance. We adapt all nontermiTable 1: Size of corpora (in types) nals except nontermials with recursive rules. For the AG results, of our development languages we report the average of five different runs. We also run Morfes3 sor2 as a baseline (Virpioja et al., 2013). We evaluate the segmentation against the DEV data from MorphoChallenge. Our evaluation metric is EMMA (Spiegler and Monson, 2010), which is based on morph recognition. EMMA has the advantage that it can return a meaningful result on unsegmented words (also see (Virpioja et al., 2011)). 4 Underlying Grammars There are three fundamental dimensions in designing the grammars. The first dimension is how the grammar generates prefix, stem, and suffix. The first option is that a grammar does not explicitly model the division into prefix, stem, and suffix at all and only has morphs (“morph-only”); this is illustrated by Morph+SM in Figure 1. If we assume that we do want an explicit modeling of prefixes, stems, and suffixes, we"
C16-1086,C10-1115,0,0.527362,"Missing"
C16-1207,W11-0705,1,0.550627,"time since the publication of (Jørgensen et al., 2016), we have not been able to obtain their data or system to compare to ours, which we intend to do in the future. Other research has used statistical approaches to automatically characterize dialect variation in Twitter across cities and to show how the geographical distribution of lexical variation changes over time (Eisenstein, 2015). There has been quite a bit of work examining other kinds of phenomena on Twitter; researchers have developed systems to analyze accommodation (Danescu-Niculescu-Mizil et al., 2011), sentiment analysis (e.g., (Agarwal et al., 2011; Rosenthal et al., 2015)) and clues to geolocation (Dredze et al., 2016). 3 Our Corpus 3.1 Data Collection, Corpus, and Qualitative Analysis To create our corpus, we analyzed publicially available Twitter communication from Gakirah Barnes, who became a gang member in Chicago at age 13 and was killed at age 17, as well as tweets from people who communicated with her. Barnes changed her Twitter handle to @TyquanAssassin in memory of her friend Tyquan Tyler, who was killed in 2012. She subsequently swore to avenge Tyler’s death and became a known gang leader with 9 killings to her name before sh"
C16-1207,P07-1033,0,0.0401474,"Missing"
C16-1207,R13-1026,0,0.0154157,"d network data (Radil et al., 2010) and automated the analysis of graffiti style features (Piergallini et al., 2014) to predict gang affiliation. Research has also studied the psychological impact of crime on urban populations by analyzing social media, finding that crime exposure over a year can result in negative emotion and anxiety (Valdes et al., 1 The dataset is available at http://dx.doi.org/10.7916/D84F1R07 . 2197 2015). Yet others are analyzing news reports to build a database of gun violence incidents (Pavlick and Callison-Burch, 2016). There has been work on POS tagging for Twitter (Derczynski et al., 2013; Owoputi et al., 2013), including for other languages (Rehbein, 2013). We discuss (Owoputi et al., 2013) in detail in Section 4.1 The most closely related work is (Jørgensen et al., 2016), which studies African American Vernacular English in three genres (movie scripts, lyrics, tweets). They use a very large unlabeled corpus. In contrast, we use a small labeled corpus and investigate domain adaptation using additional data. Given the short amount of time since the publication of (Jørgensen et al., 2016), we have not been able to obtain their data or system to compare to ours, which we intend"
C16-1207,N16-1122,0,0.0192531,"able to obtain their data or system to compare to ours, which we intend to do in the future. Other research has used statistical approaches to automatically characterize dialect variation in Twitter across cities and to show how the geographical distribution of lexical variation changes over time (Eisenstein, 2015). There has been quite a bit of work examining other kinds of phenomena on Twitter; researchers have developed systems to analyze accommodation (Danescu-Niculescu-Mizil et al., 2011), sentiment analysis (e.g., (Agarwal et al., 2011; Rosenthal et al., 2015)) and clues to geolocation (Dredze et al., 2016). 3 Our Corpus 3.1 Data Collection, Corpus, and Qualitative Analysis To create our corpus, we analyzed publicially available Twitter communication from Gakirah Barnes, who became a gang member in Chicago at age 13 and was killed at age 17, as well as tweets from people who communicated with her. Barnes changed her Twitter handle to @TyquanAssassin in memory of her friend Tyquan Tyler, who was killed in 2012. She subsequently swore to avenge Tyler’s death and became a known gang leader with 9 killings to her name before she was in turn shot and killed at age 17. We focus on Gakirah because she"
C16-1207,N16-1130,0,0.0184473,"l impact of crime on urban populations by analyzing social media, finding that crime exposure over a year can result in negative emotion and anxiety (Valdes et al., 1 The dataset is available at http://dx.doi.org/10.7916/D84F1R07 . 2197 2015). Yet others are analyzing news reports to build a database of gun violence incidents (Pavlick and Callison-Burch, 2016). There has been work on POS tagging for Twitter (Derczynski et al., 2013; Owoputi et al., 2013), including for other languages (Rehbein, 2013). We discuss (Owoputi et al., 2013) in detail in Section 4.1 The most closely related work is (Jørgensen et al., 2016), which studies African American Vernacular English in three genres (movie scripts, lyrics, tweets). They use a very large unlabeled corpus. In contrast, we use a small labeled corpus and investigate domain adaptation using additional data. Given the short amount of time since the publication of (Jørgensen et al., 2016), we have not been able to obtain their data or system to compare to ours, which we intend to do in the future. Other research has used statistical approaches to automatically characterize dialect variation in Twitter across cities and to show how the geographical distribution o"
C16-1207,P07-2045,0,0.00528189,"Missing"
C16-1207,N13-1039,0,0.0868543,"Missing"
C16-1207,E14-1012,0,0.394258,"Missing"
C16-1207,S13-2079,0,0.0207509,"n tweets depends on identifying the emotion expressed. We use the Dictionary of Affect in Language (DAL) in order to obtain the emotional content of individual words. The DAL is a lexicon that maps over 8000 English words to a three dimensional score. The three dimensions of this score are pleasantness; activation, which is a measure of a word’s intensity; and imagery, which is a measure of the ease with which a word can be visualized. Our system extends the DAL with WordNet in order to identify the emotional content of Standard English words in our data that do not occur in the DAL following Rosenthal and McKeown (2013). For each word that is not found in the DAL and is found in WordNet, the synonyms from the first (most common) synset are searched against the DAL. We assume that the emotion of a synonym will be similar to that of the original word. Thus, if there is a match between the synonyms and the DAL, the emotion score of the synonym is used for the original word. A more difficult task is to apply the DAL to the nonstandard English and Twitter-specific elements of the tweets. We assume each token that is not found in the DAL or WordNet is not a Standard English word. We considered various lexicons for"
C16-1207,S15-2078,0,0.0470825,"Missing"
C16-1326,al-sabbagh-girju-2012-yadac,0,0.0609447,"Missing"
C16-1326,L16-1207,1,0.798443,"nalyzer for the new dialect. Because of the close relationship among the variants of Arabic, we also make use of an existing analyzer for MSA, and (in the case of Levantine), an existing Egyptian analyzer. The resulting analyzer is then used in a tagger, which uses the annotated corpus to learn classifiers that choose among all possible analyses. Crucially, we do not require the corpus to be fully annotated, allowing the annotator to concentrate on the most frequent words. We are currently annotating five more dialects with this sort of corpus, with initial corpora available for two dialects (Al-Shargi et al., 2016). The primary contribution of this paper is that we describe a methodology for creating morphological analyzers and taggers for any Arabic dialect. We show the effectiveness of our approach by measuring performance based on training corpora of different sizes. A secondary contribution of this paper is that we present new resources for Levantine Arabic (a morphological analyzer and a morphological tagger), which to our knowledge are the first of their kinds. While we restrict our attentions to Arabic and its dialects, we believe our approach may be relevant to other situations in which we face"
C16-1326,bouamor-etal-2014-multidialectal,1,0.927048,"Missing"
C16-1326,J95-4004,0,0.729663,"Missing"
C16-1326,E06-1047,1,0.913803,"Missing"
C16-1326,W05-0708,0,0.0946921,"Missing"
C16-1326,W13-2301,1,0.926411,"x∼ar be late PV+PVSUFF SUBJ:2MS verb VRB AtAxr HQ HQ  lhAlwqt waqt I»ñÊêË lhlwkt I ¯ñËAêË time PREP+DEM PRON+DET+NOUN noun NOM wqt ?? ?? ? ? PUNC punc PNX ?   k AK@   Ë èñK. @ ñË A sAlw ˆ Abwh lyˇs AtAxrt ˆ lhlwkt? Table 1: An example Levantine sentence ? I»ñÊêË HQ ‘His father asked him why he was late?’. The various columns are for the CODA spelling and different morphological features: lemma, gloss, Buckwalter POS tag, two reduced POS tags and stem. 3.2 Egyptian Data Corpus We use the Egyptian Arabic corpora developed by the Linguistic Data Consortium (LDC) (Maamouri et al., 2012; Eskander et al., 2013a). The corpora are morphologically annotated in a similar style to the annotations done at the LDC for MSA. Words are provided with contextually appropriate CODA form, lemmas, POS tags (Buckwalter, 2004), and English glosses. We ran the E GY corpus through our E GY morphological analyzer CALIMAEgy (Habash et al., 2012b) in order to generate morphological features similar to the ones described in MADAMIRA (Pasha et al., 2014). In this process, we replaced the human-annotated corpus analysis by the closest CALIMAEgy analysis when it does not match any of the CALIMAEgy analyses for a given word."
C16-1326,D13-1105,1,0.910991,"x∼ar be late PV+PVSUFF SUBJ:2MS verb VRB AtAxr HQ HQ  lhAlwqt waqt I»ñÊêË lhlwkt I ¯ñËAêË time PREP+DEM PRON+DET+NOUN noun NOM wqt ?? ?? ? ? PUNC punc PNX ?   k AK@   Ë èñK. @ ñË A sAlw ˆ Abwh lyˇs AtAxrt ˆ lhlwkt? Table 1: An example Levantine sentence ? I»ñÊêË HQ ‘His father asked him why he was late?’. The various columns are for the CODA spelling and different morphological features: lemma, gloss, Buckwalter POS tag, two reduced POS tags and stem. 3.2 Egyptian Data Corpus We use the Egyptian Arabic corpora developed by the Linguistic Data Consortium (LDC) (Maamouri et al., 2012; Eskander et al., 2013a). The corpora are morphologically annotated in a similar style to the annotations done at the LDC for MSA. Words are provided with contextually appropriate CODA form, lemmas, POS tags (Buckwalter, 2004), and English glosses. We ran the E GY corpus through our E GY morphological analyzer CALIMAEgy (Habash et al., 2012b) in order to generate morphological features similar to the ones described in MADAMIRA (Pasha et al., 2014). In this process, we replaced the human-annotated corpus analysis by the closest CALIMAEgy analysis when it does not match any of the CALIMAEgy analyses for a given word."
C16-1326,P05-1071,1,0.852418,"Missing"
C16-1326,habash-etal-2012-conventional,1,0.931467,"n be done with a small annotation effort. In both our previous work and our new one, we use an analyze-and-choose approach to morphological tagging, following the work of Hajiˇc (2000) (also used by Habash and Rambow (2005) for MSA). We also compare against our previous work in our evaluation. 3 3.1 Data Orthography Arabic dialects do not have a standard orthography. This is a big challenge to the annotation process as it allows the coexistence of uninteresting orthographic variations. To address this challenge, we previously developed the Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012a). The CODA choices aim at reducing differences between variants (DA and MSA) when possible while maintaining the distinctive morphological inventories of the different variants. The first CODA specifications were developed for Egyptian Arabic (henceforth E GY) and utilized in the E GY corpus which we also use (Maamouri et al., 2012). The E GY CODA guidelines were extended to Levantine Arabic (henceforth L EV) by the creators of the L EV corpus we use (Jarrar et al., 2014). Since the L EV corpus was annotated without diacritics, all diacritics were also stripped from the E GY corpus for the s"
C16-1326,W12-2301,1,0.920026,"Missing"
C16-1326,N13-1044,1,0.914769,"Missing"
C16-1326,A00-2013,0,0.238912,"Missing"
C16-1326,W14-3603,1,0.81476,"c variations. To address this challenge, we previously developed the Conventional Orthography for Dialectal Arabic (CODA) (Habash et al., 2012a). The CODA choices aim at reducing differences between variants (DA and MSA) when possible while maintaining the distinctive morphological inventories of the different variants. The first CODA specifications were developed for Egyptian Arabic (henceforth E GY) and utilized in the E GY corpus which we also use (Maamouri et al., 2012). The E GY CODA guidelines were extended to Levantine Arabic (henceforth L EV) by the creators of the L EV corpus we use (Jarrar et al., 2014). Since the L EV corpus was annotated without diacritics, all diacritics were also stripped from the E GY corpus for the study we present in this paper. The only exception is that lemmas are represented using diacritics in the corpora for both dialects so that fine-grained distinctions between different lexemes can be made. 3456 Word CODA Lemma Gloss BW Tag POS POS5 Stem ˆ ˆ ˆ ˆ ñË A sAlw éË A sAlh saAal ask PV+PVSUFF SUBJ:3MS+PVSUFF DO:3MS verb VRB sAl èñK. @ Abwh èñK. @ Abwh Ab father NOUN+POSS PRON 3MS noun NOM Ab  Ë lyˇs  Ë lyˇs layˇs why INTERROG ADV adv interrog PRT lyˇs    k A"
C16-1326,L16-1679,1,0.826526,"Missing"
C16-1326,maamouri-etal-2014-developing,1,0.870461,"Missing"
C16-1326,masmoudi-etal-2014-corpus,1,0.906692,"Missing"
C16-1326,mohamed-etal-2012-annotating,0,0.0504507,"Missing"
C16-1326,pasha-etal-2014-madamira,1,0.933304,"Missing"
C16-1326,W11-2602,1,0.914567,"Missing"
C16-1326,voss-etal-2014-finding,0,0.0978076,"Missing"
C16-1326,P06-1073,0,0.0789517,"Missing"
C98-1102,C96-1058,0,0.0831199,"ime complexity of the algorithm is O(nalalQm~×), where G is the number of ruleFSMs derived from the dependency and LP rules in the g r a m m a r and Qmax is the m a x i m u m number of states in any of the rule-FSMs. 4 A F o r m a l i z a t i o n of PP-Dependency Grammars Recall that in a pseudo-projective tree, we make a distinction between a syntactic governor and a linear governor. A node can bc &quot;lifted&quot; along a lifting path from being a dependent of its syntactic governor to being a dependent of its linear ~This type of parser has been proposed previously. See for example (Lombardi, 1996; Eisner, 1996), who also discuss Early-style parsers for projective dependency grammars. 6We can use pre-computed top-down prediction to limit the number of pairs added. 649 governor, which nmst be an ancestor of the governor. In defining a formal rewriting system for pseudo-projective trees, we will not attempt to model the &quot;lifting&quot; as a transformational step in the derivation. Rather, we will directly derive the &quot;lifted&quot; version of the tree, where a node is dependent of its linear governor. Thus, the derived structure resembles more a unistratal dependency representation like those used by (Hudson, 1990)"
C98-1102,1995.iwpt-1.23,1,0.317064,"Missing"
C98-1102,P97-1043,0,0.102938,"Missing"
C98-1102,P97-1003,0,0.0349333,"sable Non-Projective Dependency Grammar Sylvain Kahane* and Alexis Nasr t and Owen Rambow ~ • T A L A N A Universitfi Paris 7 (sk(~ccr. j u s s i e u . f r ) LIA Universit~ d &apos; A v i g n o n ( a l e x i s . n a s r O l i a , u n i v - a v i g n o n , f r ) ~cCoGenTex, Inc. ( o w e n @ c o g e n t e x . c o m ) 1 Introduction Dependency g r a m m a r has a long tradition in syntactic theory, dating back to at least TesniSre&apos;s work from the thirties. 1 Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not. One problem that has posed an impediment to more wide-spread acceptance of dependency grammars is the fact that there is no computationally tractable version of dependency g r a m m a r which is not restricted to projective analyses. However, it is well known that there are some syntactic phenomena (such as wh-movement in English or clitic climbing in Romance) that require nonprojective analyses. In this paper, we present a form of projectivity which we call pseudoprojectivity, and we pr"
C98-1102,C96-2122,0,\N,Missing
C98-1114,A92-1006,1,0.918939,"ts add to it. All modules use declarative knowledge bases distinguished from the generator engine. This facilitates the reuse of the framework for new application domains with minimal impact on the modules composing the generator. As a result, P R E S E N T O R c a n allow non-programmers to develop their own generator applications. Specifically, PRESENTOR uses the following types of knowledge bases: • E n v i r o n m e n t variables: an open list of variables with corresponding values used to specify the configuration. • E x e m p l a r s : a library of schema-like structures (McKeown, 1985; Rambow and Korelsky, 1992) specifying the presentation to be generated at different levels of abstraction (rhetori719 cal, conceptual, syntactic, surface form). • R h e t o r i c a l dictionary: a knowledge base indicating how to realize rhetorical relations linguistically. • C o n c e p t u a l dictionary: a knowledge base used to map language-independent conceptual structures to language-specific syntactic structures. • L i n g u i s t i c g r a m m a r : transformation rules specifying the transformation of syntactic structures into surface word forms and punctuation marks. • Lexicon: a knowledge base containing the"
C98-1114,W94-0319,0,0.0254216,"syntactic, and surface form) and which can be used for deep or shallow generation. In Section 2, we describe the overall architecture of PRESENTOR. In Section 3 to Section 6, we present the different specifications used to define domain communication knowledge and linguistic knowledge. Finally, in Section 7, we describe the outlook for PRESENTOR. 2 PRESENTOR A r c h i t e c t u r e The architecture of PRESENTOR illustrated in Figure 1 consists of a core generator with several associated knowledge bases. The core generator has a pipeline architecture which is similar to many existing systems (Reiter, 1994): an incoming request is received by the generator interface triggering sequentially the macroplanning, micro-planning, realization and fiCore Generator Configurable Knowledge I t r . . . . . . &apos; * -@- / II Manager i i ~ I L _ _ "" Request i E &apos;~ o ~_e s en[a[i_ou i Realizer (Realpro) la Figure 1: Architecture of PRESENTOR nally the formatting of a presentation which is then returned by the system. This pipeline architecture Ininimizes the interdependencies between the different modules facilitating the upgrade of each module with minimal impact on tile overall system. It has been proposed that"
C98-1114,A97-1037,1,\N,Missing
C98-1114,A97-1039,1,\N,Missing
D07-1116,N07-2014,1,0.898226,"Missing"
D07-1116,W05-0711,0,0.0816376,"Missing"
D07-1116,W04-1612,0,0.15119,"Missing"
D07-1116,P06-1073,0,0.324405,"Missing"
D07-1116,P97-1003,0,\N,Missing
D10-1100,W10-1803,1,0.49406,"Missing"
D10-1100,P04-1054,0,0.48757,"Missing"
D10-1100,doddington-etal-2004-automatic,0,0.109113,"Missing"
D10-1100,P10-1015,0,0.0971986,"Missing"
D10-1100,P05-1053,0,0.0810504,"Missing"
D10-1100,P08-1030,0,0.0421266,"Missing"
D10-1100,P04-1043,0,0.147308,"ion task. Now we present the “discrete” structures followed by the kernel we used. We use the structures previously used by Nguyen et al. (2009), and propose one new structure. Although we experimented with all of their structures,3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs. Following are the structures that we refer to in our experiments and results section: PET: This refers to the smallest common phrase structure tree that contains the two target entities. Dependency Words (DW) tree: This is the smallest common dependency tree that contains the two target entities. In Figure 1, since the target entities are at the leftmost and rightmost branch of the depen3 We omitted SK6, which is the worst performing sequence kernel in (Nguyen et al., 2009). 1028 said ccomp T1-Individual info"
D10-1100,E06-1015,0,0.0522221,"ucture that we introduce which, to the best of our knowledge, has not been used before for similar tasks. It is the sequence of nodes from one target to the other in the GRW tree. For example, in Figure 1, this would be Toujan Faisal nsubj T1Individual said ccomp informed prep by T2-Group pobj committee. We also use combinations of these structures (which we refer to as “combined-structures”). For example, PET GR SqGRW means we used the three structures (PET, GR and SqGRW) together with a kernel that calculates similarity between forests. We use the Partial Tree (PT) kernel, first proposed by Moschitti (2006a), for structures derived from dependency trees and Subset Tree (SST) kernel, proposed by Collins and Duffy (2002), for structures derived from phrase structure trees. PT is a relaxed version of the SST; SST measures the similarity between two PSTs by counting all subtrees common to the two PSTs. However, there is one constraint: all daughter nodes of a node must be included. In PTs this constraint is removed. Therefore, in contrast to SSTs, PT kernels compare many more substructures. They have been used successfully by (Moschitti, 2004) for the task of semantic role labeling. The choices we"
D10-1100,D09-1143,0,0.197935,"e a recursive calculation over the “parts” of a discrete structure. This calculation is usually made computationally efficient using Dynamic Programming techniques. Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data). Therefore, we use convolution kernels with a linear learning machine (Support Vector Machines) for our classification task. Now we present the “discrete” structures followed by the kernel we used. We use the structures previously used by Nguyen et al. (2009), and propose one new structure. Although we experimented with all of their structures,3 here we only present the ones that perform best for our classification task. All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs. Following are the structures that we refer"
D10-1100,W02-1010,0,0.154296,"Missing"
D10-1100,P06-1104,0,0.038377,"Missing"
D10-1100,P05-1052,0,0.239814,"Missing"
D10-1100,D07-1076,0,0.107128,"Missing"
D10-1112,W09-0807,0,0.0986682,"Missing"
D10-1112,hughes-etal-2006-reconsidering,0,0.0902646,"Missing"
D10-1112,N01-1020,0,0.0441157,"German dialects has been taken by the ChochichästliOrakel.1 By specifying the pronunciation of ten predefined words, the web site creates a probability map that shows the likelihood of these pronunciations in the Swiss German dialect area. Our model is heavily inspired by this work, but extends the set of cues to the entire lexicon. As mentioned, the ID model is based on a large Swiss German lexicon. Its derivation from a Standard German lexicon can be viewed as a case of lexicon induction. Lexicon induction methods for closely related languages using phonetic similarity have been proposed by Mann and Yarowsky (2001) and Schafer and Yarowsky (2002), and applied to Swiss German data by Scherrer (2007). The extraction of digital data from hand-drawn dialectological maps is a time-consuming task. Therefore, the data should be made available for different uses. Our Swiss German raw data is accessible 1 http://dialects.from.ch 1152 on an interactive web page (Scherrer, 2010), and we have proposed ideas for reusing this data for machine translation and dialect parsing (Scherrer and Rambow, 2010). An overview of digital dialectological maps for other languages is available on http://www.ericwheeler.ca/atlaslist."
D10-1112,W02-2026,0,0.0235376,"n by the ChochichästliOrakel.1 By specifying the pronunciation of ten predefined words, the web site creates a probability map that shows the likelihood of these pronunciations in the Swiss German dialect area. Our model is heavily inspired by this work, but extends the set of cues to the entire lexicon. As mentioned, the ID model is based on a large Swiss German lexicon. Its derivation from a Standard German lexicon can be viewed as a case of lexicon induction. Lexicon induction methods for closely related languages using phonetic similarity have been proposed by Mann and Yarowsky (2001) and Schafer and Yarowsky (2002), and applied to Swiss German data by Scherrer (2007). The extraction of digital data from hand-drawn dialectological maps is a time-consuming task. Therefore, the data should be made available for different uses. Our Swiss German raw data is accessible 1 http://dialects.from.ch 1152 on an interactive web page (Scherrer, 2010), and we have proposed ideas for reusing this data for machine translation and dialect parsing (Scherrer and Rambow, 2010). An overview of digital dialectological maps for other languages is available on http://www.ericwheeler.ca/atlaslist. 3 Swiss German dialects The Ger"
D10-1112,P07-3010,1,0.843,"ten predefined words, the web site creates a probability map that shows the likelihood of these pronunciations in the Swiss German dialect area. Our model is heavily inspired by this work, but extends the set of cues to the entire lexicon. As mentioned, the ID model is based on a large Swiss German lexicon. Its derivation from a Standard German lexicon can be viewed as a case of lexicon induction. Lexicon induction methods for closely related languages using phonetic similarity have been proposed by Mann and Yarowsky (2001) and Schafer and Yarowsky (2002), and applied to Swiss German data by Scherrer (2007). The extraction of digital data from hand-drawn dialectological maps is a time-consuming task. Therefore, the data should be made available for different uses. Our Swiss German raw data is accessible 1 http://dialects.from.ch 1152 on an interactive web page (Scherrer, 2010), and we have proposed ideas for reusing this data for machine translation and dialect parsing (Scherrer and Rambow, 2010). An overview of digital dialectological maps for other languages is available on http://www.ericwheeler.ca/atlaslist. 3 Swiss German dialects The German-speaking area of Switzerland encompasses the Nort"
D10-1112,2010.jeptalnrecital-demonstration.9,1,0.729486,"d on a large Swiss German lexicon. Its derivation from a Standard German lexicon can be viewed as a case of lexicon induction. Lexicon induction methods for closely related languages using phonetic similarity have been proposed by Mann and Yarowsky (2001) and Schafer and Yarowsky (2002), and applied to Swiss German data by Scherrer (2007). The extraction of digital data from hand-drawn dialectological maps is a time-consuming task. Therefore, the data should be made available for different uses. Our Swiss German raw data is accessible 1 http://dialects.from.ch 1152 on an interactive web page (Scherrer, 2010), and we have proposed ideas for reusing this data for machine translation and dialect parsing (Scherrer and Rambow, 2010). An overview of digital dialectological maps for other languages is available on http://www.ericwheeler.ca/atlaslist. 3 Swiss German dialects The German-speaking area of Switzerland encompasses the Northeastern two thirds of the Swiss territory, and about two thirds of the Swiss population define (any variety of) German as their first language. In German-speaking Switzerland, dialects are used in speech, while Standard German is used nearly exclusively in written contexts"
D13-1105,brants-hansen-2002-developments,0,0.0447376,"Missing"
D13-1105,E12-1067,0,0.0121916,"articular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological inflectional classes in a language. Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. 1033 The concept of a paradigm is also used in many published efforts on unsupervised learning of morphology, although not always in a way consistent with its use in linguistics. For instance, Snover et al. (2002) (and later on Can and Manandhar (2012)) define a paradigm as “a set of suffixes and the stems that attach to those suffixes and no others”. This definition is quite limited since it is not modeling the notion of lexeme. Chan (2006) defines a simpler concept of paradigms in his probabilistic paradigm model, which has many limitations, such as not handling syncretism or irregular morphology, nor distinguishing inflection and derivation. Dreyer and Eisner (2011) learn complete German verb paradigms from a small set of complete seed paradigms (50 or 100), which they choose randomly from all verbs in the language. They model stem chang"
D13-1105,W06-3209,0,0.0745043,". Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. 1033 The concept of a paradigm is also used in many published efforts on unsupervised learning of morphology, although not always in a way consistent with its use in linguistics. For instance, Snover et al. (2002) (and later on Can and Manandhar (2012)) define a paradigm as “a set of suffixes and the stems that attach to those suffixes and no others”. This definition is quite limited since it is not modeling the notion of lexeme. Chan (2006) defines a simpler concept of paradigms in his probabilistic paradigm model, which has many limitations, such as not handling syncretism or irregular morphology, nor distinguishing inflection and derivation. Dreyer and Eisner (2011) learn complete German verb paradigms from a small set of complete seed paradigms (50 or 100), which they choose randomly from all verbs in the language. They model stem changes using letter-based models, and use a large unannotated corpus in addition to the seed paradigms. Durrett and DeNero (2013) attack the same problem as Dreyer and Eisner (2011). Instead of usi"
D13-1105,clement-etal-2004-morphology,0,0.206172,"Missing"
D13-1105,W02-2006,0,0.173852,"systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Ma"
D13-1105,D11-1057,0,0.447036,"ings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Approaches to Morphological Modeling Much work has been done in the area of computational morphology ranging from systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this pa"
D13-1105,N13-1138,0,0.206061,". This definition is quite limited since it is not modeling the notion of lexeme. Chan (2006) defines a simpler concept of paradigms in his probabilistic paradigm model, which has many limitations, such as not handling syncretism or irregular morphology, nor distinguishing inflection and derivation. Dreyer and Eisner (2011) learn complete German verb paradigms from a small set of complete seed paradigms (50 or 100), which they choose randomly from all verbs in the language. They model stem changes using letter-based models, and use a large unannotated corpus in addition to the seed paradigms. Durrett and DeNero (2013) attack the same problem as Dreyer and Eisner (2011). Instead of using unannotated text, they model explicit rules for affixes and stem changes. The major difference between these two efforts and our work is that the problem is defined differently: we assume that the training and test data is defined by a corpus, not by complete paradigms. Our methods therefore are more sensitive to frequency effects of tokens. We believe that our way of stating the problem is more relevant to actual computational challenges for languages with limited morphological resources. We empirically compare our approac"
D13-1105,W13-2301,1,0.894623,"Missing"
D13-1105,W02-0502,0,0.0467801,"interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and modern theories of inflectional morphology organize natural language morphology by paradigms (Stump, 2001; Walther, 2011; Camilleri, 2011). Within the continuum we discussed above, we find hierarchical representations of paradigm knowledge that have been used in manually constructed morphological models (Finkel and Stump, 2002; Habash et al., 2005). Furthermore, D´etrez and Ranta (2012) introduce an implementation of Smart Paradigms – heuristically organized paradigms minimizing the number of forms needed to predict the full paradigm of a particular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological inflectional classes in a language. Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. 1033 The c"
D13-1105,P06-1086,1,0.883461,"introduce the key linguistic concepts we use (Section 3). We present our basic language-independent method in Section 4, and our language-specific modeling of stem variation in Section 5. 1032 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1032–1043, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Approaches to Morphological Modeling Much work has been done in the area of computational morphology ranging from systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other"
D13-1105,W05-0703,1,0.577033,"omplete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and modern theories of inflectional morphology organize natural language morphology by paradigms (Stump, 2001; Walther, 2011; Camilleri, 2011). Within the continuum we discussed above, we find hierarchical representations of paradigm knowledge that have been used in manually constructed morphological models (Finkel and Stump, 2002; Habash et al., 2005). Furthermore, D´etrez and Ranta (2012) introduce an implementation of Smart Paradigms – heuristically organized paradigms minimizing the number of forms needed to predict the full paradigm of a particular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological inflectional classes in a language. Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. 1033 The concept of a paradigm i"
D13-1105,W12-2301,1,0.79686,"new word forms from morphosyntactic features and lemmas, unlike the largely unsupervised work. Our work also differs from most previous work in that we investigate how to model stem change explicitly. Whereas other approaches model stem syncretism through letterbased models (Yarowsky and Wicentowski, 2000; Neuvel and Fulop, 2002; Dreyer and Eisner, 2011), we explore the use of abstract stems. In our previous work on the EGY morphological analyzer CALIMA, we similarly used a lexicon of annotated morphological forms and extended it automatically using a simpler approach to paradigm completion (Habash et al., 2012). 3 Linguistic Terminology In this section, we review key concepts from morphology, and introduce the terminology we will use in this paper.2 We then introduce our own formalization of stems using vocalic templates. Morphology is the study of word forms and their decomposition into elementary morphemes, which are the smallest meaning-bearing units of a language. There are two types of morphological processes: inflectional and derivational morphology. In inflectional morphology, a core meaning is retained and different word forms reflect different types of morphosyntactic features such as perso"
D13-1105,J11-2002,0,0.09257,"Missing"
D13-1105,W02-0604,0,0.094647,"d by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and moder"
D13-1105,W02-0602,0,0.0332541,"to predict the full paradigm of a particular lexeme. Both Forsberg et al. (2006) and Cl´ement et al. (2004) describe methods for automatically populating a lexicon from raw data given a set of morphological inflectional classes in a language. Our work differs in that we use annotated data, but do not start with a complete set of inflectional classes; thus, our work is exactly complementary to this work. 1033 The concept of a paradigm is also used in many published efforts on unsupervised learning of morphology, although not always in a way consistent with its use in linguistics. For instance, Snover et al. (2002) (and later on Can and Manandhar (2012)) define a paradigm as “a set of suffixes and the stems that attach to those suffixes and no others”. This definition is quite limited since it is not modeling the notion of lexeme. Chan (2006) defines a simpler concept of paradigms in his probabilistic paradigm model, which has many limitations, such as not handling syncretism or irregular morphology, nor distinguishing inflection and derivation. Dreyer and Eisner (2011) learn complete German verb paradigms from a small set of complete seed paradigms (50 or 100), which they choose randomly from all verbs"
D13-1105,P08-1084,0,0.0864841,"1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledge. Morphological Paradigms Many traditional and modern theories of inflectional m"
D13-1105,P00-1027,0,0.825964,"ational morphology ranging from systems painstakingly designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012) to unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Hammarstr¨om and Borin, 2011; Dreyer and Eisner, 2011). There is a large continuum between these two approaches. Closer to one end, we find work on minimally supervised methods for morphology learning that make use of available resources such as parallel data, dictionaries or some additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). The work presented in this paper falls in the middle of this continuum: we are interested in learning complete morphological models using rich morphological annotations and, optionally, limited linguistic knowledge. We compare the value of different amounts of annotation and how they relate to additional linguistic knowledg"
D13-1105,E12-1066,0,\N,Missing
D14-1157,W12-2105,1,0.898729,"Defense or the U.S. Government. We also thank the anonymous reviewers for their constructive feedback. Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SITS topic segmentation system that captures fluctuations in each candidate’s topic shifting tendencies, which we found to be correlated with their power. We also showed that features based on topic shift improve the prediction of the relative rankings of candidates. In futur"
D14-1157,P11-1078,0,0.129191,"pon work supported by the DARPA DEFT Program. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We also thank the anonymous reviewers for their constructive feedback. Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SITS topic segmentation system that captures fluctuations in each candidate’s topic shifting tendencies, which we f"
D14-1157,P12-1009,0,0.0537663,"topics. However, segmenting interactions into coherent topic segments is an active area of research and a variety of topic modeling approaches have been proposed for that purpose. In this paper, we explore the utility of one such topic modeling approach to tackle this problem. While most of the early approaches for topic segmenting in interactions have focused on the 1481 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1481–1486, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics content of the contribution, Nguyen et al. (2012) introduced a system called Speaker Identity for Topic Segmentation (SITS) which also takes into account the topic shifting tendencies of the participants of the conversation. In later work, Nguyen et al. (2013) demonstrated the SITS system’s utility in detecting influencers in Crossfire debates and Wikipedia discussions. They also applied the SITS system to the domain of political debates. However they were able to perform only a qualitative analysis of its utility in the debates domain since the debates data did not have influence annotations. In this paper, we use the SITS system to assign"
D14-1157,I13-1025,1,0.793569,"Program. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We also thank the anonymous reviewers for their constructive feedback. Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SITS topic segmentation system that captures fluctuations in each candidate’s topic shifting tendencies, which we found to be correlated with their power. We al"
D14-1157,P14-2056,1,0.845425,"re those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government. We also thank the anonymous reviewers for their constructive feedback. Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SITS topic segmentation system that captures fluctuations in each candidate’s topic shifting tendencies, which we found to be correlated with their power. We also showed that features based o"
D14-1157,I13-1042,1,0.878997,"such topic modeling approach to tackle this problem. While most of the early approaches for topic segmenting in interactions have focused on the 1481 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1481–1486, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics content of the contribution, Nguyen et al. (2012) introduced a system called Speaker Identity for Topic Segmentation (SITS) which also takes into account the topic shifting tendencies of the participants of the conversation. In later work, Nguyen et al. (2013) demonstrated the SITS system’s utility in detecting influencers in Crossfire debates and Wikipedia discussions. They also applied the SITS system to the domain of political debates. However they were able to perform only a qualitative analysis of its utility in the debates domain since the debates data did not have influence annotations. In this paper, we use the SITS system to assign topics to turns and perform a quantitative analysis of how the topic shift features calculated using the SITS system relate to the notion of power as captured by (Prabhakaran et al., 2013a). The SITS system asso"
D14-1157,W14-2710,1,0.140738,"and interruption patterns). Another finding was that the candidates’ power correlates with the distribution of topics they speak about in the debates: candidates with more power spoke significantly more about certain topics (e.g., economy) and less about certain other topics (e.g., energy). However, these findings relate to the specific election cycle that was analyzed and will not carry over to political debates in general. A further dimension with relevance beyond a specific election campaign is how topics evolve during the course of an interaction (e.g., who attempts to shift topics). In (Prabhakaran et al., 2014), we explored this dimension and found that candidates with higher power introduce significantly more topics in the debates, but attempt to shift topics significantly less often while responding to a moderator. We used the basic LDA topic modeling method (with a filter for substantivity of turns) to assign topics to turns, which were then used to detect shifts in topics. However, segmenting interactions into coherent topic segments is an active area of research and a variety of topic modeling approaches have been proposed for that purpose. In this paper, we explore the utility of one such topi"
D14-1157,D13-1010,0,0.0730597,"andidates’ relative rankings. 1 Introduction The field of computational social sciences has created many interesting applications for natural language processing in recent years. One of the areas where NLP techniques have shown great promise is in the analysis of political speech. For example, researchers have applied NLP techniques to political texts for a variety of tasks such as predicting voting patterns (Thomas et al., 2006), identifying markers of persuasion (Guerini et al., 2008), capturing cues that signal charisma (Rosenberg and Hirschberg, 2009), and detecting ideological positions (Sim et al., 2013). Our work also analyzes political speech, more specifically, presidential debates. The contribution of this paper is to show that the topic shifting tendency of a presidential candidate changes over the course of the election campaign, and that it is correlated with his or her relative power. We also show that this insight can help computational systems that predict the candidates’ relative rankings based on their interactions in the debates. 2 Motivation The motivation for this paper stems from prior work done by the first author in collaboration with other researchers (Prabhakaran et al., 2"
D14-1157,C12-1155,0,0.120604,"k the anonymous reviewers for their constructive feedback. Related Work Studies in sociolinguistics (e.g., (Ng et al., 1993; Ng et al., 1995; Reid and Ng, 2000)) have long established that dialog structure in interactions relates to power and influence. Researchers in the NLP community have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013; Prabhakaran and Rambow, 2014), online discussion forums (Danescu-Niculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word and phrase patterns, 8 Conclusion In this paper, we studied how topic shift patterns in the 2012 Republican presidential debates correlate with the power of candidates. We proposed an alternate formulation of the SITS topic segmentation system that captures fluctuations in each candidate’s topic shifting tendencies, which we found to be correlated with their power. We also showed that features based on topic shift improve the prediction of the relative rankings of candidates. In future work, we will explore a model that captures indivi"
D14-1157,W06-1639,0,0.251774,"candidates to shift topics changes over the course of the election campaign, and that it is correlated with their relative power. We also show that our topic shift features help predict candidates’ relative rankings. 1 Introduction The field of computational social sciences has created many interesting applications for natural language processing in recent years. One of the areas where NLP techniques have shown great promise is in the analysis of political speech. For example, researchers have applied NLP techniques to political texts for a variety of tasks such as predicting voting patterns (Thomas et al., 2006), identifying markers of persuasion (Guerini et al., 2008), capturing cues that signal charisma (Rosenberg and Hirschberg, 2009), and detecting ideological positions (Sim et al., 2013). Our work also analyzes political speech, more specifically, presidential debates. The contribution of this paper is to show that the topic shifting tendency of a presidential candidate changes over the course of the election campaign, and that it is correlated with his or her relative power. We also show that this insight can help computational systems that predict the candidates’ relative rankings based on the"
D14-1211,W12-2105,1,0.838932,"ng. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily"
D14-1211,P11-1078,0,0.172443,"ender in classification experiments, and finds that the linguistic gender norms can be influenced by the style of their interlocutors. Within the NLP community, there has been substantial research exploring language use and power. A large number of these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line inter"
D14-1211,W11-1709,0,0.0897844,"and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had an unfamiliar name or a name that could be of either gender, they marked his/her gender as unknown and excluded them from their study.1 For example, the gender of the employee Kay Mann was marked as unknown in their gender assignment. However, in our"
D14-1211,N13-1099,1,0.907928,"Missing"
D14-1211,W11-0711,0,0.48396,"Missing"
D14-1211,I13-1025,1,0.871684,"exploring language use and power. A large number of these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender."
D14-1211,P14-2056,1,0.526846,"these studies are performed in the domain of organizational email where the notion of power is well defined in terms of organizational hierarchy. It is also aided by the availability of the moderately large Enron email corpus which captures email interactions in an organizational setting. Earlier approaches used simple lexical features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender aff"
D14-1211,N12-1057,1,0.918011,"Missing"
D14-1211,I13-1042,1,0.895058,"Missing"
D14-1211,W14-2710,1,0.700979,"inguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had an unfamiliar name or a name that could be of either gender, they marked his/her gender as unknown and excluded them from their st"
D14-1211,C12-1155,0,0.0614795,"al features alone (e.g. (Bramsen et al., 2011; Gilbert, 2012)) as a means to predict power. Later studies have used more complex linguistic and structural features, such as formality (Peterson et al., 2011), dialog acts (Prabhakaran and Rambow, 2013), and thread structure (Prabhakaran and Rambow, 2014). Our work is also on the Enron email corpus, and our baseline features are derived from some of this prior work. Researchers have also studied power and influence in other genres of interactions, such as online forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012), multi-party chats (Strzalkowski et al., 2012) and off-line interactions such as presidential debates (Nguyen et al., 2013; Prabhakaran et al., 2013; Prabhakaran et al., 2014). There is also some work within the NLP field on analyzing language use in relation to gender. Mohammad and Yang (2011) analyzed the way gender affects the expression of sentiments in text, while we are interested in how gender relates to manifestations of organizational power. For their study, they assigned gender for the core employees in the Enron email corpus based on whether the first name of the person was easily gender identifiable or not. If the person had a"
D14-1211,P12-2032,1,\N,Missing
D15-1304,abdul-mageed-diab-2014-sana,0,0.347756,"Association for Computational Linguistics. SenL outperforms the state-of-the-art Arabic sentiment lexicons. However, we show that SLSA has better coverage and quality. Moreover, ArSenL uses SAMA which is not publicly available for free, as opposed to SLSA which is based on free resources. Another similar work to ArSenL is the resource developed by Alhazmi et al. (2013). They linked the Arabic WordNet to SentiWordNet via the provided synset offset information. However, the constructed lexicon has a limited coverage of nearly 10K lemmas, which makes it not very useful for further applications. Abdul-Mageed and Diab (2014) presented SANA, a subjectivity and sentiment lexicon for Arabic. The lexicon combines pre-existing lexicons and involves automatic machine translation, manual annotations and gloss matching across several resources such as THARWA (Diab et al., 2014) and SAMA. SANA includes about 225K entries, where many of them are duplicates, inflected or not diacritized, which makes the resource noisy and less useable. Additionally, the automatic translation does not utilize the POS information, which affects the quality of the resource. Other work that follows the translation approach includes the one pres"
D15-1304,P11-2103,0,0.236358,"Missing"
D15-1304,baccianella-etal-2010-sentiwordnet,0,0.676596,"ed Work Work on building Arabic sentiment lexicons mainly falls into two categories: 1) linking an Arabic sentiment lexicon with an English one, and 2) applying semi-supervised or supervised learning techniques on Arabic resources. We summarize these two types in turn. We start with a survey of work based on translation, which our work falls into as well. The most similar work to the one presented in this paper is ArSenL (Badaro et al., 2014). ArSenL is considered the first publicly available largescale Standard Arabic sentiment lexicon. It was constructed using a combination of SentiWordNet (Baccianella et al., 2010), Arabic WordNet (Black et al., 2006) and SAMA (Graff et al., 2009). Ar1 The lexicon is available at http://volta.ldeo. columbia.edu/˜rambow/slsa.html 2545 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2545–2550, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. SenL outperforms the state-of-the-art Arabic sentiment lexicons. However, we show that SLSA has better coverage and quality. Moreover, ArSenL uses SAMA which is not publicly available for free, as opposed to SLSA which is based on free resources. An"
D15-1304,W14-3623,0,0.263996,"lected forms. This makes the lexicon more useful when used by other research. Public Availability SLSA is based on free resources and is publicly available for free.1 2 Related Work Work on building Arabic sentiment lexicons mainly falls into two categories: 1) linking an Arabic sentiment lexicon with an English one, and 2) applying semi-supervised or supervised learning techniques on Arabic resources. We summarize these two types in turn. We start with a survey of work based on translation, which our work falls into as well. The most similar work to the one presented in this paper is ArSenL (Badaro et al., 2014). ArSenL is considered the first publicly available largescale Standard Arabic sentiment lexicon. It was constructed using a combination of SentiWordNet (Baccianella et al., 2010), Arabic WordNet (Black et al., 2006) and SAMA (Graff et al., 2009). Ar1 The lexicon is available at http://volta.ldeo. columbia.edu/˜rambow/slsa.html 2545 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2545–2550, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. SenL outperforms the state-of-the-art Arabic sentiment lexicons. Howev"
D15-1304,P14-5010,0,0.00159143,"ordNet. The 2546 cleaned-up AraMorph is closer to SAMA (used in ArSenL), which is itself a modified version of AraMorph. Practically, SAMA can replace AraMorph. However, the integration of AraMorph allows the lexicon to be publicly available for free, which SAMA prohibits. Gloss Normalization Since the entries in AraMorph are bound to stems, the English glosses are inflected for number. As a result, we lemmatize the English glosses in AraMorph in order to be able to match to the synset terms in SentiWordNet. The lemmatization is done using Stanford CoreNLP Natural Language Processing Toolkit (Manning et al., 2014). Additionally, we remove from the glosses any descriptive text between parentheses, as well as the stop words be, a, an and the (unless be is the actual lemma of the AraMorph gloss). Moreover, if any of the lemmatized words in an AraMorph gloss does not match any of the synset terms in SentiWordNet and has a regular morphological derivation, the effect of the derivation is removed if the removal results in an existing synset term, e.g., voluntariness is converted to voluntary and orientalization becomes orientalize. We created the list of the derivational patterns manually by examining AraMor"
D15-1304,H05-1044,0,0.327721,"tized, which makes the resource noisy and less useable. Additionally, the automatic translation does not utilize the POS information, which affects the quality of the resource. Other work that follows the translation approach includes the one presented by El-Halees (2011) where SentiStrength (Thelwall et al., 2010) was translated using a dictionary along with manual correction. Another instance is SIFAAT (Abdul-Mageed and Diab, 2012), an earlier version of SANA but with more reliance on translation. Another lexicon was built by Elarnaoty et al. (2012) who manually translated the MPQA lexicon (Wilson et al., 2005). The common aspect among those resources is the lack of adequate coverage and quality. Mobarz et al. (2011) created a sentiment Arabic lexical Semantic Database (SentiRDI) by using a dictionary-based approach. The database has many inflected forms, i.e., it is not lemma-based. Moreover, the authors reported insufficient quality and plan to try other alternatives. We now turn to work based entirely on Arabic resources. Mahyoub et al. (2014) created an Arabic sentiment lexicon that assigns sentiment scores to the words in Arabic WordNet using a lexiconbased approach. The lexicon was initially b"
D15-1304,diab-etal-2014-tharwa,1,\N,Missing
D17-1180,P16-1231,0,0.145344,"of 0.826. Table 2 compares this result to past work. The supertagger outperforms all other models besides the Word Vector model. Since this Word Vector model (like the MaxEnt model) is specifically trained for this task, and given that our supertagger is not trained for this particular task, the accuracy is reasonably encouraging. This result suggests that TAG supertagging is a reasonable intermediate level between only resolving PP attachment and conducting full parsing. System Malt (Nivre et al., 2006) MaxEnt (Ratnaparkhi et al., 1994) Word Vector (Belinkov et al., 2014) Parsey McParseface (Andor et al., 2016) BLSTM Supertagger PP Attachment Accuracy 79.7* 81.6* 88.7* 82.3 82.6 Table 2: Various PP attachment results. * denotes the results on a different dataset. 5.3 Parsing Results Parsing results and comparison with prior models are summarized in Tables 3, 4 (Section 00), and 5 (Section 23). From Table 4, we see that the combination of the BLSTM supertagger, MICA chart parser, and the neural network parser achieves state-of-the-art performance, even compared to parsers that make use of lexical information, POS tags, and hand-engineered features. With gold supertags, the neural network parser with"
D17-1180,N09-2047,1,0.730968,"se given a sequence of lexical units motivated Bangalore and Joshi (1999) to decompose the parsing problem into two phases: supertagging, where elementary objects, or supertags, are assigned to each word, and stapling, where these supertags are combined together. They claim that given a perfect supertagger, a parse of a sentence follows from syntactic features provided by the supertags, and therefore, supertagging is “almost parsing.” This claim has been confirmed in subsequent work: it has been shown that the task of parsing given a gold sequence of supertags can achieve high accuracy (TAG: (Bangalore et al., 2009; Chung et al., 2016), CCG: (Lewis et al., 2016)). However, it has also been revealed that the difficulty of supertagging, because of the large set of possible supertags, re1712 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1712–1722 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics sults in inaccuracies that prevent us from effectively utilizing syntactic information provided by the imperfect set of supertags that are assigned. This problem is even more severe for TAG parsing. TAG differs from CCG in hav"
D17-1180,J99-2004,0,0.901854,"n of these units in a derivation could be even more informative for subsequent tasks involving semantic interpretation, translation and the like, which have been the focus of CCG-based work. The elementary units of CCG and TAG (categories for CCG, and elementary trees for TAG) determine a word’s combinatory potential, in a way that is not the case for the usual part-ofspeech tags used in parsing. Indeed, the assignment of elementary objects to the words in a sentence almost determines the possible parse for a sentence. The near uniqueness of a parse given a sequence of lexical units motivated Bangalore and Joshi (1999) to decompose the parsing problem into two phases: supertagging, where elementary objects, or supertags, are assigned to each word, and stapling, where these supertags are combined together. They claim that given a perfect supertagger, a parse of a sentence follows from syntactic features provided by the supertags, and therefore, supertagging is “almost parsing.” This claim has been confirmed in subsequent work: it has been shown that the task of parsing given a gold sequence of supertags can achieve high accuracy (TAG: (Bangalore et al., 2009; Chung et al., 2016), CCG: (Lewis et al., 2016))."
D17-1180,C00-1007,1,0.469077,"G grammar from the WSJ part of the Penn Treebank corpus, resulting in a grammar and derivation trees labeled with the grammar Chen (2001). For example, in Figure 1, t27 is the basic tree for a transitive verb (regulate), while t722 is the tree for a transitive verb which forms an object relative clause with an overt relative pronoun but an empty subject (Figure 2).2 The corpus and grammar were iteratively refined to obtain linguistically plausible derivation trees which could serve 2 Our full grammar is shown at http://mica.lif. univ-mrs.fr/d6.clean2-backup.pdf as input for a generation task (Bangalore and Rambow, 2000). As a result, the dependency structure is similar to Universal Dependency (Nivre et al., 2016), apart from the different treatment of long-distance wh-movement noted above: the primary dependencies are between the core meaningbearing lexical words, while function words (auxiliaries, determiners, complementizers) depend on their lexical head and have no dependents.3 We label verbal argument arcs with deep dependency labels: Subject, Object, and Indirect Object normalized for passive and dative shift. All other arcs are labeled as Adjuncts. This means that our label set is small, but determinin"
D17-1180,Q14-1043,0,0.0502821,"Missing"
D17-1180,D14-1082,0,0.0783022,"1 Figure 4: Shift-Reduce Parser Neural Network Architecture. one supertag into another, allowing the generalization across these contexts. Indeed, as we will show in a later section, the substitution memory embeddings and supertag embeddings turn out to yield interpretable and linguistically sensible structures. Finally, we concatenate the vectors associated with the relevant elements from the stack and buffer into a 2dk dimensional vector and feed it to the network to obtain a probability distribution over the possible transition actions. The architecture is visualized in Figure 4. Following Chen and Manning (2014), we use the cube activation function for the first layer, which could better capture interactions. We, again, optimize the negative loglikelihood in a mini-batch stochastic fashion with the Adam optimization algorithm with l = 0.001 (Kingma and Ba, 2015). With regards to decoding, we consider both greedy parsing as well as a beam search algorithm, where we keep transition action hypotheses at each time step, in the experiments we report below. 4.3 Supertag Input to the Parser We consider three types of supertag inputs to the neural network parser: gold supertags, 1-best supertags from the BLS"
D17-1180,W03-1006,1,0.730571,"d the neural network parser achieves state-of-the-art performance, even compared to parsers that make use of lexical information, POS tags, and hand-engineered features. With gold supertags, the neural network parser with beam size 16 performs slightly better than the chart parser. As shown in Table 5, our supertag-based parser outperforms SyntaxNet (Andor et al., 2016) with the computationally expensive global normalization. This suggests that, besides providing the grammars and linguistic features that can be used in downstream tasks in addition to derivation trees (Semantic Role Labeling: (Chen and Rambow, 2003), Textual Entailments: (Xu et al., 2017)), supertagging also improves parsing performance. 5.4 Learned Vector Representation We motivated the use of embeddings in the parser to encode properties of the supertags and the substitution operations performed on them. We can examine their structure in a way similar to what Mikolov et al. (2013) did for word embeddings by performing analogy tests on the learned supertag embeddings. Consider, for example, the analogy that an elementary tree representing a clause headed by a transitive verb (t27) is to a clause headed by an intransitive verb (t81) as a"
D17-1180,W16-3309,1,0.503636,"exical units motivated Bangalore and Joshi (1999) to decompose the parsing problem into two phases: supertagging, where elementary objects, or supertags, are assigned to each word, and stapling, where these supertags are combined together. They claim that given a perfect supertagger, a parse of a sentence follows from syntactic features provided by the supertags, and therefore, supertagging is “almost parsing.” This claim has been confirmed in subsequent work: it has been shown that the task of parsing given a gold sequence of supertags can achieve high accuracy (TAG: (Bangalore et al., 2009; Chung et al., 2016), CCG: (Lewis et al., 2016)). However, it has also been revealed that the difficulty of supertagging, because of the large set of possible supertags, re1712 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1712–1722 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics sults in inaccuracies that prevent us from effectively utilizing syntactic information provided by the imperfect set of supertags that are assigned. This problem is even more severe for TAG parsing. TAG differs from CCG in having a smaller set of"
D17-1180,P82-1020,0,0.860511,"Missing"
D17-1180,J07-3004,0,0.050816,"anguage Processing, pages 1712–1722 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics sults in inaccuracies that prevent us from effectively utilizing syntactic information provided by the imperfect set of supertags that are assigned. This problem is even more severe for TAG parsing. TAG differs from CCG in having a smaller set of combinatory operations, but a more varied set of elementary objects: the TAG-annotated version of the Penn Treebank that we use (Chen, 2001) includes 4727 distinct supertags (2165 occur once) while the CCG-annotated version (Hockenmaier and Steedman, 2007) includes 1286 distinct supertags (439 occur once). As a result, building a robust, broad-coverage TAG parser has proven difficult. In this work, we show that robust supertaggingbased parsing of TAG is indeed possible by using a dense representation of supertags that is induced using neural networks. In the first half of the paper, we present a neural network supertagger based on a bi-directional LSTM (BLSTM) architecture, inspired by the work of Xu (2015) and Lewis et al. (2016) in CCG, and we make crucial use of synchronized dropout (Gal and Ghahramani, 2016). This supertagger achieves the s"
D17-1180,D15-1169,0,0.109483,"vercomes the drawbacks for statistical models of TAG as compared to CCG parsing, raising the possibility that TAG is a viable alternative for NLP tasks that require the assignment of richer structural descriptions to sentences. 1 R. Thomas McCoy Dept. of Linguistics Yale University richard.mccoy@yale.edu Introduction Recent work has applied Combinatory Categorial Grammar (CCG, Steedman and Baldridge (2011)) to the problem of broad-coverage parsing in order to derive grammatical representations that are sufficiently rich to support tasks requiring deeper representation of a sentence’s meaning (Lewis et al., 2015; Reddy et al., 2016; Nadejde et al., 2017). Yet CCG is only one of a number of mildly context-sensitive grammar formalisms that can provide such rich representations, and each has distinct advantages. In this paper we explore the applicability of another formalism, Tree Adjoining Grammar (TAG, Joshi and Schabes (1997)), to the task of broad-coverage parsing. TAG and CCG share the property of lexicalization: words are associated with elementary units of grammatical structure which are composed during a derivation using one of a small set of operations to produce a parse tree. The task of parsi"
D17-1180,nivre-etal-2006-maltparser,0,0.106193,"our test set of 1951 sentences, 1616 had supertags modifying the correct part of speech, to give an accuracy of 0.826. Table 2 compares this result to past work. The supertagger outperforms all other models besides the Word Vector model. Since this Word Vector model (like the MaxEnt model) is specifically trained for this task, and given that our supertagger is not trained for this particular task, the accuracy is reasonably encouraging. This result suggests that TAG supertagging is a reasonable intermediate level between only resolving PP attachment and conducting full parsing. System Malt (Nivre et al., 2006) MaxEnt (Ratnaparkhi et al., 1994) Word Vector (Belinkov et al., 2014) Parsey McParseface (Andor et al., 2016) BLSTM Supertagger PP Attachment Accuracy 79.7* 81.6* 88.7* 82.3 82.6 Table 2: Various PP attachment results. * denotes the results on a different dataset. 5.3 Parsing Results Parsing results and comparison with prior models are summarized in Tables 3, 4 (Section 00), and 5 (Section 23). From Table 4, we see that the combination of the BLSTM supertagger, MICA chart parser, and the neural network parser achieves state-of-the-art performance, even compared to parsers that make use of lex"
D17-1180,D14-1162,0,0.0808709,"Missing"
D17-1180,W17-6214,1,0.705078,"the-art performance, even compared to parsers that make use of lexical information, POS tags, and hand-engineered features. With gold supertags, the neural network parser with beam size 16 performs slightly better than the chart parser. As shown in Table 5, our supertag-based parser outperforms SyntaxNet (Andor et al., 2016) with the computationally expensive global normalization. This suggests that, besides providing the grammars and linguistic features that can be used in downstream tasks in addition to derivation trees (Semantic Role Labeling: (Chen and Rambow, 2003), Textual Entailments: (Xu et al., 2017)), supertagging also improves parsing performance. 5.4 Learned Vector Representation We motivated the use of embeddings in the parser to encode properties of the supertags and the substitution operations performed on them. We can examine their structure in a way similar to what Mikolov et al. (2013) did for word embeddings by performing analogy tests on the learned supertag embeddings. Consider, for example, the analogy that an elementary tree representing a clause headed by a transitive verb (t27) is to a clause headed by an intransitive verb (t81) as a subject relative clause headed by a tra"
D17-1180,P11-1069,0,0.02522,"l. (2016) in CCG, and we make crucial use of synchronized dropout (Gal and Ghahramani, 2016). This supertagger achieves the stateof-the-art accuracy on the WSJ Penn Treebank. When combined with an existing TAG chart parser (Bangalore et al., 2009), the LSTM-based supertagger already yields state-of-the-art unlabeled and labeled attachment scores. In the second half of the work, we present a shift-reduce parsing model based on a feedforward neural network that makes use of dense supertag embeddings. Although this approach has much in common with the approach to shiftreduce CCG parsing taken by Zhang and Clark (2011), it differs in its additive structures in supertag embeddings. When a CCG operation combines two supertags (categories), it yields a resulting category that is typically distinct from the two that are combined, and CCG shift-reduce parsers (e.g. Xu (2015)) make use of this result to guide subsequent actions. When the resulting category is the same as some lexical category assignment (for example when function application over (SN P )/N P N P yields SN P , the same as an intransitive verb), the parser will benefit from sharing statistics across these contexts. For TAG however, substitution o"
D17-1180,N10-1049,1,0.806749,"sponds to the obligatory addition of an argument, and adjunction is used to add adjuncts, as well as function words to a lexical head. The one exception is the treatment of long distance whmovement in TAG. Here, a matrix clause is represented by a predicative auxiliary tree which is adjoined into the embedded clause, so that the whelement moved from the embedded clause can still be substituted locally into the tree headed by its verb. As a result, the dependency between the matrix and embedded verbs is inverted relative to the 1 For the difference between formal and linguistic dependency, see Rambow (2010). 1713 regulate t27 SUBJ ADJ bill t3 the t1 OBJ which t100 ADJ ADJ pass t722 PREDAUX OBJ SUBJ would t45 emissions t3 ADJ the ADJ failed t119 to t31 bill SUBJ they SUBJ they t29 regulate ADJ ADJ failed OBJ OBJ would emissions OBJ pass ADJ which to Figure 1: TAG derivation tree (left) and closely related dependency tree (right) for The bill, which they failed to pass, would regulate emissions. Substitution edges are labeled SUBJ or OBJ, predicative auxiliary edges are labeled PREDAUX, while all other adjoining edges are labeled ADJ. We use the same edge labels in the dependency tree. The derivat"
D17-1180,H94-1048,0,0.551047,"e in the context of prepositional phrase (PP) attachment ambiguity. Normally, in dependency parsing, PP attachment is resolved by the parser. However, in our case, it can be resolved before parsing, during the supertagging step. This is because the supertags for prepositions vary depending on the type of constituent modified by the PP containing the preposition; for example, t4 is the supertag for a preposition whose PP modifies an NP, while t13 is the supertag for a preposition whose PP modifies a VP. To test how well our supertagger resolves PP attachment ambiguity, we used the dataset from Ratnaparkhi et al. (1994) (derived from the PTB WSJ) to extract a test set of sentences with PPs that are ambiguous between attaching to a VP or to an NP.7 7 We were unable to use the full test set because, in order to run the supertagger on the test set, we had to map the test examples back to their full sentences, but some of those original sentences are no longer available in PTB3. 1718 We then supertagged these sentences and checked whether the supertag for the preposition in the ambiguous PP is a VP modifier or an NP modifier. Of our test set of 1951 sentences, 1616 had supertags modifying the correct part of spe"
D17-1180,Q16-1010,0,0.0921511,"Missing"
D17-1180,W04-2407,0,\N,Missing
D17-1180,D16-1181,0,\N,Missing
D17-1180,L16-1262,0,\N,Missing
E06-1047,N04-4038,1,0.632821,"biguous in MSA. For example, the LA words  mn ‘from’ and  myn ‘who’ both are translated into an orthographically ambiguous form in MSA  mn ‘from’ or ‘who’. 5.1 Implementation Each word in the LA sentence is translated into a bag of MSA words, producing a sausage lattice. The lattice is scored and decoded using the SRILM toolkit with a trigram language model trained on 54 million MSA words from Arabic Gigaword (Graff, 2003). The text used for language modeling was tokenized to match the tokenization of the Arabic used in the ATB and LATB. The tokenization was done using the ASVM Toolkit (Diab et al., 2004). The 1-best path in the lattice is passed on to the Bikel parser (Bikel, 2002), which was trained on the MSA training ATB. Finally, the terminal nodes in the resulting parse structure are replaced with the original LA words. 5.2 Experimental Results Table 1 describes the results of the sentence transduction path on the development corpus (DEV) in different settings: using no POS tags in the input versus using gold POS tags in the input, and using SLXUN versus BLXUN. The baseline results are obtained by parsing the LA sentence directly using the MSA parser (with and without gold POS tags). The"
E06-1047,A00-1002,0,0.0265605,"ency trees for the MSA and LA sentences (not shown here for space considerations) are isomorphic. They differ only in the node labels. 5 Sentence Transduction In this approach, we parse an MSA translation of the LA sentence and then link the LA sentence to the MSA parse. Machine translation (MT) is not easy, especially when there are no MT resources available such as naturally occurring parallel text or transfer lexicons. However, for this task we have three encouraging insights. First, for really close languages it is possible to obtain better translation quality by means of simpler methods (Hajic et al., 2000). Second, suboptimal MSA output can still be helpful for the parsing task without necessarily being fluent or accurate (since our goal is parsing LA, not translating it to MSA). And finally, translation from LA to MSA is easier than from MSA to LA. This is a result of the availability of abundant resources for MSA as compared to LA: for example, text corpora and tree banks for 4 Levantine also has other negation markers that precede the verb, as well as the circumfi x m- -$. language modeling and a morphological generation system (Habash, 2004). One disadvantage of this approach is the lack of"
E06-1047,maamouri-etal-2006-developing,1,0.666828,"Missing"
E06-1047,J01-1004,1,0.72461,"can be thought of as a variant of the treebank-transduction approach in which the syntactic transformations are localized to elementary trees. Moreover, because a parsed MSA translation is produced as a byproduct, we can also think of this approach as being related to the sentence-transduction approach. 7.1 Preliminaries The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. In its present form, the formalism is tree-substitution grammar (Schabes, 1990) with an additional operation called sister-adjunction (Rambow et al., 2001). Because of space constraints, we omit discussion of the sister-adjunction operation in this paper. A tree-substitution grammar is a set of elementary trees. A frontier node labeled with a nonterminal label is called a substitution site. If an elementary tree has exactly one terminal symbol, that symbol is called its lexical anchor. A derivation starts with an elementary tree and proceeds by a series of composition operations. In the substitution operation, a substitution site is rewritten with an elementary tree with a matching root label. The final product is a tree with no more substitutio"
E06-1047,P00-1008,0,0.0658914,"Missing"
E06-1047,W04-3207,0,0.0165264,"LA and MSA (Section 4). We then proceed to discuss three approaches: sentence transduction, in which the LA sentence to be parsed is turned into an MSA sentence and then parsed with an MSA parser (Section 5); treebank transduction, in which the MSA treebank is turned into an LA treebank (Section 6); and grammar transduction, in which an MSA grammar is turned into an LA grammar which is then used for parsing LA (Section 7). We summarize and discuss the results in Section 8. 2 Related Work There has been a fair amount of interest in parsing one language using another language, see for example (Smith and Smith, 2004; Hwa et al., 2004) for recent work. Much of this work uses synchronized formalisms as do we in the grammar transduction approach. However, these approaches rely on parallel corpora. For MSA and its dialects, there are no naturally occurring parallel corpora. It is this fact that has led us to investigate the use of explicit linguistic knowledge to complement machine learning. We refer to additional relevant work in the appropriate sections. aries. The resulting development data comprises 1928 sentences and 11151 tokens (DEV). The test data comprises 2051 sentences and 10,644 tokens (TEST). Fo"
E06-1047,W00-1307,0,0.0120082,"stic synchronous TSG, using a straightforward generalization of the CKY and Viterbi algorithms, we obtain the highestprobability paired derivation which includes a parse for S on one side, and a parsed translation of S on the other side. It is also straightforward to calculate inside and outside probabilities for reestimation by Expectation-Maximization (EM). 7.2 An MSA-dialect synchronous grammar We now describe how we build our MSA-dialect synchronous grammar. As mentioned above, the MSA side of the grammar is extracted from the ATB in a process described by Chiang and others (Chiang, 2000; Xia et al., 2000; Chen, 2001). This process also gives us MSA-only substitution probabilities P (α |η). We then apply various transformation rules (described below) to the MSA elementary trees to produce a dialect grammar, at the same time assigning probabilities P (α0 |α). The synchronoussubstitution probabilities can then be estimated as: P (α, α0 |η, η 0 ) ≈ P (α |η)P (α0 |α) ≈ P (α |η)P (w 0 , t0 |w, t) P (¯ α0 |α ¯ , w0 , t0 , w, t) where w and t are the lexical anchor of α and its POS tag, and α ¯ is the equivalence class of α modulo lexical anchors and their POS tags. P (w0 , t0 |w, t) is assigned as d"
E06-1047,P00-1058,1,0.647244,"andwritten rules into dialect elementary trees to yield an MSAdialect synchronous grammar. This synchronous grammar can be used to parse new dialect sentences using statistics gathered from the MSA data. Thus this approach can be thought of as a variant of the treebank-transduction approach in which the syntactic transformations are localized to elementary trees. Moreover, because a parsed MSA translation is produced as a byproduct, we can also think of this approach as being related to the sentence-transduction approach. 7.1 Preliminaries The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. In its present form, the formalism is tree-substitution grammar (Schabes, 1990) with an additional operation called sister-adjunction (Rambow et al., 2001). Because of space constraints, we omit discussion of the sister-adjunction operation in this paper. A tree-substitution grammar is a set of elementary trees. A frontier node labeled with a nonterminal label is called a substitution site. If an elementary tree has exactly one terminal symbol, that symbol is called its lexical anchor. A derivation starts with an elemen"
E06-1047,W97-0119,0,\N,Missing
E06-1047,H05-1107,0,\N,Missing
E06-1047,J03-3002,0,\N,Missing
E06-1047,W02-2026,0,\N,Missing
E06-1047,N01-1020,0,\N,Missing
E06-1047,J90-2002,0,\N,Missing
E06-1047,P99-1067,0,\N,Missing
E06-1047,P95-1032,0,\N,Missing
E06-1047,W01-0713,0,\N,Missing
E06-1047,J00-2004,0,\N,Missing
E06-1047,N04-1034,0,\N,Missing
E14-1023,P05-1053,0,0.0561947,"Missing"
E14-1023,D10-1100,1,0.836029,"add less value to the overall performance in comparison with the frame-semantic tree kernels. We believe this is due to the fact that hand-crafted features require frame parses to be highly accurate and complete. In contrast, tree kernels are able to find and leverage less strict patterns without requiring the semantic parse to be entirely accurate or complete. Apart from introducing semantic features and tree structures, we evaluate on the task of social network extraction, which is a combination of two sub-tasks: social event detection and social event classification. In our previous work (Agarwal and Rambow, 2010), we presented results for the two Introduction Social network extraction from text has recently been gaining a considerable amount of attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). One of the reason for this attention, we believe, is that being able to extract social networks from unstructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to q"
E14-1023,W10-1803,1,0.555931,"Missing"
E14-1023,P13-1129,0,0.0276561,"verage less strict patterns without requiring the semantic parse to be entirely accurate or complete. Apart from introducing semantic features and tree structures, we evaluate on the task of social network extraction, which is a combination of two sub-tasks: social event detection and social event classification. In our previous work (Agarwal and Rambow, 2010), we presented results for the two Introduction Social network extraction from text has recently been gaining a considerable amount of attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). One of the reason for this attention, we believe, is that being able to extract social networks from unstructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to quickly extract networks and assess their size, nature, and cohesiveness, a task that would otherwise be impossible with corpora numbering millions of documents. It would also make it possible to make falsifiable claims about these networks, bringing the"
E14-1023,I13-1171,1,0.800281,"contrast, tree kernels are able to find and leverage less strict patterns without requiring the semantic parse to be entirely accurate or complete. Apart from introducing semantic features and tree structures, we evaluate on the task of social network extraction, which is a combination of two sub-tasks: social event detection and social event classification. In our previous work (Agarwal and Rambow, 2010), we presented results for the two Introduction Social network extraction from text has recently been gaining a considerable amount of attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). One of the reason for this attention, we believe, is that being able to extract social networks from unstructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to quickly extract networks and assess their size, nature, and cohesiveness, a task that would otherwise be impossible with corpora numbering millions of documents. It would also make it possible to make falsifiable"
E14-1023,I13-2009,1,0.618676,"contrast, tree kernels are able to find and leverage less strict patterns without requiring the semantic parse to be entirely accurate or complete. Apart from introducing semantic features and tree structures, we evaluate on the task of social network extraction, which is a combination of two sub-tasks: social event detection and social event classification. In our previous work (Agarwal and Rambow, 2010), we presented results for the two Introduction Social network extraction from text has recently been gaining a considerable amount of attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). One of the reason for this attention, we believe, is that being able to extract social networks from unstructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to quickly extract networks and assess their size, nature, and cohesiveness, a task that would otherwise be impossible with corpora numbering millions of documents. It would also make it possible to make falsifiable"
E14-1023,P03-1054,0,0.00662021,"onverted to a vector of dimension 1,174, in which xi (the ith component of vector ~x) is 1 if the frame number i appears in the example, and 0 otherwise. 4.4 Hand-crafted semantic features (RULES) We use the manual of the FrameNet resource to hand-craft 199 rules that are intended to detect the presence and determine the type of social events between two entities mentioned in a sentence. An example of one such rule is given in section 3, which we reformulate here. We also present another example: Bag of words (BOW) We create a vocabulary from our training data by using the Stanford tokenizer (Klein and Manning, 2003) followed by removal of stop words 2 An input example is a sentence with a pair of entity mentions between whom we predict and classify social events. 213 Feature Vectors BOW Lexical Syntactic Semantic (novel) ! BOF RULES ! ! Tree Structures AR2010 ! ! FrameForest ! ! ! FrameTree FrameTreeProp ! ! Table 2: Features and tree structures and the level of abstraction they fall into. (3) If the frame is Statement, and the first target entity mention is contained in the FE Speaker, and the second is contained in the FE Message, then there is an OBS social event from the first entity to the second. F"
E14-1023,P98-1013,0,0.143411,"Automated Content Extraction1 (ACE2005). We defined a social event to be a happening between two entities (of type person) E1 and E2 (E1 6= E2), in which at least one entity is cognitively aware of the other and of the happening taking place. We defined two broad cate1 INR 199 Table 1: Data distribution; INR are interaction social events. OBS are observation social events. 1. We design and propose novel frame semantic features and tree-based representations and show that tree kernels are well suited to work with noisy semantic parses. 2 No-Event 1,609 3 Frame Semantics and FrameNet FrameNet (Baker et al., 1998) is a resource which associates words of English with their meaning. Word meanings are based on the notion of “semantic frame”. A frame is a conceptual description of a type of event, relation, or entity, and it Version: 6.0, Catalog number: LDC2005E18 212 includes a list of possible participants in terms of the roles they play; these participants are called “frame elements”. Through the following example, we present the terminology and acronyms that will be used throughout the paper. Example (2) shows the frame annotations for the sentence Toujan Faisal said she was informed of the refusal by"
E14-1023,D09-1143,0,0.077328,"). In this paper, we evaluate our system on the above tasks as well as a combined task: social network extraction (SNE): given a sentence and a pair of entity mentions, predict the class of the example from one of the following three categories: {No-Event, INR, OBS}. For the purposes of this paper, we use gold named entity mentions to avoid errors caused due to named entity recognition systems. This is a common practice used in the literature for reporting relation extraction systems (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 2009). We use standard terminology from the literature to refer to the pair of entities mentions as target entities T1 and T2 . 2. We show that in order to achieve the best performing system, we need to include features and tree structures from all levels of abstractions, lexical, syntactic, and semantic, and that the convolution kernel framework is well-suited for creating such a combination. 3. We combine the previously proposed subtasks (social event detection and classification) into a single task, social network extraction, and show that combining the models using a hierarchical design is sign"
E14-1023,S10-1059,0,0.0624568,"tations may broadly be categorized into one or more of the following levels of abstraction: {Lexical, Syntactic, Semantic}. Table 2 presents this distribution. Our final results show that all of our top performing models use a data representation that is a combination of features and structures from all levels of abstraction. We review previously proposed features and tree structures in subsections 4.1, 4.2, and 4.3. To the best of our knowledge, the remaining features and structures presented in this section are novel. 4.1 Syntactic structures (AR2010) 4.3 Bag of frames (BOF) We use Semafor (Chen et al., 2010) for obtaining the semantic parse of a sentence. Semafor found instances of 1,174 different FrameNet frames in our corpus. Each example (~x) is converted to a vector of dimension 1,174, in which xi (the ith component of vector ~x) is 1 if the frame number i appears in the example, and 0 otherwise. 4.4 Hand-crafted semantic features (RULES) We use the manual of the FrameNet resource to hand-craft 199 rules that are intended to detect the presence and determine the type of social events between two entities mentioned in a sentence. An example of one such rule is given in section 3, which we refo"
E14-1023,P13-1147,0,0.0266871,"Missing"
E14-1023,P02-1034,0,0.093379,"Missing"
E14-1023,P04-1054,0,0.125665,"Missing"
E14-1023,P05-1052,0,0.0408346,"the type of social event (INR or OBS, social event classification, SEC). In this paper, we evaluate our system on the above tasks as well as a combined task: social network extraction (SNE): given a sentence and a pair of entity mentions, predict the class of the example from one of the following three categories: {No-Event, INR, OBS}. For the purposes of this paper, we use gold named entity mentions to avoid errors caused due to named entity recognition systems. This is a common practice used in the literature for reporting relation extraction systems (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 2009). We use standard terminology from the literature to refer to the pair of entities mentions as target entities T1 and T2 . 2. We show that in order to achieve the best performing system, we need to include features and tree structures from all levels of abstractions, lexical, syntactic, and semantic, and that the convolution kernel framework is well-suited for creating such a combination. 3. We combine the previously proposed subtasks (social event detection and classification) into a single task, social network extraction, an"
E14-1023,P10-1015,0,0.0637007,"Missing"
E14-1023,C98-1013,0,\N,Missing
E91-1005,W90-0214,1,0.751211,"Missing"
E91-1005,C88-2121,1,0.794063,"Missing"
farber-etal-2008-improving,N04-1043,0,\N,Missing
farber-etal-2008-improving,C02-1054,0,\N,Missing
farber-etal-2008-improving,W02-1001,0,\N,Missing
farber-etal-2008-improving,P05-1071,1,\N,Missing
farber-etal-2008-improving,W04-3234,1,\N,Missing
H01-1055,P00-1059,1,0.873365,"Missing"
H01-1055,C00-1007,1,0.920937,"dels of what should be learned. In this respect, the work reported in this paper differs from other recent work on generation in the context of dialog systems [12, 16], which does not modularize the generation process and proposes a single stochastic model for the entire process. We start out by reviewing the generation architecture (Section 2). In Section 3, we discuss the issue of text planning for Communicator. In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomplish this communicative goal. A communicative goal is a goal to affect the user’s cognitive state, e.g., his or her beliefs about the world, desires with respect to the world, or intentions about his or her actions in the world. Following (at least) [13], it has been customary to divide the generation process into three phases, the first two of which are planning phases. Reiter [17] calls this architecture a “consensus architecture”"
H01-1055,W00-1401,1,0.834193,"he intended dialogs is not as relevant: we can try and mimic the human-human transcripts as closely as possible. To show this, we have performed some initial experiments using FERGUS (Flexible Empiricist-Rationalist Generation Using Syntax), a stochastic surface realizer which incorporates a tree model and a linear language model [2]. We have developed a metric which can be computed automatically from the syntactic dependency structure of the sentence and the linear order chosen by the realizer, and we have shown that this metric correlates with human judgments of the felicity of the sentence [3]. Using this metric, we have shown that the use of both the tree model and the linear language model improves the quality of the output of FERGUS over the use of only one or the other of these resources. FERGUS was originally trained on the Penn Tree Bank corpus consisting of Wall Street Journal text (WSJ). The results on an initial set of Communicator sentences were not encouraging, presumably because there are few questions in the WSJ corpus, and furthermore, specific constructions (including what as determiner) appear to be completely absent (perhaps due to a newspaper style file). In an in"
H01-1055,W00-1407,0,0.0206978,"system response to users has been studied by the Natural Language Generation (NLG) community, though rarely in the context of dialog systems. What have emerged from this work are a “consensus architecture” [17] which modularizes the large number of tasks performed during NLG in a particular way, and a range of linguistic representations which can be used in accomplishing these tasks. Many systems have been built using NLG technology, including report generators [8, 7], system description generators [10], and systems that attempt to convince the user of a particular view through argumentation [20, 4]. In this paper, we claim that the work in NLG is relevant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of what should be learned. In this respect, the work reported in this paper differs from other recent"
H01-1055,A97-1037,1,0.824416,"ity in terms of informativeness and of adaptation to the context. For a long time, the issue of system response to users has been studied by the Natural Language Generation (NLG) community, though rarely in the context of dialog systems. What have emerged from this work are a “consensus architecture” [17] which modularizes the large number of tasks performed during NLG in a particular way, and a range of linguistic representations which can be used in accomplishing these tasks. Many systems have been built using NLG technology, including report generators [8, 7], system description generators [10], and systems that attempt to convince the user of a particular view through argumentation [20, 4]. In this paper, we claim that the work in NLG is relevant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of"
H01-1055,W00-0306,0,0.113299,"evant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of what should be learned. In this respect, the work reported in this paper differs from other recent work on generation in the context of dialog systems [12, 16], which does not modularize the generation process and proposes a single stochastic model for the entire process. We start out by reviewing the generation architecture (Section 2). In Section 3, we discuss the issue of text planning for Communicator. In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomp"
H01-1055,A92-1006,1,0.711248,"In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomplish this communicative goal. A communicative goal is a goal to affect the user’s cognitive state, e.g., his or her beliefs about the world, desires with respect to the world, or intentions about his or her actions in the world. Following (at least) [13], it has been customary to divide the generation process into three phases, the first two of which are planning phases. Reiter [17] calls this architecture a “consensus architecture” in NLG. During text planning, a high-level communicative goal is broken down into a structured representation of atomic communicative goals, i.e., goals that can be attained with a single communicative act (in language, by uttering a single clause). The atomic communicative goals may be linked by rhetorical relations which show how attaining the atomic goals contributes to attaining the high-level goal. The work r"
H01-1055,A00-2026,0,0.0354348,"evant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog systems and their special needs. The dialog system we use is the AT&T Communicator travel planning system.We use machine learning and stochastic approaches where hand-crafting appears to be too complex an option, but we also use insight gained during previous work on NLG in order to develop models of what should be learned. In this respect, the work reported in this paper differs from other recent work on generation in the context of dialog systems [12, 16], which does not modularize the generation process and proposes a single stochastic model for the entire process. We start out by reviewing the generation architecture (Section 2). In Section 3, we discuss the issue of text planning for Communicator. In Section 4, we summarize some initial work in using machine learning for sentence planning [19]. Finally, in Section 5 we summarize work using stochastic tree models in generation [2]. 2. TEXT GENERATION ARCHITECTURE . NLG is conceptualized as a process leading from a high-level communicative goal to a sequence of communicative acts which accomp"
H01-1055,W94-0319,0,0.122438,"eraction, the user can supply more and different information at any time in the dialog. The dialog system must then support a mixed-initiative dialog strategy. While this strategy places greater requirements on ASR, it also increases the range of system responses and the requirements on their quality in terms of informativeness and of adaptation to the context. For a long time, the issue of system response to users has been studied by the Natural Language Generation (NLG) community, though rarely in the context of dialog systems. What have emerged from this work are a “consensus architecture” [17] which modularizes the large number of tasks performed during NLG in a particular way, and a range of linguistic representations which can be used in accomplishing these tasks. Many systems have been built using NLG technology, including report generators [8, 7], system description generators [10], and systems that attempt to convince the user of a particular view through argumentation [20, 4]. In this paper, we claim that the work in NLG is relevant to dialog systems as well. We show how the results can be incorporated, and report on some initial work in adapting NLG approaches to dialog syst"
H01-1055,W98-1415,0,0.0323366,"Missing"
H01-1055,N01-1003,1,\N,Missing
H01-1055,P01-1056,1,\N,Missing
H01-1055,A97-1039,0,\N,Missing
habash-etal-2012-conventional,I11-1036,1,\N,Missing
habash-etal-2012-conventional,P11-2007,0,\N,Missing
habash-etal-2012-conventional,N09-1045,1,\N,Missing
habash-etal-2012-conventional,elfardy-diab-2012-simplified,1,\N,Missing
han-etal-2000-handling,palmer-etal-1998-rapid,1,\N,Missing
han-etal-2000-handling,A00-1009,1,\N,Missing
han-etal-2000-handling,P97-1003,0,\N,Missing
han-etal-2000-handling,C90-3001,0,\N,Missing
han-etal-2000-handling,1997.mtsummit-workshop.12,1,\N,Missing
han-etal-2000-handling,J94-4004,0,\N,Missing
han-etal-2000-handling,A97-1039,1,\N,Missing
I11-1138,I08-2099,0,0.0172547,"notation guidelines, the guideline designers need to choose a theoretical framework and a set of linguistic phenomena to be captured. Next, they need to determine a linguistic analysis for each linguistic phenomenon, and demonstrate the analysis with descriptions and examples (e.g., sentences and the corresponding DS or PS trees). Take the HUTB as an example. Because it contains both representation types, DS and PS, it has two sets of guidelines for syntactic annotation, one for each representation type. The DS annotation guidelines follow the Paninian grammatical model (Bharati et al., 1995; Begum et al., 2008). The PS guidelines are inspired by the Principlesand-Parameters methodology, as instantiated by the theoretical developments starting with Government and Binding Theory (Chomsky, 1981). 3 Compatibility and conversion As mentioned in the previous section, annotation guidelines provide linguistic analyses for a set of linguistic phenomena, and they are tied to a representation type (DS or PS). Now given two sets of annotation guidelines (one for DS and the other for PS), the central question is whether automatic conversion between DS and PS is possible; that is, is it possible to write a conver"
I11-1138,P99-1065,0,0.351981,"Missing"
I11-1138,J07-3004,0,0.0397981,"arning. This paper provides a framework in which to think about the question of when such a conversion is possible. 1 Introduction There has been much interest in converting treebanks from one representation to another; for instance, from phrase structure to dependency structure (e.g., motivated by the recent surge in interest in dependency parsing), or from phrase structure to other grammatical frameworks such as LTAG, HPSG, CCG, or LFG. While there has been much work on converting between treebank representations (Collins et al., 1999; Xia and Palmer, 2001; Cahill et al., 2002; Nivre, 2003; Hockenmaier and Steedman, 2007), there has not been a general yet precise discussion of what conditions are necessary for such conversion to happen. In this paper, we provide an analytical framework for determining how difficult it would be to convert representations under one set of annotation guidelines M 1 to representations under another set of guidelines M 2 . We are only interested in cases where annotation guidelines are available for both levels of representation, since it is not clear how one would interpret an undocumented representation, and thus it would not be clear how to evaluate the conversion results. Given"
I11-1138,N10-1049,1,0.871312,"procedure for comparing two sets of annotation guidelines with respect to conversion. Section 5 discuss examples from the HUTB that fall into the two “harder” scenarios for conversion. 1234 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1234–1242, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2 Important concepts in a treebank This study focuses on the relation between DS and PS treebanks. To understand whether an automatic conversion between DS and PS is possible, it is important to distinguish a few concepts in a treebank. Following (Rambow, 2010), we distinguish three concepts: the linguistic phenomena (what he calls “content”), the representation type, and the linguistic theory (what he calls “syntactic theory”). We reinterpret these concepts and extend them, in terms of the HUTB. 2.1 Linguistic phenomena The linguistic phenomena are what we want to represent about the words which make up our treebank: they are the reason for treebanking. If there were no interesting linguistic phenomena, there would be no reason to create treebanks. The task of treebanking consists of identifying which of the phenomena of interest appear in a given"
I11-1138,H01-1014,1,0.878438,"to another (e.g., dependency) before applying machine learning. This paper provides a framework in which to think about the question of when such a conversion is possible. 1 Introduction There has been much interest in converting treebanks from one representation to another; for instance, from phrase structure to dependency structure (e.g., motivated by the recent surge in interest in dependency parsing), or from phrase structure to other grammatical frameworks such as LTAG, HPSG, CCG, or LFG. While there has been much work on converting between treebank representations (Collins et al., 1999; Xia and Palmer, 2001; Cahill et al., 2002; Nivre, 2003; Hockenmaier and Steedman, 2007), there has not been a general yet precise discussion of what conditions are necessary for such conversion to happen. In this paper, we provide an analytical framework for determining how difficult it would be to convert representations under one set of annotation guidelines M 1 to representations under another set of guidelines M 2 . We are only interested in cases where annotation guidelines are available for both levels of representation, since it is not clear how one would interpret an undocumented representation, and thus"
I13-1025,P12-2032,1,0.628694,"enable us to perform the analysis of how power affects dialog behavior. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. There are about 8.5 participants per thread. There are 221 active participants (participants of a thread who has sent at least one email message in the thread) in the corpus. Table 1 presents the counts and percentages of active participants with each type of power in the corpus. We now define the four types of power we investigate in this paper. Hierarchical Power (HP): We use the gold organizational hierarchy for Enron released by Agarwal et al. (2012) to model hierarchical power. It contains relations between 1,518 employees, and 1 The manual annotations also capture the perception of hierarchical power. In this work, we use only the actual gold hierarchy (Agarwal et al., 2012) as described above. 2 In (Prabhakaran et al., 2012a), power over communication was called “control of communication”. 218 From: Kathryn Cordes To: Leslie Hansen, Sara Shackleton, Brent Hendry CC: Mark Greenberg, Erik Eller, Thomas D Gros ———————————————– M1.1. Leslie Sara, and Brent: [Conventional] our annotator judged Mark to have influence over Brent since the lat"
I13-1025,W12-2105,1,0.471828,"nce the real identities of the members of such communities are often not revealed and their hierarchies may not be available to the law enforcement agencies. The power differential between the DPs may be based on a multitude of factors such as status, authority, role, knowledge and so on. Early computational approaches to analyzing power in interactions relied solely on static power structures such as corporate hierarchies as the source of the power differential (Rowe et al., 2007; Bramsen et al., 2011). More recent studies have looked into dynamic notions of power as well, such as influence (Biran et al., 2012). However, not much work has been done to understand how different types of power differ in the ways they affect how people interact in dialog. In this paper, we study four different types of power — hierarchical power, situational power, influence and power over communication. We investigate whether all four social power relations are manifested in dialog behavior; we restrict our attention to written dialog, specifically email exchanged in an American corporation. By “dialog behavior”, we mean the choices a DP makes while engaging in dialog. Dialog behavior includes choices that affect dialo"
I13-1025,P11-1078,0,0.181285,"rcement agencies to detect leaders and influencers in suspicious online communities. This is especially useful since the real identities of the members of such communities are often not revealed and their hierarchies may not be available to the law enforcement agencies. The power differential between the DPs may be based on a multitude of factors such as status, authority, role, knowledge and so on. Early computational approaches to analyzing power in interactions relied solely on static power structures such as corporate hierarchies as the source of the power differential (Rowe et al., 2007; Bramsen et al., 2011). More recent studies have looked into dynamic notions of power as well, such as influence (Biran et al., 2012). However, not much work has been done to understand how different types of power differ in the ways they affect how people interact in dialog. In this paper, we study four different types of power — hierarchical power, situational power, influence and power over communication. We investigate whether all four social power relations are manifested in dialog behavior; we restrict our attention to written dialog, specifically email exchanged in an American corporation. By “dialog behavio"
I13-1025,prabhakaran-etal-2012-annotations,1,0.479405,"ocial relations from online communication. 217 Researchers have also applied NLP techniques on message content to detect power relations. Earlier approaches used simple lexical features (e.g. (Bramsen et al., 2011; Gilbert, 2012)) while later studies have performed deeper discourse analysis and used features such as linguistic coordination (Danescu-Niculescu-Mizil et al., 2012), language uses such as attempts to persuade and various other dialog patterns (Biran et al., 2012). We present a more detailed discussion of the above mentioned studies and how they differ from our line of research in (Prabhakaran et al., 2012c). Our research also falls into the category of studies that go beyond pure lexical features and use dialog structure based features to extract social power relations. In (Prabhakaran et al., 2012c), we studied the notion of situational power in depth and presented a system to detect persons with situational power using dialog features. In this paper as well, we use the system described in (Prabhakaran et al., 2012c). However, this work differs from (Prabhakaran et al., 2012c) and other studies described above in that our focus is on how different types of power are manifested differently in"
I13-1025,N12-1057,1,0.747258,"ocial relations from online communication. 217 Researchers have also applied NLP techniques on message content to detect power relations. Earlier approaches used simple lexical features (e.g. (Bramsen et al., 2011; Gilbert, 2012)) while later studies have performed deeper discourse analysis and used features such as linguistic coordination (Danescu-Niculescu-Mizil et al., 2012), language uses such as attempts to persuade and various other dialog patterns (Biran et al., 2012). We present a more detailed discussion of the above mentioned studies and how they differ from our line of research in (Prabhakaran et al., 2012c). Our research also falls into the category of studies that go beyond pure lexical features and use dialog structure based features to extract social power relations. In (Prabhakaran et al., 2012c), we studied the notion of situational power in depth and presented a system to detect persons with situational power using dialog features. In this paper as well, we use the system described in (Prabhakaran et al., 2012c). However, this work differs from (Prabhakaran et al., 2012c) and other studies described above in that our focus is on how different types of power are manifested differently in"
I13-1025,C12-1138,1,0.459973,"ocial relations from online communication. 217 Researchers have also applied NLP techniques on message content to detect power relations. Earlier approaches used simple lexical features (e.g. (Bramsen et al., 2011; Gilbert, 2012)) while later studies have performed deeper discourse analysis and used features such as linguistic coordination (Danescu-Niculescu-Mizil et al., 2012), language uses such as attempts to persuade and various other dialog patterns (Biran et al., 2012). We present a more detailed discussion of the above mentioned studies and how they differ from our line of research in (Prabhakaran et al., 2012c). Our research also falls into the category of studies that go beyond pure lexical features and use dialog structure based features to extract social power relations. In (Prabhakaran et al., 2012c), we studied the notion of situational power in depth and presented a system to detect persons with situational power using dialog features. In this paper as well, we use the system described in (Prabhakaran et al., 2012c). However, this work differs from (Prabhakaran et al., 2012c) and other studies described above in that our focus is on how different types of power are manifested differently in"
I13-1025,W09-3953,1,0.924365,"t because Kathryn is following up on and assigning a task to others, and because Kathryn uses language that shows that she is in charge of the situation. Power over Communication (PC): A person is said to have power over communication if he actively attempts to achieve the intended goals of the communication.2 These are people who ask questions, request others to take action, etc., and not Data and Annotations We use the subset of the Enron email corpus with power annotations presented in Prabhakaran et al. (2012a) for our experiments. The corpus also contains manual dialog act annotations by Hu et al. (2009), which enable us to perform the analysis of how power affects dialog behavior. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. There are about 8.5 participants per thread. There are 221 active participants (participants of a thread who has sent at least one email message in the thread) in the corpus. Table 1 presents the counts and percentages of active participants with each type of power in the corpus. We now define the four types of power we investigate in this paper. Hierarchical Power (HP): We use the gold organizational hierarchy for Enron rele"
I13-1025,P90-1010,0,0.857359,"ion 2, we discuss related work in the field. Section 3-4 presents the data, annotations, and inter-rater agreement studies on the annotations. Section 5 summarizes the dimensions of interactions we analyze. We then present the main contributions of this paper: Section 6 analyzes the variations in the manifestations of power among the four types, and Section 7 describes a system to predict persons with any of the four types of power. We then conclude and discuss future work. 2 Related Work Within the dialog community, researchers have studied notions of control and initiative in dialogs (e.g. (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997)). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information. They use utterance level rules to determine which discourse participant (whether the speaker or the hearer) is in control, and extend it to segments of discourse. Their notion of control differs from our notion of power over communication. They model control locally over discourse segments. What we are interested in (and what our annotations capture) is the possession of controlling power by one (or more) parti"
I13-1025,C00-2137,0,0.119915,"Missing"
I13-1171,D10-1100,1,0.919603,"a social network from Alice in Wonderland that is not restricted to interactions signaled by quoted speech. We define a social network for a fictional text as follows: nodes are characters and links are social events. Two nodes in the network are connected if the characters engage in a social event. We introduced the notion of social events in our previous work (Agarwal et al., 2010), in which we presented our annotation scheme for annotating social events on the Automatic Content Extraction (ACE-2005) corpus. We presented a preliminary system for social event detection and classification in Agarwal and Rambow (2010). This system was trained and tested only on the ACE-2005 corpus. A priori, it is unclear if a system trained on a news corpus will be able to extract a high quality social network from a text from a very different genre (literary fiction). There are many syntactic and lexical differences between these genres. For example, news corpora have almost no questions, very little dialog presented as direct speech, and very little use of the first and second person pronouns. The vocabulary in literature can also be very different (relating to, say, whaling, passion, or teenage angst rather than curren"
I13-1171,W10-1803,1,0.662603,"Missing"
I13-1171,W12-2513,1,0.870209,"y committee] overseeing election preparations. In the above example, the people (or groups of people) involved in social events are Toujan Faisal and the Interior Ministry committee. There are two social events in this example: one described by the word said, in which Toujan Faisal is talking about the committee, and the other described by the word informed, in which Toujan Faisal presumably has a mutual interaction with the committee. We annotated two corpora for social events: 1) The Automatic Content Extraction (ACE) dataset1 (Agarwal et al., 2010) and 2) the Alice in Wonderland data-set2 (Agarwal et al., 2012). For each pair of entity mentions in a sentence, if the annotators annotate a social event, we count this as a positive example for the task of social event detection. If no social event is annotated between a pair of entity mentions, we count this as a negative example. Note that we only consider pairs of entity mentions that correspond to different entities; our annotation scheme disregards selfinteractions (talking to oneself). 1 2 # of No-event 1,101 128 Table 1: The distribution of social events in the training and test sets used for experiments 3 Social Events and Data In Agarwal et al."
I13-1171,P10-1015,0,0.351182,"Missing"
I13-1171,P05-1053,0,0.0208073,"Missing"
I13-1171,W12-4102,0,0.0868485,"f connected components (CC) and the number of triads (T). Using these results, we conclude that the network predicted by our system that uses tree kernels performs well in terms of extracting an unweighted, undirected network from Alice in Wonderland. In particular, the tree structure derived from the phrase structure tree (PET) performs the best on most of the SNA metrics. 1206 6 Literature Survey With the advent of the internet and social media, researchers have got access to different forms of communication such as Email (Klimt and Yang, 2004; Rowe et al., 2007), online discussion threads (Hassan et al., 2012), Slashdot, Epinions, and Wikipedia (Jure Leskovec and Kleinberg, 2010). There have also been approaches of extracting networks based on Information Retrieval techniques – Jing et al. (2007) extract a network from conversational speech data. The events they are interested in are custody, death, hiding, liberation, marriage, migration, survival and violence. Tang et al. (2008) aim at extracting and mining academic social networks. Aron Culotta and McCallum (2004) extract social networks and contact information from email and the Web. Mori et al. (2006) mine networks based on the collective cont"
I13-1171,P13-1129,0,0.14442,"interested in are custody, death, hiding, liberation, marriage, migration, survival and violence. Tang et al. (2008) aim at extracting and mining academic social networks. Aron Culotta and McCallum (2004) extract social networks and contact information from email and the Web. Mori et al. (2006) mine networks based on the collective context in which entities appear. Our notion of social network is different from the aforementioned work. We are interested in extracting interaction networks from unstructured text. In terms of our goals, our work is closest to the work by Elson et al. (2010) and He et al. (2013). Elson et al. (2010) and He et al. (2013) are also interested in extracting a social network from literary texts. However, they restrict their definition of interaction to interactions that are signaled by quotation marks. Our system does not have this limitation and is therefore able to extract interaction links appearing even in reported speech (nondialogue text). 7 Conclusion and Future Work In this paper, we have addressed the problem of extracting a social network from literary narrative text. We have used our previous system that detects social events to extract a network from Alice in"
I13-1171,P07-1131,0,0.0161002,"an unweighted, undirected network from Alice in Wonderland. In particular, the tree structure derived from the phrase structure tree (PET) performs the best on most of the SNA metrics. 1206 6 Literature Survey With the advent of the internet and social media, researchers have got access to different forms of communication such as Email (Klimt and Yang, 2004; Rowe et al., 2007), online discussion threads (Hassan et al., 2012), Slashdot, Epinions, and Wikipedia (Jure Leskovec and Kleinberg, 2010). There have also been approaches of extracting networks based on Information Retrieval techniques – Jing et al. (2007) extract a network from conversational speech data. The events they are interested in are custody, death, hiding, liberation, marriage, migration, survival and violence. Tang et al. (2008) aim at extracting and mining academic social networks. Aron Culotta and McCallum (2004) extract social networks and contact information from email and the Web. Mori et al. (2006) mine networks based on the collective context in which entities appear. Our notion of social network is different from the aforementioned work. We are interested in extracting interaction networks from unstructured text. In terms of"
I13-1171,P03-1054,0,0.0039669,"ion is deliberate or conscious. Put differently, at least one person must be aware of the interaction. # of social events 396 81 SINNET: Social Interaction Network Extractor from Text In Agarwal and Rambow (2010), we presented a preliminary system that extracts social events from news articles. We used Support Vector Machines (SVM) in conjunction with tree kernels for detecting social events between pairs of entities, called target entities, that co-occur in a sentence. Following is a brief description of the tree structures that we used for building our models. We used the Stanford parser’s (Klein and Manning, 2003) phrase structure and dependency tree representations. Of the following tree structures, 1-3 have previously been proposed by Nguyen et al. (2009) for the relation extraction task, while we introduced the fourth structure in Agarwal and Rambow (2010) for social event detection task. Version: 6.0, Catalog number: LDC2005E18 http://www.gutenberg.org/ebooks/19551 1203 1. PET: This refers to the smallest phrase structure tree that contains the two target entities. 2. Grammatical Relation (GR) tree: This refers to the smallest dependency tree that contains the two target entities. We replace the wo"
I13-1171,E06-1015,0,0.0141009,"om the node. Degree centrality is viewed as an index of a node’s communication activity. 4. Sequence in GRW tree (SqGRW): This is the sequence of nodes from one target to the other in the GRW tree. For example, in Figure 1, this would be Toujan Faisal nsubj T1Individual said ccomp informed prep by T2Group pobj committee. We also use combinations of the aforementioned structures. For example, PET GR SqGRW refers to a kernel that considers a linear combination of three structures (PET, GR and SqGRW) for calculating similarities between examples. We use the Partial Tree kernel, first proposed by Moschitti (2006a), to calculate similarities between these tree structures. In this paper, we use a Bag of Words Model (BOW) as a baseline. In the BOW model, each sentence is represented as a vector of three feature spaces. The first feature space encodes the presence and absence of words between the start of sentence and the start of the first target entity mention. The second feature space encodes the presence and absence of words between the end of the first target entity mention and the start of the second target entity mention. The third feature space encodes the presence and absence of words between th"
I13-1171,D09-1143,0,0.0363488,"ction Network Extractor from Text In Agarwal and Rambow (2010), we presented a preliminary system that extracts social events from news articles. We used Support Vector Machines (SVM) in conjunction with tree kernels for detecting social events between pairs of entities, called target entities, that co-occur in a sentence. Following is a brief description of the tree structures that we used for building our models. We used the Stanford parser’s (Klein and Manning, 2003) phrase structure and dependency tree representations. Of the following tree structures, 1-3 have previously been proposed by Nguyen et al. (2009) for the relation extraction task, while we introduced the fourth structure in Agarwal and Rambow (2010) for social event detection task. Version: 6.0, Catalog number: LDC2005E18 http://www.gutenberg.org/ebooks/19551 1203 1. PET: This refers to the smallest phrase structure tree that contains the two target entities. 2. Grammatical Relation (GR) tree: This refers to the smallest dependency tree that contains the two target entities. We replace the words (in the nodes of the tree) with their corresponding grammatical roles. For example, in Figure 1, if we replace Toujan Faisal by nsubj, 54 by a"
I13-1171,W93-0231,0,0.139274,"n. The third feature space encodes the presence and absence of words between the end of the second target entity mention and the end of the sentence. This feature space has previously been used by GuoDong et 2. Betweenness centrality of a node in the network measures the frequency with which a point falls between pairs of other nodes on the shortest paths connecting them. Nodes with high betweenness centrality are strategically located on the communication paths linking pairs of others, thus having the potential of influencing the group by withholding or distorting information (Bavelas, 1948; Shaw, 1954; Shimbel, 1953). Another aspect of social networks that SNA researchers are interested in has to do with finding communities in the network and structural properties of networks. Following are some basic metrics used for this task: 1204 1. Graph density: The density of a graph is the ratio of the number of edges to the number of possible edges. This measures how close the network is to being complete. 2. Connected components: a connected component of an undirected graph is a subgraph in which any two vertices are connected to each other by some path. The number of connected components is an i"
I13-2004,habash-etal-2006-design,1,0.651317,"h or h H. We address this problem in the context of natural language processing of Arabic dialect by proposing a conventional orthography for representing dialecIntroduction The Arabic language poses two problems for information retrieval (IR). First, Arabic is morphologically rich, which increases the likelihood of mismatch between words used in queries and words in documents. Much work has been done on addressing this issue in the context of Modern Standard Arabic (MSA), primarily using different methods of stemming and query reformulation (Al-Kharashi and Evens, 1999; Darwish et al., 2005; Habash et al., 2006; Larkey et al., 2007).1 Secondly, the Arabic-speaking world displays diglossia, meaning that a standard language, MSA, co-exists with dialects, such as Egyptian Arabic (EGY). The dialects differ from MSA in many dimensions, which limits the effectiveness of using MSA tools to handle the dialects. Relevant to IR are lexical and morphological differences. Lexically, different words may be used to 2 Arabic transliteration throughout the paper is presented in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007): (in alphabetical order) ˇ AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional ˇ ¯ sy"
I13-2004,W05-0704,0,0.0320462,"Y may be written as ë h or h H. We address this problem in the context of natural language processing of Arabic dialect by proposing a conventional orthography for representing dialecIntroduction The Arabic language poses two problems for information retrieval (IR). First, Arabic is morphologically rich, which increases the likelihood of mismatch between words used in queries and words in documents. Much work has been done on addressing this issue in the context of Modern Standard Arabic (MSA), primarily using different methods of stemming and query reformulation (Al-Kharashi and Evens, 1999; Darwish et al., 2005; Habash et al., 2006; Larkey et al., 2007).1 Secondly, the Arabic-speaking world displays diglossia, meaning that a standard language, MSA, co-exists with dialects, such as Egyptian Arabic (EGY). The dialects differ from MSA in many dimensions, which limits the effectiveness of using MSA tools to handle the dialects. Relevant to IR are lexical and morphological differences. Lexically, different words may be used to 2 Arabic transliteration throughout the paper is presented in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007): (in alphabetical order) ˇ AbtθjHxdðrzsšSDTDςγfqklmnhwy and t"
I13-2004,W12-2301,1,0.952931,"éËðA£  ð P zawja¯h ék . hawlA’ ZBñë ˆ Table 1: Four examples showing lexical variation among Arabic dialects and MSA. convey the same meaning in different dialects and MSA. Table 1 presents the same set of four words in English, MSA, Egyptian Arabic and Levantine Arabic.2 Morphologically, the dialects may use different forms from MSA, e.g., the short phrase ‘he writes’  JºJ K. appears as I in MSA , but as I . JºK yaktubu . JºK X dayiktib in Iraqi Arabic biyiktib in EGY, I .  and I . JºJ » kayiktib in Moroccan Arabic. The differences between MSA and dialect morphology can be rather large: Habash et al. (2012a) report that over one-third of EGY words cannot be analyzed using an MSA morphological analyzer; and Habash and Rambow (2006) report similar figures for Levantine verbs. Furthermore, while MSA has a standard orthography, the dialects are not orthographically standardized, which leads to the coexistence of multiple spellings for the same word, e.g., the future marker in EGY may be written as ë h or h H. We address this problem in the context of natural language processing of Arabic dialect by proposing a conventional orthography for representing dialecIntroduction The Arabic language poses tw"
I13-2004,habash-etal-2012-conventional,1,0.905655,"éËðA£  ð P zawja¯h ék . hawlA’ ZBñë ˆ Table 1: Four examples showing lexical variation among Arabic dialects and MSA. convey the same meaning in different dialects and MSA. Table 1 presents the same set of four words in English, MSA, Egyptian Arabic and Levantine Arabic.2 Morphologically, the dialects may use different forms from MSA, e.g., the short phrase ‘he writes’  JºJ K. appears as I in MSA , but as I . JºK yaktubu . JºK X dayiktib in Iraqi Arabic biyiktib in EGY, I .  and I . JºJ » kayiktib in Moroccan Arabic. The differences between MSA and dialect morphology can be rather large: Habash et al. (2012a) report that over one-third of EGY words cannot be analyzed using an MSA morphological analyzer; and Habash and Rambow (2006) report similar figures for Levantine verbs. Furthermore, while MSA has a standard orthography, the dialects are not orthographically standardized, which leads to the coexistence of multiple spellings for the same word, e.g., the future marker in EGY may be written as ë h or h H. We address this problem in the context of natural language processing of Arabic dialect by proposing a conventional orthography for representing dialecIntroduction The Arabic language poses tw"
I13-2004,P06-1086,1,0.880511,"vey the same meaning in different dialects and MSA. Table 1 presents the same set of four words in English, MSA, Egyptian Arabic and Levantine Arabic.2 Morphologically, the dialects may use different forms from MSA, e.g., the short phrase ‘he writes’  JºJ K. appears as I in MSA , but as I . JºK yaktubu . JºK X dayiktib in Iraqi Arabic biyiktib in EGY, I .  and I . JºJ » kayiktib in Moroccan Arabic. The differences between MSA and dialect morphology can be rather large: Habash et al. (2012a) report that over one-third of EGY words cannot be analyzed using an MSA morphological analyzer; and Habash and Rambow (2006) report similar figures for Levantine verbs. Furthermore, while MSA has a standard orthography, the dialects are not orthographically standardized, which leads to the coexistence of multiple spellings for the same word, e.g., the future marker in EGY may be written as ë h or h H. We address this problem in the context of natural language processing of Arabic dialect by proposing a conventional orthography for representing dialecIntroduction The Arabic language poses two problems for information retrieval (IR). First, Arabic is morphologically rich, which increases the likelihood of mismatch be"
I13-2009,W10-1803,1,0.46696,"Missing"
I13-2009,W12-2513,1,0.830757,"xtractor from Text (SINNET). SINNET is able to extract a social network from unstructured text. Nodes in the network are people and links are social events. of text such as reported speech and other nondialogue text. Our system overcomes this limitation. The rest of the paper is structured as follows: In section 2, we briefly describe the research that has gone into building the system. In section ??, we present the technical details of SINNET and describe our web demo. Introduction 2 The SINNET system is the result of several years of research (Agarwal et al., 2010; Agarwal and Rambow, 2010; Agarwal et al., 2012; Agarwal et al., 2013). In Agarwal et al. (2010), we introduced the notion of social events. A social event is a happening between two people, at least one of whom is cognizant of the other and of the event taking place. At a broad level, there are two types of social events: interaction (INR) and observation (OBS). INR is a bi-directional event in which both parties are mutually aware of each other. Examples of INR are a meeting or a dinner. OBS is a one-directional event in which only one party is aware of the other. Examples of OBS are thinking about someone, or missing someone. In Agarwal"
I13-2009,I13-1171,1,0.45399,"NNET). SINNET is able to extract a social network from unstructured text. Nodes in the network are people and links are social events. of text such as reported speech and other nondialogue text. Our system overcomes this limitation. The rest of the paper is structured as follows: In section 2, we briefly describe the research that has gone into building the system. In section ??, we present the technical details of SINNET and describe our web demo. Introduction 2 The SINNET system is the result of several years of research (Agarwal et al., 2010; Agarwal and Rambow, 2010; Agarwal et al., 2012; Agarwal et al., 2013). In Agarwal et al. (2010), we introduced the notion of social events. A social event is a happening between two people, at least one of whom is cognizant of the other and of the event taking place. At a broad level, there are two types of social events: interaction (INR) and observation (OBS). INR is a bi-directional event in which both parties are mutually aware of each other. Examples of INR are a meeting or a dinner. OBS is a one-directional event in which only one party is aware of the other. Examples of OBS are thinking about someone, or missing someone. In Agarwal and Rambow (2010), we"
I13-2009,P10-1015,0,0.13616,"Missing"
I13-2009,P13-1129,0,0.143624,"oups of human beings who are connected to each other through various relationships by the virtue of participating in social events. We define social events to be events that occur between people where at least one person is aware of the other and of the event taking place. For example, in the sentence John talks to Mary, entities John and Mary are aware of each other and of the talking event. In the sentence John thinks Mary is great, only John is aware of Mary and the event is the thinking event. There has been recent work on extracting social networks from literary text (Elson et al., 2010; He et al., 2013). However, both these works focus on extracting only conversational links between people, signaled in text by quotation marks. They do not extract social event links from other parts 1 A web demo is http://nlp.ldeo.columbia.edu/sinnet/ available Research at 33 The Companion Volume of the Proceedings of IJCNLP 2013: System Demonstrations, pages 33–36, Nagoya, Japan, 14-18 October 2013. Rabbit Rabbit Alice Alice Mouse Alice asked the Mouse, “do you know the way out of this pool?” Alice saw the Rabbit run by her (a) (b) Figure 1: Two figures exemplifying the meaning of social events and social ne"
I13-2009,P03-1054,0,0.00534126,"al et al. (2012), we argued that a static network does not bring out the true nature of a network. For example, even though the centrality of the Mouse in a static network is high, a dynamic network analysis shows that the mouse is central only in one chapter of the novel (Chapter 3 – The drying ceremony). Figure 4 shows the the network at the end of chapter 1 and chapter 3. 3 System details and Web demo SINNET is fully implemented in Java. Following is a list of external off-the-shelf tools used by our current pipeline: Jet sentence splitter, Jet NER (Grishman et al., 2005), Stanford parser (Klein and Manning, 2003), SVM-Light-TK (Moschitti, 2006), Input to SINNET may be provided in two formats: as raw text or text with entity annotations. Figure 5: Image of our web demo If the text is input as raw text without any entity annotations, SINNET first runs an off-the-shelf named entity recognizer and co-reference resolution (NER) tool. Currently, we run Jet (Grish35 References man et al., 2005), but an interface makes it easy to plug-in any other NER tool. Once the text is annotated with entity mentions, for each sentence, for each entity mention pair per sentence, we create test examples in the format that"
I13-2009,E06-1015,0,0.0155902,"c network does not bring out the true nature of a network. For example, even though the centrality of the Mouse in a static network is high, a dynamic network analysis shows that the mouse is central only in one chapter of the novel (Chapter 3 – The drying ceremony). Figure 4 shows the the network at the end of chapter 1 and chapter 3. 3 System details and Web demo SINNET is fully implemented in Java. Following is a list of external off-the-shelf tools used by our current pipeline: Jet sentence splitter, Jet NER (Grishman et al., 2005), Stanford parser (Klein and Manning, 2003), SVM-Light-TK (Moschitti, 2006), Input to SINNET may be provided in two formats: as raw text or text with entity annotations. Figure 5: Image of our web demo If the text is input as raw text without any entity annotations, SINNET first runs an off-the-shelf named entity recognizer and co-reference resolution (NER) tool. Currently, we run Jet (Grish35 References man et al., 2005), but an interface makes it easy to plug-in any other NER tool. Once the text is annotated with entity mentions, for each sentence, for each entity mention pair per sentence, we create test examples in the format that our models accept. We use tree k"
I13-2009,D10-1100,1,\N,Missing
J01-1004,E91-1005,1,0.838818,"Missing"
J01-1004,1995.iwpt-1.6,1,0.81509,"Missing"
J01-1004,W98-0107,0,0.223773,"Missing"
J01-1004,E99-1029,1,0.906601,"Missing"
J01-1004,W00-2007,1,0.862891,"Missing"
J01-1004,P97-1003,0,0.100262,"Missing"
J01-1004,W98-0116,0,0.0543715,"Missing"
J01-1004,C96-1091,0,0.0632209,"Missing"
J01-1004,W98-0117,0,0.0362597,"Missing"
J01-1004,P95-1013,1,0.883932,"Missing"
J01-1004,W98-0124,0,0.0468347,"Missing"
J01-1004,P83-1020,0,0.179008,"Missing"
J01-1004,W98-0135,1,0.911043,"Missing"
J01-1004,P95-1021,1,0.931775,"Missing"
J01-1004,P92-1010,1,0.861998,"2, we give some formal definitions and in Section 3 discuss some of the formal properties of DSG. In Section 4, we present analyses in DSG for various linguistic constructions in several languages, and compare them to the corresponding LTAG analyses. In Section 5, we discuss the particular problem of modeling syntactic dependency. We conclude with a discussion of some related work and summary. 2. D e f i n i t i o n of D S G D-trees are the primitive elements of a DSG. D-trees are descriptions of trees, in particular, certain types of expressions in a tree description language such as that of Rogers and Vijay-Shanker (1992). In this section we define tree descriptions and substitution of tree descriptions (Section 2.1) and d-trees (Section 2.2) together with some associated terminology and the graphical representation (Section 2.3). We then define d-tree substitution grammars, along with derivations of d-tree substitution grammars (Section 2.4) and languages generated by these grammars (Section 2.5), and close with an informal discussion of path constraints (Section 2.6). 2.1 Tree D e s c r i p t i o n s and S u b s t i t u t i o n In the following, we are interested in a tree description language that provides"
J01-1004,J94-1004,0,0.260946,"Missing"
J01-1004,J92-4004,1,0.961676,"the tree into which adjunction occurs. In LTAG, the lexicalized elementary objects are defined in such a w a y that the structural relationships between the anchor and each of its dependents change during the course of a derivation through the operation of adjunction, as just illustrated. This approach is not the only possibility. An alternative would be to define the relationships between the nodes of the elementary objects in such a w a y that these relationships hold throughout the derivation, regardless of how the derivation proceeds. This perspective on the LTAG formalism was explored in Vijay-Shanker (1992) where, following the principles of d-theory parsing (Marcus, Hindle, and Fleck 1983), LTAG was seen as a system manipulating descriptions of trees rather than as a tree 2 The same analysis holds for wh-movement, but w e use topicalization as an example in order to avoid the superficial complication of the auxiliary n e e d e d in English questions. Sometimes, topicalized sentences s o u n d s o m e w h a t less natural than the corresponding wh-questions, which are always structurally equivalent. 88 Rambow, Vijay-Shanker, and Weir fl&apos;: NP I Peter S NP S D-Tree Substitution Grammars I I I S yo"
J01-1004,1995.iwpt-1.30,1,0.873684,"Missing"
J01-1004,P94-1036,1,\N,Missing
J13-1008,E12-1069,1,0.937896,"th 2012). The Elixir-FM analyzer (Smrž 2007) readily provides the 4 PATB-tokenized words; see Section 2.5. 5 We ignore duals, which are regular in Arabic, and case/state variations in this discussion for simplicity. 6 Note that the functional and form-based feature values for verbs always coincide. 165 Computational Linguistics Volume 39, Number 1 functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, and rationality features (Alkuhlani and Habash 2012). See Section 5.2 for more details. 2.4 Morpho-Syntactic Interactions Inflectional features and rationality interact with syntax in two ways. In agreement relations, two words in a specific syntactic configuration have coordinated values for specific sets of features. MSA has standard (i.e., matching value) agreement for subject– verb pairs on PERSON, GENDER, and NUMBER, and for noun–adjective pairs on NUMBER, GENDER, CASE , and DET . There are, however, three very common cases of exceptional agreement: Verbs preceding subjects are always singular, adjectives of irrational plural nouns are alw"
J13-1008,W06-2920,0,0.0791718,"Rambow Arabic Parsing with Lexical and Inflectional Features comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima’an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing. As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser. Our results agree with previo"
J13-1008,P99-1065,0,0.172472,"ill be of little or no help for parsing, even if helpful when its gold values are provided. As we will see, the CASE feature is very relevant and not redundant, but it cannot be predicted with high accuracy and overall it is not useful. Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. In the past, it has been shown that if we can recognize the relevant morphological features in assignment configurations well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al. 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. It has been more difficult showing that agreement morphology helps parsing, however, with negative results for dependency parsing in several languages (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009). In this article we investigate morphological features for dependency parsing of Modern Standard Arabic (MSA). For MSA, the space of possible morphological features is fairly large. We determine which morphological features help and why. We further determine the upper bound for thei"
J13-1008,H05-1100,0,0.0777303,"Missing"
J13-1008,W07-0802,0,0.109652,"Missing"
J13-1008,N04-4038,0,0.0532459,"Missing"
J13-1008,J08-3003,0,0.160042,"Missing"
J13-1008,N10-1115,0,0.200839,"features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb–subject and noun–adjective relations. The result holds for both the MaltParser (Nivre 2008) and the Easy-First Parser (Goldberg and Elhadad 2010). Additionally, almost all work to date in MSA morphological analysis and part-ofspeech (POS) tagging has concentrated on the morphemic form of the words. Often, however, the functional morphology (which is relevant to agreement, and relates to the meaning of the word) is at odds with the “surface” (form-based) morphology; a well-known example of this are the “broken” (irregular) plurals of nominals. We show that by modeling the functional morphology rather than the form-based morphology, we obtain a further increase in parsing performance (again, both when using gold and when using predicted"
J13-1008,C10-1045,0,0.0169008,"orms other combinations. Our approach is 168 Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima’an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing. As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT. His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here we"
J13-1008,D07-1116,1,0.910316,"Missing"
J13-1008,P05-1071,1,0.925477,"the correspondence between inflectional features and morphemes, and inspired by Smrž (2007), we distinguish between two types of inflectional features: formbased (a.k.a. surface, or illusory) features and functional features.6 Most available Arabic NLP tools and resources model morphology using formbased (“surface”) inflectional features, and do not mark rationality; this includes the Penn Arabic Treebank (PATB) (Maamouri et al. 2004), the Buckwalter morphological analyzer (Buckwalter 2004), and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow 2005; Habash, Rambow, and Roth 2012). The Elixir-FM analyzer (Smrž 2007) readily provides the 4 PATB-tokenized words; see Section 2.5. 5 We ignore duals, which are regular in Arabic, and case/state variations in this discussion for simplicity. 6 Note that the functional and form-based feature values for verbs always coincide. 165 Computational Linguistics Volume 39, Number 1 functional inflectional number feature, but not full functional gender (only for adjectives and verbs but not for nouns), nor rationality. In this article, we use an in-house system which provides functional gender, number, an"
J13-1008,P09-2056,1,0.477046,"in which both elements generally exhibit agreement in definiteness (and agreement in other features, too). Although only N-N may be followed by additional N elements in Idafa relation, both constructions may be followed by one or more adjectival modifiers. Lexical features do not constrain syntactic structure as inflectional features do. Instead, bilexical dependencies are used to model semantic relations that often are the only way to disambiguate among different possible syntactic structures. 2.5 Corpus, CATiB Format, and the CATIB 6 POS Tag Set We use the Columbia Arabic Treebank (CATiB) (Habash and Roth 2009). Specifically, we use the portion converted from Part 3 of the PATB to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information. CATiB’s dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations. It has a reduced POS tag set consisting of six tags only (henceforth CATIB 6). The tags are: NOM (non-proper nominals including nouns, pronouns, adjectives, and adverbs), PROP (proper nouns), VRB (active-voice verbs), VRB-PASS (passive-voice verbs), PRT (particles such as prepositions or conjunctions), and PNX"
J13-1008,P98-1080,0,0.174298,"Missing"
J13-1008,N12-1032,0,0.167008,"go beyond previous work, however, and explore additional lexical and inflectional features. Previous work with MaltParser in Russian, Turkish, and Hindi showed gains with CASE but not with agreement features (Eryigit, Nivre, and Oflazer 2008; Nivre, Boguslavsky, and Iomdin 2008; Nivre 2009). Our work is the first to show gains using agreement in MaltParser and in Arabic dependency parsing, and the first to use functional features for this task. Furthermore, we demonstrate that our results carry over successfully to another parser, the Easy-First Parser (Goldberg and Elhadad 2010) (Section 6). Hohensee and Bender (2012) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor. These features are potentially powerful, because they generalize to the very notion of agreement, away from the specific values of the attributes on which agreement occurs.9 We expect this kind of feature to yield lower gains for Arabic, unless: r one uses functional feature values (such as those used here for the first time in Arabic NLP), r one uses yet another representation level to account for the otherwise non-id"
J13-1008,W10-1402,1,0.636974,"Missing"
J13-1008,nilsson-nivre-2008-malteval,0,0.0121534,"ng the robustness of our findings across these frameworks. In Section 7 we explore alternative training methods, and their impact on key models. All results are reported mainly in terms of labeled attachment accuracy score (the parent word and the type of dependency relation to it, abbreviated as L AS), which is also used for greedy (hill-climbing) decisions for feature combination. Unlabeled attachment accuracy score (U AS) and label accuracy (dependency relation regardless of parent, L S) are also given. For statistical significance, we use McNemar’s test on non-gold L AS, as implemented by Nilsson and Nivre (2008). We denote p < 0.05 and p < 0.01 with + and ++ , respectively. 4.1 Data Sets and Parser For all the experiments reported in this article, we used the training portion of PATB Part 3 v3.1 (Maamouri et al. 2004), converted to the CATiB Treebank format, as mentioned in Section 2.5. We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set.10 We kept the test unseen (“blind”) during training and mode"
J13-1008,W03-3017,0,0.0314375,"rted to the CATiB Treebank format, as mentioned in Section 2.5. We used the same training / devtest split as in Zitouni, Sorensen, and Sarikaya (2006); and we further split the devtest into two equal parts: a development (dev) set and a blind test set. For all experiments, unless specified otherwise, we used the dev set.10 We kept the test unseen (“blind”) during training and model development. Statistics about this split (after conversion to the CATiB dependency format) are given in Table 1. For all experiments reported in this section we used the syntactic dependency parser MaltParser v1.3 (Nivre 2003, 2008; Kübler, McDonald, and Nivre 2009), a transition-based parser with an input buffer and a stack, which uses SVM classifiers 10 We use the term “dev set” to denote a non-blind test set, used for model development (feature selection and feature engineering). We do not perform further weight optimization (which, if done, is done on a separate “tuning set”). 170 Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features Table 1 Penn Arabic Treebank part 3 v3.1 data split. split # tokens # sentences sentence length (avg. # tokens) training dev unseen test 341,094 31,208"
J13-1008,J08-4003,0,0.666889,"lectional Features results, assignment features, specifically CASE, are very helpful in MSA, though only under gold conditions: Because CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in a machine-predicted (real, non-gold) condition. In contrast with previous results, we show agreement features are quite helpful in both gold and predicted conditions. This is likely a result of MSA having a rich agreement system, covering both verb–subject and noun–adjective relations. The result holds for both the MaltParser (Nivre 2008) and the Easy-First Parser (Goldberg and Elhadad 2010). Additionally, almost all work to date in MSA morphological analysis and part-ofspeech (POS) tagging has concentrated on the morphemic form of the words. Often, however, the functional morphology (which is relevant to agreement, and relates to the meaning of the word) is at odds with the “surface” (form-based) morphology; a well-known example of this are the “broken” (irregular) plurals of nominals. We show that by modeling the functional morphology rather than the form-based morphology, we obtain a further increase in parsing performance"
J13-1008,C08-1081,0,0.0584324,"Missing"
J13-1008,petrov-etal-2012-universal,0,0.0949504,"Missing"
J13-1008,W07-2219,0,0.0658174,"Missing"
J13-1008,P06-1073,0,0.121283,"Missing"
J13-1008,P11-2062,1,\N,Missing
J13-1008,P11-1159,1,\N,Missing
J13-1008,C98-1077,0,\N,Missing
J13-1008,J08-4010,0,\N,Missing
J13-1008,rambow-etal-2006-parallel,1,\N,Missing
J13-1008,N06-1020,0,\N,Missing
L16-1207,W15-3206,1,0.79763,"There are few annotated corpora for dialectal Arabic. We note three corpora in particular: the Levantine Arabic Treebank (specifically Jordanian) (Maamouri et al., 2006), the Egyptian Arabic Treebank (Maamouri et al., 2014) and Curras, the Palestinian Arabic annotated corpus (Jarrar et al., 2014). Additionally, (Tratz et al., 2014) present a corpus of Moroccan dialect which has been annotated for language variety. Our work follows the work of Curras (Jarrar et al., 2014), which consists of around 43,000 words of a balanced genre corpus. The corpus was manually annotated using the DIWAN tool (Al-Shargi and Rambow, 2015), which we also use. The annotation in Curras is done by first using a morphological tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. Since Arabic dialects do not have spelling standards, the team working on Curras followed previous efforts to create conventional orthographies (or CODA) for the dialect they worked on (Habash et al., 2012a; Zribi et al., 2014). We also follow this approach and define CODAs for MOR and YEMS. The effort to annotate corpora in context is a central step in"
L16-1207,bouamor-etal-2014-multidialectal,1,0.855928,"012a; Zribi et al., 2014). We also follow this approach and define CODAs for MOR and YEMS. The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). Other notable approaches and efforts have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multi-dialectal dictionary Tharwa (Diab et al., 2014), multi-dialectal corpora (Bouamor et al., 2014; Smaïli et al., 2014), the Gulf Arabic corpus (Khalifa et al., 2016) or extending MSA analyzers and resources (Salloum and Habash, 2014; Smaïli et al., 2014; Boujelbane et al., 2013). 1300 1 http://www.semarch.uni-hd.de 3. Linguistic Facts Dialectal Arabic poses many challenges for NLP. Arabic in general is a morphologically complex language which includes rich inflectional morphology, expressed both templatically and affixationally, and several classes of attachable clitics. For example, the Moroccan Arabic (MOR) word AëñJ.JºJ «ð w+γa+y-ktb-uw+hA2 ‘and they will write it’ has two proclitics"
L16-1207,I13-1048,0,0.0384164,"analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). Other notable approaches and efforts have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multi-dialectal dictionary Tharwa (Diab et al., 2014), multi-dialectal corpora (Bouamor et al., 2014; Smaïli et al., 2014), the Gulf Arabic corpus (Khalifa et al., 2016) or extending MSA analyzers and resources (Salloum and Habash, 2014; Smaïli et al., 2014; Boujelbane et al., 2013). 1300 1 http://www.semarch.uni-hd.de 3. Linguistic Facts Dialectal Arabic poses many challenges for NLP. Arabic in general is a morphologically complex language which includes rich inflectional morphology, expressed both templatically and affixationally, and several classes of attachable clitics. For example, the Moroccan Arabic (MOR) word AëñJ.JºJ «ð w+γa+y-ktb-uw+hA2 ‘and they will write it’ has two proclitics (+ ð w+ ‘and’ and +« ga+ ‘will’), one prefix -K y- ‘3rd person masculine imperfective’, one suffix ð- -uw ‘plural’ and one pronominal enclitic Aë+ +hA ‘it/her’. In Sanaani Yemeni Ara"
L16-1207,diab-etal-2014-tharwa,1,0.887158,"the dialect they worked on (Habash et al., 2012a; Zribi et al., 2014). We also follow this approach and define CODAs for MOR and YEMS. The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). Other notable approaches and efforts have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multi-dialectal dictionary Tharwa (Diab et al., 2014), multi-dialectal corpora (Bouamor et al., 2014; Smaïli et al., 2014), the Gulf Arabic corpus (Khalifa et al., 2016) or extending MSA analyzers and resources (Salloum and Habash, 2014; Smaïli et al., 2014; Boujelbane et al., 2013). 1300 1 http://www.semarch.uni-hd.de 3. Linguistic Facts Dialectal Arabic poses many challenges for NLP. Arabic in general is a morphologically complex language which includes rich inflectional morphology, expressed both templatically and affixationally, and several classes of attachable clitics. For example, the Moroccan Arabic (MOR) word AëñJ.JºJ «ð w+γa+y-ktb-uw+"
L16-1207,D13-1105,1,0.85685,"done by first using a morphological tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. Since Arabic dialects do not have spelling standards, the team working on Curras followed previous efforts to create conventional orthographies (or CODA) for the dialect they worked on (Habash et al., 2012a; Zribi et al., 2014). We also follow this approach and define CODAs for MOR and YEMS. The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). Other notable approaches and efforts have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multi-dialectal dictionary Tharwa (Diab et al., 2014), multi-dialectal corpora (Bouamor et al., 2014; Smaïli et al., 2014), the Gulf Arabic corpus (Khalifa et al., 2016) or extending MSA analyzers and resources (Salloum and Habash, 2014; Smaïli et al., 2014; Boujelbane et al., 2013). 1300 1 http://www."
L16-1207,habash-etal-2012-conventional,1,0.910331,"rrar et al., 2014), which consists of around 43,000 words of a balanced genre corpus. The corpus was manually annotated using the DIWAN tool (Al-Shargi and Rambow, 2015), which we also use. The annotation in Curras is done by first using a morphological tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. Since Arabic dialects do not have spelling standards, the team working on Curras followed previous efforts to create conventional orthographies (or CODA) for the dialect they worked on (Habash et al., 2012a; Zribi et al., 2014). We also follow this approach and define CODAs for MOR and YEMS. The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). Other notable approaches and efforts have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multi-dialectal dictionary Tharwa (Diab et al., 2014), multi-dialectal corpora (Bou"
L16-1207,W12-2301,1,0.945635,"rrar et al., 2014), which consists of around 43,000 words of a balanced genre corpus. The corpus was manually annotated using the DIWAN tool (Al-Shargi and Rambow, 2015), which we also use. The annotation in Curras is done by first using a morphological tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. Since Arabic dialects do not have spelling standards, the team working on Curras followed previous efforts to create conventional orthographies (or CODA) for the dialect they worked on (Habash et al., 2012a; Zribi et al., 2014). We also follow this approach and define CODAs for MOR and YEMS. The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). Other notable approaches and efforts have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multi-dialectal dictionary Tharwa (Diab et al., 2014), multi-dialectal corpora (Bou"
L16-1207,N13-1044,1,0.843093,"orphological tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. Since Arabic dialects do not have spelling standards, the team working on Curras followed previous efforts to create conventional orthographies (or CODA) for the dialect they worked on (Habash et al., 2012a; Zribi et al., 2014). We also follow this approach and define CODAs for MOR and YEMS. The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). Other notable approaches and efforts have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multi-dialectal dictionary Tharwa (Diab et al., 2014), multi-dialectal corpora (Bouamor et al., 2014; Smaïli et al., 2014), the Gulf Arabic corpus (Khalifa et al., 2016) or extending MSA analyzers and resources (Salloum and Habash, 2014; Smaïli et al., 2014; Boujelbane et al., 2013). 1300 1 http://www.semarch.uni-hd.de 3. L"
L16-1207,W14-3603,1,0.854216,"Missing"
L16-1207,L16-1679,1,0.503005,"ODAs for MOR and YEMS. The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). Other notable approaches and efforts have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multi-dialectal dictionary Tharwa (Diab et al., 2014), multi-dialectal corpora (Bouamor et al., 2014; Smaïli et al., 2014), the Gulf Arabic corpus (Khalifa et al., 2016) or extending MSA analyzers and resources (Salloum and Habash, 2014; Smaïli et al., 2014; Boujelbane et al., 2013). 1300 1 http://www.semarch.uni-hd.de 3. Linguistic Facts Dialectal Arabic poses many challenges for NLP. Arabic in general is a morphologically complex language which includes rich inflectional morphology, expressed both templatically and affixationally, and several classes of attachable clitics. For example, the Moroccan Arabic (MOR) word AëñJ.JºJ «ð w+γa+y-ktb-uw+hA2 ‘and they will write it’ has two proclitics (+ ð w+ ‘and’ and +« ga+ ‘will’), one prefix -K y- ‘3rd person mascu"
L16-1207,maamouri-etal-2006-developing,1,0.865078,"; Naïm-Sanbar, 1994; Al-Iryani, 1996; Behnstedt, 2006). There have been several data collections centered on Arabic dialects, specifically spoken Arabic. A very useful resource is the Semitisches Tonarchiv at the University of Heidelberg in Germany1 under the direction of Prof. Werner Arnold. We have included one Yemeni transcription from this resource in our Yemeni corpus (see Section 4.). Further data collections include (al Salam Al-Amri, 2000). There are few annotated corpora for dialectal Arabic. We note three corpora in particular: the Levantine Arabic Treebank (specifically Jordanian) (Maamouri et al., 2006), the Egyptian Arabic Treebank (Maamouri et al., 2014) and Curras, the Palestinian Arabic annotated corpus (Jarrar et al., 2014). Additionally, (Tratz et al., 2014) present a corpus of Moroccan dialect which has been annotated for language variety. Our work follows the work of Curras (Jarrar et al., 2014), which consists of around 43,000 words of a balanced genre corpus. The corpus was manually annotated using the DIWAN tool (Al-Shargi and Rambow, 2015), which we also use. The annotation in Curras is done by first using a morphological tagger for another Arabic dialect, namely MADAMIRA Egyptia"
L16-1207,maamouri-etal-2014-developing,1,0.879161,". There have been several data collections centered on Arabic dialects, specifically spoken Arabic. A very useful resource is the Semitisches Tonarchiv at the University of Heidelberg in Germany1 under the direction of Prof. Werner Arnold. We have included one Yemeni transcription from this resource in our Yemeni corpus (see Section 4.). Further data collections include (al Salam Al-Amri, 2000). There are few annotated corpora for dialectal Arabic. We note three corpora in particular: the Levantine Arabic Treebank (specifically Jordanian) (Maamouri et al., 2006), the Egyptian Arabic Treebank (Maamouri et al., 2014) and Curras, the Palestinian Arabic annotated corpus (Jarrar et al., 2014). Additionally, (Tratz et al., 2014) present a corpus of Moroccan dialect which has been annotated for language variety. Our work follows the work of Curras (Jarrar et al., 2014), which consists of around 43,000 words of a balanced genre corpus. The corpus was manually annotated using the DIWAN tool (Al-Shargi and Rambow, 2015), which we also use. The annotation in Curras is done by first using a morphological tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was the"
L16-1207,pasha-etal-2014-madamira,1,0.929033,"Missing"
L16-1207,zribi-etal-2014-conventional,1,0.858671,"ich consists of around 43,000 words of a balanced genre corpus. The corpus was manually annotated using the DIWAN tool (Al-Shargi and Rambow, 2015), which we also use. The annotation in Curras is done by first using a morphological tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. Since Arabic dialects do not have spelling standards, the team working on Curras followed previous efforts to create conventional orthographies (or CODA) for the dialect they worked on (Habash et al., 2012a; Zribi et al., 2014). We also follow this approach and define CODAs for MOR and YEMS. The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). Other notable approaches and efforts have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multi-dialectal dictionary Tharwa (Diab et al., 2014), multi-dialectal corpora (Bouamor et al., 2014; Sma"
L16-1322,W11-0707,0,0.562174,"ed Work There is a wide array of computational studies analyzing the dynamics of the collaborative editing process of building Wikipedia. One line of work focuses mainly on meta information such as history of edits, deletes, reverts, and dispute tags (e.g., (Vuong et al., 2008; Rad and Barbosa, 2012; Jurgens and Lu, 2012)), whereas others analyze the interaction dynamics exhibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, wher"
L16-1322,W12-2105,1,0.838066,"hibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, whereas we present a large general purpose corpus that captures broader aspects of interactions and their participants. 3. Corpus In this section, we present our WikiTalk corpus, describe its construction process, and discuss the format in which the discussions are represented in it. 3.1. Data source: Discussion Threads Our starting point is the list of controversial issues i"
L16-1322,E12-1079,0,0.0223564,"regarding online social interactions. 2. Related Work There is a wide array of computational studies analyzing the dynamics of the collaborative editing process of building Wikipedia. One line of work focuses mainly on meta information such as history of edits, deletes, reverts, and dispute tags (e.g., (Vuong et al., 2008; Rad and Barbosa, 2012; Jurgens and Lu, 2012)), whereas others analyze the interaction dynamics exhibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for"
L16-1322,P13-1162,0,0.0198836,"e of work focuses mainly on meta information such as history of edits, deletes, reverts, and dispute tags (e.g., (Vuong et al., 2008; Rad and Barbosa, 2012; Jurgens and Lu, 2012)), whereas others analyze the interaction dynamics exhibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, whereas we present a large general purpose corpus that captures broader aspects of interactions and their participants. 3. Corpus In this section, we p"
L16-1322,C12-1155,0,0.030014,". At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, whereas we present a large general purpose corpus that captures broader aspects of interactions and their participants. 3. Corpus In this section, we present our WikiTalk corpus, describe its construction process, and discuss the format in which the discussions are represented in it. 3.1. Data source: Discussion Threads Our starting point is the list of controversial issues in Wikipedia that is collaboratively compiled by Wikipedia"
L16-1322,W14-2617,0,0.216954,"of the collaborative editing process of building Wikipedia. One line of work focuses mainly on meta information such as history of edits, deletes, reverts, and dispute tags (e.g., (Vuong et al., 2008; Rad and Barbosa, 2012; Jurgens and Lu, 2012)), whereas others analyze the interaction dynamics exhibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, whereas we present a large general purpose corpus that captures broader aspects of"
L16-1322,P14-2113,0,0.239322,"of the collaborative editing process of building Wikipedia. One line of work focuses mainly on meta information such as history of edits, deletes, reverts, and dispute tags (e.g., (Vuong et al., 2008; Rad and Barbosa, 2012; Jurgens and Lu, 2012)), whereas others analyze the interaction dynamics exhibited by the editors in the Wikipedia Talk pages. At the level of modeling the language and structure of these interactions, researchers have attempted to assign dialog acts (Ferschke et al., 2012), to assign social acts (Bender et al., 2011), and to identify agreements, disagreements and disputes (Wang and Cardie, 2014b; Wang and Cardie, 2014a) as well as biases (Recasens et al., 2013) in these interactions. There is also work connecting the linguistic patterns to the social context of these interactions, such as power (Danescu-Niculescu-Mizil et al., 2012), influence and pursuit of power (Biran et al., 2012; Swayamdipta and Rambow, 2012; Strzalkowski et al., 2012; Nguyen et al., 2013), and social roles (Ferschke et al., 2015). Most of these studies use data collected specifically for the research questions they investigate, whereas we present a large general purpose corpus that captures broader aspects of"
L16-1640,K15-1005,1,0.473735,"and SeparatePunc commands. TagSounds detects sound patterns in an input String (e.g., hmmm, ahh, etc.). 5. Case Study As we are a specialized group in Arabic processing with many publicly available projects, the need for having a consistent preprocessing behavior across our projects is a must. Though, it is not necessary to have the same preprocessing pipeline for all projects, having the same implementation of the shared steps is crucial. Figures 5, 6, and 7 show the preprocessing pipelines of our publicly available tools for Arabic; AIDA for Arabic dialect identification and classification (Al-Badrashiny et al., 2015), MADAMIRA for morphological analysis and disambiguation of Egyptian and modern standard Arabic text (Pasha et al., 2014), and 3ARRIB, for converting dialectal Arabic written in Latin characters in social media to normalized Arabic orthography (Al-Badrashiny et al., 2014) and (Eskander et al., 2014). SPLIT has maintained the same system performance for these tools, but it significantly simplified the code by separating the text preprocessing part from the core engines. This enabled us to simply try different preprocessing schemes in a streamlined manner expediting the turn around for the exper"
L16-1640,althobaiti-etal-2014-aranlp,0,0.0202847,"nts to segment text into paragraphs, sentences, words and other kinds of tokens. GetItFull is a tool for downloading and preprocessing full-text journals. It performs various commonly used preprocessing steps and puts the output in a structured XML document for each article with tags identifying the various sections and journal information articles (Natarajan et al., 2006). AraNLP is a preprocessing tool developed specifically for preprocessing Arabic texts. It includes a sentence detector, tokenizer, light stemmer, root stemmer, part-of-speech tagger, and a punctuation and diacritic remover (Althobaiti et al., 2014). In this paper, we introduce SPLIT, the Smart Preprocessing (Quasi) Language Independent Tool. By language independence we mean that the tool is built in allows it to be as flexible as possible and not be restricted to a certain language. The tool consists of a list of commands. Where each of them performs only one task. The users can then build their own preprocessing pipeline using a simple configuration file. This list of commands can easily be expanded just by adding a file that contains the code of the new preprocessing task to the source directory of the tool. SPLIT is then able to use"
L16-1640,W14-3901,1,0.84161,"is not necessary to have the same preprocessing pipeline for all projects, having the same implementation of the shared steps is crucial. Figures 5, 6, and 7 show the preprocessing pipelines of our publicly available tools for Arabic; AIDA for Arabic dialect identification and classification (Al-Badrashiny et al., 2015), MADAMIRA for morphological analysis and disambiguation of Egyptian and modern standard Arabic text (Pasha et al., 2014), and 3ARRIB, for converting dialectal Arabic written in Latin characters in social media to normalized Arabic orthography (Al-Badrashiny et al., 2014) and (Eskander et al., 2014). SPLIT has maintained the same system performance for these tools, but it significantly simplified the code by separating the text preprocessing part from the core engines. This enabled us to simply try different preprocessing schemes in a streamlined manner expediting the turn around for the experimental investigations. 4058 Figure 7: 3ARRIB preprocessing pipeline Figure 5: AIDA preprocessing pipeline Figure 6: MADAMIRA preprocessing pipeline 6. Conclusions In this paper, we introduced version 1.01 of SPLIT. The tool provides the most preprocessing tasks that are needed to clean and prepare"
L16-1640,grover-etal-2000-lt,0,0.0782072,"tivate the need for a simple standard preprocessing framework that has a unified implementation of the most important preprocessing steps taking into consideration the different behaviors of various languages and genres. This enables the researcher to focus more on the research point. Some attempts toward this objective have been introduced. PRETO is a preprocessing tool developed specifically for preprocessing Turkish texts only (Tunali and Bilgin, 2012). JPreText is another tool that focuses on stemming, stopword removal, and term weighting (TF/IDF) for English text (Nogueira et al., 2008). Grover et al. (2000) introduced a tool called “A Flexible Tokenisation Tool” that includes ready-made components to segment text into paragraphs, sentences, words and other kinds of tokens. GetItFull is a tool for downloading and preprocessing full-text journals. It performs various commonly used preprocessing steps and puts the output in a structured XML document for each article with tags identifying the various sections and journal information articles (Natarajan et al., 2006). AraNLP is a preprocessing tool developed specifically for preprocessing Arabic texts. It includes a sentence detector, tokenizer, ligh"
L16-1640,pasha-etal-2014-madamira,1,0.830646,"specialized group in Arabic processing with many publicly available projects, the need for having a consistent preprocessing behavior across our projects is a must. Though, it is not necessary to have the same preprocessing pipeline for all projects, having the same implementation of the shared steps is crucial. Figures 5, 6, and 7 show the preprocessing pipelines of our publicly available tools for Arabic; AIDA for Arabic dialect identification and classification (Al-Badrashiny et al., 2015), MADAMIRA for morphological analysis and disambiguation of Egyptian and modern standard Arabic text (Pasha et al., 2014), and 3ARRIB, for converting dialectal Arabic written in Latin characters in social media to normalized Arabic orthography (Al-Badrashiny et al., 2014) and (Eskander et al., 2014). SPLIT has maintained the same system performance for these tools, but it significantly simplified the code by separating the text preprocessing part from the core engines. This enabled us to simply try different preprocessing schemes in a streamlined manner expediting the turn around for the experimental investigations. 4058 Figure 7: 3ARRIB preprocessing pipeline Figure 5: AIDA preprocessing pipeline Figure 6: MADA"
L18-1535,W14-1604,1,0.907171,"t. • Think of more than one translation into his/her dialect and carefully specify the city. • Use external informants to get more information for cities in his/her area if it is not his original city. • Enter the CODA and CAPHI versions of each entry, using the guidelines provided. Very recently, automatic DA processing has attracted a considerable amount of research in NLP (Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/ma"
L18-1535,L16-1207,1,0.824624,"lectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but a native would not prod"
L18-1535,bouamor-etal-2014-multidialectal,1,0.953924,"cities has 12,000 sentences that are five-way parallel translations, and that could be used to build several Dialectal Arabic NLP applications such as machine translation. An example of a 28-way parallel sentences extracted from C ORPUS -25 is given in Figure 1.5 Translators, identified from each of the 25 cities specifically, were asked to read a set of sentences provided in English or French, and translate them into their dialects. The translators are all native speakers of the dialects of the cities they hail from. We did not choose MSA as a starting point to avoid biasing the translation (Bouamor et al., 2014). 6 4 The English, French and MSA versions we use are those provided in the IWSLT evaluation campaign (Eck and Hori, 2005). 5 The MADAR Corpus is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 6 The translation was handled by Ramitechs (http://www. ramitechs.com/), a company that creates and annotates several types of corpora and lexicons using expert linguists. 3388 English French MSA Beirut Cairo Doha Rabat Tunis Aleppo Alexandria Algiers Amman Aswan Baghdad Basra Benghazi Damascus Fes Jeddah Jerusalem Khartoum Mosul Muscat This room is too small. Cette chambre est trop pe"
L18-1535,cotterell-callison-burch-2014-multi,0,0.122726,"ri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions tha"
L18-1535,W14-3629,0,0.0380058,"these differences. Phonology An example of phonological differences is in the pronunciation of dialectal words whose MSA cog nate has the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (AS"
L18-1535,diab-etal-2014-tharwa,1,0.957896,"itten language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-way lexicon of 1,045"
L18-1535,habash-etal-2012-conventional,1,0.901479,"the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq"
L18-1535,W12-2301,1,0.897253,"the letter Qaf (  q).2 It is often observed that in Tunisian Arabic, this consonant appears as /q/ (similar to MSA), while in Egyptian and Levantine Arabic it is /P/ (glottal stop) and in Gulf Arabic it is /g/ (Haeri, 1991; Habash, 2010). Orthography While MSA has a standard orthography, the dialects do not. Often people write words reflecting the phonology or the etymology of these words. DA is sometimes written in the so-called Arabizi Romanization script (Darwish, 2014). In the context of NLP, a set of conventional orthography guidelines (CODA) has been proposed for a number of dialects (Habash et al., 2012a; Jar2 Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007): (in alphabetical orˇ der) AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional sym   ˇ ¯ ˆ ð', yˆ ø , ¯ bols: ’ Z, Â @, A @, A @, w h è, ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq"
L18-1535,L18-1574,1,0.896503,"I) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Syntax Comparative studies of several Arabic dialects suggest that the syntactic differences between the dialects are relat"
L18-1535,W14-3603,1,0.936447,"Missing"
L18-1535,W14-3627,1,0.896251,"and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but"
L18-1535,L16-1679,1,0.93096,"Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Syntax Comparative studies of several Arabic di"
L18-1535,Y15-1004,0,0.20582,"h of their coverage and the fine location granularity. The focus on cities, as opposed to regions in studying Arabic dialects, opens new avenues to many areas of research from dialectology to dialect identification and machine translation. Keywords: Arabic Dialects, Parallel Corpus, Lexicon 1. Introduction 2. Dialectal Arabic (DA) is emerging nowadays as the primary written language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified fra"
L18-1535,pasha-etal-2014-madamira,1,0.875164,"les from the BTEC parallel corpus. Tuples are then clustered based on their semantic similarity, such that each cluster represents a concept. The automatic process is followed by manual validation and fixing of errors resulting from the automatic process. 4.2.1. Automatic Extraction of Concept Keys Data Preprocessing Since the concept triplet words are represented in terms of lemmas, we pre-process the parallel data to map it into the lemma space. For English, we use the Stanford POS tagger (Toutanova et al., 2003) and for French, we use Treetagger (Schmid, 1994). For Arabic, we use MADAMIRA (Pasha et al., 2014) to tokenize words into the D3 scheme, which separates all clitics from the basewords. Arabic tokenization is required as the clitics attached to basewords in Arabic, are typically represented as separate words in English and French. The most common examples are the proclitic definite article + È@ Al+ ‘the’, and the enclitic possessive pronouns, such as è+ +h ‘his’. The goal here is to harmonize the forms of the three languages to encourage better word alignment and concept extraction. Triplet Extraction Our trilingual concept extraction approach focuses on collecting frequently used triplets."
L18-1535,W15-3208,1,0.95203,"hreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK AK nktb (Tunisian). Synt"
L18-1535,P13-2001,0,0.0207968,"nformal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-way lexicon of 1,045 entries in each city’s"
L18-1535,salama-etal-2014-youdacc,1,0.898896,"developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted i"
L18-1535,W15-3205,0,0.305387,"ponsible for. • Delete all entries that are NOT relevant to the cities he/she is responsible for. • Apply the necessary changes for some entries that may need some minor fixes. • Add new words that are not on the AUTO list. • Think of more than one translation into his/her dialect and carefully specify the city. • Use external informants to get more information for cities in his/her area if it is not his original city. • Enter the CODA and CAPHI versions of each entry, using the guidelines provided. Very recently, automatic DA processing has attracted a considerable amount of research in NLP (Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell"
L18-1535,N03-1033,0,0.0111141,"t key identification relies on an automatic process that extracts (English, French, Arabic) related tuples from the BTEC parallel corpus. Tuples are then clustered based on their semantic similarity, such that each cluster represents a concept. The automatic process is followed by manual validation and fixing of errors resulting from the automatic process. 4.2.1. Automatic Extraction of Concept Keys Data Preprocessing Since the concept triplet words are represented in terms of lemmas, we pre-process the parallel data to map it into the lemma space. For English, we use the Stanford POS tagger (Toutanova et al., 2003) and for French, we use Treetagger (Schmid, 1994). For Arabic, we use MADAMIRA (Pasha et al., 2014) to tokenize words into the D3 scheme, which separates all clitics from the basewords. Arabic tokenization is required as the clitics attached to basewords in Arabic, are typically represented as separate words in English and French. The most common examples are the proclitic definite article + È@ Al+ ‘the’, and the enclitic possessive pronouns, such as è+ +h ‘his’. The goal here is to harmonize the forms of the three languages to encourage better word alignment and concept extraction. Triplet Ex"
L18-1535,L18-1111,1,0.771723,"ons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the use of some Egyptian expressions that might be accepted in other dialects, but a native would not produce naturally. The same conce"
L18-1535,P11-2007,0,0.621852,"erging nowadays as the primary written language of informal communication online in the Arab World: in emails, blogs, discussion forums, chats, SMS, etc. There has been a rising interest in research on computational models of Arabic dialects in the last decade (Meftouh et al., 2015). There have been several efforts on creating different resources to allow building models for several Natural Language Processing (NLP) applications. However, these efforts have been disjoint from each other, and most of them have focused on a small number of dialects that represent vast regions of the Arab World (Zaidan and Callison-Burch, 2011; Diab et al., 2014; Sajjad et al., 2013). In this paper, we present two resources we created as part of the Multi Arabic Dialect Applications and Resources (MADAR) project.1 The goal of MADAR is to create, for a large number of dialects, a unified framework with common annotation guidelines and decisions, and targeting applications of Dialect Identification (DID) and Machine Translation (MT). The first resource is a large parallel corpus of 25 Arabic city dialects, in addition to the pre-existing parallel set for English, French and Modern Standard Arabic (MSA). The second resource is a 25-wa"
L18-1535,N12-1006,0,0.0438526,"(Shoufan and Alameri, 2015), facilitated by the newly developed monolingual and multilingual dialectal corpora and lexicons. Several mono-dialectal corpora covering different Arabic dialects were built and made available. Al-Badrashiny et al. (2014) compiled a large dialect-identified corpus of DA from several Egyptian sources, but with a large presence of MSA. In a related effort, McNeil and Faiza (2011) built a four-million-word corpus of Tunisian Spoken Arabic. Various other research work resulted in multidialectal non parallel corpora at different scales (Zaidan and Callison-Burch, 2011; Zbib et al., 2012; Cotterell and Callison-Burch, 2014; Salama et al., 2014; Jeblee et al., 2014; Al-Shargi et al., 2016; Zaghouani and Charfi, 2018). 10 The latest version of the lexicon is available for browsing online at http://nlp.qatar.cmu.edu/madar/. 3393 As for dialect-to-dialect parallel corpora, Bouamor et al. (2014) presented the first small-scale 7-way parallel corpus covering several dialects in addition to MSA, and English, all translated from Egyptian sentences. The fact that Egyptian was chosen as a starting point affected the quality of the translation. The sentences produced were biased by the"
L18-1535,zribi-etal-2014-conventional,1,0.960805,"ý ø. 3387 Region Maghreb Sub-region Morocco Algeria Tunisia Cities Rabat Algiers Tunis (RAB) (ALG) (TUN) Fes Sfax (FES) (SFX) Libya Tripoli (TRI) Benghazi (BEN) Nile Basin Egypt/Sudan Cairo (CAI) Alexandria (ALX) Aswan (ASW) Khartoum (KHA) Levant South Levant North Levant Jerusalem Beirut (JER) (BEI) Amman Damascus (AMM) (DAM) Salt Aleppo (SAL) (ALE) Gulf Iraq Gulf Mosul Doha (MOS) (DOH) Baghdad Muscat (BAG) (MUS) Basra Riyadh (BAS) (RIY) Jeddah (JED) Yemen Yemen Sana’a (SAN) Table 1: Different region, sub-region, and city dialects considered in building the MADAR resources. rar et al., 2014; Zribi et al., 2014; Saadane and Habash, 2015; Turki et al., 2016; Khalifa et al., 2016), and has been recently unified under the CODA∗ effort (Habash et al., 2018). Morphology Morphological differences are quite common. One example is the future marker particle which appears as +  s+ or ¬ñ swf in MSA, + hH+ or hP  . bAš rH in Levantine dialects, +ë h+ in Egyptian and AK in Tunisian. This together with variation in the templatic morphology make the forms of some verbs rather different: J» A sÂktb (MSA), IJ» Ag HÂktub e.g., ’I will write’ is I . .   . bAš (Palestinian), I . Jºë hktb (Egyptian) and I.JºK"
L18-1574,C16-2044,1,0.881168,"Missing"
L18-1574,2014.iwslt-papers.1,0,0.119633,"Missing"
L18-1574,P11-2062,1,0.860318,"f MSA patterns will retain the spelling choice of the MSA pattern if the difference in pronunciation can be expressed using diacritics (for vowel change or absence), or if the pronunciation is a shortened form of the MSA pattern vowels. Alif Maqsura The MSA rules for spelling the AlifMaqsura ( ø ý), which are sometimes based on roots and sometimes on patterns, apply in CODA*. 6.4.2. The Ta-Marbuta  The Ta-Marbuta ( è ¯h) is a secondary letter of the Arabic alphabet used to represent a particular suffix morpheme that is often (but not exclusively) associated with the femininesingular feature (Alkuhlani and Habash, 2011). This morpheme has a number of allomorphs with differing pronunciations. Most notably, it appears as a vowel at the end of nominals, and changes to a ∼ /t/ when followed by possessive pronominal enclitics. The Ta-Marbuta should be writ ten as è ¯h in word-final positions, regardless of its pronunciation, and following general CODA rules in non-word-final positions. See Table 3 for example cases. 6.3.3. Clitic Spelling The general rule on phonological clitic spelling is that clitics that are mapped into single letters (with possible diacritics) will be spelled attached to the word, and will n"
L18-1574,W14-3612,1,0.893572,"1 which it can be confused by speakers of dialects which do not have the phoneme [p]. Thus, pQ is included in CAPHI as /p./ as it is useful in describing the dialectal differences between Iraqi and other dialects. The complete CAPHI inventory is listed in Figure 2. 6. CODA* General Rules and Specifications While the goals of the CODA* guidelines is to precisely define the CODA choices, it is unavoidable that different versions of the guidelines will need to be presented differently for specific annotators on specific tasks for specific dialects: e.g., conversion form Arabizi to Arabic script (Bies et al., 2014), or lexicon construction (Diab et al., 2014). In this paper, we summarize and highlight specific contributions of the effort; but the full set of CODA* guidelines is described on its online page (See Section 8.). We start with a description of the technical terminology we use; then we discuss the various rules and how to use them. The border between the general rules and the specification rules is broadly drawn along the lines that general rules do not refer to any specific lexical items (morphemes or words) and pertain to the meta-mechanics of CODA; while the specification rules are lexicall"
L18-1574,L18-1535,1,0.789785,"presented in previous work; (c) the introduction of the concept of a multidialectal Seed Lexicon that is used to allow users of CODA* to have access to previous decisions when identifying spellings for new words in new dialects; and finally, (d) a set of online pages that give users easy public access to all of these resources. The CODA* guidelines and their connected resources are being used by three large Arabic dialect processing projects in three universities: The Multi-Arabic Dialect Applications and Resources at Carnegie Mellon University Qatar and New York University Abu Dhabi (NYUAD) (Bouamor et al., 2018), The Gulf Arabic Annotated Corpus (NYUAD) (Khalifa et al., 2018), and The Columbia Arabic Dialect Annotation project (Columbia University and NYUAD). The CODA* effort is large and ongoing; the goal of this paper is to introduce the effort and some of its important contributions on how to conceptualize and address the question of orthographic decisions in dialectal Arabic computational processing. The rest of the paper is structured as follows. We present common challenges to Arabic processing in Section 2. This is followed by related work in Section 3. We introduce CODA* in Section 4., and di"
L18-1574,diab-etal-2014-tharwa,1,0.959854,"sals such as the Asaakir system (‘Asaakir, 1950) and Akl’s system (Arkadiusz, 2006), neither of which are broadly used today. Various DA dictionaries used Arabic, Latin or mixed script orthographies (Badawi and Hinds, 1986). In the context of NLP, the Linguistic Data Consortium (LDC) guidelines for transcribing Levantine Arabic (Maamouri et al., 2004) and the COLABA project at Columbia University (Diab et al., 2010) were precursors to the work of Habash et al. (2012). After the CODA-Egyptian guidelines were created and used for the creation of Egyptian Arabic resources (Maamouri et al., 2014; Diab et al., 2014; Pasha et al., 2014; Eskander et al., 2013; Al-Badrashiny et al., 2014), two additional sets of guidelines were created for CODATunisian (Zribi et al., 2014) and CODA-Palestinian (Jarrar et al., 2014). These were part of projects involving morphology annotation (Palestinian) or speech recognition (Tunisian). A variant on CODA was proposed for speech recognition by Ali et al. (2014) and was shown to reduce OOV and perplexity. Since then, four more dialects followed: CODA-Algerian (Saadane and Habash, 2015), CODA-Gulf (Khalifa et al., 2016), CODA-Moroccan and CODA-Yemeni (Al-Shargi et al., 2016"
L18-1574,N13-1066,1,0.885238,"Missing"
L18-1574,habash-etal-2012-conventional,1,0.57243,"vary from MSA and from each other in terms of phonology, morphology, lexicon and syntax (Watson, 2007), using MSA orthographic standards cannot fully address the needs of the dialects. As an example of the degree of variety in dialectal spelling, Figure 1. presents the 27 actually attested spellings of one Egyptian Arabic word online. The large number of possibilities results from independent decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Hab"
L18-1574,W14-3603,1,0.933108,"Missing"
L18-1574,L16-1679,1,0.936174,"should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological forms are presented in IPA or in the CAPHI scheme, which is discussed in Section 5. Frequency ≈ 26,000 ≈ 13,000 ≤ 10,000 ≤ 1,000 ≤ 100 ≤ 10 Figure 1: 27 encountered ways"
L18-1574,L18-1607,1,0.954641,"f a multidialectal Seed Lexicon that is used to allow users of CODA* to have access to previous decisions when identifying spellings for new words in new dialects; and finally, (d) a set of online pages that give users easy public access to all of these resources. The CODA* guidelines and their connected resources are being used by three large Arabic dialect processing projects in three universities: The Multi-Arabic Dialect Applications and Resources at Carnegie Mellon University Qatar and New York University Abu Dhabi (NYUAD) (Bouamor et al., 2018), The Gulf Arabic Annotated Corpus (NYUAD) (Khalifa et al., 2018), and The Columbia Arabic Dialect Annotation project (Columbia University and NYUAD). The CODA* effort is large and ongoing; the goal of this paper is to introduce the effort and some of its important contributions on how to conceptualize and address the question of orthographic decisions in dialectal Arabic computational processing. The rest of the paper is structured as follows. We present common challenges to Arabic processing in Section 2. This is followed by related work in Section 3. We introduce CODA* in Section 4., and discuss its components in Section 5. (CAPHI), Section 6. (General R"
L18-1574,maamouri-etal-2014-developing,1,0.931749,"actually attested spellings of one Egyptian Arabic word online. The large number of possibilities results from independent decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological fo"
L18-1574,Y15-1004,0,0.179652,"Missing"
L18-1574,pasha-etal-2014-madamira,1,0.908097,"Missing"
L18-1574,W15-3208,1,0.95202,"t decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological forms are presented in IPA or in the CAPHI scheme, which is discussed in Section 5. Frequency ≈ 26,000 ≈ 13,000 ≤ 10,000 ≤ 1,"
L18-1574,zaghouani-etal-2014-large,1,0.923929,"Missing"
L18-1574,N18-1087,1,0.880283,"Missing"
L18-1574,zribi-etal-2014-conventional,1,0.961603,"ults from independent decisions such as whether the proclitic /ma/ should be written attached or separated (+Ó m+1 or AÓ mA), or whether to write the stem in a way  w), ˆ or etymology (  q). that reflects its phonology (ð' Habash et al. (2012) introduced the concept of Conventional Orthography for Dialectal Arabic (CODA); and they proposed a set of guidelines and exception lists for Egyptian Arabic. Their conventions were used in the Linguistic Data Consortium for annotating Egyptian Arabic (Maamouri et al., 2014). Since then, a number of additional efforts followed suit for other dialects (Zribi et al., 2014; Saadane and Habash, 2015; Jarrar et al., 2016; Khalifa et al., 2016). While the original CODA guidelines aimed at being easy to adjust to new dialects and contained some 1 Arabic script transliteration is presented in the Habash-SoudiBuckwalter transliteration scheme (Habash et al., 2007):   P     ¨ ¨ ¬   È Ð à è ð ø @ H. H H h. h p X XP ˇ ς γ f q k l m n hw y Â b t θ j H x dðr z s š S D T D ˇ ¯  and the additional symbols: ’ Z, Â @, A @, A @, wˆ ð', yˆ Zø', ¯h è, ý ø. Phonological forms are presented in IPA or in the CAPHI scheme, which is discussed in Section 5. Frequency ≈ 26,"
maamouri-etal-2006-developing,E06-1047,1,\N,Missing
maamouri-etal-2006-developing,W04-1602,1,\N,Missing
maamouri-etal-2006-developing,N04-4038,1,\N,Missing
maamouri-etal-2006-developing,P05-1071,1,\N,Missing
N01-1003,W98-1411,0,0.0507918,"Missing"
N01-1003,P95-1018,0,0.0119121,"ining data. In general there may be no corpus available for a new application area, or if there is a corpus available, it is a transcript of human-human dialogs. Human-human dialogs, however, may not provide a very good model of sentence planning strategies for a computational system because the sentence planner must plan communicative goals such as implicit confirmation which are needed to prevent and correct errors in automatic speech recognition but which are rare in human-human dialog. Other related work deals with discourse-related aspects of sentence planning such as cue word placement (Moser and Moore, 1995), clearly a crucial task whose integration into our approach we leave to future work. Mellish et al. (1998) investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations. Using hand-crafted evaluation metrics, they show that a genetic algorithm achieves good results in finding discourse trees. However, they do not address clausecombining, and we do not use hand-crafted metrics. 7 Discussion We have presented SPoT, a trainable sentence planner. SPoT re-conceptualizes the sentence planning task as consisting of"
N01-1003,W00-0306,0,0.114168,"expected interactions (Hovy and Wanner, 1996), and it is difficult to develop new applications quickly. Presumably, this is the reason why dialog systems to date have not used this kind of sentence planning. Most dialog systems today use template-based generation. The template outputs are typically concatenated to produce a turn realizing all the communicative goals. It is hard to achieve high quality output by concatenating the template-based output for individual communicative goals, and templates are difficult to develop and maintain for a mixed-initiative dialog system. For these reasons, Oh and Rudnicky (2000) use e -gram models and Ratnaparkhi (2000), maximum entropy to choose templates, using hand-written rules to score different candidates. But syntactically simplistic approaches may have quality problems, and more importantly, these approaches only deal with inform speech acts. And crucially, these approaches suffer from the need for training data. In general there may be no corpus available for a new application area, or if there is a corpus available, it is a transcript of human-human dialogs. Human-human dialogs, however, may not provide a very good model of sentence planning strategies for"
N01-1003,A92-1006,1,0.740532,"need only to be in a relation of synonymy or hyperonymy (rather than being identical). S OFT- MERGE - GENERAL. Same as M ERGE - GENERAL, except that the verbs need only to be in a relation of synonymy or hyperonymy. C ONJUNCTION. This is standard conjunction with conjunction reduction. R ELATIVE - CLAUSE. This includes participial adjuncts to nouns. A DJECTIVE. This transforms a predicative use of an adjective into an adnominal construction. P ERIOD. Joins two complete clauses with a period. These operations are not domain-specific and are similar to those of previous aggregation components (Rambow and Korelsky, 1992; Shaw, 1998; Danlos, 2000), although the various M ERGE operations are, to our knowledge, novel in this form. The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts Rule M ERGE M ERGE - GENERAL S OFT- MERGE S OFT- MERGE - Sample first argument You are leaving from Newark. What time would you like to leave? You are leaving from Newark Sample second argument You are leaving at 5 You are leaving from Newark. You are going to Dallas C ONJUNCTION What time would you like to leave? You are le"
N01-1003,A00-2026,0,0.0714695,"and it is difficult to develop new applications quickly. Presumably, this is the reason why dialog systems to date have not used this kind of sentence planning. Most dialog systems today use template-based generation. The template outputs are typically concatenated to produce a turn realizing all the communicative goals. It is hard to achieve high quality output by concatenating the template-based output for individual communicative goals, and templates are difficult to develop and maintain for a mixed-initiative dialog system. For these reasons, Oh and Rudnicky (2000) use e -gram models and Ratnaparkhi (2000), maximum entropy to choose templates, using hand-written rules to score different candidates. But syntactically simplistic approaches may have quality problems, and more importantly, these approaches only deal with inform speech acts. And crucially, these approaches suffer from the need for training data. In general there may be no corpus available for a new application area, or if there is a corpus available, it is a transcript of human-human dialogs. Human-human dialogs, however, may not provide a very good model of sentence planning strategies for a computational system because the sentenc"
N01-1003,W98-1415,0,0.092665,"ion of synonymy or hyperonymy (rather than being identical). S OFT- MERGE - GENERAL. Same as M ERGE - GENERAL, except that the verbs need only to be in a relation of synonymy or hyperonymy. C ONJUNCTION. This is standard conjunction with conjunction reduction. R ELATIVE - CLAUSE. This includes participial adjuncts to nouns. A DJECTIVE. This transforms a predicative use of an adjective into an adnominal construction. P ERIOD. Joins two complete clauses with a period. These operations are not domain-specific and are similar to those of previous aggregation components (Rambow and Korelsky, 1992; Shaw, 1998; Danlos, 2000), although the various M ERGE operations are, to our knowledge, novel in this form. The result of applying the operations is a sentence plan tree (or sp-tree for short), which is a binary tree with leaves labeled by all the elementary speech acts Rule M ERGE M ERGE - GENERAL S OFT- MERGE S OFT- MERGE - Sample first argument You are leaving from Newark. What time would you like to leave? You are leaving from Newark Sample second argument You are leaving at 5 You are leaving from Newark. You are going to Dallas C ONJUNCTION What time would you like to leave? You are leaving from N"
N01-1003,P97-1026,0,0.0247795,"Missing"
N01-1003,A97-1039,0,\N,Missing
N01-1003,C98-1114,1,\N,Missing
N01-1003,P98-1118,1,\N,Missing
N04-4027,W01-0719,0,0.0954711,"tain email-specific features can help in identifying relevant sentences for extraction. In addition, in presenting the extracted summary, special “wrappers” ensure that 1 The work reported in this paper was funded under the KDD program. We would like to thank three anonymous reviewers for very insightful and helpful comments. the reader can reconstruct the interactional aspect of the thread, which we assume is crucial for understanding the summary. We acknowledge that other techniques should also be explored for email summarization, but leave that to separate work. 2 Previous and Related Work Muresan et al. (2001) describe work on summarizing individual email messages using machine learning approaches to learn rules for salient noun phrase extraction. In contrast, our work aims at summarizing whole threads and at capturing the interactive nature of email. Nenkova and Bagga (2003) present work on generating extractive summaries of threads in archived discussions. A sentence from the root message and from each response to the root extracted using ad-hoc algorithms crafted by hand. This approach works best when the subject of the root email best describes the “issue” of the thread, and when the root email"
N04-4027,J02-4003,0,0.0500619,"though the content of the message to be summarized is “expanded” using the content from its ancestor messages. The expanded message is passed to a document summarizer which is used as a black box to generate summaries. Our work, in contrast, aims at summarizing the whole thread, and we are precisely interested in changing the summarization algorithm itself, not in using a black box summarizer. In addition, there has been some work on summarizing meetings. As discussed in Section 1, email is different in important respects from (multi-party) dialog. However, some important aspects are related. Zechner (2002), for example, presents a meeting summarization system which uses the MMR algorithm to find sentences that are most similar to the segment and most dissimilar to each other. The similarity weights in the MMR algorithm are modified using three features, including whether a sentence belongs to a question-answer pair. The use of the question-answer pair detection is an interesting proposal that is also applicable to our work. However, overall most of the issues tackled by Zechner (2002) are not relevant to email summarization. 3 The Data Our corpus consists of 96 threads of email sent during one"
N07-1054,W05-0613,0,0.0291209,"henomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of NAACL HLT 2007, pages 428–435, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. Some work in RSR detection has enlisted syntactic analysis as a tool. Marcu and Echihabi (2002) filter training instances based on Part-of-Speech (POS) tags, and Soricut and Marcu (2003) use syntactic features t"
N07-1054,P96-1025,0,0.017207,"Missing"
N07-1054,P03-1071,1,0.82182,"Missing"
N07-1054,W03-1210,0,0.0380148,"of the core phenomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of NAACL HLT 2007, pages 428–435, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. Some work in RSR detection has enlisted syntactic analysis as a tool. Marcu and Echihabi (2002) filter training instances based on Part-of-Speech (POS) tags, and Soricut and Marc"
N07-1054,P02-1047,0,0.9041,"timizing a set of basic parameters such as smoothing weights, vocabulary size and stoplisting. We then focus on improving the quality of the automaticallymined training examples, using topic segmentation and syntactic heuristics to filter out training instances which may be wholly or partially invalid. We find that the parameter optimization and segmentation-based filtering techniques achieve significant improvements in classification performance. We report results of experiments which build and refine models of rhetoricalsemantic relations such as Cause and Contrast. We adopt the approach of Marcu and Echihabi (2002), using a small set of patterns to build relation models, and extend their work by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. Using human-annotated and automatically-extracted test sets, we find that each of these techniques results in improved relation classification accuracy. 2 Related Work 1 Introduction Relations such as Cause and Contrast, which we call rhetorical-semantic relations (RSRs), may be signaled in text by cue phrases like because or however which join clauses or sentences and explicitly express the r"
N07-1054,J96-3006,0,0.0147759,"uthors and do not necessarily reflect the views of DARPA. The first author performed most of the research reported in this paper while at Columbia University. Rhetorical and discourse theory has a long tradition in computational linguistics (Moore and WiemerHastings, 2003). While there are a number of different relation taxonomies (Hobbs, 1979; McKeown, 1985; Mann and Thompson, 1988; Martin, 1992; Knott and Sanders, 1998), many researchers have found that, despite small differences, these theories have wide agreement in terms of the core phenomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of"
N07-1054,N03-1030,0,0.0692976,"t and Sanders, 1998), many researchers have found that, despite small differences, these theories have wide agreement in terms of the core phenomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of NAACL HLT 2007, pages 428–435, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. Some work in RSR detection has enlisted syntactic analysis as a"
N07-1054,W06-1317,0,0.138548,"ferences, these theories have wide agreement in terms of the core phenomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of NAACL HLT 2007, pages 428–435, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. Some work in RSR detection has enlisted syntactic analysis as a tool. Marcu and Echihabi (2002) filter training instances based on Par"
N07-1054,J05-2005,0,0.0111075,"ve found that, despite small differences, these theories have wide agreement in terms of the core phenomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of NAACL HLT 2007, pages 428–435, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. Some work in RSR detection has enlisted syntactic analysis as a tool. Marcu and Echihabi (2002) filter"
N07-1054,C04-1020,0,\N,Missing
N07-1054,W01-1605,0,\N,Missing
N07-1054,N04-1020,0,\N,Missing
N07-2014,A00-2013,0,\N,Missing
N07-2014,H05-1060,0,\N,Missing
N07-2014,W04-1612,0,\N,Missing
N07-2014,P05-1071,1,\N,Missing
N07-2014,P06-1073,0,\N,Missing
N07-2014,P03-1004,0,\N,Missing
N07-2014,gimenez-marquez-2004-svmtool,0,\N,Missing
N09-2047,J99-2004,1,0.793606,"A returns n-best parses for arbitrary n; parse trees are associated with probabilities. A packed forest can also be returned. • MICA is freely available2 , easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency"
N09-2047,W07-2213,1,0.853249,"d by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, several optimizations have been applied, including guiding techniques (Boullier, 2003), extensive static (offline) computations over the grammar, and efficient data structures. Moreover, Earley’s algorithm has been extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge shared parse forest (over 1010 trees) are often generated in a few dozens of milliseconds. Within MICA, the first module that is applied on top of the shared parse forest is S YNTAX’s n-best module. This module adapts and implements the algorithm of (Huang and Chiang, 2005) for efficient n-best trees extraction from a shared parse forest. In practice, and within the current version of MICA, this module is usually used with n = 1, which identifies the optimal tree w.r.t. the probabilistic model embedded in the"
N09-2047,W03-3005,1,0.86506,"et of the symbols are specialized. 4 Parser S YNTAX (Boullier and Deschamp, 1988) is a system used to generate lexical and syntactic analyzers (parsers) (both deterministic and non-deterministic) for all kind of context-free grammars (CFGs) as well as some classes of contextual grammars. It has been under development at INRIA for several decades. S YNTAX handles most classes of deterministic (unambiguous) grammars (LR, LALR, RLR) as well as general context-free grammars. The non-deterministic features include, among others, an Earley-like parser generator used for natural language processing (Boullier, 2003). Like most S YNTAX Earley-like parsers, the architecture of MICA’s PCFG-based parser is the following: • The Earley-like parser proper computes a shared parse forest that represents in a factorized (polynomial) way all possible parse trees according to the underlying (non-probabilistic) CFG that represents the TIG; • Filtering and/or decoration modules are applied on the shared parse forest; in MICA’s case, an nbest module is applied, followed by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, sev"
N09-2047,P04-1014,0,0.0349589,"are associated with probabilities. A packed forest can also be returned. • MICA is freely available2 , easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MIC"
N09-2047,P81-1022,0,0.788449,"processing (Boullier, 2003). Like most S YNTAX Earley-like parsers, the architecture of MICA’s PCFG-based parser is the following: • The Earley-like parser proper computes a shared parse forest that represents in a factorized (polynomial) way all possible parse trees according to the underlying (non-probabilistic) CFG that represents the TIG; • Filtering and/or decoration modules are applied on the shared parse forest; in MICA’s case, an nbest module is applied, followed by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, several optimizations have been applied, including guiding techniques (Boullier, 2003), extensive static (offline) computations over the grammar, and efficient data structures. Moreover, Earley’s algorithm has been extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge shared parse forest (over 101"
N09-2047,W05-1506,0,0.0313205,"een extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge shared parse forest (over 1010 trees) are often generated in a few dozens of milliseconds. Within MICA, the first module that is applied on top of the shared parse forest is S YNTAX’s n-best module. This module adapts and implements the algorithm of (Huang and Chiang, 2005) for efficient n-best trees extraction from a shared parse forest. In practice, and within the current version of MICA, this module is usually used with n = 1, which identifies the optimal tree w.r.t. the probabilistic model embedded in the original PCFG; other values can also be used. Once the n-best trees have been extracted, the dependency extractor module transforms each of these trees into a dependency tree, by exploiting the fact that the CFG used for parsing has been built from a TIG. 5 Evaluation We compare MICA to the MALT parser. Both parsers are trained on sections 02-21 of our depe"
N09-2047,P04-1042,0,0.0251422,"nd we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MICA’s, so that direct comparison is difficult. For comparison purposes, we therefore use the MALT parser generator (Nivre et al., 2004), which allows us to train a dependency parser on our own dependency structures. MALT has been among the top performers in the CoNLL dependency parsing competitions. 2 Supertags and Supertagging Supertags are elementary trees of a lexicalized tree grammar such as a Tree-Adjoining Grammar (TAG) (Joshi, 1987)"
N09-2047,C94-1079,0,0.0973378,"rk of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MICA’s, so that direct comparison is difficult. For comparison purposes, we therefore use the MALT parser generator (Nivre et al., 2004), which allows us to train a dependency parser on our own dependency structures. MALT has been among the top performers in the CoNLL dependency parsing competitions. 2 Supertags and Supertagging Supertags are elementary tree"
N09-2047,W04-2407,0,0.102365,"Missing"
N09-2047,J94-1004,0,0.118342,"es anchored by Xl from the symbol Xl∗ . No adjunction, the first adjunction, and the second adjunction are modeled explicitly in the grammar and the associated probabilistic model, while the third and all subsequent adjunctions are modeled together. This conversion method is basically the same as that presented in (Schabes and Waters, 1995), except that our PCFG models multiple adjunctions at the same node by positions (a concern Schabes and Waters (1995) do not share, of course). Our PCFG construction differs from that of Hwa (2001) in that she does not allow multiple adjunction at one node (Schabes and Shieber, 1994) (which we do since we are interested in the derivation structure as a representation of linguistic dependency). For more information about the positional model of adjunction and a discussion of an alternate model, the “bigram model”, see (Nasr and Rambow, 2006). Tree tdi from Section 2 gives rise to the following rule (where tdi and tCO are terminal symbols and the rest are nonterminals): S → S∗l NP VP∗l Vl∗ tdi Vr∗ NP PP∗l P∗l tCO P∗r NP PP∗r VP∗r S∗r The probabilities of the PCFG rules are estimated using maximum likelihood. The probabilistic model refers only to supertag names, not to word"
N09-2047,H05-1102,0,0.0299565,"which derives the syntactic structure from the n-best chosen supertags. Only the supertagger uses lexical information, the parser only sees the supertag hypotheses. • MICA returns n-best parses for arbitrary n; parse trees are associated with probabilities. A packed forest can also be returned. • MICA is freely available2 , easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a dec"
N09-2047,W04-0307,0,0.115845,"Missing"
N10-1049,E91-1005,1,0.675538,"re in the tree. Empty pronouns are widely used in both DT- and PST-based representations. While most DT-based approaches do not use traces, Lombardo and Lesmo (1998) do; and while traces are commonly found in PST-based approaches, there are many that do not use them, such as the c-structure of LFG. Discontinuous Constituents or Non-Projectivity. Both types of trees can be used with or without discontinuous constituents; PSTs are more likely to use traces to avoid discontinuous constituents, but linguistic proposals for PSTs with discontinuous constituents have been made (work by McCawley, or (Becker et al., 1991)). Labeled Arcs. In DTs, arcs often have labels; arcs in PSTs usually do not, but we can of course label PST arcs as well, as is done in the German TIGER corpus.I note that in both DTs and PSTs we can represent the arc label as a feature on the daughter node, or as a separate node. 3 Syntactic Content While there is lots of disagreement about the proper representation type for syntax, there is actually a broad consensus among theoretical and descriptive syntacticians of all persuasions about the range of syntactic phenomena that exist. What exactly is this content, then? It is not a theory-neu"
N10-1049,W09-3036,1,0.733627,"Missing"
N10-1049,P98-2130,0,0.120801,"y (Sgall et al., 1986) uses unordered DTs at the deeper level of representation and ordered DTs at a more surfacy level. GPSG (Gazdar et al., 1985) uses unordered trees (or at any rate context-free rules whose righthand side is ordered by a separate component of the grammar), as does current Chomskyan theory (the PST at spell-out may be unordered). Empty categories. Empty categories can be empty pronouns, or traces, which are co-indexed with a word elsewhere in the tree. Empty pronouns are widely used in both DT- and PST-based representations. While most DT-based approaches do not use traces, Lombardo and Lesmo (1998) do; and while traces are commonly found in PST-based approaches, there are many that do not use them, such as the c-structure of LFG. Discontinuous Constituents or Non-Projectivity. Both types of trees can be used with or without discontinuous constituents; PSTs are more likely to use traces to avoid discontinuous constituents, but linguistic proposals for PSTs with discontinuous constituents have been made (work by McCawley, or (Becker et al., 1991)). Labeled Arcs. In DTs, arcs often have labels; arcs in PSTs usually do not, but we can of course label PST arcs as well, as is done in the Germ"
N10-1049,H94-1020,0,0.0864131,"herent representation” means that the different choices made for conceptually independent content are also representationally independent, so that we can compose representational choices. Note that a theory can decide to omit some content; for example, we can have a theory which does not distinguish raising from control (the English PTB does not). There are different types of syntactic theories. A descriptive theory is an account of the syntax of one language. Examples of descriptive grammars include works such as Quirk for English, or the annotation manuals of monolingual treebanks, such as (Marcus et al., 1994; Maamouri et al., 2003). The annotation manual serves two purposes: it tells the annotators how to represent a syntactic phenomenon, and it tells the users of the treebank (us!) how to interpret the annotation. A treebank without manual is meaningless. And an arborescent structure does not mean the same thing in all treebanks (for example, a “flat NP” indicates an unannotated constituent in the English ATB but a fully annotated construction in the Arabic Treebank is). An explanatory theory is a theory which attempts to account for the syntax of all languages, for example by reducing their div"
N10-1049,H01-1014,0,0.0417577,"f the two representations are independently devised and both are linguistically motivated, then we have no reason to believe that the conversion can be done using a specific simple approach, or using conversion rules which have some fixed property (say, the depth of the trees in the rules templates). In the general case, the only way to write an automatic converter between two representations is to study the two annotation manuals and to create a case-by-case converter, covering all linguistic phenomena represented in the target representation. Machine learning-based conversion (for example, (Xia and Palmer, 2001)) is an interesting exercise, but it does not give us any general insights into dependency or phrase structure. Suppose the source contains all the information that the target should contain. Then if machine learning-based conversion fails or does not perform completely correctly, the exercise merely shows that the machine learning is not adequate. Now suppose that the source does not contain all the information that the target should contain. Then no fancy machine learning can ever provide a completely correct conversion. Also, note that unlike, for example, parsers which are based on machine"
N10-1049,C98-2125,0,\N,Missing
N12-1057,P11-1078,0,0.123661,"in language use (e.g., (O’Barr, 1982)). Locher (2004) recognizes “restriction of an interactant’s action-environment” (Wartenberg, 1990) as a key element by which exercise of power in interactions can be identified. Through ODP we capture this action-restriction at an utterance level. In the computational field, several studies have used Social Network Analysis (e.g., (Diesner and Carley, 2005)) for extracting social relations from online communication. Only recently have researchers started using NLP to analyze the content of messages to deduce social relations (e.g., (Diehl et al., 2007)). Bramsen et al. (2011) use knowledge of the actual organizational structure to create two sets of messages: messages sent from a superior to a subordinate, and vice versa. Their task is to determine the direction of power (since all their data, by construction of the corpus, has a power relationship). Their reported results cannot be directly compared with ours since their results are on classifying aggregations of messages as being to a superior or to a subordinate, whereas our results are on predicting whether a single utterance has an ODP or not. 518 2012 Conference of the North American Chapter of the Associati"
N12-1057,W09-3953,1,0.708218,"Missing"
N12-1057,prabhakaran-etal-2012-annotations,1,0.762636,"Missing"
N13-1044,J95-4004,0,0.652028,"Missing"
N13-1044,E06-1047,1,0.883658,"Missing"
N13-1044,W07-0812,0,0.0536773,"Missing"
N13-1044,W05-0708,0,0.22811,"Missing"
N13-1044,N13-1066,1,0.6732,"ture proclitic +  sa+ appears in ARZ as +ë ha+. There are some morphemes in ARZ that do not exist in MSA such as the negation circum-clitic + . . . + AÓ mA+ . . . +š. And there are MSA features that are absent from ARZ, most notably case and mood. Since there are no orthographic standards, ARZ words may be written in a variety of ways reflecting different writing rules, e.g., phonologically or etymologically. A conventional orthography for Dialectal Arabic (CODA) has been proposed and used for writing ARZ in the context of NLP applications (Habash et al., 2012a; Al-Sabbagh and Girju, 2012; Eskander et al., 2013). Finally, MSA and ARZ coexist and are often used interchangeably, especially in more formal settings. The CALIMA morphological analyzer we use addresses several of these issues by modeling both ARZ and MSA together, including a limited set of inter-dialect morphology phenomena, and by mapping ARZ words into CODA orthography internally while accepting a wide range of spelling variants. 4 4.1 Approach The MADA Approach MADA is a method for Arabic morphological analysis and disambiguation (Habash and Rambow, 2005; Roth et al., 2008). MADA uses a morphological analyzer to produce, for each input"
N13-1044,W08-0509,0,0.033935,"Missing"
N13-1044,P05-1071,1,0.827859,"n the context of NLP applications (Habash et al., 2012a; Al-Sabbagh and Girju, 2012; Eskander et al., 2013). Finally, MSA and ARZ coexist and are often used interchangeably, especially in more formal settings. The CALIMA morphological analyzer we use addresses several of these issues by modeling both ARZ and MSA together, including a limited set of inter-dialect morphology phenomena, and by mapping ARZ words into CODA orthography internally while accepting a wide range of spelling variants. 4 4.1 Approach The MADA Approach MADA is a method for Arabic morphological analysis and disambiguation (Habash and Rambow, 2005; Roth et al., 2008). MADA uses a morphological analyzer to produce, for each input word, a list of analyses specifying every possible morphological interpretation of that word, covering all morphological features of the word (diacritization, POS, lemma, and 13 inflectional and clitic features). MADA then applies a set of models (support vector machines and N-gram language models) to produce a prediction, per word in-context, for different morphological features, such as POS, lemma, gender, number or person. A ranking component scores the analyses produced by the morphological analyzer using a"
N13-1044,P06-1086,1,0.693833,"Missing"
N13-1044,N06-2013,1,0.877369,"Missing"
N13-1044,habash-etal-2012-conventional,1,0.598422,"of Arabic (Habash and Rambow, 2005). The approach used in MADA, which was inspired by earlier work by Hajiˇc (2000), disambiguates in context for every aspect of Arabic morphology, thus solving all tasks in “one fell swoop”. The disadvantage of the MADA approach is its dependence on two complex resources: a morphological analyzer for the language and a large collection of manually annotated words for all morphological features in the same representation used by the analyzer. For ARZ, such resources have recently become available, with the development of the CALIMA ARZ morphological analyzer (Habash et al., 2012b) and the release by the Linguistic Data Consortium (LDC) of a large ARZ corpus annotated morphologically in a manner compatible with CALIMA (Maamouri et al., 2012a). In the work presented here, we utilize these new resources within the paradigm of MADA, transforming MADA into MADA-ARZ. The elegance of the MADA solution makes this conceptually a simple extension. Our evaluation demonstrates that our Egyptian DA version of MADA, henceforth MADA-ARZ, outperforms MADA for MSA on ARZ morphological tagging and improves the quality of ARZ to English statistical machine translation (MT). The rest of"
N13-1044,W12-2301,1,0.811988,"of Arabic (Habash and Rambow, 2005). The approach used in MADA, which was inspired by earlier work by Hajiˇc (2000), disambiguates in context for every aspect of Arabic morphology, thus solving all tasks in “one fell swoop”. The disadvantage of the MADA approach is its dependence on two complex resources: a morphological analyzer for the language and a large collection of manually annotated words for all morphological features in the same representation used by the analyzer. For ARZ, such resources have recently become available, with the development of the CALIMA ARZ morphological analyzer (Habash et al., 2012b) and the release by the Linguistic Data Consortium (LDC) of a large ARZ corpus annotated morphologically in a manner compatible with CALIMA (Maamouri et al., 2012a). In the work presented here, we utilize these new resources within the paradigm of MADA, transforming MADA into MADA-ARZ. The elegance of the MADA solution makes this conceptually a simple extension. Our evaluation demonstrates that our Egyptian DA version of MADA, henceforth MADA-ARZ, outperforms MADA for MSA on ARZ morphological tagging and improves the quality of ARZ to English statistical machine translation (MT). The rest of"
N13-1044,A00-2013,0,0.167854,"Missing"
N13-1044,P07-2045,0,0.0056658,"Missing"
N13-1044,maamouri-etal-2006-developing,1,0.918685,"Missing"
N13-1044,mohamed-etal-2012-annotating,0,0.102356,"Missing"
N13-1044,P03-1021,0,0.0143296,"English MT MT Experimental Settings We use the opensource Moses toolkit (Koehn et al., 2007) to build a phrase-based SMT system. We use MGIZA++ for word alignment (Gao and Vogel, 2008). Phrase translations of up to 8 words are extracted in the phrase table. We use SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing to build two 4gram language models. The first model is trained on the English side of the bitext, while the other is trained on the English Gigaword data. Feature weights are tuned to maximize BLEU (Papineni et al., 2002) on a development set using Minimum Error Rate Training (Och, 2003). We perform caseinsensitive evaluation in terms of BLEU , METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) metrics. Data We trained on DA-English parallel data (Egyptian and Levantine) obtained from several LDC corpora. The training data amounts to 3.8M untokenized words on the Arabic side. The dev set, used for tuning the parameters of the MT system, has 15,585 untokenized Arabic words. The test set has 12,116 untokenized Arabic words. Both dev and test data contain two sets of reference translations. The English data is lower-cased and tokenized using simple punctuation-based"
N13-1044,P02-1040,0,0.105292,"Missing"
N13-1044,P08-2030,1,0.554215,"ications (Habash et al., 2012a; Al-Sabbagh and Girju, 2012; Eskander et al., 2013). Finally, MSA and ARZ coexist and are often used interchangeably, especially in more formal settings. The CALIMA morphological analyzer we use addresses several of these issues by modeling both ARZ and MSA together, including a limited set of inter-dialect morphology phenomena, and by mapping ARZ words into CODA orthography internally while accepting a wide range of spelling variants. 4 4.1 Approach The MADA Approach MADA is a method for Arabic morphological analysis and disambiguation (Habash and Rambow, 2005; Roth et al., 2008). MADA uses a morphological analyzer to produce, for each input word, a list of analyses specifying every possible morphological interpretation of that word, covering all morphological features of the word (diacritization, POS, lemma, and 13 inflectional and clitic features). MADA then applies a set of models (support vector machines and N-gram language models) to produce a prediction, per word in-context, for different morphological features, such as POS, lemma, gender, number or person. A ranking component scores the analyses produced by the morphological analyzer using a tuned weighted sum"
N13-1044,W11-2602,1,0.571602,"Missing"
N13-1044,N13-1036,1,0.795899,"Missing"
N13-1044,2006.amta-papers.25,0,0.0321823,"ased SMT system. We use MGIZA++ for word alignment (Gao and Vogel, 2008). Phrase translations of up to 8 words are extracted in the phrase table. We use SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing to build two 4gram language models. The first model is trained on the English side of the bitext, while the other is trained on the English Gigaword data. Feature weights are tuned to maximize BLEU (Papineni et al., 2002) on a development set using Minimum Error Rate Training (Och, 2003). We perform caseinsensitive evaluation in terms of BLEU , METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006) metrics. Data We trained on DA-English parallel data (Egyptian and Levantine) obtained from several LDC corpora. The training data amounts to 3.8M untokenized words on the Arabic side. The dev set, used for tuning the parameters of the MT system, has 15,585 untokenized Arabic words. The test set has 12,116 untokenized Arabic words. Both dev and test data contain two sets of reference translations. The English data is lower-cased and tokenized using simple punctuation-based rules. Systems We build three translation systems which vary in tokenization of the Arabic text. The first system applies"
N13-1044,P12-2063,0,0.0558195,"Missing"
N13-1044,N12-1006,0,0.416831,"Missing"
N13-1044,P06-1073,0,0.1305,"Missing"
N13-1044,W05-0909,0,\N,Missing
N13-1044,D08-1076,0,\N,Missing
N13-1066,abuhakema-etal-2008-annotating,0,0.133349,"NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure, respectively (Kukich, 1992; Oflazer, 1996; Ben Othmane Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Shaalan et al., 2010; Alkanhal et al., 2012). Supervised approaches learn models of correction by training on paired examples of errors and their corrections. This data is hard to come by naturally, though for applications such as OCR corpora can be created from the application itself (Kolak and Resnik, 2002; Magdy and Darwish, 2006; Abuhakema et al., 2008; Habash and Roth, 2011). There has been some work on conversion of dialectal Arabic to MSA. Al-Gaphari and Al-Yadoumi (2010) introduced a rule-based method to convert Sanaani dialect to MSA, and Shaalan et al. (2007) used a rule-based lexical transfer approach to transform from EGY to MSA. Similarly, both Sawaf (2010) and Salloum and Habash (2011) showed that translating dialectal Arabic to MSA can improve dialectal Arabic machine translation into English by pivoting on MSA. A common feature across these conversion efforts is the use of morphological analysis and morphosyntactic transformatio"
N13-1066,I11-1036,0,0.0614695,"EGY to MSA. Similarly, both Sawaf (2010) and Salloum and Habash (2011) showed that translating dialectal Arabic to MSA can improve dialectal Arabic machine translation into English by pivoting on MSA. A common feature across these conversion efforts is the use of morphological analysis and morphosyntactic transformation rules (for example, Al-Gaphari and Al-Yadoumi (2010)). While all this work is similar to ours in that dialectal input is processed, our output is still dialectal, while the work on conversion aims for a transformation into MSA. The work most closely related to ours is that of Dasigi and Diab (2011). They identify the spelling variants in a given document and normalize them. However, they do not present a system that converts spontaneous spelling to a pre-existing convention such as CODA, and thus their results cannot be directly related to ours. Furthermore, their technique is different. First, similarity metrics based on string difference are used to identify if two strings are similar. Also, a contextual string similarity is used based on the fact that if two words are orthographic variants of each other, then they are bound to appear in similar contexts. After identifying the similar"
N13-1066,W08-0509,0,0.0133854,"Missing"
N13-1066,P05-1071,1,0.114663,"Missing"
N13-1066,P11-1088,1,0.874123,"nd heuristics, e.g., morphological analyzers, language models, and edit-distance measure, respectively (Kukich, 1992; Oflazer, 1996; Ben Othmane Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Shaalan et al., 2010; Alkanhal et al., 2012). Supervised approaches learn models of correction by training on paired examples of errors and their corrections. This data is hard to come by naturally, though for applications such as OCR corpora can be created from the application itself (Kolak and Resnik, 2002; Magdy and Darwish, 2006; Abuhakema et al., 2008; Habash and Roth, 2011). There has been some work on conversion of dialectal Arabic to MSA. Al-Gaphari and Al-Yadoumi (2010) introduced a rule-based method to convert Sanaani dialect to MSA, and Shaalan et al. (2007) used a rule-based lexical transfer approach to transform from EGY to MSA. Similarly, both Sawaf (2010) and Salloum and Habash (2011) showed that translating dialectal Arabic to MSA can improve dialectal Arabic machine translation into English by pivoting on MSA. A common feature across these conversion efforts is the use of morphological analysis and morphosyntactic transformation rules (for example, Al"
N13-1066,habash-etal-2012-conventional,1,0.319733,"ears in EGY as /ha/+ or /Ha/+. The two forms appear in free variation, and we have not been able to find a variable that predicts which form is used when. This variation is not a general phonological variation between /h/ and /H/, we find it only in this morpheme. Predictably, this leads to two spellings in EGY: + h H+ and + ë h+. Negation in EGY is realized as the circum-clitic /m¯a/+ . . . +/š/. The principal orthographic question is whether the prefix is a separate word or is part of the main word; both variants are found. 3 CODA CODA is a conventionalized orthography for Arabic dialects (Habash et al., 2012). In this section, we summarize CODA so that the reader can understand the goals of this paper. CODA has five key properties. 1. CODA is an internally consistent and coherent convention for writing DA: every word has a  always written as è h ¯ in CODA, e.g., /’arbaςa/ ‘four’ single orthographic rendering. 2. CODA is created for computational purposes. 3. CODA uses the Arabic script as used for MSA, with no extra symbols from, for example, Persian or Urdu. 4. CODA is intended as a unified framework for writing all dialects. In this paper, we only discuss the instantiation of CODA for EGY. 5. C"
N13-1066,N13-1044,1,0.65939,"rm as seen in the training data. This assumes that the underlying word exists in the training corpus. For unseen words, the technique keeps them with no change. The MLE approach chooses the correct CODA form for most of the words seen in training, making this approach highly dependent on the training data. It is efficient at correcting common misspellings in frequent words, especially those that are from closed classes. 6.4 Morphological Tagger In addition to the approaches discussed above, we use a morphological tagger, MADAARZ (Morphological Analysis and Disambiguation for Egyptian Arabic) (Habash et al., 2013). Although MADAARZ is originally developed to work as a morphological tagger, it still can help the codafication process, since the choice of a full morphological analysis for a word in context determines its CODA spelling. Therefore, MADAARZ is able to correct many word misspellings that are common in spontaneous orthography. These corrections include  ˇ A), ¯ ø/ ø y/ý and è/ è h/¯ ( @/ @/ @/ @ A/Â/A/ h transforma tions. However, MADAARZ , as a codafication technique, uses the context of the word, which makes it a contextual modeling approach unlike CEC and MLE. It is much slower than they"
N13-1066,I08-2131,0,0.0278911,"Missing"
N13-1066,P07-2045,0,0.00829527,"Missing"
N13-1066,W06-1648,0,0.0265451,"of-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure, respectively (Kukich, 1992; Oflazer, 1996; Ben Othmane Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Shaalan et al., 2010; Alkanhal et al., 2012). Supervised approaches learn models of correction by training on paired examples of errors and their corrections. This data is hard to come by naturally, though for applications such as OCR corpora can be created from the application itself (Kolak and Resnik, 2002; Magdy and Darwish, 2006; Abuhakema et al., 2008; Habash and Roth, 2011). There has been some work on conversion of dialectal Arabic to MSA. Al-Gaphari and Al-Yadoumi (2010) introduced a rule-based method to convert Sanaani dialect to MSA, and Shaalan et al. (2007) used a rule-based lexical transfer approach to transform from EGY to MSA. Similarly, both Sawaf (2010) and Salloum and Habash (2011) showed that translating dialectal Arabic to MSA can improve dialectal Arabic machine translation into English by pivoting on MSA. A common feature across these conversion efforts is the use of morphological analysis and morph"
N13-1066,P03-1021,0,0.0135224,"Missing"
N13-1066,J96-1003,0,0.0368266,"ome similarity to automatic spelling correction (ASC) and related tasks such as post editing for optical character recognition (OCR). Our task is different from ASC since ASC work assumes a standard orthography that the writer is also assumed to aim for. Both supervised and unsupervised approaches to this task have been explored. Unsupervised approaches rely on improving the fluency of the text and reducing the percentage of out-of-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure, respectively (Kukich, 1992; Oflazer, 1996; Ben Othmane Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Shaalan et al., 2010; Alkanhal et al., 2012). Supervised approaches learn models of correction by training on paired examples of errors and their corrections. This data is hard to come by naturally, though for applications such as OCR corpora can be created from the application itself (Kolak and Resnik, 2002; Magdy and Darwish, 2006; Abuhakema et al., 2008; Habash and Roth, 2011). There has been some work on conversion of dialectal Arabic to MSA. Al-Gaphari and Al-Yadoumi (2010) introdu"
N13-1066,P02-1040,0,0.104397,"Missing"
N13-1066,W11-2602,1,0.250709,"correction by training on paired examples of errors and their corrections. This data is hard to come by naturally, though for applications such as OCR corpora can be created from the application itself (Kolak and Resnik, 2002; Magdy and Darwish, 2006; Abuhakema et al., 2008; Habash and Roth, 2011). There has been some work on conversion of dialectal Arabic to MSA. Al-Gaphari and Al-Yadoumi (2010) introduced a rule-based method to convert Sanaani dialect to MSA, and Shaalan et al. (2007) used a rule-based lexical transfer approach to transform from EGY to MSA. Similarly, both Sawaf (2010) and Salloum and Habash (2011) showed that translating dialectal Arabic to MSA can improve dialectal Arabic machine translation into English by pivoting on MSA. A common feature across these conversion efforts is the use of morphological analysis and morphosyntactic transformation rules (for example, Al-Gaphari and Al-Yadoumi (2010)). While all this work is similar to ours in that dialectal input is processed, our output is still dialectal, while the work on conversion aims for a transformation into MSA. The work most closely related to ours is that of Dasigi and Diab (2011). They identify the spelling variants in a given"
N13-1066,2010.amta-papers.5,0,0.109513,"s learn models of correction by training on paired examples of errors and their corrections. This data is hard to come by naturally, though for applications such as OCR corpora can be created from the application itself (Kolak and Resnik, 2002; Magdy and Darwish, 2006; Abuhakema et al., 2008; Habash and Roth, 2011). There has been some work on conversion of dialectal Arabic to MSA. Al-Gaphari and Al-Yadoumi (2010) introduced a rule-based method to convert Sanaani dialect to MSA, and Shaalan et al. (2007) used a rule-based lexical transfer approach to transform from EGY to MSA. Similarly, both Sawaf (2010) and Salloum and Habash (2011) showed that translating dialectal Arabic to MSA can improve dialectal Arabic machine translation into English by pivoting on MSA. A common feature across these conversion efforts is the use of morphological analysis and morphosyntactic transformation rules (for example, Al-Gaphari and Al-Yadoumi (2010)). While all this work is similar to ours in that dialectal input is processed, our output is still dialectal, while the work on conversion aims for a transformation into MSA. The work most closely related to ours is that of Dasigi and Diab (2011). They identify the"
N13-1066,D08-1076,0,\N,Missing
N13-1099,P12-2032,1,0.62499,"re that the reader adopt a certain belief. It covers many different types of information that can be conveyed including answers to questions, beliefs (committed or not), attitudes, and elaborations on prior DAs. Data In this study, we use the email corpus presented in (Hu et al., 2009), which is manually annotated for DA tags. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006; Agarwal et al., 2012). Most emails are concerned with exchanging information, scheduling meetings, or solving problems, but there are also purely social emails. In a R EQUEST-I NFORMATION, the writer signals her desire that the reader perform a specific communicative act, namely that he provide information (either facts or opinion). A C ONVENTIONAL dialog act does not signal any specific communicative intention on the part of the writer, but rather it helps structure and thus facilitate the communication. Examples include greetings, introductions, expressions of gratitude, etc. 4 Dialog Act Tag R EQUEST-ACTION (R-"
N13-1099,W04-3240,0,0.523922,"Missing"
N13-1099,W09-3953,1,0.93083,"Missing"
N13-1099,D10-1084,0,0.110709,"nt our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu et al., 2009). Ou"
N13-1099,W10-2923,0,0.049703,"nt our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu et al., 2009). Ou"
N13-1099,N12-1057,1,0.870324,"ramework was built with the ClearTK toolkit (Ogren et al., 2008) with its wrapper for SVMLight (Joachims, 1999). The ClearTK wrapper internally shifts the prediction threshold based on posterior probabilistic scores calculated using the algorithm of Lin et al. (2007). We report results from 5-fold cross validation performed on the entire corpus. 4.2 Feature Engineering In developing our system, we classified our features into three categories: lexical, verbal and messagelevel. Lexical features consists of n-grams of words, n-grams of POS tags, mixed n-grams of closed class words and POS tags (Prabhakaran et al., 2012), as well as a small set of specialized features — StartPOS/Lemma (POS tag and lemma of the first word), LastPOS/Lemma (POS tag and lemma of the last word), MDCount (number of modal verbs in the DFU) and QuestionMark (is there a question mark in the DFU). We used the POS tags produced by the OpenNLP POS tagger. Verbal features capture the position and identity of the first verb in the DFU. Finally, message-level features capture aspects of the location of the DFU in the message and of the message in the thread (relative position and size). In optimizing each system, we first performed an exhau"
N13-1099,J00-3003,0,0.269331,"tion for the other classes and for the overall classifier. This paper is structured as follows. We start out by discussing related work (Section 2). We then present our data in Section 3, and in Section 4 we present the experiments with our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one anoth"
N13-1099,W12-0603,0,0.110434,"ith our systems and the results. We report the results of an extrinsic evaluation in Section 5, and conclude. 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle (1976), have been used in the NLP community to understand and model dialog. Initial work was done on spoken interactions (see for example (Stolcke et al., 2000)). Recently, studies have explored dialog act tagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim et al., 2010b), instant messaging (Kim et al., 2010a) and Twitter (Zhang et al., 2012). Most DA tagging systems for written interactions use a message/post level tagging scheme, and allow multiple tags for each message/post. In such a tagging scheme, indi802 Proceedings of NAACL-HLT 2013, pages 802–807, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics vidual binary classifiers for each tag are independent of one another. However, recent studies have found merit in segmenting each message into functional units and assigning a single DA to each segment (Hu et al., 2009). Our work falls in this paradigm (we choose a single DA for smaller textual u"
N18-1096,P12-2032,1,0.838969,"(p1 , p2 ) in the thread, predict whether p1 is the superior or subordinate of p2 . In this formulation, a related interacting participant pair (RIPP) is a pair of participants of the thread such that there is at least one message exchanged within the thread between them (in either direction) and that they are hierarchically related with a superior/subordinate relation. 3.2 Data We use the same dataset we used in (Prabhakaran and Rambow, 2014), which is a version of the Enron email corpus in which the thread structure of email messages is reconstructed (Yeh and Harnly, 2006), and enriched by Agarwal et al. (2012) with gold organizational power relations, manually determined using information from Enron organizational charts. The corpus captures dominance relations between 13,724 pairs of Enron employees. As in (Prabhakaran and Rambow, 2014), we use these dominance relation tuples to obtain gold labels for the superior or subordinate relationships between pairs of participants. We use the same train-test-dev split as in (Prabhakaran and Rambow, 2014). We summarize the number of threads and related interacting participant pairs in each subset of the data in Table 1. 4 Research Hypotheses Our first objec"
N18-1096,W10-3001,0,0.0213727,"Missing"
N18-1096,W09-3012,1,0.883234,"2014). Another area of research that has recently garnered interest within the NLP community is the modeling of author commitment in text. Initial studies in this area were done in processing hedges, uncertainty and lack of commitment, specifically focused on scientific text (Mercer et al., 2004; Di Marco et al., 2006; Farkas et al., 2010). More recently, researchers have also looked into capturing author commitment in nonscientific text, e.g., levels of factuality in newswire (Saur´ı and Pustejovsky, 2009), types of commitment of beliefs in a variety of genres including conversational text (Diab et al., 2009; Prabhakaran et al., 2015). These approaches are motivated from an information extraction perspective, for instance in aiding tasks such as knowledge base population.1 However, it has not been studied whether such sophisticated author commitment analysis can go beyond what is expressed in language and reveal the underlying social contexts in which language is exchanged. In this paper, we bring together these two lines of research; we study how power relations correlate with the levels of commitment authors express in interactions. We use the power analysis framework built by Prabhakaran and R"
N18-1096,W08-0607,0,0.0105405,"that is obtained on online discussion forums, which is closer to our genre. Automatic hedge/uncertainty detection is a very closely related task to belief detection. The belief tagging framework we use aims to capture the cognitive states of authors, whereas hedges are linguistic expressions that convey one of those cognitive states — non-committed beliefs. Automatic hedge/uncertainty detection has generated active research in recent years within the NLP community. Early work in this area focused on detecting speculative language in scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschbe"
N18-1096,W10-3006,0,0.0227311,"Missing"
N18-1096,S15-1009,1,0.931333,"a of research that has recently garnered interest within the NLP community is the modeling of author commitment in text. Initial studies in this area were done in processing hedges, uncertainty and lack of commitment, specifically focused on scientific text (Mercer et al., 2004; Di Marco et al., 2006; Farkas et al., 2010). More recently, researchers have also looked into capturing author commitment in nonscientific text, e.g., levels of factuality in newswire (Saur´ı and Pustejovsky, 2009), types of commitment of beliefs in a variety of genres including conversational text (Diab et al., 2009; Prabhakaran et al., 2015). These approaches are motivated from an information extraction perspective, for instance in aiding tasks such as knowledge base population.1 However, it has not been studied whether such sophisticated author commitment analysis can go beyond what is expressed in language and reveal the underlying social contexts in which language is exchanged. In this paper, we bring together these two lines of research; we study how power relations correlate with the levels of commitment authors express in interactions. We use the power analysis framework built by Prabhakaran and Rambow (2014) to perform thi"
N18-1096,W15-0302,0,0.0208689,"in this area focused on detecting speculative language in scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschberg, 2014). In our work, we use the lexicons from (Prokofieva and Hirschberg, 2014) to capture hedges in text. Sociolinguists have long studied the association between level of commitment and social contexts (Lakoff, 1973; O’Barr and Atkins, 1980; Hyland, 1998). A majority of this work studies gender differences in the use of hedges, triggered by the influential work by Robin Lakoff (Lakoff, 1973). She argued that women use linguistic strategies such as hedging and hesitat"
N18-1096,I13-1042,1,0.899855,"Missing"
N18-1096,I13-1025,1,0.837951,"5; Creamer et al., 2009) or email traffic patterns (Namata et al., 2007). Using NLP to deduce social relations from online communication is a relatively new area of active research. Bramsen et al. (2011) and Gilbert (2012) first applied NLP based techniques to predict power relations in Enron emails, approaching this task as a text classification problem using bag of words or ngram features. More recently, our work has used dialog structure features derived from deeper dialog act analysis for the task of power prediction in Enron emails (Prabhakaran and Rambow, 2014; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013). In this paper, We use the framework of (Prabhakaran and Rambow, 2014), but we analyze a novel aspect of interaction that has not been studied before — what level of commitment do the authors express in language. There has also been work on analyzing power in other genres of interactions. Strzalkowski et al. (2010) and Taylor et al. (2012) concentrate on lower-level constructs called Language Uses such as agenda control to predict power in Wikipedia talk pages. Danescu-Niculescu-Mizil et al. (2012) study how social power and linguistic coordination are correlated in Wikipedia interactions as"
N18-1096,C10-1117,0,0.0564892,"Missing"
N18-1096,P14-2056,1,0.89234,"Missing"
N18-1096,C10-2117,1,0.839063,"Missing"
N18-1096,C12-1138,1,0.81554,"005; Shetty and Adibi, 2005; Creamer et al., 2009) or email traffic patterns (Namata et al., 2007). Using NLP to deduce social relations from online communication is a relatively new area of active research. Bramsen et al. (2011) and Gilbert (2012) first applied NLP based techniques to predict power relations in Enron emails, approaching this task as a text classification problem using bag of words or ngram features. More recently, our work has used dialog structure features derived from deeper dialog act analysis for the task of power prediction in Enron emails (Prabhakaran and Rambow, 2014; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013). In this paper, We use the framework of (Prabhakaran and Rambow, 2014), but we analyze a novel aspect of interaction that has not been studied before — what level of commitment do the authors express in language. There has also been work on analyzing power in other genres of interactions. Strzalkowski et al. (2010) and Taylor et al. (2012) concentrate on lower-level constructs called Language Uses such as agenda control to predict power in Wikipedia talk pages. Danescu-Niculescu-Mizil et al. (2012) study how social power and linguistic coordination are correlate"
N18-1096,J12-2005,0,0.0130652,"ns that convey one of those cognitive states — non-committed beliefs. Automatic hedge/uncertainty detection has generated active research in recent years within the NLP community. Early work in this area focused on detecting speculative language in scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschberg, 2014). In our work, we use the lexicons from (Prokofieva and Hirschberg, 2014) to capture hedges in text. Sociolinguists have long studied the association between level of commitment and social contexts (Lakoff, 1973; O’Barr and Atkins, 1980; Hyland, 1998). A majority of this w"
N18-1096,C14-1174,0,0.0128194,"scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschberg, 2014). In our work, we use the lexicons from (Prokofieva and Hirschberg, 2014) to capture hedges in text. Sociolinguists have long studied the association between level of commitment and social contexts (Lakoff, 1973; O’Barr and Atkins, 1980; Hyland, 1998). A majority of this work studies gender differences in the use of hedges, triggered by the influential work by Robin Lakoff (Lakoff, 1973). She argued that women use linguistic strategies such as hedging and hesitations in order to adopt an unassertive communica"
N18-1096,P13-2011,0,0.0190235,"munity. Early work in this area focused on detecting speculative language in scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008). The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012). Most of this work was aimed at formal scientific text in English. More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschberg, 2014). In our work, we use the lexicons from (Prokofieva and Hirschberg, 2014) to capture hedges in text. Sociolinguists have long studied the association between level of commitment and social contexts (Lakoff, 1973; O’Barr and Atkins, 1980; Hyland, 1998). A majority of this work studies gender differences in the use of hedges, triggered by the influential work by Robin Lakoff (Lakoff, 1973). She argued that women use linguistic strategies s"
N18-1096,C00-2137,0,0.19275,"Missing"
N18-1096,P11-1078,0,\N,Missing
N18-1107,W16-3309,1,0.726171,"od of the observed sequences of supertags in a minibatch stochastic fashion with the Adam optimization algorithm with batch size 100 and ` = 0.01 (Kingma and Ba, 2015). In order to obtain predicted POS tags and supertags of the training data for subsequent parser input, we also perform 10fold jackknife training. After each training epoch, we test the supertagger on the dev set. When classification accuracy does not improve on five consecutive epochs, training ends. 2.2 More recent work, however, has shown that data-driven transition-based parsing systems outperform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017). Kasai et al. (2017) and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations. Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) that allows for global training and inference. Parsing Model Until recently, TAG parsers have been grammar based, requiring as input a set of elemenetary t"
N18-1107,K17-3002,0,0.0318412,"grammar provided as input. Input Representations The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representation obtained from CNNs in the same fashion as in the supertagger.1 We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding predicted POS tags boosted performance in Stanford Dependencies. In Universal Dependencies, Dozat et al. (2017) empirically showed that their dependency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tagger. Indeed, Kasai et al. (2017) and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as input predicted supertags can achieve state-of-theart performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger. The other embeddings are randomly initialized. 2.2.2 Biaffine Parser We train our parser to predict edges between"
N18-1107,W17-6213,1,0.495291,"voiding a time-consuming and complicated pipeline process, and instead produces a full syntactic analysis, consisting of supertags and the derivation that combines them, simultaneously. Moreover, this multi-task learning framework further improves performance in all three tasks. We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that generalize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analogies across induced vector representations of supertags (Kasai et al., 2017; Friedman et al., 2017). The end-to-end TAG parser substantially outperforms the previously reported best results. Finally, we apply our new parsers to the downstream tasks of Parsing Evaluation using Textual Entailements (PETE, Yuret et al. (2010)) and Unbounded Dependency Recovery (Rimell et al., 2009). We demonstrate that our end-to-end parser outperforms the best results in both tasks. These results illustrate that TAG is a viable formalism for tasks that benefit from the assignment of rich structural descriptions to sentences. 2 Our Models TAG parsing can be decomposed into supertagging and parsing. Supertaggin"
N18-1107,Q13-1033,0,0.0297508,"models for transitionbased and graph-based parsing can be viewed as remedies for the aforementioned limitations. Andor et al. (2016) developed a transition-based parser using feed-forward neural networks that performs global training approximated by beam search. The globally normalized objective addresses the label bias problem and makes global 7 The substantially better performance of the C&C parser is in fact the result of additions that were made to the training data. training effective in the transition-based parsing setting. Kiperwasser and Goldberg (2016) incorporated a dynamic oracle (Goldberg and Nivre, 2013) in a BiLSTM transition-based parser that remedies global error propagation. Kiperwasser and Goldberg (2016) and Dozat and Manning (2017) proposed graph-based parsers that have access to rich feature representations obtained from BiLSTMs. Previous work integrated CCG supertagging and parsing using belief propagation and dual decomposition approaches (Auli and Lopez, 2011). Nguyen et al. (2017) incorporated a graph-based dependency parser (Kiperwasser and Goldberg, 2016) with POS tagging. Our work followed these lines of effort and improved TAG parsing performance. 7 Conclusion and Future Work"
N18-1107,J07-3004,0,0.164217,"Missing"
N18-1107,D17-1180,1,0.667107,"Missing"
N18-1107,Q16-1023,0,0.459692,"ormed into a variant of a probabilistic CFG. One advantage of such a parser is that its parses are guaranteed to be well-formed according to the TAG grammar provided as input. Input Representations The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representation obtained from CNNs in the same fashion as in the supertagger.1 We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017). The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding predicted POS tags boosted performance in Stanford Dependencies. In Universal Dependencies, Dozat et al. (2017) empirically showed that their dependency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tagger. Indeed, Kasai et al. (2017) and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as input predicted supertags can achieve state-of-theart performance, with lexical inputs providing no improvement in performance. We initialize word embeddings to be the pre-trained GloVe vectors"
N18-1107,N16-1026,0,0.573762,"parsing can be decomposed into two phases (e.g. TAG: Bangalore and Joshi (1999); CCG: Clark and Curran (2007)): supertagging, where elementary units or supertags are assigned to each lexical item and parsing where these supertags are combined together. The first phase of supertagging can be considered as “almost parsing” because supertags for a sentence almost always determine a unique parse (Bangalore and Joshi, 1999). This near uniqueness of a parse given a gold sequence of supertags has been confirmed empirically (TAG: Bangalore et al. (2009); Chung et al. (2016); Kasai et al. (2017); CCG: Lewis et al. (2016)). We focus on TAG parsing in this work. TAG differs from CCG in having a more varied set of supertags. Concretely, the TAG-annotated version of the WSJ Penn Treebank (Marcus et al., 1993) that we use (Chen et al., 2005) includes 4727 distinct supertags (2165 occur once) while the CCGannotated version (Hockenmaier and Steedman, 2007) only includes 1286 distinct supertags (439 occur once). This large set of supertags in TAG presents a severe challenge in supertagging and causes a large discrepancy in parsing performance with gold supertags and predicted supertags (Bangalore et al., 2009; Chung"
N18-1107,E14-3009,0,0.0231499,"Missing"
N18-1107,P16-1101,0,0.719979,"onvolutional Neural Networks (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections. 2.1.1 Input Representations The input for each word is represented via concatenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30dimensional character-level representation from CNNs that have been found to capture morphological information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016). The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word. We initialize the word embeddings to be the pre-trained GloVe vectors (Pennington et al., 2014); for words not in GloVe, we initialize their embedding to a zero vector. The other embeddings are randomly initialized. We obtain predicted POS tags from a BiLSTM POS tagger with the same configuration as in Ma and Hovy (2016). 2.1.2 Deep Highway BiLSTM The core of the supertagging model is a deep bidirectional Long Short-Term Memory network (Graves and Schmidhuber, 2005"
N18-1107,J93-2004,0,0.0621587,"Missing"
N18-1107,J11-1007,0,0.175642,"-CNN BiLSTM4-CNN BiLSTM4-HW-CNN BiLSTM5-CNN BiLSTM5-HW-CNN Joint (Stag) Joint (POS+Stag) Joint (Shuffled Stag) Table 3: Parsing results on the dev and test sets. POS tagging, supertagging, and parsing further improves performance in all of the three tasks, yielding the test result of 91.89 LAS and 93.26 UAS points, an improvement of more than 2.2 points each from the state-of-the-art. Figures 2 and 3 illustrate the relative performance of the feed-forward neural network shiftreduce TAG parser (Kasai et al., 2017) and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse. The graph-based parser outperforms the shift-reduce parser across all conditions. Most interesting is the fact that the graph-based parser shows less of an effect of dependency length. Since the shiftreduce parser builds a parse sequentially with one parsing action depending on those that come before it, we would expect to find a propogation of errors made in establishing shorter dependencies to the establishment of longer dependencies. Lastly, it is worth noting our joint parsing arOur Joint Parser Shift-red"
N18-1107,H05-1066,0,0.0668685,"is construction. Nonetheless, we see that the rich structural representations that a TAG parser provides enables substantial improvements in the extraction of unbounded dependencies. In the future, we hope to evaluate state-of-the-art Stanford dependency parsers automatically. 6 Related Work The two major classes of data-driven methods for dependency parsing are often called transitionbased and graph-based parsing (K¨ubler et al., 2009). Transition-based parsers (e.g. MALT (Nivre, 2003)) learn to predict the next transition given the input and the parse history. Graph-based parsers (e.g. MST (McDonald et al., 2005)) are trained to directly assign scores to dependency graphs. Empirical studies have shown that a transitionbased parser and a graph-based parser yield similar overall performance across languages (McDonald and Nivre, 2011), but the two strands of data-driven parsing methods manifest the fundamental trade-off of parsing algorithms. The former prefers rich feature representations with parsing history over global training and exhaustive search, and the latter allows for global training and inference at the expense of limited feature representations (K¨ubler et al., 2009). Recent neural network m"
N18-1107,K17-3014,0,0.0326791,"ser is in fact the result of additions that were made to the training data. training effective in the transition-based parsing setting. Kiperwasser and Goldberg (2016) incorporated a dynamic oracle (Goldberg and Nivre, 2013) in a BiLSTM transition-based parser that remedies global error propagation. Kiperwasser and Goldberg (2016) and Dozat and Manning (2017) proposed graph-based parsers that have access to rich feature representations obtained from BiLSTMs. Previous work integrated CCG supertagging and parsing using belief propagation and dual decomposition approaches (Auli and Lopez, 2011). Nguyen et al. (2017) incorporated a graph-based dependency parser (Kiperwasser and Goldberg, 2016) with POS tagging. Our work followed these lines of effort and improved TAG parsing performance. 7 Conclusion and Future Work In this work, we presented a state-of-the-art TAG supertagger, a parser, and a joint parser that performs POS tagging, supertagging, and parsing. The joint parser has the benefit of giving a full syntactic analysis of a sentence simultaneously. Furthermore, the joint parser achieved the best performance, an improvement of over 2.2 LAS points from the previous state-of-the-art. We have also see"
N18-1107,W03-3017,0,0.148091,"ccuracy on the 7 constructions. trained.7 For RNR, rarity may be an issue as well as the limits of the TAG analysis of this construction. Nonetheless, we see that the rich structural representations that a TAG parser provides enables substantial improvements in the extraction of unbounded dependencies. In the future, we hope to evaluate state-of-the-art Stanford dependency parsers automatically. 6 Related Work The two major classes of data-driven methods for dependency parsing are often called transitionbased and graph-based parsing (K¨ubler et al., 2009). Transition-based parsers (e.g. MALT (Nivre, 2003)) learn to predict the next transition given the input and the parse history. Graph-based parsers (e.g. MST (McDonald et al., 2005)) are trained to directly assign scores to dependency graphs. Empirical studies have shown that a transitionbased parser and a graph-based parser yield similar overall performance across languages (McDonald and Nivre, 2011), but the two strands of data-driven parsing methods manifest the fundamental trade-off of parsing algorithms. The former prefers rich feature representations with parsing history over global training and exhaustive search, and the latter allows"
N18-1107,C10-1094,0,0.0792448,"Missing"
N18-1107,D14-1162,0,0.0846307,"nnections. 2.1.1 Input Representations The input for each word is represented via concatenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30dimensional character-level representation from CNNs that have been found to capture morphological information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016). The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word. We initialize the word embeddings to be the pre-trained GloVe vectors (Pennington et al., 2014); for words not in GloVe, we initialize their embedding to a zero vector. The other embeddings are randomly initialized. We obtain predicted POS tags from a BiLSTM POS tagger with the same configuration as in Ma and Hovy (2016). 2.1.2 Deep Highway BiLSTM The core of the supertagging model is a deep bidirectional Long Short-Term Memory network (Graves and Schmidhuber, 2005). We use the following formulas to compute the activation of a single LSTM cell at time step t: it = σ (Wi [xt ; ht−1 ] + bi ) (1) ft = σ (Wf [xt ; ht−1 ] + bf ) (2) c˜t = tanh (Wc [xt ; ht−1 ] + bc ) (3) ot = σ (Wo [xt ; ht−"
N18-1107,S10-1060,0,0.0705323,"Missing"
N18-1107,D09-1085,0,0.149682,"ypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that generalize better (Caruana, 1997). We provide support for this hypothesis by analyzing syntactic analogies across induced vector representations of supertags (Kasai et al., 2017; Friedman et al., 2017). The end-to-end TAG parser substantially outperforms the previously reported best results. Finally, we apply our new parsers to the downstream tasks of Parsing Evaluation using Textual Entailements (PETE, Yuret et al. (2010)) and Unbounded Dependency Recovery (Rimell et al., 2009). We demonstrate that our end-to-end parser outperforms the best results in both tasks. These results illustrate that TAG is a viable formalism for tasks that benefit from the assignment of rich structural descriptions to sentences. 2 Our Models TAG parsing can be decomposed into supertagging and parsing. Supertagging assigns to words elementary trees (supertags) chosen from a finite set, and parsing determines how these elementary trees can be combined to form a derivation tree that yield the observed sentence. The combinatory operations consist of substitution, which inserts obligatory argum"
N18-1107,N16-1027,0,0.0421104,"these elementary trees can be combined to form a derivation tree that yield the observed sentence. The combinatory operations consist of substitution, which inserts obligatory arguments, and adjunction, which is responsible for the introduction of modifiers, function words, as well as the derivation of sentences involving long-distance dependencies. In this section, we present our supertagging models, parsing models, and joint models. 2.1 Supertagging Model Recent work has explored neural network models for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve performance beyond non-neural models. We extend previously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Networks (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections. 2.1.1 Input Representations The input for each word is represented via concatenation of a 100-dimensional embedding of the word, a 1"
N18-1107,W17-6214,1,0.842903,"Missing"
N18-1107,D16-1181,0,0.209781,"can be combined to form a derivation tree that yield the observed sentence. The combinatory operations consist of substitution, which inserts obligatory arguments, and adjunction, which is responsible for the introduction of modifiers, function words, as well as the derivation of sentences involving long-distance dependencies. In this section, we present our supertagging models, parsing models, and joint models. 2.1 Supertagging Model Recent work has explored neural network models for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve performance beyond non-neural models. We extend previously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Networks (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections. 2.1.1 Input Representations The input for each word is represented via concatenation of a 100-dimensional embedding of the word, a 100-dimensio"
N18-1107,P15-2041,0,0.0591688,"nite set, and parsing determines how these elementary trees can be combined to form a derivation tree that yield the observed sentence. The combinatory operations consist of substitution, which inserts obligatory arguments, and adjunction, which is responsible for the introduction of modifiers, function words, as well as the derivation of sentences involving long-distance dependencies. In this section, we present our supertagging models, parsing models, and joint models. 2.1 Supertagging Model Recent work has explored neural network models for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve performance beyond non-neural models. We extend previously proposed BiLSTM-based models (Lewis et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Networks (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections. 2.1.1 Input Representations The input for each word is represented via concatenation of a"
N18-1107,J99-2004,0,\N,Missing
N18-1107,P11-1048,0,\N,Missing
N18-1107,S10-1009,0,\N,Missing
N18-1107,S10-1069,0,\N,Missing
N19-1075,W18-2501,0,0.0386863,"Missing"
N19-1075,J02-3001,0,0.552126,"combines the strengths of earlier models that performed SRL on the basis of a full dependency parse with more recent models that use no syntactic information at all. Our local and non-ensemble model achieves state-of-the-art performance on the CoNLL 09 English and Spanish datasets. SRL models benefit from syntactic information, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system. 1 Introduction Semantic role labeling (SRL) is the task of identifying the semantic relationships between each predicate in a sentence and its arguments (Gildea and Jurafsky, 2002). While early research assumed that SRL models required syntactic information to perform well (Punyakanok et al., 2008), recent work has demonstrated that neural networks can achieve competitive and even state-of-the-art performance without any syntactic information at all (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017). These systems have the benefits of being simpler to implement and performing more robustly on foreign languages and outof-domain data, cases where syntactic parsing is more difficult (Marcheggiani et al., 2017). In this paper, we show that using supertags is an"
N19-1075,N09-2047,1,0.753656,"ith Supertags Jungo Kasai♣∗ Dan Friedman♠∗ Robert Frank♦ Dragomir Radev♦ Owen Rambow♥ ♣ University of Washington ♠ Google ♦ Yale University ♥ Elemental Cognition, LLP jkasai@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015)."
N19-1075,P17-1044,0,0.0928139,"supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system. 1 Introduction Semantic role labeling (SRL) is the task of identifying the semantic relationships between each predicate in a sentence and its arguments (Gildea and Jurafsky, 2002). While early research assumed that SRL models required syntactic information to perform well (Punyakanok et al., 2008), recent work has demonstrated that neural networks can achieve competitive and even state-of-the-art performance without any syntactic information at all (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017). These systems have the benefits of being simpler to implement and performing more robustly on foreign languages and outof-domain data, cases where syntactic parsing is more difficult (Marcheggiani et al., 2017). In this paper, we show that using supertags is an effective middle ground between using full syntactic parses and using no syntactic information ∗ Work partially done at Yale University. 701 Proceedings of NAACL-HLT 2019, pages 701–709 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Token No , it was n’t black Monday Model 1 DEP/R P/R S"
N19-1075,J99-2004,0,0.686404,"nformation ∗ Work partially done at Yale University. 701 Proceedings of NAACL-HLT 2019, pages 701–709 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Token No , it was n’t black Monday Model 1 DEP/R P/R SBJ/R ROOT+L R ADV/L NAME/R PRD/L+L Model TAG DEP/R P/R ROOT+SBJ/L PRD/R ADV/L NAME/R - lacks such obligatory children, we encode whether it possesses non-obligatory dependents to the left (L) or right (R) as in Model 1. Model TAG. We propose Model TAG supertags that represent syntactic information analogously to TAG supertags (elementary trees) (Bangalore and Joshi, 1999). A Model TAG supertag encodes the dependency relation and the direction of the head of a word similarly to Model 0 if the dependency relation is non-obligatory (corresponding to adjunction nodes), and the information about obligatory dependents of verbs if any similarly to Model 2 (corresponding to substitution nodes). Table 1: Supertags for the sentence “No, it wasn’t black Monday.” Position Obligatory Parent Optional Parent Obligatory Dep. Optional Dep. Feature Direction Relation Direction Relation Direction Relation Direction 0 ·· ·· 1 ·· ·· · · 2 ·· ·· ·· · TAG ·· ·· 2.2 Motivated by rece"
N19-1075,P18-1192,0,0.376754,"al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM for semantic role labeling. Predicted supertags are represented by real-valued vectors, contrasting with approaches based on syntactic paths (Roth and Lapata, 2016; He et al., 2018) and syntactic edges (Marcheggiani and Titov, 2017; Strubell et al., 2018). This way of incorporating information alleviates the issue of error propagation from parsing. Supertagging has many advantages as part of a natural language processing pipeline. First, as a straightforward sequence-labeling task, the supertagging architecture is much simpler than comparable systems for structured parsing. Second, it is simple to extract different forms of supertags from a dependency corpus to test different hypotheses about which kinds of syntactic information are most useful for downstream tasks. Our"
N19-1075,W09-1206,0,0.273579,"Missing"
N19-1075,D17-1180,1,0.946368,"rt Frank♦ Dragomir Radev♦ Owen Rambow♥ ♣ University of Washington ♠ Google ♦ Yale University ♥ Elemental Cognition, LLP jkasai@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to"
N19-1075,Q17-1010,0,0.219706,"tenation of a dense vector representation of the word, a vector embedding of a predicted PTBstyle POS tag (only for English),2 and a vector output by character-level Convolutional Neural Networks (CNNs) for morphological information. For POS tagging before English supertagging, we use the same hyperparameters as in Ma and Hovy (2016). For supertagging, we follow the hyperparameters chosen in Kasai et al. (2018) regardless of the supertag model that is employed. We initialize the word embeddings by the pretrained 100 dimensional GloVe (Pennington et al., 2014) and the 300 dimensional FastText (Bojanowski et al., 2017) vectors for English and Spanish respectively. Table 2: Supertag models for SRL. Models 1 and 2 are from Ouchi et al. (2014) and Model 0 is from Nguyen and Nguyen (2016). sults show that supertags, by encoding just enough information, can improve SRL performance even compared to systems that incorporate complete dependency parses. 2 Our Models 2.1 Supertagger Model Supertag Design We experiment with four supertag models, two from Ouchi et al. (2014), one from Nguyen and Nguyen (2016), and one of our own design inspired by Tree Adjoining Grammar supertags (Bangalore and Joshi, 1999). Each model"
N19-1075,N18-1107,1,0.861608,"the-art supertaggers (TAG: Kasai et al. (2017, 2018); CCG: Lewis et al. (2016); Xu (2016)), we employ a bi-directional LSTM (BiLSTM) architecture for our supertagging. The input for each word is the conncatenation of a dense vector representation of the word, a vector embedding of a predicted PTBstyle POS tag (only for English),2 and a vector output by character-level Convolutional Neural Networks (CNNs) for morphological information. For POS tagging before English supertagging, we use the same hyperparameters as in Ma and Hovy (2016). For supertagging, we follow the hyperparameters chosen in Kasai et al. (2018) regardless of the supertag model that is employed. We initialize the word embeddings by the pretrained 100 dimensional GloVe (Pennington et al., 2014) and the 300 dimensional FastText (Bojanowski et al., 2017) vectors for English and Spanish respectively. Table 2: Supertag models for SRL. Models 1 and 2 are from Ouchi et al. (2014) and Model 0 is from Nguyen and Nguyen (2016). sults show that supertags, by encoding just enough information, can improve SRL performance even compared to systems that incorporate complete dependency parses. 2 Our Models 2.1 Supertagger Model Supertag Design We exp"
N19-1075,W03-1006,1,0.633442,"ammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM for semantic role labeling. Predicted supertags are represented by real-valued vectors, contrasting with approaches based on syntactic paths (Roth and Lapata, 2016; He et al., 2018) and syntactic edges (Marcheggiani and Titov, 2017; Strubell et al., 2018). This way of incorporating information alleviates the issue of error propagation from parsing. Supertagging has many advantages as part of a nat"
N19-1075,J07-4004,0,0.0694181,"Yale University ♥ Elemental Cognition, LLP jkasai@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and"
N19-1075,P10-1036,0,0.0328416,"ntal Cognition, LLP jkasai@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech"
N19-1075,D15-1112,0,0.203749,"Missing"
N19-1075,D15-1169,0,0.072174,"ngalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM for semantic role labeling. Predicted supertags are represented by real-valued vectors, contrasting with approaches based on syntactic paths (Roth and Lapata, 2016; He et al., 2018) and syntactic edges (Marcheggiani and Titov, 2017; Strubell et al., 2018). This way of incorporating information alleviates the issue of error propagation from parsing. Supertagging has many advantages as part of a natural language process"
N19-1075,W17-6213,1,0.876586,"i♣∗ Dan Friedman♠∗ Robert Frank♦ Dragomir Radev♦ Owen Rambow♥ ♣ University of Washington ♠ Google ♦ Yale University ♥ Elemental Cognition, LLP jkasai@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirection"
N19-1075,N16-1026,0,0.133999,"i@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to anoth"
N19-1075,P16-1101,0,0.0220744,"n 0 ·· ·· 1 ·· ·· · · 2 ·· ·· ·· · TAG ·· ·· 2.2 Motivated by recent state-of-the-art supertaggers (TAG: Kasai et al. (2017, 2018); CCG: Lewis et al. (2016); Xu (2016)), we employ a bi-directional LSTM (BiLSTM) architecture for our supertagging. The input for each word is the conncatenation of a dense vector representation of the word, a vector embedding of a predicted PTBstyle POS tag (only for English),2 and a vector output by character-level Convolutional Neural Networks (CNNs) for morphological information. For POS tagging before English supertagging, we use the same hyperparameters as in Ma and Hovy (2016). For supertagging, we follow the hyperparameters chosen in Kasai et al. (2018) regardless of the supertag model that is employed. We initialize the word embeddings by the pretrained 100 dimensional GloVe (Pennington et al., 2014) and the 300 dimensional FastText (Bojanowski et al., 2017) vectors for English and Spanish respectively. Table 2: Supertag models for SRL. Models 1 and 2 are from Ouchi et al. (2014) and Model 0 is from Nguyen and Nguyen (2016). sults show that supertags, by encoding just enough information, can improve SRL performance even compared to systems that incorporate comple"
N19-1075,D16-1181,0,0.106279,"danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM f"
N19-1075,K17-1041,0,0.545632,"ormation, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system. 1 Introduction Semantic role labeling (SRL) is the task of identifying the semantic relationships between each predicate in a sentence and its arguments (Gildea and Jurafsky, 2002). While early research assumed that SRL models required syntactic information to perform well (Punyakanok et al., 2008), recent work has demonstrated that neural networks can achieve competitive and even state-of-the-art performance without any syntactic information at all (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017). These systems have the benefits of being simpler to implement and performing more robustly on foreign languages and outof-domain data, cases where syntactic parsing is more difficult (Marcheggiani et al., 2017). In this paper, we show that using supertags is an effective middle ground between using full syntactic parses and using no syntactic information ∗ Work partially done at Yale University. 701 Proceedings of NAACL-HLT 2019, pages 701–709 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Token No , it was n’t black Monday M"
N19-1075,W09-1209,0,0.691839,"# Stags 88 220 503 317 Spanish Dev 92.97 90.63 90.08 92.33 ID 92.67 90.37 89.84 92.18 Table 3: Supertagging accuracies for English and Spanish. ID and OOD indicate the in-domain and out-of-domain evaluation data respectively. The # Stags columns show the number of supertags in the corresponding training set. For pre-trained word embeddings, we use the same word embeddings as the ones in Marcheggiani et al. (2017) for English and the 300-dimensional FastText vectors (Bojanowski et al., 2017) for Spanish. We use the predicates predicted by the mate-tools (Bj¨orkelund et al., 2009) (English) and Zhao et al. (2009) (Spanish) system in our models, again following Marcheggiani et al. (2017) to facilitate comparison. Our code is available online for easy replication of our results.4 BiLSTM model, which is our implementation of the syntax-agnostic model in Marcheggiani et al. (2017). We also present results for a BiLSTM model with dropout and highway connections but without supertags (BDH model), to distinguish the effects of supertags from the effects of better LSTM regularization. In every experiment we train the model five times, and present the mean score. Table 4 shows that Model 1 yields the best perf"
N19-1075,D17-1159,0,0.549148,"rove dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM for semantic role labeling. Predicted supertags are represented by real-valued vectors, contrasting with approaches based on syntactic paths (Roth and Lapata, 2016; He et al., 2018) and syntactic edges (Marcheggiani and Titov, 2017; Strubell et al., 2018). This way of incorporating information alleviates the issue of error propagation from parsing. Supertagging has many advantages as part of a natural language processing pipeline. First, as a straightforward sequence-labeling task, the supertagging architecture is much simpler than comparable systems for structured parsing. Second, it is simple to extract different forms of supertags from a dependency corpus to test different hypotheses about which kinds of syntactic information are most useful for downstream tasks. Our reWe introduce a new syntax-aware model for depend"
N19-1075,P15-1109,0,0.0684731,"from syntactic information, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system. 1 Introduction Semantic role labeling (SRL) is the task of identifying the semantic relationships between each predicate in a sentence and its arguments (Gildea and Jurafsky, 2002). While early research assumed that SRL models required syntactic information to perform well (Punyakanok et al., 2008), recent work has demonstrated that neural networks can achieve competitive and even state-of-the-art performance without any syntactic information at all (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017). These systems have the benefits of being simpler to implement and performing more robustly on foreign languages and outof-domain data, cases where syntactic parsing is more difficult (Marcheggiani et al., 2017). In this paper, we show that using supertags is an effective middle ground between using full syntactic parses and using no syntactic information ∗ Work partially done at Yale University. 701 Proceedings of NAACL-HLT 2019, pages 701–709 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Token No"
N19-1075,N19-1392,1,0.881902,"Missing"
N19-1075,P18-2106,0,0.0578599,"Missing"
N19-1075,D14-1162,0,0.0869394,"for our supertagging. The input for each word is the conncatenation of a dense vector representation of the word, a vector embedding of a predicted PTBstyle POS tag (only for English),2 and a vector output by character-level Convolutional Neural Networks (CNNs) for morphological information. For POS tagging before English supertagging, we use the same hyperparameters as in Ma and Hovy (2016). For supertagging, we follow the hyperparameters chosen in Kasai et al. (2018) regardless of the supertag model that is employed. We initialize the word embeddings by the pretrained 100 dimensional GloVe (Pennington et al., 2014) and the 300 dimensional FastText (Bojanowski et al., 2017) vectors for English and Spanish respectively. Table 2: Supertag models for SRL. Models 1 and 2 are from Ouchi et al. (2014) and Model 0 is from Nguyen and Nguyen (2016). sults show that supertags, by encoding just enough information, can improve SRL performance even compared to systems that incorporate complete dependency parses. 2 Our Models 2.1 Supertagger Model Supertag Design We experiment with four supertag models, two from Ouchi et al. (2014), one from Nguyen and Nguyen (2016), and one of our own design inspired by Tree Adjoinin"
N19-1075,N18-1202,0,0.0492232,"We also present results for a BiLSTM model with dropout and highway connections but without supertags (BDH model), to distinguish the effects of supertags from the effects of better LSTM regularization. In every experiment we train the model five times, and present the mean score. Table 4 shows that Model 1 yields the best performance in the English dev set, and thus we only use Model 1 supertags for test evaluation. We primarily show results only with word type embeddings to conduct fair comparisons with prior work, but we also provide results with deep contextual word representations, ELMo (Peters et al., 2018), and compare our results with recent work that utilizes ELMo (He et al., 2018). 5 English in-domain. Table 5 summarizes the results on the English in-domain test set. First, we were able to approximately replicate the results from Marcheggiani et al. (2017). Adding dropout and highway connections to our BiLSTM model improves performance by 0.5 points, to 88.1, and adding supertags improves results even further to 88.6. Our supertag model performs even better than the non-ensemble model in Marcheggiani and Titov (2017), in which the model is given the complete dependency parse of the sentence."
N19-1075,J08-2005,0,0.333326,"ls that use no syntactic information at all. Our local and non-ensemble model achieves state-of-the-art performance on the CoNLL 09 English and Spanish datasets. SRL models benefit from syntactic information, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system. 1 Introduction Semantic role labeling (SRL) is the task of identifying the semantic relationships between each predicate in a sentence and its arguments (Gildea and Jurafsky, 2002). While early research assumed that SRL models required syntactic information to perform well (Punyakanok et al., 2008), recent work has demonstrated that neural networks can achieve competitive and even state-of-the-art performance without any syntactic information at all (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017). These systems have the benefits of being simpler to implement and performing more robustly on foreign languages and outof-domain data, cases where syntactic parsing is more difficult (Marcheggiani et al., 2017). In this paper, we show that using supertags is an effective middle ground between using full syntactic parses and using no syntactic information ∗ Work partially done a"
N19-1075,P16-1113,0,0.592626,"ing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM for semantic role labeling. Predicted supertags are represented by real-valued vectors, contrasting with approaches based on syntactic paths (Roth and Lapata, 2016; He et al., 2018) and syntactic edges (Marcheggiani and Titov, 2017; Strubell et al., 2018). This way of incorporating information alleviates the issue of error propagation from parsing. Supertagging has many advantages as part of a natural language processing pipeline. First, as a straightforward sequence-labeling task, the supertagging architecture is much simpler than comparable systems for structured parsing. Second, it is simple to extract different forms of supertags from a dependency corpus to test different hypotheses about which kinds of syntactic information are most useful for down"
N19-1075,E14-4030,0,\N,Missing
N19-1075,D18-1548,0,\N,Missing
P00-1059,J99-2004,1,\N,Missing
P00-1059,C00-1007,1,\N,Missing
P00-1059,P98-1116,0,\N,Missing
P00-1059,C98-1112,0,\N,Missing
P00-1059,J94-4004,0,\N,Missing
P00-1059,W00-1401,1,\N,Missing
P01-1038,J96-2004,0,0.0127446,"Missing"
P01-1038,J97-4002,1,0.858717,"We collected both negative and positive examples from Sections 5 and 6 of the PTB. The negative examples were collected using a mixture of manual and automatic techniques. First, candidate examples were identified automatically if there were two occurrences of the same verb, separated by fewer than 10 intervening verbs. Then, the collected examples were manually examined to determine whether the two verb phrases had identical meanings or not.3 If not, the examples were eliminated. This yielded 111 negative examples. The positive examples were taken from the corpus collected in previous work (Hardt, 1997). This is a corpus of several hundred examples of VPE from the Treebank, based on their syntactic analysis. VPE is not annotated uniformly in the PTB. We found several different bracketing patterns and searched for these patterns, but one cannot be certain that no other bracketing patterns were used in the PTB. This yielded 15 positive examples in Sections 5 and 6. The negative and positive examples from Sections 5 and 6 – 126 in total – form our basic corpus, which we will refer to as S ECTIONS 5+6. While not pathologically peripheral, VPE is a 3 The proper characterization of the identity co"
P01-1038,P93-1009,0,0.0853681,"Missing"
P01-1038,A92-1006,1,0.717075,"pusbased static analysis of factors affecting VPE. In this section, we take a computational approach. We would like to use a trainable module that learns rules to decide whether or not to perform VPE. Trainable components have the advantage of easily being ported to new domains. For this reason we use the machine learning system Ripper (Cohen, 1996). However, before we can use Ripper, we must discuss the issue of how our new trainable VPE module fits into the architecture of generation. 5.1 VPE in the Generation Architecture Tasks in the generation process have been divided into three stages (Rambow and Korelsky, 1992): the text planner has access only to information about communicative goals, the discourse context, and semantics, and generates a non-linguistic representation of text structure and content. The sentence planner chooses abstract linguistic resources (meaning-bearing lexemes, syntactic constructions) and determines sentence boundaries. It passes an abstract lexico-syntactic specification5 to the Realizer, which inflects, adds function words, and linearizes, thus producing the surface string. The question arises where in this architecture the decision about VPE should be made. We will investiga"
P01-1038,J96-2005,0,0.0328785,"Missing"
P01-1056,P98-1116,0,0.383749,"eering studies of template development and maintenance, this claim is supported by abundant anecdotal evidence. independent. However, the quality of the output for a particular domain, or a particular situation in a dialog, may be inferior to that of a templatebased system without considerable investment in domain-specific rules or domain-tuning of general rules. Furthermore, since rule-based systems use sophisticated linguistic representations, this handcrafting requires linguistic knowledge. Recently, several approaches for automatically training modules of an NLG system have been proposed (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000). These hold the promise that the complex step of customizing NLG systems by hand can be automated, while avoiding the need for tedious hand-crafting of templates. While the engineering benefits of trainable approaches appear obvious, it is unclear whether the utterance quality is high enough. In (Walker et al., 2001) we propose a new model of sentence planning called SP OT. In SP OT, the sentence planner is automatically trained, using feedback from two human judges, to choose the best from among different options for realizing a set of communicative goals"
P01-1056,A97-1039,0,0.530528,"Missing"
P01-1056,J97-1004,0,0.0268803,"ve rule-based sentence planners, and performs as well as the hand-crafted TEMPLATE system, but is more easily and quickly tuned to a new domain: the training materials for the SP OT sentence planner can be collected from subjective judgements from a small number of judges with little or no linguistic knowledge. Previous work on evaluation of natural language generation has utilized three different approaches to evaluation (Mellish and Dale, 1998). The first approach is a subjective evaluation methodology such as we use here, where human subjects rate NLG outputs produced by different sources (Lester and Porter, 1997). Other work has evaluated template-based spoken dialog generation with a task-based approach, i.e. the generator is evaluated with a metric such as task completion or user satisfaction after dialog completion (Walker, 2000). This approach can work well when the task only involves one or two exchanges, when the choices have large effects over the whole dialog, or the choices vary the content of the utterance. Because sentence planning choices realize the same content and only affect the current utterance, we believed it important to get local feedback. A final approach focuses on subproblems o"
P01-1056,W98-1411,0,0.169868,"Missing"
P01-1056,A92-1006,1,0.840926,"e little or no linguistic training is needed to write templates, it is a tedious and time-consuming task: one or more templates must be written for each combination of goals and discourse contexts, and linguistic issues such as subject-verb agreement and determiner-noun agreement must be repeatedly encoded for each template. Furthermore, maintenance of the collection of templates becomes a software engineering problem as the complexity of the dialog system increases.1 The second approach is natural language generation (NLG), which customarily divides the generation process into three modules (Rambow and Korelsky, 1992): (1) Text Planning, (2) Sentence Planning, and (3) Surface Realization. In this paper, we discuss only sentence planning; the role of the sentence planner is to choose abstract lexico-structural resources for a text plan, where a text plan encodes the communicative goals for an utterance (and, sometimes, their rhetorical structure). In general, NLG promises portability across application domains and dialog situations by focusing on the development of rules for each generation module that are general and domain1 Although we are not aware of any software engineering studies of template developm"
P01-1056,N01-1003,1,0.927935,"es. Furthermore, since rule-based systems use sophisticated linguistic representations, this handcrafting requires linguistic knowledge. Recently, several approaches for automatically training modules of an NLG system have been proposed (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000). These hold the promise that the complex step of customizing NLG systems by hand can be automated, while avoiding the need for tedious hand-crafting of templates. While the engineering benefits of trainable approaches appear obvious, it is unclear whether the utterance quality is high enough. In (Walker et al., 2001) we propose a new model of sentence planning called SP OT. In SP OT, the sentence planner is automatically trained, using feedback from two human judges, to choose the best from among different options for realizing a set of communicative goals. In (Walker et al., 2001), we evaluate the performance of the learning component of SP OT, and show that SP OT learns to select sentence plans that are highly rated by the two human judges. While this evaluation shows that SP OT has indeed learned from the human judges, it does not show that using only two human judgments is sufficient to produce more b"
P01-1056,J97-1007,0,0.0160307,"completion (Walker, 2000). This approach can work well when the task only involves one or two exchanges, when the choices have large effects over the whole dialog, or the choices vary the content of the utterance. Because sentence planning choices realize the same content and only affect the current utterance, we believed it important to get local feedback. A final approach focuses on subproblems of natural language generation such as the generation of referring expressions. For this type of problem it is possible to evaluate the generator by the degree to which it matches human performance (Yeh and Mellish, 1997). When evaluating sentence planning, this approach doesn’t make sense because many different realizations may be equally good. However, this experiment did not show that trainable sentence planners produce, in general, better-quality output than template-based or rulebased sentence planners. That would be impossible: given the nature of template and rulebased systems, any quality standard for the output can be met given sufficient person-hours, elapsed time, and software engineering acumen. Our principal goal, rather, is to show that the quality of the TEMPLATE output, for a currently operatio"
P01-1056,C98-1112,0,\N,Missing
P01-1056,W98-1415,0,\N,Missing
P01-1056,P97-1026,0,\N,Missing
P05-1071,N04-4038,0,0.774426,"in One Fell Swoop Nizar Habash and Owen Rambow Center for Computational Learning Systems Columbia University New York, NY 10115, USA {habash,rambow}@cs.columbia.edu Abstract morphological tag, cannot be done successfully using methods developed for English because of data sparseness. Hajiˇc (2000) demonstrates convincingly that morphological disambiguation can be aided by a morphological analyzer, which, given a word without any context, gives us the set of all possible morphological tags. The only work on Arabic tagging that uses a corpus for training and evaluation (that we are aware of), (Diab et al., 2004), does not use a morphological analyzer. In this paper, we show that the use of a morphological analyzer outperforms other tagging methods for Arabic; to our knowledge, we present the best-performing wide-coverage tokenizer on naturally occurring input and the bestperforming morphological tagger for Arabic. We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process. We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries f"
P05-1071,A00-2013,0,0.425099,"Missing"
P05-1071,P03-1004,0,0.0361062,"on (but not from training). In contrast, Diab et al. (2004) treat NO FUNC like any other POS tag, but it is unclear whether this is meaningful. Thus, when comparing results from different approaches which make different choices about the data (for example, the NO FUNC cases), one should bear in mind that small differences in performance are probably not meaningful. Method Test POS Conj Part Pron Det Gen Num Per Voice Asp 5 Classifiers for Linguistic Features We now describe how we train classifiers for the morphological features in Figure 2. We train one classifier per feature. We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding. 6 As training features, we use two sets. These sets are based on the ten morphological features in Figure 2, plus four other “hidden” morphological features, for which we do not train classifiers, but which are represented in the analyses returned by the morphological analyzer. The reason we do not train classifiers for the hidden features is that they are only returned by the morphological analyzer when they are marked overtly in orthography, but they are not disambiguated in case they are not overtly marked. The"
P05-1071,P03-1051,0,0.0576065,"Missing"
P05-1071,P03-1050,0,0.0152138,"Missing"
P05-1071,W02-0506,0,\N,Missing
P06-1086,J94-1003,0,\N,Missing
P06-1086,C88-1064,0,\N,Missing
P06-1086,E87-1002,0,\N,Missing
P06-1086,J00-1006,0,\N,Missing
P06-1086,W05-0703,1,\N,Missing
P06-1086,W02-0506,0,\N,Missing
P06-1086,W98-1007,0,\N,Missing
P06-1086,maamouri-etal-2006-developing,1,\N,Missing
P07-1105,C04-1180,0,0.0573198,"ntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar. 1 Introduction There is considerable interest in learning computational grammars.1 While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto1 This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank"
P07-1105,W05-0602,0,0.0930758,"atural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar. 1 Introduction There is considerable interest in learning computational grammars.1 While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto1 This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank Judith Klavans for her contributions over the course"
P07-1105,P06-1111,0,0.020267,"rning computational grammars.1 While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto1 This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank Judith Klavans for her contributions over the course of this research, Kathy McKeown for her input, and several anonymous reviewers for very useful feedback on earlier drafts of this paper. In this paper, we present a new grammar formalism and a new learning method which together address the problem of learning a syntactic-semantic grammar in the presence of a r"
P07-1105,P99-1013,0,0.846434,"Missing"
P08-2030,N07-2014,1,0.69545,"Missing"
P08-2030,A00-2013,0,0.302047,"Missing"
P08-2030,P06-1073,0,0.106827,"Missing"
P08-2030,P05-1071,1,\N,Missing
P11-1159,P11-2062,1,0.748432,"Missing"
P11-1159,W06-2920,0,0.0813362,"Missing"
P11-1159,P99-1065,0,0.379153,"Missing"
P11-1159,H05-1100,0,0.374859,"Missing"
P11-1159,W07-0802,0,0.34456,"Missing"
P11-1159,J08-3003,0,0.14088,"Missing"
P11-1159,C10-1045,0,0.0649513,"Missing"
P11-1159,P05-1071,1,0.924217,"Missing"
P11-1159,P09-2056,1,0.715528,"s on syntactic structure, such as agreement, which often plays an important role in morphologically rich languages. In this paper, we explore the role of morphological features in parsing Modern Standard Arabic (MSA). For MSA, the space of possible morphological features is fairly large. We determine which morphological features help and why. We also explore going beyond the easily detectable, regular form-based (“surface”) features, by representing functional values for some morphological features. We expect that representing lexical abstracCorpus We use the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009). Specifically, we use the portion converted automatically from part 3 of the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information. CATiB’s dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations. It has a reduced POS tagset (with six tags only – henceforth CATIB 6), but a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively, (whether they appear preor post-verbally); IDF for the idaf"
P11-1159,P98-1080,0,0.260714,"Missing"
P11-1159,W10-1402,1,0.467971,"h are extremely rare (in our training corpus of about 300,000 words, we encounter only 430 of such POS tags with complete morphology). Therefore, researchers have proposed tagsets for MSA whose size is similar to that of the English PTB tagset, as this has proven to be a useful size computationally. These tagsets are hybrids in the sense that they are neither simply the core POS, nor the complete morphologically enriched tagset, but instead they selectively enrich the core POS tagset with only certain morphological features. A full dicussion of how these tagsets affect parsing is presented in Marton et al. (2010); we summarize the main points here. The following are the various tagsets we use in this paper: (a) the core POS tagset CORE 12; (b) the CATiB treebank tagset CATIB E X, a newly introduced extension of CATIB 6 (Habash and Roth, 2009) by simple regular expressions of the word form, indicating particular morphemes such as the +wn; this tagset prefix È@ Al+ or the suffix àð is the best-performing tagset for Arabic on predicted values. (c) the PATB full tagset (BW), size ≈2000+ (Buckwalter, 2004); We only discuss here the best performing tagsets (on predicted values), and BW for comparison. 1589"
P11-1159,nilsson-nivre-2008-malteval,0,0.0616297,"bination of lexical and inflectional features (Section 5.3); an extension of the DET feature (Section 5.4); using functional NUMBER and GENDER feature values, as well as the RATIONALITY feature (Section 5.5); finally, putting best feature combinations to test with the best-performing POS tagset, and on an unseen test set (Section 5.6). All results are reported mainly in terms of labeled attachment accuracy score (parent word and the dependency relation to it, a.k.a. L AS). Unlabeled attachment accuracy score (UAS) is also given. We use McNemar’s statistical significance test as implemented by Nilsson and Nivre (2008), and denote p &lt; 0.05 and p &lt; 0.01 with + and ++ , respectively. 5.1 Parser For all experiments reported here we used the syntactic dependency parser MaltParser v1.3 (Nivre, 2003; Nivre, 2008; Kübler et al., 2009) – a transition-based parser with an input buffer and a stack, using SVM classifiers to predict the next state in the parse derivation. All experiments were done using the Nivre &quot;eager&quot; algorithm.6 For training, de5 In this paper, we do not examine the contribution of different POS tagsets, see Marton et al. (2010) for details. 6 Nivre (2008) reports that non-projective and pseudoproj"
P11-1159,C08-1081,0,0.57356,"Missing"
P11-1159,W03-3017,0,0.0771246,"re (Section 5.5); finally, putting best feature combinations to test with the best-performing POS tagset, and on an unseen test set (Section 5.6). All results are reported mainly in terms of labeled attachment accuracy score (parent word and the dependency relation to it, a.k.a. L AS). Unlabeled attachment accuracy score (UAS) is also given. We use McNemar’s statistical significance test as implemented by Nilsson and Nivre (2008), and denote p &lt; 0.05 and p &lt; 0.01 with + and ++ , respectively. 5.1 Parser For all experiments reported here we used the syntactic dependency parser MaltParser v1.3 (Nivre, 2003; Nivre, 2008; Kübler et al., 2009) – a transition-based parser with an input buffer and a stack, using SVM classifiers to predict the next state in the parse derivation. All experiments were done using the Nivre &quot;eager&quot; algorithm.6 For training, de5 In this paper, we do not examine the contribution of different POS tagsets, see Marton et al. (2010) for details. 6 Nivre (2008) reports that non-projective and pseudoprojective algorithms outperform the &quot;eager&quot; projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. The Nivre &quot;standard&quot; algorithm"
P11-1159,J08-4003,0,0.548223,"r verbs) outperforms other combinations. Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima’an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing. As for work on Arabic, results have been reported on PATB (Kulick et al., 2006; Diab, 2007; Green and Manning, 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi, 2006; Nivre, 2008) and the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009). Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short Idafa constructions and verbal or equational clauses. Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al., 2007), trained on the PADT. His results are not directly comparable to ours because of the different treebanks’ representations, even though all the experiments reported here were performed using MaltParser. Our results agr"
P11-1159,P08-2030,1,0.896384,"Missing"
P11-1159,W07-2219,0,0.17495,"Missing"
P11-1159,C98-1077,0,\N,Missing
P11-1159,J08-4010,0,\N,Missing
P12-2032,P11-1078,0,0.0588676,"e SNA-based systems outperforms the NLP-based system. 2 Work on Enron Hierarchy Prediction The Enron email corpus was introduced by Klimt and Yang (2004). Since then numerous researchers have analyzed the network formed by connecting people with email exchange links (Diesner et al., 2005; Shetty and Adibi, 2004; Namata et al., 2007; Rowe et al., 2007; Diehl et al., 2007; Creamer et al., 2009). Rowe et al. (2007) use the email exchange network (and other features) to predict the dominance relations between people in the Enron email corpus. They however do not present a quantitative evaluation. Bramsen et al. (2011b) and Gilbert (2012) present NLP based models to predict dominance relations between Enron employees. Neither the test-set nor the system of Bramsen et al. (2011b) is publicly available. Therefore, we compare our baseline SNA based system with that of Gilbert (2012). Gilbert (2012) produce training and test data as follows: an email message is labeled upward only when every recipient outranks the sender. An email message is labeled not-upward only when every recipient does not outrank the sender. They use an n-gram based model with Support Vector Machines (SVM) to predict if an email is of cl"
P14-1127,P01-1005,0,0.0882182,"Missing"
P14-1127,D10-1056,0,0.0224652,"The size of the languages (|L|) suggests that we are suffering from vast overgeneration; we overgenerate because in our model any affix can attach to any stem, which is not in general true. Thus there is a lack of linguistic knowledge such as paradigm information (Stump, 2001) for each word category in our model. In other words, all morphemes are treated the same in our model which is not true in natural languages. One way to tackle this problem is through an unsupervised POS tagger. The challenge here is that fully unsupervised POS taggers (without any tag dictionary) are not very accurate (Christodoulopoulos et al., 2010). Another way is through using joint mor1355 Figure 3: Trends for token-based OOV reduction with different sizes for the Fixed Affix model with trigraph reranking. phology and tagging models such as Frank et al. (2013). Language |pr ||stm ||sf| |L| |If| Assamese 4 4791 564 10.8M 1.8 Bengali 3 6496 378 7.4M 1.5 Pashto 1 5395 271 1.5M 1.3 Persian 49 6998 538 184M 2.0 Tagalog 179 4259 299 228M 1.5 Turkish 45 5266 1801 427M 2.3 Zulu 2254 5680 427 5.5B 2.8 Persian-N 3 6121 268 4.9M 1.5 Persian-V 43 788 44 1.5M 3.4 Table 4: Information about the number of unique morphemes in the Fixed Affix model fo"
P14-1127,clement-etal-2004-morphology,0,0.276789,"Missing"
P14-1127,W02-2006,0,0.208519,"um, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike"
P14-1127,E12-1066,0,0.0832502,"Missing"
P14-1127,D11-1057,0,0.0467994,"mplete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike these approaches which provide segmentations for training data and produce models that can be used to segment unseen words, our approach can generate words that have not been seen in the training data. The focus of efforts i"
P14-1127,N13-1138,0,0.0197157,"eeded, and the amount of machine learning used. At one extreme, we find systems that are painstakingly and carefully designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012). Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we a"
P14-1127,D13-1105,1,0.820969,"achine learning used. At one extreme, we find systems that are painstakingly and carefully designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012). Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from t"
P14-1127,D13-1004,0,0.0573526,"Missing"
P14-1127,P06-1086,1,0.870002,"aining data. The paper is structured as follows. We first discuss related work in Section 2. We then present our method in Section 3, and present experimental results in Section 4. We conclude with a discussion of future work in Section 5. 2 Related Work Approaches to Morphological Modeling Computational morphology is a very active area of research with a multitude of approaches that vary in the degree of manual annotation needed, and the amount of machine learning used. At one extreme, we find systems that are painstakingly and carefully designed by hand (Koskenniemi, 1983; Buckwalter, 2004; Habash and Rambow, 2006; D´etrez and Ranta, 2012). Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphologica"
P14-1127,P08-2015,1,0.78393,"e pronunciation lexicon via generating all possible pronunciations for a word before lattice generation and indexation. There are also other methods for generating abbreviations in voice search systems such as Yang et al. (2012). While all of these approaches involve lexicon expansion, they do not employ any morphological information. In the context of MT, several researchers have addressed the problem of OOV words by relating them to known in-vocabulary (INV) words. Yang and Kirchhoff (2006) anticipated OOV words that are potentially morphologically related using phrase-based backoff models. Habash (2008) considered different techniques for vocabulary expansion online. One of their techniques learned models of morphological mapping between morphologically rich source words in Arabic that produce the same English translation. This was used to relate an OOV word to a morphologically related INV word. Another technique expanded the MT phrase tables with possible transliterations and spelling alternatives. 3 segmented stems and affixes. We explore two different techniques for morphology-based vocabulary expansion that we discuss below. The output of these models is represented as a weighted finite"
P14-1127,W02-0604,0,0.0542663,"on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike these approaches which"
P14-1127,J96-1003,0,0.148277,"le 4: Information about the number of unique morphemes in the Fixed Affix model for each dataset including empty affixes. |L |shows the upper bound of the number of possible unique words that can be generated from the word generation model. |If |is the average number of unique prefix-suffix pairs (including empty pairs) for each stem. Error Analysis on Turkish Unfortunately for most languages we could not find an available rule-based or supervised morphological analyzer to verify the words generated by our model. The only available tool for us is a Turkish finite-state morphological analyzer (Oflazer, 1996) implemented with the Xerox FST toolkit (Beesley and Karttunen, 2003). As we can see in Table 5, the system with the largest proportion of correct generated words reranks the expansion with trigraph probabilities using a Fixed Affix model. Results also show that we are overgenerating many nonsense words that we ought to be pruning from our results. Another observation is that the recognition percentage of the morphological analyzer on INV words is much higher than on OOVs, which shows that OOVs in Turkish dataset are much harder to analyze. 1356 Tr. WFST Model Fixed Affix Model Bigram Affix Mo"
P14-1127,N13-1031,1,0.785358,"-babel103bv0.4b), Pashto (IARPA-babel104b-v0.4bY), Tagalog (IARPA-babel106-v0.2g), Turkish (IARPAbabel105b-v0.4) and Zulu (IARPA-babel206bv0.1e). Speech annotation such as silences and hesitations are removed from transcription and all words are turned into lower-case (for languages using the Roman script – Tagalog, Turkish and Zulu). Moreover, in order to be able to perform a manual error analysis, we include a language that has rich morphology and of which the first author is a native speaker: Persian. We sampled data from the training and development set of the Persian dependency treebank (Rasooli et al., 2013) to create a comparable seventh dataset in Persian. Statistics about the datasets are shown in Table 1. We also conduct further experiments on just verbs and nouns in the data set for Persian (Persian-N and Persian V). As shown in Table 1, the training data is very small and the OOV rate is high especially in terms of types. For some languages that have richer morphology such as Turkish and Zulu, the OOV rate is much higher than other languages. Word Generation Tools and Settings For unsupervised learning of morphology, we use Morfessor CAT-MAP (v. 0.9.2) which was shown to be a very accurate"
P14-1127,N09-4005,0,0.0633433,"Missing"
P14-1127,Q13-1021,0,0.0255187,"lly supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike these approaches which provide segmentations for training data and produce models that can be used to segment unseen words, our approach can generate words that have not been seen in the training data. The focus of efforts is rather complementary: we a"
P14-1127,P08-1084,0,0.0566489,"al models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008). Unlike these approaches which provide segmentations for tr"
P14-1127,E06-1006,0,0.0347412,"ary to build an expanded lexicon. Word recognition is done in the second run based on the lexicon. Lei et al. (2009) expanded the pronunciation lexicon via generating all possible pronunciations for a word before lattice generation and indexation. There are also other methods for generating abbreviations in voice search systems such as Yang et al. (2012). While all of these approaches involve lexicon expansion, they do not employ any morphological information. In the context of MT, several researchers have addressed the problem of OOV words by relating them to known in-vocabulary (INV) words. Yang and Kirchhoff (2006) anticipated OOV words that are potentially morphologically related using phrase-based backoff models. Habash (2008) considered different techniques for vocabulary expansion online. One of their techniques learned models of morphological mapping between morphologically rich source words in Arabic that produce the same English translation. This was used to relate an OOV word to a morphologically related INV word. Another technique expanded the MT phrase tables with possible transliterations and spelling alternatives. 3 segmented stems and affixes. We explore two different techniques for morphol"
P14-1127,P00-1027,0,0.0999458,"anta, 2012). Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text (Cl´ement et al., 2004; Forsberg et al., 2006). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information (Durrett and DeNero, 2013; Eskander et al., 2013). Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations (Yarowsky and Wicentowski, 2000; Cucerzan and Yarowsky, 2002; Neuvel and Fulop, 2002; Snyder and Barzilay, 2008). At the other extreme, we find unsupervised methods that learn morphology models from unannotated data (Creutz and Lagus, 2007; Monson et al., 2008; Dreyer and Eisner, 2011; Sirts and Goldwater, 2013). The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specifically. First, consider work in automatic morphological segmentation learning from unannotated data (Creutz and Lagus, 2007;"
P14-2056,P12-2032,1,0.844306,"Missing"
P14-2056,N13-1099,1,0.91251,"Missing"
P14-2056,I13-1025,1,0.865401,"irection of power; our new features significantly improve the results over using previously proposed features. 1 Introduction Computationally analyzing the social context in which language is used has gathered great interest within the NLP community recently. One of the areas that has generated substantial research is the study of how social power relations between people affect and/or are revealed in their interactions with one another. Researchers have proposed systems to detect social power relations between participants of organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online forums (Danescu-NiculescuMizil et al., 2012; Biran et al., 2012; DanescuNiculescu-Mizil et al., 2013), chats (Strzalkowski et al., 2012), and off-line interactions such as presidential debates (Prabhakaran et al., 2013; Nguyen et al., 2013). Automatically identifying power and influence from interactions can have many practical applications ranging from law enforcement and intelligence to online marketing. A significant number of these studies are performed in the domain of organizational email where there is a well defined notion of power (organizational hierarchy). Bramsen et al. (2"
P14-2056,W12-2105,1,0.675625,"eviously proposed features. 1 Introduction Computationally analyzing the social context in which language is used has gathered great interest within the NLP community recently. One of the areas that has generated substantial research is the study of how social power relations between people affect and/or are revealed in their interactions with one another. Researchers have proposed systems to detect social power relations between participants of organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online forums (Danescu-NiculescuMizil et al., 2012; Biran et al., 2012; DanescuNiculescu-Mizil et al., 2013), chats (Strzalkowski et al., 2012), and off-line interactions such as presidential debates (Prabhakaran et al., 2013; Nguyen et al., 2013). Automatically identifying power and influence from interactions can have many practical applications ranging from law enforcement and intelligence to online marketing. A significant number of these studies are performed in the domain of organizational email where there is a well defined notion of power (organizational hierarchy). Bramsen et al. (2011) and Gilbert (2012) predict hierarchical power relations between peo"
P14-2056,N12-1057,1,0.887992,"Missing"
P14-2056,P11-1078,0,0.24877,"ised learning system to predict the direction of power; our new features significantly improve the results over using previously proposed features. 1 Introduction Computationally analyzing the social context in which language is used has gathered great interest within the NLP community recently. One of the areas that has generated substantial research is the study of how social power relations between people affect and/or are revealed in their interactions with one another. Researchers have proposed systems to detect social power relations between participants of organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online forums (Danescu-NiculescuMizil et al., 2012; Biran et al., 2012; DanescuNiculescu-Mizil et al., 2013), chats (Strzalkowski et al., 2012), and off-line interactions such as presidential debates (Prabhakaran et al., 2013; Nguyen et al., 2013). Automatically identifying power and influence from interactions can have many practical applications ranging from law enforcement and intelligence to online marketing. A significant number of these studies are performed in the domain of organizational email where there is a well defined notion of power"
P14-2056,I13-1042,1,0.883912,"Missing"
P14-2056,P13-1025,0,0.151807,"Missing"
P14-2056,C12-1155,0,0.108838,"ing the social context in which language is used has gathered great interest within the NLP community recently. One of the areas that has generated substantial research is the study of how social power relations between people affect and/or are revealed in their interactions with one another. Researchers have proposed systems to detect social power relations between participants of organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online forums (Danescu-NiculescuMizil et al., 2012; Biran et al., 2012; DanescuNiculescu-Mizil et al., 2013), chats (Strzalkowski et al., 2012), and off-line interactions such as presidential debates (Prabhakaran et al., 2013; Nguyen et al., 2013). Automatically identifying power and influence from interactions can have many practical applications ranging from law enforcement and intelligence to online marketing. A significant number of these studies are performed in the domain of organizational email where there is a well defined notion of power (organizational hierarchy). Bramsen et al. (2011) and Gilbert (2012) predict hierarchical power relations between people in the Enron email corpus using lexical features extracted from all t"
P14-5009,J09-3007,0,0.41745,"Missing"
P14-5009,W02-1503,0,0.0681326,"swers to produce a PC-PATR grammar (McConnel, 1995). The LinGO Grammar Matrix (Bender et al., 2002) is a similar tool developed for HPSG that uses a type hierarchy to represent cross-linguistic generalizations. The most commonly used resource for formally documenting semantics across languages is FrameNet (Filmore et al., 2003). FrameNets have been developed for many languages, including Spanish, Japanese, and Portuguese. Most start with English FrameNet and adapt it for the new language; a large portion of the frames end up being substantially the same across languages (Baker, 2008). ParSem (Butt et al., 2002) is a Vignette Semantics and VigNet To interpret input text, WordsEye uses a lexical resource called VigNet (Coyne et al., 2011a). VigNet is inspired by and based on FrameNet (Baker et al., 1998), a resource for lexical semantics. In FrameNet, lexical items are grouped together in frames according to their shared semantic structure. Every frame contains a number of frame elements (semantic roles) which are participants in this structure. The English FrameNet defines the mapping between syntax and semantics for a lexical item by providing lists of valence patterns that map syntactic functions t"
P14-5009,W11-0905,1,0.947055,"erring to objects. The system assembles scenes from a library of 2,500 3D objects and 10,000 images tied to an English lexicon of about 15,000 nouns. The system includes a user interface where the user can type simple sentences that are processed to produce a 3D scene. The user can then modify the text to refine the scene. In addition, individual objects and their parts can be selected and highlighted with a bounding box to focus attention. Several thousand real-world people have used WordsEye online (http://www.wordseye.com). It has also been used as a tool in education, to enhance literacy (Coyne et al., 2011b). In this paper, we describe how we are using WordsEye to create a comprehensive tool for field linguistics. Related Work One of the most widely-used computer toolkits for field linguistics is SIL Fieldworks. FieldWorks is a collection of software tools; the most relevant for our research is FLEx, Fieldworks Language Explorer. FLEx includes tools for eliciting and recording lexical information, dictionary development, interlinearization of texts, analysis of discourse features, and morphological analysis. An important part of FLEx is its “linguist-friendly” morphological parser (Black and Si"
P14-5009,P98-1013,0,0.0594057,"generalizations. The most commonly used resource for formally documenting semantics across languages is FrameNet (Filmore et al., 2003). FrameNets have been developed for many languages, including Spanish, Japanese, and Portuguese. Most start with English FrameNet and adapt it for the new language; a large portion of the frames end up being substantially the same across languages (Baker, 2008). ParSem (Butt et al., 2002) is a Vignette Semantics and VigNet To interpret input text, WordsEye uses a lexical resource called VigNet (Coyne et al., 2011a). VigNet is inspired by and based on FrameNet (Baker et al., 1998), a resource for lexical semantics. In FrameNet, lexical items are grouped together in frames according to their shared semantic structure. Every frame contains a number of frame elements (semantic roles) which are participants in this structure. The English FrameNet defines the mapping between syntax and semantics for a lexical item by providing lists of valence patterns that map syntactic functions to frame elements. VigNet extends FrameNet in two ways in order to capture “graphical semantics’,’ the knowledge needed to generate graphical scenes from language. First, graphical semantics are a"
P14-5009,C98-1013,0,\N,Missing
P15-5003,E09-1004,0,0.0116512,"some time, most research using it does not make use of many of its features. We believe this is because the MPQA annotation is quite complex and requires a deeper understanding of the phenomenon of “private state”, which is what the annotation is 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11, c Beijing, China, Jul"
P15-5003,D08-1083,0,0.0160594,"07) has been around for some time, most research using it does not make use of many of its features. We believe this is because the MPQA annotation is quite complex and requires a deeper understanding of the phenomenon of “private state”, which is what the annotation is 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11"
P15-5003,E14-1040,1,0.840074,"rsity of Pittsburgh. She has worked on issues related to private states for some time, originally in the context of tracking point of view in narrative (Wiebe, 1994), and later in the context of recognizing sentiment in other genres such as news articles (Wilson et al., 2005). She has approached the area from the perspective of corpus annotation (Wiebe et al., 2005; Deng et al., 2013), lexical semantics (Wiebe and Mihalcea, 2006), and discourse (Somasundaran et al., 2009). In addition to continuing these lines of research, she has recently begun investigating implicatures in opinion analysis (Deng and Wiebe, 2014). She has received funding for her research from NSF, NIH, DARPA, ONR, NSA, ARDA, and Homeland Security. She was Program Chair of NAACL 2000 and Program Co-Chair of ACL-IJCNLP 2009. She has been on the editorial board of Computational Linguistics and is currently an action editor for Transactions of the ACL. http://people. cs.pitt.edu/˜wiebe/ 3. Break (15 minutes) 4. Representing belief: a presentation of FactBank, the LU corpus, and the ongoing LDC annotation under the DARPA DEFT program. (30 minutes) 5. Integration and looking forward: a discussion of how sentiment and belief interact, and h"
P15-5003,P13-2022,1,0.835708,"Missing"
P15-5003,W09-3012,1,0.867045,"tes with other types of attitudes. This tutorial will present the original MPQA annotation scheme (V2) and its recent extension to include eTarget annotations (V3), which we believe is a valuable new resource for the community. Pustejovsky, 2009), which represents the source of the belief, the target, the strength, and the polarity (using a system of 10 tags which cover strength and polarity). Following (Wiebe et al., 2005), the sources are nested, reflecting the same nesting of private states we also observe for sentiment. FactBank is a rich and complex annotation; the so-called LU corpus of Diab et al. (2009) was created independently, and represents a subset of the annotations of FactBank. The LU corpus annotates only the writer’s belief in the propositions in the text, only distinguishes 3 types of belief, but does clearly represent the target. Unlike FactBank, which is annotated on top of the Penn Treebank, the LU corpus represents a diverse set of texts. The recent annotations at the LDC for the DARPA DEFT project follow the simplicity of the LU corpus annotations, but extend the tagset of the LU corpus to four tags. An annotation effort in the spring of 2015 will include the source of the bel"
P15-5003,P07-1055,0,0.0253856,"corpus (Wiebe et al., 2005; Wilson, 2007) has been around for some time, most research using it does not make use of many of its features. We believe this is because the MPQA annotation is quite complex and requires a deeper understanding of the phenomenon of “private state”, which is what the annotation is 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of t"
P15-5003,W02-1011,0,0.0165495,"ns that have been made. The issue of annotation is crucial for private states: while the MPQA corpus (Wiebe et al., 2005; Wilson, 2007) has been around for some time, most research using it does not make use of many of its features. We believe this is because the MPQA annotation is quite complex and requires a deeper understanding of the phenomenon of “private state”, which is what the annotation is 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive"
P15-5003,C10-2117,1,0.895857,"Missing"
P15-5003,W12-3807,1,0.909088,"Missing"
P15-5003,W93-0227,1,0.549478,"Missing"
P15-5003,P11-1138,0,0.0641946,"Missing"
P15-5003,D09-1018,1,0.795945,"argets). (45 minutes) Janyce Wiebe Janyce Wiebe is Professor of Computer Science and Professor and Co-Director of the Intelligent Systems at the University of Pittsburgh. She has worked on issues related to private states for some time, originally in the context of tracking point of view in narrative (Wiebe, 1994), and later in the context of recognizing sentiment in other genres such as news articles (Wilson et al., 2005). She has approached the area from the perspective of corpus annotation (Wiebe et al., 2005; Deng et al., 2013), lexical semantics (Wiebe and Mihalcea, 2006), and discourse (Somasundaran et al., 2009). In addition to continuing these lines of research, she has recently begun investigating implicatures in opinion analysis (Deng and Wiebe, 2014). She has received funding for her research from NSF, NIH, DARPA, ONR, NSA, ARDA, and Homeland Security. She was Program Chair of NAACL 2000 and Program Co-Chair of ACL-IJCNLP 2009. She has been on the editorial board of Computational Linguistics and is currently an action editor for Transactions of the ACL. http://people. cs.pitt.edu/˜wiebe/ 3. Break (15 minutes) 4. Representing belief: a presentation of FactBank, the LU corpus, and the ongoing LDC a"
P15-5003,H05-1116,1,0.62816,"ncreasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Or, to augment an automatic wikification system (Ratinov et al., 2011), which could include information about whom or what the subject supports or opposes. A recent NIST evaluation – The Knowledge Base Population (KBP) Sentiment track1 — aims at using corpora to collect information regarding sentiments expressed toward or by named entities. Annotated corpora of reviews (Hu and Liu, 2004; Titov and McDonald, 200"
P15-5003,P08-1036,0,0.0201005,"many of its features. We believe this is because the MPQA annotation is quite complex and requires a deeper understanding of the phenomenon of “private state”, which is what the annotation is 1.2 Current Work on Annotating Sentiment To date, the computational analyses of sentiment are often fairly superficial. Much work in sentiment analysis and opinion mining is at the document level (Pang et al., 2002). There is increasing interest in more fine-grained levels: sentence-level (McDonald et al., 2007), phrase-level (Choi and Cardie, 2008; Agarwal et al., 2009), aspect-level (Hu and Liu, 2004; Titov and McDonald, 2008), etc. Sentiments toward entities and events (“eTargets”) expressed in blogs, newswire, editorials, etc. are particularly important. A system that could recognize sentiments toward entities and events would be valuable in an application such as Automatic Question Answering, to support answering questions such as “Toward whom/what is X negative/positive?” “Who is negative/positive toward X?” (Stoyanov et al., 2005). 7 Proceedings of the Tutorials of the 53rd Annual Meeting of the ACL and the 7th IJCNLP, pages 7–11, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguisti"
P15-5003,W94-0320,1,0.593979,"Missing"
P15-5003,P06-1134,1,0.736603,"MPQA Version 3 (extension of MPQA V2 to eTargets). (45 minutes) Janyce Wiebe Janyce Wiebe is Professor of Computer Science and Professor and Co-Director of the Intelligent Systems at the University of Pittsburgh. She has worked on issues related to private states for some time, originally in the context of tracking point of view in narrative (Wiebe, 1994), and later in the context of recognizing sentiment in other genres such as news articles (Wilson et al., 2005). She has approached the area from the perspective of corpus annotation (Wiebe et al., 2005; Deng et al., 2013), lexical semantics (Wiebe and Mihalcea, 2006), and discourse (Somasundaran et al., 2009). In addition to continuing these lines of research, she has recently begun investigating implicatures in opinion analysis (Deng and Wiebe, 2014). She has received funding for her research from NSF, NIH, DARPA, ONR, NSA, ARDA, and Homeland Security. She was Program Chair of NAACL 2000 and Program Co-Chair of ACL-IJCNLP 2009. She has been on the editorial board of Computational Linguistics and is currently an action editor for Transactions of the ACL. http://people. cs.pitt.edu/˜wiebe/ 3. Break (15 minutes) 4. Representing belief: a presentation of Fac"
P15-5003,C90-2069,1,0.440593,"larger goal) and the technical details of the annotations (achieving the immediate goal). 1.1 Introduction Over the last ten years, there has been an explosion in interest in sentiment analysis, with many interesting and impressive results. For example, the first twenty publications on Google Scholar returned for the Query “sentiment analysis” all date from 2003 or later, and have a total citation count of 12,140. The total number of publications is in the thousands. Partly, this interest is driven by the immediate commercial applications of sentiment analysis. Sentiment is a “private state” (Wiebe, 1990). However, it is not the only private state that has received attention in the computational literature; others include belief and intention. In this tutorial, we propose to provide a deeper understanding of what a private state is. We will concentrate on sentiment and belief. We will provide background that will allow the tutorial participants to understand the notion of a private state as a cognitive phenomenon, which can be manifested linguistically in communication in various ways. We will explain the formalization in terms of a triple of state, source, and target. We will discuss how to m"
P15-5003,J94-2004,1,0.272914,"eridicity (Karttunen, 1971) and modality), and cognitive science. (45 minutes) 3.2 2. Representing sentiment: a presentation of early work, of MPQA V2 (with nested sources, and attitude, expressive-subjective element, and target span annotations), and of MPQA Version 3 (extension of MPQA V2 to eTargets). (45 minutes) Janyce Wiebe Janyce Wiebe is Professor of Computer Science and Professor and Co-Director of the Intelligent Systems at the University of Pittsburgh. She has worked on issues related to private states for some time, originally in the context of tracking point of view in narrative (Wiebe, 1994), and later in the context of recognizing sentiment in other genres such as news articles (Wilson et al., 2005). She has approached the area from the perspective of corpus annotation (Wiebe et al., 2005; Deng et al., 2013), lexical semantics (Wiebe and Mihalcea, 2006), and discourse (Somasundaran et al., 2009). In addition to continuing these lines of research, she has recently begun investigating implicatures in opinion analysis (Deng and Wiebe, 2014). She has received funding for her research from NSF, NIH, DARPA, ONR, NSA, ARDA, and Homeland Security. She was Program Chair of NAACL 2000 and"
P15-5003,H05-1044,1,0.0357036,"iment: a presentation of early work, of MPQA V2 (with nested sources, and attitude, expressive-subjective element, and target span annotations), and of MPQA Version 3 (extension of MPQA V2 to eTargets). (45 minutes) Janyce Wiebe Janyce Wiebe is Professor of Computer Science and Professor and Co-Director of the Intelligent Systems at the University of Pittsburgh. She has worked on issues related to private states for some time, originally in the context of tracking point of view in narrative (Wiebe, 1994), and later in the context of recognizing sentiment in other genres such as news articles (Wilson et al., 2005). She has approached the area from the perspective of corpus annotation (Wiebe et al., 2005; Deng et al., 2013), lexical semantics (Wiebe and Mihalcea, 2006), and discourse (Somasundaran et al., 2009). In addition to continuing these lines of research, she has recently begun investigating implicatures in opinion analysis (Deng and Wiebe, 2014). She has received funding for her research from NSF, NIH, DARPA, ONR, NSA, ARDA, and Homeland Security. She was Program Chair of NAACL 2000 and Program Co-Chair of ACL-IJCNLP 2009. She has been on the editorial board of Computational Linguistics and is c"
P94-1036,E93-1042,0,0.0163046,"the multiset of active dominance requirements of p, and by .l-i(p) the multiset of passive dominance requirements of Bi, 1 &lt; i &lt; n. Add to P the following production: A T(p) Related Formalisms Based on word-order facts from Turkish, Hoffman (1992) proposes an extension to CCG called {}-CCG, in which arguments of functors form sets, rather than being represented in a curried notation. Under function composition, these sets are unioned. Thus the move from CCG to {}-CCG corresponds very much to the move from LIG to {}-LIG. We conjecture that (a version of) {}-CCG is weakly equivalent to {}-LIG. Staudacher (1993) defines a related system called distributed index g r a m m a r or DIG. DIG is like LIG, except that the stack of index symbols can be split into chunks and distributed among the daughter nodes. However, the formalism is not convincingly motivated by the linguistic data given (which can also be handled by a simple LIG) or by other considerations. Several extensions to {}-LIG and UVG-DL are defined in (Rambow, 1994). First, we can introduce the &quot;integrity&quot; constraint suggested by Becker et al. (1991) which restricts long-distance relations through nodes. This is necessary to implement the ling"
P94-1036,J92-4004,0,0.520678,"rmalisms have been shown to provide adequate formal power for a wide range of linguistic phenomena (including the aforementioned Swiss German construction), the need for other mathematical formalisms has arisen in several unrelated areas. In this paper, we discuss three such cases. First, capturing several semantic and syntactic issues in unificationbased formalisms leads to the use of multiset-valued feature structures. Second, word order facts from languages such as German, Russian, or Turkish cannot be derived by LIG-equivalent formalisms. Third, a generalization of trees to &quot;quasi-trees&quot; (Vijay-Shanker, 1992) in the spirit of D-Theory (Marcus et al., 1983) leads to the definition of a new formal system. In this paper, we introduce two new equivalent mathematical formalisms which provide adequate descriptions for these three phenomena. The paper is structured as follows. First, we present the three phenomena in more detail. We then introduce multiset-valued LIG and present some formal properties. Thereafter, we introduce a second rewriting system and show that it is weakly equivalent to the LIG variant. We then briefly mention some related formalisms. We conclude with a brief summary. This paper de"
P94-1036,P87-1015,0,0.0407265,"r, 1985). Several mathematical models have been proposed which extend the formal power of CFGs, while still maintaining the formal properties that make CFGs attractive formalisms for formal and computational linguists, in particular, polynomial parsability and restricted weak generative capacity. These mathematical models include tree adjoining grammar (TAG) (Joshi et al., 1975; Joshi, 1985), head grammar (Pollard, 1984), combinatory categorial grammar (CCG) (Steedman, 1985), and linear index grammar (LIG) (Gazdar, 1988). These formalisms have been shown to be weakly equivalent to each other (Vijay-Shanker et al., 1987; Vijay-Shanker and Weir, 1994); we will refer to them as &quot;LIG-equivalent formalisms&quot;. LIG is a variant of index grammar (IG) (Aho, 1968). Like CFG, IG is a context-free string rewriting system, except that the nonterminal symbols in a CFG are augmented with stacks of index symbols. The rewrite rules push or pop indices from the index stack. In an IG, the index stack is copied to all nonterminal symbols on the right-hand side of a rule. In a LIG, the stack is copied to exactly one right-hand side nonterminal. 1 Three Problems for LIG-Equivalent Formalisms The three problems we present are of a"
P94-1036,E91-1005,1,0.801086,"ers for existing HPSG grammars. For example, it has been proposed that HPSG grammars can be &quot;compiled&quot; into TAGs in order to obtain a computationally more tractable system (Kasper, 1992), thus sidestepping the issue of building parsers for HPSG directly. However, LIG-equivalent formalisms cannot serve as targets for compilations in cases in which HPSG uses multiset-valued feature structures. Word Order ... that so-far, no-one yet has promised the client to repair the refrigerator Similar data has been observed in the literature for other languages, for example for Finnish by Karttunen (1989). Becker et al. (1991) argue that a simple TAG (and the other LIG-equivalent formalisms) cannot derive the full range of scrambled sentences. Rambow and Satta (1994) propose the use of unordered vector grammar (UVG) to model the data. In UVG (Cremers and Mayer, 1973), several context-free string rewriting rules are grouped into vectors, as for verspricht &apos;promises&apos;: (3) ((S --+ NPnom VP), (VP -4 NPdat VP), (VP --~ Sinf V), (V ~ verspricht) ) During a derivation, rules from a vector can be applied in any order, and rules from different vectors call be interleaved, but at the end, all rules from an instance of a vect"
P94-1036,P92-1044,0,0.0135107,"G-DL) C_ L:({}-LIG). Let G = (VN, Vw, V, S) be a UVG-DL, where V = { V l , . . . , vK} with vi = (pi,1,...,pi,k,), kl = Ivil, 1 &lt; i &lt; K . We construct a {}-LIG G&apos; = (VN, VT, Yi, P, S). Let Yi = {li,j,k I 1 &lt; i &lt; K, 1 &lt; j, k &lt; ki }. Define P as follows. Let v in V, and let p in v be the production A ) WoBlWl...B~w, be in yr. In the following, we will denote by T(p) the multiset of active dominance requirements of p, and by .l-i(p) the multiset of passive dominance requirements of Bi, 1 &lt; i &lt; n. Add to P the following production: A T(p) Related Formalisms Based on word-order facts from Turkish, Hoffman (1992) proposes an extension to CCG called {}-CCG, in which arguments of functors form sets, rather than being represented in a curried notation. Under function composition, these sets are unioned. Thus the move from CCG to {}-CCG corresponds very much to the move from LIG to {}-LIG. We conjecture that (a version of) {}-CCG is weakly equivalent to {}-LIG. Staudacher (1993) defines a related system called distributed index g r a m m a r or DIG. DIG is like LIG, except that the stack of index symbols can be split into chunks and distributed among the daughter nodes. However, the formalism is not convi"
P94-1036,P83-1020,0,0.242521,"mal power for a wide range of linguistic phenomena (including the aforementioned Swiss German construction), the need for other mathematical formalisms has arisen in several unrelated areas. In this paper, we discuss three such cases. First, capturing several semantic and syntactic issues in unificationbased formalisms leads to the use of multiset-valued feature structures. Second, word order facts from languages such as German, Russian, or Turkish cannot be derived by LIG-equivalent formalisms. Third, a generalization of trees to &quot;quasi-trees&quot; (Vijay-Shanker, 1992) in the spirit of D-Theory (Marcus et al., 1983) leads to the definition of a new formal system. In this paper, we introduce two new equivalent mathematical formalisms which provide adequate descriptions for these three phenomena. The paper is structured as follows. First, we present the three phenomena in more detail. We then introduce multiset-valued LIG and present some formal properties. Thereafter, we introduce a second rewriting system and show that it is weakly equivalent to the LIG variant. We then briefly mention some related formalisms. We conclude with a brief summary. This paper defines multiset-valued linear index grammar and u"
P95-1021,E91-1005,1,0.76567,"n Section 1.2, there are cases where satisfactory analyses cannot be obtained with adjunction. In particular, using adjunction in this way cannot handle cases in which parts of the clausal complement are required to be placed within the structure of the adjoined tree. The DTG operation of subsertion is designed to overcome this limitation. Subsertion can be viewed as a generalization of adjunction in which components of the clausal complement (the subserted structure) which are not substituted can be interspersed within the structure that is the site of the subsertion. Following earlier work (Becket et al., 1991; Vijay-Shanker, 1992), DTG provide a mechanism involving the use of domination links (d-edges) that ensure that parts of the subserted structure that are not substituted dominate those parts that are. Furthermore, there is a need to constrain the way in which the non-substituted components can be interspersed 3. This is done by either using appropriate feature constraints at nodes or by means of subsertion-insertion constraints (see Section 2). We end this section by briefly commenting on the other DTG operation of sister-adjunction. In TAG, modification is performed with adjunction of modifi"
P95-1021,J94-1004,0,0.271262,"hextraction in Kashmiri proceeds as in English, except that the wh-word ends up in sentence-second position, with a topic from the matrix clause in sentence-initial position. This is illustrated in (2a) for a simple clause and in (2b) for a complex clause. (2) a. rameshan kyaa dyutnay t s e RameshzRG whatNOM g a v e yOUDAT What did you give Ramesh? b. rameshan kyaal chu baasaan [ ki RameshzRG what is believeNperf that me kor ti] IZRG do What does Ramesh beheve that I did? claim SUBJ ~ "" ~ O M P seem he I COMP adore SUB~BJ Mary hotdog MOD ~ . ~ O D spicy small Figure 2: Dependency tree for (1) Schabes & Shieber (1994) solve the first problem 1For clarity, we depart from standard TAG notational practice and annotate nodes with lexemes and arcs with grammatical function: 152 Since the moved element does not appear in sentence-initial position, the TAG analysis of English wh-extraction of Kroch (1987; 1989) (in which the matrix clause is adjoined into the embedded clause) cannot be transferred, and in fact no linguistically plausible TAG analysis appears to be available. In the past, variants of TAG have been developed to extend the range of possible analyses. In Multi-Component TAG (MCTAG) (Joshi, 1987), tre"
P95-1021,J92-4004,1,0.799715,"s a separate functional (f-) structure, and dependency grammars (see e.g. Mel'~uk (1988)) use these notions as the principal basis for syntactic representation. We will follow the dependency literature in referring to complementation and modification as syntactic dependency. As observed by Rambow and Joshi (1992), for TAG, the importance of the dependency structure means that not only the derived phrase-structure tree is of interest, but also the operations by which we obtained it from elementary structures. This information is encoded in the derivation tree (Vijay-Shanker, 1987). However, as Vijay-Shanker (1992) observes, the TAG composition operations are not used uniformly: while substitution is used only to add a (nominal) complement, adjunction is used both for modification and (clausal) complementation. Clausal complementation could not be handled uniformly by substitution because of the existence of syntactic phenomena such as long-distance wh-movement in English. Furthermore, there is an inconsistency in the directionality of the operations used for complementation in TAG@: nominal complements are substituted into their governing verb's tree, while the governing verb's tree is adjoined into it"
P95-1021,P95-1013,1,\N,Missing
P95-1021,P94-1036,1,\N,Missing
P96-1016,C90-3001,0,0.339326,"Missing"
P96-1016,J94-3001,0,0.031173,"n, and for corresponding algorithms for transduction. Several different transduction systems have been used in the past by the computational and theoretical linguistics communities. These systems have been borrowed from translation theory, a subfield of formal language theory, or have been originally (and sometimes redundantly) developed. Finite state transducers (for an overview, see, e.g., (Aho and Ullman, 1972)) provide translations between regular languages. These devices have been popular in computational morphology and computational phonology since the early eighties (Koskenniemi, 1983; Kaplan and Kay, 1994), and more recently in parsing as well (see, e.g., (Gross, 1989; Pereira, 1991; Roche, 1993)). Pushdown transducers and syntax directed translation schemata (SDTS) (Aho and Ullman, 1969) translate between contextfree languages and are therefore more powerful than finite state transducers. Pushdown transducers are a standard model for parsing, and have also been used (usually implicitly) in speech understanding. Recently, variants of SDTS have been proposed as models for simultaneously bracketing parallel corpora (Wu, 1995). Synchronization of tree adjoining grammars (TAGs) (Shieber and Schabes"
P96-1016,P91-1032,0,0.0285908,"ystems have been used in the past by the computational and theoretical linguistics communities. These systems have been borrowed from translation theory, a subfield of formal language theory, or have been originally (and sometimes redundantly) developed. Finite state transducers (for an overview, see, e.g., (Aho and Ullman, 1972)) provide translations between regular languages. These devices have been popular in computational morphology and computational phonology since the early eighties (Koskenniemi, 1983; Kaplan and Kay, 1994), and more recently in parsing as well (see, e.g., (Gross, 1989; Pereira, 1991; Roche, 1993)). Pushdown transducers and syntax directed translation schemata (SDTS) (Aho and Ullman, 1969) translate between contextfree languages and are therefore more powerful than finite state transducers. Pushdown transducers are a standard model for parsing, and have also been used (usually implicitly) in speech understanding. Recently, variants of SDTS have been proposed as models for simultaneously bracketing parallel corpora (Wu, 1995). Synchronization of tree adjoining grammars (TAGs) (Shieber and Schabes, 1990; Shieber, 1994) are even more powerful than the previous formalisms, an"
P96-1016,P95-1021,1,0.928057,"c). The vector derivation tree can be seen as representing an &quot;outline&quot; for the derivation. Such a view is attractive from a linguistic perspective: if each vector represents a lexeme and its projection (where the synchronous production is the basis of the lexical projection that the vector represents), then the vector derivation tree is in fact the dependency tree of the sentence (representing direct relations between lexemes such as grammatical function). In this respect, the vector derivation tree of UVG-DL is like the derivation tree of tree adjoining grammar and of D-tree grammars (DTG) (Rambow, Vijay-Shanker, and Weir, 1995), which is not surprising, since all three formalisms share the same extended domain of locality. Furthermore, the vector derivation tree of SynchUVG-DL shares with the the derivation tree of D T G the property that it reflects linguistic dependency uniformly; however, while the definition of D T G was motivated precisely from considerations of dependency, the vector derivation tree is merely a by-product of our definition of SynchUVG-DL, which was motivated from the desire to have a computationally tractable model of synchronization more powerful than SynchTAG.2 We briefly discuss a sample d"
P96-1016,C90-3045,0,0.719437,"plan and Kay, 1994), and more recently in parsing as well (see, e.g., (Gross, 1989; Pereira, 1991; Roche, 1993)). Pushdown transducers and syntax directed translation schemata (SDTS) (Aho and Ullman, 1969) translate between contextfree languages and are therefore more powerful than finite state transducers. Pushdown transducers are a standard model for parsing, and have also been used (usually implicitly) in speech understanding. Recently, variants of SDTS have been proposed as models for simultaneously bracketing parallel corpora (Wu, 1995). Synchronization of tree adjoining grammars (TAGs) (Shieber and Schabes, 1990; Shieber, 1994) are even more powerful than the previous formalisms, and have been applied in machine translation (Abeill6, Schabes, and Joshi, 1990; Egedi and Palmer, 1994; Harbusch and Poller, 1994; Prigent, 1994), natural language generation (Shieber and Schabes, 1991), and theoretical syntax (Abeilld, 1994). The common underlying idea in all of these formalisms is to combine two generative devices through a pairing of their productions (or, in the case of the corresponding automata, of their transitions) in such a way that right-hand side nonterminal symbols in the paired productions are"
P96-1016,P95-1033,0,0.0251903,"onal phonology since the early eighties (Koskenniemi, 1983; Kaplan and Kay, 1994), and more recently in parsing as well (see, e.g., (Gross, 1989; Pereira, 1991; Roche, 1993)). Pushdown transducers and syntax directed translation schemata (SDTS) (Aho and Ullman, 1969) translate between contextfree languages and are therefore more powerful than finite state transducers. Pushdown transducers are a standard model for parsing, and have also been used (usually implicitly) in speech understanding. Recently, variants of SDTS have been proposed as models for simultaneously bracketing parallel corpora (Wu, 1995). Synchronization of tree adjoining grammars (TAGs) (Shieber and Schabes, 1990; Shieber, 1994) are even more powerful than the previous formalisms, and have been applied in machine translation (Abeill6, Schabes, and Joshi, 1990; Egedi and Palmer, 1994; Harbusch and Poller, 1994; Prigent, 1994), natural language generation (Shieber and Schabes, 1991), and theoretical syntax (Abeilld, 1994). The common underlying idea in all of these formalisms is to combine two generative devices through a pairing of their productions (or, in the case of the corresponding automata, of their transitions) in such"
P96-1016,W90-0102,0,\N,Missing
P98-1106,1995.iwpt-1.23,1,0.308224,"ependents. (4) • The order of the newly introduced dependents is consistent with the LP rule associated with the governor. • The introduced terminal string (head) is m a p p e d to the rewritten category. (5) 3We follow (Gaifman, 1965) throughout this paper by modeling a dependency grammar with a string-rewriting system. However, we will identify a derivation with its representation as a tree, and we will sometimes refer to symbols introduced in a rewrite step as ""dependent nodes"". For a model of a DG based on tree-rewriting (in the spirit of Tree Adjoining Grammar (Joshi et al., 1975)), see (Nasr, 1995). 4In this paper, we will allow finite feature structures on categories, which we will notate using subscripts; e.g., Vtrans. Since the feature structures are finite, this is simply a notational variant of a system defined only with simple category labels. N o b j Adv f J Figure 3: A sample GDG derivation LP rules are represented as regular expressions (actually, only a limited form of regular expressions) associated with each category. We use the hash sign ( # ) to denote the position of the governor (head). For example: pl:Yt. . . . Nnom I Here is a sample m-rule. d3 : V Adv Carlos ) gnom, N"
P98-1106,P97-1043,0,0.0606835,"overed by ~2"" ((Nasr, 1996)). The resulting pseudo-projectivity is a fairly weak extension to projectivity, which nevertheless covers major nonprojective linguistic structures. However, we do not pursue a purely structural definition of pseudo-projectivity in this paper. In order to define pseudo-projectivity, we in647 linguistic example of lifting rule is given in Section 4. The idea of building a projective tree by means of lifting appears in (Kunze, 1968) and is used by (Hudson, 1990) and (Hudson, unpublished). This idea can also be compared to the notion of word order domain (Reape, 1990; BrSker and Neuhaus, 1997), to the Slash feature of G P S G and HPSG, to the functional uncertainty of LFG, and to the Move-a of GB theory. 3 Projective Revisited Dependency Vclause (2) ~ gnom, Y (3) ~ Adv = (Adv)Nnom(Aux)Adv*#YobjAdv*Yt . . . . yesterday Fernando thought Vtrans Nnom eats beans slowly We will call this system g e n e r a t i v e depend e n c y g r a m m a r or GDG for short. Derivations in GDG are defined as follows. In a rewrite step, we choose a multiset of dependency rules (i.e., a set of instances of dependency rules) which contains exactly one srule and zero or more m-rules. The left-hand side non"
P98-1106,P97-1003,0,0.0396601,"e e is a tree enriched with a linear order over the set of its nodes. Finally, if l is an arc of an ordered tree T, then Supp(1) represents the s u p p o r t of l, i.e. the set of the nodes of T situated between the extremities of l, extremities included. We will say that the elements of Supp(1) are c o v e r e d by I. Introduction Dependency grammar has a long tradition in syntactic theory, dating back to at least Tesni~re's work from the thirties3 Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not. One problem that has posed an impediment to more wide-spread acceptance of dependency grammars is the fact that there is no computationally tractable version of dependency grammar which is not restricted to projective analyses. However, it is well known that there are some syntactic phenomena (such as wh-movement in English or clitic climbing in Romance) that require nonprojective analyses. In this paper, we present a form of projectivity which we call pseudoprojectivity, and we present"
P98-1106,C96-2122,0,\N,Missing
P98-1106,C96-1058,0,\N,Missing
P98-1118,A92-1006,1,0.897796,"Missing"
P98-1118,W94-0319,0,0.0254336,"syntactic, and surface form) and which can be used for deep or shallow generation. In Section 2, we describe the overall architecture of PRESENTOR. In Section 3 to Section 6, we present the different specifications used to define domain communication knowledge and linguistic knowledge. Finally, in Section 7, we describe the outlook for PRESENTOR. 2 PRESENTOR A r c h i t e c t u r e The architecture of PRESENTOR illustrated in Figure 1 consists of a core generator with several associated knowledge bases. The core generator has a pipeline architecture which is similar to many existing systems (Reiter, 1994): an incoming request is received by the generator interface triggering sequentially the macroplanning, micro-planning, realization and fiCoreGenerator Configurable Knowledge DomainData , Macro-Planner Manager ~ - i Y [Micro-Planner ~ . ~ 1 Request I _ Realizer (Realpro) Presentation i i &quot; Figure 1: Architecture of PRESENTOR nally the formatting of a presentation which is then returned by the system. This pipeline architecture minimizes the interdependencies between the different modules facilitating the upgrade of each module with minimal impact on the overall system. It has been proposed tha"
P98-1118,A97-1037,1,\N,Missing
P98-1118,A97-1039,1,\N,Missing
palmer-etal-1998-rapid,W97-0311,0,\N,Missing
palmer-etal-1998-rapid,A97-1050,0,\N,Missing
palmer-etal-1998-rapid,C94-1024,0,\N,Missing
palmer-etal-1998-rapid,1997.mtsummit-workshop.12,1,\N,Missing
palmer-etal-1998-rapid,P96-1025,0,\N,Missing
palmer-etal-1998-rapid,A97-1039,0,\N,Missing
pasha-etal-2014-madamira,P05-1071,1,\N,Missing
pasha-etal-2014-madamira,P08-2030,1,\N,Missing
pasha-etal-2014-madamira,P09-2056,1,\N,Missing
pasha-etal-2014-madamira,P06-1086,1,\N,Missing
pasha-etal-2014-madamira,mohamed-etal-2012-annotating,0,\N,Missing
pasha-etal-2014-madamira,N13-1044,1,\N,Missing
pasha-etal-2014-madamira,W12-2301,1,\N,Missing
passonneau-etal-2006-inter,W98-1507,0,\N,Missing
passonneau-etal-2006-inter,W04-2709,1,\N,Missing
passonneau-etal-2006-inter,W04-3254,0,\N,Missing
passonneau-etal-2006-inter,passonneau-2006-measuring,1,\N,Missing
passonneau-etal-2006-inter,passonneau-2004-computing,1,\N,Missing
passonneau-etal-2006-inter,di-eugenio-2000-usage,0,\N,Missing
prabhakaran-etal-2012-annotations,W09-3953,1,\N,Missing
prabhakaran-etal-2012-annotations,P11-1078,0,\N,Missing
prabhakaran-etal-2012-annotations,N12-1057,1,\N,Missing
rambow-etal-2002-dependency,J99-2004,0,\N,Missing
rambow-etal-2002-dependency,J93-2004,0,\N,Missing
rambow-etal-2002-dependency,P97-1003,0,\N,Missing
rambow-etal-2002-dependency,C00-1007,1,\N,Missing
rambow-etal-2002-dependency,H94-1020,0,\N,Missing
rambow-etal-2002-dependency,P95-1037,0,\N,Missing
rambow-etal-2006-parallel,W04-2709,1,\N,Missing
rambow-etal-2006-parallel,passonneau-etal-2006-inter,1,\N,Missing
rambow-etal-2006-parallel,J93-2004,0,\N,Missing
rambow-etal-2006-parallel,W00-0204,0,\N,Missing
rambow-etal-2006-parallel,W02-1503,0,\N,Missing
rambow-etal-2006-parallel,rambow-etal-2002-dependency,1,\N,Missing
ramos-etal-2008-using,Y01-1001,0,\N,Missing
reeder-etal-2004-interlingual,W04-2709,1,\N,Missing
reeder-etal-2004-interlingual,W03-1601,0,\N,Missing
reeder-etal-2004-interlingual,W03-1604,0,\N,Missing
reeder-etal-2004-interlingual,P98-1013,0,\N,Missing
reeder-etal-2004-interlingual,C98-1013,0,\N,Missing
reeder-etal-2004-interlingual,1991.mtsummit-papers.9,1,\N,Missing
reeder-etal-2004-interlingual,J96-2004,0,\N,Missing
reeder-etal-2004-interlingual,A97-1011,0,\N,Missing
S12-1026,P09-1004,0,0.350414,"g (2006). However, since their model uses hand-crafted rules, they are able to predict and evaluate against actual PropBank role labels, whereas our approach has to be evaluated in terms of clustering quality. The problem of unsupervised semantic role labeling has recently attracted some attention (Lang and Lapata, 2011a; Lang and Lapata, 2011b; Titov and Klementiev, 2012). While the present paper shares the general aim of inducing semantic role clusters in an unsupervised way, it differs in treating syntax-semantics linkings explicitly and modeling predicate-specific distributions over them. Abend et al. (2009) address the problem of unsupervised argument recognition, which we do not address in the present paper. For the purpose of building a complete unsupervised semantic parser, a method such as theirs would be complementary to our work. 3 Model In this section, we decribe a model that generates arguments for a given predicate instance. Specifically, this generative model describes the probability of a given set of argument head words and associated syntactic functions in terms of underlying semantic roles, which are modelled as latent variables. The semantic role labeling task is therefore framed"
S12-1026,P98-1013,0,0.821433,"rate with SHK. b. SHK increased the response rate. c. The response rate increased. The subject of increase can be the agent (1a), the instrument (1b), or the theme (what is being increased) (1c). Other verbs that show this pattern include break or melt. Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (Burzio, 1986; Levin, 1993). The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (Baker et al., 1998; Palmer et al., 2005). Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (Lapata and Brew, 1999; Merlo and Stevenson, 2001). More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora. We are specifically interested in learning linkings. A linking is a mapping for one verb from its syntactic arguments and adjuncts to al"
S12-1026,de-marneffe-etal-2006-generating,0,0.0456041,"Missing"
S12-1026,W06-1601,0,0.577115,"en an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora. We are specifically interested in learning linkings. A linking is a mapping for one verb from its syntactic arguments and adjuncts to all of its semantic roles, so that individual semantic roles are not modeled independently of one another and so that we can exploit the relation between different mappings for the same verb (as in (1) above), or between mappings for different verbs. We therefore follow Grenager and Manning (2006) in treating linkings as first-class objects; however, we differ from their work in two important respects. First, we use semantic clustering of head words of arguments in an approach that resembles topic modeling, rather than directly modeling the subcategorization of verbs with a distribution over words. Second and most importantly, we do not make any assumptions about the linkings, as do Grenager and Manning (2006). They list a small set of rules from which they derive all linkings possible in their model; in contrast, we are able to learn any linking observed in the data. Therefore, our ap"
S12-1026,P11-1112,0,0.297117,"nd not their identification. The latter is a substantially different task, and likely to be best addressed by other approaches, such as that of (Abend et al., 2 version 1.6.8, available at http://nlp.stanford. edu/software/lex-parser.shtml 185 Evaluation Measures As explained above, our model does not predict specific role labels, such as those annotated in PropBank, but rather aims at clustering like argument instances together. Since the (numbered) labels of these clusters are arbitrary, we cannot evaluate the predictions of our model against the PropBank gold annotation directly. We follow Lang and Lapata (2011b) in measuring the quality of our clustering in terms of cluster purity and collocation instead. Cluster purity is a measure of the degree to which the predicted clusters meet the goal of containing only instances with the same gold standard class label. Given predicted clusters C1 , . . . , CnC and gold clusters G1 , . . . , GnG over a set of n argument instances, it is defined as nC 1X Pu = max |Ci ∩ Gj | j=1,...,nG n i=1 Similarly, cluster collocation measures how well the clustering meets the goal of clustering all gold instances with the same label into a single predicted cluster, formal"
S12-1026,D11-1122,0,0.311694,"nd not their identification. The latter is a substantially different task, and likely to be best addressed by other approaches, such as that of (Abend et al., 2 version 1.6.8, available at http://nlp.stanford. edu/software/lex-parser.shtml 185 Evaluation Measures As explained above, our model does not predict specific role labels, such as those annotated in PropBank, but rather aims at clustering like argument instances together. Since the (numbered) labels of these clusters are arbitrary, we cannot evaluate the predictions of our model against the PropBank gold annotation directly. We follow Lang and Lapata (2011b) in measuring the quality of our clustering in terms of cluster purity and collocation instead. Cluster purity is a measure of the degree to which the predicted clusters meet the goal of containing only instances with the same gold standard class label. Given predicted clusters C1 , . . . , CnC and gold clusters G1 , . . . , GnG over a set of n argument instances, it is defined as nC 1X Pu = max |Ci ∩ Gj | j=1,...,nG n i=1 Similarly, cluster collocation measures how well the clustering meets the goal of clustering all gold instances with the same label into a single predicted cluster, formal"
S12-1026,W99-0632,0,0.0608245,"hat show this pattern include break or melt. Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (Burzio, 1986; Levin, 1993). The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (Baker et al., 1998; Palmer et al., 2005). Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (Lapata and Brew, 1999; Merlo and Stevenson, 2001). More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora. We are specifically interested in learning linkings. A linking is a mapping for one verb from its syntactic arguments and adjuncts to all of its semantic roles, so that individual semantic roles are not modeled independently of one another and so that we can exploit the relation between different mappings for the same verb (as in (1) above), or b"
S12-1026,J93-2004,0,0.0393319,"ents we averaged over 10 consecutive samples of the latent distributions, at the end of the sampling process (i.e., when convergence has been reached). 4 Experimental Setup 4.2 We train and evaluate our linking model on the data set produced for the CoNLL-08 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies (Surdeanu et al., 2008), which is based on the PropBank corpus (Palmer et al., 2005). This data set includes part-of-speech tags, lemmatized tokens, and syntactic dependencies, which have been converted from the manual syntactic annotation of the underlying Penn Treebank (Marcus et al., 1993). 4.1 2009). We therefore use gold standard information of the CoNLL-08 data set for identifying argument sets as input to our model. The task of our model is then to classify these arguments into semantic roles. We train our model on a corpus consisting of the training and the test part of the CoNLL-08 data set, which is permissible since as a unsupervised system our model does not make any use of the annotated argument labels for training. We test the model performance against the gold argument classification on the test part. For development purposes (both designing the model and tuning the"
S12-1026,J01-3003,0,0.0363434,"nclude break or melt. Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (Burzio, 1986; Levin, 1993). The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (Baker et al., 1998; Palmer et al., 2005). Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (Lapata and Brew, 1999; Merlo and Stevenson, 2001). More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora. We are specifically interested in learning linkings. A linking is a mapping for one verb from its syntactic arguments and adjuncts to all of its semantic roles, so that individual semantic roles are not modeled independently of one another and so that we can exploit the relation between different mappings for the same verb (as in (1) above), or between mappings for differen"
S12-1026,J05-1004,0,0.935122,"K increased the response rate. c. The response rate increased. The subject of increase can be the agent (1a), the instrument (1b), or the theme (what is being increased) (1c). Other verbs that show this pattern include break or melt. Much theoretical and lexicographic (descriptive) work has been devoted to determining how verbs map their lexical predicate-argument structure to syntactic arguments (Burzio, 1986; Levin, 1993). The last decades have seen a surge in activity on the computational front, spurred in part by efforts to annotate large corpora for lexical semantics (Baker et al., 1998; Palmer et al., 2005). Initially, we have seen computational efforts devoted to finding classes of verbs that share similar syntax-semantics mappings from annotated and unannotated corpora (Lapata and Brew, 1999; Merlo and Stevenson, 2001). More recently, there has been an explosion of interest in semantic role labeling (with too many recent publications to cite). In this paper, we explore learning syntaxsemantics mappings for verbs from unannotated corpora. We are specifically interested in learning linkings. A linking is a mapping for one verb from its syntactic arguments and adjuncts to all of its semantic role"
S12-1026,W08-2121,0,0.257452,"Missing"
S12-1026,E12-1003,0,0.19609,"Missing"
S12-1026,C98-1013,0,\N,Missing
S15-1009,P98-1013,0,0.149614,"the syntactic head of the text passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals"
S15-1009,baker-etal-2010-modality,0,0.0482113,"Missing"
S15-1009,W13-2322,0,0.0158937,"xt passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals, groups, artifacts, etc., which"
S15-1009,W09-3012,1,0.690346,"space we do not provide an overview over all definitions. While at first the terms “belief” and “factuality” appear to relate to rather different things (a subjective state versus truth), in the NLP community they in fact refer to the same phenomenon, while having rather different connotations. The phenomenon is the communicative intention of a writer1 to present propositional content as something that she firmly believes is true, weakly believes is true, or has some other attitude towards, namely a wish or a reported belief. The term “belief” here describes the cognitive state of the writer (Diab et al., 2009), and comes from artificial intelligence and cognitive science, as in the Belief-Desire-Intention model of Bratman (1999 1987). The term “factuality” describes the communicative intention of the writer (Saur´ı and Pustejovsky, 2012, p. 263) (our emphasis): The fact that an eventuality is depicted as holding or not does not mean that this is the case in the world, but that this is how it is characterized by its informant. Similarly, it does not mean that this is the real knowledge that informant has (his true cognitive state regarding that event) but what he wants us to believe it is. We would"
S15-1009,doddington-etal-2004-automatic,1,0.834711,"sity/George Washington University, the Florida Institute for Human and Machine Cognition, and the University of Albany. The goal of our research project is not linguistic annotation, but the identification of meaning which is expressed in a non-linguistic manner. Such a meaning representation is useful for many applications; in our project we are specifically interested in knowledge base population. A different part of the DEFT program is concerned with the representation of propositional meaning, following the tradition of the ACE program in representing entities, relations and events (ERE) (Doddington et al., 2004). The work presented here is concerned with the attitude of agents towards propositional content: do the agents express a committed belief or a non-committed belief in the propositional content? Our work has several characteristics that set it apart from other work: we are interested in annotation which can be done fairly quickly; we are not interested in annotating linguistic elements (such as trigger words); and we are planning an integration with sentiment annotation. The structure of the paper is as follows: we start out by situating our notion of “belief” with respect to other notions of"
S15-1009,W10-3001,0,0.293703,"Missing"
S15-1009,P11-2102,0,0.0364363,"rotates around the earth, as was his (presumably) honest communicative intention. Therefore, to us as researchers interested in describing how language 2 Sarcasm and irony differ from lying in that the communicative intention and the cognitive state are aligned, but they do not align with the standard interpretation of the utterance. Here, the intention is that the reader recognizes that the form of the utterance does not literally express the cognitive state. We leave aside sarcasm and irony in this paper; for current computational work on sarcasm detection, see for example (Gonz´alez-Ib´an˜ ez et al., 2011). is used to communicate, it does not matter that astronomers now believe that Ptolemy was wrong, it does not change our account of communication and it does not change the communication that happened two millennia ago. And since we do not need to make the assumption that the writer knows what she is talking about, we choose not to make this assumption. In the case of Ptolemy, we leave this determination – what is actually true – to astronomers. In other cases, we typically have models of trustworthiness: if a writer sends her spouse a text message saying she is hungry, the spouse has no reaso"
S15-1009,P09-2078,0,0.0256017,"d, we could assume that the writer knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumptio"
S15-1009,P11-1032,0,0.0145471,"er knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumption may be false in c"
S15-1009,C10-2117,1,0.747479,"on-committed belief in the annotations, the heuristic rules (mainly based on the presence of modal auxiliaries) that we added for the purpose of classifying the beliefs (CB, NCB, ROB, NA) did not work reliably in all cases. 4.3 System C System C uses a supervised learning approach to identify tokens denoting the heads of propositions that denote author’s expressed beliefs. It approaches this problem as a 5-way (CB, NCB, ROB, NA, nil) multi-class classification task at the word level. System C is adapted from a previous system which uses an earlier, simpler definition and annotation of belief (Prabhakaran et al., 2010). The system uses lexical and syntactic features for this task, which are extracted using the part-of-speech tags and dependency parses obtained from the Stanford CoreNLP system. In addition to the features described in (Prabhakaran et al., 2010), System C uses a set of new features including features based on a dictionary of hedge-words (Prokofieva and Hirschberg, 2014). The hedge features improved the NCB Fmeasure by around 2.2 percentage points (an overall F-measure improvement of 0.25 percentage points) in experiments conducted on a separate development set. It uses a quadratic kernel SVM"
S15-1009,W12-3807,1,0.915772,"Missing"
S15-1009,J12-2002,0,0.157236,"Missing"
S15-1009,W15-1304,1,0.70443,"ore, the FactBank annotation is basically compatible with ours. Our annotation is much simpler than that of FactBank in order to allow for a quicker annotation. We summarize the main points of simplification here. • We have taken the source always to be the writer. As we will discuss in Section 7.1, we will adopt the FactBank annotation in the next iteration of our annotation. • We do not distinguish between possible and probable; this distinction may be hard to annotate and not too valuable. • We ignore negation. If present, we simply assume it is part of the proposition which is the target. Werner et al. (2015) study the relation between belief and factuality in more detail. They provide an automatic way of mapping the annotations in FactBank to the 4-way distinction of speaker/writer’s belief that we present in this paper. 3.3 Corpus and Annotation Results The annotation effort for this phase of belief annotation for DEFT produced a training corpus of 852,836 words and an evaluation corpus of 100,037 words. All annotated data consisted of English text from discussion forum threads. The discussion forum threads were originally collected for the DARPA BOLT program, and were harvested from a wide vari"
S15-1009,C98-1013,0,\N,Missing
W00-1401,P98-1006,1,0.753619,"use a second metric, G e n e r a t i o n S t r i n g A c c u r a c y , shown in Equation (3), which treats deleWe employ two metrics that measure the accuracy tion of a token at one location in the string and the of a generated string. The first metric, s i m p l e acinsertion of the same token at another location in c u r a c y , is the same string distance metric used for the string as one single movement error (M). This measuring speech recognition accuracy. This metis in addition to the remaining insertions (I&apos;) and ric has also been used to measure accuracy of MT deletions (D&apos;). systems (Alshawi et al., 1998). It is based on string edit distance between the output of the generation (3) G e n e r a t i o n S t r i n g A c c u r a c y = system and the reference corpus string. Simple ac( 1 -- M~-/~.P-~--~-~) curacy is the number of insertion (I), deletion (D) and substitutions (S) errors between the reference In our example sentence (2), we see that the inserstrings in the test corpus and the strings produced by tion and deletion of no can be collapsed into one the generation model. An alignment algorithm using substitution, insertion and deletion of tokens as move. However, the wrong positions of co"
W00-1401,J99-2004,1,0.215788,"Missing"
W00-1401,C00-1007,1,0.604742,"Missing"
W00-1401,P98-1116,0,0.215224,"tly cannot m a p large corpora of syntactic parses onto such semantic representations, and therefore cannot create the input representation for the evaluation. The second question is t h a t of fairness of the evaluation. FE[,tGt.&apos;S as described in this paper is of limited use. since it only chooses word order (and, to a certain extent, syntactic structure). Other realization and sentence planning tin{ks-which are needed for most applications and which may profit from a stochastic model include lexical choice, introduction of function words and punctuation, and generation of morphology. (See (Langkilde and Knight, 1998a) for a relevant discussion. FERGUS currently can perform punctuation and function word insertion, and morphology and lexical choice are under development.) The question arises whether our metrics will . fairly m e a s u r e the:quality,~of,a, more comp!ete real~ .... ization module (with some sentence planning). Once the range of choices t h a t the generation component makes expands, one quickly runs into the problem that, while the gold standard may be a good way of communicating the input structure, there are usually other good ways of doing so as well (using other words, other syntactic"
W00-1401,W98-1426,0,0.279927,"tly cannot m a p large corpora of syntactic parses onto such semantic representations, and therefore cannot create the input representation for the evaluation. The second question is t h a t of fairness of the evaluation. FE[,tGt.&apos;S as described in this paper is of limited use. since it only chooses word order (and, to a certain extent, syntactic structure). Other realization and sentence planning tin{ks-which are needed for most applications and which may profit from a stochastic model include lexical choice, introduction of function words and punctuation, and generation of morphology. (See (Langkilde and Knight, 1998a) for a relevant discussion. FERGUS currently can perform punctuation and function word insertion, and morphology and lexical choice are under development.) The question arises whether our metrics will . fairly m e a s u r e the:quality,~of,a, more comp!ete real~ .... ization module (with some sentence planning). Once the range of choices t h a t the generation component makes expands, one quickly runs into the problem that, while the gold standard may be a good way of communicating the input structure, there are usually other good ways of doing so as well (using other words, other syntactic"
W00-1401,A00-2023,0,0.0545747,"Missing"
W00-1401,W98-1400,0,0.060105,"enre, register, idiosyncratic choices, and so on). Assuming the test corpus is representative of the training corpus, we can then use our metrics to measure deviance from the corpus, whether it be merely in word order or in terms of more complex tasks such as lexical choice as well. Thus, as long as the goal of the realizer is to enmlate as closely as possible a given corpus (rather than provide a maximal range of paraphrastic capability), then our approach can be used for evaluation, r As in the case of machine translation, evaluation in generation is a complex issue. (For a discussion, see (Mellish and Dale, 1998).) Presumably, the quality of most generation systems can only be assessed at a system level in a task-oriented setting (rather than by taking quantitative measures or by asking humans for quality assessments). Such evaluations are costly, and they cannot be the basis of work in stochastic generation, for which evaluation is a frequent step in research and development. An advantage of our approach is that our quantitative metrics allow us to evaluate without human intervention, automatically and objectively (objectively with respect to the defined metric,-that is).- Independently, the use of t"
W00-1401,P97-1035,0,0.059365,"Missing"
W00-1401,J97-1004,0,\N,Missing
W00-1401,C98-1112,0,\N,Missing
W00-2004,P98-1006,1,0.671057,"Missing"
W00-2004,J99-2004,1,0.62835,"Missing"
W00-2004,W00-1401,1,0.403517,"Missing"
W00-2004,C00-1007,1,0.796749,"Missing"
W00-2004,C96-1034,0,0.0328893,"Missing"
W00-2004,P98-1116,0,0.107091,"Missing"
W00-2004,W98-1426,0,0.118114,"Missing"
W00-2004,P95-1021,1,0.346341,"Missing"
W00-2004,P97-1026,0,0.0186598,"Missing"
W00-2004,W98-0143,0,0.0411512,"Missing"
W00-2004,W98-1422,0,\N,Missing
W01-0801,W00-1401,1,0.818553,"cause there is some hidden truth which traditional linguistic methodologies have access to but corpusbased methods do not, because they are not in fact in opposition to each other. Finally, the emphasis on evaluation that the corpus-based techniques in NLU have brought with them have often aroused animosity in the NLG community. Evaluation is necessary for development purposes when using corpus-based techniques: it is easy to generate many different hypotheses, and we need to be able to choose among them. Since this is crucial, increased attention needs to be paid to evaluation in generation (Bangalore et al., 2000; Rambow et al., 2001). But again, the situation is in fact not different from a traditional linguistic methodology: theories about language use in context need to be defeasible on empirical grounds and hence need to be evaluated against a corpus. Of course, the choice of evaluation corpus is an important one, and the costs associated with compiling and annotating corpora can greatly impact the choice of evaluation corpus and hence the evaluation. In conclusion, NLG has nothing to fear from corpus-based methods. Instead, the NLG community can continue to provide a test-bed for linguists to exe"
W01-0801,P95-1034,0,0.0357365,"specified, then all the rules that map to a correct output can also be explicitly specified. However, this paper will argue that this view is not correct, and NLG can and does profit from corpusbased methods. The resistance to corpus-based approaches in NLG may have more to do with the fact that in many NLG applications (such as report or description generation) the output to be generated is extremely limited. As is the case with NLU, if the language is limited, hand-crafted methods are adequate and successful. Thus, it is not a surprise that the first use of corpus-based techniques, at ISI (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) was motivated by the use of NLG not in “traditional” NLG applications, but in machine translation, in which the range of output language is (potentially) much larger. In fact, the situations in NLU and NLG do not actually differ with respect to the notion of ambiguity. Though it is not a trivial task, we can fully specify a grammar such that the generated text is not ungrammatical. But the problem for NLG is not specifying a grammar, but determining which part of the grammar to use: to give a simple example, a give-event can be generated with the double-object fra"
W01-0801,W98-1426,0,0.0165758,"map to a correct output can also be explicitly specified. However, this paper will argue that this view is not correct, and NLG can and does profit from corpusbased methods. The resistance to corpus-based approaches in NLG may have more to do with the fact that in many NLG applications (such as report or description generation) the output to be generated is extremely limited. As is the case with NLU, if the language is limited, hand-crafted methods are adequate and successful. Thus, it is not a surprise that the first use of corpus-based techniques, at ISI (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998) was motivated by the use of NLG not in “traditional” NLG applications, but in machine translation, in which the range of output language is (potentially) much larger. In fact, the situations in NLU and NLG do not actually differ with respect to the notion of ambiguity. Though it is not a trivial task, we can fully specify a grammar such that the generated text is not ungrammatical. But the problem for NLG is not specifying a grammar, but determining which part of the grammar to use: to give a simple example, a give-event can be generated with the double-object frame (give Mary a book) or with"
W01-0801,P01-1056,1,0.824683,"en truth which traditional linguistic methodologies have access to but corpusbased methods do not, because they are not in fact in opposition to each other. Finally, the emphasis on evaluation that the corpus-based techniques in NLU have brought with them have often aroused animosity in the NLG community. Evaluation is necessary for development purposes when using corpus-based techniques: it is easy to generate many different hypotheses, and we need to be able to choose among them. Since this is crucial, increased attention needs to be paid to evaluation in generation (Bangalore et al., 2000; Rambow et al., 2001). But again, the situation is in fact not different from a traditional linguistic methodology: theories about language use in context need to be defeasible on empirical grounds and hence need to be evaluated against a corpus. Of course, the choice of evaluation corpus is an important one, and the costs associated with compiling and annotating corpora can greatly impact the choice of evaluation corpus and hence the evaluation. In conclusion, NLG has nothing to fear from corpus-based methods. Instead, the NLG community can continue to provide a test-bed for linguists to exercise their theories ("
W02-2214,W02-2236,1,\N,Missing
W02-2214,J99-2004,1,\N,Missing
W02-2214,J95-4002,0,\N,Missing
W02-2214,1997.iwpt-1.11,0,\N,Missing
W02-2214,P00-1058,0,\N,Missing
W02-2214,P98-1106,1,\N,Missing
W02-2214,C98-1102,1,\N,Missing
W02-2214,P95-1037,0,\N,Missing
W02-2214,P98-1061,0,\N,Missing
W02-2214,C98-1059,0,\N,Missing
W02-2214,P98-2190,0,\N,Missing
W02-2214,C98-2185,0,\N,Missing
W02-2214,J94-1004,0,\N,Missing
W02-2235,W98-0135,1,0.35987,"Missing"
W02-2235,J01-1004,1,0.803415,"wo salient linguistic uses of adjunction: for analyzing long-distance wh-movement (and related movement types) in many languages and for analyzing cross-serial dependencies (CSD) in Dutch and Swiss German. While the need for and the adequacy of adjunction to model wh-type movement have been questioned (Rambow and Vijay-Sbanker, 1998; Rambow et al., 2001), CSD seems ideally suited for a TAG analysis, since, as Shieber ( 1985) showed, CSD cannot be derived by a context-free grammar. In fact, some ofthe altemate tree rewriting systems proposed which do not include adjunction, such as the DSG of (Rambow et al., 2001}, cannot provide a satisfactory analysis of CSD, either: it is specifically the definition of adjunction as an tree-rewriting operation that inserts one tree in its entirety into the center of another that is crucial for deriving CSD. What is somewhat troubling, however, is that the construction appears tobe limited to two West Germanic languages/dialects, Dutch and Swiss Gennan. In this paper, we show that the same construction, though with different syntactic characteristics, is found in a completely unrelated language, Tagalog. We show how the analysis of Kroch and Santorini (1991) for Dut"
W02-2236,W01-0520,1,0.885176,"Missing"
W02-2236,E99-1025,1,0.821222,"Missing"
W02-2236,2000.iwpt-1.9,1,0.941088,"Chen*, Srinivas Bangalore*, Michael Collins*, and Owen Rambowt *AT&T Labs-Research, t University ofPennsylvania {jchen,srini,mcollins}@research.att.com,rarnbow@unagi.cis.upenn.edu 1. Introduction As shown by Srinivas (1997), standard n-gram modeling may be used to perfonn supertag disambiguation with accuracy that is adequate for partial parsing, but in general not sufficient for füll parsing. A serious problem is that n-gram modeling usually considers a very small, fixed context and does not perfonn weil with large tag sets, such as those generated by automatic grammar extraction (Xia, 1999; Chen and Vijay-Shanker, 2000; Chlang, 2000). As an alternative, Chen, Bangalore and Vijay-Shanker (1999) introduce class-based supertagging. An example of class tagging is n-best trigram-based supertagging, which assigns to each word the top n most likely supertags as detennined by an n-gram supertagging model. Class-based supertagging can be performed much more accurately than supertagging with only a small increase in ambiguity. In a second phase, the most likely candidate from the class is chosen. In this paper, we investigate an approach to such a choice based on reranking a set of candidate supertags and their confi"
W02-2236,P00-1058,0,0.0381466,"g has been found tobe usefül in other applications such as infonnation retrieval (Chandrasekhar and Srinivas, l 997b) and text simplification (Chandrasekhar and Srinivas, 1997a). © 2002 John Chen, Srinivas Bangalore, Michael Collins, and Owen Rarnbow. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 259-268. Universitä di Venezia. 260 Proceedings ofTAG+6 2.2. Automatically Extracted Grammars Recently, procedures have been developed that automatically extract TAGs from broad coverage treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000). They have the advantage that linguistically motivated TAGs can be extracted from widely available treebanks without a huge investment in manual labor. Furthermore, because of their direct extraction from a treebank, parameters can be easily and accurately estimated for building statistical TAG models for parsing (Chiang, 2000; Sarkar, 2001) or geoeration (Bangalore, Chen and Rambow, 2001). In our experiments, we use an automatically extracted TAG grammar similar to the ones described by Chen and Vijay-Shanker (2000). This grammar has been extracted from Sections 02-21 of the Penn Treebank (M"
W02-2236,P97-1003,1,0.826017,"Missing"
W02-2236,J93-2004,0,0.022848,"Missing"
W02-2236,W96-0213,0,0.360487,"Missing"
W02-2236,W00-2027,0,0.029134,"r Output We claim that supertagging is a viable option to explore for use as a preprocessing step in order to speed up füll parsing. In order to substantiate this claim, we perform exploratory experiments that show the relationship between n-best supertagging and parsing performance. Using the grammar that is described in Section 2.2, we train n-best supertaggers on Sections 02-21 of the Perut Treebank. For each supertagger, we supertag Section 22, which consists of about 40,100 words in 1,700 sentences. We then feed the resulting output through the LEM parser, a head-driven TAG chart parser (Sarkar, 2000). Given an input sentence and a grammar, this parser either outputs nothing, or a packed derivation forest of every parse that can be assigned to the sentence by the grammar. lt does not retum partial parses. The results of these experiments are shown in Table 1. The input to the parser can be the output of either a 1, 2, or 4-best supertagger. lt can also be sentences where each word is associated with all of the supertags with that word's part of speech, as detennined by a trigram part of speech tagger. This is labeled as &quot;POS-tag&quot; in the table. Lastly, it can simply be sentences where each"
W02-2236,N01-1023,0,0.045366,"rks (TAG+6), pp. 259-268. Universitä di Venezia. 260 Proceedings ofTAG+6 2.2. Automatically Extracted Grammars Recently, procedures have been developed that automatically extract TAGs from broad coverage treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000). They have the advantage that linguistically motivated TAGs can be extracted from widely available treebanks without a huge investment in manual labor. Furthermore, because of their direct extraction from a treebank, parameters can be easily and accurately estimated for building statistical TAG models for parsing (Chiang, 2000; Sarkar, 2001) or geoeration (Bangalore, Chen and Rambow, 2001). In our experiments, we use an automatically extracted TAG grammar similar to the ones described by Chen and Vijay-Shanker (2000). This grammar has been extracted from Sections 02-21 of the Penn Treebank (Marcus, Santorini and Marcinkiewicz, 1993). lt contains 3964 tree frames (non-lexicalized elementary trees). The parameters of extraction are set as follows. Each tree frame contains nodes that are labeled using a label set similar to the XTAG (XTAG-Group, 2001) label set. Furthermore, tree frames are extracted corresponding to a &quot;moderate&quot; do"
W02-2236,C88-2121,0,0.206461,"Missing"
W02-2236,1997.iwpt-1.22,0,0.0396543,"hroughout this section, we describe the kinds oflinguistic resources that we use in all of our experiments and the kinds of notation that we will employ in the rest of this paper. 2.1. Supertagging Supertagging (Bangalore and Joshl, 1999) is the process of assigning the best TAG elementary tree, or supertag, to each word in the input sentence. lt performs the task of parsing disambiguation to such an extent that it may be characterized as providing an almost parse. There exist linear time approaches to supertagging, providing one promising route to linear time parsing disambiguation. However, Srinivas (1997) shows that standard n-grarn modeling may be used to perform supertagging with accuracy that is adequate for partial parsing, but not for füll parsing. On the other hand, n-gram modeling of supertagging has been found tobe usefül in other applications such as infonnation retrieval (Chandrasekhar and Srinivas, l 997b) and text simplification (Chandrasekhar and Srinivas, 1997a). © 2002 John Chen, Srinivas Bangalore, Michael Collins, and Owen Rarnbow. Proceedings ofthe Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 259-268. Universitä di Venezia. 260 Pr"
W02-2236,N01-1003,1,0.882532,"Missing"
W02-2236,W00-1208,0,0.0225814,"Missing"
W02-2236,W98-0143,0,0.0337296,"Missing"
W02-2236,J99-2004,1,\N,Missing
W02-2236,J03-4003,1,\N,Missing
W03-1006,J99-2004,0,0.0626338,"Missing"
W03-1006,2000.iwpt-1.9,1,0.751709,"s derived indirectly from the PropBank through TAG. The second stage of our methodology entails using these features to predict semantic roles. We first experiment with prediction of semantic roles given gold-standard parses from the test corpus. We subsequently experiment with their prediction given raw text fed through a deterministic dependency parser. 4 Extraction of TAGs from the PropBank Our experiments depend upon automatically extracting TAGs from the PropBank. In doing so, we follow the work of others in extracting grammars of various kinds from the PTB, whether it be TAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000), combinatory categorial grammar (Hockenmaier and Steedman, 2002), or constraint dependency grammar (Wang and Harper, 2002). We will discuss TAGs and an important principle guiding their formation, the extraction procedure from the PTB that is described in (Chen, 2001) including extensions to extract a TAG from the PropBank, and finally the extraction of deeper linguistic features Figure 4: Parse tree associated with the sentence Prices are falling has been fragmented into three tree frames. from the resulting TAG. A TAG is defined to be a set of lexicalized elementary trees (Jo"
W03-1006,P00-1058,0,0.0103075,"PropBank through TAG. The second stage of our methodology entails using these features to predict semantic roles. We first experiment with prediction of semantic roles given gold-standard parses from the test corpus. We subsequently experiment with their prediction given raw text fed through a deterministic dependency parser. 4 Extraction of TAGs from the PropBank Our experiments depend upon automatically extracting TAGs from the PropBank. In doing so, we follow the work of others in extracting grammars of various kinds from the PTB, whether it be TAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000), combinatory categorial grammar (Hockenmaier and Steedman, 2002), or constraint dependency grammar (Wang and Harper, 2002). We will discuss TAGs and an important principle guiding their formation, the extraction procedure from the PTB that is described in (Chen, 2001) including extensions to extract a TAG from the PropBank, and finally the extraction of deeper linguistic features Figure 4: Parse tree associated with the sentence Prices are falling has been fragmented into three tree frames. from the resulting TAG. A TAG is defined to be a set of lexicalized elementary trees (Joshi and Schabes"
W03-1006,P02-1031,0,0.587294,"semantic interpretation have not been available. The completion of the first phase of the PropBank (Kingsbury et al., 2002) represents an important step. The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994). The arc labels chosen for the arguments are specific to the predicate, not universal. In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). Specifically, we show that the syntactic dependency structure that results arg0 arg2 arg1 John hay truck Figure 1: PropBank-style semantic representation for both John loaded the truck with hay and John loaded hay into the truck from the extraction of a Tree Adjoining Grammar (TAG) from the PTB, and the features that accompany this structure, form a better basis for determining semantic role labels. Crucially, the same structure is also produced when parsing with TAG. We suggest that the syntactic representation chosen in the PTB is less well suited for semantic processing than the other, d"
W03-1006,hockenmaier-steedman-2002-acquiring,0,0.0126623,"ethodology entails using these features to predict semantic roles. We first experiment with prediction of semantic roles given gold-standard parses from the test corpus. We subsequently experiment with their prediction given raw text fed through a deterministic dependency parser. 4 Extraction of TAGs from the PropBank Our experiments depend upon automatically extracting TAGs from the PropBank. In doing so, we follow the work of others in extracting grammars of various kinds from the PTB, whether it be TAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000), combinatory categorial grammar (Hockenmaier and Steedman, 2002), or constraint dependency grammar (Wang and Harper, 2002). We will discuss TAGs and an important principle guiding their formation, the extraction procedure from the PTB that is described in (Chen, 2001) including extensions to extract a TAG from the PropBank, and finally the extraction of deeper linguistic features Figure 4: Parse tree associated with the sentence Prices are falling has been fragmented into three tree frames. from the resulting TAG. A TAG is defined to be a set of lexicalized elementary trees (Joshi and Schabes, 1991). They may be composed by several well-defined operations"
W03-1006,P95-1037,0,0.00511622,"ciple is that dependencies, including long-distance dependencies, are typically localized the same elementary tree by appropriate grouping of syntactically or semantically related elements. The extraction procedure fragments a parse tree from the PTB that is provided as input into elementary trees. See Figure 4. These elementary trees can be composed by TAG operations to form the original parse tree. The extraction procedure determines the structure of each elementary tree by localizing dependencies through the use of heuristics. Salient heuristics include the use of a head percolation table (Magerman, 1995), and another table that distinguishes between complements and adjunct nodes in the tree. For our current work, we use the head percolation table to determine heads of phrases. Also, we treat a PropBank argument (ARG0 . . . ARG9) as a complement and a PropBank adjunct (ARGM’s) as an adjunct when such annotation is available. 1 Otherwise, we basically follow the approach of (Chen, 2001).2 Besides introducing one kind of TAG extraction 1 The version of the PropBank we are using is not fully annotated with semantic role information, although the most common predicates are. 2 Specifically, CA1. pr"
W03-1006,J93-2004,0,0.0481706,"tures. 1 Introduction Syntax mediates between surface word order and meaning. The goal of parsing (syntactic analysis) is ultimately to provide the first step towards giving a semantic interpretation of a string of words. So far, attention has focused on parsing, because the semantically annotated corpora required for learning semantic interpretation have not been available. The completion of the first phase of the PropBank (Kingsbury et al., 2002) represents an important step. The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994). The arc labels chosen for the arguments are specific to the predicate, not universal. In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). Specifically, we show that the syntactic dependency structure that results arg0 arg2 arg1 John hay truck Figure 1: PropBank-style semantic representation for both John loaded the truck with hay and John loaded hay into the truck from the extraction of a Tree Adjoi"
W03-1006,H94-1020,0,0.0991439,"Syntax mediates between surface word order and meaning. The goal of parsing (syntactic analysis) is ultimately to provide the first step towards giving a semantic interpretation of a string of words. So far, attention has focused on parsing, because the semantically annotated corpora required for learning semantic interpretation have not been available. The completion of the first phase of the PropBank (Kingsbury et al., 2002) represents an important step. The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994). The arc labels chosen for the arguments are specific to the predicate, not universal. In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). Specifically, we show that the syntactic dependency structure that results arg0 arg2 arg1 John hay truck Figure 1: PropBank-style semantic representation for both John loaded the truck with hay and John loaded hay into the truck from the extraction of a Tree Adjoining Grammar (TAG) fro"
W03-1006,1997.iwpt-1.22,0,0.0548111,"Missing"
W03-1006,W02-1031,0,0.00781917,"We first experiment with prediction of semantic roles given gold-standard parses from the test corpus. We subsequently experiment with their prediction given raw text fed through a deterministic dependency parser. 4 Extraction of TAGs from the PropBank Our experiments depend upon automatically extracting TAGs from the PropBank. In doing so, we follow the work of others in extracting grammars of various kinds from the PTB, whether it be TAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000), combinatory categorial grammar (Hockenmaier and Steedman, 2002), or constraint dependency grammar (Wang and Harper, 2002). We will discuss TAGs and an important principle guiding their formation, the extraction procedure from the PTB that is described in (Chen, 2001) including extensions to extract a TAG from the PropBank, and finally the extraction of deeper linguistic features Figure 4: Parse tree associated with the sentence Prices are falling has been fragmented into three tree frames. from the resulting TAG. A TAG is defined to be a set of lexicalized elementary trees (Joshi and Schabes, 1991). They may be composed by several well-defined operations to form parse trees. A lexicalized elementary tree where t"
W03-1006,J03-4003,0,\N,Missing
W04-1503,J00-1004,0,0.0239731,"or the recovery of the dependency structure both from the derivation tree and from a parse forest represented in polynomial space. (In fact, our parsing algorithm draws on this work.) However, the approach of course requires the introduction of additional nonterminal nodes. Finally, we observe that Recursive Transition Networks (Woods, 1970) can be used to encode a grammar whose derivation trees are dependency trees. However, they are more a general framework for encoding grammars than a specific type of grammar (for example, we can also use them to encode CFGs). In a somewhat related manner, Alshawi et al. (2000) use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized. Eisner (2000) provides a formalization of a system that uses two different automata to generate left and right children of a head. His formalism is very close to the one we present, but it is not a string-rewriting formalism (and not really generative at all). We are looking for a precise formulation of a generative dependency grammar, and the question has remained open whether there is an alternate formalism which allows for an unbounded number of adjuncts, introduces all daughter n"
W04-1503,J99-2004,0,0.0901481,"Missing"
W04-1503,P00-1058,0,0.0564617,"Missing"
W04-1503,P97-1003,0,0.061391,"act parse forest in a straightforward manner. In this paper, we present a simple generative formalism for dependency grammars based on Extended Context-Free Grammar, along with a parser; the formalism captures the intuitions of previous formalizations while deviating minimally from the much-used Context-Free Grammar. 1 Introduction Dependency grammar has a long tradition in syntactic theory, dating back to at least Tesni`ere’s work from the thirties. Recently, it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not. In this paper, we address an important issue in using grammar formalisms: the compact representation of the parse forest. Why is this an important issue? It is well known that for non-toy grammars and non-toy examples, a sentence can have a staggeringly large number of analyses (for example, using a contextfree grammar (CFG) extracted from the Penn Treebank, a sentence of 25 words may easily have 1,000,000 or more analyses). By way of an example of an ambiguous sentence (though with only t"
W04-1503,W98-0503,0,0.0418103,"tic sense as a type of syntactic dependency (another type being argument). We use head (or mother) and dependent (or daughter) to refer to nodes in a tree. Sometimes, in the formal and parsing literature, modifier is used to designate any dependent node, but we consider that usage confusing because of the related but different meaning of the term modifier that is well-established in the linguistic literature. 4 In fact, much of our formalism is very similar to (Lombardo, 1996), who however does not discuss parsing (only recognition), nor the representation of the parse forest. 5 Kahane et al. (1998) present three different types of rules, for subcategorization, modification, and linear precedence. In the formalism presented in this paper, they have been collapsed into one. 6 We leave aside here work on tree rewriting systems such as Tree Adjoining Grammar, which, when lexicalized, have derivation structures which are very similar to dependency trees. See (Rambow and Joshi, 1997) for a discussion related to TAG, and see (Rambow et al., 2001) for the definition of a tree-rewriting system which can be used to develop grammars whose derivations faithfully mirror syntactic dependency. 2 Previ"
W04-1503,P98-1106,1,0.547588,"n and easy-to-use generative formalism with a straightforward notion of parse forest. In particular, our formalism, Generative Dependency Grammar, allows for an unbounded number of daughter nodes in the derivation tree through the use of regular expressions in its rules. The parser uses the corresponding finite-state machines which straightforwardly allows for a binary-branching representation of the derivation structure for the purpose of parsing, and thus for a compact (polynomial and not exponential) representation of the parse forest. This formalism is based on previous work presented in (Kahane et al., 1998), which has been substantially reformulated in order to simplify it. 5 In particular, we do not address non-projectivity here, but acknowledge that for certain languages it is a crucial issue. We will extend our basic approach in the spirit of (Kahane et al., 1998) in future work. The paper is structured as follows. We start out by surveying previous formalizations of dependency grammar in Section 2. In Section 3, we introduce several formalisms, including Generative Dependency Grammar. We present a parsing algorithm in Section 4, and mention empirical results in Section 5. We then conclude. 3"
W04-1503,W04-3308,1,0.831117,"Missing"
W04-1503,J01-1004,1,0.796959,"formalism is very similar to (Lombardo, 1996), who however does not discuss parsing (only recognition), nor the representation of the parse forest. 5 Kahane et al. (1998) present three different types of rules, for subcategorization, modification, and linear precedence. In the formalism presented in this paper, they have been collapsed into one. 6 We leave aside here work on tree rewriting systems such as Tree Adjoining Grammar, which, when lexicalized, have derivation structures which are very similar to dependency trees. See (Rambow and Joshi, 1997) for a discussion related to TAG, and see (Rambow et al., 2001) for the definition of a tree-rewriting system which can be used to develop grammars whose derivations faithfully mirror syntactic dependency. 2 Previous Formalizations of Dependency Grammar We start out by observing that “dependency grammar” should be contrasted with “phrase structure grammar”, not “CFG”, which is a particular formalization of phrase structure grammar. Thus, just as it only makes sense to study the formal properties of a particular formalization of phrase structure grammar, the question about the formal properties of dependency grammar in general is not well defined, nor the"
W04-1503,1993.iwpt-1.22,0,0.0201459,"es make reference to other strucsaw saw H  HH man Pilar  HH a Pilar with telescope H  HH HH man with a telescope a a Figure 1: Two dependency trees V N Pilar V saw N D man a N P with saw Pilar N D P man with N a D N D telesope telesope a a Figure 2: Two dependency trees with lexical categories tures, these approaches cannot be formalized in a straightforward manner as context-free rewriting formalisms. In the third approach, which includes formalizations of dependency structure such as Dependency Tree Grammar of Modina (see (Dikovsky and Modina, 2000) for an overview), Link Grammar (Sleator and Temperley, 1993) or the tree-composition approach of Nasr (1996), rules construct the dependency tree incrementally; in these approaches, the grammar licenses dependency relations which, in a derivation, are added to the tree one by one, or in groups. In contrast, we are interested in a stringrewriting system; in such a system, we cannot add dependency relations incrementally: all daughters of a node must be added at once to represent a single rewrite step. In the fourth approach, the dependency grammar is converted into a headed context-free grammar (Abney, 1996; Holan et al., 1998), also the Basic Dependenc"
W04-1503,W04-3313,0,0.0284371,"Missing"
W04-1503,W00-1307,0,0.0605429,"Missing"
W04-2709,P98-1013,0,0.473703,"dependency parser (details in section 6) and is a useful starting point for semantic annotation at IL1, since it allows annotators to see how textual units relate syntactically when making semantic judgments. 4.1.3 IL2 IL2 is intended to be an interlingua, a representation of meaning that is reasonably independent of language. IL2 is intended to capture similarities in meaning across languages and across different lexical/syntactic realizations within a language. For example, IL2 is expected to normalize over conversives (e.g. X bought a book from Y vs. Y sold a book to X) (as does FrameNet (Baker et al 1998)) and non-literal language usage (e.g. X started its business vs. X opened its doors to customers). The exact definition of IL2 will be the major research contribution of this project. 4.2 The Omega Ontology In progressing from IL0 to IL1, annotators have to select semantic terms (concepts) to represent the nouns, verbs, adjectives, and adverbs present in each sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998)"
W04-2709,J96-2004,0,0.0756983,"Missing"
W04-2709,2003.mtsummit-eval.3,1,0.726095,"al content, modality, speech acts, etc. At the same time, while incorporating these items, vagueness and redundancy must be eliminated from the annotation language. Many inter-event relations would need to be captured such as entity reference, time reference, place reference, causal relationships, associative relationships, etc. Finally, to incorporate these, crosssentence phenomena remain a challenge. From an MT perspective, issues include evaluating the consistency in the use of an annotation language given that any source text can result in multiple, different, legitimate translations (see Farwell and Helmreich, 2003) for discussion of evaluation in this light. Along these lines, there is the problem of annotating texts for translation without including in the annotations inferences from the source text. 9 Conclusions This is a radically different annotation project from those that have focused on morphology, syntax or even certain types of semantic content (e.g., for word sense disambiguation competitions). It is most similar to PropBank (Kingsbury et al 2002) and FrameNet (Baker et al 1998). However, it is novel in its emphasis on: (1) a more abstract level of mark-up (interpretation); (2) the assignment"
W04-2709,P03-1001,1,0.790541,"sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998), NMSU’s Mikrokosmos (Mahesh and Nirenburg, 1995), ISI's Upper Model (Bateman et al., 1989) and ISI's SENSUS (Knight and Luk, 1994). After the uppermost region of Omega was created by hand, these various resources’ contents were incorporated and, to some extent, reconciled. After that, several million instances of people, locations, and other facts were added (Fleischman et al., 2003). The ontology, which has been used in several projects in recent years (Hovy et al., 2001), can be browsed using the DINO browser at http://blombos.isi.edu:8000/dino; this browser forms a part of the annotation environment. Omega remains under continued development and extension. 4.1.2 IL1 IL1 is an intermediate semantic representation. It associates semantic concepts with lexical units like nouns, adjectives, adverbs and verbs (details of the ontology in section 4.2). It also replaces the syntactic relations in IL0, like subject and object, with thematic roles, like agent, theme and goal (de"
W04-2709,C18-2019,0,0.0664532,"Missing"
W04-2709,A97-1011,0,0.0452745,"first present the annotation process and tools used with it as well as the annotation manuals. Finally, setup issues relating to negotiating multi-site annotations are discussed. 6.1 Annotation process The annotation process was identical for each text. For the initial testing period, only English texts were annotated, and the process described here is for English text. The process for non-English texts will be, mutatis mutandis, the same. Each sentence of the text is parsed into a dependency tree structure. For English texts, these trees were first provided by the Connexor parser at UMIACS (Tapanainen and Jarvinen, 1997), and then corrected by one of the team PIs. For the initial testing period, annotators were not permitted to alter these structures. Already at this stage, some of the lexical items are replaced by features (e.g., tense), morphological forms are replaced by features on the citation form, and certain constructions are regularized (e.g., passive) and empty arguments inserted. It is this dependency structure that is loaded into the annotation tool and which each annotator then marks up. The annotator was instructed to annotate all nouns, verbs, adjectives, and adverbs. This involves annotating e"
W04-2709,1994.amta-1.25,0,0.0933693,"Missing"
W04-2709,C98-1013,0,\N,Missing
W04-3308,J00-1004,0,0.0316841,"aselines on the test corpus (Section 23) the active valency of the lexical head in the FSM. A result, in retrieving the derivation tree, each item in the parse tree corresponds to an attachment of one word to another, and there are fewer items. Furthermore, our FSMs are built left-to-right, while Evans and Weir only explore FSMs constructed bottom-up from the lexical anchor of the tree (not unlike (Eisner, 2000)). As a result, we can perform a strict left-to-right parse, which is not straightforwardly possible in standard TAG parsing using FSMs. Our parsing algorithm is similar to the work of Alshawi et al. (2000). They use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized. Eisner (2000) provides a formalization of a system that uses two different automata to generate left and right children of a head. His formalism is very close to the one we present, but we use a single automaton. Also, the relation to an independently proposed syntactic formalism such as TAG is less obvious. In related work (Rambow et al., 2002), we have used the same automata constructed from an extracted TAG for parsing, but instead of using them in a chart parser, we have us"
W04-3308,J99-2004,0,0.26168,"ntary trees, we distinguish passive transitives without byphrase from active intransitives, and we include strongly governed prepositions (as determined in the PTB annotation, including passive by-phrases) in elementary verbal trees. Generally, function words such as auxiliaries or determiners are dependents of the lexical head,1 conjunctions (including punctuation functioning as conjunction) are dependent on the first conjunct and take the second conjunct as their argument, and conjunction chains are represented as right-branching rather than flat. 2.2 But Is Supertag-Based Parsing Feasible? Bangalore and Joshi (1999) claim that supertagging is “almost parsing”. What this means is that the syntactic information provided by supertags is so rich that there is little structural ambiguity left and the parse is almost 1 This is a linguistic choice and not forced by the formalism or the PTB. We prefer this representation as the resulting dependency tree is closer to predicate-argument structure. 57 In the second step, we directly compile a set of FSMs which are used by the parser. To derive a set of FSMs from a TAG, we do a depth-first traversal of each elementary tree in the grammar (but excluding the root and"
W04-3308,P00-1058,0,0.295422,"om bilexical information that they use (Gildea, 2001; Klein and Manning, 2003). 3 Representing a TAG as a Set of FSMs For the purpose of our parser, we represent a Tree Adjoining Grammar as a set of finite-state machines (FSMs). The FSMs form a (lexicalized) Recursive Transition Network (RTN). To extract an RTN from the Penn Treebank (PTB), we first extract a TAG, and then convert it to an RTN. This first step does not represent the research reported in this paper, and we describe it only for the sake of clarity. We use the approach of (Chen, 2001) (which is similar to (Xia et al., 2000) and (Chiang, 2000)). We use sections 02 to 21 of the Penn Treebank. However, we optimize the head percolation in the grammar extraction module to create meaningful dependency structures, rather than (for example) maximally simple elementary tree structures. For example, we include long-distance dependencies (wh-movement, relativization) in elementary trees, we distinguish passive transitives without byphrase from active intransitives, and we include strongly governed prepositions (as determined in the PTB annotation, including passive by-phrases) in elementary verbal trees. Generally, function words such as aux"
W04-3308,P02-1042,0,0.128533,"Missing"
W04-3308,P97-1003,0,0.163256,"a new domain, a nonlexical structural model can be reused from a previous domain, and only a supertagged corpus in the new domain is needed (to train the supertagger), not a structurally annotated corpus. Furthermore, this approach uses an explicit lexicalized grammar. As a consequence, when porting a parser to a new domain, learned parser preferences in the supertagger can be overridden explicitly for domain-idiosyncratic words before the parse happens. This overriding can happen through manually written or learned rules. By way of anecdotal example, in a recent application of the parser of Collins (1997) in which the WSJ-trained parser was applied to rather different text, sentences such as John put the book on the table were mostly analyzed with the PP attached to the noun, not the verb (as was always required in that domain). In the application, this had to be fixed by writing special post-processing code to rearrange the output of the parser; in our approach, we could simply state that put should always have a PP argument before the parse, and correct any output of the supertagger using simple hand-written rules. And finally, we point out that is is a different approach from the dominant b"
W04-3308,W01-0521,0,0.0271495,"by writing special post-processing code to rearrange the output of the parser; in our approach, we could simply state that put should always have a PP argument before the parse, and correct any output of the supertagger using simple hand-written rules. And finally, we point out that is is a different approach from the dominant bilexical one, and it is always worthwhile to pursue new approaches, especially as the performance of the bilexical parsers seems to be plateauing. In fact recent work has questioned to what extent bilexical parsers even profit from bilexical information that they use (Gildea, 2001; Klein and Manning, 2003). 3 Representing a TAG as a Set of FSMs For the purpose of our parser, we represent a Tree Adjoining Grammar as a set of finite-state machines (FSMs). The FSMs form a (lexicalized) Recursive Transition Network (RTN). To extract an RTN from the Penn Treebank (PTB), we first extract a TAG, and then convert it to an RTN. This first step does not represent the research reported in this paper, and we describe it only for the sake of clarity. We use the approach of (Chen, 2001) (which is similar to (Xia et al., 2000) and (Chiang, 2000)). We use sections 02 to 21 of the Penn"
W04-3308,P02-1043,0,0.104902,"Missing"
W04-3308,P03-1054,0,0.0166784,"ecial post-processing code to rearrange the output of the parser; in our approach, we could simply state that put should always have a PP argument before the parse, and correct any output of the supertagger using simple hand-written rules. And finally, we point out that is is a different approach from the dominant bilexical one, and it is always worthwhile to pursue new approaches, especially as the performance of the bilexical parsers seems to be plateauing. In fact recent work has questioned to what extent bilexical parsers even profit from bilexical information that they use (Gildea, 2001; Klein and Manning, 2003). 3 Representing a TAG as a Set of FSMs For the purpose of our parser, we represent a Tree Adjoining Grammar as a set of finite-state machines (FSMs). The FSMs form a (lexicalized) Recursive Transition Network (RTN). To extract an RTN from the Penn Treebank (PTB), we first extract a TAG, and then convert it to an RTN. This first step does not represent the research reported in this paper, and we describe it only for the sake of clarity. We use the approach of (Chen, 2001) (which is similar to (Xia et al., 2000) and (Chiang, 2000)). We use sections 02 to 21 of the Penn Treebank. However, we opt"
W04-3308,C02-2026,1,0.839924,"ht parse, which is not straightforwardly possible in standard TAG parsing using FSMs. Our parsing algorithm is similar to the work of Alshawi et al. (2000). They use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized. Eisner (2000) provides a formalization of a system that uses two different automata to generate left and right children of a head. His formalism is very close to the one we present, but we use a single automaton. Also, the relation to an independently proposed syntactic formalism such as TAG is less obvious. In related work (Rambow et al., 2002), we have used the same automata constructed from an extracted TAG for parsing, but instead of using them in a chart parser, we have used them to construct a single large FSM that produces a dependency tree. Needless to say, the number of embeddings allowed by such an approach is limited. in a first pass before structure is constructed, and structure is constructed only in a second pass in which no lexical information is used (other than the lexical emit probability for supertags). This result motivates further research into supertagging accuracy. If supertagging accuracy is improved, a lightw"
W04-3308,C92-2065,0,0.0324549,"e joint event A1 , . . . , An . A large range of different models can be used to compute such a joint probability, from the simplest which considers that all events are independent to the model that considers that they are all dependent. The three models that we descibe in this section vary in the way they model multi-adjunction (when several auxiliary trees are attached to a single node from the same direction). The reason to focus on this phenomenon comes P (A) = P (Root) Y × P (A) A∈A|O(A)=subst × Y P (LEF T (s, i)) s∈S,i∈nodes(s) × Y P (RIGHT (s, i)) s∈S,i∈nodes(s) This basically follows (Resnik, 1992; Schabes, 1992). The models we discuss here differ in how to compute the terms P (RIGHT (s, i)) and P (LEF T (s, i)). The probability of each attachment is estimated by maximum likelihood (the counts are obtained in the same step as the grammar extraction), and are added to the corresponding transition in the governor’s automaton as its weight. When the probabilistic model associates different 59 a,P(a|a) P(pos=0) p, P(b) ) 0 P(E T AR ND ST (a| a,P b,P(b,pos>2) 1 |a) a,P(a,pos=1) 0 a,P(a|b) b,P (b| b,P(b|a) 3 0 b,P(b,pos=1) |b) ST 2 P(pos>2) 3 b,P(b,pos=2) ND AR T) a,P(a,pos=2) 1 2 P(E b,P(b,"
W04-3308,P94-1022,0,0.010494,"he LDA are obtained by using the LDA as developed previously by Bangalore Srinivas, but using the same grammar we used for the full parser. Note that none of the numbers reported in this section can be directly compared to any numbers reported elsewhere, as 7 Related Work We are not aware of any other work that directly investigates the extent to which supertagging determines parsing. Chiang (2000) also parses with an automatically extracted TIG, but unlike our approach, he uses standard TAG/TIG parsing techniques (i.e., he reconstructs the derived tree in the chart, not the derivation tree). Rogers (1994) proposes a different context-free variant, “regular-form TAG”. The set of regular-form TAGs is a superset of the set of TIGs, and our construction cannot capture the added expressive power of regular-form TAG. Our conversion to FSMs is very similar to that of Evans and Weir (1997). One important difference is that they model TAG, while we model TIG. Another difference is that they use FSMs to encode the sequence of actions that need to be taken during a standard TAG parse (i.e., reconstructing the derived tree), while we encode 61 Method Baseline: LDA Baseline: full parse with random choice M"
W04-3308,J94-1004,0,0.0189536,"closer to predicate-argument structure. 57 In the second step, we directly compile a set of FSMs which are used by the parser. To derive a set of FSMs from a TAG, we do a depth-first traversal of each elementary tree in the grammar (but excluding the root and foot nodes of adjunct auxiliary trees) to obtain a sequence of nonterminal nodes. As usual, the elementary trees are tree schemas, with positions for the lexical heads. Substitution nodes are represented by obligatory transitions, adjunction by optional transitions (selfloops). (Note that in this paper, we assume adjunction as defined by Schabes and Shieber (1994).) Each node becomes two states of the FSM, one state representing the node on the downward traversal on the left side (the left node state), the other representing the state on the upward traversal, on the right side (the right node state). For leaf nodes, the two states immediately follow one another. The states are linearly connected with -transitions, with the left node state of the root node the start state, and its right node state the final state (except for predicative auxiliary trees – see below). We give a sample grammar in Figure 1 and the result of converting one of its trees to a"
W04-3308,J95-4002,0,0.210292,"onnected with -transitions, with the left node state of the root node the start state, and its right node state the final state (except for predicative auxiliary trees – see below). We give a sample grammar in Figure 1 and the result of converting one of its trees to an FSM in Figure 2. For each pair of adjacent states representing a substitution node, we add transitions between them labeled with the names of the trees that can substitute there. For the lexical head, we add a transition on that head. For footnodes of predicative auxiliary trees which are left auxiliary trees (in the sense of Schabes and Waters (1995), i.e., all nonempty frontier nodes are to the left of the footnode), we take the left node state as the final state. Finally, in the basic model in which adjunctions are modeled as independent, we proceed as follows for non-leaf nodes. (In Section 5, we will see two other models that treat non-leaf nodes in a more complex manner.) To each nonleaf state, we add one self loop transition for each tree in the grammar that can adjoin at that state from the specified direction (i.e., for a state representing a node on the downward traversal, the auxiliary tree must adjoin from the left), labeled wi"
W04-3308,C92-2066,0,0.0607005,"A1 , . . . , An . A large range of different models can be used to compute such a joint probability, from the simplest which considers that all events are independent to the model that considers that they are all dependent. The three models that we descibe in this section vary in the way they model multi-adjunction (when several auxiliary trees are attached to a single node from the same direction). The reason to focus on this phenomenon comes P (A) = P (Root) Y × P (A) A∈A|O(A)=subst × Y P (LEF T (s, i)) s∈S,i∈nodes(s) × Y P (RIGHT (s, i)) s∈S,i∈nodes(s) This basically follows (Resnik, 1992; Schabes, 1992). The models we discuss here differ in how to compute the terms P (RIGHT (s, i)) and P (LEF T (s, i)). The probability of each attachment is estimated by maximum likelihood (the counts are obtained in the same step as the grammar extraction), and are added to the corresponding transition in the governor’s automaton as its weight. When the probabilistic model associates different 59 a,P(a|a) P(pos=0) p, P(b) ) 0 P(E T AR ND ST (a| a,P b,P(b,pos>2) 1 |a) a,P(a,pos=1) 0 a,P(a|b) b,P (b| b,P(b|a) 3 0 b,P(b,pos=1) |b) ST 2 P(pos>2) 3 b,P(b,pos=2) ND AR T) a,P(a,pos=2) 1 2 P(E b,P(b,pos>2) a,P(a) P("
W04-3308,W00-1307,0,0.0509159,"parsers even profit from bilexical information that they use (Gildea, 2001; Klein and Manning, 2003). 3 Representing a TAG as a Set of FSMs For the purpose of our parser, we represent a Tree Adjoining Grammar as a set of finite-state machines (FSMs). The FSMs form a (lexicalized) Recursive Transition Network (RTN). To extract an RTN from the Penn Treebank (PTB), we first extract a TAG, and then convert it to an RTN. This first step does not represent the research reported in this paper, and we describe it only for the sake of clarity. We use the approach of (Chen, 2001) (which is similar to (Xia et al., 2000) and (Chiang, 2000)). We use sections 02 to 21 of the Penn Treebank. However, we optimize the head percolation in the grammar extraction module to create meaningful dependency structures, rather than (for example) maximally simple elementary tree structures. For example, we include long-distance dependencies (wh-movement, relativization) in elementary trees, we distinguish passive transitives without byphrase from active intransitives, and we include strongly governed prepositions (as determined in the PTB annotation, including passive by-phrases) in elementary verbal trees. Generally, functio"
W04-3308,1997.iwpt-1.11,0,\N,Missing
W05-0703,J94-1003,0,\N,Missing
W05-0703,C88-1064,0,\N,Missing
W05-0703,E87-1002,0,\N,Missing
W05-0703,J00-1006,1,\N,Missing
W05-0703,W98-1007,0,\N,Missing
W06-1501,C02-1126,1,0.849314,"monolingual parser, is simply set to 1 if an estimate is available for that level, so that it completely overrides the further backed-off models. The initial estimates for the Pt1i are set by hand. The availability of three backoff models makes it easy to specify the initial guesses at an appropriate level of detail: for example, one might give a general probability of some α ¯ mapping to α ¯ 0 using Pt13 , but then make special exceptions for particular lexical anchors using Pt11 or Pt12 . Finally Pt2 is reestimated by EM on some heldout unannotated sentences of L0 , using the same method as Chiang and Bikel (2002) but on the syntactic transfer probabilities instead of the monolingual parsing model. Another difference is that, following Bikel (2004), we do not recalculate the λi at each iteration, but use the initial values throughout. Ps (α, α0 |η, η 0 ) ≈ Ps (α |η)Pt (α0 |α) (5) 5 A Synchronous TSG-SA for Dialectal Arabic Parameter estimation and smoothing Ps and Psa are the parameters of a monolingual TSG+SA and can be learned from a monolingual Just as the probability model discussed in the preceding section factored the rewriting probabilities 5 into three parts, we create a synchronous TSG-SA and"
W06-1501,J94-1004,0,0.050562,"pendency trees for the MSA and LA sentences are isomorphic, as shown in Figure 2. They differ only in the node labels. 4 Model 4.1 The synchronous TSG+SA formalism Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sisteradjunction (TSG+SA). Tree-substitution grammar (Schabes, 1990) is TAG without auxiliary trees or adjunction; instead we include a weaker composition operation, sister-adjunction (Rambow et al., 2001), in which an initial tree is inserted between two sister nodes (see Figure 4). We allow multiple sister-adjunctions at the same site, similar to how Schabes and Shieber (1994) allow multiple adjunctions of modifier auxiliary trees. 3 S S NP VP NP NNP VP NP Qintex VBD NNS sold assets ⇒ VP NNP VBD PRT NP Qintex sold RP NNS off assets PRT RP off Figure 4: Sister-adjunction, with inserted material shown with shaded background                 S NPi ↓ 1 S VP VP , V   ‘like’  NP NP↓ 2 V &+' ti ‘like’ NP↓ 1 NP↓ 2                Figure 5: Example elementary tree pair of a synchronous TSG: the SVO transformation (LA on left, MSA on right) 4 hη, η 0 i is Ps (α, α0 |η, η 0 ), and the probability of sister-adjoining hα, α0 i at a sister"
W06-1501,E06-1047,1,0.800615,"while in MSA, the VSO order is more common. Second, we see that the demonstrative determiner follows the noun in LA, but precedes it in MSA. Finally, we see that the negation marker follows the verb in LA, while it precedes the verb in MSA. (Levantine also has other negation markers that precede the verb, as well as the circumfix m- -$.) The two phrase structure trees are shown in Figure 1 in the convention of the Linguistic Data Consortium (Maamouri et al., 2004). Unlike the phrase 2 Related Work This paper is part of a larger investigation into parsing Arabic dialects (Rambow et al., 2005; Chiang et al., 2006). In that investigation, we examined three different approaches: • Sentence transduction, in which a dialect sentence is roughly translated into one or more MSA sentences and then parsed by an MSA parser. • Treebank transduction, in which the MSA treebank is transduced into an approximation of a LA treebank, on which a LA parer is then trained. • Grammar transduction, which is the name given in the overview papers to the approach discussed in this paper. The present paper provides for the first time a complete technical presentation of this approach. Overall, grammar transduction outperformed"
W06-1501,P00-1058,1,0.843629,"us (except for development and testing), nor of a parallel LA-MSA corpus. The approach described in this paper uses a special parameterization of stochastic synchronous TAG (Shieber, 1994) which we call a “hidden TAG model.” This model couples a model of MSA trees, learned from the Arabic Treebank, with a model of MSA-LA translation, which is initialized by hand and then trained in an unsupervised fashion. Parsing new LA sentences then entails simultaneously building a forest of MSA trees and the corresponding forest of LA trees. Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution Abstract This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA). We use it to parse a language for which there is no training data, by leveraging off a second, related language for which there is abundant training data. The grammar for the resource-rich side is automatically extracted from a treebank; the grammar on the resource-poor side and the synchronization are created by handwritten rules. Our approach thus represents a combination of grammar-based and empirical natural langua"
W06-1501,W04-3207,0,0.0319903,"e or more MSA sentences and then parsed by an MSA parser. • Treebank transduction, in which the MSA treebank is transduced into an approximation of a LA treebank, on which a LA parer is then trained. • Grammar transduction, which is the name given in the overview papers to the approach discussed in this paper. The present paper provides for the first time a complete technical presentation of this approach. Overall, grammar transduction outperformed the other two approaches. In other work, there has been a fair amount of interest in parsing one language using another language, see for example (Smith and Smith, 2004; 2 S S NP-TPC VP VP    ) ‘men’i V NEG  NP-SBJ NP-OBJ ‘like’ ti N ‘not’ DET    ‘work’ ‘this’ NEG V NP-SBJ ( &+'    ‘not’ ‘like’ ‘men’ NP-OBJ DET N # * ‘this’ ‘work’ Figure 1: LDC-style left-to-right phrase structure trees for LA (left) and MSA (right) for sentence (1)   !    ) ‘work’ ‘men’ $ &+' ’like’ ‘not’ *    ) ( ‘work’ ‘men’ ‘not’ # ‘this’ ‘like’ ‘this’ Figure 2: Unordered dependency trees for LA (left) and MSA (right) for sentence (1) NP NNP Qintex S NP VP VBD NP PRT NP RP NNS off assets (α3 ) (α4 ) A synchronous TSG+SA"
W06-1501,levy-andrew-2006-tregex,0,0.0122304,"rect) tag. The results are shown in Table 1. The baseline is simply the application of a pure MSA Chiang parser to LA. We see that important improvements are obtained using the lexical mapping. Adding the SVO transformation does not improve the results, but the NEG and BD transformations help slightly, and their effect is (partly) Syntactic Mapping Because of the underlying syntactic similarity between the two varieties of Arabic, we assume that every tree in the MSA grammar extracted from the MSA treebank is also a LA tree. In addition, we define tree transformations in the Tsurgeon package (Levy and Andrew, 2006). These consist of a pattern which matches MSA elementary trees in the extracted grammar, and a transformation which produces a LA elementary tree. We perform the following tree transformations on all elementary trees which match the underlying MSA pattern. Thus, each MSA tree corresponds to at least two LA trees: the original one and the transformed one. If several transformations apply, we obtain multiple transformed trees. • Negation (NEG): we insert a $ negation marker immediately following each verb. 6                S S VP N PRP$ VP S0 ↓ 1  ‘want’  , V S0 ↓ 1 ' ‘"
W06-1501,W00-1307,0,0.0330982,"ubject-VerbObject (SVO) constructions occur in MSA and LA treebanks. But pure VSO constructions (without pro-drop) occur in the LA corpus only 10ordering in MSA. Hence, the goal is to skew the distributions of the SVO constructions in the MSA data. Therefore, VSO constructions are replicated and converted to SVO constructions. One possible resulting pair of trees is shown in Figure 5. • Ps and Psa are the parameters of a monolingual TSG+SA for MSA. We extract a grammar for the resource-rich language (MSA) from the Penn Arabic Treebank in a process described by Chiang and others (Chiang, 2000; Xia et al., 2000; Chen, 2001). • For the lexical transfer model Pt2 , we create by hand a probabilistic mapping between (word, POS tag) pairs in the two languages. • For the syntactic transfer model P t1 , we created by hand a grammar for the resource-poor language and a mapping between elementary trees in the two grammars, along with initial guesses for the mapping probabilities. • The bd construction (BD): bd is a LA noun that means ‘want’. It acts like a verb in verbal constructions yielding VP constructions headed by NN. It is typically followed by an enclitic possessive pronoun. Accordingly, we defined a"
W06-1501,maamouri-etal-2006-developing,1,0.825899,"shown in Figure 6. We discuss the hand-crafted lexicon and synchronous grammar in the following subsections. 5.1 Lexical Mapping We used a small, hand-crafted lexicon of 100 words which mapped all LA function words and some of the most common open-class words to MSA. We assigned uniform probabilities to the mapping. All other MSA words were assumed to also be LA words. Unknown LA words were handled using the standard unknown word mechanism. 5.2 6 Experimental Results While our approach does not rely on any annotated corpus for LA, nor on a parallel corpus MSALA, we use a small treebank of LA (Maamouri et al., 2006) to analyze and test our approach. The LA treebank is divided into a development corpus and a test corpus, each about 11,000 tokens (using the same tokenization scheme as employed in the MSA treebank). We first use the development corpus to determine which of the transformations are useful. We use two conditions. In the first, the input text is not tagged, and the parser hypothesizes tags. In the second, the input text is tagged with the gold (correct) tag. The results are shown in Table 1. The baseline is simply the application of a pure MSA Chiang parser to LA. We see that important improvem"
W06-1501,J01-1004,1,\N,Missing
W06-1503,P03-1024,1,0.901833,"Missing"
W06-1503,W05-1522,0,0.110278,"Missing"
W06-1503,W06-1520,0,\N,Missing
W06-1503,C96-1034,0,\N,Missing
W06-1503,P98-1033,0,\N,Missing
W06-1503,C98-1033,0,\N,Missing
W08-2318,W04-3325,0,0.0375143,"Missing"
W08-2318,W07-0402,0,0.023102,"Missing"
W08-2318,J01-1004,1,0.81464,"represent, as in TAG, a lexical head, its (extended) projection, and positions for its arguments. Rambow (1994) shows that the parsing problem (with a fixed grammar) is polynomial in the length of the input sentence, if the UVGDL is lexicalized (as we assume all our grammars are). This formalism can also be seen as a tree description language, with the context-free rules in a set as statements of immediate dominance between one node and one or more daughters (along with constraints on linear precedence among the daughters), the dominance links as statements of dominance (Vijay-Shanker, 1992; Rambow et al., 2001). We choose the rewriting formulation because a synchronous version (SynchUVGDL) was defined by Rambow and Satta (1996) and some initial results on computation were proposed. Specifically, they claim that the parse-to-forest translation problem for a lexicalized SynchUVGDL can be computed in polynomial time. 4 Overview of Semantics with SynchUVGDL We have been developing a semantic formalism that can be easily modeled using SynchUVGDL (see (Lerman and Rambow, 2008) for details). We adopt the notation of Montague semantics, wherein e is the type of entities, t’ is the type of truth values, and"
W08-2318,E03-1030,0,0.0222146,"in the linked node in the tree in the other grammar. Several different definitions of SynchTAG are possible (Shieber, 1994), and the most interesting definition has the property that the derivation trees for the two derivations in the two synchronized grammars are isomorphic, so that we can talk of a single, TAG-style derivation tree for a SynchTAG. Subsequently, a series of research was published which did not use SynchTAG for semantics, but instead generated a semantic representation during the syntactic derivation, often using feature structures (Kallmeyer, 2002; Kallmeyer and Joshi, 2003; Gardent and Kallmeyer, 2003; M. Romero, 2004). The principal difference is that in this line of work, the semantics is not itself modeled in a TAG. Recently, Nesson and Shieber (2006; 2007) have revived the approach using SynchTAG. They have shown that a large number of different constructions can be given elegant analyses using a SynchTAG-based analysis. Our work is in this tradition. 3 UVGDL and SynchUVGDL If TAG can be seen as a partial derivation in a CFG, pre-assembled for convenience, in a UVGDL (Rambow, 1994) the partial derivation is colProceedings of The Ninth International Workshop on Tree Adjoining Grammars a"
W08-2318,C90-3045,0,0.328674,"er to get around this problem, and present the implications of such a representation. The paper is structured as follows. We first review work in semantics and Tree Adjoining Grammar (TAG), and then briefly present synchronous UVGDL (Section 3). In Section 4, we summarize our approach to modeling semantics using SynchUVGDL, and then discuss the similarities between quantification and conjunction in Section 5 and propose a simple approach. Section 6 presents a serious problem for the proposed approach (the conjunction of quantified NPs), and Section 7 presents our solution. 2 TAG and Semantics Shieber and Schabes (1990) were the first to propose a syntactic-semantic grammar in the TAG framework, by using synchronous TAG (SynchTAG). In SynchTAG, two TAGs are linked in such a way that trees in one grammar correspond to trees in the other grammar, and the nodes in corresponding trees are linked. When we substitute or adjoin a tree in a node, then we must substitute or adjoin a corresponding tree in the linked node in the tree in the other grammar. Several different definitions of SynchTAG are possible (Shieber, 1994), and the most interesting definition has the property that the derivation trees for the two der"
W08-2318,J87-1005,0,0.403003,"e “pick any” (implying, as per the normal universal quantifier, that the same truth conditions hold for any member of the set). For an existential quantifier, the choice function would be something like “nondeterministically pick a privileged member” (implying that there exists at least one privileged member of whom something is true). These quantifiers are now contiguous, and may be used with our conjunction framework trivially, as seen in figure 7. 6 The notion of underspecifying a quantifier in some manner is not new; as will be shown shortly, this representation is similar to one used in (Hobbs and Shieber, 1987). Proceedings of The Ninth International Workshop on Tree Adjoining Grammars and Related Formalisms Tübingen, Germany. June 6-8, 2008. 138 Lerman and Rambow Figure 7: “Every boy and most girls lifted-thepiano [together]” with underspecified quantifiers. The only challenge arising from this representation is that we have destroyed all notion of quantifier scope. Because quantifiers are now local to their restrictor sets, we are no longer able to distinguish the two common readings of sentences such as “Every boy visited a store.” At some level, this is a good thing, as sentences such as these a"
W08-2318,J92-4004,0,0.143088,"UVGDL can be used to represent, as in TAG, a lexical head, its (extended) projection, and positions for its arguments. Rambow (1994) shows that the parsing problem (with a fixed grammar) is polynomial in the length of the input sentence, if the UVGDL is lexicalized (as we assume all our grammars are). This formalism can also be seen as a tree description language, with the context-free rules in a set as statements of immediate dominance between one node and one or more daughters (along with constraints on linear precedence among the daughters), the dominance links as statements of dominance (Vijay-Shanker, 1992; Rambow et al., 2001). We choose the rewriting formulation because a synchronous version (SynchUVGDL) was defined by Rambow and Satta (1996) and some initial results on computation were proposed. Specifically, they claim that the parse-to-forest translation problem for a lexicalized SynchUVGDL can be computed in polynomial time. 4 Overview of Semantics with SynchUVGDL We have been developing a semantic formalism that can be easily modeled using SynchUVGDL (see (Lerman and Rambow, 2008) for details). We adopt the notation of Montague semantics, wherein e is the type of entities, t’ is the type"
W08-2318,W02-2218,0,0.0193862,"t substitute or adjoin a corresponding tree in the linked node in the tree in the other grammar. Several different definitions of SynchTAG are possible (Shieber, 1994), and the most interesting definition has the property that the derivation trees for the two derivations in the two synchronized grammars are isomorphic, so that we can talk of a single, TAG-style derivation tree for a SynchTAG. Subsequently, a series of research was published which did not use SynchTAG for semantics, but instead generated a semantic representation during the syntactic derivation, often using feature structures (Kallmeyer, 2002; Kallmeyer and Joshi, 2003; Gardent and Kallmeyer, 2003; M. Romero, 2004). The principal difference is that in this line of work, the semantics is not itself modeled in a TAG. Recently, Nesson and Shieber (2006; 2007) have revived the approach using SynchTAG. They have shown that a large number of different constructions can be given elegant analyses using a SynchTAG-based analysis. Our work is in this tradition. 3 UVGDL and SynchUVGDL If TAG can be seen as a partial derivation in a CFG, pre-assembled for convenience, in a UVGDL (Rambow, 1994) the partial derivation is colProceedings of The N"
W08-2318,P96-1016,1,\N,Missing
W08-2318,W90-0102,0,\N,Missing
W09-3012,J04-3002,0,0.199612,"Missing"
W09-3012,krestel-etal-2008-minding,0,0.0374878,"Missing"
W09-3012,C08-1101,0,0.0165575,"is is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief concerns propositions about the future, such as GM wil"
W09-3012,W05-0308,0,0.0144516,"d we are only interested in the writer’s beliefs. This is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief"
W09-3036,H05-1066,0,0.0147844,"ation of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dependency structure and phrase structure in Section 2. Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our conversion approach. 2 Two Kinds of Syntactic Structure Two different approaches to describing syntactic structure, dependency structure (DS) (Mel’čuk 1979) and phrase structure (PS) (Chomsky, 1981), have in a sense divided the field in two, with parallel efforts on both sides. Formally, in a PS tree,"
W09-3036,P99-1065,0,0.0911636,"ed; for example, in a case of longdistance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent). Having made this choice, we can determine how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dep"
W09-3036,P03-1054,0,0.00169422,"how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is itself an issue in parsing research. In the application of the Collins parser to the Prague Dependency Treebank (Collins et al. 1999) the automatic mapping from dependency to phrase-structure was a major area of research. Similarly, automatically changing the representation in a phrase structure treebank can also improve parsing results (for example Klein & Manning 2003). Finally, there is increasing interest in the use of dependency parses in NLP applications, as they are considered to be simpler structures which can be computed more rapidly and are closer to the kinds of semantic representations that applications can make immediate use of (McDonald et al. 2005, CoNLL 2006 Shared Task). We first provide a comparison of dependency structure and phrase structure in Section 2. Section 3 describes our treebank, Section 4 explores language-specific linguistic issues that require special attention to ensure consistent conversion, and Section 5 summarizes our conve"
W09-3036,C02-2025,0,0.0152376,"d theories. The next section highlights our most salient representation choices in Treebank design. 3 Treebank Design Our goal is the delivery of a treebank that is multi-representational: it will have a syntactic dependency version and a phrase structure version. Another recent trend in treebanking is the addition of deeper, semantic levels of annotation on top of the syntactic annotations of the PTB, for example PropBank (Palmer et al. 2005). A multi-layered approach is also found in the Prague Dependency Treebank (Hajič et al. 2001), or in treebanks based on LFG (King et al. 2003) or HPSG (Oepen et al. 2002). A lesson learned here is that the addition of deeper, more semantic levels may be complicated if the syntactic annotation was not designed with the possibility of multiple layers of annotation in mind. We therefore also propose a treebank that is from the start multi-layered: we will include a PropBank-style predicate-argument annotation in the release. Crucially, the lexical subcategorization frames that are made explicit during the process of propbanking should always inform the syntactic structure of the treebanking effort. In addition, some of the distinctions made by PS that are not nat"
W09-3036,J05-1004,1,0.175107,"s are carefully coordinated. 1 Introduction Annotated corpora have played an increasingly important role in the training of supervised natural language processing components. Today, treebanks have been constructed for many languages, including Arabic, Chinese, Czech, English, French, German, Korean, Spanish, and Turkish. This paper describes the creation of a Hindi/Urdu multi-representational and multilayered treebank. Multi-layered means that we design the annotation process from the outset to include both a syntactic annotation and a lexical semantic annotation such as the English PropBank (Palmer et al. 2005). Multirepresentational means that we distinguish conceptually what is being represented from how it is represented; for example, in a case of longdistance wh-movement in English as in Who do you think will come, we can choose to represent the fact that who is an argument of come, or not (what to represent). Having made this choice, we can determine how to represent it: For example, we can use a discontinuous constituent (crossing arcs), or we can use a trace and coindexation. Flexibility of representation is important because the proper choice of representation of the syntax of a language is"
W09-3036,P07-1021,0,\N,Missing
W09-3953,J97-1005,1,\N,Missing
W09-3953,C04-1128,0,\N,Missing
W09-3953,W01-1607,0,\N,Missing
W09-3953,H01-1015,1,\N,Missing
W09-3953,P04-1085,0,\N,Missing
W09-3953,J00-3003,0,\N,Missing
W09-3953,H93-1005,0,\N,Missing
W09-3953,P08-1081,0,\N,Missing
W09-3953,kruijff-korbayova-etal-2006-sammie,0,\N,Missing
W09-3953,devillers-etal-2002-annotations,0,\N,Missing
W09-3953,W04-1008,0,\N,Missing
W09-3953,D08-1081,0,\N,Missing
W09-3953,W06-3406,0,\N,Missing
W10-1402,W06-2920,0,0.166177,"Missing"
W10-1402,P99-1065,0,0.536202,"Missing"
W10-1402,H05-1100,0,0.287657,"Missing"
W10-1402,J08-3003,0,0.384489,"Missing"
W10-1402,P05-1071,1,0.203632,"Missing"
W10-1402,P09-2056,1,0.575385,"aspect of syntax, which is often not explicitly modeled in parsing, involves morphological constraints on syntactic structure, such as agreement. In this paper, we explore the role of morphological features in parsing Modern Standard Arabic (MSA). For MSA, the space of possible morphological features is fairly large. We determine which morphological features help and why, and we determine the upper bound for their contribution to parsing quality. We first present the corpus we use (§2), then relevant Arabic linguistic facts (§3); we survey related We use the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009). Specifically, we use the portion converted from part 3 of the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) to the CATiB format, which enriches the CATiB dependency trees with full PATB morphological information. CATiB’s dependency representation is based on traditional Arabic grammar and emphasizes syntactic case relations. It has a reduced POS tagset (with six tags only), but a standard set of eight dependency relations: SBJ and OBJ for subject and (direct or indirect) object, respectively, (whether they appear pre- or post-verbally); IDF for the idafa (possessive) relation; MOD for"
W10-1402,P98-1080,0,0.306207,"Missing"
W10-1402,E06-1011,0,0.0595554,"Missing"
W10-1402,C08-1081,0,0.370938,"Missing"
W10-1402,W03-3017,0,0.0667882,"ld vs. predicted POS and morphological feature values for all models; (c) prediction accuracy of each POS tagset and morphological feature; (d) the contribution of numerous morphological features in a controlled fashion; and (e) the contribution of certain feature and POS tagset combinations. All results are reported mainly in terms of labeled attachment accuracy (parent word and the dependency relation to it). Unlabeled attachment accuracy and label accuracy are also given, space permitting. 5.2 Parser For all experiments reported here we used the syntactic dependency parser MaltParser v1.3 (Nivre, 2003; Nivre, 2008; Kübler et al., 2009) – a transition-based parser with an input buffer and a stack, using SVM classifiers to predict the next state in the parse derivation. All experiments were done using the Nivre &quot;eager&quot; algorithm.7 We trained the parser on the training portion of PATB part 3 (Maamouri et al., 2004). We used the same split as in Zitouni et al. (2006) for dev/test, and kept the test unseen during training. There are five default attributes, in the MaltParser terminology, for each token in the text: word ID (ordinal position in the sentence), word form, POS 7 Nivre (2008) report"
W10-1402,J08-4003,0,0.173037,"s, and verbs; and mode for verbs) outperforms other combinations. Our approach is comparable to their work in terms of its systematic exploration of the space of morphological features. We also find that the number feature helps for Arabic. Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima’an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing. As for work on Arabic, results have been reported on PATB (Kulick et al., 2006; Diab, 2007), the Prague Dependency Treebank (PADT) (Buchholz and Marsi, 2006; Nivre, 2008) and the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009). Besides the work we describe in §3, Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al., 2007), trained on the PADT. His results are not directly comparable to ours because of the different treebanks representations and tokenization used, even though all our experiments reported here were performed using the MaltParser. Our results agree with previous published work on Arabic and Hebrew in that marking the definite article is helpful for parsing. However, we go beyond previous work in that we a"
W10-1402,W07-2219,0,0.303941,"Missing"
W10-1402,P06-1073,0,0.233763,"curacy (parent word and the dependency relation to it). Unlabeled attachment accuracy and label accuracy are also given, space permitting. 5.2 Parser For all experiments reported here we used the syntactic dependency parser MaltParser v1.3 (Nivre, 2003; Nivre, 2008; Kübler et al., 2009) – a transition-based parser with an input buffer and a stack, using SVM classifiers to predict the next state in the parse derivation. All experiments were done using the Nivre &quot;eager&quot; algorithm.7 We trained the parser on the training portion of PATB part 3 (Maamouri et al., 2004). We used the same split as in Zitouni et al. (2006) for dev/test, and kept the test unseen during training. There are five default attributes, in the MaltParser terminology, for each token in the text: word ID (ordinal position in the sentence), word form, POS 7 Nivre (2008) reports that non-projective and pseudoprojective algorithms outperform the &quot;eager&quot; projective algorithm in MaltParser; however, our training data did not contain any non-projective dependencies, so there was no point in using these algorithms. The Nivre &quot;standard&quot; algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did slightly worse"
W10-1402,C98-1077,0,\N,Missing
W10-1402,J08-4010,0,\N,Missing
W10-1803,day-etal-2004-callisto,0,0.0222587,"example highlights the difference between our definition of Interaction events and ACE’s definition of Contact events. For this reason, in Figure 5, 51 of our INR relations do not overlap with ACE event categories. (6) In central Baghdad, [a Reuters cameraman] and [a cameraman for Spain’s Telecinco] died when an American tank fired on the Palestine Hotel 4 ACE has annotated the above example as an event of type C ONFLICT-ATTACK in which there are two entities that are of type person: the Reuters cameraman and the cameraman for Annotation Procedure We used Callisto (a configurable workbench) (Day et al., 2004) to annotate the ACE-2005 corpus for 6 The ACE event annotated in the sentence is of type “Personell-Elect” (span election) which is not recorded as an event between two or more entities and is not relevant here. 5 Recall that our event annotations are between exactly two entities of type PER.Individual or PER.Group. 24 62 Documents Verbal INR NonVerbal Conflict (5) Contact (32) Attack Meet Phone-Write Justice-* (13) Life (7) Transaction (2) Die Divorce Injure Transfer-Money Not Found Near (66) 0 26 0 9 0 0 0 0 31 Far (17) 0 0 3 3 0 1 0 0 10 Near (14) 3 0 0 0 2 0 0 1 8 Far (3) 0 0 0 0 0 0 0 1"
W10-1803,doddington-etal-2004-automatic,0,0.189027,"mail exchanges Figure 3: Network formed by augmenting the email exas links. Identical color or shape implies structural equivachange network above with links that occur in the content of lence. Only Sam and Mary are structurally equivalent the emails. Now, Kate and Mary are structurally equivalent, as are Sam and Jacob. for such a technique to be created. This is because our annotations capture interactions described in the content of the email such as face-to-face meetings, physical co-presence and cognizance. notating entities, relations and events in free text, most notably the ACE effort (Doddington et al., 2004). We intend to leverage this work as much as possible. The task of social network extraction can be broadly divided into 3 tasks: 1) entity extraction; 2) social relation extraction; 3) social event extraction. We are only interested in the third task, social event extraction. For the first two tasks, we can simply use the annotation guidelines developed by the ACE effort. Our social events, however, do not clearly map to the ACE events: we introduce a comprehensive set of social events which are very different from the event annotation that already exists for ACE. This paper is about the anno"
W11-0705,E09-1004,1,0.167584,"ection 6 we present the design of our tree kernel. In section 7 we give details of our feature based approach. In section 8 we present our experiments and discuss the results. We conclude and give future directions of research in section 9. 2 Literature Survey Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons like “:)” “:-)” as positive and negative emoticons like “:(” “:-(” as negative. They build models using Naive Bayes, MaxEnt and Support Vector Machines (SVM), and they report SVM outper"
W11-0705,C10-2005,0,0.777352,"baseline. In addition we explore a different method of data representation and report significant improvement over the unigram models. Another contribution of this paper is that we report results on manually annotated data that does not suffer from any known biases. Our data is a random sample of streaming tweets unlike data collected by using specific queries. The size of our hand-labeled data allows us to perform crossvalidation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). They use polarity predictions from three websites as noisy labels to train a model and use 1000 manually labeled tweets for tuning and another 1000 manually labeled tweets for testing. They however do not mention how they collect their test data. They propose the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS of words. We extend their approach by using real valued prior polarity, and by combining prior polarity with POS. Our results show that the features that enhance the perfo"
W11-0705,C04-1121,0,0.0262243,"d tweets for testing. They however do not mention how they collect their test data. They propose the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS of words. We extend their approach by using real valued prior polarity, and by combining prior polarity with POS. Our results show that the features that enhance the performance of our classifiers the most are features that combine prior polarity of words with their parts of speech. The tweet syntax features help but only marginally. Gamon (2004) perform sentiment analysis on feeadback data from Global Support Services survey. One aim of their paper is to analyze the role of linguistic features like POS tags. They perform extensive feature analysis and feature selection and demonstrate that abstract linguistic analysis features contributes to the classifier accuracy. In this paper we perform extensive feature analysis and show that the use of only 100 abstract linguistic features performs as well as a hard unigram baseline. 3 Data Description Twitter is a social networking and microblogging service that allows users to post real time"
W11-0705,C04-1200,0,0.0301245,"ional resources. In section 5 we present our prior polarity scoring scheme. In section 6 we present the design of our tree kernel. In section 7 we give details of our feature based approach. In section 8 we present our experiments and discuss the results. We conclude and give future directions of research in section 9. 2 Literature Survey Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons like “:)” “:-)” as positive and negative emoticons like “:(” “:-(” as negative. They build models using"
W11-0705,P03-1054,0,0.0424695,"negations (e.g. not, no, never, n’t, cannot) by tag “NOT”, and e) replace a sequence of repeated characters by three characters, for example, convert coooooooool to coool. We do not replace the sequence by only two characters since we want to differentiate between the regular usage and emphasized usage of the word. Acronym gr8, gr8t lol rotf bff English expansion great laughing out loud rolling on the floor best friend forever Table 1: Example acrynom and their expansion in the acronym dictionary. We present some preliminary statistics about the data in Table 3. We use the Stanford tokenizer (Klein and Manning, 2003) to tokenize the tweets. We use a stop word dictionary3 to identify stop words. All the other words which are found in WordNet (Fellbaum, 1998) are counted as English words. We use 1 http://en.wikipedia.org/wiki/List of emoticons http://www.noslang.com/ 3 http://www.webconfs.com/stop-words.php 2 Emoticon :-) :) :o) :] :3 :c) :D C: :-( :( :c :[ D8 D; D= DX v.v :| Polarity Positive Extremely-Positive Negative Extremely-Negative Neutral Table 2: Part of the dictionary of emoticons the standard tagset defined by the Penn Treebank for identifying punctuation. We record the occurrence of three stand"
W11-0705,pak-paroubek-2010-twitter,0,0.566631,"Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons like “:)” “:-)” as positive and negative emoticons like “:(” “:-(” as negative. They build models using Naive Bayes, MaxEnt and Support Vector Machines (SVM), and they report SVM outperforms other classifiers. In terms of feature space, they try a Unigram, Bigram model in conjunction 31 with parts-of-speech (POS) features. They note that the unigram model outperforms all other models. Specifically, bigrams and POS features do not help. Pak and Paroubek (2010) collect data following a"
W11-0705,P04-1035,0,0.114343,"ils about the data. In section 4 we discuss our pre-processing technique and additional resources. In section 5 we present our prior polarity scoring scheme. In section 6 we present the design of our tree kernel. In section 7 we give details of our feature based approach. In section 8 we present our experiments and discuss the results. We conclude and give future directions of research in section 9. 2 Literature Survey Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons like “:)” “:-)” as po"
W11-0705,P02-1053,0,0.0445841,", we give details about the data. In section 4 we discuss our pre-processing technique and additional resources. In section 5 we present our prior polarity scoring scheme. In section 6 we present the design of our tree kernel. In section 7 we give details of our feature based approach. In section 8 we present our experiments and discuss the results. We conclude and give future directions of research in section 9. 2 Literature Survey Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons"
W11-0705,H05-1044,0,0.355008,"scoring scheme. In section 6 we present the design of our tree kernel. In section 7 we give details of our feature based approach. In section 8 we present our experiments and discuss the results. We conclude and give future directions of research in section 9. 2 Literature Survey Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons like “:)” “:-)” as positive and negative emoticons like “:(” “:-(” as negative. They build models using Naive Bayes, MaxEnt and Support Vector Machines (SVM), and"
W11-0905,P84-1106,0,0.696239,"Missing"
W11-0905,P98-1013,0,0.126381,"c argument slots describing participants in the frame. For instance, the word buy evokes the frame for a commercial transaction scenario, which includes a buyer and a seller that exchange money for goods. A speaker is aware of what typical buyers, sellers, and goods are. He may also have a mental prototype of the visual scenario itself (e.g. standing at a counter in a store). In FS the role of syntactic theory and the lexicon is to explain how the syntactic dependents of a word that realizes a frame (i.e. arguments and adjuncts) are mapped to frame elements via valence patterns. FrameNet (FN; Baker et al. (1998), Ruppenhofer et al. (2010)) is a lexical resource based on FS. Frames in FN (around 1000) 1 are defined in terms of their frame elements, relations to other frames and semantic types of FEs. Beyond this, the meaning of the frame (how the FEs are related to each other) is only described in natural language. FN contains about 11,800 lexical units, which are pairings of words and frames. These come with annotated example sentences (about 150,000) to illustrate their valence patterns. FN contains a network of directed frame-to-frame relations. In the INHERI TANCE relation a child-frame inherits a"
W11-0905,W01-1301,0,0.0427164,"a sink. From world knowledge we know (via instances of the TYPICAL - LOCATION frame) that washing food typically takes place in the KITCHEN. To create a scene we compose the two vignettes together by unifying the sink in the location vignette with the sink in the action vignette. 6 Related Work The grounding of natural language to graphical relations has been investigated in very early text-toscene systems (Boberg, 1972), (Simmons, 1975), (Kahn, 1979), (Adorni et al., 1984), and then later in Put (Clay and Wilhelms, 1996), and WordsEye (Coyne and Sproat, 2001). Other systems, such as CarSim (Dupuy et al., 2001), Jack (Badler et al., 1998), and CONFUCIUS (Ma and McKevitt, 2006) target animation and virtual environments rather than scene construction. A graphically grounded lexical-semantic resource such as VigNet would be of use to these and related domains. The concept of vignettes as graphical realizations of more general frames was introduced in (Coyne et al., 2010). In addition to FrameNet, much work has been done in developing theories and resources for lexical semantics and common-sense knowledge. VerbNet (Kipper et al., 2000) focuses on verb subcat patterns grouped by Levin verb classes (Levin"
W11-0905,T75-2004,0,0.68408,"directly applied or composed together. Composing vignettes involves unifying their frame elements. For example, in washing an apple, the WASH - SMALL - FRUIT vignette uses a sink. From world knowledge we know (via instances of the TYPICAL - LOCATION frame) that washing food typically takes place in the KITCHEN. To create a scene we compose the two vignettes together by unifying the sink in the location vignette with the sink in the action vignette. 6 Related Work The grounding of natural language to graphical relations has been investigated in very early text-toscene systems (Boberg, 1972), (Simmons, 1975), (Kahn, 1979), (Adorni et al., 1984), and then later in Put (Clay and Wilhelms, 1996), and WordsEye (Coyne and Sproat, 2001). Other systems, such as CarSim (Dupuy et al., 2001), Jack (Badler et al., 1998), and CONFUCIUS (Ma and McKevitt, 2006) target animation and virtual environments rather than scene construction. A graphically grounded lexical-semantic resource such as VigNet would be of use to these and related domains. The concept of vignettes as graphical realizations of more general frames was introduced in (Coyne et al., 2010). In addition to FrameNet, much work has been done in devel"
W11-0905,C98-1013,0,\N,Missing
W11-2127,W05-0909,0,0.828278,"ses), and a corresponding improvement in the percentage of syntactically wellformed subjects under a manual evaluation. The rest of the paper is structured as follows. Section 2 gives a review of research on this topic. Section 3 motivates the approach discussed in Section 4. 227 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 227–236, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Section 5 presents the results of a set of machine translation experiments using the automatic metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and a manual-evaluation of subject integrity. Section 6 discusses our conclusions and future plans. 2 Related Work The general approach pursued in this paper—that of using pre-ordering to improve translation output– has been explored by many researchers. Most work has focused on automatically learning reordering rules (Xia and McCord, 2004; Habash, 2007b; Elming, 2008; Elming and Habash, 2009; Dyer and Resnik, 2010). Xia and McCord (2004) describe an approach for translation from French to English, where context-free constituency reordering rules are acquired automatically using source and t"
W11-2127,W10-1735,0,0.313793,"es is still quite poor. Collins et al. (2005), for example, assume that the parse trees they use are correct. While the state-of-the-art in English parsing is fairly good (though far from perfect), this is not the case in other languages, where parsing shows substantial error rates. Moreover, when attempting to reorder so as to bring the source text more grammatically in line with the target language, a bad parse can be disastrous: moving parts of the sentence that shouldn’t be moved, and introducing more distortion error than it is able to correct. To address the problem of noisy parse data, Bisazza and Federico (2010) identify the subject using a chunker, then fuzzify it, creating a lattice in which the translation system has a choice of several different paths, corresponding to re-orderings of different subject spans. In investigating syntax-based reordering for Arabic specifically, Carpuat et al. (2010) show that a syntax-driven reordering of the training data only for the purpose of alignment improvement leads to a substantial improvement in translation quality, but do not report a corresponding improvement when reordering test data in a similar fashion. Interestingly, Bisazza and Federico (2010) report"
W11-2127,P10-2033,1,0.907361,"attempting to reorder so as to bring the source text more grammatically in line with the target language, a bad parse can be disastrous: moving parts of the sentence that shouldn’t be moved, and introducing more distortion error than it is able to correct. To address the problem of noisy parse data, Bisazza and Federico (2010) identify the subject using a chunker, then fuzzify it, creating a lattice in which the translation system has a choice of several different paths, corresponding to re-orderings of different subject spans. In investigating syntax-based reordering for Arabic specifically, Carpuat et al. (2010) show that a syntax-driven reordering of the training data only for the purpose of alignment improvement leads to a substantial improvement in translation quality, but do not report a corresponding improvement when reordering test data in a similar fashion. Interestingly, Bisazza and Federico (2010) report that fuzzy reordering the test data improves MT output, suggesting that fuzzification may be the mechanism necessary to render reordering on test data useful. To the best of our knowledge, nobody has yet used fuzzification to correct the identified subject span of complete Arabic dependency"
W11-2127,P11-2031,0,0.017868,"and left-attachment cases), it is guaranteed to appear as one path through the lattice. 5 5.1 Evaluation 5-gram language model with modified Kneser-Ney smoothing implemented using the SRILM toolkit (Stolcke, 2002). Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approachspecific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al., 2011). English data was tokenized using simple punctuation-based rules. Arabic data was segmented with to the Arabic Treebank tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological disambiguator and tokenizer (Habash and Rambow, 2005; Habash, 2007a; Roth et al., 2008). The Arabic text was also Alif/Ya normalized (Habash, 2010). MADAproduced Arabic lemmas were used for word alignment. We compare four settings with predicted parses (as opposed to the gold parse experiments discussed in Section 3): • BASE An un-reordered test set; • FORCE A test set which forced reordering on m"
W11-2127,P05-1066,0,0.388507,"Missing"
W11-2127,N10-1128,0,0.0734522,"Missing"
W11-2127,P08-1115,0,0.0186757,"nslations of up to 10 words are extracted in the Moses phrase table. The same baseline phrase table was used in all experiments. The system’s language model was trained both on the English portion of the training corpus and English Gigaword (Graff and Cieri, 2003). We used a 5 All data is available from the Linguistic Data Consortium: http://www.ldc.upenn.edu. 232 • SPAN A test set with fuzzification through optional reordering on matrix verbs and through fuzzification of the subject span according to the algorithm shown in Section 4.2. Each reordering corpus used Moses’ lattice input format (Dyer et al., 2008) (including the baselines, which had only one path). Results are presented in terms of the standard BLEU metric (Papineni et al., 2002), METEOR metric (Banerjee and Lavie, 2005) and a manual evaluation targeting subject span translation correctness. 5.2 Automatic Evaluation Results Table 2 presents the results for the experiments discussed above. Columns three and Four (Prec-1g and Prec-4g) indicate the corresponding 1-gram and 4-gram (sub-BLEU) precision scores, respectively. System BASE FORCE OPT SPAN BLEU Prec-1g Prec-4g METEOR 47.13 81.91 29.52 53.09 47.03 81.78 29.52 53.11 47.42 81.88 30."
W11-2127,W09-0809,1,0.888195,"Missing"
W11-2127,W08-0406,0,0.0449602,"Missing"
W11-2127,2009.mtsummit-caasl.4,0,0.174897,"eeebank (CATiB) (Habash and Roth, 2009). To answer this question, we examined more servation that even VS-ordered matrix verbs in Arabic are sometimes translated monotonically into English (as, for example, in passive constructions). An alternative explanation may be that since the training data itself is not re-ordered, it is plausible that some re-ordering may cause otherwise good possible matches in the phrase table to not match any more. 3.2 Parser Error The problem of finding correct subject span boundaries for reordering, however, is a particularly difficult one. Both Habash (2007b) and Green et al. (2009) have noted previously that even state-ofthe-art Arabic dependency parsers tend to perform poorly, and we would expect that incorrect boundaries would do more harm than good for translation. In order to determine how to “fix” these spans, it is first necessary to understand the kinds of errors that the parser makes. A set of predicted parses of the NIST MT05 data was compared to the gold parses of the same data set. There are three categories of error the parser can make in identifying subjects: labeling errors, attachment errors and span errors. In labeling errors, the parser either incorrect"
W11-2127,P05-1071,1,0.770901,"ERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approachspecific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al., 2011). English data was tokenized using simple punctuation-based rules. Arabic data was segmented with to the Arabic Treebank tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological disambiguator and tokenizer (Habash and Rambow, 2005; Habash, 2007a; Roth et al., 2008). The Arabic text was also Alif/Ya normalized (Habash, 2010). MADAproduced Arabic lemmas were used for word alignment. We compare four settings with predicted parses (as opposed to the gold parse experiments discussed in Section 3): • BASE An un-reordered test set; • FORCE A test set which forced reordering on matrix verbs; • OPT A test set with fuzzification through optional reordering on matrix verbs; and Experimental Setup We used the open-source Moses PSMT toolkit (Koehn et al., 2007). Training data was a newswire (MSA-English) parallel text with 12M word"
W11-2127,P09-2056,1,0.914406,"Missing"
W11-2127,2007.mtsummit-papers.29,1,0.976961,"burgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Section 5 presents the results of a set of machine translation experiments using the automatic metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and a manual-evaluation of subject integrity. Section 6 discusses our conclusions and future plans. 2 Related Work The general approach pursued in this paper—that of using pre-ordering to improve translation output– has been explored by many researchers. Most work has focused on automatically learning reordering rules (Xia and McCord, 2004; Habash, 2007b; Elming, 2008; Elming and Habash, 2009; Dyer and Resnik, 2010). Xia and McCord (2004) describe an approach for translation from French to English, where context-free constituency reordering rules are acquired automatically using source and target parses and word alignment. Elming (2008) and Elming and Habash (2009) use a large set of linguistic features to automatically learn reordering rules for English-Danish and English-Arabic; the rules are used to pre-order the input into a lattice of variant orders. Habash (2007b) learns syntactic reordering rules targeting Arabic-English word order di"
W11-2127,P07-2045,0,0.0216307,"04) using the MADA+TOKAN morphological disambiguator and tokenizer (Habash and Rambow, 2005; Habash, 2007a; Roth et al., 2008). The Arabic text was also Alif/Ya normalized (Habash, 2010). MADAproduced Arabic lemmas were used for word alignment. We compare four settings with predicted parses (as opposed to the gold parse experiments discussed in Section 3): • BASE An un-reordered test set; • FORCE A test set which forced reordering on matrix verbs; • OPT A test set with fuzzification through optional reordering on matrix verbs; and Experimental Setup We used the open-source Moses PSMT toolkit (Koehn et al., 2007). Training data was a newswire (MSA-English) parallel text with 12M words on the Arabic side (LDC2007E103)5 Sentences were reordered only for alignment, following the approach of Carpuat et al. (2010). Parses were obtained using a publicly available parser for Arabic (Marton et al., 2010). GIZA++ was used for word alignment (Och and Ney, 2003) and phrase translations of up to 10 words are extracted in the Moses phrase table. The same baseline phrase table was used in all experiments. The system’s language model was trained both on the English portion of the training corpus and English Gigaword"
W11-2127,W04-3250,0,0.204998,"Missing"
W11-2127,W10-1402,1,0.910449,"Missing"
W11-2127,J03-1002,0,0.00412204,"Section 3): • BASE An un-reordered test set; • FORCE A test set which forced reordering on matrix verbs; • OPT A test set with fuzzification through optional reordering on matrix verbs; and Experimental Setup We used the open-source Moses PSMT toolkit (Koehn et al., 2007). Training data was a newswire (MSA-English) parallel text with 12M words on the Arabic side (LDC2007E103)5 Sentences were reordered only for alignment, following the approach of Carpuat et al. (2010). Parses were obtained using a publicly available parser for Arabic (Marton et al., 2010). GIZA++ was used for word alignment (Och and Ney, 2003) and phrase translations of up to 10 words are extracted in the Moses phrase table. The same baseline phrase table was used in all experiments. The system’s language model was trained both on the English portion of the training corpus and English Gigaword (Graff and Cieri, 2003). We used a 5 All data is available from the Linguistic Data Consortium: http://www.ldc.upenn.edu. 232 • SPAN A test set with fuzzification through optional reordering on matrix verbs and through fuzzification of the subject span according to the algorithm shown in Section 4.2. Each reordering corpus used Moses’ lattice"
W11-2127,P03-1021,0,0.0247076,"ists of tuples, where each tuple defines a single reordering, and each list of tuples defines a set of spans that must be moved to the left of the matrix verb for one reordering. These re-orderings are then joined together to form the final lattice. If a single-constituent correction to the span exists (except in the aforementioned pathological and left-attachment cases), it is guaranteed to appear as one path through the lattice. 5 5.1 Evaluation 5-gram language model with modified Kneser-Ney smoothing implemented using the SRILM toolkit (Stolcke, 2002). Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approachspecific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al., 2011). English data was tokenized using simple punctuation-based rules. Arabic data was segmented with to the Arabic Treebank tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological disambiguator and tokenizer (Habash and Ramb"
W11-2127,P02-1040,0,0.087095,"the maximum possible using gold parses), and a corresponding improvement in the percentage of syntactically wellformed subjects under a manual evaluation. The rest of the paper is structured as follows. Section 2 gives a review of research on this topic. Section 3 motivates the approach discussed in Section 4. 227 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 227–236, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics Section 5 presents the results of a set of machine translation experiments using the automatic metrics BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and a manual-evaluation of subject integrity. Section 6 discusses our conclusions and future plans. 2 Related Work The general approach pursued in this paper—that of using pre-ordering to improve translation output– has been explored by many researchers. Most work has focused on automatically learning reordering rules (Xia and McCord, 2004; Habash, 2007b; Elming, 2008; Elming and Habash, 2009; Dyer and Resnik, 2010). Xia and McCord (2004) describe an approach for translation from French to English, where context-free constituency reordering rules are acq"
W11-2127,P08-2030,1,0.810201,"NIST MT06 corpus. MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approachspecific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al., 2011). English data was tokenized using simple punctuation-based rules. Arabic data was segmented with to the Arabic Treebank tokenization scheme (Maamouri et al., 2004) using the MADA+TOKAN morphological disambiguator and tokenizer (Habash and Rambow, 2005; Habash, 2007a; Roth et al., 2008). The Arabic text was also Alif/Ya normalized (Habash, 2010). MADAproduced Arabic lemmas were used for word alignment. We compare four settings with predicted parses (as opposed to the gold parse experiments discussed in Section 3): • BASE An un-reordered test set; • FORCE A test set which forced reordering on matrix verbs; • OPT A test set with fuzzification through optional reordering on matrix verbs; and Experimental Setup We used the open-source Moses PSMT toolkit (Koehn et al., 2007). Training data was a newswire (MSA-English) parallel text with 12M words on the Arabic side (LDC2007E103)5"
W11-2127,C04-1073,0,0.306764,"Missing"
W11-2127,C08-1027,0,\N,Missing
W11-2127,D08-1076,0,\N,Missing
W11-4416,altantawy-etal-2010-morphological,1,0.906756,"logy, the finite-state transducers (FSTs) tend to become extremely large, causing a significant deterioration in response time. Proceedings of the 9th International Workshop on Finite State Methods and Natural Language Processing, pages 116–124, c Blois (France), July 12-15, 2011. 2011 Association for Computational Linguistics In this paper, we present a solution to the modeling of rich morphologies that combines the best of these two approaches. In previous work, we presented M AGEAD, a multi-tier finite-state implementation of Arabic morphology (Habash et al., 2005; Habash and Rambow, 2006; Altantawy et al., 2010). We improve the speed by automatically converting our FST-based M AGEAD system to a precompiled tabular implementation that preserves all of the rich linguistic information used in M AGEAD’s design. The new system, M AGEAD -E XPRESS is not only much faster and smaller in size, but it also still allows linguistically based abstract changes and updates in its model. Furthermore, M AGEAD E XPRESS produces complete linguistic analyses that include intermediate levels of representation, an advantage M AGEAD does not have readily in its output. The only disadvantage of M AGEAD -E XPRESS is its inab"
W11-4416,W00-1801,0,0.0388487,"hat it has particle proclitic l ‘for/that’; and that it has a pronominal 4.1 M AGEAD’s Representation of Linguistic enclitic that is 3rd person masculine singular. Knowledge 3 Related Work There has been a considerable amount of work on Arabic morphological analysis; for an overview, see (Al-Sughaiyer and Al-Kharashi, 2004). The first large-scale implementation of Arabic morphology within the constraints of finite-state methods is that of Beesley et al. (1989) (the “Xerox system”) with a ‘detouring’ mechanism for access to multiple lexica, which gives rise to other works by Beesley (1998) and Beesley and Karttunen (2000) and, independently, by Buckwalter (2004). Unlike the Xerox system, the Buckwalter Arabic Morphological Analyzer (BAMA) uses a hard-coded tabular approach with a focus on analysis into surface morphemes (discussed above). Buckwalter’s work has been since extended to handle generation as well as the lexeme-and-features representation (A L MORGEANA , (Habash, 2007)) and functional morphology in Arabic (ELIXIR, (Smrž, 2007)). Finite-state handling of templatic morphology has been demonstrated using a variety of techniques for several Semitic languages other than Arabic including Akkadian (Kataja"
W11-4416,E09-1036,0,0.0191382,"ard-coded tabular approach with a focus on analysis into surface morphemes (discussed above). Buckwalter’s work has been since extended to handle generation as well as the lexeme-and-features representation (A L MORGEANA , (Habash, 2007)) and functional morphology in Arabic (ELIXIR, (Smrž, 2007)). Finite-state handling of templatic morphology has been demonstrated using a variety of techniques for several Semitic languages other than Arabic including Akkadian (Kataja and Koskenniemi, 1988), Syriac (Kiraz, 2000), Hebrew (Yona and Wintner, 2005), Amharic (Amsalu and Gibbon, 2005), and Tigrinya (Gasser, 2009). Kay (1987) proposes a framework for handling templatic morphology in which each templatic morpheme is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form. Kiraz (2000) extends Kay’s approach and implements a multi-tape system for Modern Standard Arabic (MSA) and Syriac. The M AGEAD system (Habash et al., 2005; Habash and Rambow, 2006) extended Kiraz’s work 118 M AGEAD relates (bidirectionally) a lexeme and a set of feature-value pairs to a surface word form through a sequence of transformations. In a generation perspective, the features are tran"
W11-4416,P06-1086,1,0.948957,"its two poles: on one end, very abstract and linguistically rich representations and rules (often based on particular theories of morphology) are used to derive surface forms; while on the other end, simple and shallow techniques focus on efficient search in a space of precompiled (tabulated) 116 solutions. The first type is typically implemented using finite-state technology and can be at many different degrees of sophistication and detail. An example of this type of implementation is the M AGEAD (Morphological Analysis and GEneration for Arabic and its Dialects) system (Habash et al., 2005; Habash and Rambow, 2006). This system, which we use as starting point in this paper, compiles abstract high-level linguistic information of different types to finite state machinery. The second type is typically not implemented in finite-state technology. Examples include the Buckwalter Arabic Morphological Analyzer (BAMA) (Buckwalter, 2004) and its extension A LMORGEANA (Habash, 2007). These systems do not represent the morphemic, phonological and orthographic rules directly at all, and instead compile their effect into the lexicon itself. Numerous intermediate points exist between these two extremes (e.g., (Smrž, 2"
W11-4416,W05-0703,1,0.937932,"is characterized by its two poles: on one end, very abstract and linguistically rich representations and rules (often based on particular theories of morphology) are used to derive surface forms; while on the other end, simple and shallow techniques focus on efficient search in a space of precompiled (tabulated) 116 solutions. The first type is typically implemented using finite-state technology and can be at many different degrees of sophistication and detail. An example of this type of implementation is the M AGEAD (Morphological Analysis and GEneration for Arabic and its Dialects) system (Habash et al., 2005; Habash and Rambow, 2006). This system, which we use as starting point in this paper, compiles abstract high-level linguistic information of different types to finite state machinery. The second type is typically not implemented in finite-state technology. Examples include the Buckwalter Arabic Morphological Analyzer (BAMA) (Buckwalter, 2004) and its extension A LMORGEANA (Habash, 2007). These systems do not represent the morphemic, phonological and orthographic rules directly at all, and instead compile their effect into the lexicon itself. Numerous intermediate points exist between these tw"
W11-4416,E87-1002,0,0.537265,"ar approach with a focus on analysis into surface morphemes (discussed above). Buckwalter’s work has been since extended to handle generation as well as the lexeme-and-features representation (A L MORGEANA , (Habash, 2007)) and functional morphology in Arabic (ELIXIR, (Smrž, 2007)). Finite-state handling of templatic morphology has been demonstrated using a variety of techniques for several Semitic languages other than Arabic including Akkadian (Kataja and Koskenniemi, 1988), Syriac (Kiraz, 2000), Hebrew (Yona and Wintner, 2005), Amharic (Amsalu and Gibbon, 2005), and Tigrinya (Gasser, 2009). Kay (1987) proposes a framework for handling templatic morphology in which each templatic morpheme is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form. Kiraz (2000) extends Kay’s approach and implements a multi-tape system for Modern Standard Arabic (MSA) and Syriac. The M AGEAD system (Habash et al., 2005; Habash and Rambow, 2006) extended Kiraz’s work 118 M AGEAD relates (bidirectionally) a lexeme and a set of feature-value pairs to a surface word form through a sequence of transformations. In a generation perspective, the features are translated to ab"
W11-4416,2010.jeptalnrecital-long.29,1,0.832216,"Missing"
W11-4416,J00-1006,0,0.913933,"er (2004). Unlike the Xerox system, the Buckwalter Arabic Morphological Analyzer (BAMA) uses a hard-coded tabular approach with a focus on analysis into surface morphemes (discussed above). Buckwalter’s work has been since extended to handle generation as well as the lexeme-and-features representation (A L MORGEANA , (Habash, 2007)) and functional morphology in Arabic (ELIXIR, (Smrž, 2007)). Finite-state handling of templatic morphology has been demonstrated using a variety of techniques for several Semitic languages other than Arabic including Akkadian (Kataja and Koskenniemi, 1988), Syriac (Kiraz, 2000), Hebrew (Yona and Wintner, 2005), Amharic (Amsalu and Gibbon, 2005), and Tigrinya (Gasser, 2009). Kay (1987) proposes a framework for handling templatic morphology in which each templatic morpheme is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form. Kiraz (2000) extends Kay’s approach and implements a multi-tape system for Modern Standard Arabic (MSA) and Syriac. The M AGEAD system (Habash et al., 2005; Habash and Rambow, 2006) extended Kiraz’s work 118 M AGEAD relates (bidirectionally) a lexeme and a set of feature-value pairs to a surface wo"
W11-4416,W05-0702,0,0.0179376,"Xerox system, the Buckwalter Arabic Morphological Analyzer (BAMA) uses a hard-coded tabular approach with a focus on analysis into surface morphemes (discussed above). Buckwalter’s work has been since extended to handle generation as well as the lexeme-and-features representation (A L MORGEANA , (Habash, 2007)) and functional morphology in Arabic (ELIXIR, (Smrž, 2007)). Finite-state handling of templatic morphology has been demonstrated using a variety of techniques for several Semitic languages other than Arabic including Akkadian (Kataja and Koskenniemi, 1988), Syriac (Kiraz, 2000), Hebrew (Yona and Wintner, 2005), Amharic (Amsalu and Gibbon, 2005), and Tigrinya (Gasser, 2009). Kay (1987) proposes a framework for handling templatic morphology in which each templatic morpheme is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form. Kiraz (2000) extends Kay’s approach and implements a multi-tape system for Modern Standard Arabic (MSA) and Syriac. The M AGEAD system (Habash et al., 2005; Habash and Rambow, 2006) extended Kiraz’s work 118 M AGEAD relates (bidirectionally) a lexeme and a set of feature-value pairs to a surface word form through a sequence of tra"
W11-4416,C88-1064,0,\N,Missing
W11-4416,P00-1025,0,\N,Missing
W11-4416,W98-1007,0,\N,Missing
W12-2105,andreas-etal-2012-annotating,1,0.874501,"fall into that category of conversational behavior and these Language Uses are used directly as features in a supervised machine learning model to predict whether or not a participant is an influencer. For example, Dialog Patterns contributes the Language Uses Initiative, Irrelevance, Incitation, Investment and Interjection. The Language Uses of the Persuasion and Agreement/Disagreement components are not described in detail in this paper, and instead are treated as black boxes (indicated by solid boxes in Figure 1). We have previously published work on some of these (Biran and Rambow, 2011; Andreas et al., 2012). The remainder of this section describes them briefly and provides the results of evaluations of their performance (in Table 2). The next section describes the features of the Dialog Patterns component. 41 Persuasion This component identifies three Language Uses: Attempt to Persuade, Claims and Argumentation. We define an attempt to persuade as a set of contributions made by a single participant which may be made anywhere within the thread, and which are all concerned with stating and supporting a single claim. The subject of the claim does not matter: an opinion may seem trivial, but the arg"
W12-2513,W10-1803,1,0.840424,"constructed from Hamlet in order to delve deeper into its infamously dense character network. While this approach is clearly powerful, it is not without drawbacks. As Moretti (2011) points out, undirected and unweighted networks are blunt instruments and limited in their use. While, as discussed below, some researchers have sought to rectify these limitations, few have done so with a strict and specific rubric for categorizing interactions. In this paper, we annotate Lewis Carroll’s Alice in Wonderland using a well-defined annotation scheme which we have previously developed on newswire text Agarwal et al. (2010). It is well suited to deal with the aforementioned limitations. We show that using different types of networks can be useful by allowing us to provide a model for determining pointof-view. We also show that social networks allow characters to be categorized into roles based on how they function in the text, but that this approach is limited when using static social networks. We then build and visualize dynamic networks and show that static networks can distort the importance of characters. By using dynamic networks, we can build a fuller picture of how each character works in a literary text."
W12-2513,P10-1015,0,0.0710569,"ysis. We propose the use of dynamic network analysis to overcome these limitations. 1 Introduction In recent years, the wide availability of digitized literary works has given rise to a computational approach to analyzing these texts. This approach has been used, sometimes in conjunction with more traditional literary analysis techniques, to better grasp the intricacies of several literary works. As the field matured, new approaches and ideas gave rise to the use of techniques, like social networks, usually reserved for quantitive fields in order to gain new insights into the works. Recently, Elson et al. (2010) extracted networks from a corpus of 19th century texts in order to debunk long standing hypotheses from comparative literature (Elson et al., 2010). Moretti (2011) examined a social event network constructed from Hamlet in order to delve deeper into its infamously dense character network. While this approach is clearly powerful, it is not without drawbacks. As Moretti (2011) points out, undirected and unweighted networks are blunt instruments and limited in their use. While, as discussed below, some researchers have sought to rectify these limitations, few have done so with a strict and speci"
W12-2513,D10-1100,1,\N,Missing
W12-3807,P11-2049,0,0.0155415,"tion and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable amount of interest in modality in the biomedical domain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical const"
W12-3807,W09-1324,0,0.0193045,"nd hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the prese"
W12-3807,baker-etal-2010-modality,1,0.685998,"Missing"
W12-3807,J12-2006,1,0.846788,"features used to train our modality tagger and presents experiments and results. Section 5 concludes and discusses future work. 58 2 Related Work Previous related work includes TimeML (Sauri et al., 2006), which involves modality annotation on events, and Factbank (Sauri and Pustejovsky, 2009), where event mentions are marked with degree of factuality. Modality is also important in the detection of uncertainty and hedging. The CoNLL shared task in 2010 (Farkas et al., 2010) deals with automatic detection of uncertainty and hedging in Wikipedia and biomedical sentences. Baker et al. (2010) and Baker et al. (2012) analyze a set of eight modalities which include belief, require and permit, in addition to the five modalities we focus on in this paper. They built a rule-based modality tagger using a semi-automatic approach to create rules. This earlier work differs from the work described in this paper in that the our emphasis is on the creation of an automatic modality tagger using machine learning techniques. Note that the annotation and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable"
W12-3807,W09-3012,1,0.821042,"y of the information. Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagg"
W12-3807,P03-1004,0,0.0266465,"removed the ones which did not in fact have a modality. In the remaining sentences (94 sentences), our expert annotated the target predicate. We refer to this as the Gold dataset in this paper. The MTurk and Gold datasets differ in terms of genres as well as annotators (Turker vs. Expert). The distribution of modalities in both MTurk and Gold annotations are given in Table 2. 4.2 Gold Ability Table 1: For each modality, the number of sentences returned by the simple tagger that we posted on MTurk. 4 MTurk Table 2: Frequency of Modalities modalities in context. For tagging, we used the Yamcha (Kudo and Matsumoto, 2003) sequence labeling system which uses the SVMlight (Joachims, 1999) package for classification. We used One versus All method for multi-class classification on a quadratic kernel with a C value of 1. We report recall and precision on word tokens in our corpus for each modality. We also report Fβ=1 (F)-measure as the harmonic mean between (P)recision and (R)ecall. 4.3 Features We used lexical features at the token level which can be extracted without any parsing with relatively high accuracy. We use the term context width to denote the window of tokens whose features are considered for predictin"
W12-3807,W09-1304,0,0.0189282,"ain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they"
W12-3807,Y05-1014,0,0.0194818,", absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes moda"
W12-3807,W06-3907,0,0.0243548,"of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes modality annotation schemes. Identifying modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al. (2007) include polarity based rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn et al., 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modifiers. 3 Constructing Modality Training Data In this section, we will discuss the procedure we followed to construct the training data for building the automatic modality tagger. In a pilot study, we obtained and ran the modality tagger described in (Baker et al., 2010) on the English side of the Urdu-English LDC language pack.2 We randomly selected"
W12-3807,C10-2117,1,0.929409,". Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagger by using a semiautomatic"
W12-3807,C94-1018,0,0.029939,"Missing"
W12-3807,W08-0606,0,\N,Missing
W12-4619,I11-1138,1,0.830207,"k1 windows/N(e3) k2 CASEi /NULL vmod suddenly/Adv(e2) Figure 1: The three annotation levels of the HTB: Dependency Structure, PropBank, and Phase Structure (top row); the consistent DS (DSconst ) and the derivation tree (DSderiv ) obtained from these structures (bottom row); ei in the DSderiv tree refers to elementary trees in Figures 5-6. dicating movement from the object position to the subject position using a coindexed trace. (1) 3 khir.kiy˜a: acaanak t.uut.˜ı: windows.F suddenly broke.FPl ‘The windows broke suddenly.’ Consistency between DS and PS In our previous study (Xia et al., 2009; Bhatt et al., 2011), we proposed a DS-to-PS conversion algorithm, which extracts conversion rules from a small number of (DS, PS) pairs, and then applies the rules to a new DS to generate a PS for the DS. The algorithm introduces two concepts: consistency and compatibility. A DS and a PS tree are consistent if and only if there exists an assignment of head words for the internal nodes in the PS such that after merging all the (head child, parent) nodes in the PS, the new PS is identical to the DS. In Figure 1, the DS in (a) and the PS in (c) are not consistent because the empty category CASE appears only in (c)."
W12-4619,W98-0106,0,0.072946,"s in Figure 5 In the case of our unaccusative example, the extracted TAG will include the rule in Figure 6, which replace the first and last rules in Figure 5. The new DSderiv is in Figure 1e., where e2 and e3 are two etrees in Figure 5, and e5 is the etree in Figure 6.2 6 Issues in Word Order It has been known for a long time that in a lexicalized TAG, the derivation tree is a lexical dependency tree (since the nodes are bijectively identified with the words of the sentence), and that this dependency structure is not necessarily the linguistically plausible structure (Rambow and Joshi, 1997; Candito and Kahane, 1998; Rambow et al., 2001). Consider the following example (2): 2 It is not a coincident that in this example DSderiv and DS are isomorphic, as the DS-to-DSconst module involves adding ECs and DSconst -to-DSderiv involves removing ECs. But in theory, DSderiv and DS are not necessarily always isomorphic as the DS-to-DSconst module is not limited to adding ECs. ˜ kitaab˜e mE-ne khariid-nii caah-ii books.F I-Erg buy-Inf.F want-Pfv.F ‘Books, I had wanted to buy.’ ˜ mE-ne yeh kitaab sab-se khariid-ne-ko I-Erg this book all-Instr buy-Inf-Acc kah-aa say-Pfv ‘I told everyone to buy this book.’ In (3), thi"
W12-4619,E99-1029,0,0.0632938,"linguistic interpretation (since buy does not have a clausal argument). Both problems have a common solution: nonlocal multicomponent TAGs with dominance links (nlMC-TAG-DL), for example in the definition given in (Rambow et al., 2001) as D-Tree Substitution Grammars (DSG). In a DSG, there is only the substitution operation, but all links in trees are interpreted as non-immediate dominance links; in fact, the trees can be seen as tree descriptions rather than as trees; this allows components of trees to move up and be inserted into other trees. For large-scale grammar development in DSG, see (Carroll et al., 1999). There are good reasons to want to try and restrict the generative power of a formal system used 168 for the description of syntax: on the one hand, we may want to restrict the formal power in order to obtain parsing algorithms of certain restricted complexities (Kallmeyer, 2005), and on the other hand, we may want to make assumptions about the underlying formalism in order to make predictions about what word orders are grammatical (or plausible) (Chen-Main and Joshi, 2008). However, in this paper we do not address the tradeoff between non-local and restricted MC-TAG systems, and we do not pr"
W12-4619,W08-2302,0,0.0136127,"this allows components of trees to move up and be inserted into other trees. For large-scale grammar development in DSG, see (Carroll et al., 1999). There are good reasons to want to try and restrict the generative power of a formal system used 168 for the description of syntax: on the one hand, we may want to restrict the formal power in order to obtain parsing algorithms of certain restricted complexities (Kallmeyer, 2005), and on the other hand, we may want to make assumptions about the underlying formalism in order to make predictions about what word orders are grammatical (or plausible) (Chen-Main and Joshi, 2008). However, in this paper we do not address the tradeoff between non-local and restricted MC-TAG systems, and we do not present empirical arguments from Hindi in order to address the issue of what formal complexity is required for the syntactic description of Hindi. We leave those issues to future work. Instead, we assume a simple framework in which we can explore the issue of grammar extraction. The algorithm for extraction of a TAG can be extended straightforwardly to an algorithm for extracting a DSG: whenever in DSconst we have a node labeled with an empty category which is coindexed with a"
W12-4619,J05-2003,0,0.0223303,"here is only the substitution operation, but all links in trees are interpreted as non-immediate dominance links; in fact, the trees can be seen as tree descriptions rather than as trees; this allows components of trees to move up and be inserted into other trees. For large-scale grammar development in DSG, see (Carroll et al., 1999). There are good reasons to want to try and restrict the generative power of a formal system used 168 for the description of syntax: on the one hand, we may want to restrict the formal power in order to obtain parsing algorithms of certain restricted complexities (Kallmeyer, 2005), and on the other hand, we may want to make assumptions about the underlying formalism in order to make predictions about what word orders are grammatical (or plausible) (Chen-Main and Joshi, 2008). However, in this paper we do not address the tradeoff between non-local and restricted MC-TAG systems, and we do not present empirical arguments from Hindi in order to address the issue of what formal complexity is required for the syntactic description of Hindi. We leave those issues to future work. Instead, we assume a simple framework in which we can explore the issue of grammar extraction. The"
W12-4619,J01-1004,1,0.658704,"of our unaccusative example, the extracted TAG will include the rule in Figure 6, which replace the first and last rules in Figure 5. The new DSderiv is in Figure 1e., where e2 and e3 are two etrees in Figure 5, and e5 is the etree in Figure 6.2 6 Issues in Word Order It has been known for a long time that in a lexicalized TAG, the derivation tree is a lexical dependency tree (since the nodes are bijectively identified with the words of the sentence), and that this dependency structure is not necessarily the linguistically plausible structure (Rambow and Joshi, 1997; Candito and Kahane, 1998; Rambow et al., 2001). Consider the following example (2): 2 It is not a coincident that in this example DSderiv and DS are isomorphic, as the DS-to-DSconst module involves adding ECs and DSconst -to-DSderiv involves removing ECs. But in theory, DSderiv and DS are not necessarily always isomorphic as the DS-to-DSconst module is not limited to adding ECs. ˜ kitaab˜e mE-ne khariid-nii caah-ii books.F I-Erg buy-Inf.F want-Pfv.F ‘Books, I had wanted to buy.’ ˜ mE-ne yeh kitaab sab-se khariid-ne-ko I-Erg this book all-Instr buy-Inf-Acc kah-aa say-Pfv ‘I told everyone to buy this book.’ In (3), this book is an argument"
W13-4910,P11-2062,1,0.787315,"it et al., 2008; Nivre, 2009). In contrast to these negative results, Marton et al. (2013) showed positive results for using agreement morphology for Arabic. 2.2 Methodology In Marton et al. (2013), we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. We used both the MaltParser (Nivre, 2008) and the Easy-First 2 For more information on Arabic morphology in the context of natural language processing see Habash (2010). For a detailed analysis of morpho-syntactic agreement, see Alkuhlani and Habash (2011). 87 Parser (Goldberg and Elhadad, 2010). Since the Easy-First Parser performed better, we use it in all experiments reported in this paper. For MSA, the space of possible morphological features is quite large. We determined which morphological features help by performing a search through the feature space. In order to do this, we separated part-of-speech (POS) from the morphological features. We defined a core set of 12 POS features, and then explored combinations of morphological features in addition to this POS tagset. This core set of POS tags is similar to those proposed in cross-lingual"
W13-4910,E12-1069,1,0.838619,"erent from the data split we used in (Marton et al., 2013), so we retrained our models on the new splits (Diab et al., 2013). The data released for the Shared Task showed inconsistent availability of lemmas across gold and predicted input, so we used the ALMOR analyzer (Habash, 2007) with the SAMA databases (Graff et al., 2009) to determine a lemma given the word form and the provided (gold or predicted) POS tags. In addition to the lemmas, the ALMOR analyzer also provides morphological features in the feature-value representation our approach requires. Finally, we ran our existing converter (Alkuhlani and Habash, 2012) over this representation to obtain functional number and gender, as well as the rationality feature.3 For simplicity reasons, we used the MLE:W2+CATiB model (Alkuhlani and Habash, 2012), which was the best performing model on seen words, as opposed to the combination system that used a syntactic component with better results on unseen words. We did not perform Alif or Ya normalization on the data. We trained two models: one on 5,000 sentences of training data and one on the entire training data. 3.2 Results Our performance in the Shared Task for Arabic Dependency, Gold Tokenization, Predicted"
W13-4910,P99-1065,0,0.101049,"n which case it will be of little or no help for parsing, even if helpful when its gold values are provided. The CASE feature is very relevant and not redundant, but it cannot be predicted with high accuracy and overall it is not useful. Different languages vary with respect to which features may be most helpful given various tradeoffs among these three factors. It has been shown previously that if the relevant morphological features in assignment configurations can be recognized well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al., 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages (Nivre et al., 2008; Eryigit et al., 2008; Nivre, 2009). In contrast to these negative results, Marton et al. (2013) showed positive results for using agreement morphology for Arabic. 2.2 Methodology In Marton et al. (2013), we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurat"
W13-4910,J08-3003,0,0.0223474,"to which features may be most helpful given various tradeoffs among these three factors. It has been shown previously that if the relevant morphological features in assignment configurations can be recognized well enough, then they contribute to parsing accuracy. For example, modeling CASE in Czech improves Czech parsing (Collins et al., 1999): CASE is relevant, not redundant, and can be predicted with sufficient accuracy. However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages (Nivre et al., 2008; Eryigit et al., 2008; Nivre, 2009). In contrast to these negative results, Marton et al. (2013) showed positive results for using agreement morphology for Arabic. 2.2 Methodology In Marton et al. (2013), we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. We used both the MaltParser (Nivre, 2008) and the Easy-First 2 For more information on Arabic morphology in the context of natural language processing see Habash (2010). For a detailed analysis of morpho-syntactic agreement, see Alkuhlani and H"
W13-4910,N10-1115,0,0.327358,"st to these negative results, Marton et al. (2013) showed positive results for using agreement morphology for Arabic. 2.2 Methodology In Marton et al. (2013), we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. We used both the MaltParser (Nivre, 2008) and the Easy-First 2 For more information on Arabic morphology in the context of natural language processing see Habash (2010). For a detailed analysis of morpho-syntactic agreement, see Alkuhlani and Habash (2011). 87 Parser (Goldberg and Elhadad, 2010). Since the Easy-First Parser performed better, we use it in all experiments reported in this paper. For MSA, the space of possible morphological features is quite large. We determined which morphological features help by performing a search through the feature space. In order to do this, we separated part-of-speech (POS) from the morphological features. We defined a core set of 12 POS features, and then explored combinations of morphological features in addition to this POS tagset. This core set of POS tags is similar to those proposed in cross-lingual work (Rambow et al., 2006; Petrov et al."
W13-4910,P05-1071,1,0.758187,"orphological features is quite large. We determined which morphological features help by performing a search through the feature space. In order to do this, we separated part-of-speech (POS) from the morphological features. We defined a core set of 12 POS features, and then explored combinations of morphological features in addition to this POS tagset. This core set of POS tags is similar to those proposed in cross-lingual work (Rambow et al., 2006; Petrov et al., 2012). We performed this search independently for Gold input features and predicted input features. We used our MADA+TOKAN system (Habash and Rambow, 2005; Habash et al., 2009; Habash et al., 2012) for the prediction. As the EasyFirst Parser predicts links separately before labels, we first optimized for unlabeled attachment score, and then optimized the Easy-First Parser labeler for label score. As had been found in previous results, assignment features, specifically CASE and STATE, are very helpful in MSA. However, in MSA this is true only under gold conditions: since CASE is rarely explicit in the typically undiacritized written MSA, it has a dismal accuracy rate, which makes it useless when used in machine-predicted (real, non-gold) conditi"
W13-4910,J13-1008,1,0.634264,"System: The CADIM Arabic Dependency Parser Yuval Marton Microsoft Corporation City Center Plaza Bellevue, WA, USA Nizar Habash, Owen Rambow CCLS Columbia University New York, NY, USA Sarah Alkuhlani CS Department Columbia University New York, NY, USA cadim@ccls.columbia.edu 2.1 Abstract We describe the submission from the Columbia Arabic & Dialect Modeling group (CADIM) for the Shared Task at the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL’2013). We participate in the Arabic Dependency parsing task for predicted POS tags and features. Our system is based on Marton et al. (2013). 1 Introduction In this paper, we discuss the system that the Columbia Arabic & Dialect Modeling group (CADIM) submitted to the 2013 Shared Task on Parsing Morphologically Rich Languages (Seddah et al., 2013). We used a system for Arabic dependency parsing which we had previously developed, but retrained it on the training data splits used in this task. We only participated in the Arabic dependency parsing track, and in it, only optimized for predicted (non-gold) POS tags and features. We first summarize our previous work (Section 2). We then discuss our submission and the results (Section 3)"
W13-4910,C08-1081,0,0.0564306,"Missing"
W13-4910,J08-4003,0,0.0184236,"h sufficient accuracy. However, it had been more difficult showing that agreement morphology helps parsing, with negative results for dependency parsing in several languages (Nivre et al., 2008; Eryigit et al., 2008; Nivre, 2009). In contrast to these negative results, Marton et al. (2013) showed positive results for using agreement morphology for Arabic. 2.2 Methodology In Marton et al. (2013), we investigated morphological features for dependency parsing of Modern Standard Arabic (MSA). The goal was to find a set of relevant, accurate and non-redundant features. We used both the MaltParser (Nivre, 2008) and the Easy-First 2 For more information on Arabic morphology in the context of natural language processing see Habash (2010). For a detailed analysis of morpho-syntactic agreement, see Alkuhlani and Habash (2011). 87 Parser (Goldberg and Elhadad, 2010). Since the Easy-First Parser performed better, we use it in all experiments reported in this paper. For MSA, the space of possible morphological features is quite large. We determined which morphological features help by performing a search through the feature space. In order to do this, we separated part-of-speech (POS) from the morphologica"
W13-4910,petrov-etal-2012-universal,0,0.0385524,"lhadad, 2010). Since the Easy-First Parser performed better, we use it in all experiments reported in this paper. For MSA, the space of possible morphological features is quite large. We determined which morphological features help by performing a search through the feature space. In order to do this, we separated part-of-speech (POS) from the morphological features. We defined a core set of 12 POS features, and then explored combinations of morphological features in addition to this POS tagset. This core set of POS tags is similar to those proposed in cross-lingual work (Rambow et al., 2006; Petrov et al., 2012). We performed this search independently for Gold input features and predicted input features. We used our MADA+TOKAN system (Habash and Rambow, 2005; Habash et al., 2009; Habash et al., 2012) for the prediction. As the EasyFirst Parser predicts links separately before labels, we first optimized for unlabeled attachment score, and then optimized the Easy-First Parser labeler for label score. As had been found in previous results, assignment features, specifically CASE and STATE, are very helpful in MSA. However, in MSA this is true only under gold conditions: since CASE is rarely explicit in t"
W13-4910,J08-4010,0,\N,Missing
W13-4910,rambow-etal-2006-parallel,1,\N,Missing
W14-1604,P97-1017,0,0.0863408,"there is some work on converting from dialectal Arabic to MSA, which is similar to our work in terms of processing a dialectal input. However, our final output is in EGY and not MSA. Shaalan et al. (2007) introduced a rule-based approach to convert EGY to MSA. AlGaphari and Al-Yadoumi (2010) also used a rulebased method to transform from Sanaani dialect to MSA. Sawaf (2010), Salloum and Habash (2011) and Salloum and Habash (2013) used morphological analysis and morphosyntactic transformation rules for processing EGY and Levantine Arabic. There has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic, which Habash (2008) employed as part of generating English transliterations from Arabic words in the context of machine translation. This work is similar to ours in terms of text transliteration. However, our work is not restricted to names. 4 Approach 4.1 Defining the Task Our task is as follows: for each Arabizi word in the input, we choose the Arabic script word which is the correct CODA spelling of th"
W14-1604,W12-4808,0,0.141881,"ransliteration from Arabizi to Arabic script, and then applied language modeling on the transliterated text. This is similar to our proposed work in terms of transliteration and language modeling. However, Darwish (2013) does not target a conventionalized orthography, while our system targets CODA. Additionally, Darwish (2013) transliterates Arabic words only after filtering out non-Arabic words, while we transliterate the whole input Arabizi. Finally, he does not use any morphological information, while we introduce the use of a morphological analyzer to support the transliteration pipeline. Chalabi and Gerges (2012) presented a hybrid approach for Arabizi transliteration. Their work relies on the use of character transformation rules that are either handcrafted by a linguist or automatically generated from training data. They also employ word-based and character-based language models for the final transliteration choice. Like Darwish (2013), the work done by Chalabi and Gerges (2012) is similar to ours except that it does not target a conventionalized orthography, and does not use deep morphological information, while our system does. CODA has been used as part of different approaches for modeling DA mor"
W14-1604,N13-1066,1,0.790154,"ld we convert Arabizi into? Our answer to this question is to use CODA, a conventional orthography created for the purpose of supporting NLP tools (Habash et al., 2012a). The goal of CODA is to reduce the data sparseness that comes from the same word form appearing in many spontaneous orthographies in data (be it annotated or unannotated). CODA has been defined for EGY as well as Tunisian Arabic (Zribi et al., 2014), and it has been used as part of different approaches for modeling DA morphology (Habash et al., 2012b), tagging (Habash et al., 2013; Pasha et al., 2014) and spelling correction (Eskander et al., 2013; Farra et al., 2014). This paper makes two main contributions. First, we clearly define the computational problem of transforming Arabizi to CODA. This improves over previous work by unambiguously fixing the In this paper, we address the problem of converting Dialectal Arabic (DA) text that is written in the Latin script (called Arabizi) into Arabic script following the CODA convention for DA orthography. The presented system uses a finite state transducer trained at the character level to generate all possible transliterations for the input Arabizi words. We then filter the generated list us"
W14-1604,P14-2027,1,0.915777,"nto? Our answer to this question is to use CODA, a conventional orthography created for the purpose of supporting NLP tools (Habash et al., 2012a). The goal of CODA is to reduce the data sparseness that comes from the same word form appearing in many spontaneous orthographies in data (be it annotated or unannotated). CODA has been defined for EGY as well as Tunisian Arabic (Zribi et al., 2014), and it has been used as part of different approaches for modeling DA morphology (Habash et al., 2012b), tagging (Habash et al., 2013; Pasha et al., 2014) and spelling correction (Eskander et al., 2013; Farra et al., 2014). This paper makes two main contributions. First, we clearly define the computational problem of transforming Arabizi to CODA. This improves over previous work by unambiguously fixing the In this paper, we address the problem of converting Dialectal Arabic (DA) text that is written in the Latin script (called Arabizi) into Arabic script following the CODA convention for DA orthography. The presented system uses a finite state transducer trained at the character level to generate all possible transliterations for the input Arabizi words. We then filter the generated list using a DA morphologica"
W14-1604,N06-1060,0,0.0836161,"ctal input. However, our final output is in EGY and not MSA. Shaalan et al. (2007) introduced a rule-based approach to convert EGY to MSA. AlGaphari and Al-Yadoumi (2010) also used a rulebased method to transform from Sanaani dialect to MSA. Sawaf (2010), Salloum and Habash (2011) and Salloum and Habash (2013) used morphological analysis and morphosyntactic transformation rules for processing EGY and Levantine Arabic. There has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic, which Habash (2008) employed as part of generating English transliterations from Arabic words in the context of machine translation. This work is similar to ours in terms of text transliteration. However, our work is not restricted to names. 4 Approach 4.1 Defining the Task Our task is as follows: for each Arabizi word in the input, we choose the Arabic script word which is the correct CODA spelling of the input word and which carries the intended meaning (as determined in the context of the entire available text). We do n"
W14-1604,habash-etal-2012-conventional,1,0.732573,"established conventions to render the sounds of the DA sentence. Because the sound-to-letter rules of English are very different from those of Arabic, we obtain complex mappings between the two writing systems. This issue is compounded by the underlying problem that DA itself does not have any standard orthography in the Arabic script. Table 1 shows different plausible ways of writing an Egyptian Arabic (EGY) sentence in Arabizi and in Arabic script. Arabizi poses a problem for natural language processing (NLP). While some tools have recently become available for processing EGY input, e.g., (Habash et al., 2012b; Habash et al., 2013; Pasha et al., 2014), they expect Arabic script input (or a Buckwalter transliteration). They cannot process Arabizi. We therefore need a tool that converts from Arabizi to Arabic script. However, the lack of standard orthography in EGY compounds the problem: what should we convert Arabizi into? Our answer to this question is to use CODA, a conventional orthography created for the purpose of supporting NLP tools (Habash et al., 2012a). The goal of CODA is to reduce the data sparseness that comes from the same word form appearing in many spontaneous orthographies in data"
W14-1604,W12-2301,1,0.879522,"established conventions to render the sounds of the DA sentence. Because the sound-to-letter rules of English are very different from those of Arabic, we obtain complex mappings between the two writing systems. This issue is compounded by the underlying problem that DA itself does not have any standard orthography in the Arabic script. Table 1 shows different plausible ways of writing an Egyptian Arabic (EGY) sentence in Arabizi and in Arabic script. Arabizi poses a problem for natural language processing (NLP). While some tools have recently become available for processing EGY input, e.g., (Habash et al., 2012b; Habash et al., 2013; Pasha et al., 2014), they expect Arabic script input (or a Buckwalter transliteration). They cannot process Arabizi. We therefore need a tool that converts from Arabizi to Arabic script. However, the lack of standard orthography in EGY compounds the problem: what should we convert Arabizi into? Our answer to this question is to use CODA, a conventional orthography created for the purpose of supporting NLP tools (Habash et al., 2012a). The goal of CODA is to reduce the data sparseness that comes from the same word form appearing in many spontaneous orthographies in data"
W14-1604,N13-1044,1,0.945017,"ns to render the sounds of the DA sentence. Because the sound-to-letter rules of English are very different from those of Arabic, we obtain complex mappings between the two writing systems. This issue is compounded by the underlying problem that DA itself does not have any standard orthography in the Arabic script. Table 1 shows different plausible ways of writing an Egyptian Arabic (EGY) sentence in Arabizi and in Arabic script. Arabizi poses a problem for natural language processing (NLP). While some tools have recently become available for processing EGY input, e.g., (Habash et al., 2012b; Habash et al., 2013; Pasha et al., 2014), they expect Arabic script input (or a Buckwalter transliteration). They cannot process Arabizi. We therefore need a tool that converts from Arabizi to Arabic script. However, the lack of standard orthography in EGY compounds the problem: what should we convert Arabizi into? Our answer to this question is to use CODA, a conventional orthography created for the purpose of supporting NLP tools (Habash et al., 2012a). The goal of CODA is to reduce the data sparseness that comes from the same word form appearing in many spontaneous orthographies in data (be it annotated or un"
W14-1604,P08-2015,1,0.572407,"e-based approach to convert EGY to MSA. AlGaphari and Al-Yadoumi (2010) also used a rulebased method to transform from Sanaani dialect to MSA. Sawaf (2010), Salloum and Habash (2011) and Salloum and Habash (2013) used morphological analysis and morphosyntactic transformation rules for processing EGY and Levantine Arabic. There has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic, which Habash (2008) employed as part of generating English transliterations from Arabic words in the context of machine translation. This work is similar to ours in terms of text transliteration. However, our work is not restricted to names. 4 Approach 4.1 Defining the Task Our task is as follows: for each Arabizi word in the input, we choose the Arabic script word which is the correct CODA spelling of the input word and which carries the intended meaning (as determined in the context of the entire available text). We do not merge two or more input words into a single Arabic script word. If CODA requires two con"
W14-1604,P07-2045,0,0.0151876,"following preprocessing steps to the input Arabizi text: • We separate all attached emoticons such as (:D, :p, etc.) and punctuation from the words. We only keep the apostrophe because it is used in Arabizi to distinguish between different sounds. 3ARRIB keeps track of any word offset change, so that it can reconstruct the same number of tokens at the end of the pipeline. The words in the parallel data are turned into space-separated character tokens, which we align using Giza++ (Och and Ney, 2003). We then use the phrase extraction utility in the Moses statistical machine translation system (Koehn et al., 2007) to extract a phrase table which operates over characters. The phrase table is then used to build a finite-state transducer (FST) that maps sequences of Arabizi characters into sequences of Arabic script characters. We use the negative logarithmic conditional probabilities of the Arabizi-to-Arabic pairs in the phrase tables as costs inside the FST. We use the FST to transduce an input Arabizi word to one or more words in Arabic script, where every resulting word in Arabic script is given a probabilistic score. • We tag emoticons and punctuation to protect them from any change through the pipel"
W14-1604,J03-1002,0,0.0107327,"input words (which constitute a “sausage” lattice) using an n-gram language model. 4.3 Preprocessing We apply the following preprocessing steps to the input Arabizi text: • We separate all attached emoticons such as (:D, :p, etc.) and punctuation from the words. We only keep the apostrophe because it is used in Arabizi to distinguish between different sounds. 3ARRIB keeps track of any word offset change, so that it can reconstruct the same number of tokens at the end of the pipeline. The words in the parallel data are turned into space-separated character tokens, which we align using Giza++ (Och and Ney, 2003). We then use the phrase extraction utility in the Moses statistical machine translation system (Koehn et al., 2007) to extract a phrase table which operates over characters. The phrase table is then used to build a finite-state transducer (FST) that maps sequences of Arabizi characters into sequences of Arabic script characters. We use the negative logarithmic conditional probabilities of the Arabizi-to-Arabic pairs in the phrase tables as costs inside the FST. We use the FST to transduce an input Arabizi word to one or more words in Arabic script, where every resulting word in Arabic script"
W14-1604,pasha-etal-2014-madamira,1,0.53623,"Missing"
W14-1604,W11-2602,1,0.832085,"t CODAFY works on spontaneous text written in Arabic script, while our system works on Arabizi, which involves a higher degree of ambiguity. However, we use CODAFY as a black-box module in our preprocessing. Additionally, there is some work on converting from dialectal Arabic to MSA, which is similar to our work in terms of processing a dialectal input. However, our final output is in EGY and not MSA. Shaalan et al. (2007) introduced a rule-based approach to convert EGY to MSA. AlGaphari and Al-Yadoumi (2010) also used a rulebased method to transform from Sanaani dialect to MSA. Sawaf (2010), Salloum and Habash (2011) and Salloum and Habash (2013) used morphological analysis and morphosyntactic transformation rules for processing EGY and Levantine Arabic. There has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic, which Habash (2008) employed as part of generating English transliterations from Arabic words in the context of machine translation. This work is similar to ours in terms of text translite"
W14-1604,N13-1036,1,0.764251,"text written in Arabic script, while our system works on Arabizi, which involves a higher degree of ambiguity. However, we use CODAFY as a black-box module in our preprocessing. Additionally, there is some work on converting from dialectal Arabic to MSA, which is similar to our work in terms of processing a dialectal input. However, our final output is in EGY and not MSA. Shaalan et al. (2007) introduced a rule-based approach to convert EGY to MSA. AlGaphari and Al-Yadoumi (2010) also used a rulebased method to transform from Sanaani dialect to MSA. Sawaf (2010), Salloum and Habash (2011) and Salloum and Habash (2013) used morphological analysis and morphosyntactic transformation rules for processing EGY and Levantine Arabic. There has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic, which Habash (2008) employed as part of generating English transliterations from Arabic words in the context of machine translation. This work is similar to ours in terms of text transliteration. However, our work is n"
W14-1604,2010.amta-papers.5,0,0.0449809,"system is that CODAFY works on spontaneous text written in Arabic script, while our system works on Arabizi, which involves a higher degree of ambiguity. However, we use CODAFY as a black-box module in our preprocessing. Additionally, there is some work on converting from dialectal Arabic to MSA, which is similar to our work in terms of processing a dialectal input. However, our final output is in EGY and not MSA. Shaalan et al. (2007) introduced a rule-based approach to convert EGY to MSA. AlGaphari and Al-Yadoumi (2010) also used a rulebased method to transform from Sanaani dialect to MSA. Sawaf (2010), Salloum and Habash (2011) and Salloum and Habash (2013) used morphological analysis and morphosyntactic transformation rules for processing EGY and Levantine Arabic. There has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic, which Habash (2008) employed as part of generating English transliterations from Arabic words in the context of machine translation. This work is similar to ours"
W14-1604,zribi-etal-2014-conventional,1,0.904005,"r transliteration). They cannot process Arabizi. We therefore need a tool that converts from Arabizi to Arabic script. However, the lack of standard orthography in EGY compounds the problem: what should we convert Arabizi into? Our answer to this question is to use CODA, a conventional orthography created for the purpose of supporting NLP tools (Habash et al., 2012a). The goal of CODA is to reduce the data sparseness that comes from the same word form appearing in many spontaneous orthographies in data (be it annotated or unannotated). CODA has been defined for EGY as well as Tunisian Arabic (Zribi et al., 2014), and it has been used as part of different approaches for modeling DA morphology (Habash et al., 2012b), tagging (Habash et al., 2013; Pasha et al., 2014) and spelling correction (Eskander et al., 2013; Farra et al., 2014). This paper makes two main contributions. First, we clearly define the computational problem of transforming Arabizi to CODA. This improves over previous work by unambiguously fixing the In this paper, we address the problem of converting Dialectal Arabic (DA) text that is written in the Latin script (called Arabizi) into Arabic script following the CODA convention for DA o"
W14-1604,W02-0505,0,\N,Missing
W14-1604,W14-3629,0,\N,Missing
W14-2202,P98-1013,0,0.13163,"spoken by cul4 Elicitation with WELT WELT organizes elicitation sessions around a set of 3D scenes, which are created by inputting English text into WordsEye. Scenes can be imported and exported between sessions, so that useful scenes can be reused and data compared. WELT also provides tools for recording audio (which is automatically synced with open scenes), textual descriptions, glosses, and notes during a session. Screenshots are included in Figure 2. 4.1 Cultural Adaptation of VigNet To interpret input text, WordsEye uses VigNet (Coyne et al., 2011), a lexical resource based on FrameNet (Baker et al., 1998). As in FrameNet, lexical items are grouped in frames according to shared semantic structure. A frame contains a set of frame elements (semantic roles). FrameNet defines the mapping between syntax and semantics for a lexical item with valence patterns that map syntactic functions to frame elements. VigNet extends FrameNet in order to capture “graphical semantics”, a set of graphical constraints representing the position, orientation, size, color, texture, and poses of objects in the scene, 8 Figure 2: Screenshots of WELT elicitation interfaces people, it would instead invoke a woman sitting on"
W14-2202,I11-2002,0,0.0302543,"ary development, interlinearization of texts, analysis of discourse features, and morphological analysis. An important part of FLEx is its “linguistfriendly” morphological parser (Black and Simons, 2006), which uses an underlying model of morphology familiar to linguists, is fully integrated into lexicon development and interlinear text analysis, and produces a human-readable grammar sketch as well as a machine-interpretable parser. The morphological parser is constructed “stealthily” in the background, and can help a linguist by predicting glosses for interlinear texts. Linguist’s Assistant (Beale, 2011) provides a corpus of semantic representations for linguists to use as a guide for elicitation. After eliciting the language data, a linguist writes rules translating these semantic representations into surface forms. The result is a description of the language that can be used to generate text from documents that have been converted into the semantic representation. Linguists are encouraged to collect their own elicitations and naturally occurring texts and translate them into the semantic representation. The LinGO Grammar Matrix (Bender et al., 2002) facilitates formal modeling of syntax by"
W14-2202,C12-2013,0,0.0236747,"ut knowledge of specific computational formalisms. This is similar to the way FLEx allows linguists to create a formal model of morphology while also documenting the lexicon of a language and glossing interlinear texts. Computational tools for field linguistics fall into two categories: tools for native speakers to use directly, without substantial linguist intervention, and tools for field linguists to use. Tools intended for native speakers include the PAWS starter kit (Black and Black, 2009), which uses the answers to a series of guided questions to produce a draft of a grammar. Similarly, Bird and Chiang (2012) describe a simplified workflow and supporting MT software that lets native speakers produce useable documentation of their language on their own. One of the most widely-used toolkits in the latter category is SIL FieldWorks (SIL FieldWorks, 2014), or specifically, FieldWorks Language Explorer (FLEx). FLEx includes tools for eliciting and recording lexical information, dictionary development, interlinearization of texts, analysis of discourse features, and morphological analysis. An important part of FLEx is its “linguistfriendly” morphological parser (Black and Simons, 2006), which uses an un"
W14-2202,J09-3007,0,0.251859,"Missing"
W14-2202,W11-0905,1,0.872107,"tion of other languages that are similar linguistically or spoken by cul4 Elicitation with WELT WELT organizes elicitation sessions around a set of 3D scenes, which are created by inputting English text into WordsEye. Scenes can be imported and exported between sessions, so that useful scenes can be reused and data compared. WELT also provides tools for recording audio (which is automatically synced with open scenes), textual descriptions, glosses, and notes during a session. Screenshots are included in Figure 2. 4.1 Cultural Adaptation of VigNet To interpret input text, WordsEye uses VigNet (Coyne et al., 2011), a lexical resource based on FrameNet (Baker et al., 1998). As in FrameNet, lexical items are grouped in frames according to shared semantic structure. A frame contains a set of frame elements (semantic roles). FrameNet defines the mapping between syntax and semantics for a lexical item with valence patterns that map syntactic functions to frame elements. VigNet extends FrameNet in order to capture “graphical semantics”, a set of graphical constraints representing the position, orientation, size, color, texture, and poses of objects in the scene, 8 Figure 2: Screenshots of WELT elicitation in"
W14-2202,C98-1013,0,\N,Missing
W14-2518,chang-manning-2012-sutime,0,0.208749,"In this second phase of the research, we turn our attention to a novel task: the temporal dimension captured in these texts. By identifying the timing of geographically-cited events, we are able to examine how the history and projected future of the art world evolve alongside, and as a result of, its current geographical structure. To determine whether cities named in press releases refer to events that occur in the past, the present, or the future, we need methods for resolving time expressed in text relative to the time of the release of the text. We use the Stanford Temporal TaggerSUTime (Chang and Manning, 2012), as well as rules we have built on top of the Stanford part-of-speech tagger (Toutanova et al., 2003), to identify the temporal referent of each city mentioned. The two systems in combination perform slightly better than either does alone, and at a high 2 The Goals of the Project The shifting geography of contemporary art is of interest to social scientists as an instance of cultural globalization (Bourdieu, 1999; Crane, 2008). Scholars and contemporary art practitioners alike have remarked on changes in the geography of globally-significant art activity in recent decades (Vanderlinden and Fi"
W14-2518,J93-2004,0,0.045026,"pproach analyzes explicit time ex67 Accuracy Baseline∗ Lexical (L) Grammaticized (G) LG GL LG & GL 63.8 66.6 73.4 69.2 74.0 78.0 P 0 79.3 70.0 77.2 71.0 70.5 Past R 0 56.6 48.8 73.6 51.2 76.0 F1 0 66.1 57.5 75.4 59.5 73.1 P 63.8 75.3 74.9 81.2 75.5 82.2 Current R 100 75.5 89.0 71.8 89.0 85.6 F1 77.9 75.4 81.4 76.2 81.7 83.9 P 0 21.6 64.5 23.2 64.5 65.5 Future R 0 36.5 38.5 42.3 38.5 36.5 F1 0 27.1 48.2 29.9 48.2 46.9 Table 1: Accuracy and precision, recall and F-measure for past, current and future events. ∗ Baseline: Tagging all events as current. the tag sets from the Penn Treebank Project (Marcus et al., 1993). We use only the verbal tags: VB (Verb, base form), VBD (Verb, past tense), VBG (Verb, gerund or present participle), etc. and MD (Modal). Here are some examples of the rules; events associated with: (1) VBP or VBZ are tagged as current. (2) VBD are tagged as past. (3) VB that are preceded by will/MD are tagged as future. pressions in dates, time durations, holidays, etc. The second approach uses verbal tense to resolve grammaticized reference to time. In both approaches: (1) We use distance within a sentence as a heuristic to associate a temporal expression to a city. (2) Cities associated w"
W14-2518,N03-1033,0,0.0268755,"ptured in these texts. By identifying the timing of geographically-cited events, we are able to examine how the history and projected future of the art world evolve alongside, and as a result of, its current geographical structure. To determine whether cities named in press releases refer to events that occur in the past, the present, or the future, we need methods for resolving time expressed in text relative to the time of the release of the text. We use the Stanford Temporal TaggerSUTime (Chang and Manning, 2012), as well as rules we have built on top of the Stanford part-of-speech tagger (Toutanova et al., 2003), to identify the temporal referent of each city mentioned. The two systems in combination perform slightly better than either does alone, and at a high 2 The Goals of the Project The shifting geography of contemporary art is of interest to social scientists as an instance of cultural globalization (Bourdieu, 1999; Crane, 2008). Scholars and contemporary art practitioners alike have remarked on changes in the geography of globally-significant art activity in recent decades (Vanderlinden and Filipovic, 2005; McAndrew, 2008; Quemin, 2006). Accounts of these dynamics often converge: for example,"
W14-2710,W12-2105,1,0.567166,"d Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. Figure 2: Pearson Correlations for Topical Features We obtained significant strong positive correlation for TS Attempt# and TS AttemptAfterMod#. However, the normalized measure TS Attempt#N did not h"
W14-2710,I13-1025,1,0.794707,"and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. Figure 2: Pearson Correlations for Topical Features We obtained significant strong positive correlation for TS Atte"
W14-2710,P11-1078,0,0.300702,"hers have studied notions of control and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. Figure 2: Pearson Correlations for Topical Features We obtained signi"
W14-2710,I13-1042,1,0.883544,"t candidates’ power correlates with the distribution of topics they speak about in the debates. They found that when candidates have more power, they speak significantly more about certain topics (e.g., economy) and less about certain other topics (e.g., energy). However, these findings relate to the specific election cycle they analyzed and will not carry over to all political debates in general. A topical dimension with broader relevance is how topics change during the course of an interaction (e.g., who introduces more topics, who attempts to shift topics etc.). For instance, Nguyen et al. (2013) found that topic shifts within an interaction are correlated with the role a participant plays in it (e.g., being a moderator). They also analyzed US presidential debates, but with the objective of validating a topic segmentation method they proposed earlier (Nguyen et al., 2012). They do not study the topic shifting tendencies among the candidates in relation to their power differences. In this paper, we bring these two ideas together. We analyze the 2012 Republican presidential debates, modeling the power of a candidate based on poll scores as proposed by Prabhakaran et al. (2013a) and inve"
W14-2710,D13-1010,0,0.111378,"ower, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating personal attributes to political speech, Rosenberg and Hirschberg (2009) analyzed speech transcripts in the context of 2004 Democra"
W14-2710,C12-1155,0,0.086026,"cation” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion forums (DanescuNiculescu-Mizil et al., 2012; Biran et al., 2012) and online chat dialogs (Strzalkowski et al., 2012). The correlates analyzed in these studies range from word/phrase patterns, to derivatives of such patterns such as linguistic coordination, to deeper dialogic features such as argumentation and dialog acts. Our work differs from these studies in that we study the correlates of power in topic dynamics. Furthermore, we analyze spoken interactions. Figure 2: Pearson Correlations for Topical Features We obtained significant strong positive correlation for TS Attempt# and TS AttemptAfterMod#. However, the normalized measure TS Attempt#N did not have any significant correlation, suggesting that the"
W14-2710,W06-1639,0,0.328428,"perform this study on the US presidential debates and show that a candidate’s power, modeled after their poll scores, affects how often he/she attempts to shift topics and whether he/she succeeds. We ensure the validity of topic shifts by confirming, through a simple but effective method, that the turns that shift topics provide substantive topical content to the interaction. 1 Introduction Analyzing political speech has gathered great interest within the NLP community. Researchers have analyzed political text to identify markers of persuasion (Guerini et al., 2008), predict voting patterns (Thomas et al., 2006; Gerrish and Blei, 2011), and detect ideological positions (Sim et al., 2013). Studies have also looked into how personal attributes of political personalities such as charisma, confidence and power affect how they interact (Rosenberg and Hirschberg, 2009; Prabhakaran et al., 2013b). Our work belongs to this genre of studies. We analyze how a presidential candidate’s power, modeled after his/her relative poll standings, affect the dynamics of topic shifts during the course of a presidential debate. 2 Motivation In early work on correlating personal attributes to political speech, Rosenberg an"
W14-2710,P12-1009,0,0.270053,"ever, these findings relate to the specific election cycle they analyzed and will not carry over to all political debates in general. A topical dimension with broader relevance is how topics change during the course of an interaction (e.g., who introduces more topics, who attempts to shift topics etc.). For instance, Nguyen et al. (2013) found that topic shifts within an interaction are correlated with the role a participant plays in it (e.g., being a moderator). They also analyzed US presidential debates, but with the objective of validating a topic segmentation method they proposed earlier (Nguyen et al., 2012). They do not study the topic shifting tendencies among the candidates in relation to their power differences. In this paper, we bring these two ideas together. We analyze the 2012 Republican presidential debates, modeling the power of a candidate based on poll scores as proposed by Prabhakaran et al. (2013a) and investigate various features that capture the topical dynamics in the debates. We show that the power affects how often candidates atIn this paper, we investigate how topic dynamics during the course of an interaction correlate with the power differences between its participants. We p"
W14-2710,P90-1010,0,0.595258,"he instances of topic shift attempts by the candidates when responding to a question from the moderator (TS AttemptAfterMod# and 6 Analysis and Results We performed a correlation analysis on the features described in the previous section with respect to each candidate against the power he/she had at the time of the debate (based on recent poll scores). Figure 2 shows the Pearson’s product correlation between each topical feature and candidate’s power. Dark bars denote statistically significant (p &lt; 0.05) features. 80 nity, researchers have studied notions of control and initiative in dialogs (Walker and Whittaker, 1990; Jordan and Di Eugenio, 1997). Walker and Whittaker (1990) define “control of communication” in terms of whether the discourse participants are providing new, unsolicited information in their utterances. Their notion of control differs from our notion of power; however, the way we model topic shifts is closely related to their utterance level control assignment. Within the NLP community, researchers have studied power and influence in various genres of interactions, such as organizational email threads (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran and Rambow, 2013), online discussion foru"
W14-3008,D10-1100,1,0.929591,"al et al., 2010), a particular kind of event which involves (at least) two people such that at least one of them is aware of the other person. If only one person is aware of the event, we call it Observation (OBS): for example, someone is talking about someone else in their absence. If both people are aware of the event, we call it Interaction (INR): for example, one person is telling the other a story. Our claim is that links in social networks are in fact made up of social events: OBS social events give rise to one-way links, and INR social events to twoway links. For more information, see (Agarwal and Rambow, 2010; Agarwal et al., 2013a; Agarwal et al., 2013b). From an NLP point of view, we have a difficult cluster of phenomena: we have a precise definition of what we want to find, but it is based on the cognitive state of the event participants, which is almost never described explicitly in the text. Furthermore, the definitions cover a large number of diverse situations such as talking, spying, having lunch, fist fighting, or kissing. Furthermore, some semantic differences are not relevant: verbs such as talk, tell, deny, all have the same meaning with respect to social events. Finally, in order to d"
W14-3008,W10-1803,1,0.900829,"Missing"
W14-3008,I13-1171,1,0.838696,"ular kind of event which involves (at least) two people such that at least one of them is aware of the other person. If only one person is aware of the event, we call it Observation (OBS): for example, someone is talking about someone else in their absence. If both people are aware of the event, we call it Interaction (INR): for example, one person is telling the other a story. Our claim is that links in social networks are in fact made up of social events: OBS social events give rise to one-way links, and INR social events to twoway links. For more information, see (Agarwal and Rambow, 2010; Agarwal et al., 2013a; Agarwal et al., 2013b). From an NLP point of view, we have a difficult cluster of phenomena: we have a precise definition of what we want to find, but it is based on the cognitive state of the event participants, which is almost never described explicitly in the text. Furthermore, the definitions cover a large number of diverse situations such as talking, spying, having lunch, fist fighting, or kissing. Furthermore, some semantic differences are not relevant: verbs such as talk, tell, deny, all have the same meaning with respect to social events. Finally, in order to decode the events in te"
W14-3008,I13-2009,1,0.825834,"ular kind of event which involves (at least) two people such that at least one of them is aware of the other person. If only one person is aware of the event, we call it Observation (OBS): for example, someone is talking about someone else in their absence. If both people are aware of the event, we call it Interaction (INR): for example, one person is telling the other a story. Our claim is that links in social networks are in fact made up of social events: OBS social events give rise to one-way links, and INR social events to twoway links. For more information, see (Agarwal and Rambow, 2010; Agarwal et al., 2013a; Agarwal et al., 2013b). From an NLP point of view, we have a difficult cluster of phenomena: we have a precise definition of what we want to find, but it is based on the cognitive state of the event participants, which is almost never described explicitly in the text. Furthermore, the definitions cover a large number of diverse situations such as talking, spying, having lunch, fist fighting, or kissing. Furthermore, some semantic differences are not relevant: verbs such as talk, tell, deny, all have the same meaning with respect to social events. Finally, in order to decode the events in te"
W14-3008,E14-1023,1,0.811574,"cover a large number of diverse situations such as talking, spying, having lunch, fist fighting, or kissing. Furthermore, some semantic differences are not relevant: verbs such as talk, tell, deny, all have the same meaning with respect to social events. Finally, in order to decode the events in terms of social events, we need to understand the roles: if I am talking to Sudeep about Mae, Sudeep and I have an INR social event with each other, and we both have a OBS social event with Mae. Thus, this problem sounds like an excellent application for frame semantics! We present initial results in (Agarwal et al., 2014), and summarize them here. We use Semafor (Chen et al., 2010) as a black box to obtain the semantic parse of a sentence. However, there are several problems: We summarize our experience using FrameNet in two rather different projects in natural language processing (NLP). We conclude that NLP can benefit from FrameNet in different ways, but we sketch some problems that need to be overcome. 1 Introduction We present two projects at Columbia in which we use FrameNet. In these projects, we do not develop basic NLP tools for FrameNet, and we do not develop FramNets for new languages: we simply use"
W14-3008,S10-1059,0,0.0249641,"ng, having lunch, fist fighting, or kissing. Furthermore, some semantic differences are not relevant: verbs such as talk, tell, deny, all have the same meaning with respect to social events. Finally, in order to decode the events in terms of social events, we need to understand the roles: if I am talking to Sudeep about Mae, Sudeep and I have an INR social event with each other, and we both have a OBS social event with Mae. Thus, this problem sounds like an excellent application for frame semantics! We present initial results in (Agarwal et al., 2014), and summarize them here. We use Semafor (Chen et al., 2010) as a black box to obtain the semantic parse of a sentence. However, there are several problems: We summarize our experience using FrameNet in two rather different projects in natural language processing (NLP). We conclude that NLP can benefit from FrameNet in different ways, but we sketch some problems that need to be overcome. 1 Introduction We present two projects at Columbia in which we use FrameNet. In these projects, we do not develop basic NLP tools for FrameNet, and we do not develop FramNets for new languages: we simply use FrameNet or a FrameNet parser in an NLP application. The firs"
W14-3008,W11-0905,1,0.90801,"an claimed [he]T 1−Ind bought drugs from the [defendants]T 2−Grp .”. The tree on the left is FrameForest and the tree on the right is FrameTree. 4 in FrameForest refers to the subtree (bought (T1-Ind) (from T2-Grp)). Ind refers to individual and Grp refers to group. graph structure that can represent a full text fragment (including coreference). We are planning to develop parsers that convert text directly into such graph-based representations, inspired by recent work on semantic parsing (Jones et al., 2012). We extend FrameNet in two ways to obtain the resource we need, which we call VigNet (Coyne et al., 2011). The pictures created by the WordsEye system are based on spatial arrangements (scenes) of predefined 3D models. At a low level, scenes are described by primitive spatial relations between sets of these models (The man is in front of the woman. He is looking at her. His mouth is open.). We would like to use WordsEye to depict scenarios, events, and actions (John told Mary his life story). These can be seen as complex relations between event participants. We turn to FrameNet frames as representations for such relations. FrameNet offers a large inventory of frames, together with additional stru"
W14-3008,C12-1042,1,0.860355,"allel, is used to decompose a Vignette into 32 References graphical sub-relations, which are in turn frames (either graphical primitives or other vignettes). Like any frame-to-frame relation, it maps frame elements of the source frame to frame elements of the target frame. New frame elements can also be introduced. For instance, one Vignette for I N GESTION that can be used if the INGESTIBLE is a liquid contains a new frame element CONTAINER. The I NGESTOR is holding the container and the liquid is in the container. We have populated the VigNet resource using a number of different approaches (Coyne et al., 2012), including multiple choice questions on Amazon Mechanical Turk to define vignettes for locations (rooms), using the system itself to define locations, and a number of web-based annotation tools to define vignettes for actions. An ongoing project is exploring the use of WordsEye and VigNet as a tool for field linguists and for language documentation and preservation. The WordsEye Linguistics Toolkit (WELT, (Ulinski et al., 2014)) makes it easy to produce pictures for field linguistic elicitation. It will also provide an environment to essentially develop language specific VigNets as models of"
W14-3008,C12-1083,1,0.826272,"rce buy Coleman Buyer Seller T1’-Ind T2-Grp he defendants Figure 1: Semantic trees for the sentence “Coleman claimed [he]T 1−Ind bought drugs from the [defendants]T 2−Grp .”. The tree on the left is FrameForest and the tree on the right is FrameTree. 4 in FrameForest refers to the subtree (bought (T1-Ind) (from T2-Grp)). Ind refers to individual and Grp refers to group. graph structure that can represent a full text fragment (including coreference). We are planning to develop parsers that convert text directly into such graph-based representations, inspired by recent work on semantic parsing (Jones et al., 2012). We extend FrameNet in two ways to obtain the resource we need, which we call VigNet (Coyne et al., 2011). The pictures created by the WordsEye system are based on spatial arrangements (scenes) of predefined 3D models. At a low level, scenes are described by primitive spatial relations between sets of these models (The man is in front of the woman. He is looking at her. His mouth is open.). We would like to use WordsEye to depict scenarios, events, and actions (John told Mary his life story). These can be seen as complex relations between event participants. We turn to FrameNet frames as repr"
W14-3008,W14-2202,1,0.883941,"Missing"
W14-3612,W14-1604,1,0.873348,"buṣṣ/ “look” is  أنظر/’unZur/ in MSA. 3 per as part of the automatic transliteration step because they target the same conventional orthography of dialectal Arabic (CODA) (Habash et al., 2012a, 2012b), which we also target. There are several commercial products that convert Arabizi to Arabic script, namely: Microsoft Maren, 2 Google Ta3reeb, 3 Basis Arabic chat translator4 and Yamli.5 Since these products are for commercial purposes, there is little information available about their approaches, and whatever resources they use are not publicly available for research purposes. Furthermore, as Al-Badrashiny et al. (2014) point out, Maren, Ta3reeb and Yamli are primarily intended as input method support, not full text transliteration. As a result, their users’ goal is to produce Arabic script text not Arabizi text, which affects the form of the romanization they utilize as an intermediate step. The differences between such “functional romanization” and real Arabizi include that the users of these systems will use less or no code switching to English, and may employ character sequences that help them arrive at the target Arabic script form faster, which otherwise they would not write if they were targeting Arab"
W14-3612,W12-4808,0,0.117255,"(MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter-sentence level. They crawled a large dataset of MSA-DA news commentaries, and used Amazon Mechanical Turk to annotate the dataset at the sentence level. Elfardy et al. (2013) presented a system, AIDA, that tags each word in a sentence as either DA or MSA based on the context. Lui et al. (2014) proposed a system for language identification in Related Work Arabizi-Arabic Script Transliteration Previous efforts on automatic transliterations from Arabizi to Arabic script include work by Chalabi and Gerges (2012), Darwish (2013) and AlBadrashiny et al. (2014). All of these approaches rely on a model for character-to-character mapping that is used to generate a lattice of multiple alternative words which are then selected among using a language model. The training data used by Darwish (2013) is publicly available but it is quite limited (2,200 word pairs). The work we are describing here can help substantially improve the quality of such system. We use the system of Al-Badrashiny et al. (2014) in this pa2 http://www.getmaren.com http://www.google.com/ta3reeb 4 http://www.basistech.com/arabic-chat-trans"
W14-3612,R13-1026,0,0.0169932,"d topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level of internet communication. 4.1 In BOLT Phase 2"
W14-3612,W14-3901,1,0.757221,"Missing"
W14-3612,N13-1066,1,0.386912,"&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level of internet communication. 4.1 In BOLT Phase 2, LDC collected large volumes of naturally occurring informal text (SMS) and chat messages from individual users in English, Chinese and Egyptian Arabic (Song et al., 2014). Altogether we recruited 46 Egyptian Arabic participants, and of those 26 contributed data. To protect privacy, participation was"
W14-3612,P11-2008,0,0.0502639,"Missing"
W14-3612,W11-0704,0,0.0923825,"mixture model that is based on supervised topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level"
W14-3612,habash-etal-2012-conventional,1,0.702911,"cation in Arabic takes place using a variety of orthographies and writing systems, including Arabic script, Arabizi, and a mixture of the two. Although not all social media communication uses Arabizi, the use of Arabizi is prevalent enough to pose a challenge for Arabic NLP research. In the context of natural language processing of social media Arabic, transliterating from Arabizi of various dialects to Arabic script is a necessary step, since many of the existing stateof-the-art resources for Arabic dialect processing and annotation expect Arabic script input (e.g., Salloum and Habash, 2011; Habash et al. 2012c; Pasha et al., 2014). To our knowledge, there are no naturally occurring parallel texts of Arabizi and Arabic script. In this paper, we describe the process of creating such a novel resource at the Linguistic Data Consortium (LDC). We believe this corpus will be essential for developing robust tools for converting Arabizi into Arabic script. Abstract This paper describes the process of creating a novel resource, a parallel Arabizi-Arabic script corpus of SMS/Chat data. The language used in social media expresses many differences from other written genres: its vocabulary is informal with inte"
W14-3612,W12-2301,1,0.860943,"cation in Arabic takes place using a variety of orthographies and writing systems, including Arabic script, Arabizi, and a mixture of the two. Although not all social media communication uses Arabizi, the use of Arabizi is prevalent enough to pose a challenge for Arabic NLP research. In the context of natural language processing of social media Arabic, transliterating from Arabizi of various dialects to Arabic script is a necessary step, since many of the existing stateof-the-art resources for Arabic dialect processing and annotation expect Arabic script input (e.g., Salloum and Habash, 2011; Habash et al. 2012c; Pasha et al., 2014). To our knowledge, there are no naturally occurring parallel texts of Arabizi and Arabic script. In this paper, we describe the process of creating such a novel resource at the Linguistic Data Consortium (LDC). We believe this corpus will be essential for developing robust tools for converting Arabizi into Arabic script. Abstract This paper describes the process of creating a novel resource, a parallel Arabizi-Arabic script corpus of SMS/Chat data. The language used in social media expresses many differences from other written genres: its vocabulary is informal with inte"
W14-3612,P97-1017,0,0.0276316,"tion. As a result, their users’ goal is to produce Arabic script text not Arabizi text, which affects the form of the romanization they utilize as an intermediate step. The differences between such “functional romanization” and real Arabizi include that the users of these systems will use less or no code switching to English, and may employ character sequences that help them arrive at the target Arabic script form faster, which otherwise they would not write if they were targeting Arabizi (Al-Badrashiny et al., 2014). Name Transliteration There has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic. Although the general goal of transliterating from one script to another is shared between these efforts and ours, we are considering a more general form of the problem in that we do not restrict ourselves to names. Code Switching There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter"
W14-3612,maamouri-etal-2014-developing,1,0.720718,"ect Arabizi is used to write in multiple dialects of Arabic, and differences between the dialects themselves have an effect on the spellings chosen by individual writers using Arabizi. Because Egyptian Arabic is the dialect of the corpus cre1 http://www.darpa.mil/Our_Work/I2O/Programs/Broad_Op erational_Language_Translation_%28BOLT%29.aspx 94 ated for this project, we will briefly discuss some of the most relevant features of Egyptian Arabic with respect to Arabizi transliteration. For a more extended discussion of the differences between MSA and Egyptian Arabic, see Habash et al. (2012a) and Maamouri et al. (2014). Phonologically, Egyptian Arabic is characterized by the following features, compared with MSA: (a) The loss of the interdentals /ð/ and /θ/ which are replaced by /d/ or /z/ and /t/ or /s/ respectively, thus giving those two original consonants a heavier load. Examples include  ذكر/zakar/ “to mention”,  ذبح/dabaħ/ “to slaughter”,  ثلج/talg/ “ice”,  ثمن/taman/ “price”, and  ثبت/sibit/ “to stay in place, become immobile”. (b) The exclusion of /q/ and /ǰ/ from the consonantal system, being replaced by the /ʔ/ and /g/, e.g.,  قطن/ʔuṭn/ “cotton”, and جمل /gamal/ “camel”. At the level"
W14-3612,pasha-etal-2014-madamira,1,0.89422,"Missing"
W14-3612,D11-1141,0,0.0398961,"is based on supervised topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic yet. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of dialectal Arabic such as the lack of a standard orthography. Eskander et al. (2013) described a method for normalizing spontaneous orthography into CODA. 4 Egyptian Arabic has the advantage over all other dialects of Arabic of being the language of the largest linguistic community in the Arab region, and also of having a rich level of internet communic"
W14-3612,W11-2602,1,0.567837,"013). Social media communication in Arabic takes place using a variety of orthographies and writing systems, including Arabic script, Arabizi, and a mixture of the two. Although not all social media communication uses Arabizi, the use of Arabizi is prevalent enough to pose a challenge for Arabic NLP research. In the context of natural language processing of social media Arabic, transliterating from Arabizi of various dialects to Arabic script is a necessary step, since many of the existing stateof-the-art resources for Arabic dialect processing and annotation expect Arabic script input (e.g., Salloum and Habash, 2011; Habash et al. 2012c; Pasha et al., 2014). To our knowledge, there are no naturally occurring parallel texts of Arabizi and Arabic script. In this paper, we describe the process of creating such a novel resource at the Linguistic Data Consortium (LDC). We believe this corpus will be essential for developing robust tools for converting Arabizi into Arabic script. Abstract This paper describes the process of creating a novel resource, a parallel Arabizi-Arabic script corpus of SMS/Chat data. The language used in social media expresses many differences from other written genres: its vocabulary i"
W14-3612,voss-etal-2014-finding,0,0.0375887,"then selected among using a language model. The training data used by Darwish (2013) is publicly available but it is quite limited (2,200 word pairs). The work we are describing here can help substantially improve the quality of such system. We use the system of Al-Badrashiny et al. (2014) in this pa2 http://www.getmaren.com http://www.google.com/ta3reeb 4 http://www.basistech.com/arabic-chat-translatortransforms-social-media-analysis/ 5 http://www.yamli.com/ 3 95 multilingual documents using a generative mixture model that is based on supervised topic modeling algorithms. Darwish (2013) and Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, Voss et al. (2014) deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. Darwish (2013)&apos;s data is more focused on Egyptian and Levantine Arabic and code switching with English. Processing Social Media Text Finally, while English NLP for social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much work on Arabic y"
W14-3612,P11-2007,0,0.0118384,"ere has been some work on machine transliteration by Knight and Graehl (1997). Al-Onaizan and Knight (2002) introduced an approach for machine transliteration of Arabic names. Freeman et al. (2006) also introduced a system for name matching between English and Arabic. Although the general goal of transliterating from one script to another is shared between these efforts and ours, we are considering a more general form of the problem in that we do not restrict ourselves to names. Code Switching There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter-sentence level. They crawled a large dataset of MSA-DA news commentaries, and used Amazon Mechanical Turk to annotate the dataset at the sentence level. Elfardy et al. (2013) presented a system, AIDA, that tags each word in a sentence as either DA or MSA based on the context. Lui et al. (2014) proposed a system for language identification in Related Work Arabizi-Arabic Script Transliteration Previous efforts on automatic transliterations from Arabizi to Arabic script include work by Chalabi and Gerges (2012), Darwish (2013) and AlBadrashiny et al."
W14-3612,Q14-1003,0,0.0204432,"een these efforts and ours, we are considering a more general form of the problem in that we do not restrict ourselves to names. Code Switching There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) were interested in this problem at the inter-sentence level. They crawled a large dataset of MSA-DA news commentaries, and used Amazon Mechanical Turk to annotate the dataset at the sentence level. Elfardy et al. (2013) presented a system, AIDA, that tags each word in a sentence as either DA or MSA based on the context. Lui et al. (2014) proposed a system for language identification in Related Work Arabizi-Arabic Script Transliteration Previous efforts on automatic transliterations from Arabizi to Arabic script include work by Chalabi and Gerges (2012), Darwish (2013) and AlBadrashiny et al. (2014). All of these approaches rely on a model for character-to-character mapping that is used to generate a lattice of multiple alternative words which are then selected among using a language model. The training data used by Darwish (2013) is publicly available but it is quite limited (2,200 word pairs). The work we are describing here"
W14-3612,W02-0505,0,\N,Missing
W14-3612,W14-3629,0,\N,Missing
W14-3612,song-etal-2014-collecting,1,\N,Missing
W14-3612,N06-1060,0,\N,Missing
W14-3901,W14-1604,1,0.912297,"al documents, using a generative mixture model that is based on supervised topic modeling algorithms. This is similar to our work in terms of identifying code switching. However, our system deals with Arabizi, a non-standard orthography with high variability, making the identification task much harder. Concerning specifically NLP for Arabizi, Darwish (2013) (published in an updated version as (Darwish, 2014)) is similar to our work in that he identifies English in Arabizi text and he also transliterates Arabic text from Arabizi to Arabic script. We compare our transliteration method to his in Al-Badrashiny et al. (2014). For identification of non-Arabic words in Arabizi, Darwish (2013) uses word and sequence-level features with CRF modeling; while we use SVMs and decision trees. Darwish (2013) identifies three tags: Arabic, foreign and others (such as email addresses and URLs). In contrast, we identify a bigger set: Arabic, foreign, names, sounds, punctuation use as a black box an existing component that we developed to transliterate from Arabizi to Arabic script (Al-Badrashiny et al., 2014). This paper concentrates on the task of identifying which tokens should be transliterated. A recent release of annotat"
W14-3901,W14-3612,1,0.804573,"rwish (2013) uses word and sequence-level features with CRF modeling; while we use SVMs and decision trees. Darwish (2013) identifies three tags: Arabic, foreign and others (such as email addresses and URLs). In contrast, we identify a bigger set: Arabic, foreign, names, sounds, punctuation use as a black box an existing component that we developed to transliterate from Arabizi to Arabic script (Al-Badrashiny et al., 2014). This paper concentrates on the task of identifying which tokens should be transliterated. A recent release of annotated data by the Linguistic Data Consortium (LDC, 2014c; Bies et al., 2014) has enabled novel research on this topic. The corpus provides each token with a tag, as well as a transliteration if appropriate. The tags identify foreign words, as well as Arabic words, names, punctuation, and sounds. Only Arabic words and names are transliterated. (Note that code switching is not distinguished from borrowing.) Emoticons, which may be isolated or part of an input token, are also identified, and converted into a conventional symbol (#). This paper presents taggers for the tags, and an end-to-end system which takes Arabizi input and produces a complex output which consists of"
W14-3901,W12-4808,0,0.0953446,"ally the case, hard. And, as in any language used in social media and chat, Arabizi may also include abbreviations, such as isa which An means é<Ë@ ZA à@ ˇ šA’ Allh ‘God willing’ and lol ‘laugh out loud’. The rows marked with Arabizi in Figure 1 demonstrate some of the salient features of Arabizi. The constructed example in the figure is of an SMS conversation in Egyptian Arabic. and emoticons. Furthermore, Darwish (2013) uses around 5K words for training his taggers and 3.5K words for testing; this is considerably smaller than our training and test sets of 113K and 32K words, respectively. Chalabi and Gerges (2012) presented a hybrid approach for Arabizi transliteration. Their work does not address the detection of English words, punctuation, emoticons, and so on. They also do not handle English when mixed with Arabizi. Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, they deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. There are many differences between our work and theirs: they have noisy training data, and they have a much more balanced test set. They also only deal with fore"
W14-3901,habash-etal-2012-conventional,1,0.842679,"aid interpretation by indicating division of text into sentences and clauses, etc. Examples of punctuation marks are the semicolon ;, the exclamation mark ! and the right brace }. Emoticons are not considered punctuation and are handled as part of the transliteration task discussed below. Arabizi-to-Arabic Transliteration The second annotation task is about converting Arabizi to an Arabic-script-based orthography. Since, dialectal Arabic including Egyptian Arabic has no standard orthography in Arabic script, the annotation uses a conventionalized orthography for Dialectal Arabic called CODA (Habash et al., 2012a; Eskander et al., 2013; Zribi et al., 2014). Every word has a single orthographic representation in CODA. In the corpus we use, only words tagged as Arabic or Name are manually checked and corrected. The transliteration respects the whitespace boundaries of the original Arabizi words. In cases where an Arabizi word represents a prefix or suffix that should be joined in CODA to the next or previous word, a [+] symbol is added to mark this decision. Similarly, for Arabizi words that should be split into multiple CODA words, the CODA words are written with added [-] symbol delimiting the word b"
W14-3901,W12-2301,1,0.867263,"aid interpretation by indicating division of text into sentences and clauses, etc. Examples of punctuation marks are the semicolon ;, the exclamation mark ! and the right brace }. Emoticons are not considered punctuation and are handled as part of the transliteration task discussed below. Arabizi-to-Arabic Transliteration The second annotation task is about converting Arabizi to an Arabic-script-based orthography. Since, dialectal Arabic including Egyptian Arabic has no standard orthography in Arabic script, the annotation uses a conventionalized orthography for Dialectal Arabic called CODA (Habash et al., 2012a; Eskander et al., 2013; Zribi et al., 2014). Every word has a single orthographic representation in CODA. In the corpus we use, only words tagged as Arabic or Name are manually checked and corrected. The transliteration respects the whitespace boundaries of the original Arabizi words. In cases where an Arabizi word represents a prefix or suffix that should be joined in CODA to the next or previous word, a [+] symbol is added to mark this decision. Similarly, for Arabizi words that should be split into multiple CODA words, the CODA words are written with added [-] symbol delimiting the word b"
W14-3901,W14-3629,0,0.0893449,"ord is unknown in the LM, then its tag is assigned through MADAMIRA, a morphological disambiguator Pasha et al. (2014). Lui et al. (2014) proposed a system that does language identification in multilingual documents, using a generative mixture model that is based on supervised topic modeling algorithms. This is similar to our work in terms of identifying code switching. However, our system deals with Arabizi, a non-standard orthography with high variability, making the identification task much harder. Concerning specifically NLP for Arabizi, Darwish (2013) (published in an updated version as (Darwish, 2014)) is similar to our work in that he identifies English in Arabizi text and he also transliterates Arabic text from Arabizi to Arabic script. We compare our transliteration method to his in Al-Badrashiny et al. (2014). For identification of non-Arabic words in Arabizi, Darwish (2013) uses word and sequence-level features with CRF modeling; while we use SVMs and decision trees. Darwish (2013) identifies three tags: Arabic, foreign and others (such as email addresses and URLs). In contrast, we identify a bigger set: Arabic, foreign, names, sounds, punctuation use as a black box an existing compon"
W14-3901,R13-1026,0,0.016535,"system. This paper is structured as follows. We start by presenting related work (Section 2), and then we present relevant linguistic facts and explain how the data is annotated (Section 3). After summarizing our system architecture (Section 4) and experimental setup (Section 5), we present our systems for tagging in Sections 6, 7 and 8. The evaluation results are presented in Section 9. 2 Related Work While natural language processing for English in social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much 2 T (which itself can be also be represented using the digit “6”). Text written in Arabizi also tends to have a large number of foreign words, that are either borrowings such as telephone, or code switching, such as love you!. Note that Arabizi often uses the source language orthography for borrowings (especially recent borrowings), even if the Arabic pronunciation is somewhat modified. As a result, distinguishing borrowings from code switching is, as is usually the case, hard. And, as in any language used in social media and chat, Arabizi may also include abbreviatio"
W14-3901,W14-3911,1,0.823497,"al with DA written in the Roman alphabet (though they do discuss non-Arabic characters). There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) are interested in this problem at the intersentence level. They crawl a large dataset of MSA-DA news commentaries. They use Amazon Mechanical Turk to annotate the dataset at the sentence level. Then they use a language modeling approach to predict the class (MSA or DA) for an unseen sentence. There is other work on dialect identification, such as AIDA (Elfardy et al., 2013; Elfardy et al., 2014). In AIDA, some statistical and morphological analyses are applied to capture code switching between MSA and DA within the same sentence. Each word in the sentence is tagged to be either DA or MSA based on the context. The tagging process mainly depends on the language modeling (LM) approach, but if a word is unknown in the LM, then its tag is assigned through MADAMIRA, a morphological disambiguator Pasha et al. (2014). Lui et al. (2014) proposed a system that does language identification in multilingual documents, using a generative mixture model that is based on supervised topic modeling alg"
W14-3901,Q14-1003,0,0.0220534,"eling approach to predict the class (MSA or DA) for an unseen sentence. There is other work on dialect identification, such as AIDA (Elfardy et al., 2013; Elfardy et al., 2014). In AIDA, some statistical and morphological analyses are applied to capture code switching between MSA and DA within the same sentence. Each word in the sentence is tagged to be either DA or MSA based on the context. The tagging process mainly depends on the language modeling (LM) approach, but if a word is unknown in the LM, then its tag is assigned through MADAMIRA, a morphological disambiguator Pasha et al. (2014). Lui et al. (2014) proposed a system that does language identification in multilingual documents, using a generative mixture model that is based on supervised topic modeling algorithms. This is similar to our work in terms of identifying code switching. However, our system deals with Arabizi, a non-standard orthography with high variability, making the identification task much harder. Concerning specifically NLP for Arabizi, Darwish (2013) (published in an updated version as (Darwish, 2014)) is similar to our work in that he identifies English in Arabizi text and he also transliterates Arabic text from Arabizi"
W14-3901,N13-1066,1,0.89723,"Missing"
W14-3901,pasha-etal-2014-madamira,1,0.885156,"Missing"
W14-3901,P11-2008,0,0.0248831,"Missing"
W14-3901,D11-1141,0,0.0179926,"valuate the complete system. This paper is structured as follows. We start by presenting related work (Section 2), and then we present relevant linguistic facts and explain how the data is annotated (Section 3). After summarizing our system architecture (Section 4) and experimental setup (Section 5), we present our systems for tagging in Sections 6, 7 and 8. The evaluation results are presented in Section 9. 2 Related Work While natural language processing for English in social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much 2 T (which itself can be also be represented using the digit “6”). Text written in Arabizi also tends to have a large number of foreign words, that are either borrowings such as telephone, or code switching, such as love you!. Note that Arabizi often uses the source language orthography for borrowings (especially recent borrowings), even if the Arabic pronunciation is somewhat modified. As a result, distinguishing borrowings from code switching is, as is usually the case, hard. And, as in any language used in social media and chat, Arabizi ma"
W14-3901,W11-0704,0,0.0699388,"us components, and evaluate the complete system. This paper is structured as follows. We start by presenting related work (Section 2), and then we present relevant linguistic facts and explain how the data is annotated (Section 3). After summarizing our system architecture (Section 4) and experimental setup (Section 5), we present our systems for tagging in Sections 6, 7 and 8. The evaluation results are presented in Section 9. 2 Related Work While natural language processing for English in social media has attracted considerable attention recently (Clark and Araki, 2011; Gimpel et al., 2011; Gouws et al., 2011; Ritter et al., 2011; Derczynski et al., 2013), there has not been much 2 T (which itself can be also be represented using the digit “6”). Text written in Arabizi also tends to have a large number of foreign words, that are either borrowings such as telephone, or code switching, such as love you!. Note that Arabizi often uses the source language orthography for borrowings (especially recent borrowings), even if the Arabic pronunciation is somewhat modified. As a result, distinguishing borrowings from code switching is, as is usually the case, hard. And, as in any language used in social media"
W14-3901,voss-etal-2014-finding,0,0.251586,"in Figure 1 demonstrate some of the salient features of Arabizi. The constructed example in the figure is of an SMS conversation in Egyptian Arabic. and emoticons. Furthermore, Darwish (2013) uses around 5K words for training his taggers and 3.5K words for testing; this is considerably smaller than our training and test sets of 113K and 32K words, respectively. Chalabi and Gerges (2012) presented a hybrid approach for Arabizi transliteration. Their work does not address the detection of English words, punctuation, emoticons, and so on. They also do not handle English when mixed with Arabizi. Voss et al. (2014) deal with exactly the problem of classifying tokens in Arabizi as Arabic or not. More specifically, they deal with Moroccan Arabic, and with both French and English, meaning they do a three-way classification. There are many differences between our work and theirs: they have noisy training data, and they have a much more balanced test set. They also only deal with foreignness, and do not address the other tags we deal with, nor do they actually discuss transliteration itself. 3 3.2 The data set we use in this paper was created by the Linguistic Data Consortium (Bies et al., 2014; LDC, 2014a;"
W14-3901,P11-2007,0,0.119699,", pages 1–12, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics work on Arabic yet. We give a brief summary of relevant work on Arabic. Darwish et al. (2012) discuss NLP problems in retrieving Arabic microblogs (tweets). They discuss many of the same issues we do, notably the problems arising from the use of DA such as the lack of a standard orthography. However, they do not deal with DA written in the Roman alphabet (though they do discuss non-Arabic characters). There is some work on code switching between Modern Standard Arabic (MSA) and dialectal Arabic (DA). Zaidan and Callison-Burch (2011) are interested in this problem at the intersentence level. They crawl a large dataset of MSA-DA news commentaries. They use Amazon Mechanical Turk to annotate the dataset at the sentence level. Then they use a language modeling approach to predict the class (MSA or DA) for an unseen sentence. There is other work on dialect identification, such as AIDA (Elfardy et al., 2013; Elfardy et al., 2014). In AIDA, some statistical and morphological analyses are applied to capture code switching between MSA and DA within the same sentence. Each word in the sentence is tagged to be either DA or MSA base"
W14-3901,zribi-etal-2014-conventional,1,0.894091,"Missing"
W14-3901,diab-etal-2014-tharwa,1,\N,Missing
W14-5816,W11-0132,0,0.174798,"ll is Figure 2: Derivation tree for ‘Jill is running’. The dashed node indicates adjunction and the solid node indicates substitution 129 same lexical item realized as the anchor of varying syntactic realizations. For example a verb such as run will anchor a different elementary tree for its passive or interrogative variant. 3 Data In this section, we introduce the nominal predicates that will be the focus of our LTAG analysis. Such nominals allow an agentive (ergative-marked2 ) subject with the light verb kar ‘do’. In contrast, the same nominal does not have an agentive subject with ho ‘be’ (Ahmed and Butt, 2011). The alternation with ho ‘be’ has an intransitivizing effect. In (1) and (2), a change in the light verb results in the presence or absence of the agent argument. The nominal chorii is the same, but the LVC in (1) requires only a Theme argument, whereas (2) needs an Agent and a Theme. (1) gehene chorii hue. jewels.M theft.F be.Perf.MPl ‘The jewels got stolen’ (2) Ram-ne gehene chorii kiye. Ram-Erg jewels.M.Pl theft.F do.Perf.M.Pl ‘Ram stole the jewels ’ In English, a similar alternation structure may be found with light verbs in bring to light vs. come to light (Claridge, 2000). Here, two lig"
W14-5816,ahmed-etal-2012-reference,0,0.0669763,"ents for the light verb construction is instead represented in the nominal’s tree. The light verb can only choose 3 Based on the comments of the reviewers we are now considering a revision of the noun-centric analysis in this paper. It may seem that a verb-centric analysis may be more appropriate for Hindi LVCs. However, due to lack of space, we do not explore the second option fully in this paper and leave it to future work. 131 the semantic property of the nominal it may combine with (e.g., the light verb ho may combine only with nominals that have no agentive arguments). Other analyses e.g Ahmed et al. (2012) represent the light verb kar ‘do’ with arguments of its own. We discuss this in Section 5. Our work follows Han and Rambow (2000)’s representation of Sino-Korean LVCs. This work has also proposed separate trees for the nominal and light verb. The elementary tree of the nominal is an an initial tree, and as it is considered the true predicate, it also chooses a syntactic structure that will realize all its arguments. The light verb on the other hand is represented as an auxiliary tree, therefore it is an adjunct to the nominal’s basic structure. However, as it is a predicate, it is also a spec"
W14-5816,W12-4619,1,0.84393,"two approaches need to be explored more thoroughly within the TAG framework and we leave this to future work. While this work has examined one class of nominals that occur as part of light verb constructions, it does not complete the analysis of light verb constructions in Hindi. The behaviour of other nominal classes remains to be explored. There are also nominals that occur with light verbs other than kar ‘do’ and ho ‘be’. Finally, while the work presented here is mainly theoretical, it is in keeping with recent proposals for extracting a Hindi TAG grammar from a phrase structure treebank (Bhatt et al., 2012; Mannem et al., 2009). The algorithm in Bhatt et al. (2012) relies on the annotated Hindi Dependency Treebank and proposes a rule extraction system for elementary trees. Therefore, the description of Hindi LVCs in TAG would be a useful addition to the implementation of a grammar extraction task. Acknowledgements The first author was supported by DAAD (Deutscher Akademischer Austausch Dienst) for a research stay at the University of Konstanz in 2013-14s. We are also thankful to Prof Miriam Butt for hosting the first author at the University of Konstanz. We would like to thank Bhuvana Narasimha"
W14-5816,W00-2013,1,0.639076,"comments of the reviewers we are now considering a revision of the noun-centric analysis in this paper. It may seem that a verb-centric analysis may be more appropriate for Hindi LVCs. However, due to lack of space, we do not explore the second option fully in this paper and leave it to future work. 131 the semantic property of the nominal it may combine with (e.g., the light verb ho may combine only with nominals that have no agentive arguments). Other analyses e.g Ahmed et al. (2012) represent the light verb kar ‘do’ with arguments of its own. We discuss this in Section 5. Our work follows Han and Rambow (2000)’s representation of Sino-Korean LVCs. This work has also proposed separate trees for the nominal and light verb. The elementary tree of the nominal is an an initial tree, and as it is considered the true predicate, it also chooses a syntactic structure that will realize all its arguments. The light verb on the other hand is represented as an auxiliary tree, therefore it is an adjunct to the nominal’s basic structure. However, as it is a predicate, it is also a special type of auxiliary tree viz., a predicative auxiliary tree (Abeill´e and Rambow, 2000). The second feature of this analysis, al"
W14-5816,C88-2147,0,0.857551,"Feature based Tree Adjoining Grammar Tree-Adjoining Grammar (TAG) is a formal tree-rewriting system that is used to describe the syntax of natural languages (Joshi and Schabes, 1997). The basic structure of a TAG grammar is an elementary tree, which is a fragment of a phrase structure tree labelled with both terminal and non-terminal nodes. The elementary trees are combined by the operations of substitution (where a terminal node is replaced with a new tree) or adjunction (where an internal node is split to add a new tree). The elementary trees in TAG can be enriched with feature structures (Vijay-Shanker and Joshi, 1988). These can capture linguistic descriptions in a more precise manner and also capture adjunction constraints. TAG with feature structures is also known as FTAG (Feature-structure based TAG). A TAG can also be lexicalized i.e., an elementary tree has a lexical item as one of its terminal nodes. Lexicalized TAG enhanced with feature structures is known as Lexicalized Feature-based Tree-Adjoining Grammar (LF-TAG). This has been used for developing computational grammars for English (XTAG-Group, 2001), French (Abeill´e and Candito, 2000) and Korean (Han et al., 2000). In our analysis, we will also"
W15-0704,D10-1100,1,0.869947,"ct about three characters (Elton, Mr. Knightley, and Mr. Weston). However, the author does not tell us about the characters’ cognitive states, and thus there is no social event between the characters. (3) [Elton]’s manners are superior to [Mr. Knightley]’s or [Mr. Weston]’s. NoEvent As these examples demonstrate, the definition of a social event is quite broad. While quoted speech (detected by Elson et al. (2010)) represents only a strict sub-set of interactions, social events may be linguistically expressed using other types of speech as well, such as reported speech. In our subsequent work (Agarwal and Rambow, 2010; Agarwal et al., 2013b; Agarwal et al., 2014), we leveraged and extended ideas from the relation extraction literature (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 37 2009) to build a tree kernel-based supervised system for automatically detecting and classifying social events. We used this system for extracting observation and interaction networks from novels. We will refer to it as SINNET throughout this paper. 5.3 Terminology Regarding Networks A network (or graph), G = (V, E), is a set of vertices (V ) and ed"
W15-0704,W10-1803,1,0.943907,", even when one goes beyond conversations to more general and different notions of interactions. Specifically, we extend the work of Elson et al. (2010) in four significant ways: (1) we extract interaction networks, a conceptual generalization of conversation networks; (2) we extract observation networks, a new type of network with directed links; (3) we consider unweighted networks in addition to weighted networks; and (4) we investigate the number and size of communities in the extracted networks. For extracting interaction and observation networks, we use our existing system called SINNET (Agarwal et al., 2010; Agarwal et al., 2013b; Agarwal et al., 2014). In addition to validating a richer set of hypotheses using SINNET, we present an evaluation of the system on the task of automatic social network extraction from literary texts. Our results show that SINNET is effective in extracting interaction networks from a genre quite different from the genre it was trained on, namely news articles. The paper is organized as follows. In section 2 we revisit the theories postulated by various literary theorists and critics. In Section 3, we present an expanded set of literary hypotheses. Section 4 presents th"
W15-0704,I13-1171,1,0.931103,"eyond conversations to more general and different notions of interactions. Specifically, we extend the work of Elson et al. (2010) in four significant ways: (1) we extract interaction networks, a conceptual generalization of conversation networks; (2) we extract observation networks, a new type of network with directed links; (3) we consider unweighted networks in addition to weighted networks; and (4) we investigate the number and size of communities in the extracted networks. For extracting interaction and observation networks, we use our existing system called SINNET (Agarwal et al., 2010; Agarwal et al., 2013b; Agarwal et al., 2014). In addition to validating a richer set of hypotheses using SINNET, we present an evaluation of the system on the task of automatic social network extraction from literary texts. Our results show that SINNET is effective in extracting interaction networks from a genre quite different from the genre it was trained on, namely news articles. The paper is organized as follows. In section 2 we revisit the theories postulated by various literary theorists and critics. In Section 3, we present an expanded set of literary hypotheses. Section 4 presents the methodology used by"
W15-0704,I13-2009,1,0.791611,"eyond conversations to more general and different notions of interactions. Specifically, we extend the work of Elson et al. (2010) in four significant ways: (1) we extract interaction networks, a conceptual generalization of conversation networks; (2) we extract observation networks, a new type of network with directed links; (3) we consider unweighted networks in addition to weighted networks; and (4) we investigate the number and size of communities in the extracted networks. For extracting interaction and observation networks, we use our existing system called SINNET (Agarwal et al., 2010; Agarwal et al., 2013b; Agarwal et al., 2014). In addition to validating a richer set of hypotheses using SINNET, we present an evaluation of the system on the task of automatic social network extraction from literary texts. Our results show that SINNET is effective in extracting interaction networks from a genre quite different from the genre it was trained on, namely news articles. The paper is organized as follows. In section 2 we revisit the theories postulated by various literary theorists and critics. In Section 3, we present an expanded set of literary hypotheses. Section 4 presents the methodology used by"
W15-0704,E14-1023,1,0.840746,"more general and different notions of interactions. Specifically, we extend the work of Elson et al. (2010) in four significant ways: (1) we extract interaction networks, a conceptual generalization of conversation networks; (2) we extract observation networks, a new type of network with directed links; (3) we consider unweighted networks in addition to weighted networks; and (4) we investigate the number and size of communities in the extracted networks. For extracting interaction and observation networks, we use our existing system called SINNET (Agarwal et al., 2010; Agarwal et al., 2013b; Agarwal et al., 2014). In addition to validating a richer set of hypotheses using SINNET, we present an evaluation of the system on the task of automatic social network extraction from literary texts. Our results show that SINNET is effective in extracting interaction networks from a genre quite different from the genre it was trained on, namely news articles. The paper is organized as follows. In section 2 we revisit the theories postulated by various literary theorists and critics. In Section 3, we present an expanded set of literary hypotheses. Section 4 presents the methodology used by Elson et al. (2010) for"
W15-0704,P10-1015,0,0.0719295,"Theories Using Automatic Social Network Extraction Prashant Arun Jayannavar Department of Computer Science Columbia University NY, USA pj2271@columbia.edu Apoorv Agarwal Department of Computer Science Columbia University NY, USA apoorv@cs.columbia.edu Melody Ju Columbia College Columbia University NY, USA mj2558@columbia.edu Owen Rambow Center for Computational Learning Systems Columbia University NY, USA rambow@ccls.columbia.edu Abstract In this paper, we investigate whether longstanding literary theories about nineteenthcentury British novels can be verified using computational techniques. Elson et al. (2010) previously introduced the task of computationally validating such theories, extracting conversational networks from literary texts. Revisiting their work, we conduct a closer reading of the theories themselves, present a revised and expanded set of hypotheses based on a divergent interpretation of the theories, and widen the scope of networks for validating this expanded set of hypotheses. 1 Introduction In his book Graphs, Maps, Trees: Abstract Models for Literary History, literary scholar Franco Moretti proposes a radical transformation in the study of literature (Moretti, 2005). Advocating"
W15-0704,P05-1053,0,0.00875496,"on]’s manners are superior to [Mr. Knightley]’s or [Mr. Weston]’s. NoEvent As these examples demonstrate, the definition of a social event is quite broad. While quoted speech (detected by Elson et al. (2010)) represents only a strict sub-set of interactions, social events may be linguistically expressed using other types of speech as well, such as reported speech. In our subsequent work (Agarwal and Rambow, 2010; Agarwal et al., 2013b; Agarwal et al., 2014), we leveraged and extended ideas from the relation extraction literature (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 37 2009) to build a tree kernel-based supervised system for automatically detecting and classifying social events. We used this system for extracting observation and interaction networks from novels. We will refer to it as SINNET throughout this paper. 5.3 Terminology Regarding Networks A network (or graph), G = (V, E), is a set of vertices (V ) and edges (E). The set of edges incident on vertex v is written Ev . In weighted networks, each edge between nodes u and v is associated with a weight, denoted by wu,v . In the networks we consider, weight repre"
W15-0704,P13-1129,0,0.0910748,"nt types of networks we consider in our study. 5.1 Conversation Network Elson et al. (2010) defined a conversation network as a network in which nodes are characters and links are conversations. The authors defined a conversation as a continuous span of narrative time in which a set of characters exchange dialogues. Since dialogues are denoted in text by quotation marks, Elson et al. (2010) used simple regular expressions for dialogue detection. However, associating dialogues with their speakers (a task known as quoted speech attribution) turned out to be non-trivial (Elson and McKeown, 2010; He et al., 2013). In a separate work, Elson and McKeown (2010) presented a feature-based, supervised machine learning system for performing quoted speech attribution. Using this system, Elson et al. (2010) successfully extracted conversation networks from the novels in their corpus. We refer to the system as EDM2010 throughout this paper. 5.2 Observation and Interaction Networks In our past work (Agarwal et al., 2010), we defined a social network as a network in which nodes are characters and links are social events. We defined two broad categories of social events: observations (OBS) and interactions (INR)."
W15-0704,D09-1143,0,0.0472989,"Missing"
W15-0704,P05-1052,0,0.0132397,"the characters. (3) [Elton]’s manners are superior to [Mr. Knightley]’s or [Mr. Weston]’s. NoEvent As these examples demonstrate, the definition of a social event is quite broad. While quoted speech (detected by Elson et al. (2010)) represents only a strict sub-set of interactions, social events may be linguistically expressed using other types of speech as well, such as reported speech. In our subsequent work (Agarwal and Rambow, 2010; Agarwal et al., 2013b; Agarwal et al., 2014), we leveraged and extended ideas from the relation extraction literature (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 37 2009) to build a tree kernel-based supervised system for automatically detecting and classifying social events. We used this system for extracting observation and interaction networks from novels. We will refer to it as SINNET throughout this paper. 5.3 Terminology Regarding Networks A network (or graph), G = (V, E), is a set of vertices (V ) and edges (E). The set of edges incident on vertex v is written Ev . In weighted networks, each edge between nodes u and v is associated with a weight, denoted by wu,v . In the networks we"
W15-1304,W09-3012,1,0.804462,"(SW) level of commitment to a given proposition, which could be their own or a reported proposition. Modeling this type of knowledge explicitly is useful in determining an SWs cognitive state, also referred to as person’s private state (Wiebe et al., 2005). Wiebe et al. (2005) use the definition of (Quirk et al., 1985), who defines a private state to be an “internal (state) Initial work addressed the task of automatically identifying LCB of the SW. Approaches to date have relied on supervised models dependent on manually annotated data. There are two standard annotated corpora, the LU corpus (Diab et al., 2009) and FactBank (Saur´ı and Pustejovsky, 2009). Though in effect aiming for the same objective, both corpora use different terminology, different annotation standards, and they cover different genres. Previous studies performed on these corpora were conducted independently. In this work, we explore both corpora systematically and investigate their respective proposed tag sets. We experiment with multiple machine learning algorithms, varying the tag sets as we go along. Our goal is to build an automatic LCB tagger that is robust in a multi-genre context. Eventually we aim to adapt this tagger to"
W15-1304,P14-5010,0,0.0128785,"Missing"
W15-1304,W09-1501,0,0.0661361,"Missing"
W15-1304,C10-2117,1,0.948009,"s from the LU corpus in two major respects (other than the granularity in which they capture annotations): 1) FactBank is roughly four times the size of the LU corpus, and 2) FactBank is more homogeneous in terms of genre than the LU corpus as it consists primarily of newswire. In this paper, we unify the factuality annotations in Factback and the level of committed belief annotations present in the LU corpus to a 4-way committed-belief distinction.2 2 For an additional discussion of the relation between factuality and belief, see (Prabhakaran et al., 2015) 3 Approach Following previous work (Prabhakaran et al., 2010), we adopt a supervised approach to the LCB problem. We experiment with the two available manually annotated corpora, the LU and FB corpora. Going beyond previous approaches to the problem reported in the literature, our goal is to create a robust LCB system while gaining a deeper understanding of the phenomenon of LCB as an expressed modality by systematically teasing apart the different factors that affect performance. 3.1 Annotation Transformations The NCB category of the LU tagging scheme captures two different notions: that of uncertainty of the speaker/writer and that of belief being att"
W15-1304,S15-1009,1,0.70443,"o model LCB. From a computational perspective, FactBank differs from the LU corpus in two major respects (other than the granularity in which they capture annotations): 1) FactBank is roughly four times the size of the LU corpus, and 2) FactBank is more homogeneous in terms of genre than the LU corpus as it consists primarily of newswire. In this paper, we unify the factuality annotations in Factback and the level of committed belief annotations present in the LU corpus to a 4-way committed-belief distinction.2 2 For an additional discussion of the relation between factuality and belief, see (Prabhakaran et al., 2015) 3 Approach Following previous work (Prabhakaran et al., 2010), we adopt a supervised approach to the LCB problem. We experiment with the two available manually annotated corpora, the LU and FB corpora. Going beyond previous approaches to the problem reported in the literature, our goal is to create a robust LCB system while gaining a deeper understanding of the phenomenon of LCB as an expressed modality by systematically teasing apart the different factors that affect performance. 3.1 Annotation Transformations The NCB category of the LU tagging scheme captures two different notions: that of"
W15-1304,bethard-etal-2014-cleartk,0,\N,Missing
W15-3206,P06-1086,1,0.811827,"ctable) parts of the lexicon. 1. Introduction Arabic is a Central Semitic language, closely related to Aramaic, Hebrew, Ugaritic and Phoenician. It is spoken by 420 million speakers (native and non-native) in the Arab World. Arabic also is a liturgical language of 1.6 billion Muslims around the world. Current natural language processing (NLP) tools work well with MSA because they were designed specifically for the processing of MSA, and because of the abundance of MSA resources. Applying the NLP tools designed for MSA directly to DA yields significantly lower performance (Chiang et al., 2006; Habash and Rambow, 2006; Benajiba et al., 2010; Habash et al., 2012). This makes it imperative to direct research to building resources and tools for DA processing. Modern Standard Arabic (MSA) is the official Arabic language. It is the educational language and official language used in news and official communication across the Arabic-speaking world. When Arabs communicate spontaneously in informal settings, they use dialectal Arabic (DA). There are divisions of many dialects of the Arabic language that occur between the spoken languages of different regions. Some varieties of Arabic in North Africa, for example, a"
W15-3206,habash-etal-2012-conventional,1,0.94538,"abic is a Central Semitic language, closely related to Aramaic, Hebrew, Ugaritic and Phoenician. It is spoken by 420 million speakers (native and non-native) in the Arab World. Arabic also is a liturgical language of 1.6 billion Muslims around the world. Current natural language processing (NLP) tools work well with MSA because they were designed specifically for the processing of MSA, and because of the abundance of MSA resources. Applying the NLP tools designed for MSA directly to DA yields significantly lower performance (Chiang et al., 2006; Habash and Rambow, 2006; Benajiba et al., 2010; Habash et al., 2012). This makes it imperative to direct research to building resources and tools for DA processing. Modern Standard Arabic (MSA) is the official Arabic language. It is the educational language and official language used in news and official communication across the Arabic-speaking world. When Arabs communicate spontaneously in informal settings, they use dialectal Arabic (DA). There are divisions of many dialects of the Arabic language that occur between the spoken languages of different regions. Some varieties of Arabic in North Africa, for example, are incom1 When Arabic speakers of different d"
W15-3206,W12-2301,0,0.653038,"abic is a Central Semitic language, closely related to Aramaic, Hebrew, Ugaritic and Phoenician. It is spoken by 420 million speakers (native and non-native) in the Arab World. Arabic also is a liturgical language of 1.6 billion Muslims around the world. Current natural language processing (NLP) tools work well with MSA because they were designed specifically for the processing of MSA, and because of the abundance of MSA resources. Applying the NLP tools designed for MSA directly to DA yields significantly lower performance (Chiang et al., 2006; Habash and Rambow, 2006; Benajiba et al., 2010; Habash et al., 2012). This makes it imperative to direct research to building resources and tools for DA processing. Modern Standard Arabic (MSA) is the official Arabic language. It is the educational language and official language used in news and official communication across the Arabic-speaking world. When Arabs communicate spontaneously in informal settings, they use dialectal Arabic (DA). There are divisions of many dialects of the Arabic language that occur between the spoken languages of different regions. Some varieties of Arabic in North Africa, for example, are incom1 When Arabic speakers of different d"
W15-3206,N13-1044,1,0.877986,"esigned and built DIWAN as a desktop application which can work locally (offline) or online. As an annotation tool, we have designed DIWAN with two types of users: administrators and annotators. The administrator’s responsibility is to create the DIWAN database, specify its settings, and track the annotator’s work. The third resource is MADAMIRA (Pasha et al. 2014). MADAMIRA is a system for morphological analysis and disambiguation of Arabic that combines some of the best aspects of two previously commonly used systems for Arabic processing, MADA (Habash and Rambow, 2005; Habash et al., 2009; Habash et al., 2013) and AMIRA (Diab et al., 2007). MADAMIRA improves upon the two systems with a more streamlined Java implementation that is more robust, portable, extensible, and is faster than its ancestors by more than an order of magnitude. Contrary to SAMA and CALIMA-EGY, which provide all morphological analyses for a word regardless of context, MADAMIRA chooses a single analysis given the context of the word in a sentence. For example, in the sentence ﻣﺎﺷﻲ ﻛﺪﻩه ؟ mA$i kdh? `Is that OK?’, the interjection meaning will be chosen. The administrator has several roles: 1. She can create, edit and delete tabl"
W15-3206,W14-3603,0,0.108808,"morphological analyzer developed in conjunction with the Egyptian Arabic Treebank is in fact, the same CALIMAEgyptian system we use. In contrast to DIWAN, there is no attempt at incorporating resources from other dialects, which is also due to the fact that the Egyptian Treebank was a pioneer in the area of resources for dialectal Arabic. Furthermore, the LDC interface does not support annotation of functional number and gender, and concentrates on morpheme-based annotation (which DIWAN also supports, following the LDC approach). DIWAN has been used to annotate Levantine (Palestinian) Arabic (Jarrar et al. 2014). The annotators for Levantine quickly became proficient with using the tool after annotating about 100 words. The Palestinian corpus includes 45,000 annotated words (tokens). We are currently using DIWAN to annotate Yemeni (Sana’ai) Arabic. The Yemeni corpus contains 32,325 words (tokens), and the annotator for Yemeni is the first author of the present paper. Finally, we have embarked on a small project for Moroccan Arabic. We have collected 64,171 words of Moroccan for annotation. In separate publications in the future, we will report on the Yemeni and Moroccan annotation efforts. We will al"
W15-3206,maamouri-etal-2014-developing,0,0.345806,") in order to lighten the annotator burden. It uses a conventionalized spelling for Arabic dialects which is maintained in parallel with the naturally occurring spontaneous orthography. And it generates a file format which preserves the linear order of the input text, so that it can be used both for deriving morphological analyzers, and for training morphological taggers. 6. Related Work There are two related interfaces that have been used for annotating dialectal Arabic that we are familiar with. The annotation tool used at the Linguistic Data Consortium for annotating the Egyptian Treebank (Maamouri et al. 2014) is based on previous interfaces used at the LDC for treebanking, notably for MSA. The approach towards morphological annotation used at the LDC is a bootstrapping approach, which aims at developing an annotated corpus in conjunction with a morphological analyzer. The morphological analyzer developed in conjunction with the Egyptian Arabic Treebank is in fact, the same CALIMAEgyptian system we use. In contrast to DIWAN, there is no attempt at incorporating resources from other dialects, which is also due to the fact that the Egyptian Treebank was a pioneer in the area of resources for dialecta"
W15-3206,pasha-etal-2014-madamira,1,0.877123,"Missing"
W15-3206,P05-1071,1,\N,Missing
W16-2011,E12-1066,0,0.0316469,"Missing"
W16-2011,D11-1057,0,0.0579233,"Missing"
W16-2011,D13-1105,1,0.808386,"some additional morphological annotations (Yarowsky and Wicentowski, 2000; Snyder and Barzilay, 2008; Cucerzan and Yarowsky, 2002). Closer to the other end, we find work that focuses on defining morphological models with limited lexicons that are then extended using raw text (Clément et al., 2004; Forsberg et al., 2006). The setting of the shared task on morphological reinflection (Cotterell et al., 2016), which provides a rich partly annotated training data set, encourages methods that are supervised. Our shared task submission builds on our previously published work on paradigm completion (Eskander et al., 2013), which falls somewhere in the middle of the continuum outlined above. Our approach learns complete morphological models using rich morphological annotations. The match of the requirements for our approach and those for the shared task was far from perfect, but it was interesting to participate since we have always wanted to explore ways to reduce some of the expected input annotations – in particular word segmentations, which are not provided in the shared task. We present a high-level description and error analysis of the Columbia-NYUAD system for morphological reinflection, which builds on"
W16-2011,P06-1086,1,0.674397,"Missing"
W16-2011,J11-2002,0,0.0408374,"Missing"
W16-2011,P14-1127,1,0.782293,"Missing"
W16-2011,P08-1084,0,0.0701975,"Missing"
W16-2011,P00-1027,0,0.258465,"Missing"
W16-2011,clement-etal-2004-morphology,0,0.120772,"Missing"
W16-2011,W16-2002,0,0.0627876,"Missing"
W16-2011,W02-2006,0,0.195647,"Missing"
W16-3309,J99-2004,1,\N,Missing
W16-3311,braune-etal-2014-mapping,1,0.850359,"ates back to the 70s (Nagl, 1979; Drewes et al., 1997), their use in Natural Language Processing is more recent. Fischer (2003) use string generating HRG to model discontinuous constituents in German. Jones et al. (2012) introduce SHRG and demonstrate an application to construct intermediate semantic representations in machine translation. Peng et al. (2015) automatically extract SHRG rules from corpora annotated with graph based 109 meaning representations (Abstract Meaning Representation), using Markov Chain Monte Carlo techniques. They report competitive results on string-to-graph parsing. Braune et al. (2014) empirically compare SHRG to cascades of tree transducers as devices to translate English strings into reentrant semantic graphs. In agreement with the result we show more formally in this paper, they observe that, to generate graphs that contain a larger number of long-distance dependencies, a larger grammar with more nonterminals is needed, because the derivations of the grammar are limited to string CFG derivations. Synchronous context free string-graph grammars have also been studied in the framework of Interpreted Regular Tree Grammar (Koller and Kuhlmann, 2011) using S-Graph algebras (Ko"
W16-3311,P13-1091,1,0.888381,"ecause the derivations of the grammar are limited to string CFG derivations. Synchronous context free string-graph grammars have also been studied in the framework of Interpreted Regular Tree Grammar (Koller and Kuhlmann, 2011) using S-Graph algebras (Koller, 2015). In the TAG community, HRGs have been discussed by Pitsch (2000), who shows a construction to convert TAGs into HRGs. Finally, Joshi and Rambow (2003) discuss a version of TAG in which the derived trees are dependency trees, similar to the SHRG approach we present here. To use string-generating HRG in practice we need a HRG parser. Chiang et al. (2013) present an efficient graph parsing algorithm. However, their implementation assumes that graph fragments are connected, which is not true for the grammar in section 3. On the other hand, since string-generation HRGs are similar to LCFRS, any LCFRS parser could be used. The relationship between the two parsing problems merits further investigation. Seifert and Fischer (2004) describe a parsing algorithm specificaly for string-generating HRGs. Formal properties of dependency structures generated by lexicalized formalisms have been studied in detail by Kuhlmann (2010). He proposes measures for d"
W16-3311,C12-1083,1,0.921368,"semantic parsing. 1 Introduction Hyperedge Replacement Grammars (HRG) are a type of context free graph grammar. Their derived objects are hypergraphs instead of strings. A synchronous extension, Synchronous Hyperedge Replacement Grammars (SHRG) can be used to translate between strings and graphs. To construct a graph for a sentence, one simply parses the input using the string side of the grammar and then interprets the derivations with the graph side to assemble a derived graph. SHRG has recently drawn attention in Natural Language Processing as a tool for semantic construction. For example, Jones et al. (2012) propose to use SHRG for semantics based machine translation, and Peng et al. (2015) describe an approach to learning SHRG rules that translate sentences into Abstract Meaning Representation (Banarescu et al., 2013). Not much work has been done, however, on understanding the limits of syntactic and semantic structures that can be modeled using HRG and SHRG. In this paper, we examine syntactic dependency structures generated by these formalisms, specifially whether they can create correct dependency trees for non-projective phenomena. We focus on non-projectivity caused by copy language like co"
W16-3311,W11-2902,0,0.152326,"uction depends on the nonterminal label and type of the replaced edge only. We can therefore represent derivations as trees, as for other context free formalisms. Context freeness also allows us to extend the formalism to a synchronous formalism, for example to are used in the literature. We use the word rank to refer to the maximum number of nonterminals in a rule right hand side. 105 translate strings into trees, as we do in section 4. We can view the resulting string and graph languages as two interpretations of the same set of possible derivation trees described by a regular tree grammar (Koller and Kuhlmann, 2011). 3 HRG Derivations as Dependency Structures We first discuss the case in which HRG is used to derive a sentence and examine the dependency structure induced by the derivation tree. Hyperedge Replacement Grammars can derive string languages as path graphs in which edges are labeled with tokens. For example, consider the path graph for the sentence in Figure 1. Wim Jan Marie dekinderen zag helpen leren zwemmen Engelfriet and Heyker (1991) show that the string languages generated by HRG in this way are equivalent to the output languages of Deterministic Tree Walking Transducers (DTWT). Weir (199"
W16-3311,W15-0127,0,0.0118999,"4) empirically compare SHRG to cascades of tree transducers as devices to translate English strings into reentrant semantic graphs. In agreement with the result we show more formally in this paper, they observe that, to generate graphs that contain a larger number of long-distance dependencies, a larger grammar with more nonterminals is needed, because the derivations of the grammar are limited to string CFG derivations. Synchronous context free string-graph grammars have also been studied in the framework of Interpreted Regular Tree Grammar (Koller and Kuhlmann, 2011) using S-Graph algebras (Koller, 2015). In the TAG community, HRGs have been discussed by Pitsch (2000), who shows a construction to convert TAGs into HRGs. Finally, Joshi and Rambow (2003) discuss a version of TAG in which the derived trees are dependency trees, similar to the SHRG approach we present here. To use string-generating HRG in practice we need a HRG parser. Chiang et al. (2013) present an efficient graph parsing algorithm. However, their implementation assumes that graph fragments are connected, which is not true for the grammar in section 3. On the other hand, since string-generation HRGs are similar to LCFRS, any LC"
W16-3311,K15-1004,0,0.361993,"ontext free graph grammar. Their derived objects are hypergraphs instead of strings. A synchronous extension, Synchronous Hyperedge Replacement Grammars (SHRG) can be used to translate between strings and graphs. To construct a graph for a sentence, one simply parses the input using the string side of the grammar and then interprets the derivations with the graph side to assemble a derived graph. SHRG has recently drawn attention in Natural Language Processing as a tool for semantic construction. For example, Jones et al. (2012) propose to use SHRG for semantics based machine translation, and Peng et al. (2015) describe an approach to learning SHRG rules that translate sentences into Abstract Meaning Representation (Banarescu et al., 2013). Not much work has been done, however, on understanding the limits of syntactic and semantic structures that can be modeled using HRG and SHRG. In this paper, we examine syntactic dependency structures generated by these formalisms, specifially whether they can create correct dependency trees for non-projective phenomena. We focus on non-projectivity caused by copy language like constructions, specifically cross-serial dependencies in Dutch. Figure 1 shows a (clas"
W16-3311,P92-1018,0,0.728565,"tures generated by these formalisms, specifially whether they can create correct dependency trees for non-projective phenomena. We focus on non-projectivity caused by copy language like constructions, specifically cross-serial dependencies in Dutch. Figure 1 shows a (classical) example sentence containing such dependencies and a dependency graph. This paper looks at dependency structures from two perspectives. We first review HRGs that derive string languages as path graphs. The set of these languages is known to be the same as the languages generated by linear context free rewriting systems (Weir, 1992). We consider HRG grammars of this type that are lexicalized (each rule contains exactly one terminal edge), so we can view their derivation trees as dependency structures. We provide an example string-generating HRG that can analyze the sentence in Figure 1 with the correct dependency structure and can generate strings with an unlimited number of crossing dependencies of the same type. Under the second perspective, we view the derived graphs of synchronous string-to-HRG grammars as dependency structures. These grammars can generate labeled dependency graphs in a more flexible way, including l"
W16-3311,W13-2322,0,\N,Missing
W16-5003,W13-3520,0,0.0119331,"pressed as tags on words, we can define the task as a word-tagging task, as was also done previously for English. 3 Features and Experimental Setup 3.1 Features Used in Both Languages In our analysis we used some common features for both languages. These features are the following: • Word: One-hot encoding representation. • Part-of-Speech (POS): One-hot encoding representation (using different tagsets for the two languages of course). The use of POS is motivated by the need to find the syntactic heads of propositions, which are typically verbs. • 64 dimension word embedding: We used Polyglot (Al-Rfou et al., 2013) to get the word embedding for the two languages. For word segmentation in Chinese and POS-tagging in Chinese and Spanish, we used the Stanford tools (Manning et al., 2014). Additionally, the process to obtain the features vector of a word is the same on Chinese and Spanish. We experimented with 3 configurations of context windows to compute above features; they differ in where in the 5-word context window the target word is found. Let w0 be the target word and wi the word in i-th position relative to w0 . • [-2/+2]: [w−2 , w−1 , w0 , w1 , w2 ] • [-3/+1]: [w−3 , w−2 , w−1 , w0 , w1 ] • [-4/0]:"
W16-5003,W09-3012,1,0.914072,"olomer@columbia.edu Keyu Lai Columbia University kl2844@columbia.edu Owen Rambow CCLS Columbia University rambow@ccls.columbia.edu Abstract There has been extensive work on detecting the level of committed belief (also known as “factuality”) that an author is expressing towards the propositions in his or her utterances. Previous work on English has revealed that this can be done as a word tagging task. In this paper, we investigate the same task for Chinese and Spanish, two very different languages from English and from each other. 1 Introduction: Committed Belief The term “committed belief” (Diab et al., 2009) has been used to refer to the commitment of a writer towards the propositions she communicates: does she fully believe the proposition, does she believe the proposition may be true, is she reporting someone else’s belief without commenting on it, or is she reporting something other than a belief, namely a hope or desire? The notion is closely related to “factuality”, which Saur´ı and Pustejovsky (2009) define as the communicative intention of the writer to make the reader believe what her beliefs are. For a fuller discussion of the relation between the two notions and related notions such as"
W16-5003,P14-5010,0,0.00314708,"h Languages In our analysis we used some common features for both languages. These features are the following: • Word: One-hot encoding representation. • Part-of-Speech (POS): One-hot encoding representation (using different tagsets for the two languages of course). The use of POS is motivated by the need to find the syntactic heads of propositions, which are typically verbs. • 64 dimension word embedding: We used Polyglot (Al-Rfou et al., 2013) to get the word embedding for the two languages. For word segmentation in Chinese and POS-tagging in Chinese and Spanish, we used the Stanford tools (Manning et al., 2014). Additionally, the process to obtain the features vector of a word is the same on Chinese and Spanish. We experimented with 3 configurations of context windows to compute above features; they differ in where in the 5-word context window the target word is found. Let w0 be the target word and wi the word in i-th position relative to w0 . • [-2/+2]: [w−2 , w−1 , w0 , w1 , w2 ] • [-3/+1]: [w−3 , w−2 , w−1 , w0 , w1 ] • [-4/0]: [w−4 , w−3 , w−2 , w−1 , w0 ] Thus, the feature vector of the target word is formed by stacking the features’ representations of all words in the context window. 3.2 Featu"
W16-5003,padro-stanilovsky-2012-freeling,0,0.0230601,"Missing"
W16-5003,C10-2117,1,0.894639,"Missing"
W16-5003,S15-1009,1,0.82599,"e commitment of a writer towards the propositions she communicates: does she fully believe the proposition, does she believe the proposition may be true, is she reporting someone else’s belief without commenting on it, or is she reporting something other than a belief, namely a hope or desire? The notion is closely related to “factuality”, which Saur´ı and Pustejovsky (2009) define as the communicative intention of the writer to make the reader believe what her beliefs are. For a fuller discussion of the relation between the two notions and related notions such as factivity and modality, see (Prabhakaran et al., 2015). Determining the writer’s degree of commitment to the propositions in her text is crucial in understanding text, since if an NLP system fails to identify a proposition as merely wished as opposed to asserted, then clearly the NLP system is failing to understand what is being communicated. While work on English has been available (both for Committed Belief and for Factuality), no resources have been available for other languages. Recently, the Linguistic Data Consortium (LDC) has annotated small corpora for Chinese and Spanish. This paper summarizes initial systems trained on these corpora. 2"
W16-5003,W15-1304,1,0.69007,"ed in Table 1. All words which are not the syntactic heads of propositions get a default “O” tag (or “Other”). We only evaluate our performance on the four belief tags. This annotation scheme extends the annotation scheme proposed by Diab et al. (2009) by splitting its NCB tag into NCB and ROB. (In the scheme of (Diab et al., 2009), our NCB and ROB were combined because in both tags we cannot infer a committed belief of the writer; however, in terms of our knowledge of the writer’s cognitive state, they clearly represent very different categories.) For a fuller discussion of the tagsets, see (Werner et al., 2015). 1 LDC2015E99 for Chinese and LDC2016E40 for Spanish. 22 Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics, pages 22–30, Osaka, Japan, December 12 2016. Tag CB NCB ROB NA Meaning Committed Belief Non-committed Belief Reported Belief Not a Belief Example John will arrive tomorrow John may arrive tomorrow Mary says that John will arrive tomorrow I hope that John will arrive tomorrow Table 1: Explanation of the four belief tags used in the annotation, along with English examples 2.2 Chinese Data The Chinese corpus is sampled from Chinese Discussio"
W17-2408,P06-2053,0,0.768306,"e the lack of availability of task-related data, due to the privacy issues surrounding email. However, two large data sets are available. First, a large dataset of real emails, the Enron corpus, was made 1 http://www.cs.columbia.edu/˜sakhar/ resources.html 57 Proceedings of TextGraphs-11: the Workshop on Graph-based Methods for Natural Language Processing, ACL 2017, pages 57–65, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 Related Work we have in-house annotated a subset of the Avocado corpus. We use these two sets as well as the dataset distributed by Jabbari et al. (2006) (which we refer to as the “Sheffield set”) for the classification task in this paper. Since the Enron corpus has been made publicly available, many researchers have worked on the Enron corpus with different tasks. To our knowledge, the previous effort most closely related to this paper is that of Jabbari et al. (2006). They released a large set of manually annotated emails, in which they categorize a subset of more than 12,000 Enron emails into two main categories: “Business” and “Personal” and then into subcategories “Core Business” and “Close Personal”. These sub-categories represent the tw"
W17-2408,D14-1162,0,0.0802888,"eight reflects the number of emails that have been exchanged between the two ends and the direction; in the case of the email-centered network, the weights are always 1. Different features from these types of graphs can be extracted. We use the whole exchange network, including all labeled and unlabeled emails to build these graphs. We include features from both the sender and the recipients (either in the “to” or “cc” list ). In case of the email has multiple recipients, we Lexical and Local Features For the classification task, we use pre-trained GloVe embedding vectors as lexical features (Pennington et al., 2014). There are various word vector sets available online, each trained from different corpora and embedded into various dimension sizes. We use GloVe pre-trained word vector sets such that each email is represented by a vector of a fixed number of dimensions equal to the dimensionality of GloVe word vector set. We average all word vectors in the email using the pre-trained word vectors as follows: Pn fe ,v vi ej = Pi n j i i fej ,vi Here, fej ,vi is the frequency of the word corresponding to vector vi in email ej , vi is the word embedding vector in GloVe set. Both the body and subjects are inclu"
W17-2408,P12-2032,1,0.853604,"ith different tasks. To our knowledge, the previous effort most closely related to this paper is that of Jabbari et al. (2006). They released a large set of manually annotated emails, in which they categorize a subset of more than 12,000 Enron emails into two main categories: “Business” and “Personal” and then into subcategories “Core Business” and “Close Personal”. These sub-categories represent the two main categories respectively. The “Core Business” category has more than 4,500 emails while the “Close Personal” has more than 1,800. We compare our data to their data in detail in Section 3. Agarwal et al. (2012) released a gold standard of the Enron power hierarchy and predict the dominance relations between two employees using the degree centrality of the email exchange network. They released this gold standard of the Enron corpus with thread structure as a MongoDB database. Hardin et al. (2014) study the relation between six social network centrality measures and the hierarchal ranking of Enron employees. Mitra and Gilbert (2013) study gossip in the Enron corpus. They use the data set in Jabbari et al. (2006) to study the proportion of gossip in business and personal emails and find that gossip app"
W17-4202,W17-2408,1,0.875697,"Missing"
W17-4202,N13-1090,0,0.0112928,"Missing"
W17-6213,W16-3309,1,0.513387,"dependency trees from surface realization. In the next section, we briefly describe the parsing model that is the foundation of our experiments, along with our four methods for constructing supertag embeddings. Section 3 lays out our experimental set-up and Section 4 explains how we perform evaluation for supertag similarity and for parsing. Section 5 reports the results and discusses their implications. might be overcome; nearly half of the supertags present in the PTB WSJ Sections 1-22 appear only once, but they may be related to other supertags that occur more frequently. Previous work by Chung et al. (2016) has proposed a way of exploiting relationships among supertags in a transition-based parser, by adding a series of hand-coded linguistic features that characterize properties of the elementary trees in the grammar. Such features had a beneficial effect on parser performance when used in conjunction with lexical identity, supertag identity, and POS tag. In this paper, we demonstrate how the use of a neural network TAG parsing model, proposed by Kasai et al. (2017), facilitates the representation of similarity among supertags. The input to this parser is a sequence of 1-best supertags output by"
W17-6213,P95-1011,0,0.554307,"Missing"
W17-6213,N09-2047,1,0.853778,"ector representation of the supertag by multiplying the hidden state of the root node by a special weight matrix WROOT ∈ Md×d (R) and applying the activation function. Figure 1 gives an example of how the RNN is used to generate a vector representation for a transitive verb. As with the Atomic Embeddings, the weight matrices W ’s and E can be learned during training of the parser via backpropogation. 2.4 Feature-based Embeddings Our third representation of supertags and corresponding embedding derives from the handselected features associated with elementary trees in the grammar used in MICA (Bangalore et al., 2009). Each elementary tree is associated with a set of dimensions describing phrase structure, interpretation (e.g., subcategorization frame), and linguistic transformations (e.g., dative shift). The features include binary- and ternary-valued dimensions, whose values can be “yes”, “no”, or “NA,” as well as dimensions whose values are a part of speech tag or a list of part of speech tags. See Chung et al. (2016) for the complete list of features. To feed these feature vectors to a neural network, and to allow us to compare the vectors directly with our recursive embeddings, we convert each feature"
W17-6213,D17-1180,1,0.452969,"the PTB WSJ Sections 1-22 appear only once, but they may be related to other supertags that occur more frequently. Previous work by Chung et al. (2016) has proposed a way of exploiting relationships among supertags in a transition-based parser, by adding a series of hand-coded linguistic features that characterize properties of the elementary trees in the grammar. Such features had a beneficial effect on parser performance when used in conjunction with lexical identity, supertag identity, and POS tag. In this paper, we demonstrate how the use of a neural network TAG parsing model, proposed by Kasai et al. (2017), facilitates the representation of similarity among supertags. The input to this parser is a sequence of 1-best supertags output by a bidirectional LSTM supertagger. As the first step in computation, the parsing network maps each supertag into a vector via an embedding matrix. Given a set of supertag vectors, we can study the similarity relations among them using methods similar to those that have been applied to lexical vectors by Mikolov et al. (2013a,b). For example, we can consider analogies between elementary trees that correspond to an operation of detransitivization, by asking whether"
W17-6213,J99-2004,0,0.810937,"Missing"
W17-6213,N16-1026,0,0.197995,"Missing"
W17-6213,D14-1082,0,0.0555689,"supertag in a parser configuration to a vector in a lowdimensional space. The input to the subsequent feed-forward layer is the concatenation of the dense vectors associated with the relevant cells in the stack and buffer. In our experiments, we vary the representation of the supertag that is provided as input to the parser, and retrain. Our hypothesis is that using a more linguistically informed embedding function will produce linguistically interpretable vector representations and improve parsing accuracy. 2.2 Atomic Embedding The first type of embedding function we consider is the one from Chen and Manning (2014) (POS tags), Lewis et al. (2016); Xu (2016) (CCG supertags), and Kasai et al. (2017) (TAG supertags): here, each supertag in a parse configuration is represented as a one-hot column vector in R|V |, where |V |is the total number of supertags. That is, each supertag is represented as a vector in which all entries are 0 except for a single entry, which is 1, corresponding to the integer ID of the supertag. The supertags are then projected into a lowerdimensional space by multiplying the one-hot vectors with an embedding matrix L ∈ Md×|V |. Thus each supertag t is associated with a distinct vecto"
W17-6213,C16-1022,0,0.0867465,"ontext ∗ Equal Contribution. 122 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 122–131, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. the context of natural langauge generation. They utilize structural information of elementary trees through convolutional neural networks. Our recursive encoding is similar to their approach in that the embedding procedure respects tree structure of supertags. It should be noted, however, that our induction process runs in the opposite direction from that in McMahan and Stone (2016). In their application to natural language generation, the objective is surface realization from (unlabeled) dependency trees, whereas the problem of interest in this paper is derivation of dependency trees from surface realization. In the next section, we briefly describe the parsing model that is the foundation of our experiments, along with our four methods for constructing supertag embeddings. Section 3 lays out our experimental set-up and Section 4 explains how we perform evaluation for supertag similarity and for parsing. Section 5 reports the results and discusses their implications. mi"
W17-6213,N13-1090,0,0.281842,"lexical identity, supertag identity, and POS tag. In this paper, we demonstrate how the use of a neural network TAG parsing model, proposed by Kasai et al. (2017), facilitates the representation of similarity among supertags. The input to this parser is a sequence of 1-best supertags output by a bidirectional LSTM supertagger. As the first step in computation, the parsing network maps each supertag into a vector via an embedding matrix. Given a set of supertag vectors, we can study the similarity relations among them using methods similar to those that have been applied to lexical vectors by Mikolov et al. (2013a,b). For example, we can consider analogies between elementary trees that correspond to an operation of detransitivization, by asking whether an elementary tree representing a clause headed by a transitive verb (t27) stands in the same relationship to an elementary tree headed by an intransitive verb (t81) that a subject relative clause elementary tree headed by a transitive verb (t99) stands in to a subject relative headed by an intransitive verb (t109). By interpreting these elementary trees as vectors, we can express this analogy by t27 − t81 + t109 ≈ t99. As we will demonstrate below, thi"
W17-6213,D14-1162,0,0.106721,"ermine its action is the supertags in the relevant cells of the stack and buffer. For the current work, we make use of the bi-LSTM supertagOur discussion will compare four alternatives for constructing supertag embeddings. Three of these are trained in conjunction with parser, and differ only in the representation of the supertag input to the parser: atomic encodings of supertag identity, recursive encoding of the structure of the elementary tree, and the hand-coded linguistic features from Chung et al. (2016). The fourth derives embeddings via a GloVe-type model of distributional similarity (Pennington et al., 2014). Recent work by McMahan and Stone (2016) has proposed a method to embed TAG supertags in 123 First, the parser makes no use of the linguistic meaning of supertags. Each supertag is considered to be a distinct entity, and the only way for the parser to associate similar supertags is to learn associations in the process of optimizing the training objective. Second, supertag data is sparse: of the 4,724 supertags in the TAG-annotated version of the Penn Treebank, 2,165 occur only once (Chen, 2001; Kasai et al., 2017). Because each supertag is considered to be distinct in the input layer, the par"
W17-6213,Q14-1017,0,0.0308647,"rsive neural network. A recursive neural network (RNN)1 in a bottom-up fashion, by using a neural network layer to combine the hidden states of the node’s constituents (Goller and Kuchler, 1996; Socher et al., 2011). This recursive model has the advantage of both encoding linguistic information about supertags and also making efficient use of sparse data: even if a supertag appears infrequently in the corpus, the parser learns the parameters for embedding that supertag from encountering other, structurally similar supertags. We use a syntactically untied RNN, similar to the model described in Socher et al. (2014). First, for each leaf node j in an elementary tree, the hidden state hj is obtained by taking the j th column of an embedding matrix E ∈ Md×|L |(R), where |L |is the size of the vocabulary of labels used in TAG trees. Then, for each non-terminal node i, let C(i) denote the set of children of node i and let rel(i, j) denote the relation between node i and its child j, defined by the label of i and the left-toright position of j. For example, if i is a VP node and j is its leftmost child, then rel(i, j) is VP0 . The hidden state of node i is then   X hi = f  Wrel(i,j) hj  , ger discussed in"
W17-6213,C92-1034,0,0.474028,"Missing"
W17-6213,J07-3004,0,\N,Missing
W17-6214,D17-1180,1,0.705174,"Missing"
W17-6214,N16-1052,0,0.0158505,"s suggests that the PETE task may suffer from an unrepresentative development set, and that we need to improve upon formalism-independent parsing evaluation methods. 1 Introduction There has been a flurry of recent work, involving neural network architectures, on parsing that has improved performance across a variety of frameworks that make different assumptions about the target output for the parsing process: Dependency grammar (Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kuncoro et al., 2017; Dozat and Manning, 2017), Combinatory Categorial Grammar (CCG) (Xu et al., 2015; Ambati et al., 2016; Lewis et al., 2016), Tree Adjoining 132 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 132–141, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. construction process, see Yuret et al. (2013). bridge (Rimell and Clark, 2010) and SCHWA (Ng et al., 2010), both based on the Clark and Curran (2007) CCG parser, as well as a later system based on an HPSG-Minimal Recursion Semantics parser (Lien, 2014). We also conduct an error analysis and discuss limitations of TAG parsing in the context of this ta"
W17-6214,E17-1117,0,0.0214511,"Missing"
W17-6214,P16-1231,0,0.172892,"t TAG parser yields state-of-the-art results on the test set when using accuracy as a metric. This sensitivity to heuristics suggests that the PETE task may suffer from an unrepresentative development set, and that we need to improve upon formalism-independent parsing evaluation methods. 1 Introduction There has been a flurry of recent work, involving neural network architectures, on parsing that has improved performance across a variety of frameworks that make different assumptions about the target output for the parsing process: Dependency grammar (Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kuncoro et al., 2017; Dozat and Manning, 2017), Combinatory Categorial Grammar (CCG) (Xu et al., 2015; Ambati et al., 2016; Lewis et al., 2016), Tree Adjoining 132 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 132–141, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. construction process, see Yuret et al. (2013). bridge (Rimell and Clark, 2010) and SCHWA (Ng et al., 2010), both based on the Clark and Curran (2007) CCG parser, as well as a later system based on an HPSG-Minimal Recursion Seman"
W17-6214,N09-2047,1,0.894255,"are applied to DT to yield a modified derivation graph DT0 . 4. Return ‘YES’ if DH is a subderivation of DT0 . In the following subsection, we describe the properties of the TAG grammar we use for supertagging and parsing and the derivation trees which result from the parser. We then describe the set of structural transformations that are applied to the Text’s derivation tree, and define how we determine the subderivation property. 3.1 The TAG Grammar and Derivation Trees Our experiments make use of the TAG grammar extracted from the Penn Treebank by Chen (2001), and used by the MICA parser (Bangalore et al., 2009). This grammar makes use of the representations developed for TAG starting with the XTAG project (XTAG Research Group, 2001). Positions for a lexical anchor’s arguments are labeled with numbers that represent the argument’s deep syntactic role. Deep subjects are labeled 0, direct objects and objects of prepositions are labeled 1 and indirect objects are labeled 2. These numbers remain constant across elementary trees that differ with respect to grammatical operations such as passivization and dative shift. In the elementary tree associated with the verb played in the passive sentence The piano"
W17-6214,E14-3009,0,0.525138,"al., 2017; Dozat and Manning, 2017), Combinatory Categorial Grammar (CCG) (Xu et al., 2015; Ambati et al., 2016; Lewis et al., 2016), Tree Adjoining 132 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 132–141, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. construction process, see Yuret et al. (2013). bridge (Rimell and Clark, 2010) and SCHWA (Ng et al., 2010), both based on the Clark and Curran (2007) CCG parser, as well as a later system based on an HPSG-Minimal Recursion Semantics parser (Lien, 2014). We also conduct an error analysis and discuss limitations of TAG parsing in the context of this task. 2 3 Applying TAG Parsing to PETE Our TAG-based PETE system determines the entailment status of a T-H pair through the following four steps: 1. T and H are tokenized using the NLTK tokenizer (Bird et al., 2009). The PETE Task 2. T and H are supertagged and parsed, yielding derivation trees DT and DH . PETE (Yuret et al., 2013) is a restricted instance of the recognizing textual entailment (RTE) task, aimed at evaluating syntactic parsers. As in other RTE tasks, the task includes a set of Text"
W17-6214,W03-3005,0,0.0977893,"Missing"
W17-6214,P14-5010,0,0.00669269,"sal verb, only up is treated as a co-head, while with is treated as the head of a PP modifying the VP. One of the major differences between the Cambridge heuristics and our subderivation notion is the former’s exclusive focus on “core arguments”. This would allow misparses of this sort to be ignored, though it might pose problems for other cases. Finally, we note a surprising parsing error arising from an error in Part of Speech tagging that is input to our supertagger. For the Hypothesis Many bear resemblances to movie personalities, both our bi-LSTM supertagger and the Stanford PCFG Parser (Manning et al., 2014) tag many bear resemblances as an adjective, noun, and verb (as in red bear runs), instead of the correct sequence determiner, verb, and noun. In the pre-trained Parsey McParseface model (Andor et al., 2016), many bear resemblances is tagged as an adjective, noun, and noun, a sequence that is locally possible, but not compatible with the sentence as a whole. This is surprising, given that POS tagging is often regarded as a largely solved task. This sentence is short and contains no unbounded dependencies, nor are its lexical items unusual. 6.5.2 Differences between Subderivation Condition and"
W17-6214,P05-1022,0,0.311653,"Missing"
W17-6214,S10-1069,0,0.58361,"mptions about the target output for the parsing process: Dependency grammar (Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kuncoro et al., 2017; Dozat and Manning, 2017), Combinatory Categorial Grammar (CCG) (Xu et al., 2015; Ambati et al., 2016; Lewis et al., 2016), Tree Adjoining 132 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 132–141, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. construction process, see Yuret et al. (2013). bridge (Rimell and Clark, 2010) and SCHWA (Ng et al., 2010), both based on the Clark and Curran (2007) CCG parser, as well as a later system based on an HPSG-Minimal Recursion Semantics parser (Lien, 2014). We also conduct an error analysis and discuss limitations of TAG parsing in the context of this task. 2 3 Applying TAG Parsing to PETE Our TAG-based PETE system determines the entailment status of a T-H pair through the following four steps: 1. T and H are tokenized using the NLTK tokenizer (Bird et al., 2009). The PETE Task 2. T and H are supertagged and parsed, yielding derivation trees DT and DH . PETE (Yuret et al., 2013) is a restricted instan"
W17-6214,D14-1082,0,0.0446332,"however. Adding such heuristics to our best TAG parser yields state-of-the-art results on the test set when using accuracy as a metric. This sensitivity to heuristics suggests that the PETE task may suffer from an unrepresentative development set, and that we need to improve upon formalism-independent parsing evaluation methods. 1 Introduction There has been a flurry of recent work, involving neural network architectures, on parsing that has improved performance across a variety of frameworks that make different assumptions about the target output for the parsing process: Dependency grammar (Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kuncoro et al., 2017; Dozat and Manning, 2017), Combinatory Categorial Grammar (CCG) (Xu et al., 2015; Ambati et al., 2016; Lewis et al., 2016), Tree Adjoining 132 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 132–141, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. construction process, see Yuret et al. (2013). bridge (Rimell and Clark, 2010) and SCHWA (Ng et al., 2010), both based on the Clark and Curran (2007) CCG parser, as well as a later system b"
W17-6214,S10-1060,0,0.718561,"frameworks that make different assumptions about the target output for the parsing process: Dependency grammar (Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kuncoro et al., 2017; Dozat and Manning, 2017), Combinatory Categorial Grammar (CCG) (Xu et al., 2015; Ambati et al., 2016; Lewis et al., 2016), Tree Adjoining 132 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 132–141, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. construction process, see Yuret et al. (2013). bridge (Rimell and Clark, 2010) and SCHWA (Ng et al., 2010), both based on the Clark and Curran (2007) CCG parser, as well as a later system based on an HPSG-Minimal Recursion Semantics parser (Lien, 2014). We also conduct an error analysis and discuss limitations of TAG parsing in the context of this task. 2 3 Applying TAG Parsing to PETE Our TAG-based PETE system determines the entailment status of a T-H pair through the following four steps: 1. T and H are tokenized using the NLTK tokenizer (Bird et al., 2009). The PETE Task 2. T and H are supertagged and parsed, yielding derivation trees DT and DH . PETE (Yuret et al.,"
W17-6214,D09-1085,0,0.146276,"Missing"
W17-6214,J07-4004,0,0.0603789,"the parsing process: Dependency grammar (Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kuncoro et al., 2017; Dozat and Manning, 2017), Combinatory Categorial Grammar (CCG) (Xu et al., 2015; Ambati et al., 2016; Lewis et al., 2016), Tree Adjoining 132 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 132–141, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. construction process, see Yuret et al. (2013). bridge (Rimell and Clark, 2010) and SCHWA (Ng et al., 2010), both based on the Clark and Curran (2007) CCG parser, as well as a later system based on an HPSG-Minimal Recursion Semantics parser (Lien, 2014). We also conduct an error analysis and discuss limitations of TAG parsing in the context of this task. 2 3 Applying TAG Parsing to PETE Our TAG-based PETE system determines the entailment status of a T-H pair through the following four steps: 1. T and H are tokenized using the NLTK tokenizer (Bird et al., 2009). The PETE Task 2. T and H are supertagged and parsed, yielding derivation trees DT and DH . PETE (Yuret et al., 2013) is a restricted instance of the recognizing textual entailment (R"
W17-6214,P15-1033,0,0.0267303,"uristics to our best TAG parser yields state-of-the-art results on the test set when using accuracy as a metric. This sensitivity to heuristics suggests that the PETE task may suffer from an unrepresentative development set, and that we need to improve upon formalism-independent parsing evaluation methods. 1 Introduction There has been a flurry of recent work, involving neural network architectures, on parsing that has improved performance across a variety of frameworks that make different assumptions about the target output for the parsing process: Dependency grammar (Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kuncoro et al., 2017; Dozat and Manning, 2017), Combinatory Categorial Grammar (CCG) (Xu et al., 2015; Ambati et al., 2016; Lewis et al., 2016), Tree Adjoining 132 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 132–141, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. construction process, see Yuret et al. (2013). bridge (Rimell and Clark, 2010) and SCHWA (Ng et al., 2010), both based on the Clark and Curran (2007) CCG parser, as well as a later system based on an HPSG-Min"
W17-6214,P15-2041,0,0.0722515,"vity to heuristics suggests that the PETE task may suffer from an unrepresentative development set, and that we need to improve upon formalism-independent parsing evaluation methods. 1 Introduction There has been a flurry of recent work, involving neural network architectures, on parsing that has improved performance across a variety of frameworks that make different assumptions about the target output for the parsing process: Dependency grammar (Chen and Manning, 2014; Dyer et al., 2015; Andor et al., 2016; Kuncoro et al., 2017; Dozat and Manning, 2017), Combinatory Categorial Grammar (CCG) (Xu et al., 2015; Ambati et al., 2016; Lewis et al., 2016), Tree Adjoining 132 Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13), pages 132–141, c 2017 Association for Computational Linguistics Umeå, Sweden, September 4–6, 2017. construction process, see Yuret et al. (2013). bridge (Rimell and Clark, 2010) and SCHWA (Ng et al., 2010), both based on the Clark and Curran (2007) CCG parser, as well as a later system based on an HPSG-Minimal Recursion Semantics parser (Lien, 2014). We also conduct an error analysis and discuss limitations of TAG parsing in t"
W17-6214,N16-1024,0,\N,Missing
W18-5808,C10-1092,0,0.0347838,"he six languages. We propose two language-independent classifiers that enable the selection of the optimal or nearly-optimal configuration for the morphological segmentation of unseen languages. 1 Introduction As natural language processing becomes more interested in many languages, including lowresource languages, unsupervised morphological segmentation remains an important area of study. For most of the languages of the world, we do not have morphologically annotated resources. However, many human language technologies profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). In this paper, we build on previous work on unsupervised morphological segmentation using Adaptor Grammars (AGs) (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016), a type of nonparametric Bayesian models that generalize probabilistic context-free grammars (PCFGs) (Johnson et al., 2007), where the PCFG is typically a morphological grammar that spec2 Problem Definition and Dataset Adaptor Grammars (AGs) have been used successfully for unsupervised morphological seg78 Proceedings of the 15th SIGMORPHON Worksh"
W18-5808,N09-1024,0,0.182091,"PrStSu+SM configuration is 81 Grammar Morph+SM Simple Simple+SM PrStSu PrStSu+SM PrStSu+Co+SM PrStSu2a+SM PrStSu2b+SM PrStSu2b+Co+SM Standard 0.647 0.651 0.680 0.642 0.701 0.648 0.676 0.682 0.532 Cascaded 0.642 0.593 0.631 0.646 0.692 0.628 0.682 0.688 0.532 proach to select the best language-independent model for each language. In addition to the use of AGs, several models have been successfully used for unsupervised morphological segmentation such as generative probabilistic models (utilized by Morfessor (Creutz and Lagus, 2007)), and log-linear models using contextual and global features (Poon et al., 2009). Narasimhan et al. (2015) use a discriminative model for unsupervised morphological segmentation that integrates orthographic and semantic properties of words. The model learns morphological chains, where a chain extends a base form available in the lexicon. Another recent notable work is introduced by Wang et al. (2016), who use neural networks for unsupervised segmentation, where they build LSTM (Hochreiter and Schmidhuber, 1997) architectures to learn word structures in order to predict morphological boundaries. Another variation of the this approach is presented by Yang et al. (2017), whe"
W18-5808,C16-1086,1,0.71873,"he research proposed by Eskander et al. (2016), who investigate a large space of parameters when using Adaptor Grammars related to (i) the underlying context-free grammar and (ii) the use of a “Cascaded” system in which one grammar chooses affixes to be seeded into another in order to simulate the situation where scholar-knowledge is available. Their results on a development set of 6 languages (English, German, Finish, Turkish, Estonian and Zulu) show that the best performing AG-based configuration (grammar and learning setup) differ from language to language. For processing unseen languages, Eskander et al. (2016) proposed the Language-Independent Morphological Segmenter (LIMS) based on the best-on-average performing configuration when running leaveone-out cross validation on the development languages. However, while LIMS works best on average and has been shown to outperform other stateof-the-art unsupervised morphological segmentation systems (Eskander et al., 2016), it is not the optimal configuration for any of the development languages except Zulu. Thus, in this paper we propose an approach to automatically select the optimal or nearly-optimal languageindependent configuration for the morphologica"
W18-5808,Q13-1021,0,0.67021,"language processing becomes more interested in many languages, including lowresource languages, unsupervised morphological segmentation remains an important area of study. For most of the languages of the world, we do not have morphologically annotated resources. However, many human language technologies profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). In this paper, we build on previous work on unsupervised morphological segmentation using Adaptor Grammars (AGs) (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016), a type of nonparametric Bayesian models that generalize probabilistic context-free grammars (PCFGs) (Johnson et al., 2007), where the PCFG is typically a morphological grammar that spec2 Problem Definition and Dataset Adaptor Grammars (AGs) have been used successfully for unsupervised morphological seg78 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 78–83 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https://doi.org/10.18653/v1/P17"
W18-5808,W08-0704,0,0.698623,"ion As natural language processing becomes more interested in many languages, including lowresource languages, unsupervised morphological segmentation remains an important area of study. For most of the languages of the world, we do not have morphologically annotated resources. However, many human language technologies profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). In this paper, we build on previous work on unsupervised morphological segmentation using Adaptor Grammars (AGs) (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016), a type of nonparametric Bayesian models that generalize probabilistic context-free grammars (PCFGs) (Johnson et al., 2007), where the PCFG is typically a morphological grammar that spec2 Problem Definition and Dataset Adaptor Grammars (AGs) have been used successfully for unsupervised morphological seg78 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 78–83 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https"
W18-5808,C10-1116,0,0.0629627,"Missing"
W18-5808,C10-1115,0,0.0807782,"Missing"
W18-5808,Q15-1012,0,0.051137,"u+SM vs. PrStSu2a+SM) classification tasks. We conduct the two classification tasks separately, and then we combine the outcome to obtain the best configuration. In the training phase, we perform leave-oneout cross validation on the six development languages. In each of the six folds of the cross validation, we choose one language in turn as the test language. We use the training and deComparison with existing unsupervised approaches. Table 7 compares the performance of the selected configurations of our system (Table 5) to three other systems; Morfessor (Creutz and Lagus, 2007), MorphoChain (Narasimhan et al., 2015) and LIMS (Eskander et al., 2016) (where the cascaded PrStSu+SM configuration is 81 Grammar Morph+SM Simple Simple+SM PrStSu PrStSu+SM PrStSu+Co+SM PrStSu2a+SM PrStSu2b+SM PrStSu2b+Co+SM Standard 0.647 0.651 0.680 0.642 0.701 0.648 0.676 0.682 0.532 Cascaded 0.642 0.593 0.631 0.646 0.692 0.628 0.682 0.688 0.532 proach to select the best language-independent model for each language. In addition to the use of AGs, several models have been successfully used for unsupervised morphological segmentation such as generative probabilistic models (utilized by Morfessor (Creutz and Lagus, 2007)), and log"
W18-5808,D14-1095,0,0.0470118,"that enable the selection of the optimal or nearly-optimal configuration for the morphological segmentation of unseen languages. 1 Introduction As natural language processing becomes more interested in many languages, including lowresource languages, unsupervised morphological segmentation remains an important area of study. For most of the languages of the world, we do not have morphologically annotated resources. However, many human language technologies profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). In this paper, we build on previous work on unsupervised morphological segmentation using Adaptor Grammars (AGs) (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016), a type of nonparametric Bayesian models that generalize probabilistic context-free grammars (PCFGs) (Johnson et al., 2007), where the PCFG is typically a morphological grammar that spec2 Problem Definition and Dataset Adaptor Grammars (AGs) have been used successfully for unsupervised morphological seg78 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, p"
W18-5808,P17-1078,0,0.0264509,"res (Poon et al., 2009). Narasimhan et al. (2015) use a discriminative model for unsupervised morphological segmentation that integrates orthographic and semantic properties of words. The model learns morphological chains, where a chain extends a base form available in the lexicon. Another recent notable work is introduced by Wang et al. (2016), who use neural networks for unsupervised segmentation, where they build LSTM (Hochreiter and Schmidhuber, 1997) architectures to learn word structures in order to predict morphological boundaries. Another variation of the this approach is presented by Yang et al. (2017), where they use partial-word information as character bigram embeddings and evaluate their work on Chinese. Table 6: Adaptor-grammar results (Emma F-scores) for the Standard and Cascaded setups for Arabic. Boldface indicates the best configuration and the choice of our system. Grammar Morfessor MorphoChain LIMS Ours Best English 0.805 0.746 0.809 0.821 0.826 German 0.740 0.625 0.777 0.790 0.790 Finnish 0.675 0.621 0.727 0.733 0.733 Turkish 0.551 0.551 0.591 0.647 0.647 Zulu 0.414 0.390 0.611 0.611 0.611 Estonian 0.779 0.679 0.805 0.828 0.847 Arabic 0.779 0.751 0.682 0.701 0.701 Avg. 0.678 0.6"
W19-4615,P11-2062,1,0.808984,"is a DIWAN file which includes the correct annotation for the entire corpus. In the last step, we automatically reformat the annotations into a format which is best suited for computational purposes; we perform a third round of error checking for format errors, which we fix automatically. Figure 1 shows these steps. • The morphemes of the word (prefixes, stem, suffixes) and their part-of-speech (POS). The stem is marked by the symbol # on either side. • The English gloss of the word. • Features indicating proclitics and enclitics. • Features indicating word POS, functional number and gender (Alkuhlani and Habash, 2011), and aspect. The annotation for one sentence in different dialects is shown in Table 2. This is not actually a sentence from our corpora, of course; we have chosen it to illustrate the annotation. Error Correction Linguistic annotation is carried out manually. In order to guarantee high levels of accuracy and precision, we performed extensive error checking and correction. After annotating the seven different corpora, the annotated words were compiled in the form of linguistic codes in either one file or separate files to be Morphological Features Annotated The DIWAN interface assists human a"
W19-4615,L16-1207,1,0.241992,"Arabic Treebank (Maamouri et al., 2014), Curras, the Pales1 The abbreviations we use intend to capture the country name and the city or region name when applicable. 2 http://www.semarch.uni-hd.de 137 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 137–147 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics tinian Arabic annotated corpus (Jarrar et al., 2014), the Gulf Arabic Annotated corpus (Khalifa et al., 2018), Syrian, Jordanian dialectal corpora (Bouamor et al., 2014; Harrat et al., 2014), a small effort on Sanaani and Moroccan (AlShargi et al., 2016) (which this paper builds on), and SUAR (Al-Twairesh et al., 2018), a morphologically annotated corpus for Najdi and Hijazi which is semiautomatically annotated using the MADAMIRA tool (Pasha et al., 2014) and subsequently manually checked. Additionally, Voss et al. (2014) present a corpus of Moroccan dialect which has been annotated for language variety (code switching). Several of these efforts have followed the approach of Curras (Jarrar et al., 2014), which consists of around 70,000 words of a balanced genre corpus. The corpus was manually annotated using the DIWAN tool (Alshargi and Rambo"
W19-4615,W15-3206,1,0.826677,"Missing"
W19-4615,bouamor-etal-2014-multidialectal,1,0.820756,"ntine Arabic Treebank (specifically Jordanian) (Maamouri et al., 2006), the Egyptian Arabic Treebank (Maamouri et al., 2014), Curras, the Pales1 The abbreviations we use intend to capture the country name and the city or region name when applicable. 2 http://www.semarch.uni-hd.de 137 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 137–147 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics tinian Arabic annotated corpus (Jarrar et al., 2014), the Gulf Arabic Annotated corpus (Khalifa et al., 2018), Syrian, Jordanian dialectal corpora (Bouamor et al., 2014; Harrat et al., 2014), a small effort on Sanaani and Moroccan (AlShargi et al., 2016) (which this paper builds on), and SUAR (Al-Twairesh et al., 2018), a morphologically annotated corpus for Najdi and Hijazi which is semiautomatically annotated using the MADAMIRA tool (Pasha et al., 2014) and subsequently manually checked. Additionally, Voss et al. (2014) present a corpus of Moroccan dialect which has been annotated for language variety (code switching). Several of these efforts have followed the approach of Curras (Jarrar et al., 2014), which consists of around 70,000 words of a balanced ge"
W19-4615,I13-1048,0,0.0212546,"nnotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). However, other notable approaches and efforts that do not use annotated corpora have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multidialectal dictionary Tharwa (Diab et al., 2014), or extending MSA analyzers and resources (Salloum and Habash, 2014; Harrat et al., 2014; Boujelbane et al., 2013). Dialectal Variations Differences among the dialects are found on all levels of linguistic description, i.e., phonology, morphology, syntax, and the lexicon. We summarize three phonological and three morphological salient examples in Table 1 for our dialects: the pronunciation of MSA /q/ written  q,3 MSA /Ã/ written h. j and MSA /k/ written  k; and the various forms of the future, progressive and possessive particles. From a lexical point of view, there are many words that have different meanings across dialects. For example, the word úæ AÓ mA$y /ma:Si/ is ‘no’ in YE.SN and MA.RB, ‘yes/ok"
W19-4615,E06-1047,1,0.654672,"ialects and MSA Arabic dialects share many commonalities with Classical Arabic and Modern Standard Arabic (MSA). All variants of Arabic are morphologically complex as they include rich inflectional and derivational morphology that is expressed in two ways: namely, via templates and affixes. Furthermore, they contain several classes of attachable clitics. However, the dialects as a class differ in consistent ways from MSA, and they differ amongst each other. In fact, the differences between MSA and Dialectal Arabic (DA) have often been compared to those between Latin and the Romance languages (Chiang et al., 2006). The principal morpho-syntactic difference between DA and MSA is the loss of productive case marking, and nunation (tanween) on nouns, and mood on imperfective verbs. Linguistic Studies There are many theoretical and descriptive linguistic studies for the dialects we work on: Yemeni dialects (Watson, 1993, 2002), Najdi (Ingham, 1994), Gulf Arabic dialect (Holes, 1990), Jordanian (Bani-Yasin and Owens, 1987), Moroccan (Harrell, 1962), Syrian (Cowell, 1964), and Iraqi (Erwin, 1963); not to mentions comparative studies across dialects and MSA (Holes, 2004; Brustad, 2000). We make extensive use o"
W19-4615,D13-1105,1,0.876469,"as (Jarrar et al., 2014), which consists of around 70,000 words of a balanced genre corpus. The corpus was manually annotated using the DIWAN tool (Alshargi and Rambow, 2015), which we also use. The annotation in Curras is done by first using a morphological tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. 3 Dialects: Linguistic Facts Other NLP Resources for Dialectal Arabic The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). However, other notable approaches and efforts that do not use annotated corpora have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multidialectal dictionary Tharwa (Diab et al., 2014), or extending MSA analyzers and resources (Salloum and Habash, 2014; Harrat et al., 2014; Boujelbane et al., 2013). Dialectal Variations Differences among the dialects are found on all levels of linguistic d"
W19-4615,W14-3603,1,0.962757,"he corpora used and some interesting facts specific to each dialect. Section 5 then presents our annotation methodology. We then briefly discuss morphological analyzers, and conclude. Introduction 2 As Arabic dialects (DA) become more widely written in social media, there is increased interest in the Arabic NLP community to have annotated corpora that will allow us to both study the dialects linguistically, and to create systems that can automatically process dialectal text. There have been important efforts to create relatively large corpora for Egyptian (Maamouri et al., 2014), Palestinian (Jarrar et al., 2014), and Emirati Arabic (Khalifa et al., 2018). While these resources are very helpful for single dialects, the problem is that there are many dialects, and in fact it is often unclear what to count as separate dialects (for example, the subdialects of Levantine). Therefore, we present a different approach in this paper: we annotate seven dialects, but with relatively smaller corpora (most around 30,000 words). Some of the dialects are closely related (Jordanian and Syrian), others are more distant (Moroccan). We use the same annotation methodology for all dialects: same guidelines, same processi"
W19-4615,C16-1326,1,0.865727,"Missing"
W19-4615,L16-1679,1,0.838055,"st around 30,000 words). Some of the dialects are closely related (Jordanian and Syrian), others are more distant (Moroccan). We use the same annotation methodology for all dialects: same guidelines, same processing steps, and same annotation file format. This makes our effort an Related Work Data Collections There have been several data collections centered on Arabic dialects, specifically spoken Arabic. A very useful resource is the Semitisches Tonarchiv at the University of Heidelberg in Germany.2 We have included two Yemeni transcriptions from this resource in our YE.TZ and YE.SN corpora. Khalifa et al. (2016) is a large collection of over 100M words of a number of Arabic dialect, although the majority is from the Gulf. Bouamor et al. (2018) created a large corpus with parallel data text from 25 Arab cities. Further data collections include (Al-Amri, 2000) which has not yet been digitized for use in NLP research. Annotated Corpora There are few annotated corpora for dialectal Arabic: the Levantine Arabic Treebank (specifically Jordanian) (Maamouri et al., 2006), the Egyptian Arabic Treebank (Maamouri et al., 2014), Curras, the Pales1 The abbreviations we use intend to capture the country name and t"
W19-4615,habash-etal-2012-conventional,1,0.903506,"gical tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. 3 Dialects: Linguistic Facts Other NLP Resources for Dialectal Arabic The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). However, other notable approaches and efforts that do not use annotated corpora have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multidialectal dictionary Tharwa (Diab et al., 2014), or extending MSA analyzers and resources (Salloum and Habash, 2014; Harrat et al., 2014; Boujelbane et al., 2013). Dialectal Variations Differences among the dialects are found on all levels of linguistic description, i.e., phonology, morphology, syntax, and the lexicon. We summarize three phonological and three morphological salient examples in Table 1 for our dialects: the pronunciation of MSA /q/ written  q,3 MSA /Ã/ written h. j and MSA /k/ writte"
W19-4615,L18-1607,1,0.947802,"specific to each dialect. Section 5 then presents our annotation methodology. We then briefly discuss morphological analyzers, and conclude. Introduction 2 As Arabic dialects (DA) become more widely written in social media, there is increased interest in the Arabic NLP community to have annotated corpora that will allow us to both study the dialects linguistically, and to create systems that can automatically process dialectal text. There have been important efforts to create relatively large corpora for Egyptian (Maamouri et al., 2014), Palestinian (Jarrar et al., 2014), and Emirati Arabic (Khalifa et al., 2018). While these resources are very helpful for single dialects, the problem is that there are many dialects, and in fact it is often unclear what to count as separate dialects (for example, the subdialects of Levantine). Therefore, we present a different approach in this paper: we annotate seven dialects, but with relatively smaller corpora (most around 30,000 words). Some of the dialects are closely related (Jordanian and Syrian), others are more distant (Moroccan). We use the same annotation methodology for all dialects: same guidelines, same processing steps, and same annotation file format."
W19-4615,W12-2301,1,0.938969,"gical tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. 3 Dialects: Linguistic Facts Other NLP Resources for Dialectal Arabic The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). However, other notable approaches and efforts that do not use annotated corpora have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multidialectal dictionary Tharwa (Diab et al., 2014), or extending MSA analyzers and resources (Salloum and Habash, 2014; Harrat et al., 2014; Boujelbane et al., 2013). Dialectal Variations Differences among the dialects are found on all levels of linguistic description, i.e., phonology, morphology, syntax, and the lexicon. We summarize three phonological and three morphological salient examples in Table 1 for our dialects: the pronunciation of MSA /q/ written  q,3 MSA /Ã/ written h. j and MSA /k/ writte"
W19-4615,maamouri-etal-2006-developing,1,0.692422,"v at the University of Heidelberg in Germany.2 We have included two Yemeni transcriptions from this resource in our YE.TZ and YE.SN corpora. Khalifa et al. (2016) is a large collection of over 100M words of a number of Arabic dialect, although the majority is from the Gulf. Bouamor et al. (2018) created a large corpus with parallel data text from 25 Arab cities. Further data collections include (Al-Amri, 2000) which has not yet been digitized for use in NLP research. Annotated Corpora There are few annotated corpora for dialectal Arabic: the Levantine Arabic Treebank (specifically Jordanian) (Maamouri et al., 2006), the Egyptian Arabic Treebank (Maamouri et al., 2014), Curras, the Pales1 The abbreviations we use intend to capture the country name and the city or region name when applicable. 2 http://www.semarch.uni-hd.de 137 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 137–147 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics tinian Arabic annotated corpus (Jarrar et al., 2014), the Gulf Arabic Annotated corpus (Khalifa et al., 2018), Syrian, Jordanian dialectal corpora (Bouamor et al., 2014; Harrat et al., 2014), a small effort on Sanaani"
W19-4615,N13-1044,1,0.845771,"), which consists of around 70,000 words of a balanced genre corpus. The corpus was manually annotated using the DIWAN tool (Alshargi and Rambow, 2015), which we also use. The annotation in Curras is done by first using a morphological tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. 3 Dialects: Linguistic Facts Other NLP Resources for Dialectal Arabic The effort to annotate corpora in context is a central step in developing morphological analyzers and taggers (Eskander et al., 2013; Habash et al., 2013). However, other notable approaches and efforts that do not use annotated corpora have focused on developing specific resources manually or semi-automatically, e.g., the Egyptian Arabic morphological analyzer (Habash et al., 2012b) which is built upon the Egyptian Colloquial Arabic Lexicon (Kilany et al., 2002), the multidialectal dictionary Tharwa (Diab et al., 2014), or extending MSA analyzers and resources (Salloum and Habash, 2014; Harrat et al., 2014; Boujelbane et al., 2013). Dialectal Variations Differences among the dialects are found on all levels of linguistic description, i.e., phon"
W19-4615,maamouri-etal-2014-developing,1,0.855497,"dialects in Section 4, summarizing the corpora used and some interesting facts specific to each dialect. Section 5 then presents our annotation methodology. We then briefly discuss morphological analyzers, and conclude. Introduction 2 As Arabic dialects (DA) become more widely written in social media, there is increased interest in the Arabic NLP community to have annotated corpora that will allow us to both study the dialects linguistically, and to create systems that can automatically process dialectal text. There have been important efforts to create relatively large corpora for Egyptian (Maamouri et al., 2014), Palestinian (Jarrar et al., 2014), and Emirati Arabic (Khalifa et al., 2018). While these resources are very helpful for single dialects, the problem is that there are many dialects, and in fact it is often unclear what to count as separate dialects (for example, the subdialects of Levantine). Therefore, we present a different approach in this paper: we annotate seven dialects, but with relatively smaller corpora (most around 30,000 words). Some of the dialects are closely related (Jordanian and Syrian), others are more distant (Moroccan). We use the same annotation methodology for all diale"
W19-4615,pasha-etal-2014-madamira,1,0.930495,"Missing"
W19-4615,voss-etal-2014-finding,0,0.0226381,"137–147 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics tinian Arabic annotated corpus (Jarrar et al., 2014), the Gulf Arabic Annotated corpus (Khalifa et al., 2018), Syrian, Jordanian dialectal corpora (Bouamor et al., 2014; Harrat et al., 2014), a small effort on Sanaani and Moroccan (AlShargi et al., 2016) (which this paper builds on), and SUAR (Al-Twairesh et al., 2018), a morphologically annotated corpus for Najdi and Hijazi which is semiautomatically annotated using the MADAMIRA tool (Pasha et al., 2014) and subsequently manually checked. Additionally, Voss et al. (2014) present a corpus of Moroccan dialect which has been annotated for language variety (code switching). Several of these efforts have followed the approach of Curras (Jarrar et al., 2014), which consists of around 70,000 words of a balanced genre corpus. The corpus was manually annotated using the DIWAN tool (Alshargi and Rambow, 2015), which we also use. The annotation in Curras is done by first using a morphological tagger for another Arabic dialect, namely MADAMIRA Egyptian (Pasha et al., 2014), to produce a base that was then corrected or accepted by a trained annotator. 3 Dialects: Linguist"
W90-0112,P88-1020,0,0.0384652,"hypothesis that any text planning task relies, explicitly or implicitly, on domainspecific text planning knowledge. This knowledge, ""domain communication knowledge"", is different from both domain knowledge and general knowledge about communication. The paper presents the text generation system Joyce, which represents such knowledge explicitly. 1. The text planner produces a list of propositions, which represents both the content and the structure of the intended text. Thus, the task of Joyce's text planner is similar in definition to TEXT's (McKeown, 1985), but different from that of Penman (Hovy, 1988), which expects the content selection task to have already been performed. Each proposition is expressed in a language-independent, conceptual framelike formalism. It encodes a minimal amount of information, but can be realized as an independent sentence if necessary. The text planner draws on domain communication knowledge expressed in a high-level schema language (see below). 2. The sentence planner takes the list of propositions and determines how to express them in natural language. This task includes choosing lexicalizations and a syntactic structure for each propositions, and assembling"
W90-0112,P89-1025,0,\N,Missing
W93-0227,P88-1020,0,0.0799085,"Missing"
W93-0227,J92-4007,0,0.0267958,"Missing"
W94-0320,J86-3001,0,0.0493933,"ance in memory, communication cost, and retrieval cost. The value for a given triple indicates whether or not information stored at this distance is accessible. T h e values in the table are determined using the simulation environment. Presumably, this process weakly corresponds to the acquisition of proper text planning strategies by h u m a n agents. While an obvious candidate for the model of H&apos;s attentional state is the AWM model itself, certain aspects of this model do not exactly m a t c h some widely believed and intuitively motivated observations aboUt hierarchical discourse structure [10, 18]. However, hierarchical structure interacts with attentional state in ways t h a t have not been fully explored in the literature to date. In particular, if a discourse segment consisting of a nucleus with a hierarchically complex satellite that is extremely long, then a further satellite to the same nucleus m a y well require repetition of the nucleus [35]. Neither R S T nor the model of [10] accounts for such effects. We conclude that it is not a priori obvious t h a t hierarchical structure contradicts our model. We will investigate this issue further. T h r o u g h o u t this paper, we hav"
W94-0320,P88-1020,0,0.0382266,"on of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task). 1 Introduction Text planning is the task for a speaker (S) of deciding what information to :communicate to a hearer (H) and how and when to communicate it. Over the last few years a consensus has emerged that the text planning task should be formulated in terms of communicative goals or intentions [19, 25, 23, 16]. Consider, for example, the RST-based planners developed at ISI [13, 21, 14]. These planners use the discourse relations proposed by Rhetorical S&apos;tructure Theory (RST) [18] as plan operators, by interpreting the requirements on the related segments as preconditions, and the resultant effect of the discourse relation as a postcondition in a traditional AI planning architecture. Two types (at least) of discourse relations have been identified in the literature. A SUBJECT-MATTER relation [18] or SEMANTIC relation [14] simply reflects a relation that exists independently in the world, such as causation. Each subject-matter relation can be seen as a rhetorical strategy for"
W94-0320,P89-1025,0,0.0260836,"on of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task). 1 Introduction Text planning is the task for a speaker (S) of deciding what information to :communicate to a hearer (H) and how and when to communicate it. Over the last few years a consensus has emerged that the text planning task should be formulated in terms of communicative goals or intentions [19, 25, 23, 16]. Consider, for example, the RST-based planners developed at ISI [13, 21, 14]. These planners use the discourse relations proposed by Rhetorical S&apos;tructure Theory (RST) [18] as plan operators, by interpreting the requirements on the related segments as preconditions, and the resultant effect of the discourse relation as a postcondition in a traditional AI planning architecture. Two types (at least) of discourse relations have been identified in the literature. A SUBJECT-MATTER relation [18] or SEMANTIC relation [14] simply reflects a relation that exists independently in the world, such as causation. Each subject-matter relation can be seen as a rhetorical strategy for"
W94-0320,J93-4004,0,0.19184,"rse relations have been identified in the literature. A SUBJECT-MATTER relation [18] or SEMANTIC relation [14] simply reflects a relation that exists independently in the world, such as causation. Each subject-matter relation can be seen as a rhetorical strategy for the linguistic realization of a *Walker was partially funded by ARO grant DAAL0389-C0031PRI and DARPA grant N00014-90-J-1863 at the University of Pennsylvania and by Hewlett Packard, U.K. t Rambow was supported by NATO on a NATO/NSF postdoctoral fellowship in France. rainbow©linguist, j ussieu, fr range of communicative intentions [22]. A PRESENTATIONAL RELATION[18] or INTERPERSONALrelation [14] holds between two discourse segments such that the juxtaposition increases H&apos;s STRENGTH of belief, desire, or intention. Each presentational relation maps directly to a communicative intention. Examples of presentational relations include the MOTIVATION relation, which increases H&apos;s desire to perform an action, hopefully persuading H to form an intention to do the action. Both subject-matter and presentational relations relate two clauses: (1) the NUCLEUS which realizes the main point; and (2) the SATELLITE which is auxiliary option"
W94-0320,J88-3006,0,0.0872994,") an analysis of naturally occurring dialogue, and (2) by simulating the generation of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task). 1 Introduction Text planning is the task for a speaker (S) of deciding what information to :communicate to a hearer (H) and how and when to communicate it. Over the last few years a consensus has emerged that the text planning task should be formulated in terms of communicative goals or intentions [19, 25, 23, 16]. Consider, for example, the RST-based planners developed at ISI [13, 21, 14]. These planners use the discourse relations proposed by Rhetorical S&apos;tructure Theory (RST) [18] as plan operators, by interpreting the requirements on the related segments as preconditions, and the resultant effect of the discourse relation as a postcondition in a traditional AI planning architecture. Two types (at least) of discourse relations have been identified in the literature. A SUBJECT-MATTER relation [18] or SEMANTIC relation [14] simply reflects a relation that exists independently in the world, such as cau"
W94-0320,C92-1054,1,0.793135,"er option. For example, in (4-4) Kim rejects the proposal in (4-3), for pursuing option-45, and proposes option-56 instead. The form of the rejection as a counter-proposal is based on observations about how rejection is communicated in naturally-occurring dialogue as codified in the GOLLABOR.ATIVE PLANNING PRINCIPLES [37]. Proposals i and 2 are inferred to be implicitly ACCEPTED because they are not rejected [37]. If a proposal is ACCEPTED, either implicitly or explicitly, then the option that was the content of the proposal becomes a mutual intention that contributes to the final design plan [27, 34, 30]. The model of AWM discussed above plays a critical role in determining agents&apos; performance. Remember that only salient beliefs can be used in means-end reasoning and deliberation, so that if the warrant for a proposal is not salient, the agent cannot properly evaluate a proposal. 4.2 Varying Discourse Strategies Agents are parametrized for different discourse strategies by placing different expansions of discourse plans in their plan libraries. In Design-World the only discourse plans required are plans for PROPOSAL, REJECTION, ACCEPTANCE, CLAR.IFICATION,OPENING and CLOSING. The only variatio"
W94-0320,P90-1010,1,0.844786,"the proposal she will attempt to retrieve the score proposition stored earlier in memory. Thus the propositions about the scores of furniture items are VCARJ1.ANTS for supporting deliberation. Agents REJECT a proposal if deliberation leads them to believe that they know of a better option. For example, in (4-4) Kim rejects the proposal in (4-3), for pursuing option-45, and proposes option-56 instead. The form of the rejection as a counter-proposal is based on observations about how rejection is communicated in naturally-occurring dialogue as codified in the GOLLABOR.ATIVE PLANNING PRINCIPLES [37]. Proposals i and 2 are inferred to be implicitly ACCEPTED because they are not rejected [37]. If a proposal is ACCEPTED, either implicitly or explicitly, then the option that was the content of the proposal becomes a mutual intention that contributes to the final design plan [27, 34, 30]. The model of AWM discussed above plays a critical role in determining agents&apos; performance. Remember that only salient beliefs can be used in means-end reasoning and deliberation, so that if the warrant for a proposal is not salient, the agent cannot properly evaluate a proposal. 4.2 Varying Discourse Strateg"
W94-0320,J92-4007,0,0.0313658,") an analysis of naturally occurring dialogue, and (2) by simulating the generation of discourses in a situation in which we can vary the cognitive parameters of the hearer. Our results show that modeling cognitive state can lead to more effective discourses (measured with respect to a simple task). 1 Introduction Text planning is the task for a speaker (S) of deciding what information to :communicate to a hearer (H) and how and when to communicate it. Over the last few years a consensus has emerged that the text planning task should be formulated in terms of communicative goals or intentions [19, 25, 23, 16]. Consider, for example, the RST-based planners developed at ISI [13, 21, 14]. These planners use the discourse relations proposed by Rhetorical S&apos;tructure Theory (RST) [18] as plan operators, by interpreting the requirements on the related segments as preconditions, and the resultant effect of the discourse relation as a postcondition in a traditional AI planning architecture. Two types (at least) of discourse relations have been identified in the literature. A SUBJECT-MATTER relation [18] or SEMANTIC relation [14] simply reflects a relation that exists independently in the world, such as cau"
W96-0503,W96-0503,1,0.0547219,"wo descriptions of is s e c t i o n o f his institution, each section belongs to exactly one course. (We have observed such cardinality mistakes in many OO models.) The analyst fixes this and reruns M o D E x on this relation, obtaining the description shown in Figure 3 (bottom). The text now contains a new section with negative examples, which makes it clear that it is no longer possible for a SECTION to belong either to z e r o COURSES or to multiple COURSES. Several other types of text can be generated, such as path descriptions and comparisons and texts about several classes. We refer to (Lavoie et al., 1996) for more detailed information. 4 How MODEx Works MODEx is implemented using the now fairly standard, modular pipeline architecture. Several modules are part of C O G E N T , CoGenTex&apos;s generator shell. M o D E x operates as a &apos;Web server&apos; which generates H T M L files that can be viewed by any Web browser. For lack of space we refrain from giving details here and refer to (Lavoie et al., 1996) for details. M o D E x is designed for use independent of the domain of the OO d a t a model that is being described: it lacks domain knowledge. This means that the system is fully portable between mode"
W96-0503,A92-1006,1,0.548089,"ent feedback from actual users during an iterative prototyping approach. • MoDEx includes examples in its texts: as well as conventional descriptions. The need for examples in documentation has been pointed out in recent work by Paris and Mittal (see for example (Mittal and Paris, 1&apos;293) and the references cited therein). However, none of the specification paraphrasers proposed to date have included examples. • M o D E x uses an interactive hypertext interface to allow users to browse through the model. Such interfaces have been used in other NLG applications, (e.g., (Reiter e t a l . , 1995; Rambow and Korelsky, 1992)), but ours is based on (now) standard html-based W W W technology. • M o D E x uses a simple modeling language, which is based on the ODL standard developed by the Object Database Management Group (OMC) (Cattail, 1994). Some previous systems have paraphrased complex modeling languages that are not widely used outside the research community (GIST, P P P ) . • M o D E x does not have access to knowledge about the domain of the data model (beyond the data model itself). At least one previous system has used such knowledge (GEMA). 3 A MoDEx Scenario Figure 1: The University O-O Diagram Suppose th"
W98-0135,P95-1021,1,0.733935,"s EDL. • The geometry of adjunction (GA). By this term, we mean the specific, mathcmatical defiuition of the adjunction operation in TAG and, especially, the shape of the resulting derived tree. Specifically, an auxiliary tree ß has a designated footnode; when ß is adjoined in a tree a at node v, it is inserted in its entirety into a. In the process, ß remains intact, but a is divided in two subtrees at node v, with ß now attached at v and the subtree formerly rooted in v now attached to the footnode of ß. However, the question arises how other treerewriting formalisms such as D-Tree Grammar (Rambow et al., 1995) can handle whmovement. Specifically, the question arises whether an equally elegant solution to the problem of wh-movement can be found. In this paper, we propose to study e:icactly which what features of the formal (mathematical) definition of TAG contribute to the correct analysis of whmovement (in English). We will mainly concentrate on TAG, but occasionally mention treelocal MC-TAG. • The factoring of recursion (FR). By definition, in an auxiliary tree ß, the footnode and the root node must have the same }abel, A. Furthermore, ß can only be adjoined at a node labeled A. We observe that th"
W98-1409,J97-1004,0,0.0135396,"be considered onerous by the expert system developer, appearing unmotivated from the point of view of the core functionality of the system, namely reasoning (as opposed to explanation). Presumably, it is difficult for one and the same person to be a domain expert and a expert on communication in the domain. In the Rex approach, the obvious problem is that in order to generate an explanation, additional reasoning must be performed which in some sense is very similar to that done by the expert 2We do not consider explanation generation from data bases (for example, (McKeown, i985; Paris, 1988; Lester and Porter, 1997)) to be the same problem as expert system reasoning explanation (even though we may use some similar techniques). I n data base explanations, the knowledge to be communicated is static and its representation is given a p r i o r i as p a r t of the statement of the generation problem. In expert system explanations, the knowledge to be explained is generated dynamically, and the proper representation for this knowledge is part of the solution to the problem of expert system exp:anation, not its statement. 79 system itself (e.g., finding causal chains). This is redundant, and does not result in"
W98-1409,J88-3006,0,0.0305365,"S imposes may be considered onerous by the expert system developer, appearing unmotivated from the point of view of the core functionality of the system, namely reasoning (as opposed to explanation). Presumably, it is difficult for one and the same person to be a domain expert and a expert on communication in the domain. In the Rex approach, the obvious problem is that in order to generate an explanation, additional reasoning must be performed which in some sense is very similar to that done by the expert 2We do not consider explanation generation from data bases (for example, (McKeown, i985; Paris, 1988; Lester and Porter, 1997)) to be the same problem as expert system reasoning explanation (even though we may use some similar techniques). I n data base explanations, the knowledge to be communicated is static and its representation is given a p r i o r i as p a r t of the statement of the generation problem. In expert system explanations, the knowledge to be explained is generated dynamically, and the proper representation for this knowledge is part of the solution to the problem of expert system exp:anation, not its statement. 79 system itself (e.g., finding causal chains). This is redundan"
