2003.mtsummit-semit.12,J94-3001,0,\N,Missing
2003.mtsummit-semit.12,P95-1003,0,\N,Missing
2004.tmi-1.1,carbonell-etal-2002-automatic,1,0.802015,"(Probst et al., 2002), and was previously used for rapid prototyping of an MT system for Hindi-to-English translation (Lavie et al., 2003). For the current Hebrew-to-English system, we manually developed a small set of transfer rules which reflect the most common local syntactic differences between Hebrew and English. This small set of rules turns out to be already sufficient for producing some legible translations of newspaper texts. Performance results are evaluated using state of the art measures and are shown to be encouraging. We also applied an automatic transfer-rule learning approach (Carbonell et al., 2002) to learning a Hebrew-to-English transfer grammar, and report performance results when using the acquired grammar. In the next section we provide some linguistic background about the Hebrew language, with an explicit focus on its challenging sources of ambiguity. Section 3 describes the structure of the MT system with an emphasis on the specific resources required for its application to the Hebrew-to-English language pair and how these resources were acquired and adapted. Section 4 provides some translation examples and describes an evaluation of the system. We conclude with directions for fut"
2004.tmi-1.1,P02-1040,0,0.0900666,", we tested the system on a set of 62 unseen sentences from HaAretz. Three versions of the system were tested on the same data set: a version using our manual transfer grammar; a version using our current automatically-learned grammar; and a version with no transfer grammar at all, which amounts to a word-to-word translation version of the system. Results were evaluated using several automatic metrics for MT evaluation, which compare the translations with human-produced reference translations for the test sentences. For this test set, two reference translations were obtained. We use the BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) automatic metrics for MT evaluation. We also include aggregate unigram-precision and unigram-recall as additional reported measures. The results can be seen in Table 1. To assess statistical significance of the differences in performance between the three versions of the system, we apply a commonly used bootstrapping technique (Efron and Tibshirani, 1986) to estimate the variability over the test set and establish confidence intervals for each reported performance score. As expected, the manual grammar system outperforms the no-grammar system according to all the m"
2004.tmi-1.1,2003.mtsummit-semit.12,1,0.754901,"” script. While a standard convention for this script officially exists, it is not strictly adhered to, even by the major newspapers and in government publications. Thus, the same word can be written in more than one way, sometimes even within the same document. This fact adds significantly to the degree of ambiguity, and requires creative solutions for practical Hebrew language processing applications. The challenge involved in constructing an MT system for Hebrew is amplified by the poverty of existing resources (Wintner, 2004). The collection of corpora for Hebrew is still in early stages (Wintner and Yona, 2003) and all existing significant corpora are monolingual. Hence the use of aligned bilingual corpora for MT purposes is currently not a viable option. There is no available large Hebrew language model which could help in disambiguation. Good morphological analyzers are proprietary and publicly available ones are limited (Wintner, 2004). No publicly available bilingual dictionaries currently exist, and no grammar is available from which transfer rules can be extracted. Still, we made full use of existing resources which we adapted and augmented to fit our needs, as we report in the next section. 1"
2010.amta-srw.4,J90-2002,0,0.48683,"East somewhat closer together by better understanding each other’s societies. 1 In certain respects, Arabic Dialects have morpho-syntactic features closer to Hebrew than Modern Standard Arabic, e.g., the absence of nominal case and verbal mood, the behavior of the feminine ending in genitive constructions, the gendernumber invariance of the relativizer, and the dominance of SVO order over VSO order. We do not discuss Arabic dialects here. Alon Lavie Shuly Wintner LTI Dept. of Computer Science Carnegie Mellon U. U. of Haifa Pittsburgh, PA Haifa, Israel The dominant paradigm in contemporary MT (Brown et al., 1990) relies on large-scale parallel corpora from which correspondences between the two languages can be extracted. However, such abundant parallel corpora currently exist only for few language pairs; and low- and medium-density languages (Varga et al., 2005) require alternative approaches. Specifically, no parallel corpora exist for Hebrew–Arabic.2 As an alternative to the pure statistical approach, we are currently developing a Hebrew-to-Arabic MT system, using the Stat-XFER framework (Lavie, 2008), which is particularly suited for low-resource language pairs. We discuss in Section 2 some linguis"
2010.amta-srw.4,N06-2013,1,0.867525,"encode information on gender and rationality of nouns, which is crucial for enforcing N-Adj agreement. The implication is that in order to generate Arabic, one must overgenerate both masculine and feminine forms, delegating the choice to the language model, which chooses poorly in long-distance dependencies. 3.2 Morphological challenges Translating between two morphologically rich languages poses challenges in analysis, transfer and generation. The complex morphology induces an inherent data sparsity problem, and the limitation imposed by the dearth of available parallel corpora is magnified (Habash and Sadat, 2006). We use a morphological analyzer (Itai and Wintner, 2008) for the Hebrew source, with no morphological disambiguation module.4 This causes many wrong analyses to be processed and dramatically increases the size of the hypothesis lattice. For generation, we use an Arabic morphological generator (Habash, 2004) which requires proper specification of the morpho-syntactic features in order to generate the correct inflected form. Clitics are generated separately and then attached as a postprocess (El Kholy and Habash, 2010). 3.3 Syntactic challenges Arabic word order is relatively free, as in Hebre"
2010.amta-srw.4,W09-0425,1,0.763174,"ntrols the underlying parsing and transfer process. Crucially, Stat-XFER is a statistical MT framework, which uses statistical information to weigh word translations, phrase correspondences and target-language hypotheses; in contrast to other paradigms, however, it can utilize both automatically-created and manuallycrafted language resources, including dictionaries, morphological processors and transfer rules. Stat-XFER has been used as a platform for developing MT systems for Hindi-to-English (Lavie et al., 2003), Hebrew-to-English (Lavie et al., 2004), Chinese-to-English, French-to-English (Hanneman et al., 2009) and many other low-resource language pairs, such as Inupiaq-to-English or Mapudungunto-Spanish. Specifically, we use a Hebrew morphological analyzer (Itai and Wintner, 2008), a mediumsized dictionary, an Arabic morphological generator (Habash, 2004), and a tokenized version of the Arabic Gigaword (Graff et al., 2006) corpus as a language model. We manually constructed a grammar, currently consisting of 42 rules. Some rules manipulate morphemes. After decoding (which uses the language model) we detokenize the output sentence in its morpheme representation (El Kholy and Habash, 2010) to produce"
2010.amta-srw.4,D07-1005,0,0.0596921,"Missing"
2010.amta-srw.4,2004.tmi-1.1,1,0.873797,"linear combination of several features, and a beam-search controls the underlying parsing and transfer process. Crucially, Stat-XFER is a statistical MT framework, which uses statistical information to weigh word translations, phrase correspondences and target-language hypotheses; in contrast to other paradigms, however, it can utilize both automatically-created and manuallycrafted language resources, including dictionaries, morphological processors and transfer rules. Stat-XFER has been used as a platform for developing MT systems for Hindi-to-English (Lavie et al., 2003), Hebrew-to-English (Lavie et al., 2004), Chinese-to-English, French-to-English (Hanneman et al., 2009) and many other low-resource language pairs, such as Inupiaq-to-English or Mapudungunto-Spanish. Specifically, we use a Hebrew morphological analyzer (Itai and Wintner, 2008), a mediumsized dictionary, an Arabic morphological generator (Habash, 2004), and a tokenized version of the Arabic Gigaword (Graff et al., 2006) corpus as a language model. We manually constructed a grammar, currently consisting of 42 rules. Some rules manipulate morphemes. After decoding (which uses the language model) we detokenize the output sentence in its"
2010.amta-srw.4,J05-4003,0,0.0364466,"pected to be ambiguous; however, Google produces the following wrong translations in such cases: (15) atm / atn =⇒ Ant you.pl.m / you.pl.f =⇒ you.sg.m/f amrti say.1sg.past qlt say.1sg.past lkm =⇒ to+you.2.pl.m-dat. =⇒ lk to+you.2.sg.m/f-gen. The second test uses the fact that plural nouns in English are unspecified for gender, whereas in Hebrew and Arabic they are. Here, gender is lost in translation of plurality, and the decoder chose the most common option according to the LM. (16) mwrim / mwrwt =⇒ mςlmyn teachers.m / teachers.f =⇒ teachers.m 5 A third approach is to use comparable corpora (Munteanu and Marcu, 2005); but with no parallel data whatsoever, this is unlikely to succeed. 6 http://www.google.com/language_tools, accessed May 5th, 2010. 7 Another Hebrew-to-Arabic MT system, http://www. microsofttranslator.com/, also uses English as a pivot language, and shows similar characteristics. In the third test, we use words which are lexically ambiguous in English but not in Hebrew or Arabic. (17)(a) Tblh =⇒ TAwl¯ h table (data) =⇒ table (furniture) (b) bnq =⇒ sAHl bank (financial) =⇒ bank (shore) (c) idni =⇒ ktyb manual (by-hand) =⇒ manual (booklet) Finally, we used proper names and morphologically comp"
2010.amta-srw.4,1987.mtsummit-1.16,0,0.657819,"ures: person, gender, number, aspect (perfective, imperfective and imperative), voice (passive or active), and mood (indicative, subjunctive or jussive). For every noun, 72 forms are returned (excluding possible clitics), as a result of the various values of the features gender, number, case, possessiveness and definiteness. 4 Possible approaches As the standard paradigm of statistical MT is not applicable to Hebrew-to-Arabic MT, due to the dearth of available parallel corpora, two alternatives present themselves. One is translating using a third language (most naturally, English) as a pivot (Muraki, 1987; Wu and Wang, 2007); the other is relying on linguistically-motivated transfer rules, augmented by deep linguistic processing of both the source and the target languages.5 We consider both approaches below. 4.1 Using English as pivot The dominant Hebrew-to-Arabic MT system is Google’s.6 Google has been known to use ‘bridge’ languages in translation (Kumar et al., 2007). We provide evidence that Google’s Hebrew-to-Arabic MT uses English as a pivot, and demonstrate the shortcomings of this approach.7 As a first test, we use the number- and genderambiguity of second-person pronouns in English (y"
2010.amta-srw.4,P07-1108,0,0.0214551,"gender, number, aspect (perfective, imperfective and imperative), voice (passive or active), and mood (indicative, subjunctive or jussive). For every noun, 72 forms are returned (excluding possible clitics), as a result of the various values of the features gender, number, case, possessiveness and definiteness. 4 Possible approaches As the standard paradigm of statistical MT is not applicable to Hebrew-to-Arabic MT, due to the dearth of available parallel corpora, two alternatives present themselves. One is translating using a third language (most naturally, English) as a pivot (Muraki, 1987; Wu and Wang, 2007); the other is relying on linguistically-motivated transfer rules, augmented by deep linguistic processing of both the source and the target languages.5 We consider both approaches below. 4.1 Using English as pivot The dominant Hebrew-to-Arabic MT system is Google’s.6 Google has been known to use ‘bridge’ languages in translation (Kumar et al., 2007). We provide evidence that Google’s Hebrew-to-Arabic MT uses English as a pivot, and demonstrate the shortcomings of this approach.7 As a first test, we use the number- and genderambiguity of second-person pronouns in English (you). Since Hebrew an"
2021.acl-short.16,D18-1366,0,0.371718,"he vocabulary from the MT model, making adaptation easier. Now, to finetune this model to generate TGT, we need TGT embeddings. Since the TGT monolingual corpus is small, training fasttext vectors on this corpus from scratch will lead (as we show) to low-quality embeddings. Leveraging the relatedness of STD and TGT and their vocabulary overlap, we use STD embeddings to transfer knowledge to TGT embeddings: for each character ngram in the TGT corpus, we initialize its embedding with the corresponding STD embedding, if available. We then continue training fasttext on the TGT monolingual corpus (Chaudhary et al., 2018). Last, we use a supervised embedding alignment method (Lample et al., 2018a) to project the learned TGT embeddings in the same space as STD. STD and TGT are expected to have a large lexical overlap, so we use identical tokens in both varieties as supervision for this alignment. The obtained embeddings, due to transfer learning from STD, inject additional knowledge in the model. Finally, to obtain a SRC→TGT model, we finetune f on psuedo-parallel SRC – TGT data. Using a STD→SRC MT model (a back-translation model 111 trained using large STD – SRC parallel data with standard settings) we (back)-"
2021.acl-short.16,2020.emnlp-main.214,0,0.248347,"ing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when TGT contains characters not present in STD.3 To counter this, we train a joint BPE model with 24K operations on the concatenation of STD and TGT corpora to tokenize TGT corpus following Chronopoulou et al. (2020). This technique increases the number of shared tokens between STD and TGT, thus enabling better cross-variety transfer while learning embeddings and while finetuning. We follow Chaudhary et al. (2018) to train embeddings on the generated TGT vocabulary where we initialize the character n-gram representations for TGT words with STD’s fasttext model wherever available and finetune them on the TGT corpus. Implementation and Evaluation We modify the standard OpenNMT-py seq2seq models of PyTorch (Klein et al., 2017) to train our model with vMF loss (Kumar and Tsvetkov, 2019). Additional hyperparam"
2021.acl-short.16,2020.emnlp-main.480,0,0.0478527,"Missing"
2021.acl-short.16,W19-6721,0,0.0282889,"i et al., 2016), and 80M MSA sentences from the CoNLL’17 shared task. For Arabic varieties, we use the MADAR corpus (Bouamor et al., 2018) which consists of 12K 6way parallel sentences between English, MSA and the 4 considered varieties. We ignore the English sentences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when"
2021.acl-short.16,2020.findings-emnlp.283,0,0.0383765,"Missing"
2021.acl-short.16,D19-1632,0,0.015424,", Belarusian, Nynorsk, and the four Arabic varieties. For reference, note that the EN→RU, EN→MSA, and EN→NO models are relatively strong, yielding BLEU scores of 24.3, 21.2, and 24.9, respectively. Synthetic Setup Considering STD and TGT as the same language is sub-optimal, as is evident from the poor performance of the non-adapted S UP ( SRC→STD ) model. Clearly, special attention ought to be paid to language varieties. Direct unsupervised translation from SRC to TGT performs poorly as well, confirming previously reported results of the ineffectiveness of such methods on unrelated languages (Guzmán et al., 2019). 5 Additional ablation results are listed in Appendix C. Translating SRC to TGT by pivoting through STD achieves much better performance owing to strong U NSUP ( STD→TGT ) models that leverage the similarities between STD and TGT. However, when resources are scarse (e.g., with 10K monolingual sentences as opposed to 1M), this performance gain considerably diminishes. We attribute this drop to overfitting during the pre-training phase on the small TGT monolingual data. Ablation results (Appendix C) also show that in such low-resource settings the learned embeddings are of low quality. Finally,"
2021.acl-short.16,2020.wmt-1.68,0,0.0324819,"Missing"
2021.acl-short.16,2011.eamt-1.19,0,0.178881,"Missing"
2021.acl-short.16,2020.acl-main.448,0,0.0253534,"and aligning the two embedding spaces. (3) The resulting model is finetuned with pseudo-parallel SRC→TGT data. We compare L ANG VAR MT with the following competitive baselines. S UP ( SRC→STD ): train a standard (softmax-based) supervised SRC→STD model, and consider the output of this model as 2 We slightly modify fasttext to not consider BPE token markers “@@” in the character n-grams. 112 3 For example, both RU and UK alphabets consist of 33 letters; RU has the letters Ёё, ъ, ы and Ээ, which are not used in UK. Instead, UK has Ґґ, Єє, Ii and Її. 4 While we recognize the limitations of BLEU (Mathur et al., 2020), more sophisticated embedding-based metrics for MT evaluation (Zhang et al., 2020; Sellam et al., 2020) are unfortunately not available for low-resource language varieties. Size of TGT corpus 10K 100K 1M 10K 100K 1M 300K Arabic Varieties (10K) Doha Beirut Rabat Tunis S UP ( SRC→STD ) U NSUP ( SRC→TGT ) P IVOT S OFTMAX L ANG VAR MT 1.7 0.3 1.5 1.9 6.1 1.7 0.6 8.6 12.7 13.5 1.7 0.9 14.9 15.4 15.3 1.5 0.4 1.15 1.5 2.3 1.5 0.6 3.9 4.5 8.8 1.5 1.4 8.0 7.9 9.8 11.3 2.7 11.9 14.4 16.6 3.7 0.2 1.8 14.5 20.1 UK BE NN 1.8 0.1 2.1 7.4 8.1 2.0 0.1 1.7 4.9 7.4 1.3 0.1 1.1 3.9 4.6 Table 1: BLEU scores on t"
2021.acl-short.16,P12-2059,0,0.0329472,"Missing"
2021.acl-short.16,2020.acl-main.156,0,0.0322545,"GT, but we show that they improve the overall performance. We discuss the implications of this noise in §4. 3 Experimental Setup Datasets We experiment with two setups. In the first (synthetic) setup, we use English (EN) as SRC, Russian (RU) as STD, and Ukrainian (UK) and Belarusian (BE) as TGTs. We sample 10M EN - RU sentences from the WMT’19 shared task (Ma et al., 2019), and 80M RU sentences from the CoNLL’17 shared task to train embeddings. To simulate lowresource scenarios, we sample 10K, 100K and 1M UK sentences from the CoNLL’17 shared task and BE sentences from the OSCAR corpus (Ortiz Suárez et al., 2020). We use TED dev/test sets for both languages pairs (Cettolo et al., 2012). The second (real world) setup has two language sets: the first one defines English as SRC, with Modern Standard Arabic (MSA) as STD and four Arabic varieties spoken in Doha, Beirut, Rabat and Tunis as TGTs. We sample 10M EN - MSA sentences from the UNPC corpus (Ziemski et al., 2016), and 80M MSA sentences from the CoNLL’17 shared task. For Arabic varieties, we use the MADAR corpus (Bouamor et al., 2018) which consists of 12K 6way parallel sentences between English, MSA and the 4 considered varieties. We ignore the Engl"
2021.acl-short.16,P02-1040,0,0.110582,"abling better cross-variety transfer while learning embeddings and while finetuning. We follow Chaudhary et al. (2018) to train embeddings on the generated TGT vocabulary where we initialize the character n-gram representations for TGT words with STD’s fasttext model wherever available and finetune them on the TGT corpus. Implementation and Evaluation We modify the standard OpenNMT-py seq2seq models of PyTorch (Klein et al., 2017) to train our model with vMF loss (Kumar and Tsvetkov, 2019). Additional hyperparameter details are outlined in Appendix B. We evaluate our methods using BLEU score (Papineni et al., 2002) based on the SacreBLEU implementation (Post, 2018).4 For the Arabic varieties, we also report a macro-average. In addition, to measure the expected impact on actual systems’ users, we follow Faisal et al. (2021) in computing a population-weighted macro-average (avgpop ) based on language community populations provided by Ethnologue (Eberhard et al., 2019). 3.1 Experiments Our proposed framework, L ANG VAR MT, consists of three main components: (1) A supervised SRC → STD model is trained to predict continuous STD word embeddings rather than discrete softmax probabilities. (2) Output STD embedd"
2021.acl-short.16,W18-6319,0,0.0133174,"and while finetuning. We follow Chaudhary et al. (2018) to train embeddings on the generated TGT vocabulary where we initialize the character n-gram representations for TGT words with STD’s fasttext model wherever available and finetune them on the TGT corpus. Implementation and Evaluation We modify the standard OpenNMT-py seq2seq models of PyTorch (Klein et al., 2017) to train our model with vMF loss (Kumar and Tsvetkov, 2019). Additional hyperparameter details are outlined in Appendix B. We evaluate our methods using BLEU score (Papineni et al., 2002) based on the SacreBLEU implementation (Post, 2018).4 For the Arabic varieties, we also report a macro-average. In addition, to measure the expected impact on actual systems’ users, we follow Faisal et al. (2021) in computing a population-weighted macro-average (avgpop ) based on language community populations provided by Ethnologue (Eberhard et al., 2019). 3.1 Experiments Our proposed framework, L ANG VAR MT, consists of three main components: (1) A supervised SRC → STD model is trained to predict continuous STD word embeddings rather than discrete softmax probabilities. (2) Output STD embeddings are replaced with TGT embeddings. The TGT embe"
2021.acl-short.16,D17-1266,0,0.0391541,"Missing"
2021.acl-short.16,2020.acl-demos.14,0,0.0283906,"how that in such low-resource settings the learned embeddings are of low quality. Finally, L ANG VAR MT consistently outperforms all baselines. Using 1M UK sentences, it achieves similar performance (for EN→UK) to the softmax ablation of our method, S OFTMAX, and small gains over unsupervised methods. However, in lower resource settings our approach is clearly better than the strongest baselines by over 4 BLEU points for UK (10K) and 3.9 points for BE (100K). To identify potential sources of error in our proposed method, we lemmatize the generated translations and test sets and evaluate BLEU (Qi et al., 2020). Across all data sizes, both UK and BE achieve a substantial increase in BLEU (up to +6 BLEU; see Appendix D for details) compared to that obtained on raw text, indicating morphological errors in the translations. In future work, we will investigate whether we can alleviate this issue by considering TGT embeddings based on morphological features of tokens (Chaudhary et al., 2018). Real-world Setup The effectiveness of L ANG VAR MT is pronounced in this setup with a dramatic improvement of more than 18 BLEU points over unsupervised baselines when translating into Doha Arabic. We hypothesize th"
2021.acl-short.16,2020.emnlp-main.365,0,0.0279739,"arallel sentences between English, MSA and the 4 considered varieties. We ignore the English sentences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when TGT contains characters not present in STD.3 To counter this, we train a joint BPE model with 24K operations on the concatenation of STD and TGT corpora to tokenize TGT corpu"
2021.acl-short.16,2021.eacl-main.115,0,0.0357741,"e sample 10M EN - MSA sentences from the UNPC corpus (Ziemski et al., 2016), and 80M MSA sentences from the CoNLL’17 shared task. For Arabic varieties, we use the MADAR corpus (Bouamor et al., 2018) which consists of 12K 6way parallel sentences between English, MSA and the 4 considered varieties. We ignore the English sentences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BP"
2021.acl-short.16,2020.acl-main.704,0,0.0247475,"data. We compare L ANG VAR MT with the following competitive baselines. S UP ( SRC→STD ): train a standard (softmax-based) supervised SRC→STD model, and consider the output of this model as 2 We slightly modify fasttext to not consider BPE token markers “@@” in the character n-grams. 112 3 For example, both RU and UK alphabets consist of 33 letters; RU has the letters Ёё, ъ, ы and Ээ, which are not used in UK. Instead, UK has Ґґ, Єє, Ii and Її. 4 While we recognize the limitations of BLEU (Mathur et al., 2020), more sophisticated embedding-based metrics for MT evaluation (Zhang et al., 2020; Sellam et al., 2020) are unfortunately not available for low-resource language varieties. Size of TGT corpus 10K 100K 1M 10K 100K 1M 300K Arabic Varieties (10K) Doha Beirut Rabat Tunis S UP ( SRC→STD ) U NSUP ( SRC→TGT ) P IVOT S OFTMAX L ANG VAR MT 1.7 0.3 1.5 1.9 6.1 1.7 0.6 8.6 12.7 13.5 1.7 0.9 14.9 15.4 15.3 1.5 0.4 1.15 1.5 2.3 1.5 0.6 3.9 4.5 8.8 1.5 1.4 8.0 7.9 9.8 11.3 2.7 11.9 14.4 16.6 3.7 0.2 1.8 14.5 20.1 UK BE NN 1.8 0.1 2.1 7.4 8.1 2.0 0.1 1.7 4.9 7.4 1.3 0.1 1.1 3.9 4.6 Table 1: BLEU scores on translation from English to Ukrainian, Belarusian, Nynorsk, and Arabic dialects with varying amounts of m"
2021.acl-short.16,P16-1162,0,0.0290828,"ences, sample dev/test sets of 1K sentences each, and consider 10K monolingual sentences for each TGT variety. The second set also has English as SRC with Norwegian Bokmål (NO) as STD and its written variety Nynorsk (NN) as TGT. We use 630K EN - NO sentences from WikiMatrix (Schwenk et al., 2021), and 26M NO sentences from ParaCrawl (Esplà et al., 2019) combined with the WikiMatrix NO sentences to train embeddings. We use 310K NN sentences from WikiMatrix, and TED dev/test sets for both varieties (Reimers and Gurevych, 2020). Preprocessing We preprocess raw text using Byte Pair Encoding (BPE, Sennrich et al., 2016) with 24K merge operations on each SRC – STD corpus trained separately on SRC and STD. We use the same BPE model to tokenize the monolingual STD data and learn fasttext embeddings (we consider character n-grams of length 3 to 6).2 Splitting the TGT words with the same STD BPE model will result in heavy segmentation, especially when TGT contains characters not present in STD.3 To counter this, we train a joint BPE model with 24K operations on the concatenation of STD and TGT corpora to tokenize TGT corpus following Chronopoulou et al. (2020). This technique increases the number of shared tokens"
2021.acl-short.16,W07-0705,0,0.168003,"Missing"
C02-1066,J89-4001,0,0.469905,"parsing problem, for a grammar and a string , is to deliver all parse trees that induces on , determining what structural descriptions are assigned by to . The rest of this paper is concerned with recognition. Unification grammars have the formal power of Turing machines, thus the recognition problem for them is undecidable. In order to ensure decidability of the recognition problem, a constraint called offline parsability (OLP) was suggested. The recognition problem is decidable for OLP grammars. There exist several variants of OLP in the literature (Pereira and Warren, 1983; Johnson, 1988; Haas, 1989; Torenvliet and Trautwein, 1995; Shieber, 1992; Wintner and Francez, 1999; Kuhn, 1999).        Shuly Wintner Department of Computer Science University of Haifa 31905 Haifa, Israel Some variants of OLP were suggested without recognizing the existence of all other variants. In this paper we make a comparative analysis of the different OLP variants for the first time. Some researchers (Haas, 1989; Torenvliet and Trautwein, 1995) conjecture that some of the OLP variants are undecidable (it is undecidable whether a grammar satisfies the constraint), although none of them gives any proof"
C02-1066,P83-1021,0,0.639146,"a grammar and a string , is whether . The parsing problem, for a grammar and a string , is to deliver all parse trees that induces on , determining what structural descriptions are assigned by to . The rest of this paper is concerned with recognition. Unification grammars have the formal power of Turing machines, thus the recognition problem for them is undecidable. In order to ensure decidability of the recognition problem, a constraint called offline parsability (OLP) was suggested. The recognition problem is decidable for OLP grammars. There exist several variants of OLP in the literature (Pereira and Warren, 1983; Johnson, 1988; Haas, 1989; Torenvliet and Trautwein, 1995; Shieber, 1992; Wintner and Francez, 1999; Kuhn, 1999).        Shuly Wintner Department of Computer Science University of Haifa 31905 Haifa, Israel Some variants of OLP were suggested without recognizing the existence of all other variants. In this paper we make a comparative analysis of the different OLP variants for the first time. Some researchers (Haas, 1989; Torenvliet and Trautwein, 1995) conjecture that some of the OLP variants are undecidable (it is undecidable whether a grammar satisfies the constraint), although n"
C02-1066,J92-2002,0,0.45142,", is to deliver all parse trees that induces on , determining what structural descriptions are assigned by to . The rest of this paper is concerned with recognition. Unification grammars have the formal power of Turing machines, thus the recognition problem for them is undecidable. In order to ensure decidability of the recognition problem, a constraint called offline parsability (OLP) was suggested. The recognition problem is decidable for OLP grammars. There exist several variants of OLP in the literature (Pereira and Warren, 1983; Johnson, 1988; Haas, 1989; Torenvliet and Trautwein, 1995; Shieber, 1992; Wintner and Francez, 1999; Kuhn, 1999).        Shuly Wintner Department of Computer Science University of Haifa 31905 Haifa, Israel Some variants of OLP were suggested without recognizing the existence of all other variants. In this paper we make a comparative analysis of the different OLP variants for the first time. Some researchers (Haas, 1989; Torenvliet and Trautwein, 1995) conjecture that some of the OLP variants are undecidable (it is undecidable whether a grammar satisfies the constraint), although none of them gives any proof of it. There exist some variants of OLP for wh"
C10-1002,P89-1010,0,0.30449,"Missing"
C10-1002,P01-1025,0,0.0308083,"n. 8. The number of occurrences of N2 in singular outside the expression. 9. The number of occurrences of N1 sˇl N2 “N1 of N2 ” in the corpus. 10. The number of occurrences of N1 m N2 “N1 from N2 ” in the corpus. 11. The number of occurrences of N1 N2 w N3 “N1 N2 and N3 ” in the corpus, where N3 is an indefinite, non-construct-state noun. 12. The number of occurrences of N1 N2 Adj in the corpus, where the adjective Adj agrees with N2 on both gender and number, while disagreeing with N1 on at least one of these attributes. We also define four features that represent known collocation measures (Evert and Krenn, 2001): Point-wise mutual information (PMI); T-Score; log-likelihood; and the raw frequency of N1 N2 in the corpus.3 3.6 Optimizing feature combination We search for the combination of linguisticallymotivated features that would yield the best performance. Training a classifier on all possible feature combinations is clearly infeasible. Instead, we follow a more efficient greedy approach, whereby we start with the best collocation mea3 A detailed description of these measures is given by Manning and Sch¨utze (1999, Chapter 5); see also http: //www.collocations.de/, where several other association me"
C10-1002,E06-1043,0,0.0843653,"lly fixed if replacing any of its constituents by a semantically (and syntactically) similar word generally results in an invalid or literal expression. Syntactically fixed expressions prohibit (or restrict) syntactic variation. For example, Van de Cruys and Villada Moir´on (2007) use lexical fixedness to extract Dutch VerbNoun idiomatic combinations (VNICs). Bannard (2007) uses syntactic fixedness to identify English VNICs. Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson, 2006). While these approaches are in line with ours, they require lexical semantic resources (e.g., a database that determines semantic similarity among words) and syntactic resources (parsers) that are unavailable for Hebrew (and many other languages). Our approach only requires morphological processing, which is more readily-available for several languages. Another unique feature of our work is that it computationally addresses Hebrew (and, more generally, Semitic) MWEs for the first time. Berman and Ravid (1986) define the dictionary degree of noun compounds in Hebrew as their 5 Conclusions and"
C10-1002,W06-1203,0,0.117506,"ely 16 easy, and requires only a theoretical linguistic investigation of the construction. We are currently interested in extending the system to cope also with Adjective-Noun, Noun-Adjective and VerbPreposition constructions in Hebrew. The accuracy of MWE acquisition systems can be further improved by combining our morphological and syntactic features with semantically informed features such as translational entropy computed from a parallel corpus (Villada Moir´on and Tiedemann, 2006), or features that can capture the local linguistic context of the expression using latent semantic analysis (Katz and Giesbrecht, 2006). We are currently working on the former direction (Tsvetkov and Wintner, 2010b), utilizing a small Hebrew-English parallel corpus (Tsvetkov and Wintner, 2010a). Finally, we are interested in evaluating the methodology proposed in this paper to other languages with complex morphology, in particular to Arabic. We leave this direction to future research. Bannard, Colin. 2007. A measure of syntactic flexibility for automatically identifying multiword expressions in corpora. In Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 1–8. Association for Computational L"
C10-1002,C10-2144,1,0.896562,"Missing"
C10-1002,W07-1104,0,0.373714,"Missing"
C10-1002,W06-2405,0,0.0905156,"Missing"
C10-1002,W04-0409,0,0.0493284,"Missing"
C10-1002,tsvetkov-wintner-2010-automatic,1,0.724379,"Missing"
C10-1002,J90-1003,0,\N,Missing
C10-1002,W07-1101,0,\N,Missing
C10-1002,W04-0407,0,\N,Missing
C10-2144,C10-1002,1,0.846835,"Galon Prime Minister Avshalom Vilan Bar On Meir Shitrit Limor Livnat Attorney General thanks a lot Gaza Strip Type NNC GT NNC-GT PN NNC PN PN NNC PN PN PN PN N-ADJ N-ADJ NNC-GT Table 1: Results: extracted MWEs 4 Evaluation MWEs are notoriously hard to define, and no clear-cut criteria exist to distinguish between MWEs and other frequent collocations. In order to evaluate the utility of our methodology, we conducted three different types of evaluations that we detail below and in Section 5. First, we use a small annotated corpus of Hebrew noun-noun constructions that was made available to us (Al-Haj and Wintner, 2010). The corpus consists of 463 high-frequency bi-grams of the same syntactic construction; of those, 202 are tagged as MWEs (in this case, noun compounds) and 258 as non-MWEs. This corpus consolidates the annotation of three annotators: only instances on which all three agreed were included. Since it includes both positive and negative instances, this corpus facilitates a robust evaluation of precision and recall. Of the 202 positive examples, only 121 occur in our parallel corpus; of the 258 negative examples, 91 occur in our corpus. We therefore limit the discussion to those 212 examples whose"
C10-2144,W03-1812,0,0.0171821,"Missing"
C10-2144,W03-1809,0,0.030972,"shed by their idiosyncratic behavior. Morphologically, some MWEs allow some of their constituents to freely inflect while restricting (or preventing) the inflection of other constituents. In some cases MWEs may allow constituents to undergo non-standard morphological inflections that they would not undergo in isolation. Syntactically, some MWEs behave like words while other are phrases; some occur in one rigid pattern (and a fixed order), while others permit various syntactic transformations. Semantically, the compositionality of MWEs is gradual, ranging from fully compositional to idiomatic (Bannard et al., 2003). Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing applications. Handling MWEs correctly is beneficial for a variety of applications, including information retrieval, building ontologies, text alignment, and machine translation. Identifying MWEs and extracting them from corpora is therefore both important and difficult. In Hebrew (which is the subject of our research), this is even more challenging due to two reasons: the rich and complex morphology of the language; and the dearth of existing language resources, in particular parallel"
C10-2144,W05-0706,0,0.0218724,"Missing"
C10-2144,W09-2901,0,0.0152265,"e for medium-density languages (Varga et al., 2005). We assume the following resources: a small bilingual, sentencealigned parallel corpus; large monolingual corpora in both languages; morphological processors (analyzers and disambiguation modules) for the two languages; and a bilingual dictionary. Our experimental setup is Hebrew-English. We use a small parallel corpus (Tsvetkov and Wintner, 2010) consisting of 19,626 sentences, mostly from newspapers. The corpus consists of 271,787 English tokens (14,142 types) and 280,508 Hebrew tokens (12,555 types), and is similar in size to that used by Caseli et al. (2009). We also use data extracted from two monolingual corpora. For Hebrew, we use the morphologically-analyzed MILA corpus (Itai and Wintner, 2008) with part-of-speech tags produced by Bar-Haim et al. (2005). This corpus is much larger, consisting of 46,239,285 tokens (188,572 types). For English we use Google’s Web 1T corpus (Brants and Franz, 2006). Finally, we use a bilingual dictionary consist1258 ing of 78,313 translation pairs. Some of the entries were collected manually, while others are produced automatically (Itai and Wintner, 2008; Kirschenbaum and Wintner, 2010). 3.3 Preprocessing the c"
C10-2144,W02-1801,0,0.327881,"Missing"
C10-2144,P89-1010,0,0.514191,"Missing"
C10-2144,W03-0305,0,0.0229613,"of the entries were collected manually, while others are produced automatically (Itai and Wintner, 2008; Kirschenbaum and Wintner, 2010). 3.3 Preprocessing the corpora Automatic word alignment algorithms are noisy, and given a small parallel corpus such as ours, data sparsity is a serious problem. To minimize the parameter space for the alignment algorithm, we attempt to reduce language specific differences by pre-processing the parallel corpus. The importance of this phase should not be underestimated, especially for alignment of two radically different languages such as English and Hebrew (Dejean et al., 2003). Hebrew,1 like other Semitic languages, has a rich, complex and highly productive morphology. Information pertaining to gender, number, definiteness, person, and tense is reflected morphologically on base forms of words. In addition, prepositions, conjunctions, articles, possessives, etc., may be concatenated to word forms as prefixes or suffixes. This results in a very large number of possible forms per lexeme. We therefore tokenize the parallel corpus and then remove punctuation. We analyze the Hebrew corpus morphologically and select the most appropriate analysis in context. Adopting this"
C10-2144,W06-1203,0,0.027267,"Missing"
C10-2144,kirschenbaum-wintner-2010-general,1,0.746778,"is similar in size to that used by Caseli et al. (2009). We also use data extracted from two monolingual corpora. For Hebrew, we use the morphologically-analyzed MILA corpus (Itai and Wintner, 2008) with part-of-speech tags produced by Bar-Haim et al. (2005). This corpus is much larger, consisting of 46,239,285 tokens (188,572 types). For English we use Google’s Web 1T corpus (Brants and Franz, 2006). Finally, we use a bilingual dictionary consist1258 ing of 78,313 translation pairs. Some of the entries were collected manually, while others are produced automatically (Itai and Wintner, 2008; Kirschenbaum and Wintner, 2010). 3.3 Preprocessing the corpora Automatic word alignment algorithms are noisy, and given a small parallel corpus such as ours, data sparsity is a serious problem. To minimize the parameter space for the alignment algorithm, we attempt to reduce language specific differences by pre-processing the parallel corpus. The importance of this phase should not be underestimated, especially for alignment of two radically different languages such as English and Hebrew (Dejean et al., 2003). Hebrew,1 like other Semitic languages, has a rich, complex and highly productive morphology. Information pertaining"
C10-2144,2005.mtsummit-papers.11,0,0.0293425,"Missing"
C10-2144,lavie-etal-2004-significance,0,0.0240886,"ality of word alignment, especially in the case of MWEs, is rather low, we remove “translations” that are longer than four words (these are most often wrong). We then associate each extracted MWE in Hebrew with all its possible English translations. The result is a bilingual dictionary containing 2,955 MWE translation pairs, and also 355 translation pairs produced by taking high-quality 1:1 word alignments (Section 3.4). We used the extracted MWE bilingual dictionary to augment the existing (78,313-entry) dictionary of a transfer-based Hebrew-to-English statistical machine translation system (Lavie et al., 2004b). We report in Table 3 the results of evaluating the performance of the MT system with its original dictionary and with the augmented dictionary. The results show a statistically-significant (p &lt; 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al., 2004a) scores. Dictionary Original Augmented BLEU 13.69 13.79 Meteor 33.38 33.99 Table 3: External evaluation As examples of improved translations, a sentence that was originally translated as “His teachers also hate to the Zionism and besmirch his HRCL and Gurion” (fully capitalized words indicate lexical omiss"
C10-2144,2004.tmi-1.1,1,0.829747,"ality of word alignment, especially in the case of MWEs, is rather low, we remove “translations” that are longer than four words (these are most often wrong). We then associate each extracted MWE in Hebrew with all its possible English translations. The result is a bilingual dictionary containing 2,955 MWE translation pairs, and also 355 translation pairs produced by taking high-quality 1:1 word alignments (Section 3.4). We used the extracted MWE bilingual dictionary to augment the existing (78,313-entry) dictionary of a transfer-based Hebrew-to-English statistical machine translation system (Lavie et al., 2004b). We report in Table 3 the results of evaluating the performance of the MT system with its original dictionary and with the augmented dictionary. The results show a statistically-significant (p &lt; 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al., 2004a) scores. Dictionary Original Augmented BLEU 13.69 13.79 Meteor 33.38 33.99 Table 3: External evaluation As examples of improved translations, a sentence that was originally translated as “His teachers also hate to the Zionism and besmirch his HRCL and Gurion” (fully capitalized words indicate lexical omiss"
C10-2144,J03-1002,0,0.0031543,"alignment) in parallel texts: either MWEs (which trigger 1:n or n:m alignments); or language-specific differences (e.g., the source language lexically realizes notions that are realized morphologically, syntactically or in some other way in the target language); or noise (e.g., poor translations, low-quality sentence alignment, and inherent limitations of word alignment algorithms). This motivation induces the following algorithm. Given a parallel, sentence-aligned corpus, it is first pre-processed as described above, to reduce the effect of language-specific differences. We then use Giza++ (Och and Ney, 2003) to wordalign the text, employing union to merge the alignments in both directions. We look up all 1:1 alignments in the dictionary. If the pair exists in our bilingual dictionary, we remove it from the sentence and replace it with a special symbol, ‘*’. Such word pairs are not parts of MWEs. If the pair is not in the dictionary, but its alignment score is very high (above 0.5) and it is sufficiently frequent (more than 5 occurrences), we add the pair to the dictionary but also retain it in the sentence. Such pairs are still candidates for being (parts of) MWEs. Example 1 Figure 1-a depicts a"
C10-2144,P02-1040,0,0.108273,"sult is a bilingual dictionary containing 2,955 MWE translation pairs, and also 355 translation pairs produced by taking high-quality 1:1 word alignments (Section 3.4). We used the extracted MWE bilingual dictionary to augment the existing (78,313-entry) dictionary of a transfer-based Hebrew-to-English statistical machine translation system (Lavie et al., 2004b). We report in Table 3 the results of evaluating the performance of the MT system with its original dictionary and with the augmented dictionary. The results show a statistically-significant (p &lt; 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al., 2004a) scores. Dictionary Original Augmented BLEU 13.69 13.79 Meteor 33.38 33.99 Table 3: External evaluation As examples of improved translations, a sentence that was originally translated as “His teachers also hate to the Zionism and besmirch his HRCL and Gurion” (fully capitalized words indicate lexical omissions that are transliterated by the MT system) is translated with the new dictionary as “His teachers also hate to the Zionism and besmirch his Herzl and David Ben-Gurion”; a phrase originally translated as “when so” is now properly translated as “likewise”; a"
C10-2144,tsvetkov-wintner-2010-automatic,1,0.82909,"on of English MWEs, along with their translations to Hebrew. This, again, contributes to the task of enriching our existing bilingual dictionary. 3.2 Resources Our methodology is in principle languageindependent and appropriate for medium-density languages (Varga et al., 2005). We assume the following resources: a small bilingual, sentencealigned parallel corpus; large monolingual corpora in both languages; morphological processors (analyzers and disambiguation modules) for the two languages; and a bilingual dictionary. Our experimental setup is Hebrew-English. We use a small parallel corpus (Tsvetkov and Wintner, 2010) consisting of 19,626 sentences, mostly from newspapers. The corpus consists of 271,787 English tokens (14,142 types) and 280,508 Hebrew tokens (12,555 types), and is similar in size to that used by Caseli et al. (2009). We also use data extracted from two monolingual corpora. For Hebrew, we use the morphologically-analyzed MILA corpus (Itai and Wintner, 2008) with part-of-speech tags produced by Bar-Haim et al. (2005). This corpus is much larger, consisting of 46,239,285 tokens (188,572 types). For English we use Google’s Web 1T corpus (Brants and Franz, 2006). Finally, we use a bilingual dic"
C10-2144,W07-1104,0,0.0392883,"Missing"
C10-2144,W06-2405,0,0.0265577,"Missing"
C10-2144,D07-1110,0,0.257706,"Missing"
C10-2144,W09-2904,0,0.0872671,"Missing"
C10-2144,J90-1003,0,\N,Missing
C10-2144,resnik-1998-parallel,0,\N,Missing
C10-2144,J93-1007,0,\N,Missing
C10-2144,W04-0412,0,\N,Missing
C10-2144,W07-1106,0,\N,Missing
C10-2144,J03-3002,0,\N,Missing
C10-2144,W04-0404,0,\N,Missing
C10-2144,W06-1204,0,\N,Missing
C10-2144,P07-2045,0,\N,Missing
C10-2144,W09-2907,0,\N,Missing
C10-2144,J00-2004,0,\N,Missing
C10-2144,P99-1068,0,\N,Missing
C10-2144,D11-1077,1,\N,Missing
C10-2144,N10-1029,0,\N,Missing
C10-2144,D11-1034,1,\N,Missing
C10-2144,W05-0603,0,\N,Missing
C10-2144,2005.mtsummit-posters.11,0,\N,Missing
C10-2144,1999.mtsummit-1.79,0,\N,Missing
D07-1046,P06-1084,0,0.269338,"periment with more sophisticated methods for combining simple classifiers to induce a coherent prediction. 2 Methodology For training and evaluation, we use a corpus of approximately 90,000 word tokens, consisting of newspaper texts, which was automatically analyzed using HAMSAH and then manually annotated (Elhadad et al., 2005). Annotation consists simply of selecting the correct analysis produced by the analyzer, or an indication that no such analysis exists. When the analyzer does not produce the correct analysis, it is added manually. This is the exact setup of the experiments reported by Adler and Elhadad (2006). Table 2 lists some statistics of the corpus, and a histogram of analyses is given in Table 3. Table 4 lists the distribution of POS in the corpus. Tokens Types Tokens with no correct analysis Tokens with no analysis Degree of ambiguity 89347 23947 8218 130 2.64 Table 2: Statistics of training corpus # analyses 1 2 3 4 5 6 # tokens 38468 15480 11194 9934 5341 3472 # analyses 7 8 9 10 11 >12 # tokens 1977 1309 785 622 238 397 POS Noun Punctuation Proper Noun Verb Preposition Adjective Participle Pronoun Adverb Conjunction Numeral Quantifier Negation Interrogative Prefix Interjection Foreign Mo"
D07-1046,W05-0706,0,0.102036,"Missing"
D07-1046,W04-3246,1,0.854937,"ar functions over the feature space. SNoW has already been used successfully as the learning vehicle in a large collection of natural language related tasks and compared favorably with other classifiers (Punyakanok and Roth, 2001; Florian, 2002). Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation values of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. Following Daya et al. (2004) and Habash and 442 Rambow (2005), we approach the problem of morphological disambiguation as a complex classification task. We train a classifier for each of the attributes that can contribute to the disambiguation of the analyses produced by HAMSAH (e.g., POS, tense, state). Each classifier predicts a small set of possible values and hence can be highly accurate. In particular, the basic classifiers do not suffer from problems of data sparseness. Of course, each simple classifier cannot fully disambiguate the output of HAMSAH, but it does induce a ranking on the analyses (see Table 6 below f"
D07-1046,W02-2010,0,0.0165891,"ng environment, with winnow as the update rule (using perceptron yielded very similar results). SNoW is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over the feature space. SNoW has already been used successfully as the learning vehicle in a large collection of natural language related tasks and compared favorably with other classifiers (Punyakanok and Roth, 2001; Florian, 2002). Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation values of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. Following Daya et al. (2004) and Habash and 442 Rambow (2005), we approach the problem of morphological disambiguation as a complex classification task. We train a classifier for each of the attributes that can contribute to the disambiguation of the anal"
D07-1046,P05-1071,0,0.039518,"7 97.66 — 95.91 95.97 1.47 Table 6: Accuracy of the simple classifiers: ERR is reduction in error rate, compared with the baseline pared with the baseline. 4 All words Unseen words Combination of Classifiers Given a set of simple classifiers, we now investigate various ways for combining their predictions. These predictions may be contradicting (for example, the POS classifier can predict ‘noun’ while the tense classifier predicts ‘past’), and we use the constraints imposed by the morphological analyzer to enforce a consistent analysis. First, we define a na¨ıve combination along the lines of Habash and Rambow (2005). The scores assigned by the simple classifiers (except segmentation, for which we use the baseline) to each analysis are accumulated, and the score of the complete analysis is their sum (experiments with different weights to the various classifiers proved futile). Even after the combination, the remaining level of ambiguity is 1.05; in ambiguous cases back off to the baseline analysis, and then choose at random one of the topranking analyses. The result of the combination is shown in Table 7. 444 baseline 86.11 67.53 classifier 90.26 78.52 ERR 29.88 33.85 Table 7: Results of the na¨ıve combin"
D07-1046,J95-3004,0,0.718061,"Missing"
D07-1046,D07-1022,0,\N,Missing
D07-1046,A00-2013,0,\N,Missing
D07-1046,A92-1018,0,\N,Missing
D07-1046,P06-3009,0,\N,Missing
D07-1046,J95-4004,0,\N,Missing
D07-1046,P08-1083,0,\N,Missing
D07-1046,P08-1043,0,\N,Missing
D07-1046,J08-3005,1,\N,Missing
D07-1046,P98-2186,0,\N,Missing
D07-1046,C98-2181,0,\N,Missing
D07-1046,adler-etal-2008-tagging,0,\N,Missing
D11-1034,N09-4002,0,0.0362236,"manually translated to the target language. Corroborating established observations of Translation Studies, we demonstrate that the latter are significantly better predictors of translated sentences than the former, and hence fit the reference set better. Furthermore, translated texts yield better language models for statistical machine translation than original texts. 1 Introduction Statistical machine translation (MT) uses large target language models (LMs) to improve the fluency of generated texts, and it is commonly assumed that for constructing language models, “more data is better data” (Brants and Xu, 2009). Not all data, however, are created the same. In this work we explore the differences between LMs compiled from texts originally written in the target language and LMs compiled from translated texts. The motivation for our work stems from much research in Translation Studies that suggests that original texts are significantly different from translated ones in various aspects (Gellerstam, 1986). Recently, corpus-based computational analysis corroborated this observation, and Kurokawa et al. (2009) apply it to statistical machine translation, showing that for an English-to-French MT system, a t"
D11-1034,W04-3250,0,0.103385,"te tuning (Och, 2003), and another set of 1,000 sentences is randomly used for evaluation. Each system is built and tuned with six different LMs: MIX, O-based and four T-based (Section 3.2). We use BLEU (Papineni et al., 2002) to evaluate translation quality. The results are listed in Table 10. These results are consistent: the translatedfrom-source systems outperform all other systems; mixture models come second; and systems that use original English LMs always perform worst. We test the statistical significance of differences between various MT systems using the bootstrap resampling method (Koehn, 2004). In all experiments, the best system (translatedfrom-source LM) is significantly better than all 370 DE to EN LM BLEU MIX 21.95 O-EN 21.35 T-DE 22.42 T-FR 21.47 T-IT 21.79 T-NL 21.59 FR to EN LM BLEU MIX 25.43 O-EN 24.85 T-DE 25.03 T-FR 25.91 T-IT 25.44 T-NL 25.17 IT to EN LM BLEU MIX 26.79 O-EN 25.69 T-DE 25.86 T-FR 26.56 T-IT 27.28 T-NL 25.77 NL to EN LM BLEU MIX 25.17 O-EN 24.46 T-DE 25.12 T-FR 24.79 T-IT 24.93 T-NL 25.73 Table 10: Machine translation with various LMs other systems (p < 0.05); (even more) significantly better than the O-EN system (p < 0.01); and the mixture systems are sig"
D11-1034,D07-1091,0,0.0398548,"-DE 22.42 T-FR 21.47 T-IT 21.79 T-NL 21.59 FR to EN LM BLEU MIX 25.43 O-EN 24.85 T-DE 25.03 T-FR 25.91 T-IT 25.44 T-NL 25.17 IT to EN LM BLEU MIX 26.79 O-EN 25.69 T-DE 25.86 T-FR 26.56 T-IT 27.28 T-NL 25.77 NL to EN LM BLEU MIX 25.17 O-EN 24.46 T-DE 25.12 T-FR 24.79 T-IT 24.93 T-NL 25.73 Table 10: Machine translation with various LMs other systems (p < 0.05); (even more) significantly better than the O-EN system (p < 0.01); and the mixture systems are significantly better than the O-EN systems (p < 0.01). We also construct a Hebrew-to-English MT system using Moses’ factored translation model (Koehn and Hoang, 2007). Every token in the training corpus is represented as two factors: surface form and lemma. Moreover, the Hebrew input is fully segmented. The system is built and tuned with O- and T-based LMs. Table 11 depicts the performance of the systems. The T-based LM yields a statistically better BLEU score than the O-based system. LM O-based LM T-based LM BLEU 11.98 12.57 p-value 0.012 Table 11: Hebrew-to-English MT results The LMs used in the above experiments are small. We now want to assess whether the benefits of using translated LMs carry over to scenarios where large original corpora exist. We bu"
D11-1034,N03-1017,0,0.0406914,"Missing"
D11-1034,P07-2045,0,0.0105103,"Missing"
D11-1034,2009.mtsummit-papers.9,0,0.441062,"exts, and it is commonly assumed that for constructing language models, “more data is better data” (Brants and Xu, 2009). Not all data, however, are created the same. In this work we explore the differences between LMs compiled from texts originally written in the target language and LMs compiled from translated texts. The motivation for our work stems from much research in Translation Studies that suggests that original texts are significantly different from translated ones in various aspects (Gellerstam, 1986). Recently, corpus-based computational analysis corroborated this observation, and Kurokawa et al. (2009) apply it to statistical machine translation, showing that for an English-to-French MT system, a translation model trained on an English-translated-to363 French parallel corpus is better than one trained on French-translated-to-English texts. Our research question is whether a language model compiled from translated texts may similarly improve the results of machine translation. We test this hypothesis on several translation tasks, where the target language is always English. For each language pair we build two English language models from two types of corpora: texts originally written in Engl"
D11-1034,P03-1021,0,0.0915664,"Missing"
D11-1034,P02-1040,0,0.0857165,"xample, we test how well a LM trained on Frenchto-English-translated texts fits the German-toEnglish reference set; and how well a LM trained on German-to-English-translated texts fits the French-to-English reference set. Finally, for the third hypothesis, we use these LMs for statistical MT (SMT). For each language pair we build several SMT systems. All systems use a translation model extracted from a parallel corpus which is oblivious to the direction of the translation; and one of the abovementioned LMs. Then, we compare the translation quality of these systems in terms of the BLEU metric (Papineni et al., 2002). 3.2 Language Models In all the experiments, we use SRILM (Stolcke, 2002) to train 4-gram language models (with the default backoff model) from various corpora. Our main corpus is Europarl (Koehn, 2005), specifically portions collected over years 1996 to 1999 and 2001 to 2009. This is a large multilingual corpus, containing sentences translated from several European languages. However, it is organized as a collection of bilingual corpora rather than as a single multilingual one, and it is hard to identify sentences that are translated to several languages. We therefore treat each bilingual su"
D11-1034,W00-1308,0,0.12,"Missing"
D11-1034,tsvetkov-wintner-2010-automatic,1,0.801255,"Missing"
D11-1034,C08-1118,0,0.538323,"Missing"
D11-1034,J00-4006,0,\N,Missing
D11-1034,W10-0701,0,\N,Missing
D11-1034,W07-0733,0,\N,Missing
D11-1034,P11-1132,1,\N,Missing
D11-1034,P02-1038,0,\N,Missing
D11-1034,2005.mtsummit-papers.11,0,\N,Missing
D11-1034,E12-1026,1,\N,Missing
D11-1034,P05-1045,0,\N,Missing
D11-1034,W11-2107,0,\N,Missing
D11-1077,W03-1809,0,0.0384817,"ituents to freely inflect while restricting (or preventing) the inflection of other constituents. In some cases MWEs may allow constituents to undergo non-standard morphological inflections that 836 Shuly Wintner Department of Computer Science University of Haifa shuly@cs.haifa.ac.il they would not undergo in isolation. Syntactically, some MWEs behave like words while other are phrases; some occur in one rigid pattern (and a fixed order), while others permit various syntactic transformations. Semantically, the compositionality of MWEs is gradual, ranging from fully compositional to idiomatic (Bannard et al., 2003). Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing applications. Correct handling of MWEs has been proven beneficial for various applications, including information retrieval, building ontologies, text alignment, and machine translation. We propose a novel architecture for identifying MWEs of various types and syntactic categories in monolingual corpora. Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of all types by focusing on the general idiosyncratic properties of MWEs r"
D11-1077,W07-1101,0,0.018996,"lds a significant improvement in performance over using pure frequency. Several works address the lexical fixedness or syntactic fixedness of (certain types of) MWEs in order to extract them from texts. An expression is considered lexically fixed if replacing any of its constituents by a semantically (and syntactically) similar word generally results in an invalid or literal expression. Syntactically fixed expressions prohibit (or restrict) syntactic variation. For example, Van de Cruys and Villada Moir´on (2007) use lexical fixedness to extract Dutch Verb-Noun idiomatic combinations (VNICs). Bannard (2007) uses syntactic fixedness to identify English VNICs. Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson, 2006). While these approaches are in line with ours, they require lexical semantic resources (e.g., a database that determines semantic similarity among words) and syntactic resources (parsers) that are unavailable for Hebrew (and many other languages). Our approach only requires morphological processing and a bilingual dictionary, which are more readil"
D11-1077,W02-1801,0,0.245988,"r future research. 1 To facilitate readability we use a transliteration of Hebrew using Roman characters; the letters used, in Hebrew lexicographic order, are abgdhwzxTiklmns‘pcqrˇst. 837 2 Related Work Early approaches to MWEs identification concentrated on their collocational behavior (Church and Hanks, 1990). Pecina (2008) compares 55 different association measures in ranking German AdjN and PP-Verb collocation candidates. He shows that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. Other results (Chang et al., 2002; Villavicencio et al., 2007) suggest that some collocation measures (especially PMI and Log-likelihood) are superior to others for identifying MWEs. Soon, however, it became clear that mere cooccurrence measurements are not enough to identify MWEs, and their linguistic properties should be exploited as well (Piao et al., 2005). Hybrid methods that combine word statistics with linguistic information exploit morphological, syntactic and semantic idiosyncrasies to extract idiomatic MWEs. Ramisch et al. (2008) evaluate a number of association measures on the task of identifying English Verb-Parti"
D11-1077,J90-1003,0,0.177306,"Missing"
D11-1077,E06-1043,0,0.0144426,"ly fixed if replacing any of its constituents by a semantically (and syntactically) similar word generally results in an invalid or literal expression. Syntactically fixed expressions prohibit (or restrict) syntactic variation. For example, Van de Cruys and Villada Moir´on (2007) use lexical fixedness to extract Dutch Verb-Noun idiomatic combinations (VNICs). Bannard (2007) uses syntactic fixedness to identify English VNICs. Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson, 2006). While these approaches are in line with ours, they require lexical semantic resources (e.g., a database that determines semantic similarity among words) and syntactic resources (parsers) that are unavailable for Hebrew (and many other languages). Our approach only requires morphological processing and a bilingual dictionary, which are more readilyavailable for several languages. Note also that these approaches target a specific syntactic construction, whereas ours is adequate for various types of MWEs. Several properties of Hebrew MWEs are described by Al-Haj (2010); Al-Haj and Wintner (2010"
D11-1077,W10-3712,0,0.0295339,"and of course semantic (Al-Haj, 2010). They are also extremely diverse: for example, on the semantic dimension alone, MWEs cover an entire spectrum, ranging from frozen, fixed idioms to free combinations of words (Bannard et al., 2003). Such a complex task calls for a combination of multiple approaches, and much research indeed suggests “hybrid” approaches to MWE identification 2 For simplicity, we focus on bi-grams of tokens (MWEs of length 2) in this work; the methodology, however, is easily extensible to longer n-grams. (Duan et al., 2009; Weller and Fritzinger, 2010; Ramisch et al., 2010; Hazelbeck and Saito, 2010). We believe that Bayesian Networks provide an optimal architecture for expressing various pieces of knowledge aimed at MWE identification, for the following reasons (Heckerman, 1995): • In contrast to many other classification methods, BN can learn (and express) causal relationships between features. This facilitates better understanding of the problem domain. • BN can encode not only statistical data, but also prior domain knowledge and human intuitions, in the form of interdependencies among features. We do indeed use this possibility here. 3.2 Linguistically-motivated Features Based on the"
D11-1077,J03-1002,0,0.0138554,"tner, 2010); similarly, when one of the constituents of a MWE is a conjunction, the entire expression is very likely to be frozen. 841 where k is the number of nodes in the BN (other than Xmwe ) and pai is the set of parents of Xi . 3.4 Automatic Generation of Training Data For training we need samples of positive and negative instances of MWEs, each associated with a vector of the values of all features discussed in Section 3.2. We generate this training material automatically. We use a small Hebrew-English bilingual corpus (Tsvetkov and Wintner, 2010a). We word-align the corpus with Giza++ (Och and Ney, 2003), and then apply the (completely unsupervised) algorithm of Tsvetkov and Wintner (2010b), which extracts MWE candidates from the aligned corpus and re-ranks them using statistics computed from a large monolingual corpus. The core idea behind this method is that MWEs tend to be translated in nonliteral ways; in a parallel corpus, words that are 1:1 aligned typically indicate literal translations and are hence unlikely constituents of MWEs. The result is a set of 134,001 Hebrew bi-gram types (from the bilingual corpus), classified as either 1:1 aligned (implying they are likely not MWEs) or unal"
D11-1077,N03-2027,0,0.441083,"Missing"
D11-1077,tsvetkov-wintner-2010-automatic,1,0.755171,"the SVM reflect several morphological and morpho-syntactic properties of such constructions. The resulting classifier performs much better than a na¨ıve baseline, reducing over one third of the errors. We rely on some of these insights, as we implement more of the linguistic properties of MWEs. Again, our methodology is not limited to a particular construction: indeed, we demonstrate that our general methodology, trained on automaticallygenerated, general training data, performs almost as well as the noun-noun-specific approach of Al-Haj and Wintner (2010) on the very same dataset. Recently, Tsvetkov and Wintner (2010b) introduced a general methodology for extracting MWEs from bilingual corpora, and applied it to Hebrew. The results were a highly accurate set of Hebrew MWEs, of various types, along with their English translations. A major limitation of this work is that it can only be used to identify MWEs in the bilingual corpus, and is thus limited in its scope. We use this methodology to extract both positive and negative instances for our training set in the current work; but we extrapolate the results much further by extending the method to monolingual corpora, which are typically much larger than bil"
D11-1077,C10-2144,1,0.908499,"the SVM reflect several morphological and morpho-syntactic properties of such constructions. The resulting classifier performs much better than a na¨ıve baseline, reducing over one third of the errors. We rely on some of these insights, as we implement more of the linguistic properties of MWEs. Again, our methodology is not limited to a particular construction: indeed, we demonstrate that our general methodology, trained on automaticallygenerated, general training data, performs almost as well as the noun-noun-specific approach of Al-Haj and Wintner (2010) on the very same dataset. Recently, Tsvetkov and Wintner (2010b) introduced a general methodology for extracting MWEs from bilingual corpora, and applied it to Hebrew. The results were a highly accurate set of Hebrew MWEs, of various types, along with their English translations. A major limitation of this work is that it can only be used to identify MWEs in the bilingual corpus, and is thus limited in its scope. We use this methodology to extract both positive and negative instances for our training set in the current work; but we extrapolate the results much further by extending the method to monolingual corpora, which are typically much larger than bil"
D11-1077,W07-1104,0,0.0318665,"Missing"
D11-1077,D07-1110,0,0.136173,"To facilitate readability we use a transliteration of Hebrew using Roman characters; the letters used, in Hebrew lexicographic order, are abgdhwzxTiklmns‘pcqrˇst. 837 2 Related Work Early approaches to MWEs identification concentrated on their collocational behavior (Church and Hanks, 1990). Pecina (2008) compares 55 different association measures in ranking German AdjN and PP-Verb collocation candidates. He shows that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. Other results (Chang et al., 2002; Villavicencio et al., 2007) suggest that some collocation measures (especially PMI and Log-likelihood) are superior to others for identifying MWEs. Soon, however, it became clear that mere cooccurrence measurements are not enough to identify MWEs, and their linguistic properties should be exploited as well (Piao et al., 2005). Hybrid methods that combine word statistics with linguistic information exploit morphological, syntactic and semantic idiosyncrasies to extract idiomatic MWEs. Ramisch et al. (2008) evaluate a number of association measures on the task of identifying English Verb-Particle Constructions and German"
D11-1077,N07-1051,0,\N,Missing
D11-1077,J93-1007,0,\N,Missing
D11-1077,W04-0412,0,\N,Missing
D11-1077,J93-2003,0,\N,Missing
D11-1077,W07-1106,0,\N,Missing
D11-1077,W06-1203,0,\N,Missing
D11-1077,W04-0404,0,\N,Missing
D11-1077,W06-1204,0,\N,Missing
D11-1077,P05-2003,0,\N,Missing
D11-1077,W03-1812,0,\N,Missing
D11-1077,W09-2907,0,\N,Missing
D11-1077,D11-1067,0,\N,Missing
D11-1077,W04-0406,0,\N,Missing
D11-1077,N10-1029,0,\N,Missing
D11-1077,bouamor-etal-2012-identifying,0,\N,Missing
D11-1077,2005.mtsummit-posters.11,0,\N,Missing
D11-1077,kirschenbaum-wintner-2010-general,1,\N,Missing
D11-1077,C10-1002,1,\N,Missing
D11-1077,P00-1056,0,\N,Missing
D18-1393,N15-1171,0,0.0790227,"Missing"
D18-1393,P15-2072,0,0.263398,"inally, unlike classification tasks where each article is assigned to a single category, most articles employ a variety of frames (Ghanem and McCombs, 2001). Recent work has attempted to address these conceptual challenges by defining broad framing categories. The Policy Frames Codebook defines a set of 15 frames (one of which is “Other”) commonly used in media for a broad range of issues (Boydstun et al., 2013). In a follow-up work, the authors use these frames to build The Media Frames Corpus (MFC), which consists of articles related to 3 issues: immigration, tobacco, and same-sex marriage (Card et al., 2015). About 11,900 articles are hand-annotated with frames: annotators highlight spans of text related to each frame in the codebook and assign a single “primary frame” to each document. However, the MFC, like other prior framing analyses, relies heavily on labor-intensive manual annotations. The primary automated methods have relied on probabilistic topic models (Tsur et al., 2015; Boydstun et al., 2013; Nguyen et al., 2013; Roberts et al., 2013). Although topic models can show researchers what themes are salient in a corpus, they have two main drawbacks: they tend to be corpus-specific and hard"
D18-1393,D16-1148,0,0.270422,"omated methods have relied on probabilistic topic models (Tsur et al., 2015; Boydstun et al., 2013; Nguyen et al., 2013; Roberts et al., 2013). Although topic models can show researchers what themes are salient in a corpus, they have two main drawbacks: they tend to be corpus-specific and hard to interpret. Topics discovered in one corpus are likely not relevant to a different corpus, and it is difficult to compare the outputs of topic models run on different corpora. Other automated framing analyses have used the annotations of the Media Frame Corpus to predict the primary frame of articles (Card et al., 2016; Ji and Smith, 2017), or used classifiers to identify language specifically related to framing (Baumer et al., 2015). Importantly, all of these methods focus exclusively on English data sets. While unsupervised methods like topic models can be applied to other languages, any supervised method requires annotated data, which does not exist in other languages. 3.2 Framing Analysis Methodology Our goal is to develop a method that is easy to interpret and applicable across-languages. In order to ensure our analysis is interpretable, we ground our method using the annotations of the Media Frames Co"
D18-1393,J90-1003,0,0.15656,"Missing"
D18-1393,P17-1092,0,0.195841,"relied on probabilistic topic models (Tsur et al., 2015; Boydstun et al., 2013; Nguyen et al., 2013; Roberts et al., 2013). Although topic models can show researchers what themes are salient in a corpus, they have two main drawbacks: they tend to be corpus-specific and hard to interpret. Topics discovered in one corpus are likely not relevant to a different corpus, and it is difficult to compare the outputs of topic models run on different corpora. Other automated framing analyses have used the annotations of the Media Frame Corpus to predict the primary frame of articles (Card et al., 2016; Ji and Smith, 2017), or used classifiers to identify language specifically related to framing (Baumer et al., 2015). Importantly, all of these methods focus exclusively on English data sets. While unsupervised methods like topic models can be applied to other languages, any supervised method requires annotated data, which does not exist in other languages. 3.2 Framing Analysis Methodology Our goal is to develop a method that is easy to interpret and applicable across-languages. In order to ensure our analysis is interpretable, we ground our method using the annotations of the Media Frames Corpus. However, becaus"
D18-1393,D17-1292,0,0.014393,"d terrorist will simply be replaced “and everything will start afresh - explosions, chases, roundups...unlucky businessmen, successful terrorists”. The articles 3577 portray the U.S. as an unsafe place to live, making Russia seem like a preferable home. A third type of article also presents Russia as safe by downplaying U.S. military threat: “the missile defense system of the USA does not pose a real threat to Russia’s strategic nuclear forces.” or describing the growth of Russian technology compared to ‘impotent’ American counterparts. to sentiment of social media posts (Nardo et al., 2016). Kang et al. (2017) combine text and Granger causality for a different task: automatically explaining causes of time series events. Our study differs from past work in that we reverse the direction: rather than using news articles to model changes in economic data, we use economic data to show changes in news articles. 6 7 Related Work Most studies on Russian media manipulation focus on state-owned television networks, such as Channel 1 and RT. Strategies identified in these outlets include spreading confusion (Paul and Matthews, 2016) and “selection attribution”, in which negative economic events are attributed"
D18-1393,P15-1157,0,0.566412,"uarterly or yearly level. 3572 of examples. Finally, we use this method to contextualize strategies of media manipulation in the Izvestia corpus. 3.1 Background on Framing Analyses While agenda-setting broadly refers to what topics a text covers, framing refers to which attributes of those topics are highlighted. Several aspects of framing make the concept difficult to analyze. First, just defining framing has been “notoriously slippery” (Boydstun et al., 2013). Frames can occur as stock phrases, i.e. “death tax” vs. “estate tax”, but they can also occur as broader associations or sub-topics (Tsur et al., 2015; McCombs, 2002). Frames also need to be distinguished from similar concepts, like sentiment and stance. For example, the same frame can be used to take different stances on an issue: one politician might argue that immigrants boost the economy by starting new companies that create jobs, while another might argue that immigrants hurt the economy by taking jobs away from U.S. citizens (Baumer et al., 2015; Gamson and Modigliani, 1989). Finally, unlike classification tasks where each article is assigned to a single category, most articles employ a variety of frames (Ghanem and McCombs, 2001). Re"
D18-1395,N12-1033,0,0.0482534,"Missing"
D18-1395,K15-1010,0,0.0245783,"intner, 2015; Volansky et al., 2015). Other features are content based; most obviously, token n-grams, but also character n-grams (Avner et al., 2016). We expect content-based features to be highly accurate but also highly domaindependent, and in the case of our dataset, topicdependent. Content-independent features are expected to be weaker yet more robust. In addition, we used features that reflect spelling and grammar errors. We assume that native and nonnative speakers make different kinds of errors in English, and that the errors of nonnatives may reveal traces of their L1 (Kochmar, 2011; Berzak et al., 2015). Aiming to enhance the quality of classification we exploited properties that can be induced from conversational networks. We hypothesize that native speakers of the same language tend to interact more with each other (than with speakers of other languages). We hypothesize further that native speakers post more than nonnatives, and hence we defined user centrality measures that reflect that. We also hypothesize that native speakers’ posts tend to be more spontaneous, coherent and clear, thereby drawing more attention. To reflect that, we counted the number of comments, upvotes and down-votes"
D18-1395,P11-1132,0,0.100117,"and Jones, 2001) typically rely on lexical and grammatical characteristics that reflect influences of L1 on L2. We used such features, but augmented them by features that can be induced from the network structure of social media outlets (Jurgens et al., 2015). To the best of our knowledge, ours is the first work that extensively exploits social network properties for the task of NLI. Our work is also inspired by research on the (related but different) task of identifying translations (Baroni and Bernardini, 2006; Rabinovich and Wintner, 2015; Volansky et al., 2015) and their source language (Koppel and Ordan, 2011; Rabinovich et al., 2017). 3 3.1 Experimental setup Dataset Reddit is an online community consisting of thousands of forums for news aggregation, content rating, and discussions. Content entries are organized by areas of interest called subreddits, ranging from main forums that receive much attention to smaller 3592 ones that foster discussion on niche areas. Subreddit topics include news, science, arts, and many others. An increasing body of work has used Reddit data for social media analysis (Jurgens et al., 2015; Newell et al., 2016, and many more). We used the Reddit dataset released by R"
D18-1395,W17-1222,0,0.0652572,"Missing"
D18-1395,P16-1176,1,0.889736,"nd content-independent features; and we evaluate our classifiers both in and outside of the domain of training. Several works address social aspects of social networks, and in particular identify “influential” users (Ghosh and Lerman, 2010; Trusov et al., 2010; Afrasiabi Rad and Benyoucef, 2011). Network structure has been shown to be useful in other tasks of user profiling, such as geolocation (Jurgens et al., 2015). Our design of the social network features (Section 3.5.4) are motivated by these works. Works that aim to distinguish between native and nonnative authors (Bergsma et al., 2012; Rabinovich et al., 2016; Tomokiyo and Jones, 2001) typically rely on lexical and grammatical characteristics that reflect influences of L1 on L2. We used such features, but augmented them by features that can be induced from the network structure of social media outlets (Jurgens et al., 2015). To the best of our knowledge, ours is the first work that extensively exploits social network properties for the task of NLI. Our work is also inspired by research on the (related but different) task of identifying translations (Baroni and Bernardini, 2006; Rabinovich and Wintner, 2015; Volansky et al., 2015) and their source"
D18-1395,P17-1049,1,0.849945,"Missing"
D18-1395,Q18-1024,1,0.825811,"Missing"
D18-1395,Q15-1030,1,0.93534,"e and nonnative authors (Bergsma et al., 2012; Rabinovich et al., 2016; Tomokiyo and Jones, 2001) typically rely on lexical and grammatical characteristics that reflect influences of L1 on L2. We used such features, but augmented them by features that can be induced from the network structure of social media outlets (Jurgens et al., 2015). To the best of our knowledge, ours is the first work that extensively exploits social network properties for the task of NLI. Our work is also inspired by research on the (related but different) task of identifying translations (Baroni and Bernardini, 2006; Rabinovich and Wintner, 2015; Volansky et al., 2015) and their source language (Koppel and Ordan, 2011; Rabinovich et al., 2017). 3 3.1 Experimental setup Dataset Reddit is an online community consisting of thousands of forums for news aggregation, content rating, and discussions. Content entries are organized by areas of interest called subreddits, ranging from main forums that receive much attention to smaller 3592 ones that foster discussion on niche areas. Subreddit topics include news, science, arts, and many others. An increasing body of work has used Reddit data for social media analysis (Jurgens et al., 2015; New"
D18-1395,tenfjord-etal-2006-ask,0,0.115985,"Missing"
D18-1395,N01-1031,0,0.202039,"Missing"
D18-1395,W07-0602,0,0.158989,"Missing"
D18-1395,W13-1736,1,0.803253,"discover the (first) closest correction for each word marked as incorrect. Based on this correction, we defined several edit-distance-based features using Python’s Python-Levenshtein extension. Edit distance Assuming that nonnative speakers will make more spelling errors than natives, we used the average Levenshtein distance between the original word and the correction offered by the spell checker, for all words in a chunk, as a feature. Spelling errors Again, we assume that the spelling errors that nonnatives make may reflect properties of their L1; this has already been shown for learners (Tsvetkov et al., 2013). Using the edit distance between a mis-spelled word w in a text chunk, marked by the spell checker, and its suggested correction c, we extract insertions, deletions and substitutions that yield c from w and use them as features. For each chunk, the value of this feature is the number of occurrences of each substitution (a character pair), insertions, and deletions in the chunk. We only used the top-400 most frequent substitutions. We initially classified spelling errors as contentindependent features, assuming that they would reflect transfer of linguistic phenomena from L1. However, having a"
D18-1395,N18-2096,0,0.0444952,"Missing"
D18-1395,W15-0614,0,0.0705727,"Missing"
D18-1395,U09-1008,0,0.139253,"Missing"
D18-1395,D11-1148,0,0.0871975,"Missing"
D18-1395,W17-1201,0,0.0611523,"Missing"
D19-1425,W19-5201,0,0.0146563,"arners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative words for baseline and LO - TOP -50 follow a similar pattern as the ones obtained with attention scores. In line with results in Table 7, salient words for ALT"
D19-1425,D18-1002,0,0.413214,"to discover stylistic features present in the input that are indicative of the author’s L1. However, a model trained to predict L1 is likely to predict that a person is, say, a native Greek speaker, if the texts authored by that person mention Greece, because the training data exhibits such topical correlations (§2). This problem is the focus of our work, and we address it in two steps. First, we introduce a novel method for representing latent confounds. Recent relevant work in the area of domain adaptation (Ganin et al., 2016) and deconfounding for text classification (Pryzant et al., 2018; Elazar and Goldberg, 2018) assumes that the set of confounds is known a priori, and their values are given as part of the training data. This is an unrealistic setting that limits the applicability of such models in real world scenarios. In contrast, we introduce a new method, based on log-odds ratio with Dirichlet prior (Monroe et al., 2008), for identifying and representing latent confounds as probability distributions (§3). Second, we propose a novel alternating learning procedure with multiple adversarial discriminators, inspired by adversarial learning (Goodfellow et al., 2014), that demotes latent confounds and r"
D19-1425,D18-1395,1,0.84017,"itialized at random and learned) and passed these embeddings to a bidirectional LSTM encoder (one layer for each direction) with attention (h(x); Pryzant et al., 2018). Each LSTM layer had a hidden dimension of 128. We used two layered feed forward networks with a tanh activation function in the middle layer (of size 256), followed by a softmax in the final layer, as c(.) and adv(.). 5.3 Baselines We consider several baselines that are intended to capture the stylistic features of the texts, explicitly avoiding content. 4157 Linear classifier with content-independent features (LR) Replicating Goldin et al. (2018), we trained a logistic regression classifier with three types of features: function words, POS trigrams, and sentence length, all of which are reflective of the style of writing. We deliberately avoided using content features (e.g., word frequencies). Classification with no adversary on masked texts (LO - TOP -K) We mask the top-K words (based on log-odds scores) in both the train and the test sets (as in §2); we train the classification model again without training adv(.). After masking the top words, we expect patterns of writing style (and, therefore, L1) to become more apparent. Adversari"
D19-1425,N19-1357,0,0.0385091,"The distribution of determiners is also a challenge for nonnatives, and 4159 the correct usage of the in particular is quite hard for learners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative words for baseline and LO -"
D19-1425,S18-2005,0,0.0313045,"on about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content.1 1 Introduction Text classification systems based on neural networks are biased towards learning frequent spurious correlations in the training data that may be confounds in the actual classification task (Leino et al., 2019). A major challenge in building such systems is to discover features that are not just correlated with the signals in the training data, but are true indicators of these signals, and therefore generalize well. For example, Kiritchenko and Mohammad (2018) found that sentiment analysis systems implicitly overfit to demographic confounds, systematically amplifying the intensity ratings of posts 1 The code is available at: https://github.com/ Sachin19/adversarial-classify written by women. Zhao et al. (2017) showed that visual semantic role labeling models implicitly capture actions stereotypically associated with men or women (e.g., women are cooking and men are fixing a faucet), and in cases of higher model uncertainty assign stereotypical labels to actions and objects, thereby amplifying social biases found in the training data. We focus on th"
D19-1425,N18-1146,0,0.149534,"e aim of this task is to discover stylistic features present in the input that are indicative of the author’s L1. However, a model trained to predict L1 is likely to predict that a person is, say, a native Greek speaker, if the texts authored by that person mention Greece, because the training data exhibits such topical correlations (§2). This problem is the focus of our work, and we address it in two steps. First, we introduce a novel method for representing latent confounds. Recent relevant work in the area of domain adaptation (Ganin et al., 2016) and deconfounding for text classification (Pryzant et al., 2018; Elazar and Goldberg, 2018) assumes that the set of confounds is known a priori, and their values are given as part of the training data. This is an unrealistic setting that limits the applicability of such models in real world scenarios. In contrast, we introduce a new method, based on log-odds ratio with Dirichlet prior (Monroe et al., 2008), for identifying and representing latent confounds as probability distributions (§3). Second, we propose a novel alternating learning procedure with multiple adversarial discriminators, inspired by adversarial learning (Goodfellow et al., 2014), that de"
D19-1425,Q18-1024,1,0.909604,"ral Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4153–4163, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Note that these two proposals are taskindependent and can be extended to a vast array of text classification tasks where confounding factors are not known a priori. For concreteness, however, we evaluate our approach on the task of L1ID (§5). We experiment with two different datasets: a small corpus of student written essays (Malmasi et al., 2017) and a large and noisy dataset of Reddit posts (Rabinovich et al., 2018). We show that classifiers trained on these datasets without any intervention learn spurious topical correlations that are not indicative of style, and that our proposed deconfounded classifiers alleviate this problem (§6). We present an analysis of the features discovered after demoting these confounds in §7. The main contributions of this work are: 1. We introduce a novel method for representing and identifying variables which are confounds in text classification tasks. 2. We propose a classification model and an algorithm aimed at learning textual representations that are invariant to the c"
D19-1425,P18-2005,0,0.061756,"Missing"
D19-1425,W17-5007,0,0.1775,"4153 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4153–4163, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Note that these two proposals are taskindependent and can be extended to a vast array of text classification tasks where confounding factors are not known a priori. For concreteness, however, we evaluate our approach on the task of L1ID (§5). We experiment with two different datasets: a small corpus of student written essays (Malmasi et al., 2017) and a large and noisy dataset of Reddit posts (Rabinovich et al., 2018). We show that classifiers trained on these datasets without any intervention learn spurious topical correlations that are not indicative of style, and that our proposed deconfounded classifiers alleviate this problem (§6). We present an analysis of the features discovered after demoting these confounds in §7. The main contributions of this work are: 1. We introduce a novel method for representing and identifying variables which are confounds in text classification tasks. 2. We propose a classification model and an algorit"
D19-1425,K17-1018,0,0.170131,"Missing"
D19-1425,P19-1282,1,0.840889,"y L2, English included). The distribution of determiners is also a challenge for nonnatives, and 4159 the correct usage of the in particular is quite hard for learners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative wor"
D19-1425,P14-1017,0,0.0780898,"Missing"
D19-1425,W13-1706,0,0.0450352,"Missing"
D19-1425,W07-0602,0,0.0433251,"Missing"
D19-1425,L18-1445,1,0.878121,"Missing"
D19-1425,D19-1002,0,0.0360982,"rminers is also a challenge for nonnatives, and 4159 the correct usage of the in particular is quite hard for learners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative words for baseline and LO - TOP -50 follow a similar pat"
D19-1425,U09-1008,0,0.0905086,"Missing"
D19-1425,D11-1148,0,0.0606105,"Missing"
D19-1425,D17-1323,0,0.0536227,"orrelations in the training data that may be confounds in the actual classification task (Leino et al., 2019). A major challenge in building such systems is to discover features that are not just correlated with the signals in the training data, but are true indicators of these signals, and therefore generalize well. For example, Kiritchenko and Mohammad (2018) found that sentiment analysis systems implicitly overfit to demographic confounds, systematically amplifying the intensity ratings of posts 1 The code is available at: https://github.com/ Sachin19/adversarial-classify written by women. Zhao et al. (2017) showed that visual semantic role labeling models implicitly capture actions stereotypically associated with men or women (e.g., women are cooking and men are fixing a faucet), and in cases of higher model uncertainty assign stereotypical labels to actions and objects, thereby amplifying social biases found in the training data. We focus on the task of native language identification (L1ID), which aims at automatically identifying the native language (L1) of an individual based on their language production in a second language (L2, English in this work). The aim of this task is to discover styl"
E09-1050,P02-1051,0,0.477858,"Missing"
E09-1050,2004.tmi-1.1,1,0.896304,", 2006), to improve the TTT identifier with respect to identifying multitoken expressions which should be transliterated. In addition, we would like to take into account the weights of the different transliteration options when deciding which to select in the translation. Finally, we are interested in applying this module to different language pairs, especially ones with limited resources. Target rabindranath essaouira chamaephyte baudelaire lorre hollies venom Table 5: Incorrect transliteration examples We provide an external evaluation in the form of BLEU (Papineni et al., 2001) and Meteor (Lavie et al., 2004b) scores for SMT with and without the transliteration module. When integrating our method in the MT system we use the best transliteration options as obtained when using the re-ranking procedure described in section 5.3. The translation results for all conditions are presented in Table 6, compared to the basic MT system where no transliteration takes place. Using the transliteration module yields a statistically significant improvement in METEOR scores (p &lt; 0.05). METEOR scores are most relevant since they reflect improvement in recall. The MT system cannot yet take into consideration the wei"
E09-1050,lavie-etal-2004-significance,0,0.0823231,", 2006), to improve the TTT identifier with respect to identifying multitoken expressions which should be transliterated. In addition, we would like to take into account the weights of the different transliteration options when deciding which to select in the translation. Finally, we are interested in applying this module to different language pairs, especially ones with limited resources. Target rabindranath essaouira chamaephyte baudelaire lorre hollies venom Table 5: Incorrect transliteration examples We provide an external evaluation in the form of BLEU (Papineni et al., 2001) and Meteor (Lavie et al., 2004b) scores for SMT with and without the transliteration module. When integrating our method in the MT system we use the best transliteration options as obtained when using the re-ranking procedure described in section 5.3. The translation results for all conditions are presented in Table 6, compared to the basic MT system where no transliteration takes place. Using the transliteration module yields a statistically significant improvement in METEOR scores (p &lt; 0.05). METEOR scores are most relevant since they reflect improvement in recall. The MT system cannot yet take into consideration the wei"
E09-1050,J93-2003,0,0.0114693,"s a phrase since it is formed as a Hebrew noun construct. Recognizing them as transliteration candidates is crucial for improving the performance of MT systems. 5 Target language model How to transliterate Once a token is classified as a TTT, it is sent to the transliteration module. Our approach handles the transliteration task as a case of phrase-based SMT, based on the noisy channel model. According to this model, when translating a string f in the source language into the target language, a string eˆ is chosen out of all target language strings e if it has the maximal probability given f (Brown et al., 1993): eˆ = arg max {P r(e|f )} e = arg max {P r(f |e) · P r(e)} e where P r(f |e) is the translation model and P r(e) is the target language model. In phrase-based translation, f is divided into phrases f¯1 . . . f¯I , and each source phrase f¯i is translated into target phrase e¯i according to a phrase translation model. Target phrases may then be reordered using a distortion model. We use SMT for transliteration; this approach views transliteration pairs as aligned sentences and $:SH|S|CH such that SH, S or CH are possible candidates for matching the Hebrew letter $. Both Hebrew and English titl"
E09-1050,2001.mtsummit-papers.68,0,0.0225025,"m parts of Web 1T (Brants and Franz, 2006), to improve the TTT identifier with respect to identifying multitoken expressions which should be transliterated. In addition, we would like to take into account the weights of the different transliteration options when deciding which to select in the translation. Finally, we are interested in applying this module to different language pairs, especially ones with limited resources. Target rabindranath essaouira chamaephyte baudelaire lorre hollies venom Table 5: Incorrect transliteration examples We provide an external evaluation in the form of BLEU (Papineni et al., 2001) and Meteor (Lavie et al., 2004b) scores for SMT with and without the transliteration module. When integrating our method in the MT system we use the best transliteration options as obtained when using the re-ranking procedure described in section 5.3. The translation results for all conditions are presented in Table 6, compared to the basic MT system where no transliteration takes place. Using the transliteration module yields a statistically significant improvement in METEOR scores (p &lt; 0.05). METEOR scores are most relevant since they reflect improvement in recall. The MT system cannot yet"
E09-1050,P08-2014,0,0.180551,"Missing"
E09-1050,P08-1045,0,0.117,"Missing"
E09-1050,W98-1005,0,0.562741,"Missing"
E09-1050,itai-etal-2006-computational,1,0.83773,"hough many of them have homographs that can be translated. Foreign words, which retain the sound patterns of their original language with no semantic translation involved, are also (back-)transliterated. On the other hand, names of countries may be subject to translation or transliteration, as demonstrated in the following examples: crpt tsarfat ‘France’ sprd sfarad ‘Spain’ qwngw kongo ‘Congo’ We use information obtained from POS tagging (Bar-Haim et al., 2008) to address the problem of identifying TTTs. Each token is assigned a POS and is additionally marked if it was not found in a lexicon (Itai et al., 2006). As a baseline, we tag for transliteration Out Of Vocabulary (OOV) tokens. Resources and Methodology Our work consists of of two sub-tasks: Identifying TTTs and then transliterating them. Specifically, we use the following resources for this work: For 436 these classes is to be learned and used by the classifier. One class classification utilizes only target class objects to learn a function that distinguishes them from any other objects. SVM (Support Vector Machine) (Vapnik, 1995) is a classification technique which finds a linear separating hyperplane with maximal margins between data insta"
E09-1050,P07-1015,0,0.114442,"Missing"
E09-1050,P97-1017,0,0.258665,"Missing"
E09-1050,C02-1099,0,\N,Missing
E09-1050,W03-1508,0,\N,Missing
E09-1050,W06-1630,0,\N,Missing
E09-1050,P02-1040,0,\N,Missing
E09-1050,N07-1046,0,\N,Missing
E09-1050,P07-2045,0,\N,Missing
E09-1050,D08-1037,0,\N,Missing
E09-1050,J03-1002,0,\N,Missing
E09-1050,W99-0613,0,\N,Missing
E12-1026,D11-1033,0,0.0677904,"han when the T → S portion is used for training the translation model. We indeed replicate these results here (Section 3), and view them as a baseline. Additionally, we show that the T → S portion is also important for machine translation and thus should not be discarded. Using information-theory measures, and in particular cross-entropy, we gain statistically significant improvements in translation quality beyond the results of Kurokawa et al. (2009). Furthermore, we eliminate the need to (manually or automatically) detect the direction of translation of the parallel corpus. Lembersky et al. (2011) also investigate the relations between translationese and machine translation. Focusing on the language model (LM), they show that LMs trained on translated texts yield better translation quality than LMs compiled from original texts. They also show that perplexity is a good discriminator between original and translated texts. Our current work is closely related to research in domain-adaptation. In a typical domain adaptation scenario, a system is trained on a large corpus of “general” (out-of-domain) training material, with a small portion of in-domain training texts. In our case, the transl"
E12-1026,W07-0717,0,0.027768,"ults qualitatively, showing that SMT systems adapted to translationese tend to produce more coherent and fluent outputs than the baseline systems. An additional advantage of our approach is that it does not require an annotation of the translation direction of the parallel corpus. It is completely generic and can be applied to any language pair, domain or corpus. This work can be extended in various directions. We plan to further explore the use of two phrase tables, one for each direction-determined subset of the parallel corpus. Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al., 2006). We also plan to upweight the S → T subset of the parallel corpus and train a single phrase table on the concatenated corpus. Finally, we intend to extend this work by combining the translation-model adaptation we present here with the language-model adaptation suggested by Lembersky et al. (2011) in a unified system that is more tuned to generating translationese. Acknowledgments We are grateful to Cyril Goutte, George Foster and Pierre Isabelle for providing us with an annotated version of the Hansard corpus. This resear"
E12-1026,D07-1103,0,0.0695765,"Missing"
E12-1026,W04-3250,0,0.353825,"Missing"
E12-1026,2005.mtsummit-papers.11,0,0.120046,"Missing"
E12-1026,P07-2045,0,0.00962577,"Missing"
E12-1026,2009.mtsummit-papers.7,0,0.0825149,"at automatic classification can identify them with very high accuracy (van Halteren, 2008; Baroni and Bernardini, 2006; Ilisei et al., 2010; Koppel and Ordan, 2011). Contemporary Statistical Machine Translation (SMT) systems use parallel corpora to train translation models that reflect source- and targetlanguage phrase correspondences. Typically, SMT systems ignore the direction of translation used to produce those corpora. Given the unique properties of translationese, however, it is reasonable to assume that this direction may affect the quality of the translation. Recently, Kurokawa et al. (2009) showed that this is indeed the case. They train a system to translate between French and English (and vice versa) using a Frenchtranslated-to-English parallel corpus, and then an English-translated-to-French one. They find that in translating into French the latter parallel corpus yields better results, whereas for translating into English it is better to use the former. Usually, of course, the translation direction of a parallel corpus is unknown. Therefore, Kurokawa et al. (2009) train an SVM-based classifier to predict which side of a bi-text is the origin and which one is the translation,"
E12-1026,P11-1132,1,0.228881,"lects both artifacts of the translation process and traces of the original language from which the texts were translated. Among the better-known properties of translationese are simplification and explicitation (Baker, 1993, 1995, 1996): translated texts tend to be shorter, to have lower type/token ratio, and to use certain discourse markers more frequently than original texts. Incidentally, translated texts are so markedly different from original ones that automatic classification can identify them with very high accuracy (van Halteren, 2008; Baroni and Bernardini, 2006; Ilisei et al., 2010; Koppel and Ordan, 2011). Contemporary Statistical Machine Translation (SMT) systems use parallel corpora to train translation models that reflect source- and targetlanguage phrase correspondences. Typically, SMT systems ignore the direction of translation used to produce those corpora. Given the unique properties of translationese, however, it is reasonable to assume that this direction may affect the quality of the translation. Recently, Kurokawa et al. (2009) showed that this is indeed the case. They train a system to translate between French and English (and vice versa) using a Frenchtranslated-to-English paralle"
E12-1026,2009.mtsummit-papers.9,0,0.198515,"at automatic classification can identify them with very high accuracy (van Halteren, 2008; Baroni and Bernardini, 2006; Ilisei et al., 2010; Koppel and Ordan, 2011). Contemporary Statistical Machine Translation (SMT) systems use parallel corpora to train translation models that reflect source- and targetlanguage phrase correspondences. Typically, SMT systems ignore the direction of translation used to produce those corpora. Given the unique properties of translationese, however, it is reasonable to assume that this direction may affect the quality of the translation. Recently, Kurokawa et al. (2009) showed that this is indeed the case. They train a system to translate between French and English (and vice versa) using a Frenchtranslated-to-English parallel corpus, and then an English-translated-to-French one. They find that in translating into French the latter parallel corpus yields better results, whereas for translating into English it is better to use the former. Usually, of course, the translation direction of a parallel corpus is unknown. Therefore, Kurokawa et al. (2009) train an SVM-based classifier to predict which side of a bi-text is the origin and which one is the translation,"
E12-1026,D11-1034,1,0.662151,"slation quality than when the T → S portion is used for training the translation model. We indeed replicate these results here (Section 3), and view them as a baseline. Additionally, we show that the T → S portion is also important for machine translation and thus should not be discarded. Using information-theory measures, and in particular cross-entropy, we gain statistically significant improvements in translation quality beyond the results of Kurokawa et al. (2009). Furthermore, we eliminate the need to (manually or automatically) detect the direction of translation of the parallel corpus. Lembersky et al. (2011) also investigate the relations between translationese and machine translation. Focusing on the language model (LM), they show that LMs trained on translated texts yield better translation quality than LMs compiled from original texts. They also show that perplexity is a good discriminator between original and translated texts. Our current work is closely related to research in domain-adaptation. In a typical domain adaptation scenario, a system is trained on a large corpus of “general” (out-of-domain) training material, with a small portion of in-domain training texts. In our case, the transl"
E12-1026,P10-2041,0,0.0834686,"omain adaptation scenario, a system is trained on a large corpus of “general” (out-of-domain) training material, with a small portion of in-domain training texts. In our case, the translation model is trained on a large parallel corpus, of which some (generally unknown) subset is “in-domain” (S → T ), and some other subset is “out-of-domain” (T → S). Most existing adaptation methods focus on selecting in-domain data from a general domain corpus. In particular, perplexity is used to score the sentences in the general-domain corpus according to an in-domain language model. Gao et al. (2002) and Moore and Lewis (2010) apply this method to language modeling, while Foster 256 et al. (2010) and Axelrod et al. (2011) use it on the translation model. Moore and Lewis (2010) suggest a slightly different approach, using crossentropy difference as a ranking function. Domain adaptation methods are usually applied at the corpus level, while we focus on an adaptation of the phrase table used for SMT. In this sense, our work follows Foster et al. (2010), who weigh out-of-domain phrase pairs according to their relevance to the target domain. They use multiple features that help distinguish between phrase pairs in the ge"
E12-1026,P03-1021,0,0.0276576,"Missing"
E12-1026,P00-1056,0,0.169903,"Missing"
E12-1026,P02-1040,0,0.0943047,"ignored; second, we show that explicit information on the direction of translation of the parallel corpus, whether manually-annotated or machine-learned, is not mandatory. This is achieved by casting the problem in the framework of domain adaptation: we use domain-adaptation techniques to direct the SMT system toward producing output that better reflects the properties of translationese. We show that SMT systems adapted to translationese produce better translations than vanilla systems trained on exactly the same resources. We confirm these findings using an automatic evaluation metric, BLEU (Papineni et al., 2002), as well as through a qualitative analysis of the results. Our departure point is the results of Kurokawa et al. (2009), which we successfully replicate in Section 3. First (Section 4), we explain why translation quality improves when the parallel corpus is translated in the ‘right’ direction. We do so by showing that the subset of the corpus that was translated in the direction of the translation task (the ‘right’ direction, henceforth source-to-target, or S → T ) yields phrase tables that are better suited for translation of the original language than the subset translated in the reverse di"
E12-1026,C08-1118,0,0.472039,"Missing"
E12-1026,D10-1044,0,\N,Missing
E12-1026,W11-2132,0,\N,Missing
E17-1101,P14-2048,0,0.0178262,"al traits in translationese A large body of previous research has established that translations constitute an autonomic language variety: a special dialect of the target language, often re1075 ferred to as translationese (Gellerstam, 1986). Recent corpus-based investigations of translationese demonstrated that originals and translations are distinguishable by means of supervised and unsupervised classification (Baroni and Bernardini, 2006; Volansky et al., 2015; Rabinovich and Wintner, 2015). The identification of machinetranslated text has also been proven an easy task (Arase and Zhou, 2013; Aharoni et al., 2014). Previous work has investigated how gender artifacts are carried over into human translation in the context of social and gender studies, as well as cultural transfer (Simon, 2003; Von Flotow, 2010). Shlesinger et al. (2009) conducted a computational study exploring the implications of the translator’s gender on the final product. They conclude that “the computer could not be trained to accurately predict the gender of the translator”. Preservation of authorial style in literary translations was studied by Lynch (2014), identifying Russian authors of translated English literature, by using (s"
E17-1101,P13-1157,0,0.0255352,"erent domains. Authorial traits in translationese A large body of previous research has established that translations constitute an autonomic language variety: a special dialect of the target language, often re1075 ferred to as translationese (Gellerstam, 1986). Recent corpus-based investigations of translationese demonstrated that originals and translations are distinguishable by means of supervised and unsupervised classification (Baroni and Bernardini, 2006; Volansky et al., 2015; Rabinovich and Wintner, 2015). The identification of machinetranslated text has also been proven an easy task (Arase and Zhou, 2013; Aharoni et al., 2014). Previous work has investigated how gender artifacts are carried over into human translation in the context of social and gender studies, as well as cultural transfer (Simon, 2003; Von Flotow, 2010). Shlesinger et al. (2009) conducted a computational study exploring the implications of the translator’s gender on the final product. They conclude that “the computer could not be trained to accurately predict the gender of the translator”. Preservation of authorial style in literary translations was studied by Lynch (2014), identifying Russian authors of translated English"
E17-1101,D11-1120,0,0.0188716,"experimental results of personalized MT models. Gender classification A large body of research has been devoted to isolating distinguishing traits of male and female linguistic variations, both theoretically and empirically. Apart from content, male and female speech has been shown to exhibit stylistic and syntactic differences. Several studies demonstrated that literary texts and blog posts produced by male and female writers can be distinguished by means of automatic classification, using (content-independent) function words and ngrams of POS tags (Koppel et al., 2002; Schler et al., 2006; Burger et al., 2011). Although the tendencies of individual word usage are a subject of controversy, distributions of word categories across male and female English speech is nearly consensual: pronouns and verbs are more frequent in female texts, while nouns and numerals are more typical to male productions. Newman et al. (2008) carried out a comprehensive empirical study corroborating these findings with large and diverse datasets. However, little effort has been dedicated to investigating the variation of individual markers of demographic traits across different languages. Johannsen et al. (2015) conducted a l"
E17-1101,2012.eamt-1.60,0,0.00644988,"English EP data distributions across two dimensions: gender (left) and trans. status (right). TED talks (transcripts and translations): a collection of texts from a completely different genre, where demographic traits may manifest differently. Testing the potential benefits of personalized SMT models on these two very diverse datasets allows us to examine the robustness of our approach. We used the TED gender-annotated data from Mirkin et al. (2015).8 This corpus contains annotation of the speaker’s gender included in the English-French corpus of the IWSLT 2014 Evaluation Campaign’s MT track (Cettolo et al., 2012). We annotated 68 additional talks from the development and test sets of IWSLT 2014, 2015 and 2016. Using the full set, we split the TED parallel corpora by gender to obtain sub-corpora of 140K and 43K sentence pairs for male and female speakers, respectively. The sizes of the datasets used for training, tuning and testing of SMT models are shown in Table 3. Relatively large test sets are used for evaluation of the MT results for the sake of reliable per-outcome gender classification (§4.1). Although the size of the training/tuning/test sets in either direction for any language-pair is the sam"
E17-1101,P96-1041,0,0.0831521,"cifically, 370 chunks for each gender. Aiming to abstract away from content and capture instead stylistic and syntactic characteristics, we used as our feature set the combination of function words (FW)10 and (the top-1,000 most frequent) POS-trigrams. We employ 10-fold crossvalidation for evaluation of classification accuracy. 4.4 Downloaded from http://cm.xrce.xerox.com/. SMT setting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference translation. 5 Personalized SMT models In order to investigate and improve gender traits transfer in MT, we devise and experiment with gender-aware SMT models. We demonstr"
E17-1101,P13-2061,0,0.0121704,"n (MT) Virtually no previous work in MT takes into account personal traits. State-of-the-art MT systems are built from examples of translations, where the general assumption is that the more data available to train models, the better, and a single model is usually produced. Exceptions to this assumption revolve around work on domain adaption, where systems are customized by using data that comes from a particular text domain (Hasler et al., 2014; Cuong and Sima’an, 2015); and work on data cleaning, where spurious data is removed from the training set to ensure the quality of the final models (Cui et al., 2013; Simard, 2014). Personal traits, sometimes well marked in the translation examples, are therefore not explicitly addressed. Learning from different, sometimes conflicting writing styles can hinder model performance and lead to translations that are unfaithful to the source text. Focusing on reader preferences, Mirkin and Meunier (2015) used a collaborative filtering approach from recommender systems, where a user’s preferred translation is predicted based on the preferences of similar users. However, the user preferences in this case refer to the overall choice between MT systems of a specifi"
E17-1101,N15-1043,0,0.0370924,"Missing"
E17-1101,W14-3358,0,0.0213763,"opic classification (Hovy, 2015), very little attention has been paid to translation. We provide here a brief summary of research relevant to our work. Machine translation (MT) Virtually no previous work in MT takes into account personal traits. State-of-the-art MT systems are built from examples of translations, where the general assumption is that the more data available to train models, the better, and a single model is usually produced. Exceptions to this assumption revolve around work on domain adaption, where systems are customized by using data that comes from a particular text domain (Hasler et al., 2014; Cuong and Sima’an, 2015); and work on data cleaning, where spurious data is removed from the training set to ensure the quality of the final models (Cui et al., 2013; Simard, 2014). Personal traits, sometimes well marked in the translation examples, are therefore not explicitly addressed. Learning from different, sometimes conflicting writing styles can hinder model performance and lead to translations that are unfaithful to the source text. Focusing on reader preferences, Mirkin and Meunier (2015) used a collaborative filtering approach from recommender systems, where a user’s preferred tra"
E17-1101,W11-2123,0,0.00745722,"t al., 2009). In all experiments we used (the maximal) equal amount of data from each category (M and F), specifically, 370 chunks for each gender. Aiming to abstract away from content and capture instead stylistic and syntactic characteristics, we used as our feature set the combination of function words (FW)10 and (the top-1,000 most frequent) POS-trigrams. We employ 10-fold crossvalidation for evaluation of classification accuracy. 4.4 Downloaded from http://cm.xrce.xerox.com/. SMT setting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference translation. 5 Personalized SMT models In order to inv"
E17-1101,P15-1073,0,0.0123096,"se of the target language in both manual and automatic translation products. The main contributions of this paper are thus: (i) a new parallel corpus annotated with gender and age information, (ii) an in-depth assessment of the projection of gender traits in manual and automatic translation, and (iii) experiments showing that gender-personalized SMT systems better project gender traits while maintaining translation quality. 2 Related work While modeling of demographic traits has been proven beneficial in some NLP tasks such as sentiment analysis (Volkova et al., 2013) or topic classification (Hovy, 2015), very little attention has been paid to translation. We provide here a brief summary of research relevant to our work. Machine translation (MT) Virtually no previous work in MT takes into account personal traits. State-of-the-art MT systems are built from examples of translations, where the general assumption is that the more data available to train models, the better, and a single model is usually produced. Exceptions to this assumption revolve around work on domain adaption, where systems are customized by using data that comes from a particular text domain (Hasler et al., 2014; Cuong and S"
E17-1101,K15-1011,0,0.0117666,"er et al., 2006; Burger et al., 2011). Although the tendencies of individual word usage are a subject of controversy, distributions of word categories across male and female English speech is nearly consensual: pronouns and verbs are more frequent in female texts, while nouns and numerals are more typical to male productions. Newman et al. (2008) carried out a comprehensive empirical study corroborating these findings with large and diverse datasets. However, little effort has been dedicated to investigating the variation of individual markers of demographic traits across different languages. Johannsen et al. (2015) conducted a large-scale study on linguistic variation over age and gender across multiple languages in a social media domain. They showed that gender differences captured by shallow syntactic features were preserved across languages, when examined by linguistic categories. However, they did not study the distribution of individual gender markers across domains and languages. Our work demonstrates that while marker categories are potentially preserved, individual words typical to male and female language vary across languages and, more prominently, across different domains. Authorial traits in"
E17-1101,P07-2045,0,0.00586138,"hine classifiers with the default linear kernel (Hall et al., 2009). In all experiments we used (the maximal) equal amount of data from each category (M and F), specifically, 370 chunks for each gender. Aiming to abstract away from content and capture instead stylistic and syntactic characteristics, we used as our feature set the combination of function words (FW)10 and (the top-1,000 most frequent) POS-trigrams. We employ 10-fold crossvalidation for evaluation of classification accuracy. 4.4 Downloaded from http://cm.xrce.xerox.com/. SMT setting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference tra"
E17-1101,C14-1037,0,0.025702,"ted text has also been proven an easy task (Arase and Zhou, 2013; Aharoni et al., 2014). Previous work has investigated how gender artifacts are carried over into human translation in the context of social and gender studies, as well as cultural transfer (Simon, 2003; Von Flotow, 2010). Shlesinger et al. (2009) conducted a computational study exploring the implications of the translator’s gender on the final product. They conclude that “the computer could not be trained to accurately predict the gender of the translator”. Preservation of authorial style in literary translations was studied by Lynch (2014), identifying Russian authors of translated English literature, by using (shallow) stylistic and syntactic features. Forsyth and Lam (2014) investigated authorial discriminability in translations of French originals into English, inspecting two distinct human translations, as well as automatic translation of the same sources. Our work, to the best of our knowledge, is the first to automatically identify speaker gender in manual, and more prominently, automatic translations over multiple domains and languagepairs, examining distribution of gender markers in source and target languages. 3 Europa"
E17-1101,P14-5010,0,0.00145718,"e employ 10-fold crossvalidation for evaluation of classification accuracy. 4.4 Downloaded from http://cm.xrce.xerox.com/. SMT setting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference translation. 5 Personalized SMT models In order to investigate and improve gender traits transfer in MT, we devise and experiment with gender-aware SMT models. We demonstrate that despite their simplicity, these models lead to better preservation of gender traits, while not harming the general quality of the translations. 9 10 8 Classification setting http://nlp.stanford.edu/software/tagger.shtml We used the lists of fu"
E17-1101,D15-1238,1,0.726418,"omain adaption, where systems are customized by using data that comes from a particular text domain (Hasler et al., 2014; Cuong and Sima’an, 2015); and work on data cleaning, where spurious data is removed from the training set to ensure the quality of the final models (Cui et al., 2013; Simard, 2014). Personal traits, sometimes well marked in the translation examples, are therefore not explicitly addressed. Learning from different, sometimes conflicting writing styles can hinder model performance and lead to translations that are unfaithful to the source text. Focusing on reader preferences, Mirkin and Meunier (2015) used a collaborative filtering approach from recommender systems, where a user’s preferred translation is predicted based on the preferences of similar users. However, the user preferences in this case refer to the overall choice between MT systems of a specific reader, rather than a choice based on traits of the writer. Mirkin et al. (2015) motivated the need for personalization of MT models by showing that automatic translation does not preserve demographic and psychometric traits. They suggested treating the problem as a domain adaptation one, but did not provide experimental results of pe"
E17-1101,D15-1130,1,0.810976,"emographic trait (partially due to the absence of parallel data annotated for other traits). We evaluate the accuracy of automatic gender classification on original texts, on their manual translations and on their automatic translations generated through statistical machine translation (SMT). We show that while gender has a strong signal in originals, this signal is obfuscated in human and machine translation. Surprisingly, determining gender over manual translation is even harder than over SMT; this may be an artifact of the translation process itself or the human translators involved in it. Mirkin et al. (2015) were the first to show that authorial gender signals tend to vanish through both manual and automatic translation, using a small TED talks dataset. We use their data and extend it with a version of Europarl that we annotated with age and gender (§3). Furthermore, we conduct experiments with two language pairs, in both directions (§4). We also adopt a different classification methodology based on the finding that the translation process itself has a stronger signal than the author’s gender (§4.1). We then move on to assessing gender traits in SMT (§5). Since SMT systems typically do not take p"
E17-1101,P03-1021,0,0.0201708,"instead stylistic and syntactic characteristics, we used as our feature set the combination of function words (FW)10 and (the top-1,000 most frequent) POS-trigrams. We employ 10-fold crossvalidation for evaluation of classification accuracy. 4.4 Downloaded from http://cm.xrce.xerox.com/. SMT setting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference translation. 5 Personalized SMT models In order to investigate and improve gender traits transfer in MT, we devise and experiment with gender-aware SMT models. We demonstrate that despite their simplicity, these models lead to better preservatio"
E17-1101,P02-1040,0,0.126265,"ting We trained phrase-based SMT models with Moses (Koehn et al., 2007), an open source SMT system. KenLM (Heafield, 2011) was used for language modeling. We trained 5-gram language models with Kneser-Ney smoothing (Chen and Goodman, 1996). The models were tuned using Minimum Error Rate Tuning (MERT) (Och, 2003). Our preprocessing included cleaning (removal of empty, long and misaligned sentences), tokenization and punctuation normalization. The Stanford tokenizer (Manning et al., 2014) was used for tokenization and standard Moses scripts were used for other preprocessing tasks. We used BLEU (Papineni et al., 2002) to evaluate MT quality against one reference translation. 5 Personalized SMT models In order to investigate and improve gender traits transfer in MT, we devise and experiment with gender-aware SMT models. We demonstrate that despite their simplicity, these models lead to better preservation of gender traits, while not harming the general quality of the translations. 9 10 8 Classification setting http://nlp.stanford.edu/software/tagger.shtml We used the lists of function words available at https://code.google.com/archive/p/stop-words. 1078 dataset EP TED language-pair en-fr & fr-en en-de & de-"
E17-1101,Q15-1030,1,0.83956,", individual words typical to male and female language vary across languages and, more prominently, across different domains. Authorial traits in translationese A large body of previous research has established that translations constitute an autonomic language variety: a special dialect of the target language, often re1075 ferred to as translationese (Gellerstam, 1986). Recent corpus-based investigations of translationese demonstrated that originals and translations are distinguishable by means of supervised and unsupervised classification (Baroni and Bernardini, 2006; Volansky et al., 2015; Rabinovich and Wintner, 2015). The identification of machinetranslated text has also been proven an easy task (Arase and Zhou, 2013; Aharoni et al., 2014). Previous work has investigated how gender artifacts are carried over into human translation in the context of social and gender studies, as well as cultural transfer (Simon, 2003; Von Flotow, 2010). Shlesinger et al. (2009) conducted a computational study exploring the implications of the translator’s gender on the final product. They conclude that “the computer could not be trained to accurately predict the gender of the translator”. Preservation of authorial style in"
E17-1101,2005.mtsummit-papers.11,0,0.00482647,"Lam (2014) investigated authorial discriminability in translations of French originals into English, inspecting two distinct human translations, as well as automatic translation of the same sources. Our work, to the best of our knowledge, is the first to automatically identify speaker gender in manual, and more prominently, automatic translations over multiple domains and languagepairs, examining distribution of gender markers in source and target languages. 3 Europarl with demographic info We created a resource1 based on the parallel corpus of the European Parliament (Europarl) Proceedings (Koehn, 2005). More specifically, we utilize the extension of its en-fr and en-de parallel versions (Rabinovich et al., 2015), where each sentence-pair is annotated with speaker name, the original language the sentence was uttered in, and the date of the corresponding session protocol. To extend speaker information with demographic properties, we used the Europarl website’s MEP information pages2 and applied a procedure of gender and age identification, as further detailed in §3.1. The final resource comprises en-fr and en-de parallel bilingual corpora where metadata of mem1 2 Available at http://cl.haifa."
E17-1101,J14-2003,0,0.00533819,"on, without harming the quality of the translation, thereby creating more personalized machine translation systems. 1 Raj Nath Patel C-DAC Mumbai Gulmohar Cross Road No. 9, Juhu Mumbai-400049, India patelrajnath@gmail.com Introduction Among many factors that mold the makeup of a text, gender and other authorial traits play a major role in our perception of the content we face. Many studies have shown that these traits can be identified by means of automatic classification methods. Classical examples include gender identification (Koppel et al., 2002), and authorship attribution and profiling (Seroussi et al., 2014). Most research, however, addressed texts in a single language, typically English. We investigate a related but different question: we are interested in understanding what happens to personality and demographic textual markers during the translation process. It is generally agreed that good translation goes beyond transformation of the original content, by preserving more subtle and implicit characteristics inferred by author’s personality, as well as era, geography, and various cultural and sociological aspects. In this work we explore whether translations preserve the stylistic characteristi"
E17-1101,2014.amta-researchers.6,0,0.0399392,"o previous work in MT takes into account personal traits. State-of-the-art MT systems are built from examples of translations, where the general assumption is that the more data available to train models, the better, and a single model is usually produced. Exceptions to this assumption revolve around work on domain adaption, where systems are customized by using data that comes from a particular text domain (Hasler et al., 2014; Cuong and Sima’an, 2015); and work on data cleaning, where spurious data is removed from the training set to ensure the quality of the final models (Cui et al., 2013; Simard, 2014). Personal traits, sometimes well marked in the translation examples, are therefore not explicitly addressed. Learning from different, sometimes conflicting writing styles can hinder model performance and lead to translations that are unfaithful to the source text. Focusing on reader preferences, Mirkin and Meunier (2015) used a collaborative filtering approach from recommender systems, where a user’s preferred translation is predicted based on the preferences of similar users. However, the user preferences in this case refer to the overall choice between MT systems of a specific reader, rathe"
E17-1101,D13-1187,0,0.0264891,"traits of the original language overshadow those of the target language in both manual and automatic translation products. The main contributions of this paper are thus: (i) a new parallel corpus annotated with gender and age information, (ii) an in-depth assessment of the projection of gender traits in manual and automatic translation, and (iii) experiments showing that gender-personalized SMT systems better project gender traits while maintaining translation quality. 2 Related work While modeling of demographic traits has been proven beneficial in some NLP tasks such as sentiment analysis (Volkova et al., 2013) or topic classification (Hovy, 2015), very little attention has been paid to translation. We provide here a brief summary of research relevant to our work. Machine translation (MT) Virtually no previous work in MT takes into account personal traits. State-of-the-art MT systems are built from examples of translations, where the general assumption is that the more data available to train models, the better, and a single model is usually produced. Exceptions to this assumption revolve around work on domain adaption, where systems are customized by using data that comes from a particular text dom"
itai-etal-2006-computational,2004.tmi-1.1,1,\N,Missing
itai-etal-2006-computational,W05-0702,1,\N,Missing
J02-3005,C88-2121,0,0.157817,"Missing"
J06-1004,W98-1312,0,0.0235909,"scent of Kaminski and Francez (1994) in the sense that it augments finite-state automata with finite memory (registers) in a restricted way, but we avoid the above-mentioned problem. In addition, our model supports a register alphabet that differs from the language alphabet, allowing the information stored in the registers to be more meaningful. Moreover, our transition relation is a more simplified extension of the standard one in FSAs, rendering our model a conservative extension of standard FSAs and allowing simple integration of existing networks with networks based on our model. Finally, Beesley (1998) directly addresses medium-distance dependencies between separated morphemes in words. He proposes a method, called flag diacritics, which adds features to symbols in regular expressions to enforce dependencies between separated parts of a string. The dependencies are forced by different kinds of unification actions. In this way, a small amount of finite memory is added, keeping the total size of the network relatively small. Unfortunately, this method is not formally defined, nor are its mathematical and computational properties proved. Furthermore, flag diacritics are manipulated at the leve"
J06-1004,W00-1801,0,0.215096,"llow moving backwards within a string and thus repeat a part of it (to model reduplication). Skip arcs allow moving forwards in a string while suppressing the spell out of some of its letters; self loop arcs model infixation. In Walther (2000b), the above technique is used to describe Temiar Figure 4 4-tape representation for the Hebrew word htpqdut. 53 Computational Linguistics Volume 32, Number 1 Figure 5 4-tape automaton for circumfixation example. reduplication, but no complexity analysis of the model is given. Moreover, this technique does not seem to be able to describe interdigitation. Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output. The compile-replace algorithm facilitates a compact definition of nonconcatenative morphological processes, but since such expressions compile to the na¨ıve networks, no space is saved. Furthermore, this is a compile-time mechanism rather than a theoretical and mathematically founded solution. Other works extend the FS model by enabling some sort of context-sensitivity. Blank (1985, 1989) presents a model, called Register Vector Grammar, introducing"
J06-1004,J94-3001,0,0.802125,"are presently not optimized for describing this kind of phenomena. We first define the model and discuss its mathematical and computational properties. Then, we provide an extended regular language whose expressions denote FSRAs. Finally, we exemplify the utility of the model by providing several examples of complex morphological and phonological phenomena, which are elegantly implemented with FSRAs. 1. Introduction Finite-state (FS) technology has been considered adequate for describing the morphological processes of the world’s languages since the pioneering works of Koskenniemi (1983) and Kaplan and Kay (1994). Several toolboxes provide extended regular expression description languages and compilers of the expressions to finite-state automata (FSAs) and transducers (FSTs) (Karttunen et al. 1996; Mohri 1996; van Noord and Gerdemann 2001a). While FS approaches to most natural languages have generally been very successful, it is widely recognized that they are less suitable for non-concatenative phenomena; in particular, FS techniques are assumed not to be able to efficiently account for the non-concatenative word formation processes that Semitic languages exhibit (Lavie et al. 1988). While much of th"
J06-1004,C88-1064,0,0.419477,"ng for the motivating examples. In Section 6 we extend FSRAs to transducers. The model is evaluated through an actual implementation in Section 7. We conclude with suggestions for future research. 2. Related Work In spite of the common view that FS technology is in general inadequate for describing non-concatenative processes, several works address the above-mentioned problems in various ways. We summarize existing approaches in this section. Several works examine the applicability of traditional two-level systems for implementing non-concatenative morphology. Two-Level Morphology was used by Kataja and Koskenniemi (1988) to create a rule system for phonological and morphophonological alternations in Akkadian, accounting for word inflection and regular verbal derivation. As this solution effectively defines lexical representations of word-forms, its main disadvantage is that the final network is the na¨ıve one, suffering from the space complexity problems discussed above. Lavie et al. (1988) examine the applicability of Two4 A complete explanation of the construction can be found in http://www.xrce.xerox.com/competencies/ content-analysis/fsCompiler/fsexamples.html#Add1. 5 Many of the formal proofs and constru"
J06-1004,E87-1002,0,0.284003,"verb inflection as a concatenation of prefix+base+suffix, implementable by the Two-Level model. However, they conclude that “The Two-Level rules are not the natural way to describe . . . verb inflection process. The only alternative choice . . . is to keep all bases . . . it seems wasteful to save all the secondary bases of verbs of the same pattern.” Other works deal with non-concatenative morphology by extending ordinary FSAs without extending their expressivity. The traditional two-level model of Koskenniemi (1983) is expanded into n-tape automata by Kiraz (2000), following the insight of Kay (1987) and Kataja and Koskenniemi (1988). The idea is to use more than two levels of expression: The surface level employs one representation, but the lexical form employs multiple representations (e.g., root, pattern) and therefore can be divided into different levels, one for each representation. Elements that are separated on the surface (such as the root’s consonants) are adjacent on a particular lexical level. For example, to describe circumfixation using this model, a 4-tape automaton of the form surface, PR pattern, circumfix, stem is constructed, so that each word is represented by 4 level"
J06-1004,J00-1006,0,0.674187,"ffixes. They attempt to describe Hebrew verb inflection as a concatenation of prefix+base+suffix, implementable by the Two-Level model. However, they conclude that “The Two-Level rules are not the natural way to describe . . . verb inflection process. The only alternative choice . . . is to keep all bases . . . it seems wasteful to save all the secondary bases of verbs of the same pattern.” Other works deal with non-concatenative morphology by extending ordinary FSAs without extending their expressivity. The traditional two-level model of Koskenniemi (1983) is expanded into n-tape automata by Kiraz (2000), following the insight of Kay (1987) and Kataja and Koskenniemi (1988). The idea is to use more than two levels of expression: The surface level employs one representation, but the lexical form employs multiple representations (e.g., root, pattern) and therefore can be divided into different levels, one for each representation. Elements that are separated on the surface (such as the root’s consonants) are adjacent on a particular lexical level. For example, to describe circumfixation using this model, a 4-tape automaton of the form surface, PR pattern, circumfix, stem is constructed, so tha"
J06-1004,A00-2039,0,0.655753,"by circumfixing roots with the three circumfixes of Example 1. Each arc is attributed with a quadruplet, consisting of four correlating symbols in the four levels. Notice that as in FSAs, the paths encoding the roots are duplicated for each circumfix, so that this automaton is as spaceinefficient as ordinary FSAs. Kiraz (2000) does not discuss the space complexity of this model, but the number of states still seems to increase with the number of roots and patterns. Moreover, the n-tape model requires specification of dependencies between symbols in different levels, which may be non-trivial. Walther (2000a) suggests a solution for describing natural language reduplication using finite-state methods. The idea is to enrich finite-state automata with three new operations: repeat, skip, and self loops. Repeat arcs allow moving backwards within a string and thus repeat a part of it (to model reduplication). Skip arcs allow moving forwards in a string while suppressing the spell out of some of its letters; self loop arcs model infixation. In Walther (2000b), the above technique is used to describe Temiar Figure 4 4-tape representation for the Hebrew word htpqdut. 53 Computational Linguistics Volume"
J06-1004,W00-1802,0,0.0219544,"by circumfixing roots with the three circumfixes of Example 1. Each arc is attributed with a quadruplet, consisting of four correlating symbols in the four levels. Notice that as in FSAs, the paths encoding the roots are duplicated for each circumfix, so that this automaton is as spaceinefficient as ordinary FSAs. Kiraz (2000) does not discuss the space complexity of this model, but the number of states still seems to increase with the number of roots and patterns. Moreover, the n-tape model requires specification of dependencies between symbols in different levels, which may be non-trivial. Walther (2000a) suggests a solution for describing natural language reduplication using finite-state methods. The idea is to enrich finite-state automata with three new operations: repeat, skip, and self loops. Repeat arcs allow moving backwards within a string and thus repeat a part of it (to model reduplication). Skip arcs allow moving forwards in a string while suppressing the spell out of some of its letters; self loop arcs model infixation. In Walther (2000b), the above technique is used to describe Temiar Figure 4 4-tape representation for the Hebrew word htpqdut. 53 Computational Linguistics Volume"
J06-1004,W05-0702,1,0.878938,"Missing"
J06-1004,P00-1025,0,\N,Missing
J08-3005,P01-1005,0,0.00953516,"oW has a number of extensions such as regularization and good treatment of multiclass classiﬁcation. SNoW provides, in addition to classiﬁcation, a reliable conﬁdence in the instance prediction which facilitates its use in an inference algorithm that combines predictors to produce a coherent inference. SNoW has already been used successfully as the learning vehicle in a large collection of natural language related tasks, including part-of-speech tagging, shallow parsing, information extraction tasks, and so forth, and compared favorably with other classiﬁers (Roth 1998; Golding and Roth 1999; Banko and Brill 2001; Punyakanok and 432 Daya, Roth, and Wintner Identifying Semitic Roots Roth 2001; Florian 2002; Punyakanok, Roth, and Yih 2005). The choice of SNoW as the learning algorithm in this work is motivated by its good performance on other, similar tasks and on the availability in SNoW of easy to tune state-of-the-art versions of three linear algorithms (Perceptron, Winnow, and naive Bayes) in a single package. As was shown by Roth (1998), Golding and Roth (1999), and in countless experimental papers thereafter, most algorithms used today, from on-line variations of Winnow and Perceptron to maximum e"
J08-3005,W98-1007,0,0.0383304,"is inserted into the ﬁrst consonantal slot of the pattern, the second ﬁlls the second slot, and the third ﬁlls the last slot. See Shimron (2003) for a survey. We present a machine learning approach, augmented by limited linguistic knowledge, to the problem of identifying the roots of Semitic words. To the best of our knowledge, this is the ﬁrst application of machine learning to this problem, and one of the few attempts to directly address the non-concatenative morphology of Semitic languages using machine learning. Although there exist programs which can extract the roots of words in Arabic (Beesley 1998a, 1998b) and Hebrew (Choueka 1990), they are all dependent on labor-intensive construction of large-scale lexicons which are components of full-scale morphological analyzers. Note that the Arabic morphological analyzer of Buckwalter (2002, software documentation) only uses “word stems—rather than root and pattern morphemes—to identify lexical items.” Buckwalter further notes that “The information on root and pattern morphemes could be added to each stem entry if this were desired.” The challenge of our work is to automate this process, avoiding the bottleneck of having to laboriously list the"
J08-3005,W04-3246,1,0.337041,"Missing"
J08-3005,W01-0502,1,0.874929,"Missing"
J08-3005,W02-2010,0,0.136782,"NoW provides, in addition to classiﬁcation, a reliable conﬁdence in the instance prediction which facilitates its use in an inference algorithm that combines predictors to produce a coherent inference. SNoW has already been used successfully as the learning vehicle in a large collection of natural language related tasks, including part-of-speech tagging, shallow parsing, information extraction tasks, and so forth, and compared favorably with other classiﬁers (Roth 1998; Golding and Roth 1999; Banko and Brill 2001; Punyakanok and 432 Daya, Roth, and Wintner Identifying Semitic Roots Roth 2001; Florian 2002; Punyakanok, Roth, and Yih 2005). The choice of SNoW as the learning algorithm in this work is motivated by its good performance on other, similar tasks and on the availability in SNoW of easy to tune state-of-the-art versions of three linear algorithms (Perceptron, Winnow, and naive Bayes) in a single package. As was shown by Roth (1998), Golding and Roth (1999), and in countless experimental papers thereafter, most algorithms used today, from on-line variations of Winnow and Perceptron to maximum entropy algorithms to SVMs, perform comparably if tuned properly, and the eventual performance"
J08-3005,P05-1071,0,0.0623824,".shtml). The system can be used for practical applications or for scientiﬁc (linguistic) research, and constitutes an important addition to the growing set of resources dedicated to Semitic languages. It is one of the few attempts to directly address the non-concatenative morphology of Semitic languages and extract non-contiguous morphemes from surface forms. As a machine learning application, this work describes a set of experiments in combination of classiﬁers under constraints. The resulting insights can be used for other applications of the same techniques for similar problems (see, e.g., Habash and Rambow 2005). Furthermore, this work demonstrates that providing a data-driven classiﬁer with limited linguistic knowledge signiﬁcantly improves the classiﬁcation results. We focus on Hebrew in the ﬁrst part of this article. After sketching the linguistic data in Section 2 and our methodology in Section 3, we discuss in Section 4 a simple, baseline, learning approach. We then propose several methods for combining the results 2 These results are challenged by Darwish and Oard (2002), who conclude that roots are inferior to character n-grams for this task. 430 Daya, Roth, and Wintner Identifying Semitic Roo"
J08-3005,W05-0639,0,0.045069,"Missing"
J08-3005,P94-1025,0,0.0349979,"Missing"
J08-3005,W03-0419,0,0.0357704,"Missing"
J08-3005,W02-0506,0,\N,Missing
J09-4012,P07-1009,0,\N,Missing
J09-4012,P06-1035,0,\N,Missing
J11-1003,2000.iwpt-1.7,0,0.105868,"Missing"
J11-1003,W02-1502,0,0.109529,"Missing"
J11-1003,I05-2035,0,0.134204,"g et al. (2005) augment LFG with a makeshift signature to allow modular development of untyped uniﬁcation grammars. In addition, they suggest that any development team should agree in advance on the feature space. This work emphasizes the observation that the modularization of the signature is the key for modular development of grammars. However, the proposed solution is ad hoc and cannot be taken seriously as a concept of modularization. In particular, the suggestion for an agreement on the feature space undermines the essence of modular design. To support rapid prototyping of deep grammars, Bender and Flickinger (2005) propose a framework in which the grammar developer can select pre-written grammar fragments, accounting for common linguistic phenomena that vary across languages (e.g., word order, yes–no questions, and sentential negation). The developer can specify how these phenomena are realized in a given language, and a grammar for that language is automatically generated, implementing that particular realization of the phenomenon, integrated with a language-independent grammar core. This framework addresses modularity in the sense that the entire grammar is distributed between several fragments that c"
J11-1003,C96-1034,0,0.617288,"nfrastructure underlying the deﬁnition of modular HPSG discussed earlier (Keselj 2001). Provisions for modularity have also been discussed in the context of tree-adjoining grammars (TAG) (Joshi, Levy, and Takahashi 1975). A wide-coverage TAG may contain hundreds or even thousands of elementary trees, and syntactic structure can be redundantly repeated in many of them (XTAG Research Group 2001; Abeill´e, Candito, and Kinyon 2000). Consequently, maintenance and extension of such grammars is a complex task. To address these issues, several high-level formalisms were developed (VijayShanker 1992; Candito 1996; Duchier and Gardent 1999; Kallmeyer 2001). These formalisms take the metagrammar approach, where the basic units are tree descriptions (i.e., formulas denoting sets of trees) rather than trees. Tree descriptions are constructed by a tree logic and combined through conjunction or inheritance; a module in this approach is merely a tree description, and modules are combined by means of the control logic. When trees are semantic objects, (i.e., the denotation of tree descriptions), there can be various ways to refer to nodes in the trees in order to control the possible combination of grammar mo"
J11-1003,P06-1019,1,0.788523,"ular development of type signatures in the context of both ALE and TRALE (Section 5). We show in Section 6 how our solution complies with the desiderata of Section 1.1, and conclude with directions for future research. 1.1 Motivation The motivation for modular grammar development is straightforward. Like software development, large-scale grammar development is much simpler when the task can be cleanly distributed among different developers, provided that well-deﬁned interfaces govern the interaction among modules. From a theoretical point of view, modularity 1 This article extends and revises Cohen-Sygal and Wintner (2006) and Sygal and Wintner (2008). 30 Sygal and Wintner Modular Typed Uniﬁcation Grammars facilitates the deﬁnition of cleaner semantics for the underlying formalism and the construction of correctness proofs. The engineering beneﬁts of modularity in programming languages are summarized by Mitchell (2003, page 235), and are equally valid for grammar construction: In an effective design, each module can be designed and tested independently. Two important goals in modularity are to allow one module to be written with little knowledge of the code in another module and to allow a module to be redesign"
J11-1003,copestake-flickinger-2000-open,0,0.481852,"Missing"
J11-1003,W02-1506,0,0.0529759,"Missing"
J11-1003,C96-2106,0,0.0920156,"5). For a comprehensive survey, see Bugliesi, Lamma, and Mello (1994). The ‘merge’ operator that we present in Section 2.4.2 is closely related to union operations proposed for logic programming languages. We deﬁne no counterpart of intersection-type operations, although such operations are indeed conceivable. Our ‘attachment’ operation is more in line with Gaifman and Shapiro (1989). 1.2.2 Initial Approaches: Modularized Parsing. Early attempts to address modularity in linguistic formalisms share a signiﬁcant disadvantage: The modularization is of the parsing process rather than the grammar. Kasper and Krieger (1996) describe a technique for dividing a uniﬁcation-based grammar into two components, roughly along the syntax/semantics axis. Their motivation is efﬁciency; observing that syntax usually imposes constraints on permissible structures, and semantics usually mostly adds structure, they propose to parse with the syntactic constraints ﬁrst, and apply the semantics later. This is achieved by recursively deleting the syntactic and semantic information (under their corresponding attributes in the rules and the lexicon) for the semantic and syntactic parsers, respectively. This proposal requires that a s"
J11-1003,W02-0103,0,0.0446312,"Missing"
J11-1003,J92-4004,0,0.425245,"Missing"
J11-1003,P06-1018,0,\N,Missing
J11-1003,C00-2087,0,\N,Missing
J12-4004,N09-4002,0,0.0546435,"Missing"
J12-4004,W10-0701,0,0.0744049,"Missing"
J12-4004,W11-2107,0,0.0234353,"r than Bleu, and show that the T-based LMs yield better MT systems even with different metrics. Second, we perform a manual evaluation of a portion of the evaluation set. The results show that human evaluators prefer translations produced by an MT system that uses a T-based LM over translations produced by a system built with an O-based LM. Finally, we provide a detailed analysis of the differences between O- and T-based LMs, explaining these differences in terms of insights from Translation Studies. 5.1 Automatic Evaluation First, we use two alternative automatic evaluation metrics, METEOR6 (Denkowski and Lavie 2011) and TER (Snover et al. 2006), to assess the quality of the MT systems described in Section 4.2. We focus on four translation tasks: From German, French, Italian, and Dutch to English.7 For each task we report the performance of two MT systems: One that uses a language model compiled from original-English texts, and one that uses a language model trained on texts translated from the source language. The results, which are reported in Table 18, fully support our previous ﬁndings (recall that lower TER is better): MT systems that use T-based LMs signiﬁcantly outperform systems that use O-based L"
J12-4004,P05-1045,0,0.0397573,"Missing"
J12-4004,W04-3250,0,0.100146,"LMs: MIX, O-based, and four T-based models (Section 3.2). We use Bleu (Papineni et al. 2002) to evaluate translation quality. The results are listed in Tables 14 and 15. The results are consistent and fully conﬁrm our hypothesis. Across all language pairs, MT systems using LMs compiled from translated-from-source texts consistently outperform all other systems. Systems that use LMs compiled from texts originally written in the target language always perform worst or second worst. We test the statistical signiﬁcance of the differences between the results using the bootstrap resampling method (Koehn 2004). In all experiments, the best system (translated-from-source LM) is signiﬁcantly better than the system that uses the O-based LM (p &lt; 0.01). We now repeat the experiment with Hebrew to English translation. We construct a Hebrew-to-English MT system with Moses, using a factored translation model (Koehn and Hoang 2007). Every token in the training corpus is represented as two factors: surface form and lemma. The Hebrew input is fully segmented (Itai and Wintner 2008). The system is built and tuned with O- and T-based LMs. The O-based LM yields a Bleu score of 11.94, whereas using the T-based LM"
J12-4004,2005.mtsummit-papers.11,0,0.169824,"n turn and evaluate its contribution to the classiﬁcation task. The most informative features are lexical variety, sentence length, and lexical density. 1 Preliminary results were published in Lembersky, Ordan, and Wintner (2011). This is an extended, revised version of that paper, providing fuller data and reporting on more language pairs. Some experiments (in particular, Section 4.2.3) are completely new, as is the bulk of the discussion in Section 5, including the human evaluation. 801 Computational Linguistics Volume 38, Number 4 van Halteren (2008) focuses on six languages from Europarl (Koehn 2005): Dutch, English, French, German, Italian, and Spanish. For each of these languages, a parallel six-lingual subcorpus is extracted, including an original text and its translations into the other ﬁve languages. The task is to identify the source language of translated texts, and the reported results are excellent. This ﬁnding is crucial: as Baker (1996) states, translations do resemble each other; in accordance with the law of interference, however, the study of van Halteren (2008) suggests that translation from different source languages constitute different sublanguages. As we show in Section"
J12-4004,D07-1091,0,0.0770532,"Missing"
J12-4004,P07-2045,0,0.015818,"Missing"
J12-4004,N03-1017,0,0.0367989,"Missing"
J12-4004,W07-0733,0,0.0298289,"odel is deﬁned by:  tˆ = argmax P(t|s) = argmax t t N   λi hi (t, s) (2) i=1 5 Except log-linear models, for which we only report the quality of machine translation, because there are two language models in this case and perplexity is harder to compute. 816 Lembersky, Ordan, and Wintner Language Models for Machine Translation We train two language models, based on T-FR and O-EN. Then, we combine these models by including them as different feature functions. The feature weight of each LM is set by minimum error-rate tuning, optimizing the translation quality; this is the same technique that Koehn and Schroeder (2007) employ for domain adaptation. In-domain, this combination is better by 0.82 Bleu points compared with an MT system that uses O-EN (p &lt; 0.001), 0.59 Bleu points compared with the one that uses T-FR (p &lt; 0.05). Out of domain, this combination is again not signiﬁcantly better than using T-FR only (improvement of 0.08 Bleu points, p = 0.255). Interpolated language models. In the interpolated scenario, two language models are mixed on a ﬁxed proportion η, according to the following equation (Weintraub et al. 1996): p(w|h) = (1 − η ) · p(w|h; LM1 ) + η · p(w|h; LM2 ) (3) where w is a word, h is its"
J12-4004,P11-1132,1,0.702354,"ity of the 2-gram LM created from texts translated from Italian is slightly higher). The perplexity of LMs constructed from texts translated from languages other than L always lies between these two extremes: It is a better ﬁt of the reference set than original texts, but not as good as texts translated from L (or mixture translations). This gives rise to yet another hypothesis, namely, that translations from typologically related languages form a similar “translationese dialect,” whereas translations from more distant source languages form two different “dialects” in the target language (see Koppel and Ordan 2011). Table 7 Reference sets. Language pair DE-EN FR-EN IT-EN NL-EN EN-DE EN-FR Hansard HE-EN Side Sentences Tokens Length DE EN FR EN IT EN NL EN EN DE EN FR FR EN HE EN 6,675 6,675 8,494 8,494 2,269 2,269 4,593 4,593 8,358 8,358 4,284 4,284 8,926 8,926 7,546 7,546 161,889 178,984 260,198 271,536 82,261 78,258 114,272 105,083 215,325 214,306 108,428 125,590 193,840 163,448 102,085 126,183 24.3 26.8 30.6 32.0 36.3 34.5 24.9 22.9 25.8 25.6 25.3 29.3 21.7 18.3 13.5 16.7 807 Computational Linguistics Volume 38, Number 4 Table 8 Fitness of various LMs to the reference set. German to English translatio"
J12-4004,2009.mtsummit-papers.9,0,0.444265,"Missing"
J12-4004,D11-1034,1,0.124602,"Missing"
J12-4004,E12-1026,1,0.867295,"Missing"
J12-4004,P03-1021,0,0.0138838,"her a better ﬁtting language model yields a better machine translation system. In other words, we expect the T-based LMs to outperform the O-based LMs when used as part of machine translation systems. We construct German-to-English, English-to-German, French-toEnglish, French-to-German, Italian-to-English, and Dutch-to-English MT systems using the Moses phrase-based SMT toolkit (Koehn et al. 2007). The systems are trained on the parallel corpora described in Section 3.3. We use the reference sets (Section 3.4) as follows: 1,000 sentences are randomly extracted for minimum error-rate training (Och 2003), and another, disjoint set of 1,000 randomly selected sentences is used for evaluation. Each system is built and tuned with six different LMs: MIX, O-based, and four T-based models (Section 3.2). We use Bleu (Papineni et al. 2002) to evaluate translation quality. The results are listed in Tables 14 and 15. The results are consistent and fully conﬁrm our hypothesis. Across all language pairs, MT systems using LMs compiled from translated-from-source texts consistently outperform all other systems. Systems that use LMs compiled from texts originally written in the target language always perform"
J12-4004,P02-1040,0,0.0841724,"an LM trained on French-translated-to-English texts ﬁts the German-translated-to-English reference set; and how well an LM trained on German-translated-to-English texts ﬁts the Frenchtranslated-to-English reference set. Finally, for the third hypothesis, we use these LMs for statistical MT (SMT). For each language pair we build several SMT systems. All systems use a translation model extracted from a parallel corpus which is oblivious to the direction of the translation; and one of the above-mentioned LMs. Then, we compare the translation quality of these systems in terms of the Bleu metric (Papineni et al. 2002) (as we show in Section 5.1, other automatic evaluation metrics reveal the same pattern). 3.2 Language Models In all the experiments, we use SRILM (Stolcke 2002) with interpolated modiﬁed KneserNey discounting (Chen 1998) and no cut-off on all n-grams, to train n-gram language models from various corpora. Unless mentioned otherwise, n = 4. We limit language models to a ﬁxed vocabulary and map out-of-vocabulary (OOV) tokens to a unique symbol to better control the OOV rates among various corpora. We experimented with two techniques for setting the vocabulary: Use all words that occur more than"
J12-4004,2006.amta-papers.25,0,0.0560553,"sed LMs yield better MT systems even with different metrics. Second, we perform a manual evaluation of a portion of the evaluation set. The results show that human evaluators prefer translations produced by an MT system that uses a T-based LM over translations produced by a system built with an O-based LM. Finally, we provide a detailed analysis of the differences between O- and T-based LMs, explaining these differences in terms of insights from Translation Studies. 5.1 Automatic Evaluation First, we use two alternative automatic evaluation metrics, METEOR6 (Denkowski and Lavie 2011) and TER (Snover et al. 2006), to assess the quality of the MT systems described in Section 4.2. We focus on four translation tasks: From German, French, Italian, and Dutch to English.7 For each task we report the performance of two MT systems: One that uses a language model compiled from original-English texts, and one that uses a language model trained on texts translated from the source language. The results, which are reported in Table 18, fully support our previous ﬁndings (recall that lower TER is better): MT systems that use T-based LMs signiﬁcantly outperform systems that use O-based LMs. 6 More precisely, we use"
J12-4004,W00-1308,0,0.137114,"Missing"
J12-4004,tsvetkov-wintner-2010-automatic,1,0.869276,"Missing"
J12-4004,C08-1118,0,0.452808,"Missing"
J12-4004,J00-4006,0,\N,Missing
J13-4007,D11-1033,0,0.039619,"Missing"
J13-4007,P11-2031,0,0.0696282,"Missing"
J13-4007,W11-2107,0,0.0531406,"Missing"
J13-4007,D10-1044,0,0.0549697,"Missing"
J13-4007,D07-1103,0,0.0748358,"Missing"
J13-4007,W04-3250,0,0.238361,"Missing"
J13-4007,2005.mtsummit-papers.11,0,0.536305,"or domain adaptation. We successfully apply this method to the problem of adapting translation models to translationese, gaining statistically significant improvements in translation quality. 3. Baseline Experiments 3.1 Europarl Experiments The task we focus on in our experiments is translation from French to English (FREN) and from English to French (EN-FR). To establish the robustness of our approach, we also conduct experiments with other translation tasks, including German–English (DE-EN), English–German (EN-DE), Italian–English (IT-EN), and English–Italian (ENIT). Our corpus is Europarl (Koehn 2005), specifically, portions collected over the years 1996–1999 and 2001–2009. This is a large multilingual corpus, containing sentences 1002 Lembersky, Ordan, and Wintner Adapting Translation Models to Translationese translated from several European languages. In most cases the corpus is annotated with the original language and the name of the speaker. For each language pair we extract from the multilingual corpus two subsets, corresponding to the original languages in which the sentences were produced. For example, in the case of FR-EN we extract from our corpus all sentences produced in French"
J13-4007,2009.mtsummit-papers.7,0,0.0803662,"Missing"
J13-4007,P07-2045,0,0.0106574,"Missing"
J13-4007,W07-0733,0,0.191629,"pairs according to their relevance to the target domain. They use multiple features that help to distinguish between phrase pairs in the general domain and those in the specific domain. We rely on features that are motivated by the findings of translation studies, having established their relevance through a comparative analysis of the phrase tables. In particular, we use measures such as translation model entropy, inspired by Koehn, Birch, and Steinberger (2009). Additionally, we apply the method suggested by Moore and Lewis (2010) using perplexity ratio instead of cross-entropy difference. Koehn and Schroeder (2007) suggest a method for adaptation of translation models. They pass two phrase tables directly to the decoder using multiple decoding paths. As we show in Section 5, the application of this method to our scenario does not result in a clear contribution, and we are able to show better results using our proposed method. Finally, Sennrich (2012) proposes perplexity minimization as a way to set the weights for translation model mixture for domain adaptation. We successfully apply this method to the problem of adapting translation models to translationese, gaining statistically significant improvemen"
J13-4007,P11-1132,1,0.870741,"lation process and traces of the original language from which the texts were translated. Among the better-known properties of translationese are simplification and explicitation (Blum-Kulka and Levenston 1983; Blum-Kulka 1986; Baker 1993): Translated texts tend to be shorter, to have lower type/token ratio, and to use certain discourse markers more frequently than original texts. Interestingly, translated texts are so markedly different from original ones that automatic classification can identify them with very high accuracy (Baroni and Bernardini 2006; van Halteren 2008; Ilisei et al. 2010; Koppel and Ordan 2011). Contemporary statistical machine translation (SMT) systems use parallel corpora to train translation models that reflect source- and target-language phrase correspondences. Typically, SMT systems ignore the direction of translation of the parallel corpus. Given the unique properties of translationese, which operate asymmetrically from source to target language, it is reasonable to assume that this direction may affect the quality of the translation. Recently, Kurokawa, Goutte, and Isabelle (2009) showed that this is indeed the case. They trained a system to translate between French and Engli"
J13-4007,2009.mtsummit-papers.9,0,0.739456,"Missing"
J13-4007,D11-1034,1,0.890031,"Missing"
J13-4007,E12-1026,1,0.221608,"Missing"
J13-4007,J12-4004,1,0.77634,"Missing"
J13-4007,P10-2041,0,0.160836,"domain adaptation scenario, a system is trained on a large corpus of “general” (out-ofdomain) training material, with a small portion of in-domain training texts. In our case, the translation model is trained on a large parallel corpus, of which some (generally unknown) subset is “in-domain” (S → T), and some other subset is “out-of-domain” (T → S). Most existing adaptation methods focus on selecting in-domain data from a general domain corpus. In particular, perplexity is used to score the sentences in the general-domain corpus according to an in-domain language model. Gao et al. (2002) and Moore and Lewis (2010) apply this method to language modeling, and Foster, Goutte, and Kuhn (2010) and Axelrod, He, and Gao (2011) apply this method to translation modeling. Moore and Lewis (2010) suggest a slightly different approach, using crossentropy difference as a ranking function. Domain adaptation methods are usually applied at the corpus level, whereas we focus on an adaptation of the phrase table used for SMT. In this sense, our work follows Foster, Goutte, and Kuhn (2010), who weigh out-of-domain phrase pairs according to their relevance to the target domain. They use multiple features that help to disti"
J13-4007,P03-1021,0,0.0423244,"Missing"
J13-4007,P00-1056,0,0.371705,"Missing"
J13-4007,P02-1040,0,0.0867048,"Missing"
J13-4007,2006.amta-papers.25,0,0.151027,"Missing"
J13-4007,C08-1118,0,0.578287,"Missing"
J13-4007,E12-1055,0,\N,Missing
J14-2007,W03-1809,0,0.0526184,"Missing"
J14-2007,bouamor-etal-2012-identifying,0,0.115291,"Missing"
J14-2007,J93-2003,0,0.0719354,"Missing"
J14-2007,N10-1029,0,0.272199,"Missing"
J14-2007,W02-1801,0,0.117347,"Missing"
J14-2007,J90-1003,0,0.272794,"ated work in the next section (borrowing from Tsvetkov and Wintner [2012]), we motivate in Section 3 the methodology we propose, and list the resources needed for implementing it. Section 4 discusses the linguistically motivated features and their implementation; the organization of the Bayesian network is described in Section 5. We explain how we generate training materials in Section 6. Section 7 provides a thorough evaluation of the results. We conclude with suggestions for future research. 2. Related Work Early approaches to MWE identification concentrated on their collocational behavior (Church and Hanks 1990). One of the first approaches was implemented as Xtract (Smadja 1993): Here, word pairs that occur with high frequency within a context of five words in a corpus are first collected, and are then ranked and filtered according to contextual considerations, including the parts of speech of their neighbors. Pecina (2008) compares 55 different association measures in ranking German Adj-N and PPVerb collocation candidates. He shows that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. Other results (Chang, D"
J14-2007,W07-1106,0,0.28937,"Missing"
J14-2007,W04-0412,0,0.436644,"edu. ∗∗ Department of Computer Science, University of Haifa Mount Carmel, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il. Submission received: 6 January 2013; revised submission received: 13 June 2013; accepted for publication: 16 August 2013. doi:10.1162/COLI a 00177 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing (NLP) applications. Awareness of MWEs was proven beneficial for a variety of applications, including information retrieval (Doucet and Ahonen-Myka 2004), building ontologies (Venkatsubramanyan and Perez-Carballo 2004), text alignment (Venkatapathy and Joshi 2006), and machine translation (Baldwin and Tanaka 2004; Uchiyama, Baldwin, and Ishizaki 2005; Carpuat and Diab 2010). We propose a novel architecture for identifying MWEs, of various types and syntactic categories, in monolingual corpora. Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of various types by zooming in on the general idiosyncratic properties of MWEs rather than on specific properties of each subclass thereof. Addre"
J14-2007,E06-1043,0,0.105949,"y fixed if replacing any of its constituents by a semantically (and syntactically) similar word generally results in an invalid or literal expression. Syntactically fixed expressions ´ prohibit (or restrict) syntactic variation. For example, Van de Cruys and Villada Moiron (2007) use lexical fixedness to extract Dutch verb-noun idiomatic combinations (VNICs). Bannard (2007) uses syntactic fixedness to identify English VNICs. Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson 2006). Recently, Green et al. (2011) use parsing, and in particular Tree Substitution Grammars, for identifying MWEs in French. Semantic properties of MWEs can be used to distinguish between compositional and non-compositional (idiomatic) expressions. Katz and Giesbrecht (2006) and Baldwin et al. (2003) use Latent Semantic Analysis (LSA) for this purpose. They show that compositional MWEs appear in contexts more similar to their constituents than non-compositional MWEs. For example, the co-occurrence measured by LSA between the expression kick the bucket and the word die is much higher than co-occu"
J14-2007,D11-1067,0,0.216921,"Missing"
J14-2007,W10-3712,0,0.0247788,"syntactic constructions, in monolingual corpora. These include proper names, noun phrases, verb-particle pairs, and so forth. We focus on bigrams (MWEs consisting of two consecutive tokens) in this work; the methodology, however, can be extended to longer n-grams. Several properties of MWEs make this task challenging: MWEs exhibit idiosyncrasies on a variety of levels, orthographic, morphological, syntactic, and of course semantic. Such a complex task calls for a combination of multiple approaches, and much research indeed suggests “hybrid” approaches to MWE identification (Duan et al. 2009; Hazelbeck and Saito 2010; Ramisch et al. 2010; Weller and Fritzinger 2010). We believe that Bayesian networks provide an optimal architecture for expressing various pieces of knowledge aimed at MWE identification, for the following reasons (noted, e.g., by Heckerman 1995): r In contrast to many other classification methods, Bayesian networks can learn (and express) causal relationships between features. This facilitates better understanding of the problem domain. 453 Computational Linguistics r Volume 40, Number 2 Bayesian networks can encode not only statistical data, but also prior domain knowledge and human intuit"
J14-2007,W06-1203,0,0.11976,"n (2007) use lexical fixedness to extract Dutch verb-noun idiomatic combinations (VNICs). Bannard (2007) uses syntactic fixedness to identify English VNICs. Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson 2006). Recently, Green et al. (2011) use parsing, and in particular Tree Substitution Grammars, for identifying MWEs in French. Semantic properties of MWEs can be used to distinguish between compositional and non-compositional (idiomatic) expressions. Katz and Giesbrecht (2006) and Baldwin et al. (2003) use Latent Semantic Analysis (LSA) for this purpose. They show that compositional MWEs appear in contexts more similar to their constituents than non-compositional MWEs. For example, the co-occurrence measured by LSA between the expression kick the bucket and the word die is much higher than co-occurrence of this expression and its component words. The disadvantage of this methodology is that to distinguish between idiomatic and non-idiomatic usages of the MWE it relies on the MWE’s known idiomatic meaning, and this information is usually not available. In addition,"
J14-2007,kirschenbaum-wintner-2010-general,1,0.78391,",787 804,704 218,108 30,000 895,632 27,880 865,632 225,660 English–Hebrew 19,626 271,787 14,142 252,183 128,987 19,626 280,508 12,555 280,506 149,688 to 69,912 MWEs. If the corpus surface form is not listed in the dictionary, we use the surface form in lieu of its lemma. The multiword entries of the DELA dictionaries are only used for evaluation. For some features we also need a bilingual dictionary. For English–Hebrew, we use a small dictionary consisting of 78,313 translation pairs. Some of the entries are collected manually, whereas others are produced automatically (Itai and Wintner 2008; Kirschenbaum and Wintner 2010). For English–French, because we are unable to obtain a good-quality dictionary, we use instead Giza++ (Och and Ney 2000) 1-1 word alignments computed automatically from the entire WMT-11 parallel corpus. In order to prepare training material automatically (Section 6), we use small bilingual corpora. For English–French, we use a random sample of 30,000 parallel sentences from the WMT-11 corpus. For English-Hebrew, we use the parallel corpus of Tsvetkov and Wintner (2010a). Statistics of the parallel corpora are listed in Table 2. For evaluation we need lists of MWEs, ideally augmented by lists"
J14-2007,P00-1056,0,0.801244,"280,506 149,688 to 69,912 MWEs. If the corpus surface form is not listed in the dictionary, we use the surface form in lieu of its lemma. The multiword entries of the DELA dictionaries are only used for evaluation. For some features we also need a bilingual dictionary. For English–Hebrew, we use a small dictionary consisting of 78,313 translation pairs. Some of the entries are collected manually, whereas others are produced automatically (Itai and Wintner 2008; Kirschenbaum and Wintner 2010). For English–French, because we are unable to obtain a good-quality dictionary, we use instead Giza++ (Och and Ney 2000) 1-1 word alignments computed automatically from the entire WMT-11 parallel corpus. In order to prepare training material automatically (Section 6), we use small bilingual corpora. For English–French, we use a random sample of 30,000 parallel sentences from the WMT-11 corpus. For English-Hebrew, we use the parallel corpus of Tsvetkov and Wintner (2010a). Statistics of the parallel corpora are listed in Table 2. For evaluation we need lists of MWEs, ideally augmented by lists of non-MWE bigrams. Such lists are notoriously hard to obtain. As a general method of evaluation, we run 10-fold cross-v"
J14-2007,J03-1002,0,0.0104764,"Missing"
J14-2007,P05-2003,0,0.0242544,"this feature is not language-specific, we assume that it should work best for pairs of rather distinct languages. Collocation. As a baseline, statistical association measure, we use pointwise mutual information (PMI). We define a binary feature, PMI, with two values, low and high, 458 Tsvetkov and Wintner Identification of Multiword Expressions FRZN CAPS MWE HYPHEN FOSSIL POS CNTXT TRANS HIST PMI Figure 1 The Bayesian network for MWE identification. reflecting an experimentally determined threshold. Clearly, other association measures (as well as combinations of more than one) could be used (Pecina 2005). 5. Feature Interdependencies Expressed as a Bayesian Network A Bayesian network (Jensen and Nielsen 2007) is organized as a directed acyclic graph whose nodes are random variables and whose edges represent interdependencies among those variables. We use a particular view of BN, known as causal networks, in which directed edges lead to a variable from each of its direct causes.4 This facilitates the expression of domain knowledge (and intuitions, beliefs, etc.) as structural properties of the network. We use the BN as a classification device: Training amounts to computing the joint probabilit"
J14-2007,N03-2027,0,0.0935168,"Missing"
J14-2007,N07-1051,0,0.0436039,"this section. In general, we require corpora (both monolingual and bilingual), morphological analyzers or stemmers, part-of-speech taggers, and bilingual dictionaries. No deeper processing is assumed (e.g., no parsers or lexical semantic resources are needed). The method we advocate is thus appropriate for medium-density languages (Varga et al. 2005). To compute the features discussed in Section 4, we need large monolingual corpora. For English and French, we use the 109 corpora released for WMT-11 (Callison-Burch et al. 2011); the corpora were syntactically parsed using the Berkeley parser (Petrov and Klein 2007), but we only use the POS tags in this work. For Hebrew, we use a monolingual corpus (Itai and Wintner 2008), which we pre-process as in Tsvetkov and Wintner (2012): We use a morphological analyzer (Itai and Wintner 2008) to segment word forms (separating prefixes and suffixes) and induce POS tags. Summary statistics for each corpus are listed in Table 1. For some features we need access to the lemma of word tokens. In Hebrew, the MILA morphological analyzer (Itai and Wintner 2008) provides the lemmas, but the parsed corpora we use in English and French do not. We therefore use the DELA dictio"
J14-2007,W09-2907,0,0.239822,"Missing"
J14-2007,J93-1007,0,0.715976,"motivate in Section 3 the methodology we propose, and list the resources needed for implementing it. Section 4 discusses the linguistically motivated features and their implementation; the organization of the Bayesian network is described in Section 5. We explain how we generate training materials in Section 6. Section 7 provides a thorough evaluation of the results. We conclude with suggestions for future research. 2. Related Work Early approaches to MWE identification concentrated on their collocational behavior (Church and Hanks 1990). One of the first approaches was implemented as Xtract (Smadja 1993): Here, word pairs that occur with high frequency within a context of five words in a corpus are first collected, and are then ranked and filtered according to contextual considerations, including the parts of speech of their neighbors. Pecina (2008) compares 55 different association measures in ranking German Adj-N and PPVerb collocation candidates. He shows that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. Other results (Chang, Danielsson, and Teubert 2002; Villavicencio et al. 2007) suggest that"
J14-2007,tsvetkov-wintner-2010-automatic,1,0.933357,"several morphological and morphosyntactic properties of such constructions. The resulting classifier performs much better than a naive baseline, reducing the error rate by over one third. We rely on some of these insights, as we implement more of the linguistic properties of MWEs. Again, our methodology is not limited to a particular construction: Indeed, we demonstrate that our general methodology, trained on automatically generated, general training data, performs almost as well as the noun-noun-specific approach of Al-Haj and Wintner (2010) on the very same data set (Section 7). Recently, Tsvetkov and Wintner (2010b, 2012) introduced a general methodology for extracting MWEs from bilingual corpora, and applied it to Hebrew. The results were a highly accurate set of Hebrew MWEs, of various types, along with their English translations. A major limitation of this work is that it can only be used to identify MWEs in the bilingual corpus, and is thus limited in its scope. We use this methodology to extract both positive and negative instances for our training set in the current work; 452 Tsvetkov and Wintner Identification of Multiword Expressions but we extrapolate the results much further by extending the"
J14-2007,C10-2144,1,0.882341,"Missing"
J14-2007,D11-1077,1,0.162692,"Missing"
J14-2007,W07-1104,0,0.176476,"Missing"
J14-2007,W06-1204,0,0.0208032,".haifa.ac.il. Submission received: 6 January 2013; revised submission received: 13 June 2013; accepted for publication: 16 August 2013. doi:10.1162/COLI a 00177 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing (NLP) applications. Awareness of MWEs was proven beneficial for a variety of applications, including information retrieval (Doucet and Ahonen-Myka 2004), building ontologies (Venkatsubramanyan and Perez-Carballo 2004), text alignment (Venkatapathy and Joshi 2006), and machine translation (Baldwin and Tanaka 2004; Uchiyama, Baldwin, and Ishizaki 2005; Carpuat and Diab 2010). We propose a novel architecture for identifying MWEs, of various types and syntactic categories, in monolingual corpora. Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of various types by zooming in on the general idiosyncratic properties of MWEs rather than on specific properties of each subclass thereof. Addressing multiple types of MWEs has its limitations: The task is less well-defined, one cannot rely on specific pr"
J14-2007,W04-0406,0,0.0113245,"of Haifa Mount Carmel, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il. Submission received: 6 January 2013; revised submission received: 13 June 2013; accepted for publication: 16 August 2013. doi:10.1162/COLI a 00177 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing (NLP) applications. Awareness of MWEs was proven beneficial for a variety of applications, including information retrieval (Doucet and Ahonen-Myka 2004), building ontologies (Venkatsubramanyan and Perez-Carballo 2004), text alignment (Venkatapathy and Joshi 2006), and machine translation (Baldwin and Tanaka 2004; Uchiyama, Baldwin, and Ishizaki 2005; Carpuat and Diab 2010). We propose a novel architecture for identifying MWEs, of various types and syntactic categories, in monolingual corpora. Unlike much existing work, which focuses on a particular syntactic construction, our approach addresses MWEs of various types by zooming in on the general idiosyncratic properties of MWEs rather than on specific properties of each subclass thereof. Addressing multiple types of MWEs has its limitations: The task is les"
J14-2007,D07-1110,0,0.35841,"Missing"
J14-2007,W17-1712,0,0.069263,"Missing"
kirschenbaum-wintner-2010-general,lavie-etal-2004-significance,0,\N,Missing
kirschenbaum-wintner-2010-general,2004.tmi-1.1,1,\N,Missing
kirschenbaum-wintner-2010-general,W98-1005,0,\N,Missing
kirschenbaum-wintner-2010-general,P02-1040,0,\N,Missing
kirschenbaum-wintner-2010-general,P07-1015,0,\N,Missing
kirschenbaum-wintner-2010-general,P08-1045,0,\N,Missing
kirschenbaum-wintner-2010-general,P02-1051,0,\N,Missing
kirschenbaum-wintner-2010-general,P08-2014,0,\N,Missing
kirschenbaum-wintner-2010-general,E09-1050,1,\N,Missing
kirschenbaum-wintner-2010-general,W09-3500,0,\N,Missing
L16-1664,W11-2838,0,0.0301888,"om the field of translation studies (Baker, 1996). In particular, these corpora have been extensively used for investigation of transfer of the source language into the target one (van Halteren, 2008; Popescu, 2011; Koppel and Ordan, 2011). Learner corpora are a different type of resource, constructed from texts written by non-native language learners, most often students acquiring a foreign language. Such corpora can reveal useful insights about the developmental process of language acquisition. Furthermore, such corpora also have practical applications, e.g., for automatic error correction (Dale and Kilgarriff, 2011). Another prominent computational application is the task of native language identification of English learners (Koppel et al., 2005; Tetreault et al., 2013; Nisioi, 2015a). Similarly, corpora of translated texts have been instrumental in automatic identification of translation, where much research demonstrates that machine learning techniques can discriminate between original and translated 1 The corpus is freely available at http://nlp.unibuc. ro/resources/ENNTT.tar.gz 4197 texts with very high accuracy in both supervised (Baroni and Bernardini, 2006; Ilisei and Inkpen, 2011; Volansky et al."
L16-1664,2005.mtsummit-papers.11,0,0.233408,"far been studied in isolation, both in the linguistic literature and in terms of computational investigations. In this paper we describe a corpus constructed from original English utterances (where we differentiate between native and non-native speakers based on their country of origin) and English translations from a variety of European languages. This corpus will be instrumental for research that aims to uniformly address both second language acquisition (more specifically, the language of highly fluent non-native speakers) and human translation. The corpus we describe is based on Europarl (Koehn, 2005), and is the first corpus that allows a uniform comparative study of both phenomena (translation and language acquisition). The texts contained in the corpus are uniform in terms of style, respecting the European Parliament’s formal standards. In addition, the original English utterances are accurately annotated with speakers’ data, including knowledge about the speakers’ native language1 . Corpora of original and translated language are essential for empirical investigation of theoretically-motivated hypotheses from the field of translation studies (Baker, 1996). In particular, these corpora"
L16-1664,P11-1132,0,0.440276,"ntained in the corpus are uniform in terms of style, respecting the European Parliament’s formal standards. In addition, the original English utterances are accurately annotated with speakers’ data, including knowledge about the speakers’ native language1 . Corpora of original and translated language are essential for empirical investigation of theoretically-motivated hypotheses from the field of translation studies (Baker, 1996). In particular, these corpora have been extensively used for investigation of transfer of the source language into the target one (van Halteren, 2008; Popescu, 2011; Koppel and Ordan, 2011). Learner corpora are a different type of resource, constructed from texts written by non-native language learners, most often students acquiring a foreign language. Such corpora can reveal useful insights about the developmental process of language acquisition. Furthermore, such corpora also have practical applications, e.g., for automatic error correction (Dale and Kilgarriff, 2011). Another prominent computational application is the task of native language identification of English learners (Koppel et al., 2005; Tetreault et al., 2013; Nisioi, 2015a). Similarly, corpora of translated texts"
L16-1664,J12-4004,1,0.894042,"a of translated texts have been instrumental in automatic identification of translation, where much research demonstrates that machine learning techniques can discriminate between original and translated 1 The corpus is freely available at http://nlp.unibuc. ro/resources/ENNTT.tar.gz 4197 texts with very high accuracy in both supervised (Baroni and Bernardini, 2006; Ilisei and Inkpen, 2011; Volansky et al., 2015) and unsupervised scenarios (Rabinovich and Wintner, 2015; Nisioi, 2015b). Such studies can be of much use for training better translation and language models for machine translation (Lembersky et al., 2012; Lembersky et al., 2013). The corpus that we describe will facilitate computational research into the similarities and differences between the two types of language contact (second language learning and translation), hopefully leading to better solutions for the related computational tasks. 2. Corpus pre-processing and annotation of the translation direction The Europarl corpus is extracted from the collection of the proceedings of the European Parliament (Koehn, 2005), dating back to 1996. The transcriptions are produced as follows: (1) the utterances of the speakers are transcribed; (2) the"
L16-1664,J13-4007,1,0.89355,"ve been instrumental in automatic identification of translation, where much research demonstrates that machine learning techniques can discriminate between original and translated 1 The corpus is freely available at http://nlp.unibuc. ro/resources/ENNTT.tar.gz 4197 texts with very high accuracy in both supervised (Baroni and Bernardini, 2006; Ilisei and Inkpen, 2011; Volansky et al., 2015) and unsupervised scenarios (Rabinovich and Wintner, 2015; Nisioi, 2015b). Such studies can be of much use for training better translation and language models for machine translation (Lembersky et al., 2012; Lembersky et al., 2013). The corpus that we describe will facilitate computational research into the similarities and differences between the two types of language contact (second language learning and translation), hopefully leading to better solutions for the related computational tasks. 2. Corpus pre-processing and annotation of the translation direction The Europarl corpus is extracted from the collection of the proceedings of the European Parliament (Koehn, 2005), dating back to 1996. The transcriptions are produced as follows: (1) the utterances of the speakers are transcribed; (2) the transcriptions are sent"
L16-1664,R11-1091,0,0.189924,"). The texts contained in the corpus are uniform in terms of style, respecting the European Parliament’s formal standards. In addition, the original English utterances are accurately annotated with speakers’ data, including knowledge about the speakers’ native language1 . Corpora of original and translated language are essential for empirical investigation of theoretically-motivated hypotheses from the field of translation studies (Baker, 1996). In particular, these corpora have been extensively used for investigation of transfer of the source language into the target one (van Halteren, 2008; Popescu, 2011; Koppel and Ordan, 2011). Learner corpora are a different type of resource, constructed from texts written by non-native language learners, most often students acquiring a foreign language. Such corpora can reveal useful insights about the developmental process of language acquisition. Furthermore, such corpora also have practical applications, e.g., for automatic error correction (Dale and Kilgarriff, 2011). Another prominent computational application is the task of native language identification of English learners (Koppel et al., 2005; Tetreault et al., 2013; Nisioi, 2015a). Similarly, cor"
L16-1664,Q15-1030,1,0.84297,"plication is the task of native language identification of English learners (Koppel et al., 2005; Tetreault et al., 2013; Nisioi, 2015a). Similarly, corpora of translated texts have been instrumental in automatic identification of translation, where much research demonstrates that machine learning techniques can discriminate between original and translated 1 The corpus is freely available at http://nlp.unibuc. ro/resources/ENNTT.tar.gz 4197 texts with very high accuracy in both supervised (Baroni and Bernardini, 2006; Ilisei and Inkpen, 2011; Volansky et al., 2015) and unsupervised scenarios (Rabinovich and Wintner, 2015; Nisioi, 2015b). Such studies can be of much use for training better translation and language models for machine translation (Lembersky et al., 2012; Lembersky et al., 2013). The corpus that we describe will facilitate computational research into the similarities and differences between the two types of language contact (second language learning and translation), hopefully leading to better solutions for the related computational tasks. 2. Corpus pre-processing and annotation of the translation direction The Europarl corpus is extracted from the collection of the proceedings of the European P"
L16-1664,W13-1706,0,0.0405858,"into the target one (van Halteren, 2008; Popescu, 2011; Koppel and Ordan, 2011). Learner corpora are a different type of resource, constructed from texts written by non-native language learners, most often students acquiring a foreign language. Such corpora can reveal useful insights about the developmental process of language acquisition. Furthermore, such corpora also have practical applications, e.g., for automatic error correction (Dale and Kilgarriff, 2011). Another prominent computational application is the task of native language identification of English learners (Koppel et al., 2005; Tetreault et al., 2013; Nisioi, 2015a). Similarly, corpora of translated texts have been instrumental in automatic identification of translation, where much research demonstrates that machine learning techniques can discriminate between original and translated 1 The corpus is freely available at http://nlp.unibuc. ro/resources/ENNTT.tar.gz 4197 texts with very high accuracy in both supervised (Baroni and Bernardini, 2006; Ilisei and Inkpen, 2011; Volansky et al., 2015) and unsupervised scenarios (Rabinovich and Wintner, 2015; Nisioi, 2015b). Such studies can be of much use for training better translation and langua"
L16-1664,C08-1118,0,0.338309,"Missing"
nir-etal-2010-morphologically,W07-0604,1,\N,Missing
P06-1019,copestake-flickinger-2000-open,0,0.156721,"natural language applications such as machine translation, speech generation, etc. Wide-coverage grammars are being developed for various languages (Oepen et al., 2002; Hinrichs et al., 2004; Bender et al., 2005; King et al., 2005) in several theoretical frameworks, e.g., LFG (Dalrymple, 2001) and HPSG (Pollard and Sag, 1994). Grammar development is a complex enterprise: it is not unusual for a single grammar to be developed by a team including several linguists, computational linguists and computer scientists. The scale of grammars is overwhelming: for example, the English resource grammar (Copestake and Flickinger, 2000) includes thousands of types. This raises problems reminiscent of those encountered in large-scale software development. Yet while software engineering provides adequate soWe assume familiarity with theories of (typed) unification grammars, as formulated by, e.g., Carpenter (1992) and Penn (2000). The definitions in this section set the notation and recall basic notions. For a partial function F , ‘F (x)↓’ means that F is defined for the value x. Definition 1 Given a partially ordered set hP, ≤i, the set of upper bounds of a subset S ⊆ P is the set S u = {y ∈ P |∀x ∈ S x ≤ y}. For a given part"
P06-1019,W02-1506,0,0.31688,"Missing"
P06-1019,W02-1502,0,0.079174,"e required to obey in order to assure the well-definedness of the combination. Keselj (2001) presents a modular HPSG, where each module is an ordinary type signature, but each of the sets F EAT and T YPE is divided into two disjoint sets of private and public elements. In this solution, modules do not support specification of partial information; module combination is not associative; and the only channel of interaction between modules is the names of types. 4 Desiderata To better understand the needs of grammar developers we carefully explored two existing grammars: the LINGO grammar matrix (Bender et al., 2002), which is a basis grammar for the rapid development of cross-linguistically consistent gram146 mars; and a grammar of a fragment of Modern Hebrew, focusing on inverted constructions (Melnik, 2006). These grammars were chosen since they are comprehensive enough to reflect the kind of data large scale grammar encode, but are not too large to encumber this process. Motivated by these two grammars, we experimented with ways to divide the signatures of grammars into modules and with different methods of module interaction. This process resulted in the following desiderata for a beneficial solution"
P06-1019,C96-1034,0,0.384283,"a makeshift signature to allow modular development of untyped unification grammars. In addition, they suggest that any development team should agree in advance on the feature space. This work emphasizes the observation that the modularization of the signature is the key for modular development of grammars. However, the proposed solution is adhoc and cannot be taken seriously as a concept of modularization. In particular, the suggestion for an agreement on the feature space undermines the essence of modular design. Several works address the problem of modularity in other, related, formalisms. Candito (1996) introduces a description language for the trees of LTAG. Combining two descriptions is done by conjunction. To constrain undesired combinations, Candito (1996) uses a finite set of names where each node of a tree description is associated with a name. The only channel of interaction between two descriptions is the names of the nodes, which can be used only to allow identification but not to prevent it. To overcome these shortcomings, Crabb´e and Duchier (2004) suggest to replace node naming by colors. Then, when unifying two trees, the colors can prevent or force the identification of nodes."
P06-1137,P98-1101,0,0.0265461,"Association for Computational Linguistics of which have recognition algorithms with time complexity O(n6 ) (Vijay-Shanker and Weir, 1993; Satta, 1994).1 As a result of the weak equivalence of four independently developed (and linguistically motivated) extensions of CFG, the class M CSL is considered to be linguistically meaningful, a natural class of languages for characterizing natural languages. Several authors tried to approximate unification grammars by means of context-free grammars (Rayner et al., 2001; Kiefer and Krieger, 2004) and even finite-state grammars (Pereira and Wright, 1997; Johnson, 1998), but we are not aware of any work which relates unification grammars with the class M CSL. The main objective of this work is to define constraints on UGs which naturally limit their generative capacity. We define two natural and easily testable syntactic constraints on UGs which ensure that grammars satisfying them generate the context-free and the mildly context-sensitive languages, respectively. The contribution of this result is twofold: • From a theoretical point of view, constraining unification grammars to generate exactly the class M CSL results in a grammatical formalism which is, on"
P06-1137,J94-2002,0,0.0564361,"ard, 1984), Tree Adjoining Grammars (TAG) (Joshi, 2003) and Combinatory Categorial Grammars (Steedman, 2000). In a seminal work, Vijay-Shanker and Weir (1994) prove that all four formalisms are weakly equivalent. They all generate the class of mildly context-sensitive languages (M CSL), all members 1089 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1089–1096, c Sydney, July 2006. 2006 Association for Computational Linguistics of which have recognition algorithms with time complexity O(n6 ) (Vijay-Shanker and Weir, 1993; Satta, 1994).1 As a result of the weak equivalence of four independently developed (and linguistically motivated) extensions of CFG, the class M CSL is considered to be linguistically meaningful, a natural class of languages for characterizing natural languages. Several authors tried to approximate unification grammars by means of context-free grammars (Rayner et al., 2001; Kiefer and Krieger, 2004) and even finite-state grammars (Pereira and Wright, 1997; Johnson, 1998), but we are not aware of any work which relates unification grammars with the class M CSL. The main objective of this work is to define"
P06-1137,J92-2002,0,0.0743153,"grammars, the problem is computationally hard. We present two natural constraints on unification grammars which limit their expressivity. We first show that non-reentrant unification grammars generate exactly the class of contextfree languages. We then relax the constraint and show that one-reentrant unification grammars generate exactly the class of tree-adjoining languages. We thus relate the commonly used and linguistically motivated formalism of unification grammars to more restricted, computationally tractable classes of languages. 1 Introduction Unification grammars (UG) (Shieber, 1986; Shieber, 1992; Carpenter, 1992) have originated as an extension of context-free grammars, the basic idea being to augment the context-free rules with non context-free annotations (feature structures) in order to express additional information. They can describe phonological, morphological, syntactic and semantic properties of languages simultaneously and are thus linguistically suitable for modeling natural languages. Several formulations of unification grammars have been proposed, and they are used extensively by computational linguists to describe the structure of a variety of natural languages. In order"
P06-1137,J93-4002,0,0.0606369,"ar, 1988), Head Grammars (Pollard, 1984), Tree Adjoining Grammars (TAG) (Joshi, 2003) and Combinatory Categorial Grammars (Steedman, 2000). In a seminal work, Vijay-Shanker and Weir (1994) prove that all four formalisms are weakly equivalent. They all generate the class of mildly context-sensitive languages (M CSL), all members 1089 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1089–1096, c Sydney, July 2006. 2006 Association for Computational Linguistics of which have recognition algorithms with time complexity O(n6 ) (Vijay-Shanker and Weir, 1993; Satta, 1994).1 As a result of the weak equivalence of four independently developed (and linguistically motivated) extensions of CFG, the class M CSL is considered to be linguistically meaningful, a natural class of languages for characterizing natural languages. Several authors tried to approximate unification grammars by means of context-free grammars (Rayner et al., 2001; Kiefer and Krieger, 2004) and even finite-state grammars (Pereira and Wright, 1997; Johnson, 1998), but we are not aware of any work which relates unification grammars with the class M CSL. The main objective of this work"
P06-1137,E95-1011,0,\N,Missing
P06-1137,C98-1098,0,\N,Missing
P06-1137,P91-1032,0,\N,Missing
P16-1176,2005.mtsummit-papers.11,0,0.0132562,"authorship profiling (Estival et al., 2007) or grammatical error correction (Chodorow et al., 2010). Most of these studies utilize techniques that are motivated by the same abstract principles associ1871 ated with L1 influences on the target language. To the best of our knowledge, our work is the first to address both translations and non-native language under a unifying computational framework, and in particular to compare both with native language. 3 3.1 Methodology and experimental setup Dataset Our dataset3 is based on the highly homogeneous corpus of the European Parliament Proceedings (Koehn, 2005). Note that the proceedings are produced as follows: (1) the utterances of the speakers are transcribed; (2) the transcriptions are sent to the speaker who may suggest minimal editing without changing the content; (3) the edited version is then translated by native speakers. Note in particular that the texts are not a product of simultaneous interpretation. In this work we utilize a subset of Europarl in which each sentence is manually annotated with speaker information, including the EU state represented and the original language in which the sentence was uttered (Nisioi et al., 2016). The te"
P16-1176,P11-1132,1,0.965506,"al import, as we briefly mention in the following section. 2 Related work Corpus-based investigation of translationese has been a prolific field of recent research, laying out an empirical foundation for the theoretically motivated hypotheses on the characteristics of translationese. More specifically, identification of translated texts by means of automatic classification shed light on the manifestation of translation universals and cross-linguistic influences as markers of translated texts (Baroni and Bernardini, 2006; van Halteren, 2008; Gaspari and Bernardini, 2008; Kurokawa et al., 2009; Koppel and Ordan, 2011; Ilisei and Inkpen, 2011; Volansky et al., 2015; Rabinovich and Wintner, 2015; Nisioi, 2015b), while Gaspari and Bernardini (2008) introduced a dataset for investigation of potential common traits between translations and non-native texts. Such studies prove to be important for the development of parallel corpora (Resnik and Smith, 2003), the improvement in quality of plagiarism detection (Potthast et al., 2011), language modeling, and statistical machine translation (Lembersky et al., 2012, 2013). Computational approaches also proved beneficial for theoretical research in second language acq"
P16-1176,2009.mtsummit-papers.9,0,0.306665,"also have some practical import, as we briefly mention in the following section. 2 Related work Corpus-based investigation of translationese has been a prolific field of recent research, laying out an empirical foundation for the theoretically motivated hypotheses on the characteristics of translationese. More specifically, identification of translated texts by means of automatic classification shed light on the manifestation of translation universals and cross-linguistic influences as markers of translated texts (Baroni and Bernardini, 2006; van Halteren, 2008; Gaspari and Bernardini, 2008; Kurokawa et al., 2009; Koppel and Ordan, 2011; Ilisei and Inkpen, 2011; Volansky et al., 2015; Rabinovich and Wintner, 2015; Nisioi, 2015b), while Gaspari and Bernardini (2008) introduced a dataset for investigation of potential common traits between translations and non-native texts. Such studies prove to be important for the development of parallel corpora (Resnik and Smith, 2003), the improvement in quality of plagiarism detection (Potthast et al., 2011), language modeling, and statistical machine translation (Lembersky et al., 2012, 2013). Computational approaches also proved beneficial for theoretical researc"
P16-1176,J12-4004,1,0.894708,"(Baroni and Bernardini, 2006; van Halteren, 2008; Gaspari and Bernardini, 2008; Kurokawa et al., 2009; Koppel and Ordan, 2011; Ilisei and Inkpen, 2011; Volansky et al., 2015; Rabinovich and Wintner, 2015; Nisioi, 2015b), while Gaspari and Bernardini (2008) introduced a dataset for investigation of potential common traits between translations and non-native texts. Such studies prove to be important for the development of parallel corpora (Resnik and Smith, 2003), the improvement in quality of plagiarism detection (Potthast et al., 2011), language modeling, and statistical machine translation (Lembersky et al., 2012, 2013). Computational approaches also proved beneficial for theoretical research in second language acquisition (Jarvis and Pavlenko, 2008). Numerous studies address linguistic processes attributed to SLA, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Nisioi, 2015a) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). These studies are instrumental for language teaching a"
P16-1176,J13-4007,1,0.925634,"Missing"
P16-1176,P14-5010,0,0.00279801,"idence in their language skills. 3.2 Preprocessing All datasets were split by sentence, cleaned (text lowercased, punctuation and empty lines removed) and tokenized using the Stanford tools 3 The dataset is available at http://nlp.unibuc. ro/resources.html 4 Appendix A provides details on the distribution of NN and T texts by various L1s. sub-corpus native (N) non-native (NN) translated (T) total sentences 60,182 29,734 738,597 828,513 tokens 1,589,215 783,742 22,309,296 24,682,253 types 28,004 18,419 71,144 117,567 Table 1: Europarl corpus statistics: native, nonnative and translated texts. (Manning et al., 2014). For the classification experiments we randomly shuffled the sentences within each language variety to prevent interference of other artifacts (e.g., authorship, topic) into the classification procedure. We divided the data into chunks of approximately 2,000 tokens, respecting sentence boundaries, and normalized the values of lexical features by the number of tokens in each chunk. For classification we used Platt’s sequential minimal optimization algorithm (Keerthi et al., 2001; Hall et al., 2009) to train support vector machine classifiers with the default linear kernel. In all the experimen"
P16-1176,L16-1664,1,0.757859,"nt Proceedings (Koehn, 2005). Note that the proceedings are produced as follows: (1) the utterances of the speakers are transcribed; (2) the transcriptions are sent to the speaker who may suggest minimal editing without changing the content; (3) the edited version is then translated by native speakers. Note in particular that the texts are not a product of simultaneous interpretation. In this work we utilize a subset of Europarl in which each sentence is manually annotated with speaker information, including the EU state represented and the original language in which the sentence was uttered (Nisioi et al., 2016). The texts in the corpus are uniform in terms of style, respecting the European Parliament’s formal standards. Translations are produced by native English speakers and all non-native utterances are selected from members not representing UK or Ireland. Europarl N consists of texts delivered by native speakers from England. Table 1 depicts statistics of the dataset.4 In contrast to other learner corpora such as ICLE (Granger, 2003), EFCAMDAT (Geertzen et al., 2013) or TOEFL-11 (Blanchard et al., 2013), this corpus contains translations, native, and nonnative English of high proficiency speakers"
P16-1176,Q15-1030,1,0.860162,"k Corpus-based investigation of translationese has been a prolific field of recent research, laying out an empirical foundation for the theoretically motivated hypotheses on the characteristics of translationese. More specifically, identification of translated texts by means of automatic classification shed light on the manifestation of translation universals and cross-linguistic influences as markers of translated texts (Baroni and Bernardini, 2006; van Halteren, 2008; Gaspari and Bernardini, 2008; Kurokawa et al., 2009; Koppel and Ordan, 2011; Ilisei and Inkpen, 2011; Volansky et al., 2015; Rabinovich and Wintner, 2015; Nisioi, 2015b), while Gaspari and Bernardini (2008) introduced a dataset for investigation of potential common traits between translations and non-native texts. Such studies prove to be important for the development of parallel corpora (Resnik and Smith, 2003), the improvement in quality of plagiarism detection (Potthast et al., 2011), language modeling, and statistical machine translation (Lembersky et al., 2012, 2013). Computational approaches also proved beneficial for theoretical research in second language acquisition (Jarvis and Pavlenko, 2008). Numerous studies address linguistic proc"
P16-1176,J03-3002,0,0.0172018,"by means of automatic classification shed light on the manifestation of translation universals and cross-linguistic influences as markers of translated texts (Baroni and Bernardini, 2006; van Halteren, 2008; Gaspari and Bernardini, 2008; Kurokawa et al., 2009; Koppel and Ordan, 2011; Ilisei and Inkpen, 2011; Volansky et al., 2015; Rabinovich and Wintner, 2015; Nisioi, 2015b), while Gaspari and Bernardini (2008) introduced a dataset for investigation of potential common traits between translations and non-native texts. Such studies prove to be important for the development of parallel corpora (Resnik and Smith, 2003), the improvement in quality of plagiarism detection (Potthast et al., 2011), language modeling, and statistical machine translation (Lembersky et al., 2012, 2013). Computational approaches also proved beneficial for theoretical research in second language acquisition (Jarvis and Pavlenko, 2008). Numerous studies address linguistic processes attributed to SLA, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al.,"
P16-1176,W13-1706,0,0.0294085,"arallel corpora (Resnik and Smith, 2003), the improvement in quality of plagiarism detection (Potthast et al., 2011), language modeling, and statistical machine translation (Lembersky et al., 2012, 2013). Computational approaches also proved beneficial for theoretical research in second language acquisition (Jarvis and Pavlenko, 2008). Numerous studies address linguistic processes attributed to SLA, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Nisioi, 2015a) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). These studies are instrumental for language teaching and student evaluation (Smith and Swan, 2001), and can improve NLP applications such as authorship profiling (Estival et al., 2007) or grammatical error correction (Chodorow et al., 2010). Most of these studies utilize techniques that are motivated by the same abstract principles associ1871 ated with L1 influences on the target language. To the best of our knowledge, our work is the first to address both translations and no"
P16-1176,N01-1031,0,0.0417972,"potential common traits between translations and non-native texts. Such studies prove to be important for the development of parallel corpora (Resnik and Smith, 2003), the improvement in quality of plagiarism detection (Potthast et al., 2011), language modeling, and statistical machine translation (Lembersky et al., 2012, 2013). Computational approaches also proved beneficial for theoretical research in second language acquisition (Jarvis and Pavlenko, 2008). Numerous studies address linguistic processes attributed to SLA, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Nisioi, 2015a) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). These studies are instrumental for language teaching and student evaluation (Smith and Swan, 2001), and can improve NLP applications such as authorship profiling (Estival et al., 2007) or grammatical error correction (Chodorow et al., 2010). Most of these studies utilize techniques that are motivated by the same abstract principles associ1871 ated with"
P16-1176,W13-1736,1,0.867014,"and Smith, 2003), the improvement in quality of plagiarism detection (Potthast et al., 2011), language modeling, and statistical machine translation (Lembersky et al., 2012, 2013). Computational approaches also proved beneficial for theoretical research in second language acquisition (Jarvis and Pavlenko, 2008). Numerous studies address linguistic processes attributed to SLA, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Nisioi, 2015a) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). These studies are instrumental for language teaching and student evaluation (Smith and Swan, 2001), and can improve NLP applications such as authorship profiling (Estival et al., 2007) or grammatical error correction (Chodorow et al., 2010). Most of these studies utilize techniques that are motivated by the same abstract principles associ1871 ated with L1 influences on the target language. To the best of our knowledge, our work is the first to address both translations and non-native language under"
P16-1176,C08-1118,0,0.212394,"Missing"
P16-1176,N12-1033,0,\N,Missing
P16-1176,D11-1034,1,\N,Missing
P16-1176,K15-1010,0,\N,Missing
P16-1176,W11-2123,0,\N,Missing
P17-1049,P06-1035,0,0.0456723,"are powerful to an extent that facilitates clustering source languages into families and (partially) reconstructing intra-families ties; so much so, that these results hold even after two rounds of translation. Moreover, we perform analysis of various linguistic phenomena in the source languages, laying out quantitative grounds for the language typology reconstruction results. 2 Related work A number of works in historical linguistics have applied methods from the field of bioinformatics, in particular algorithms for generating phylogenetic trees (Ringe et al., 2002; Nakhleh et al., 2005a,b; Ellison and Kirby, 2006; Boc et al., 2010). Most of them rely on lists of cognates, words in multiple languages with a common origin that share a similar meaning and a similar pronunciation (Dyen et al., 1992; Rexov´a et al., 2003). These works all rely on multilingual data, whereas we construct phylogenetic trees from texts in a single language. 3 3.1 Methodology Dataset This corpus-based study uses Europarl (Koehn, 2005), the proceedings of the European Parliament and their translations into all the official Eu531 3.2 ropean Union (EU) languages. Europarl is one of the most popular parallel resources in natural la"
P17-1049,2005.mtsummit-papers.11,0,0.0557132,"in historical linguistics have applied methods from the field of bioinformatics, in particular algorithms for generating phylogenetic trees (Ringe et al., 2002; Nakhleh et al., 2005a,b; Ellison and Kirby, 2006; Boc et al., 2010). Most of them rely on lists of cognates, words in multiple languages with a common origin that share a similar meaning and a similar pronunciation (Dyen et al., 1992; Rexov´a et al., 2003). These works all rely on multilingual data, whereas we construct phylogenetic trees from texts in a single language. 3 3.1 Methodology Dataset This corpus-based study uses Europarl (Koehn, 2005), the proceedings of the European Parliament and their translations into all the official Eu531 3.2 ropean Union (EU) languages. Europarl is one of the most popular parallel resources in natural language processing, and has been used extensively in machine translation. We use a version of Europarl spanning the years 1999 through 2011, in which the direction of translation has been established through a comprehensive cross-lingual validation of the speakers’ original language (Rabinovich et al., 2015). Features Following standard practice (Volansky et al., 2015; Rabinovich and Wintner, 2015), w"
P17-1049,P11-1132,1,0.945154,"for Computational Linguistics, pages 530–540 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1049 The claim that translations exhibit unique properties is well established in translation studies literature (Toury, 1980; Frawley, 1984; Baker, 1993; Toury, 1995). Based on this assumption, several works use text classification techniques employing supervised, and recently also unsupervised, machine learning approaches, to distinguish between originals and translations (Baroni and Bernardini, 2006; Ilisei et al., 2010; Koppel and Ordan, 2011; Volansky et al., 2015; Rabinovich and Wintner, 2015; Avner et al., 2016). The features used in these studies reflect both universal and interference-related traits. Along the way, interference was proven to be a robust phenomenon, operating in every single sentence, even on the morpheme level (Avner et al., 2016). Interference can also be studied on pairs of source- and target languages and focus, for example, on word order (Eetemadi and Toutanova, 2014). The powerful signal of interference is evident, e.g., by the finding that a classifier trained to distinguish between originals and transl"
P17-1049,2016.jeptalnrecital-invite.2,0,0.21511,"Missing"
P17-1049,P14-5010,0,0.00319695,"of languages within this family are not clear-cut (Ringe et al., 2002). Consequently, algorithms that attempt to reconstruct the IE languages tree face a serious evaluation challenge (Ringe et al., 2002; Rexov´a et al., 2003; Nakhleh et al., 2005a). To evaluate the quality of the reconstructed trees, we define a metric to accurately assess their distance from the “true” tree. The tree that we use as ground truth (Serva and Petroni, 2008) has All datasets were split on sentence boundary, cleaned (empty lines removed), tokenized, and annotated for part-of-speech (POS) using the Stanford tools (Manning et al., 2014). In all the tree reconstruction experiments, we sampled equal-sized chunks from each source language, using as much data as available for all languages. This yielded 27, 000 tokens from translations to English, and 30, 000 tokens from translations into French. 1 The common practice is that one translates into one’s native language; in particular, this practice is strictly imposed in the EU parliament where a translator must have perfect proficiency in the target language, meeting very high standards of accuracy. 2 We excluded source languages with insufficient amounts of data, along with Gree"
P17-1049,P13-1112,0,0.0433957,"Consequently, it is possible to accurately distinguish among translations from various source languages (van Halteren, 2008). A related task, identifying the native tongue of English language students based only on their writing in English, has been the subject of recent interest (Tetreault et al., 2013). The relations between this task and identification of the source language of translation has been emphazied, e.g., by Tsvetkov et al. (2013). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). In contrast to language learners, however, translators translate into their mother tongue, so the texts we studied were written by highly competent native speakers. Our work is the first to construct phylogenetic trees from translations. guage on the translation product. Toury (1995) defines interference as “phenomena pertaining to the make-up of the source text tend to be transferred to the target text”. Interference, by definition, is a language-pair specific phenomenon; isomorphic structures shared by the source and target languages can easily replace one another, th"
P17-1049,K16-1014,0,0.0165918,"e, which are also binary branching. Third, its branches are decorated with the approximate year in which splitting occurred. This provides a way to induce the distance between two languages, modeled as lengths of paths in the tree, based on chronological information. We projected the gold tree (Serva and Petroni, 2008) onto the set of 17 languages we considered in this work, preserving branch lengths. Figure 1 depicts the resulting gold-standard subtree. Several methods have been proposed for evaluating the quality of phylogenetic language trees (Pompei et al., 2011; Wichmann and Grant, 2012; Nouri and Yangarber, 2016). A popular metric is the Robinson-Foulds (RF) methodology (Robinson and Foulds, 1981), which is based on the symmetric difference in the number of bi-partitions, the ways in which an edge can split the leaves of a tree into two sets. The distance between two trees is then defined as the number of splits induced by one of the trees, but not the other. Despite its popularity, the RF metric has well-known shortcomings; for example, relocating a single leaf can result in a tree maximally distant from the original one (B¨ocker et al., 2013). Additional methodologies for evaluating phylogenetic tre"
P17-1049,W13-1706,0,0.0201234,".g., by the finding that a classifier trained to distinguish between originals and translations from one language, exhibits lower accuracy when tested on translations from another language, and this accuracy deteriorates proportionally to the distance between the source and target languages (Koppel and Ordan, 2011). Consequently, it is possible to accurately distinguish among translations from various source languages (van Halteren, 2008). A related task, identifying the native tongue of English language students based only on their writing in English, has been the subject of recent interest (Tetreault et al., 2013). The relations between this task and identification of the source language of translation has been emphazied, e.g., by Tsvetkov et al. (2013). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). In contrast to language learners, however, translators translate into their mother tongue, so the texts we studied were written by highly competent native speakers. Our work is the first to construct phylogenetic trees from translations. guage on the tran"
P17-1049,W13-1736,1,0.851892,"tested on translations from another language, and this accuracy deteriorates proportionally to the distance between the source and target languages (Koppel and Ordan, 2011). Consequently, it is possible to accurately distinguish among translations from various source languages (van Halteren, 2008). A related task, identifying the native tongue of English language students based only on their writing in English, has been the subject of recent interest (Tetreault et al., 2013). The relations between this task and identification of the source language of translation has been emphazied, e.g., by Tsvetkov et al. (2013). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). In contrast to language learners, however, translators translate into their mother tongue, so the texts we studied were written by highly competent native speakers. Our work is the first to construct phylogenetic trees from translations. guage on the translation product. Toury (1995) defines interference as “phenomena pertaining to the make-up of the source text tend to be transferred to the ta"
P17-1049,Q15-1030,1,0.947891,"Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1049 The claim that translations exhibit unique properties is well established in translation studies literature (Toury, 1980; Frawley, 1984; Baker, 1993; Toury, 1995). Based on this assumption, several works use text classification techniques employing supervised, and recently also unsupervised, machine learning approaches, to distinguish between originals and translations (Baroni and Bernardini, 2006; Ilisei et al., 2010; Koppel and Ordan, 2011; Volansky et al., 2015; Rabinovich and Wintner, 2015; Avner et al., 2016). The features used in these studies reflect both universal and interference-related traits. Along the way, interference was proven to be a robust phenomenon, operating in every single sentence, even on the morpheme level (Avner et al., 2016). Interference can also be studied on pairs of source- and target languages and focus, for example, on word order (Eetemadi and Toutanova, 2014). The powerful signal of interference is evident, e.g., by the finding that a classifier trained to distinguish between originals and translations from one language, exhibits lower accuracy whe"
P17-1049,C08-1118,0,0.0861077,"Missing"
P17-1049,D14-1018,0,\N,Missing
P17-1049,W14-1603,0,\N,Missing
P99-1013,P84-1027,0,0.699807,"l independent interpreter for it. For predicate logic the proof procedure behaves as such an interpreter.&quot; Shieber et al. (1995) view parsing as a. deductive process that proves claims a b o u t the grammatical status of strings from assumptions derived from the grammar. We follow their insight and notation and list a deductive system for parsing unification-based grammars. 2.3 D e n o t a t i o n a l s e m a n t i c s In this section we consider denotational semantics through a fixpoint of a transformational operator associated with grammars. -This is essentially similar to the definition of Pereira and Shieber (1984) and Carpenter (1992, pp. 204206). We then show that the denotational semantics is equivalent to the operational one. Associate with a grammar G an operator 7~ that, analogously to the immediate consequence operator of logic programming, can be thought of as a &quot;parsing step&quot; operator in the context of g r a m m a t i c a l formalisms. For the following discussion fix a particular g r a m m a r G = (n,E,A~). D e f i n i t i o n 3. The deductive parsing system associated with a grammar G = (7~,F.,AS} is defined over ITEMS and is characterized by: A x i o m s : [a, i, A, i + 1] i.f B E Z.(a) and"
P99-1013,P99-1013,1,0.0528894,"s. We review alternative approaches, operational and denotational, to the semantics of linguistic formalisms in section 2 and show that they are &quot;too crude&quot; to support grammar composition. Section 3 presents an alternative semantics, shown to be compositional (with respect to g r a m m a r union, a simple syntactic combination operation on grammars). However, this definition is &quot;too fine&quot;: in section 4 we present an adequate, compositional and fully-abstract semantics for linguistic formalisms. For lack of space, some proofs are omitted; an extended version is available as a technical report (Wintner, 1999). 2 *I a m g r a t e f u l to Nissim F r a n c e z for c o m m e n t i n g o n a n em&apos;lier version of t h i s paper. T h i s work was s u p p o r t e d by a n I R C S Fellowship a n d N S F g r a n t S B R 8920230. Grammar semantics Viewing grammars as formal entities that share many features with computer programs, it is 9{} n a t u r a l to consider the n o t i o n of semantics of ratification-based formalisms. We review in this se(:tion the o p e r a t i o n a l definition of Shieber et a,1. (1995) a n d the d e n o t a t i o n a l definition of, e.g., Pereira a n d Shieber (1984) or C a r"
Q15-1030,islam-mehler-2012-customization,0,0.0449647,"translated) mainly in the 19th century; and (iv) transcripts of TED and TEDx talks. This collection suggests diversity in genre, register, modality (written vs. spoken) and era. Table 1 details some statistical data on the corpora (after tokenization).3 We now briefly describe each dataset. Europarl is probably the most popular parallel corpus in natural language processing, and it was indeed used for many of the translationese tasks surveyed in Section 2. This corpus has been used extensively in SMT (Koehn et al., 2009), and was even adapted specifically for research in translation studies: Islam and Mehler (2012) compiled a customized version of Europarl, where the direction of translation is indicated. We use a version of Europarl (Rabinovich and Wintner, Forthcoming) that aims to further increase the confidence in the direction of translation, through a comprehensive cross-lingual validation of the original language of the speakers. The Hansard is a parallel corpus consisting of transcriptions of the Canadian parliament in English and French between 2001 and 2009. This is the largest available source of English–French sentence pairs. We use a version that is annotated with the original language of e"
Q15-1030,2009.mtsummit-papers.7,0,0.00616606,"ts of the Canadian Parliament, spanning years 2001-2009; (iii) literary classics written (or translated) mainly in the 19th century; and (iv) transcripts of TED and TEDx talks. This collection suggests diversity in genre, register, modality (written vs. spoken) and era. Table 1 details some statistical data on the corpora (after tokenization).3 We now briefly describe each dataset. Europarl is probably the most popular parallel corpus in natural language processing, and it was indeed used for many of the translationese tasks surveyed in Section 2. This corpus has been used extensively in SMT (Koehn et al., 2009), and was even adapted specifically for research in translation studies: Islam and Mehler (2012) compiled a customized version of Europarl, where the direction of translation is indicated. We use a version of Europarl (Rabinovich and Wintner, Forthcoming) that aims to further increase the confidence in the direction of translation, through a comprehensive cross-lingual validation of the original language of the speakers. The Hansard is a parallel corpus consisting of transcriptions of the Canadian parliament in English and French between 2001 and 2009. This is the largest available source of E"
Q15-1030,P11-1132,0,0.545383,"(2015), but we only consider the 1000 most frequent feature values extracted from each dataset (or a combination of datasets) being classified. This subset yields the same classification quality as the full set, reducing computation complexity. 3.3 Features 4 We focus on a set of features that reflect lexical and structural properties of the text, and have been shown to be effective for supervised classification of translationese (Volansky et al., 2015). Specifically, we use function words (FW), more precisely, the same list that was used in previous works on classification of translationese (Koppel and Ordan, 2011; Volansky et al., 2015). Feature values are raw counts (further denoted by term frequency, tf ), normalized by the number of tokens in the chunk; the chunk size may slightly vary, since the chunks respect sentence boundaries. For the clustering experiments we further scale the normalized tf by the inverse document frequency (idf), which offsets the importance of a term by a factor proportional to its frequency in the corpus. The tf-idf statistic has been shown to be effective with lexical features, and is often used as a weighting factor in information retrieval and text mining. While functio"
Q15-1030,2009.mtsummit-papers.9,0,0.797797,"Missing"
Q15-1030,D11-1034,1,0.400305,"Missing"
Q15-1030,J12-4004,1,0.86649,"ly in the 19th century; and (iv) transcripts of TED and TEDx talks. This collection suggests diversity in genre, register, modality (written vs. spoken) and era. Table 1 details some statistical data on the corpora (after tokenization).3 We now briefly describe each dataset. Europarl is probably the most popular parallel corpus in natural language processing, and it was indeed used for many of the translationese tasks surveyed in Section 2. This corpus has been used extensively in SMT (Koehn et al., 2009), and was even adapted specifically for research in translation studies: Islam and Mehler (2012) compiled a customized version of Europarl, where the direction of translation is indicated. We use a version of Europarl (Rabinovich and Wintner, Forthcoming) that aims to further increase the confidence in the direction of translation, through a comprehensive cross-lingual validation of the original language of the speakers. The Hansard is a parallel corpus consisting of transcriptions of the Canadian parliament in English and French between 2001 and 2009. This is the largest available source of English–French sentence pairs. We use a version that is annotated with the original language of e"
Q15-1030,J13-4007,1,0.8378,"Missing"
Q15-1030,C12-2076,0,0.0600745,"67 7,441 Table 1: Corpus statistics The Literature corpus consists of literary classics written (and translated) in the 18th–20th centuries by English and French authors; the raw material is available from the Gutenberg project. We use subsets that were manually or automatically paragraphaligned. Note that classifying literary texts is considered a more challenging task than classifying more “technical” translations, such as parliament proceedings, since translators of literature typically enjoy more literary freedom, thereby rendering the translation product more similar to original writing (Lynch and Vogel, 2012; Avner et al., Forthcoming). Our TED talks corpus consists of talks originally given in English and talks translated to English from French. The quality of translations in this corpus is very high: not only are translators assumed to be competent, but the common practice is that each translation passes through a review before being published. This corpus consists of talks delivered orally, but we assume that they were meticulously prepared, so the language is not spontaneous but rather planned. Compared to the other sub-corpora, the TED dataset has some unique characteristics that stem from t"
Q15-1030,P14-5010,0,0.001671,"racteristics that stem from the following reasons: (i) its size is relatively small; (ii) it exhibits stylistic disparity between the original and translated texts (the former contains more “oral” markers of a spoken language, while the latter is a written translation); and finally (iii) TED talks are not transcribed but are rather subtitled, so they undergo some editing and rephrasing.4 The vast majority of TED talks are publicly available online, which makes this corpus easily extendable for future research. 3.2 Processing and Tools All datasets are first tokenized using the Stanford tools (Manning et al., 2014) and then partitioned into 4 http://translations.ted.org/wiki/How_ to_Compress_Subtitles 422 chunks of approximately 2000 tokens (ending on a sentence boundary). We assume that translationeserelated features are present in the texts across author or speaker, thus we allow some chunks to contain linguistic information from two or more different texts simultaneously. For the main (single-corpus) classification experiments we use 2000 text chunks each from Europarl and Hansard, 800 from Literature and 88 chunks from TED; each sub-corpus consists of an equal number of original and translated chunk"
Q15-1030,R13-1070,0,0.0470947,"Missing"
Q15-1030,R11-1091,0,0.121487,"Missing"
Q15-1030,C08-1118,0,0.66087,"Missing"
Q15-1030,2005.mtsummit-papers.11,0,\N,Missing
Q15-1030,E12-1026,1,\N,Missing
Q18-1024,P14-2134,0,0.0237569,"Low German: vrecht ← Proto-Germanic *fra- + *aihtiz Mid. English: wery ← Old English: w¯eri˙g ← Proto-Germanic: *w¯or¯ıgaz French: fatigue ← French: fatiguer ← Latin: fatigare Latin: exaggerare ← Latin: ex- + Latin: aggerare English: over + do Table 2: Etymological roots of example synonym sets with corresponding part-of-speech. we employed fallback to unigram probability estimation. Additionally, we replaced all non-English words with the token ‘UNK’; and all web links, subreddit (e.g., r/compling) and user (u/userid) pointers with the ‘URL’ token.10 4.3.2 Distance estimation and clustering Bamman et al. (2014) introduced a model for incorporating contextual information (such as geography) in learning vector representations. They proposed a joint model for learning word representations in a situated language, a model that “includes information about a subject (i.e., the speaker), allowing to learn the contours of a word’s meaning that are shaped by the context in which it is uttered”. Using a large corpus of tweets, their joint model learned word representations that were sensitive to geographical factors, demonstrating that the usage of wicked in the United States (meaning bad or evil ) differs fro"
Q18-1024,N12-1033,0,0.0241827,"ile our research questions are similar, we present a computational analysis of the effects of cognates on L2 productions on a completely different scale: 31 languages, over 1000 words, and thousands of speakers whose spontaneous language production is recorded in a very large corpus. Corpus-based investigation of non-native language has been a prolific field of recent research. Numerous studies address syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based;"
Q18-1024,W14-1603,0,0.0231749,"cts on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of cognates in L2. From the lexical perspective, L2 writers have been shown to produce more overgeneralizations, use more frequent words and words with a lower degree of ambiguity (Hinkel, 2002; Crossley and McNamara, 2011). Several studies addressed crosslinguistic influences on semantic"
Q18-1024,K15-1010,0,0.239326,"language production is recorded in a very large corpus. Corpus-based investigation of non-native language has been a prolific field of recent research. Numerous studies address syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of cognates in L2. From the lexical perspective, L2 writers have been shown to produce m"
Q18-1024,de-melo-2014-etymological,0,0.0698098,"Missing"
Q18-1024,W17-5033,0,0.0269104,"opean language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of cognates in L2. From the lexical perspective, L2 writers have been shown to produce more overgeneralizations, use more frequent words and words with a lower degree of ambiguity (Hinkel, 2002; Crossley and McNamara, 2011). Several studies addressed crosslinguistic influences on semantic acquisition in L2, investigating the distribution of collocations (Siyanova-Chanturia, 2015; Kochmar and Shutova, 2017) and formulaic language (Paquot and Granger, 2012) in learner corpora. We, in contrast, address highly-fluent, advanced non-natives in this work. Nastase and Strapparava (2017) presented the first attempt to leverage etymological information for the task of native language identification of English learners. They sowed the seeds for exploitation of etymological clues in the study of non-native language, but their results were very inconclusive. In contrast to the learner corpora that dominate studies in this field (Granger, 2003; Geertzen et al., 2013; Blanchard et al., 2013), our corpus conta"
Q18-1024,W17-5007,0,0.226054,"cale: 31 languages, over 1000 words, and thousands of speakers whose spontaneous language production is recorded in a very large corpus. Corpus-based investigation of non-native language has been a prolific field of recent research. Numerous studies address syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of cognat"
Q18-1024,P13-1112,0,0.0296171,"ress syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of cognates in L2. From the lexical perspective, L2 writers have been shown to produce more overgeneralizations, use more frequent words and words with a lower degree of ambiguity (Hinkel, 2002; Crossley and McNamara, 2011). Several studies addressed crosslinguistic"
Q18-1024,D17-1286,0,0.0529633,"tic structures, but rather on the unique use of cognates in L2. From the lexical perspective, L2 writers have been shown to produce more overgeneralizations, use more frequent words and words with a lower degree of ambiguity (Hinkel, 2002; Crossley and McNamara, 2011). Several studies addressed crosslinguistic influences on semantic acquisition in L2, investigating the distribution of collocations (Siyanova-Chanturia, 2015; Kochmar and Shutova, 2017) and formulaic language (Paquot and Granger, 2012) in learner corpora. We, in contrast, address highly-fluent, advanced non-natives in this work. Nastase and Strapparava (2017) presented the first attempt to leverage etymological information for the task of native language identification of English learners. They sowed the seeds for exploitation of etymological clues in the study of non-native language, but their results were very inconclusive. In contrast to the learner corpora that dominate studies in this field (Granger, 2003; Geertzen et al., 2013; Blanchard et al., 2013), our corpus contains spontaneous productions of advanced, highly proficient non-native speakers, spanning over 80K topical threads, by 45K distinct users from 50 countries (with 46 native langu"
Q18-1024,P17-1049,1,0.909998,"Missing"
Q18-1024,W13-1706,0,0.132543,"s on L2 productions on a completely different scale: 31 languages, over 1000 words, and thousands of speakers whose spontaneous language production is recorded in a very large corpus. Corpus-based investigation of non-native language has been a prolific field of recent research. Numerous studies address syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic stru"
Q18-1024,N01-1031,0,0.0243586,"mited set of languages. While our research questions are similar, we present a computational analysis of the effects of cognates on L2 productions on a completely different scale: 31 languages, over 1000 words, and thousands of speakers whose spontaneous language production is recorded in a very large corpus. Corpus-based investigation of non-native language has been a prolific field of recent research. Numerous studies address syntactic transfer effects on L2. Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015). English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014). Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017). Our stud"
R19-1130,N16-3003,0,0.0179517,"tated (Kurokawa et al., 2009; Rabinovich et al., 2016b; Tolochinsky et al., 2018). We cleaned the data by removing editor’s comments and sentences with fewer than 5 tokens. We then down-sampled the corpora and extracted equally-sized subsets with 50,000 sentence-pairs in each language pair, distributed evenly across translation direction. These are the data we used in all the experiments described below.2 Details on the available data are presented in Table 2. Preprossessing We preprocessed the data as follows. First, all words in the two languages were tagged for part of speech using FARASA (Abdelali et al., 2016) for Arabic and TreeTagger (Schmid, 1995) for the other languages. Second, all the sentence pairs were word aligned using FastAlign (Dyer et al., 2013). With the word alignments we were able to extract the features that will be explained in the next section. Classification For the task of identifying the translation direction, we implemented various feature sets and used them for training a Logistic Regression classifier (with the implementation of Pedregosa et al. (2011)), mainly because it is faster 2 The only other parallel corpora that we are aware of where the direction of translation is"
R19-1130,W05-0909,0,0.0150188,"a human and once automatically. In contrast, f1 = HT (e1 ) and f2 = M T (e1 ); both f1 and f2 are only “once removed” from e1 : f1 was translated manually and f2 automatically, but only once. Therefore, we expect f1 and f2 (two French sentences) to be closer to each other than e1 and e2 (two English sentences) are. This is only the case if f1 is the translation of e1 ; if the translation direction is reversed, we would expect e1 and e2 to be closer to each other than f1 and f2 are. To measure the similarity between the two sentences we used three metrics: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and Levenshtein distance (Levenshtein, 1965). Each metric results in two scores: one for the distance between the two English sentences and one for the two French sentences. These six scores were used as features for the classifier. 6 Results Table 3 depicts the accuracy of 10-fold cross validation evaluation of classifiers reflecting the various features. The “All” column indicates a dataset constructed from the French–English sentence pairs in all the three different corpora; it is therefore a heterogenous dataset, which makes the task much more challenging (Rabinovich and Wintner, 2015)."
R19-1130,Q17-1010,0,0.0212815,"Missing"
R19-1130,J92-4003,0,0.429977,"Missing"
R19-1130,P11-2031,0,0.0281657,"g Marian (Junczys-Dowmunt et al., 2018). In one system (FO), the training material consisted only of French original sentence pairs; in the other (EO), we only used English original sentence pairs; and in the third (MIX), we mixed equal portions of both. In all three cases we used an equal number of sentence pairs (1.6 million). We tested the three NMT systems on a reference set of 10,000 sentences taken from French original data, following the methodology of Lembersky et al. (2013). We evaluated the quality of the resulting NMT systems by comparing BLEU, METEOR and TER scores using MultEval (Clark et al., 2011). The results, listed in Table 1, clearly corroborate our hypothesis: for the task of French to English translation, training data that were manually translated from French to English yield much better NMT systems than training data that were translated in the reverse direction. Train Data FO MIX EO BLEU↑ 41.0 38.2 34.4 METEOR↑ 38.4 36.7 35.0 TER↓ 46.1 48.5 52.8 Table 1: Accuracy of NMT systems with varying configurations of the training material 4 Methodology Task Given a sentence pair in a parallel corpus, our task is to identify the direction of translation, 1133 POS English CD one IN of NP"
R19-1130,N13-1073,0,0.0307098,"er than 5 tokens. We then down-sampled the corpora and extracted equally-sized subsets with 50,000 sentence-pairs in each language pair, distributed evenly across translation direction. These are the data we used in all the experiments described below.2 Details on the available data are presented in Table 2. Preprossessing We preprocessed the data as follows. First, all words in the two languages were tagged for part of speech using FARASA (Abdelali et al., 2016) for Arabic and TreeTagger (Schmid, 1995) for the other languages. Second, all the sentence pairs were word aligned using FastAlign (Dyer et al., 2013). With the word alignments we were able to extract the features that will be explained in the next section. Classification For the task of identifying the translation direction, we implemented various feature sets and used them for training a Logistic Regression classifier (with the implementation of Pedregosa et al. (2011)), mainly because it is faster 2 The only other parallel corpora that we are aware of where the direction of translation is marked are the Dutch Parallel Corpus (Macken et al., 2011), aligning Dutch with English abd French, and EuroParl-UdS (Karakanta et al., 2018), which la"
R19-1130,2005.mtsummit-papers.11,0,0.152873,"ask Given a sentence pair in a parallel corpus, our task is to identify the direction of translation, 1133 POS English CD one IN of NP Africa’s RBS most German POS Einer PIS der ART ber¨uhmtesten ADJA afrikanischen ADJA JJ famous NNS teachers Lehrer NN Figure 3: POS-MTUs, English–German thereby determining the source and the target sentences. Our main challenge is to define a set of features that will yield the best accuracy. Datasets We used sentence-aligned parallel corpora from three resources: the Canadian parliamentary proceedings (Hansard), with English– French sentence pairs; Europarl (Koehn, 2005), the proceedings of the European Parliament, where English is aligned with French and German; and the UN parallel corpora (Ziemski et al., 2016), in which English is aligned with Arabic, French, German, Russian and Spanish. We used subsets of these corpora in which the direction of translation has been accurately annotated (Kurokawa et al., 2009; Rabinovich et al., 2016b; Tolochinsky et al., 2018). We cleaned the data by removing editor’s comments and sentences with fewer than 5 tokens. We then down-sampled the corpora and extracted equally-sized subsets with 50,000 sentence-pairs in each lan"
R19-1130,P11-1132,0,0.446891,"the special properties of translated texts, and in particular the asymmetric nature of translation (Toury, 1980, 1995; Baker, 1993). These include the tendency of translated texts to be simpler (Blum-Kulka and Levenston, 1983; Vanderauwerea, 1985; Baker, 1993; Laviosa, 1998, 2002); the tendency of translators to explicate the source text (Blum-Kulka, 1986; Baker, 1993); the different distributions of various statistical phenomena (e.g., the frequencies of function words or certain syntactic structures) between the source and the translation (Gellerstam, 1986; Blum-Kulka, 1986; Øver˚as, 1998; Koppel and Ordan, 2011); and interference of language constructions from the source to the target (Toury, 1979; Teich, 2003). The contribution of this paper is manifold. (1) First and foremost, we introduce a method for accurately determining the translation direction of sentence pairs in parallel corpora; the method is based on the introduction of several new, linguistically motivated, types of features for this task. We show that the combination of these features outperforms the previous state-of-the-art in detection of translation direction.1 Importantly, these features help shed light on the characteristics of t"
R19-1130,2009.mtsummit-papers.9,0,0.977645,"uage pairs. Furthermore, we demonstrate that the accuracy is correlated with the (typological) distance between the two languages. 1 Introduction Parallel corpora are used for various purposes, including for training and evaluation of statistical machine translation (SMT) systems (Koehn, 2010). While traditional SMT systems are agnostic with respect to the direction in which the parallel corpora they are trained on were (manually) translated, several studies have shown that taking directionality into account when training SMT systems has a significant effect on the quality of the translation (Kurokawa et al., 2009; Lembersky et al., 2012, 2013; Twitto-Shmuel et al., 2015). In this paper we show the same effect also holds for neural machine translation (NMT) systems. We address the task of determining the direction of translation given a parallel text; this is cast as a binary classification task. To strain the classifier, we focus on retaining high accuracy when the size of text chunks to be classified is minimal: single sentence pairs. This is an extremely difficult task for humans, in most cases: a single sentence pair often does not reveal any obvious signal of which of the two sentences is the orig"
R19-1130,D14-1018,0,0.0727895,"015). One of our goals in this work is to improve the accuracy of translationese detection systems with much smaller text chunks, as available parallel texts are not guaranteed to be long. Previous research focused on identifying translationese in monolingual texts. However, in realistic scenarios, parallel texts are available and the actual task is to determine the direction of translation given texts in two languages. For such tasks one can use features drawn from each of the two languages, as well as from the alignments between words and phrases in the two texts. This approach was taken by Eetemadi and Toutanova (2014), who used the Canadian Hansard corpus of parallel texts in English and French. The motivation stems from the observation that linguistic structures tend to have different distributions in original and translated texts. Therefore, assessing the frequencies of syntactic structures in two parallel texts, especially for text chunks that are aligned with each other across two parallel sentences, may shed light on the direction of the translation. As base structures, Eetemadi and Toutanova (2014) used minimal translation units (MTUs), defined as pairs of source and target word sets that satisfy two"
R19-1130,N15-2014,0,0.035873,"Missing"
R19-1130,J12-4004,1,0.830103,", we demonstrate that the accuracy is correlated with the (typological) distance between the two languages. 1 Introduction Parallel corpora are used for various purposes, including for training and evaluation of statistical machine translation (SMT) systems (Koehn, 2010). While traditional SMT systems are agnostic with respect to the direction in which the parallel corpora they are trained on were (manually) translated, several studies have shown that taking directionality into account when training SMT systems has a significant effect on the quality of the translation (Kurokawa et al., 2009; Lembersky et al., 2012, 2013; Twitto-Shmuel et al., 2015). In this paper we show the same effect also holds for neural machine translation (NMT) systems. We address the task of determining the direction of translation given a parallel text; this is cast as a binary classification task. To strain the classifier, we focus on retaining high accuracy when the size of text chunks to be classified is minimal: single sentence pairs. This is an extremely difficult task for humans, in most cases: a single sentence pair often does not reveal any obvious signal of which of the two sentences is the original. It is also a highl"
R19-1130,J13-4007,1,0.879042,"Missing"
R19-1130,C08-1118,0,0.0642495,"Missing"
R19-1130,P18-4020,0,0.0315875,"Missing"
R19-1130,P02-1040,0,0.105967,"m e1 , being translated once by a human and once automatically. In contrast, f1 = HT (e1 ) and f2 = M T (e1 ); both f1 and f2 are only “once removed” from e1 : f1 was translated manually and f2 automatically, but only once. Therefore, we expect f1 and f2 (two French sentences) to be closer to each other than e1 and e2 (two English sentences) are. This is only the case if f1 is the translation of e1 ; if the translation direction is reversed, we would expect e1 and e2 to be closer to each other than f1 and f2 are. To measure the similarity between the two sentences we used three metrics: BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and Levenshtein distance (Levenshtein, 1965). Each metric results in two scores: one for the distance between the two English sentences and one for the two French sentences. These six scores were used as features for the classifier. 6 Results Table 3 depicts the accuracy of 10-fold cross validation evaluation of classifiers reflecting the various features. The “All” column indicates a dataset constructed from the French–English sentence pairs in all the three different corpora; it is therefore a heterogenous dataset, which makes the task much more challengi"
R19-1130,D18-1548,0,0.0282705,"Missing"
R19-1130,D14-1162,0,0.0822684,"Missing"
R19-1130,P16-1176,1,0.901275,"riginal message; they tend to use more standard language than originals), while others are related to interference, namely the “fingerprints” of the source language found in the translation product. Distinguishing between original and translated texts is a classic text classification task that has been extensively addressed both with supervised machine learning (Baroni and Bernardini, 2006; van Halteren, 2008; Kurokawa et al., 2009; Koppel and Ordan, 2011; Ilisei et al., 2010; Volansky et al., 2015; Avner et al., 2016) and with unsupervised methods (Rabinovich and Wintner, 2015; Nisioi, 2015; Rabinovich et al., 2016a). The main challenge, as is usually the case in text classification, lies in the choice of features with which text chunks are represented. For the task at hand, features frequently used include function words (FW), character n-grams, part-of-speech (POS) ngrams, special sets of words such as discourse markers, etc. With the right choice of features, accuracies can reach almost ceiling levels, depending on the dataset involved. However, the classification unit used in all the above-mentioned research was larger chunks of text, typically 2,000 tokens. The accuracy of identifying translationes"
R19-1130,P17-1049,1,0.850138,"lines of Strubell et al. (2018). Finally, observe that the results clearly support our theoretical hypothesis: the accuracy of the classification improves when the two languages involved are more typologically distant. The task is particularly hard for English-French and English-German, and easiest for English-Arabic and English-Russian. We tentatively conclude, therefore, that translationese is more pronounced, and interference is more powerful, when the two languages are more distant. This chimes in with recent results that show the relationships between interference and language typology (Rabinovich et al., 2017). 7 Conclusion We have shown that linguistically-motivated features, based on Translation Studies insights pertaining to the asymmetry of the translation process, can yield high, state-of-the-art accuracy on the task of translation direction detection. We introduced several novel features and used stacking to produce highly accurate sentence-pair-level classifiers for five language pairs. We also confirmed the hypothesis that this task is harder when the two languages involved are more closely related. In future work, we intend to provide a deeper analysis of the results, focusing on the const"
R19-1130,Q15-1030,1,0.905987,".g., translated texts tend to simplify the original message; they tend to use more standard language than originals), while others are related to interference, namely the “fingerprints” of the source language found in the translation product. Distinguishing between original and translated texts is a classic text classification task that has been extensively addressed both with supervised machine learning (Baroni and Bernardini, 2006; van Halteren, 2008; Kurokawa et al., 2009; Koppel and Ordan, 2011; Ilisei et al., 2010; Volansky et al., 2015; Avner et al., 2016) and with unsupervised methods (Rabinovich and Wintner, 2015; Nisioi, 2015; Rabinovich et al., 2016a). The main challenge, as is usually the case in text classification, lies in the choice of features with which text chunks are represented. For the task at hand, features frequently used include function words (FW), character n-grams, part-of-speech (POS) ngrams, special sets of words such as discourse markers, etc. With the right choice of features, accuracies can reach almost ceiling levels, depending on the dataset involved. However, the classification unit used in all the above-mentioned research was larger chunks of text, typically 2,000 tokens. Th"
R19-1130,W15-3002,1,0.736286,"racy is correlated with the (typological) distance between the two languages. 1 Introduction Parallel corpora are used for various purposes, including for training and evaluation of statistical machine translation (SMT) systems (Koehn, 2010). While traditional SMT systems are agnostic with respect to the direction in which the parallel corpora they are trained on were (manually) translated, several studies have shown that taking directionality into account when training SMT systems has a significant effect on the quality of the translation (Kurokawa et al., 2009; Lembersky et al., 2012, 2013; Twitto-Shmuel et al., 2015). In this paper we show the same effect also holds for neural machine translation (NMT) systems. We address the task of determining the direction of translation given a parallel text; this is cast as a binary classification task. To strain the classifier, we focus on retaining high accuracy when the size of text chunks to be classified is minimal: single sentence pairs. This is an extremely difficult task for humans, in most cases: a single sentence pair often does not reveal any obvious signal of which of the two sentences is the original. It is also a highly challenging task for machines: Fi"
R19-1130,L16-1561,0,0.0237635,"frica’s RBS most German POS Einer PIS der ART ber¨uhmtesten ADJA afrikanischen ADJA JJ famous NNS teachers Lehrer NN Figure 3: POS-MTUs, English–German thereby determining the source and the target sentences. Our main challenge is to define a set of features that will yield the best accuracy. Datasets We used sentence-aligned parallel corpora from three resources: the Canadian parliamentary proceedings (Hansard), with English– French sentence pairs; Europarl (Koehn, 2005), the proceedings of the European Parliament, where English is aligned with French and German; and the UN parallel corpora (Ziemski et al., 2016), in which English is aligned with Arabic, French, German, Russian and Spanish. We used subsets of these corpora in which the direction of translation has been accurately annotated (Kurokawa et al., 2009; Rabinovich et al., 2016b; Tolochinsky et al., 2018). We cleaned the data by removing editor’s comments and sentences with fewer than 5 tokens. We then down-sampled the corpora and extracted equally-sized subsets with 50,000 sentence-pairs in each language pair, distributed evenly across translation direction. These are the data we used in all the experiments described below.2 Details on the a"
R19-1130,K17-3009,0,0.0614891,"Missing"
tsvetkov-wintner-2010-automatic,resnik-1998-parallel,0,\N,Missing
tsvetkov-wintner-2010-automatic,A00-1004,0,\N,Missing
tsvetkov-wintner-2010-automatic,J03-3002,0,\N,Missing
tsvetkov-wintner-2010-automatic,C10-2144,1,\N,Missing
tsvetkov-wintner-2010-automatic,P09-1030,0,\N,Missing
tsvetkov-wintner-2010-automatic,J00-2004,0,\N,Missing
tsvetkov-wintner-2010-automatic,P99-1068,0,\N,Missing
tsvetkov-wintner-2010-automatic,2005.mtsummit-papers.11,0,\N,Missing
tsvetkov-wintner-2010-automatic,1999.mtsummit-1.79,0,\N,Missing
W02-0110,J94-3001,0,\N,Missing
W04-3246,W02-0506,0,0.258822,"Missing"
W04-3246,W01-0502,1,0.889779,"Missing"
W04-3246,W02-2010,0,0.143443,"fier that is specifically tailored for learning in domains in which the potential number of information sources (features) taking part in decisions is very large, of which NLP is a principal example. It works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. SNoW has already been used successfully as the learning vehicle in a large collection of natural language related tasks, including POS tagging, shallow parsing, information extraction tasks, etc., and compared favorably with other classifiers (Roth, 1998; Punyakanok and Roth, 2001; Florian, 2002). Typically, SNoW is used as a classifier, and predicts using a winner-take-all mechanism over the activation values of the target classes. However, in addition to the prediction, it provides a reliable confidence level in the prediction, which enables its use in an inference algorithm that combines predictors to produce a coherent inference. 4.1 Feature types All the experiments we describe in this work share the same features and differ only in the target classifiers. The features that are used to characterize a word are both grammatical and statistical: • Location of letters (e.g., the thir"
W04-3246,P94-1025,0,0.284892,"Missing"
W04-3246,W03-0419,0,\N,Missing
W04-3246,W98-1007,0,\N,Missing
W05-0702,C96-1017,0,0.035389,"Missing"
W05-0702,W98-1007,0,0.168422,"Missing"
W05-0702,C88-1064,0,0.705065,"Missing"
W05-0702,J00-1006,0,0.307112,"Missing"
W05-0702,2004.tmi-1.1,1,0.738135,"includes over 20,000 items. The average number of inflected forms for a lexicon item is 33 (not including prefix sequences). Due to the use of finite-state technology, the grammar can be used for generation or for analysis. It induces a very efficient morphological analyzer: in practice, over eighty words per second can be analyzed on a contemporary workstation. For lack of space we cannot fully demonstrate the output of the analyzer; refer back to figure 1 for an example. HAMSAH is now used for a number of projects, including as a front end for a Hebrew to English machine translation system (Lavie et al., 2004). It is routinely tested on a variety of texts, and tokens with zero analyses are being inspected manually. A systematic evaluation of the quality of the analyzer is difficult due to the lack of available alternative resources. Nevertheless, we conducted a small-scale evaluation experiment by asking two annotators to review the output produced by the analyzer for a randomly chosen set of newspaper articles comprising of approximately 1000 word tokens. The following table summarizes the results of this experiment. Figure 8: Adding the suffix h Figure 9 shows how plural nouns with the wt suffix"
W05-0702,J94-3001,0,\N,Missing
W05-0702,itai-etal-2006-computational,1,\N,Missing
W05-0702,W05-0706,0,\N,Missing
W05-0702,W03-0804,0,\N,Missing
W05-1108,J94-3001,0,\N,Missing
W05-1108,E99-1017,0,\N,Missing
W05-1108,W00-1804,0,\N,Missing
W05-1108,P95-1003,0,\N,Missing
W05-1108,W05-0702,1,\N,Missing
W05-1108,P96-1015,0,\N,Missing
W07-0604,J96-1002,0,0.00259386,"the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure). This parsing approach is very similar to the one used successfully by Nivre et al. (2006), but we use a maximum entropy classifier (Berger et al., 1996) to determine parser actions, which makes parsing extremely fast. In addition, our parsing approach performs a search over the space of possible parser actions, while Nivre et al.’s approach is deterministic. See Sagae and Tsujii (2007) for more information on the parser. Features used in classification to determine whether the parser takes a shift or a reduce action at any point during parsing are derived from the parser’s current configuration (contents of the stack and queue) at that point. The specific features used are:4 • Word and its POS tag: s(1), q(2), and q(1). • POS: s(3) and q(2)."
W07-0604,W06-2920,0,0.0359076,"arsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie (2006). Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly. This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007)3 . Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combin"
W07-0604,A00-2018,0,0.284917,"ng punctuation) and 65,363 words. The average utterance length is 5.3 words (including punctuation) for adult utterances, 3.6 for child, 4.5 overall. The annotated Eve corpus is available at http://childes.psy.cmu. edu/data/Eng-USA/brown.zip. It was used for the Domain adaptation task at the CoNLL-2007 dependency parsing shared task (Nivre, 2007). 3 Parsing Although the CHILDES annotation scheme proposed by Sagae et al. (2004) has been used in practice for automatic parsing of child language transcripts (Sagae et al., 2004; Sagae et al., 2005), such work relied mainly on a statistical parser (Charniak, 2000) trained on the Wall Street Journal portion of the Penn Treebank, since a large enough corpus of annotated CHILDES data was not available to train a domain-specific parser. Having a corpus of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allows us to develop a parser tailored for the CHILDES domain. Our overall parsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing a"
W07-0604,W06-2933,0,0.0264284,"used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure). This parsing approach is very similar to the one used successfully by Nivre et al. (2006), but we use a maximum entropy classifier (Berger et al., 1996) to determine parser actions, which makes parsing extremely fast. In addition, our parsing approach performs a search over the space of possible parser actions, while Nivre et al.’s approach is deterministic. See Sagae and Tsujii (2007) for more information on the parser. Features used in classification to determine whether the parser takes a shift or a reduce action at any point during parsing are derived from the parser’s current configuration (contents of the stack and queue) at that point. The specific features used are:4 • Wor"
W07-0604,P06-2089,1,0.365037,"reebank, since a large enough corpus of annotated CHILDES data was not available to train a domain-specific parser. Having a corpus of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allows us to develop a parser tailored for the CHILDES domain. Our overall parsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie (2006). Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly. This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007)3 . Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic"
W07-0604,D07-1111,1,0.215691,"bilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie (2006). Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly. This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007)3 . Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens. At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure). This parsing a"
W07-0604,sagae-etal-2004-adding,1,0.867299,"Missing"
W07-0604,P05-1025,1,0.927916,"0 adult and 8,563 child. The utterances consist of 84,226 GRs (including punctuation) and 65,363 words. The average utterance length is 5.3 words (including punctuation) for adult utterances, 3.6 for child, 4.5 overall. The annotated Eve corpus is available at http://childes.psy.cmu. edu/data/Eng-USA/brown.zip. It was used for the Domain adaptation task at the CoNLL-2007 dependency parsing shared task (Nivre, 2007). 3 Parsing Although the CHILDES annotation scheme proposed by Sagae et al. (2004) has been used in practice for automatic parsing of child language transcripts (Sagae et al., 2004; Sagae et al., 2005), such work relied mainly on a statistical parser (Charniak, 2000) trained on the Wall Street Journal portion of the Penn Treebank, since a large enough corpus of annotated CHILDES data was not available to train a domain-specific parser. Having a corpus of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allows us to develop a parser tailored for the CHILDES domain. Our overall parsing approach uses a best-first 28 probabilistic shift-reduce algorithm, working left-toright to find labeled dependencies one at a time. The algorithm is essenti"
W07-0909,N03-1003,0,0.0109416,"wo entities, which we term here Relational IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1 http://jakarta.apache.org/lucene/docs/index.html Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the ex"
W07-0909,itai-etal-2006-computational,1,0.829879,"l the partial templates of a learned template. These are templates that contain just one of variables in the original template. We then generate rules between these partial templates that correspond to the original rules. With partial templates/rules, expansion for the query in Figure 1 becomes possible. 3.3 Cross-lingual IR Until very recently, linguistic resources for Hebrew were few and far between (Wintner, 2004). The last few years, however, have seen a proliferation of resources and tools for this language. In this work we utilize a relatively large-scale lexicon of over 22,000 entries (Itai et al., 2006); a finite-state based morphological analyzer of Hebrew that is directly linked to the lexicon (Yona and Wintner, 2007); a mediumsize bilingual dictionary of some 24,000 word pairs; and a rudimentary Hebrew to English machine translation system (Lavie et al., 2004). All these resources had to be adapted to the domain of the Hecht museum. Cross-lingual language technology is utilized in Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under ‘matched query’) do not contain the original query. three different compone"
W07-0909,2004.tmi-1.1,1,0.912872,"r the query in Figure 1 becomes possible. 3.3 Cross-lingual IR Until very recently, linguistic resources for Hebrew were few and far between (Wintner, 2004). The last few years, however, have seen a proliferation of resources and tools for this language. In this work we utilize a relatively large-scale lexicon of over 22,000 entries (Itai et al., 2006); a finite-state based morphological analyzer of Hebrew that is directly linked to the lexicon (Yona and Wintner, 2007); a mediumsize bilingual dictionary of some 24,000 word pairs; and a rudimentary Hebrew to English machine translation system (Lavie et al., 2004). All these resources had to be adapted to the domain of the Hecht museum. Cross-lingual language technology is utilized in Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under ‘matched query’) do not contain the original query. three different components of the system: Hebrew documents are morphologically processed to provide better indexing; query terms in English are translated to Hebrew and vice versa; and Hebrew snippets are translated to English. We discuss each of these components in this section. Linguis"
W07-0909,P02-1006,0,0.208246,"the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may b"
W07-0909,E06-1052,1,0.803107,"ed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may be unable to read Hebrew texts, but still benefit"
W07-0909,I05-5011,0,0.0136238,"trieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1 http://jakarta.apache.org/lucene/docs/index.html Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the extracted anchor-sets. The retrieved sentences are parsed an"
W07-0909,P03-1029,0,0.0952938,"phrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may be unable to read Hebrew te"
W07-0909,W04-3206,1,0.810829,"IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1 http://jakarta.apache.org/lucene/docs/index.html Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the extracted anchor-sets. The retrieved sentenc"
W07-0909,C02-1161,0,0.0245238,"n Retrieval (IR), it is crucial to recognize that a specific target meaning can be inferred from different text variants. For example, a QA system needs to induce that “Mendelssohn wrote incidental music” can be inferred from “Mendelssohn composed incidental music” in order to answer the question “Who wrote incidental music?”. This type of reasoning has been identified as a core semantic in66 ference task by the generic textual entailment framework (Dagan et al., 2006; Bar-Haim et al., 2006). The typical way to address variability in IR is to use lexical query expansion (Lytinen et al., 2000; Zukerman and Raskutti, 2002). However, there are variability patterns that cannot be described using just constant phrase to phrase entailment. Another important type of knowledge representation is entailment rules and paraphrases. An entailment rule is a directional relation between two templates, text patterns with variables, e.g., ‘X compose Y → X write Y ’. The left hand side is assumed to entail the right hand side in certain contexts, under the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building block"
W12-0514,2009.eamt-1.9,0,0.224916,"d MT system. Her reported results show high accuracy on the task of correctly generating a preposition, but the overall improvement in the quality of the translation is not reported. Li et al. (2005) focus on three English prepositions (on, in and at) and use WordNet to infer semantic properties of the immediate context of the preposition in order to correctly translate it to Chinese. Again, this requires language resources that are unavailable to us. WordNet (and a parser) are used also by Naskar and Bandyopadhyay (2006), who work on English-toBengali translation. The closest work to ours is Agirre et al. (2009), who translate from Spanish to Basque in a rulebased framework. Like us, they focus on prepositional phrases that modify verbs, and include also the direct object (and the subject) in their approach. They propose three techniques for correctly translating prepositions, based on information that is automatically extracted from monolingual resources (including verb-preposition-head dependency triplets and verb subcategorization) as well as manually-crafted selection rules that rely on lexical, syntactic and semantic information. Our method is similar in principle, the main differences being: (i"
W12-0514,P91-1027,0,0.0178146,"ing verb-preposition-head dependency triplets and verb subcategorization) as well as manually-crafted selection rules that rely on lexical, syntactic and semantic information. Our method is similar in principle, the main differences being: (i) we incorporate linguistic knowledge in a statistical decoder, facilitating scalability of the MT system, (ii) we use much more modest resources (in particular, we do not parse either of the two languages), and (iii) we report standard evaluation measures. Much work has been done regarding the automatic acquisition of subcategorization frames in English (Brent, 1991; Manning, 1993; Briscoe and Carroll, 1997; Korhonen, 2002), Czech (Sarkar and Zeman, 2000), French (Chesley and Salmon-alt, 2006), and several other languages. The technique that we use here (Section 6) can now be considered standard. 3 Introduction to Stat-XFER The method we propose is implemented in the framework of Stat-XFER (Lavie, 2008), a statistical machine translation engine that includes a declarative formalism for symbolic transfer grammars. A grammar consists of a collection of synchronous context-free rules, which can be augmented by unification-style feature constraints. These tr"
W12-0514,A97-1052,0,0.0606151,"pendency triplets and verb subcategorization) as well as manually-crafted selection rules that rely on lexical, syntactic and semantic information. Our method is similar in principle, the main differences being: (i) we incorporate linguistic knowledge in a statistical decoder, facilitating scalability of the MT system, (ii) we use much more modest resources (in particular, we do not parse either of the two languages), and (iii) we report standard evaluation measures. Much work has been done regarding the automatic acquisition of subcategorization frames in English (Brent, 1991; Manning, 1993; Briscoe and Carroll, 1997; Korhonen, 2002), Czech (Sarkar and Zeman, 2000), French (Chesley and Salmon-alt, 2006), and several other languages. The technique that we use here (Section 6) can now be considered standard. 3 Introduction to Stat-XFER The method we propose is implemented in the framework of Stat-XFER (Lavie, 2008), a statistical machine translation engine that includes a declarative formalism for symbolic transfer grammars. A grammar consists of a collection of synchronous context-free rules, which can be augmented by unification-style feature constraints. These transfer rules specify how phrase structures"
W12-0514,chesley-salmon-alt-2006-automatic,0,0.0152654,"rules that rely on lexical, syntactic and semantic information. Our method is similar in principle, the main differences being: (i) we incorporate linguistic knowledge in a statistical decoder, facilitating scalability of the MT system, (ii) we use much more modest resources (in particular, we do not parse either of the two languages), and (iii) we report standard evaluation measures. Much work has been done regarding the automatic acquisition of subcategorization frames in English (Brent, 1991; Manning, 1993; Briscoe and Carroll, 1997; Korhonen, 2002), Czech (Sarkar and Zeman, 2000), French (Chesley and Salmon-alt, 2006), and several other languages. The technique that we use here (Section 6) can now be considered standard. 3 Introduction to Stat-XFER The method we propose is implemented in the framework of Stat-XFER (Lavie, 2008), a statistical machine translation engine that includes a declarative formalism for symbolic transfer grammars. A grammar consists of a collection of synchronous context-free rules, which can be augmented by unification-style feature constraints. These transfer rules specify how phrase structures in a source-language correspond and transfer to phrase structures in a target language,"
W12-0514,W11-2107,0,0.0456434,"Missing"
W12-0514,2005.eamt-1.16,0,0.186925,"anslating prepositions was suggested by Trujillo (1995), who deals with the problem of translating spatial prepositions between Spanish and English in the context of a lexicalist transfer-based MT framework. Trujillo (1995) categorizes spatial prepositions according to a lexical-semantic hierarchy, and after parsing the source language sentence, uses the representation of prepositions in the transfer process, showing improvement in performance compared to other transfer-based systems. This requires resources much beyond those that are available for Arabic and Hebrew. More recent works include Gustavii (2005), who uses transformation-based learning to infer rules that can correct the choice of preposition made by a rule-based MT system. Her reported results show high accuracy on the task of correctly generating a preposition, but the overall improvement in the quality of the translation is not reported. Li et al. (2005) focus on three English prepositions (on, in and at) and use WordNet to infer semantic properties of the immediate context of the preposition in order to correctly translate it to Chinese. Again, this requires language resources that are unavailable to us. WordNet (and a parser) are"
W12-0514,W09-0425,0,0.0192249,"ls the underlying parsing and transfer process. Crucially, Stat-XFER is a statistical MT framework, which uses statistical information to weigh word translations, phrase correspon107 dences and target-language hypotheses; in contrast to other paradigms, however, it can utilize both automatically-created and manually-crafted language resources, including dictionaries, morphological processors and transfer rules. StatXFER has been used as a platform for developing MT systems for Hindi-to-English (Lavie et al., 2003), Hebrew-to-English (Lavie et al., 2004), Chinese-to-English, French-to-English (Hanneman et al., 2009) and many other low-resource language pairs, such as Inupiaq-to-English and Mapudungun-to-Spanish. In this work, we use the Arabic-to-Hebrew MT system developed by Shilon et al. (2010), which uses over 40 manually-crafted rules. Other resources include Arabic morphological analyzer and disambiguator (Habash, 2004), Hebrew morphological generator (Itai and Wintner, 2008) and a Hebrew language model compiled from available corpora (Itai and Wintner, 2008). While our proposal is cast within the framework of Stat-XFER, it can be in principle adapted to other syntax-based approaches to MT; specific"
W12-0514,2004.tmi-1.1,1,0.796562,"ar combination of several features, and a beam-search controls the underlying parsing and transfer process. Crucially, Stat-XFER is a statistical MT framework, which uses statistical information to weigh word translations, phrase correspon107 dences and target-language hypotheses; in contrast to other paradigms, however, it can utilize both automatically-created and manually-crafted language resources, including dictionaries, morphological processors and transfer rules. StatXFER has been used as a platform for developing MT systems for Hindi-to-English (Lavie et al., 2003), Hebrew-to-English (Lavie et al., 2004), Chinese-to-English, French-to-English (Hanneman et al., 2009) and many other low-resource language pairs, such as Inupiaq-to-English and Mapudungun-to-Spanish. In this work, we use the Arabic-to-Hebrew MT system developed by Shilon et al. (2010), which uses over 40 manually-crafted rules. Other resources include Arabic morphological analyzer and disambiguator (Habash, 2004), Hebrew morphological generator (Itai and Wintner, 2008) and a Hebrew language model compiled from available corpora (Itai and Wintner, 2008). While our proposal is cast within the framework of Stat-XFER, it can be in pri"
W12-0514,W06-2113,0,0.152028,"nsformation-based learning to infer rules that can correct the choice of preposition made by a rule-based MT system. Her reported results show high accuracy on the task of correctly generating a preposition, but the overall improvement in the quality of the translation is not reported. Li et al. (2005) focus on three English prepositions (on, in and at) and use WordNet to infer semantic properties of the immediate context of the preposition in order to correctly translate it to Chinese. Again, this requires language resources that are unavailable to us. WordNet (and a parser) are used also by Naskar and Bandyopadhyay (2006), who work on English-toBengali translation. The closest work to ours is Agirre et al. (2009), who translate from Spanish to Basque in a rulebased framework. Like us, they focus on prepositional phrases that modify verbs, and include also the direct object (and the subject) in their approach. They propose three techniques for correctly translating prepositions, based on information that is automatically extracted from monolingual resources (including verb-preposition-head dependency triplets and verb subcategorization) as well as manually-crafted selection rules that rely on lexical, syntactic"
W12-0514,P02-1040,0,0.0824448,"Missing"
W12-0514,C00-2100,0,0.0243472,"ll as manually-crafted selection rules that rely on lexical, syntactic and semantic information. Our method is similar in principle, the main differences being: (i) we incorporate linguistic knowledge in a statistical decoder, facilitating scalability of the MT system, (ii) we use much more modest resources (in particular, we do not parse either of the two languages), and (iii) we report standard evaluation measures. Much work has been done regarding the automatic acquisition of subcategorization frames in English (Brent, 1991; Manning, 1993; Briscoe and Carroll, 1997; Korhonen, 2002), Czech (Sarkar and Zeman, 2000), French (Chesley and Salmon-alt, 2006), and several other languages. The technique that we use here (Section 6) can now be considered standard. 3 Introduction to Stat-XFER The method we propose is implemented in the framework of Stat-XFER (Lavie, 2008), a statistical machine translation engine that includes a declarative formalism for symbolic transfer grammars. A grammar consists of a collection of synchronous context-free rules, which can be augmented by unification-style feature constraints. These transfer rules specify how phrase structures in a source-language correspond and transfer to"
W12-0514,2010.amta-srw.4,1,0.900975,"ces and target-language hypotheses; in contrast to other paradigms, however, it can utilize both automatically-created and manually-crafted language resources, including dictionaries, morphological processors and transfer rules. StatXFER has been used as a platform for developing MT systems for Hindi-to-English (Lavie et al., 2003), Hebrew-to-English (Lavie et al., 2004), Chinese-to-English, French-to-English (Hanneman et al., 2009) and many other low-resource language pairs, such as Inupiaq-to-English and Mapudungun-to-Spanish. In this work, we use the Arabic-to-Hebrew MT system developed by Shilon et al. (2010), which uses over 40 manually-crafted rules. Other resources include Arabic morphological analyzer and disambiguator (Habash, 2004), Hebrew morphological generator (Itai and Wintner, 2008) and a Hebrew language model compiled from available corpora (Itai and Wintner, 2008). While our proposal is cast within the framework of Stat-XFER, it can be in principle adapted to other syntax-based approaches to MT; specifically, Williams and Koehn (2011) show how to employ unification-based constraints to the targetside of a string-to-tree model, integrating constrain evaluation into the decoding process"
W12-0514,W11-2126,0,0.0157457,"any other low-resource language pairs, such as Inupiaq-to-English and Mapudungun-to-Spanish. In this work, we use the Arabic-to-Hebrew MT system developed by Shilon et al. (2010), which uses over 40 manually-crafted rules. Other resources include Arabic morphological analyzer and disambiguator (Habash, 2004), Hebrew morphological generator (Itai and Wintner, 2008) and a Hebrew language model compiled from available corpora (Itai and Wintner, 2008). While our proposal is cast within the framework of Stat-XFER, it can be in principle adapted to other syntax-based approaches to MT; specifically, Williams and Koehn (2011) show how to employ unification-based constraints to the targetside of a string-to-tree model, integrating constrain evaluation into the decoding process. 4 Translating prepositions between Hebrew and Arabic Modern Hebrew and Modern Standard Arabic, both closely-related Semitic languages, share many orthographic, lexical, morphological, syntactic and semantic similarities, but they are still not mutually comprehensible. Machine translation between these two languages can indeed benefit from the similarities, but it remains a challenging task. Our current work is situated in the framework of th"
W12-0514,H91-1067,0,\N,Missing
W12-0514,P93-1032,0,\N,Missing
W12-0904,nir-etal-2010-morphologically,1,0.620794,"Missing"
W13-1736,J92-4003,0,0.123486,"Missing"
W13-1736,U07-1006,0,0.0307986,"fy that language. This task has a clear empirical motivation. Nonnative speakers make different errors when they write English, depending on their native language (Lado, 1957; Swan and Smith, 2001); understanding the different types of errors is a prerequisite for correcting them (Leacock et al., 2010), and systems such as the one we describe here can shed interesting light on such errors. Tutoring applications can use our system to identify the native language of students and offer better-targeted advice. Forensic linguistic applications are sometimes required to determine the L1 of authors (Estival et al., 2007b; Estival et al., 2007a). Additionally, we believe that the task is interesting in and of itself, providing a better understanding of non-native language. We are thus equally interested in defining meaningful features whose contribution to the task can be linguistically interpreted. Briefly, our features draw heavily on prior work in general text classification and authorship identification, those used in identifying so-called translationese (Volansky et al., forthcoming), and a class of features that involves determining what minimal changes would be necessary to transform the essays into “s"
W13-1736,C90-2036,0,0.0232636,"rb and Masuda, 2008). Since English’s orthography is largely phonemic—even if it is irregular in many places, we expect leaners whose native phoneme contrasts are different from those of English to make characteristic spelling errors. For example, since Japanese and Korean lack a phonemic /l/-/r/ contrast, we expect native speakers of those languages to be more likely to make spelling errors that confuse l and r relative to native speakers of languages such as Spanish in which that pair is contrastive. To make this information available to our model, we use a noisy channel spelling corrector (Kernighan, 1990) to identify and correct misspelled words in the training and test data. From these corrections, we extract minimal edit features that show what insertions, deletions, substitutions and joinings (where two separate words are written merged into a single orthographic token) were made by the author of the essay. Restored tags We focus on three important token classes defined above: punctuation marks, function words and cohesive verbs. We first remove words in these classes from the texts, and then recover the most likely hidden tokens in a sequence of words, according to an n-gram language model"
W13-1736,P08-1068,0,0.0252399,"tions). To restore hidden tokens we use the hidden-ngram utility provided in SRI’s language modeling toolkit (Stolcke, 2002). Brown clusters (Brown et al., 1992) describe an algorithm that induces a hierarchical clustering of a language’s vocabulary based on each vocabulary item’s tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in n-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text (Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013). Using an open-source implementation of the algorithm,2 we clustered 8 billion words of English into 600 classes.3 We included log counts of all 4-grams of Brown clusters that occurred at least 100 times in the NLI training data. 5.1 Main Features We use the following four feature types as the baseline features in our model. For features that are sensitive to frequency, we use the log of the (frequencyplus-one) as the feature’s value. Table 2 reports the accuracy of using each feature type in isolation (with 2 https://github.com/percyliang/brown-clu"
W13-1736,P11-1132,1,0.750772,"e markers These are 40 function words (and short phrases) that have a strong discourse function in texts (however, because, in fact, etc.). Translators tend to spell out implicit utterances and render them explicitly in the target text (Blum-Kulka, 1986). We use the list of Volansky et al. (forthcoming). Cohesive verbs This is a list of manually compiled verbs that are used, like cohesive markers, to spell out implicit utterances (indicate, imply, contain, etc.). Function words Frequent tokens, which are mostly function words, have been used successfully for various text classification tasks. Koppel and Ordan (2011) define a list of 400 such words, of which we only use 100 (using the entire list was not significantly different). Note that pronouns are included in this list. Contextual function words To further capitalize on the ability of function words to discriminate, we define pairs consisting of a function word from the list mentioned above, along with the POS tag of its adjacent word. This feature captures patterns such as verbs and the preposition or particle immediately to their right, or nouns and the determiner that precedes them. We also define 3-grams consisting of one or two function words an"
W13-1736,N13-1039,1,0.79567,"Missing"
W13-1736,W13-1706,0,0.102937,"Missing"
W13-1736,N03-1033,0,0.010714,"etreault et al., 2013). The training data consists of 1000 essays from each native language. The essays are short, consisting of 10 to 20 sentences each. We used the provided splits of 900 documents for training and 100 for development. Each document is annotated with the author’s English proficiency level (low, medium, high) and an identification (1 to 8) of the essay prompt. All essays are tokenized and split into sentences. In table 1 we provide some statistics on the training corpora, listed by the authors’ proficiency level. All essays were tagged with the Stanford part-of-speech tagger (Toutanova et al., 2003). We did not parse the dataset. # Documents # Tokens # Types Low 1,069 245,130 13,110 Medium 5,366 1,819,407 37,393 High 3,456 1,388,260 28,329 Table 1: Training set statistics. 4 Model For our classification model we used the creg regression modeling framework to train a 11-class logistic regression classifier.1 We parameterize the classifier as a multiclass logistic regression: P exp j λ j h j (x, y) pλ (y |x) = , Zλ (x) where x are documents, h j (·) are real-valued feature functions of the document being classified, λ j are the corresponding weights, and y is one of the eleven L1 class lab"
W13-1736,W07-0602,0,0.263634,"Missing"
W13-1736,P10-1040,0,0.0107663,"hidden tokens we use the hidden-ngram utility provided in SRI’s language modeling toolkit (Stolcke, 2002). Brown clusters (Brown et al., 1992) describe an algorithm that induces a hierarchical clustering of a language’s vocabulary based on each vocabulary item’s tendency to appear in similar left and right contexts in a training corpus. While originally developed to reduce the number of parameters required in n-gram language models, Brown clusters have been found to be extremely effective as lexical representations in a variety of regression problems that condition on text (Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013). Using an open-source implementation of the algorithm,2 we clustered 8 billion words of English into 600 classes.3 We included log counts of all 4-grams of Brown clusters that occurred at least 100 times in the NLI training data. 5.1 Main Features We use the following four feature types as the baseline features in our model. For features that are sensitive to frequency, we use the log of the (frequencyplus-one) as the feature’s value. Table 2 reports the accuracy of using each feature type in isolation (with 2 https://github.com/percyliang/brown-cluster http://www.ark.c"
W13-1736,U09-1008,0,0.0944702,"Missing"
W13-1736,D11-1148,0,0.0757912,"Missing"
W15-3002,D14-1018,0,0.79575,"2009) showed that French-to-English SMT systems whose translation models were constructed 47 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 47–57, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. is typically unavailable. Due to the unique properties of translationese, however, this information can be determined automatically using textclassification techniques. Several works address this task, using various feature sets, and reporting excellent accuracy (Baroni and Bernardini, 2006; van Halteren, 2008; Ilisei et al., 2010; Eetemadi and Toutanova, 2014). Some of these works, however, only conduct in-domain evaluation; much evidence suggests that out-of-domain accuracy is much lower (Koppel and Ordan, 2011; Islam and Hoenen, 2013; Avner et al., Forthcoming). A thorough investigation was conducted by Volansky et al. (2015), who focused on the features of translationese (in English) from a translation theory perspective. They defined several classifiers based on various linguisticallyinformed features, implementing several hypotheses of Translation Studies. We adopt some of their best-performing classifiers in this work.1 The main contribution"
W15-3002,N15-2014,0,0.393737,"Missing"
W15-3002,P96-1041,0,0.169637,"slationese; we extend the experimentation also to French and adapt their classifiers accordingly. 48 boundaries, we use the UIUC CCG tool.2 We use MOSES (Koehn et al., 2007) for tokenization and case normalization. Part-of-speech (POS) tagging is done with OpenNLP3 for English and the Stanford tagger4 for French. For classification we use Weka (Hall et al., 2009) with the SMO algorithm, a support-vector machine with a linear kernel, in its default configuration. To construct language models and measure perplexity, we use SRILM (Stolcke, 2002) with interpolated modified Kneser-Ney discounting (Chen and Goodman, 1996) and with a fixed vocabulary. We limit language models to a fixed vocabulary and map out-of-vocabulary (OOV) tokens to a unique symbol to overcome sparsity and better control the OOV rates among various corpora. We train and build the SMT systems using MOSES. For evaluation we use MultEval (Clark et al., 2011), which takes machine translation hypotheses from several runs of an optimizer and provides three popular metric scores, BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006)), as well as standard deviations and p-values. 3.2 3.2.1 Language model"
W15-3002,E12-1026,1,0.928935,"Missing"
W15-3002,2005.mtsummit-papers.11,0,0.0684974,"Missing"
W15-3002,J12-4004,1,0.868278,", underscore the relevance of translationese for SMT. Kurokawa et al. (2009) were the first to show that translationese matters for SMT. They defined two translation tasks, English-to-French and Frenchto-English, and used a parallel corpus in which the translation direction of each text was indicated. They showed that for the English-to-French task, translation models compiled from Englishtranslated-to-French texts were better than translation models compiled from texts translated in the reverse direction; and the same holds for the reverse translation task. These results were corroborated by Lembersky et al. (2012a, 2013), who further demonstrated that translation models can be adapted to translationese, thereby improving the quality of SMT even further. Lembersky et al. (2011, 2012b) focused on the language model (LM). They built several SMT systems for several pairs of languages. For each language pair they built two systems, one in which the LM was compiled from original English text, and another in which the LM was compiled from text translated to English from each of the languages. They showed that LMs complied from translated texts better fit the reference set in term of perplexity. Moreover, SMT"
W15-3002,P07-2045,0,0.00454273,"ion; to ensure that the length of each text does not influence the classification, we partition the training corpus in most experiments into chunks of approximately 2000 tokens (ending on a sentence boundary). We henceforth use chunk units to define the size of a sub-corpus. Our major experiments involve 2,500 chunks (of approximately 2,000 tokens each, hence 5M tokens). To detect sentence 1 Volansky et al. (2015) only identified English translationese; we extend the experimentation also to French and adapt their classifiers accordingly. 48 boundaries, we use the UIUC CCG tool.2 We use MOSES (Koehn et al., 2007) for tokenization and case normalization. Part-of-speech (POS) tagging is done with OpenNLP3 for English and the Stanford tagger4 for French. For classification we use Weka (Hall et al., 2009) with the SMO algorithm, a support-vector machine with a linear kernel, in its default configuration. To construct language models and measure perplexity, we use SRILM (Stolcke, 2002) with interpolated modified Kneser-Ney discounting (Chen and Goodman, 1996) and with a fixed vocabulary. We limit language models to a fixed vocabulary and map out-of-vocabulary (OOV) tokens to a unique symbol to overcome spa"
W15-3002,J13-4007,1,0.843871,"Missing"
W15-3002,P02-1040,0,0.0946313,"tion. To construct language models and measure perplexity, we use SRILM (Stolcke, 2002) with interpolated modified Kneser-Ney discounting (Chen and Goodman, 1996) and with a fixed vocabulary. We limit language models to a fixed vocabulary and map out-of-vocabulary (OOV) tokens to a unique symbol to overcome sparsity and better control the OOV rates among various corpora. We train and build the SMT systems using MOSES. For evaluation we use MultEval (Clark et al., 2011), which takes machine translation hypotheses from several runs of an optimizer and provides three popular metric scores, BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006)), as well as standard deviations and p-values. 3.2 3.2.1 Language model experiments Our main experiments focus on French translated to English (FR→EN), and we define a classifier that can identify English translationese. However, to further establish the robustness of our approach, we also experiment with German translated to English (DE→EN) and with English translated to French (EN→FR). We also conduct cross-corpus experiments in which we train translationese classifier on one corpus (Europarl) and test its contribution to SM"
W15-3002,P11-1132,1,0.957309,"pages 47–57, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. is typically unavailable. Due to the unique properties of translationese, however, this information can be determined automatically using textclassification techniques. Several works address this task, using various feature sets, and reporting excellent accuracy (Baroni and Bernardini, 2006; van Halteren, 2008; Ilisei et al., 2010; Eetemadi and Toutanova, 2014). Some of these works, however, only conduct in-domain evaluation; much evidence suggests that out-of-domain accuracy is much lower (Koppel and Ordan, 2011; Islam and Hoenen, 2013; Avner et al., Forthcoming). A thorough investigation was conducted by Volansky et al. (2015), who focused on the features of translationese (in English) from a translation theory perspective. They defined several classifiers based on various linguisticallyinformed features, implementing several hypotheses of Translation Studies. We adopt some of their best-performing classifiers in this work.1 The main contribution of this work is a general approach that, provided labeled data for training classifiers, can be applied to any corpus before it is used for constructing SM"
W15-3002,Q15-1030,1,0.931709,"to the unique properties of translationese, however, this information can be determined automatically using textclassification techniques. Several works address this task, using various feature sets, and reporting excellent accuracy (Baroni and Bernardini, 2006; van Halteren, 2008; Ilisei et al., 2010; Eetemadi and Toutanova, 2014). Some of these works, however, only conduct in-domain evaluation; much evidence suggests that out-of-domain accuracy is much lower (Koppel and Ordan, 2011; Islam and Hoenen, 2013; Avner et al., Forthcoming). A thorough investigation was conducted by Volansky et al. (2015), who focused on the features of translationese (in English) from a translation theory perspective. They defined several classifiers based on various linguisticallyinformed features, implementing several hypotheses of Translation Studies. We adopt some of their best-performing classifiers in this work.1 The main contribution of this work is a general approach that, provided labeled data for training classifiers, can be applied to any corpus before it is used for constructing SMT systems, resulting in systems that are as good as (or better than) those that use the entire corpus, but that rely o"
W15-3002,2009.mtsummit-papers.9,0,0.96595,"of the corpus that are translated in the direction of the translation task, and using only them for training the translation model, is again as good as using the entire corpus. We present results from several language pairs and various data sets, indicating that the approach we advocate is general and robust. Introduction Research in Translation Studies suggests that translated texts are considerably different from original texts, constituting a sublanguage known as Translationese (Gellerstam, 1986). Awareness to translationese can significantly improve statistical machine translation (SMT). Kurokawa et al. (2009) showed that French-to-English SMT systems whose translation models were constructed 47 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 47–57, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. is typically unavailable. Due to the unique properties of translationese, however, this information can be determined automatically using textclassification techniques. Several works address this task, using various feature sets, and reporting excellent accuracy (Baroni and Bernardini, 2006; van Halteren, 2008; Ilisei et al., 2010; Eetem"
W15-3002,D11-1034,1,0.94851,"of Computer Science University of Haifa Israel naama.twitto@gmail.com Noam Ordan Cluster of Excellence, MMCI Universit¨at des Saarlandes Germany noam.ordan@gmail.com Abstract shuly@cs.haifa.ac.il from human translations from French to English yielded better translation quality than ones created from translations in the other direction. These results were corroborated by Lembersky et al. (2012a, 2013), who showed that translation models can be adapted to translationese, thereby improving the quality of SMT even further. Awareness to translationese also benefits the language models used in SMT: Lembersky et al. (2011, 2012b) showed that language models complied from translated texts better fit the reference sets in term of perplexity, and SMT systems constructed from such language models perform much better than those constructed from original texts. Translated texts (in any language) are so markedly different from original ones that text classification techniques can be used to tease them apart. Previous work has shown that awareness to these differences can significantly improve statistical machine translation. These results, however, required meta-information on the ontological status of texts (origina"
W15-3002,2006.amta-papers.25,0,0.0198506,"LM (Stolcke, 2002) with interpolated modified Kneser-Ney discounting (Chen and Goodman, 1996) and with a fixed vocabulary. We limit language models to a fixed vocabulary and map out-of-vocabulary (OOV) tokens to a unique symbol to overcome sparsity and better control the OOV rates among various corpora. We train and build the SMT systems using MOSES. For evaluation we use MultEval (Clark et al., 2011), which takes machine translation hypotheses from several runs of an optimizer and provides three popular metric scores, BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011), and TER (Snover et al., 2006)), as well as standard deviations and p-values. 3.2 3.2.1 Language model experiments Our main experiments focus on French translated to English (FR→EN), and we define a classifier that can identify English translationese. However, to further establish the robustness of our approach, we also experiment with German translated to English (DE→EN) and with English translated to French (EN→FR). We also conduct cross-corpus experiments in which we train translationese classifier on one corpus (Europarl) and test its contribution to SMT on another (Hansard, News). These experiments are crucial for eva"
W15-3002,C08-1118,0,0.391095,"Missing"
W15-3002,P11-2031,0,\N,Missing
W15-3002,W11-2107,0,\N,Missing
W98-1429,P95-1035,0,0.0509499,"Missing"
