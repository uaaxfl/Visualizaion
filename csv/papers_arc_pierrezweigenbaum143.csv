2021.eval4nlp-1.1,Differential Evaluation: a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing,2021,-1,-1,6,0,8588,lucie gianola,Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems,0,None
2020.lrec-1.241,Handling Entity Normalization with no Annotated Corpus: Weakly Supervised Methods Based on Distributional Representation and Ontological Information,2020,-1,-1,6,1,17095,arnaud ferre,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Entity normalization (or entity linking) is an important subtask of information extraction that links entity mentions in text to categories or concepts in a reference vocabulary. Machine learning based normalization methods have good adaptability as long as they have enough training data per reference with a sufficient quality. Distributional representations are commonly used because of their capacity to handle different expressions with similar meanings. However, in specific technical and scientific domains, the small amount of training data and the relatively small size of specialized corpora remain major challenges. Recently, the machine learning-based CONTES method has addressed these challenges for reference vocabularies that are ontologies, as is often the case in life sciences and biomedical domains. And yet, its performance is dependent on manually annotated corpus. Furthermore, like other machine learning based methods, parametrization remains tricky. We propose a new approach to address the scarcity of training data that extends the CONTES method by corpus selection, pre-processing and weak supervision strategies, which can yield high-performance results without any manually annotated examples. We also study which hyperparameters are most influential, with sometimes different patterns compared to previous work. The results show that our approach significantly improves accuracy and outperforms previous state-of-the-art algorithms."
2020.latechclfl-1.20,{TL}-Explorer: A Digital Humanities Tool for Mapping and Analyzing Translated Literature,2020,-1,-1,6,0,18549,alex zhai,"Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"TL-Explorer is a digital humanities tool for mapping and analyzing translated literature, encompassing the World Map and the Translation Dashboard. The World Map displays collected literature of different languages, locations, and cultures and establishes the foundation for further analysis. It comprises three global maps for spatial and temporal interpretation. A further investigation into an individual point on the map leads to the Translation Dashboard. Each point represents one edition or translation. Collected translations are processed in order to build multilingual parallel corpora for a large number of under-resourced languages as well as to highlight the transnational circulation of knowledge. Our first rendition of TL-Explorer was conducted on the well-traveled American novel, Adventures of Huckleberry Finn, by Mark Twain. The maps currently chronicle nearly 400 translations of this novel. And the dashboard supports over 30 collected translations. However, the TL-Explore is easily extended to other works of literature and is not limited to type of texts, such as academic manuscripts or constitutional documents to name a few."
2020.eamt-1.57,The Multilingual Anonymisation Toolkit for Public Administrations ({MAPA}) Project,2020,-1,-1,19,0,17549,eriks ajausks,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"We describe the MAPA project, funded under the Connecting Europe Facility programme, whose goal is the development of an open-source de-identification toolkit for all official European Union languages. It will be developed since January 2020 until December 2021."
2020.coling-main.609,{C}haracter{BERT}: Reconciling {ELM}o and {BERT} for Word-Level Open-Vocabulary Representations From Characters,2020,-1,-1,5,1,8589,hicham boukkouri,Proceedings of the 28th International Conference on Computational Linguistics,0,"Due to the compelling improvements brought by BERT, many recent representation models adopted the Transformer architecture as their main building block, consequently inheriting the wordpiece tokenization system despite it not being intrinsically linked to the notion of Transformers. While this system is thought to achieve a good balance between the flexibility of characters and the efficiency of full words, using predefined wordpiece vocabularies from the general domain is not always suitable, especially when building models for specialized domains (e.g., the medical domain). Moreover, adopting a wordpiece tokenization shifts the focus from the word level to the subword level, making the models conceptually more complex and arguably less convenient in practice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations."
2020.bucc-1.2,Overview of the Fourth {BUCC} Shared Task: Bilingual Dictionary Induction from Comparable Corpora,2020,-1,-1,2,0,20907,reinhard rapp,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,0,"The shared task of the 13th Workshop on Building and Using Comparable Corpora was devoted to the induction of bilingual dictionaries from comparable rather than parallel corpora. In this task, for a number of language pairs involving Chinese, English, French, German, Russian and Spanish, the participants were supposed to determine automatically the target language translations of several thousand source language test words of three frequency ranges. We describe here some background, the task definition, the training and test data sets and the evaluation used for ranking the participating systems. We also summarize the approaches used and present the results of the evaluation. In conclusion, the outcome of the competition are the results of a number of systems which provide surprisingly good solutions to the ambitious problem."
P19-2041,Embedding Strategies for Specialized Domains: Application to Clinical Entity Recognition,2019,0,1,4,1,8589,hicham boukkouri,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Using pre-trained word embeddings in conjunction with Deep Learning models has become the {``}de facto{''} approach in Natural Language Processing (NLP). While this usually yields satisfactory results, off-the-shelf word embeddings tend to perform poorly on texts from specialized domains such as clinical reports. Moreover, training specialized word representations from scratch is often either impossible or ineffective due to the lack of large enough in-domain data. In this work, we focus on the clinical domain for which we study embedding strategies that rely on general-domain resources only. We show that by combining off-the-shelf contextual embeddings (ELMo) with static word2vec embeddings trained on a small in-domain corpus built from the task data, we manage to reach and sometimes outperform representations learned from a large corpus in the medical domain."
W18-1702,Efficient Generation and Processing of Word Co-occurrence Networks Using corpus2graph,2018,0,1,2,1,7018,zheng zhang,Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing ({T}ext{G}raphs-12),0,"Corpus2graph is an open-source NLP-application-oriented tool that generates a word co-occurrence network from a large corpus. It not only contains different built-in methods to preprocess words, analyze sentences, extract word pairs and define edge weights, but also supports user-customized functions. By using parallelization techniques, it can generate a large word co-occurrence network of the whole English Wikipedia data within hours. And thanks to its nodes-edges-weight three-level progressive calculation design, rebuilding networks with different configurations is even faster as it does not need to start all over again. This tool also works with other graph libraries such as igraph, NetworkX and graph-tool as a front end providing data to boost network generation speed."
P18-2090,{GNEG}: Graph-Based Negative Sampling for word2vec,2018,0,6,2,1,7018,zheng zhang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Negative sampling is an important component in word2vec for distributed word representation learning. We hypothesize that taking into account global, corpus-level information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution. In this purpose we pre-compute word co-occurrence statistics from the corpus and apply to it network algorithms such as random walk. We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5{\%} and improves the performance on word similarity tasks by about 1{\%} compared to the skip-gram negative sampling baseline."
L18-1025,Three Dimensions of Reproducibility in Natural Language Processing,2018,0,3,3,0,29526,bretonnel cohen,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Despite considerable recent attention to problems with reproducibility of scientific research, there is a striking lack of agreement about even the definition of the term. That is a problem, because the lack of a consensus definition makes it difficult to compare studies of reproducibility, and thus to have even a broad overview of the state of the issue in natural language processing. This paper proposes an ontology of reproducibility in that field. We show that three dimensions of reproducibility, corresponding to three kinds of claims in natural language processing papers, can account for a variety of types of research reports. These dimensions are reproducibility of a conclusion, of a finding, and of a value. Three biomedical natural language processing papers by the authors of this paper are analyzed with respect to these dimensions."
L18-1543,Combining rule-based and embedding-based approaches to normalize textual entities with an ontology,2018,0,1,3,1,17095,arnaud ferre,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1582,Automating Document Discovery in the Systematic Review Process: How to Use Chaff to Extract Wheat,2018,0,2,3,1,23937,christopher norman,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Systematic reviews in e.g. empirical medicine address research questions by comprehensively examining the entire published literature. Conventionally, manual literature surveys decide inclusion in two steps, first based on abstracts and title, then by full text, yet currentn methods to automate the process make no distinction between gold data from these two stages. In this work we compare the impact different schemes for choosing positive and negative examples from the different screening stages have on the training of automated systems. We train a ranker using logistic regression and evaluate it on a new gold standard dataset for clinical NLP, and on an existing gold standard dataset for drug class efficacy. The classification and ranking achieves an average AUC of 0.803 and 0.768 when relying on gold standard decisions based on title and abstracts of articles, and an AUC of 0.625 and 0.839 when relying on gold standard decisions based on full text. Our results suggest that it makes little difference which screening stage the gold standard decisions are drawn from, and that the decisions need not be based on the full text. The results further suggest that common-off-the-shelf algorithms can reduce the amount of work required to retrieve relevant literature."
L18-1605,A Multilingual Dataset for Evaluating Parallel Sentence Extraction from Comparable Corpora,2018,0,0,1,1,8591,pierre zweigenbaum,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
2018.jeptalnrecital-court.27,D{\\'e}tection des couples de termes translitt{\\'e}r{\\'e}s {\\`a} partir d{'}un corpus parall{\\`e}le anglais-arabe (),2018,-1,-1,3,1,30997,wafa neifar,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,
W17-2510,z{NLP}: Identifying Parallel Sentences in {C}hinese-{E}nglish Comparable Corpora,2017,8,3,2,1,7018,zheng zhang,Proceedings of the 10th Workshop on Building and Using Comparable Corpora,0,"This paper describes the zNLP system for the BUCC 2017 shared task. Our system identifies parallel sentence pairs in Chinese-English comparable corpora by translating word-by-word Chinese sentences into English, using the search engine Solr to select near-parallel sentences and then by using an SVM classifier to identify true parallel sentences from the previous results. It obtains an F1-score of 45{\%} (resp. 32{\%}) on the test (training) set."
W17-2512,Overview of the Second {BUCC} Shared Task: Spotting Parallel Sentences in Comparable Corpora,2017,9,7,1,1,8591,pierre zweigenbaum,Proceedings of the 10th Workshop on Building and Using Comparable Corpora,0,"This paper presents the BUCC 2017 shared task on parallel sentence extraction from comparable corpora. It recalls the design of the datasets, presents their final construction and statistics and the methods used to evaluate system results. 13 runs were submitted to the shared task by 4 teams, covering three of the four proposed language pairs: French-English (7 runs), German-English (3 runs), and Chinese-English (3 runs). The best F-scores as measured against the gold standard were 0.84 (German-English), 0.80 (French-English), and 0.43 (Chinese-English). Because of the design of the dataset, in which not all gold parallel sentence pairs are known, these are only minimum values. We examined manually a small sample of the false negative sentence pairs for the most precise French-English runs and estimated the number of parallel sentence pairs not yet in the provided gold standard. Adding them to the gold standard leads to revised estimates for the French-English F-scores of at most +1.5pt. This suggests that the BUCC 2017 datasets provide a reasonable approximate evaluation of the parallel sentence spotting task."
W17-2312,Representation of complex terms in a vector space structured by an ontology for a normalization task,2017,0,3,2,1,17095,arnaud ferre,{B}io{NLP} 2017,0,"We propose in this paper a semi-supervised method for labeling terms of texts with concepts of a domain ontology. The method generates continuous vector representations of complex terms in a semantic space structured by the ontology. The proposed method relies on a distributional semantics approach, which generates initial vectors for each of the extracted terms. Then these vectors are embedded in the vector space constructed from the structure of the ontology. This embedding is carried out by training a linear model. Finally, we apply a distance calculation to determine the proximity between vectors of terms and vectors of concepts and thus to assign ontology labels to terms. We have evaluated the quality of these representations for a normalization task by using the concepts of an ontology as semantic labels. Normalization of terms is an important step to extract a part of the information containing in texts, but the vector space generated might find other applications. The performance of this method is comparable to that of the state of the art for this task of standardization, opening up encouraging prospects."
W17-2343,Automatic classification of doctor-patient questions for a virtual patient record query task,2017,20,3,3,1,31973,leonardo llanos,{B}io{NLP} 2017,0,"We present the work-in-progress of automating the classification of doctor-patient questions in the context of a simulated consultation with a virtual patient. We classify questions according to the computational strategy (rule-based or other) needed for looking up data in the clinical record. We compare {`}traditional{'} machine learning methods (Gaussian and Multinomial Naive Bayes, and Support Vector Machines) and a neural network classifier (FastText). We obtained the best results with the SVM using semantic annotations, whereas the neural classifier achieved promising results without it."
2017.jeptalnrecital-demo.11,Traitement automatique de la langue biom{\\'e}dicale au {LIMSI} (Biomedical language processing at {LIMSI}),2017,-1,-1,5,1,23937,christopher norman,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 3 - D{\\'e}monstrations,0,"Nous proposons des d{\'e}monstrations de trois outils d{\'e}velopp{\'e}s par le LIMSI en traitement automatique des langues appliqu{\'e} au domaine biom{\'e}dical : la d{\'e}tection de concepts m{\'e}dicaux dans des textes courts, la cat{\'e}gorisation d{'}articles scientifiques pour l{'}assistance {\`a} l{'}{\'e}criture de revues syst{\'e}matiques, et l{'}anonymisation de textes cliniques."
2017.jeptalnrecital-court.28,D{\\'e}tection de concepts et granularit{\\'e} de l{'}annotation (Concept detection and annotation granularity ),2017,-1,-1,1,1,8591,pierre zweigenbaum,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Nous nous int{\'e}ressons ici {\`a} une t{\^a}che de d{\'e}tection de concepts dans des textes sans exigence particuli{\`e}re de passage par une phase de d{\'e}tection d{'}entit{\'e}s avec leurs fronti{\`e}res. Il s{'}agit donc d{'}une t{\^a}che de cat{\'e}gorisation de textes multi{\'e}tiquette, avec des jeux de donn{\'e}es annot{\'e}s au niveau des textes entiers. Nous faisons l{'}hypoth{\`e}se qu{'}une annotation {\`a} un niveau de granularit{\'e} plus fin, typiquement au niveau de l{'}{\'e}nonc{\'e}, devrait am{\'e}liorer la performance d{'}un d{\'e}tecteur automatique entra{\^\i}n{\'e} sur ces donn{\'e}es. Nous examinons cette hypoth{\`e}se dans le cas de textes courts particuliers : des certificats de d{\'e}c{\`e}s o{\`u} l{'}on cherche {\`a} reconna{\^\i}tre des diagnostics, avec des jeux de donn{\'e}es initialement annot{\'e}s au niveau du certificat entier. Nous constatons qu{'}une annotation au niveau de la Â« ligne Â» am{\'e}liore effectivement les r{\'e}sultats, mais aussi que le simple fait d{'}appliquer au niveau de la ligne un classifieur entra{\^\i}n{\'e} au niveau du texte est d{\'e}j{\`a} une source d{'}am{\'e}lioration."
2017.jeptalnrecital-court.29,Tri Automatique de la Litt{\\'e}rature pour les Revues Syst{\\'e}matiques (Automatically Ranking the Literature in Support of Systematic Reviews),2017,-1,-1,3,1,23937,christopher norman,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Les revues syst{\'e}matiques de la litt{\'e}rature dans le domaine biom{\'e}dical reposent essentiellement sur le travail bibliographique manuel d{'}experts. Nous {\'e}valuons les performances de la classification supervis{\'e}e pour la d{\'e}couverte automatique d{'}articles {\`a} l{'}aide de plusieurs d{\'e}finitions des crit{\`e}res d{'}inclusion. Nous appliquons un mod{\`e}le de regression logistique sur deux corpus issus de revues syst{\'e}matiques conduites dans le domaine du traitement automatique de la langue et de l{'}efficacit{\'e} des m{\'e}dicaments. La classification offre une aire sous la courbe moyenne (AUC) de 0.769 si le classifieur est contruit {\`a} partir des jugements experts port{\'e}s sur les titres et r{\'e}sum{\'e}s des articles, et de 0.835 si on utilise les jugements port{\'e}s sur le texte int{\'e}gral. Ces r{\'e}sultats indiquent l{'}importance des jugements port{\'e}s d{\`e}s le d{\'e}but du processus de s{\'e}lection pour d{\'e}velopper un classifieur efficace pour acc{\'e}l{\'e}rer l{'}{\'e}laboration des revues syst{\'e}matiques {\`a} l{'}aide d{'}un algorithme de classification standard."
W16-6113,Hybrid methods for {ICD}-10 coding of death certificates,2016,3,7,1,1,8591,pierre zweigenbaum,Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis,0,None
W16-5107,A Dataset for {ICD}-10 Coding of Death Certificates: Creation and Usage,2016,0,4,6,0.331928,8590,thomas lavergne,Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016),0,"Very few datasets have been released for the evaluation of diagnosis coding with the International Classification of Diseases, and only one so far in a language other than English. This paper describes a large-scale dataset prepared from French death certificates, and the problems which needed to be solved to turn it into a dataset suitable for the application of machine learning and natural language processing methods of ICD-10 coding. The dataset includes the free-text statements written by medical doctors, the associated meta-data, the human coder-assigned codes for each statement, as well as the statement segments which supported the coder{'}s decision for each code. The dataset comprises 93,694 death certificates totalling 276,103 statements and 377,677 ICD-10 code assignments (3,457 unique codes). It was made available for an international automated coding shared task, which attracted five participating teams. An extended version of the dataset will be used in a new edition of the shared task."
W16-5109,Supervised classification of end-of-lines in clinical text with no manual annotation,2016,0,0,1,1,8591,pierre zweigenbaum,Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016),0,"In some plain text documents, end-of-line marks may or may not mark the boundary of a text unit (e.g., of a paragraph). This vexing problem is likely to impact subsequent natural language processing components, but is seldom addressed in the literature. We propose a method which uses no manual annotation to classify whether end-of-lines must actually be seen as simple spaces (soft line breaks) or as true text unit boundaries. This method, which includes self-training and co-training steps based on token and line length features, achieves 0.943 F-measure on a corpus of short e-books with controlled format, F=0.904 on a random sample of 24 clinical texts with soft line breaks, and F=0.898 on a larger set of mixed clinical texts which may or may not contain soft line breaks, a fairly high value for a method with no manual annotation."
W16-5112,Detection of Text Reuse in {F}rench Medical Corpora,2016,12,0,5,0,32928,eva dhondt,Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016),0,"Electronic Health Records (EHRs) are increasingly available in modern health care institutions either through the direct creation of electronic documents in hospitals{'} health information systems, or through the digitization of historical paper records. Each EHR creation method yields the need for sophisticated text reuse detection tools in order to prepare the EHR collections for efficient secondary use relying on Natural Language Processing methods. Herein, we address the detection of two types of text reuse in French EHRs: 1) the detection of updated versions of the same document and 2) the detection of document duplicates that still bear surface differences due to OCR or de-identification processing. We present a robust text reuse detection method to automatically identify redundant document pairs in two French EHR corpora that achieves an overall macro F-measure of 0.68 and 0.60, respectively and correctly identifies all redundant document pairs of interest."
W16-3001,Overview of the Regulatory Network of Plant Seed Development ({S}ee{D}ev) Task at the {B}io{NLP} Shared Task 2016.,2016,18,17,8,0,26523,estelle chaix,Proceedings of the 4th {B}io{NLP} Shared Task Workshop,0,"This paper presents the SeeDev Task of the BioNLP Shared Task 2016. The purpose of the SeeDev Task is the extraction from scientific articles of the descriptions of genetic and molecular mechanisms involved in seed development of the model plant, Arabidopsis thaliana. The SeeDev task consists in the extraction of many different event types that involve a wide range of entity types so that they accurately reflect the complexity of the biological mechanisms. The corpus is composed of paragraphs selected from the full-texts of relevant scientific articles. In this paper, we describe the organization of the SeeDev task, the corpus characteristics, and the metrics used for the evaluation of participant systems. We analyze and discuss the final results of the seven participant systems to the test. The best F-score is 0.432, which is similar to the scores achieved in similar tasks on molecular biology."
L16-1320,Identification of Drug-Related Medical Conditions in Social Media,2016,0,4,3,1,35053,franccois morlanehondere,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Monitoring social media has been shown to be an interesting approach for the early detection of drug adverse effects. In this paper, we describe a system which extracts medical entities in French drug reviews written by users. We focus on the identification of medical conditions, which is based on the concept of post-coordination: we first extract minimal medical-related entities (pain, stomach) then we combine them to identify complex ones (It was the worst [pain I ever felt in my stomach]). These two steps are respectively performed by two classifiers, the first being based on Conditional Random Fields and the second one on Support Vector Machines. The overall results of the minimal entity classifier are the following: P=0.926; R=0.849; F1=0.886. A thourough analysis of the feature set shows that, when combined with word lemmas, clusters generated by word2vec are the most valuable features. When trained on the output of the first classifier, the second classifier{'}s performances are the following: p=0.683;r=0.956;f1=0.797. The addition of post-processing rules did not add any significant global improvement but was found to modify the precision/recall ratio."
L16-1366,Transfer-Based Learning-to-Rank Assessment of Medical Term Technicality,2016,0,3,5,1,35102,dhouha bouamor,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"While measuring the readability of texts has been a long-standing research topic, assessing the technicality of terms has only been addressed more recently and mostly for the English language. In this paper, we train a learning-to-rank model to determine a specialization degree for each term found in a given list. Since no training data for this task exist for French, we train our system with non-lexical features on English data, namely, the Consumer Health Vocabulary, then apply it to French. The features include the likelihood ratio of the term based on specialized and lay language models, and tests for containing morphologically complex words. The evaluation of this approach is conducted on 134 terms from the UMLS Metathesaurus and 868 terms from the Eugloss thesaurus. The Normalized Discounted Cumulative Gain obtained by our system is over 0.8 on both test sets. Besides, thanks to the learning-to-rank approach, adding morphological features to the language model features improves the results on the Eugloss thesaurus."
L16-1505,Managing Linguistic and Terminological Variation in a Medical Dialogue System,2016,10,3,3,1,31973,leonardo llanos,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We introduce a dialogue task between a virtual patient and a doctor where the dialogue system, playing the patient part in a simulated consultation, must reconcile a specialized level, to understand what the doctor says, and a lay level, to output realistic patient-language utterances. This increases the challenges in the analysis and generation phases of the dialogue. This paper proposes methods to manage linguistic and terminological variation in that situation and illustrates how they help produce realistic dialogues. Our system makes use of lexical resources for processing synonyms, inflectional and derivational variants, or pronoun/verb agreement. In addition, specialized knowledge is used for processing medical roots and affixes, ontological relations and concept mapping, and for generating lay variants of terms according to the patient{'}s non-expert discourse. We also report the results of a first evaluation carried out by 11 users interacting with the system. We evaluated the non-contextual analysis module, which supports the Spoken Language Understanding step. The annotation of task domain entities obtained 91.8{\%} of Precision, 82.5{\%} of Recall, 86.9{\%} of F-measure, 19.0{\%} of Slot Error Rate, and 32.9{\%} of Sentence Error Rate."
2016.jeptalnrecital-poster.7,Une cat{\\'e}gorisation de fins de lignes non-supervis{\\'e}e (End-of-line classification with no supervision),2016,-1,-1,1,1,8591,pierre zweigenbaum,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"Dans certains textes bruts, les marques de fin de ligne peuvent marquer ou pas la fronti{\`e}re d{'}une unit{\'e} textuelle (typiquement un paragraphe). Ce probl{\`e}me risque d{'}influencer les traitements subs{\'e}quents, mais est rarement trait{\'e} dans la litt{\'e}rature. Nous proposons une m{\'e}thode enti{\`e}rement non-supervis{\'e}e pour d{\'e}terminer si une fin de ligne doit {\^e}tre vue comme un simple espace ou comme une v{\'e}ritable fronti{\`e}re d{'}unit{\'e} textuelle, et la testons sur un corpus de comptes rendus m{\'e}dicaux. Cette m{\'e}thode obtient une F-mesure de 0,926 sur un {\'e}chantillon de 24 textes contenant des lignes repli{\'e}es. Appliqu{\'e}e sur un {\'e}chantillon plus grand de textes contenant ou pas des lignes repli{\'e}es, notre m{\'e}thode la plus prudente obtient une F-mesure de 0,898, valeur {\'e}lev{\'e}e pour une m{\'e}thode enti{\`e}rement non-supervis{\'e}e."
2016.jeptalnrecital-poster.20,Impact de l{'}agglutination dans l{'}extraction de termes en arabe standard moderne (Adaptation of a term extractor to the {M}odern {S}tandard {A}rabic language),2016,-1,-1,3,1,30997,wafa neifar,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"Nous pr{\'e}sentons, dans cet article, une adaptation {\`a} l{'}arabe standard moderne d{'}un extracteur de termes pour le fran{\c{c}}ais et l{'}anglais. L{'}adaptation a d{'}abord consist{\'e} {\`a} d{\'e}crire le processus d{'}extraction des termes de mani{\`e}re similaire {\`a} celui d{\'e}fini pour l{'}anglais et le fran{\c{c}}ais en prenant en compte certains particularit{\'e}s morpho-syntaxiques de la langue arabe. Puis, nous avons consid{\'e}r{\'e} le ph{\'e}nom{\`e}ne de l{'}agglutination de la langue arabe. L{'}{\'e}valuation a {\'e}t{\'e} r{\'e}alis{\'e}e sur un corpus de textes m{\'e}dicaux. Les r{\'e}sultats montrent que parmi 400 termes candidats maximaux analys{\'e}s, 288 sont jug{\'e}s corrects par rapport au domaine (72,1{\%}). Les erreurs d{'}extraction sont dues {\`a} l{'}{\'e}tiquetage morpho-syntaxique et {\`a} la non-voyellation des textes mais aussi {\`a} des ph{\'e}nom{\`e}nes d{'}agglutination."
W15-4660,Description of the {P}atient{G}enesys Dialogue System,2015,9,2,5,1,31973,leonardo llanos,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"This paper describes the work-in-progress prototype of a dialog system that simulates a virtual patient (VP) consultation. We report some challenges and difficulties that are found during its development, especially in managing the interaction and the vocabulary from the medical domain."
W15-3411,{BUCC} Shared Task: Cross-Language Document Similarity,2015,3,5,2,0,519,serge sharoff,Proceedings of the Eighth Workshop on Building and Using Comparable Corpora,0,"We summarise the organisation and results of the first shared task aimed at detecting the most similar texts in a large multilingual collection. The dataset of the shared was based on Wikipedia dumps with interlanguage links with further filtering to ensure comparability of the paired articles. The eleven system runs we received have been evaluated using the TREC evaluation metrics. 1 Task description Parallel corpora of original texts with their translations provide the basis for multilingual NLP applications since the beginning of the 1990s. Relative scarcity of such resources led to greater attention to comparable (=less parallel) resources to mine information about possible translations. Many studies have been produced within the paradigm of comparable corpora, including publications in"
2015.jeptalnrecital-long.3,Identification de facteurs de risque pour des patients diab{\\'e}tiques {\\`a} partir de comptes-rendus cliniques par des approches hybrides,2015,-1,-1,4,0.821627,5675,cyril grouin,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous pr{\'e}sentons les m{\'e}thodes que nous avons d{\'e}velopp{\'e}es pour analyser des comptes- rendus hospitaliers r{\'e}dig{\'e}s en anglais. L{'}objectif de cette {\'e}tude consiste {\`a} identifier les facteurs de risque de d{\'e}c{\`e}s pour des patients diab{\'e}tiques et {\`a} positionner les {\'e}v{\'e}nements m{\'e}dicaux d{\'e}crits par rapport {\`a} la date de cr{\'e}ation de chaque document. Notre approche repose sur (i) HeidelTime pour identifier les expressions temporelles, (ii) des CRF compl{\'e}t{\'e}s par des r{\`e}gles de post-traitement pour identifier les traitements, les maladies et facteurs de risque, et (iii) des r{\`e}gles pour positionner temporellement chaque {\'e}v{\'e}nement m{\'e}dical. Sur un corpus de 514 documents, nous obtenons une F-mesure globale de 0,8451. Nous observons que l{'}identification des informations directement mentionn{\'e}es dans les documents se r{\'e}v{\`e}le plus performante que l{'}inf{\'e}rence d{'}informations {\`a} partir de r{\'e}sultats de laboratoire."
2015.jeptalnrecital-demonstration.8,Un patient virtuel dialogant,2015,-1,-1,5,0,37967,leonardo campillos,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,Le d{\'e}monstrateur que nous d{\'e}crivons ici est un prototype de syst{\`e}me de dialogue dont l{'}objectif est de simuler un patient. Nous d{\'e}crivons son fonctionnement g{\'e}n{\'e}ral en insistant sur les aspects concernant la langue et surtout le rapport entre langue m{\'e}dicale de sp{\'e}cialit{\'e} et langue g{\'e}n{\'e}rale.
2015.jeptalnrecital-court.4,{\\'E}tude des verbes introducteurs de noms de m{\\'e}dicaments dans les forums de sant{\\'e},2015,-1,-1,3,1,35053,franccois morlanehondere,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Dans cet article, nous combinons annotations manuelle et automatique pour identifier les verbes utilis{\'e}s pour introduire un m{\'e}dicament dans les messages sur les forums de sant{\'e}. Cette information est notamment utile pour identifier la relation entre un m{\'e}dicament et un effet secondaire. La mention d{'}un m{\'e}dicament dans un message ne garantit pas que l{'}utilisateur a pris ce traitement mais qu{'}il effectue un retour. Nous montrons ensuite que ces verbes peuvent servir pour extraire automatiquement des variantes de noms de m{\'e}dicaments. Nous estimons que l{'}analyse de ces variantes pourrait permettre de mod{\'e}liser les erreurs faites par les usagers des forums lorsqu{'}ils {\'e}crivent les noms de m{\'e}dicaments, et am{\'e}liorer en cons{\'e}quence les syst{\`e}mes de recherche d{'}information."
2015.jeptalnrecital-court.40,"M{\\'e}dicaments qui soignent, m{\\'e}dicaments qui rendent malades : {\\'e}tude des relations causales pour identifier les effets secondaires",2015,-1,-1,4,1,35053,franccois morlanehondere,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Dans cet article, nous nous int{\'e}ressons {\`a} la mani{\`e}re dont sont exprim{\'e}s les liens qui existent entre un traitement m{\'e}dical et un effet secondaire. Parce que les patients se tournent en priorit{\'e} vers internet, nous fondons cette {\'e}tude sur un corpus annot{\'e} de messages issus de forums de sant{\'e} en fran{\c{c}}ais. L{'}objectif de ce travail consiste {\`a} mettre en {\'e}vidence des {\'e}l{\'e}ments linguistiques (connecteurs logiques et expressions temporelles) qui pourraient {\^e}tre utiles pour des syst{\`e}mes automatiques de rep{\'e}rage des effets secondaires. Nous observons que les modalit{\'e}s d{'}{\'e}criture sur les forums ne permettent pas de se fonder sur les expressions temporelles. En revanche, les connecteurs logiques semblent utiles pour identifier les effets secondaires."
W14-6301,Automatic Analysis of Scientific and Literary Texts. Presentation and Results of the {DEFT}2014 Text Mining Challenge (Analyse automatique de textes litt{\\'e}raires et scientifiques : pr{\\'e}sentation et r{\\'e}sultats du d{\\'e}fi fouille de texte {DEFT}2014) [in {F}rench],2014,-1,-1,4,0,18582,thierry hamon,TALN-RECITAL 2014 Workshop DEFT 2014 : D{\\'E}fi Fouille de Textes (DEFT 2014 Workshop: Text Mining Challenge),0,None
chatzimina-etal-2014-use,Use of unsupervised word classes for entity recognition: Application to the detection of disorders in clinical reports,2014,25,0,3,0,39606,maria chatzimina,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Unsupervised word classes induced from unannotated text corpora are increasingly used to help tasks addressed by supervised classification, such as standard named entity detection. This paper studies the contribution of unsupervised word classes to a medical entity detection task with two specific objectives: How do unsupervised word classes compare to available knowledge-based semantic classes? Does syntactic information help produce unsupervised word classes with better properties? We design and test two syntax-based methods to produce word classes: one applies the Brown clustering algorithm to syntactic dependencies, the other collects latent categories created by a PCFG-LA parser. When added to non-semantic features, knowledge-based semantic classes gain 7.28 points of F-measure. In the same context, basic unsupervised word classes gain 4.16pt, reaching 60{\%} of the contribution of knowledge-based semantic classes and outperforming Wikipedia, and adding PCFG-LA unsupervised word classes gain one more point at 5.11pt, reaching 70{\%}. Unsupervised word classes could therefore provide a useful semantic back-off in domains where no knowledge-based semantic classes are available. The combination of both knowledge-based and basic unsupervised classes gains 8.33pt. Therefore, unsupervised classes are still useful even when rich knowledge-based classes exist."
deleger-etal-2014-annotation,Annotation of specialized corpora using a comprehensive entity and relation scheme,2014,15,8,4,1,17098,louise deleger,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Annotated corpora are essential resources for many applications in Natural Language Processing. They provide insight on the linguistic and semantic characteristics of the genre and domain covered, and can be used for the training and evaluation of automatic tools. In the biomedical domain, annotated corpora of English texts have become available for several genres and subfields. However, very few similar resources are available for languages other than English. In this paper we present an effort to produce a high-quality corpus of clinical documents in French, annotated with a comprehensive scheme of entities and relations. We present the annotation scheme as well as the results of a pilot annotation study covering 35 clinical documents in a variety of subfields and genres. We show that high inter-annotator agreement can be achieved using a complex annotation scheme."
neveol-etal-2014-language,Language Resources for {F}rench in the Biomedical Domain,2014,18,15,4,0.310351,863,aurelie neveol,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The biomedical domain offers a wealth of linguistic resources for Natural Language Processing, including terminologies and corpora. While many of these resources are prominently available for English, other languages including French benefit from substantial coverage thanks to the contribution of an active community over the past decades. However, access to terminological resources in languages other than English may not be as straight-forward as access to their English counterparts. Herein, we review the extent of resource coverage for French and give pointers to access French-language resources. We also discuss the sources and methods for making additional material available for French."
W13-2503,Using {W}ord{N}et and Semantic Similarity for Bilingual Terminology Mining from Comparable Corpora,2013,23,8,3,1,35102,dhouha bouamor,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"This paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora. We study of the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors. For this purpose, we augment the standard approach by a Word Sense Disambiguation process relying on a WordNet-based semantic similarity measure. The aim of this process is to identify the translations that are more likely to give the best representation of words in the target language. On two specialized French-English comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach."
W13-2321,Automatic Named Entity Pre-annotation for Out-of-domain Human Annotation,2013,23,3,7,0,5280,sophie rosset,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"Automatic pre-annotation is often used to improve human annotation speed and accuracy. We address here out-of-domain named entity annotation, and examine whether automatic pre-annotation is still beneficial in this setting. Our study design includes two different corpora, three pre-annotation schemes linked to two annotation levels, both expert and novice annotators, a questionnaire-based subjective assessment and a corpus-based quantitative assessment. We observe that preannotation helps in all cases, both for speed and for accuracy, and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome."
W13-2001,Overview of {B}io{NLP} Shared Task 2013,2013,17,104,7,0,17099,claire nedellec,Proceedings of the {B}io{NLP} Shared Task 2013 Workshop,0,"The BioNLP Shared Task 2013 is the third edition of the BioNLP Shared Task series that is a community-wide effort to address fine-grained, structural information extraction from biomedical literature. The BioNLP Shared Task 2013 was held from January to April 2013. Six main tasks were proposed. 38 final submissions were received, from 22 teams. The results show advances in the state of the art and demonstrate that extraction methods can be successfully generalized in various aspects."
P13-2133,Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora,2013,19,16,3,1,35102,dhouha bouamor,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches."
I13-1125,Building Specialized Bilingual Lexicons Using Word Sense Disambiguation,2013,17,4,3,1,35102,dhouha bouamor,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora. We study the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors and augment the standard approach by a Word Sense Disambiguation process. Our aim is to identify the translations of words that are more likely to give the best representation of words in the target language. On two specialized French-English and RomanianEnglish comparable corpora, empirical experimental results show that the proposed method consistently outperforms the standard approach."
F13-2018,Extraction of temporal relations between clinical events in clinical documents (Extraction des relations temporelles entre {\\'e}v{\\'e}nements m{\\'e}dicaux dans des comptes rendus hospitaliers) [in {F}rench],2013,-1,-1,1,1,8591,pierre zweigenbaum,Proceedings of TALN 2013 (Volume 2: Short Papers),0,None
F13-1024,(Utilisation de la similarit{\\'e} s{\\'e}mantique pour l{'}extraction de lexiques bilingues {\\`a} partir de corpus comparables) [in {F}rench],2013,0,2,3,1,35102,dhouha bouamor,Proceedings of TALN 2013 (Volume 1: Long Papers),0,"This paper presents a new method that aims to improve the results of the standard approach used for bilingual lexicon extraction from specialized comparable corpora. We attempt to solve the problem of context vector word polysemy. Instead of using all the entries of the dictionary to translate a context vector, we only use the words of the lexicon that are more likely to give the best characterization of context vectors in the target language. On two specialised French-English comparable corpora, empirical experimental results show that our method improves the results obtained by the standard approach especially when many words are ambiguous. MOTS-CLES : lexique bilingue, corpus comparable specialise, desambiguisation semantique, WordNet."
D13-1046,Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge,2013,18,8,4,1,35102,dhouha bouamor,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires strong expertise in both languages involved and is a costly process. Several automatic methods were proposed as an alternative but they often rely on resources available in a limited number of languages and their performances are still far behind the quality of manual translations. We introduce a novel approach to the creation of specific domain bilingual lexicon that relies on Wikipedia. This massively multilingual encyclopedia makes it possible to create lexicons for a large number of language pairs. Wikipedia is used to extract domains in each language, to link domains between languages and to create generic translation dictionaries. The approach is tested on four specialized domains and is compared to three state of the art approaches using two language pairs: FrenchEnglish and Romanian-English. The newly introduced method compares favorably to existing methods in all configurations tested."
2013.mtsummit-papers.18,Towards a Generic Approach for Bilingual Lexicon Extraction from Comparable Corpora,2013,-1,-1,3,1,35102,dhouha bouamor,Proceedings of Machine Translation Summit XIV: Papers,0,None
W12-5108,Automatic Construction of a {M}ulti{W}ord Expressions Bilingual Lexicon: A Statistical Machine Translation Evaluation Perspective,2012,26,11,3,1,35102,dhouha bouamor,Proceedings of the 3rd Workshop on Cognitive Aspects of the Lexicon,0,"Identifying and translating MultiWord Expressions (MWES) in a text represent a key issue for numerous applications of Natural Language Processing (NLP), especially for Machine Translation (MT). In this paper, we present a method aiming to construct a bilingual lexicon of MWES from a French-English parallel corpus. In order to assess the quality of the mined lexicon, a Statistical Machine Translation (SMT) task-based evaluation is conducted. We investigate the performance of three dynamic strategies and of one static strategy to integrate the mined bilingual MWES lexicon in a SMT system. Experimental results shows that such a lexicon improves the quality of translation."
W12-3606,Structured Named Entities in two distinct press corpora: Contemporary Broadcast News and Old Newspapers,2012,18,16,6,0.21733,5280,sophie rosset,Proceedings of the Sixth Linguistic Annotation Workshop,0,"This paper compares the reference annotation of structured named entities in two corpora with different origins and properties. It addresses two questions linked to such a comparison. On the one hand, what specific issues were raised by reusing the same annotation scheme on a corpus that differs from the first in terms of media and that predates it by more than a century? On the other hand, what contrasts were observed in the resulting annotations across the two corpora?"
W12-1101,Indexation libre et contr{\\^o}l{\\'e}e d{'}articles scientifiques. Pr{\\'e}sentation et r{\\'e}sultats du d{\\'e}fi fouille de textes {DEFT}2012 (Controlled and free indexing of scientific papers. Presentation and results of the {DEFT}2012 text-mining challenge) [in {F}rench],2012,-1,-1,2,0,5615,patrick paroubek,"JEP-TALN-RECITAL 2012, Workshop DEFT 2012: D{\\'E}fi Fouille de Textes (DEFT 2012 Workshop: Text Mining Challenge)",0,None
galibert-etal-2012-extended,Extended Named Entities Annotation on {OCR}ed Documents: From Corpus Constitution to Evaluation Campaign,2012,16,6,4,1,13778,olivier galibert,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Within the framework of the Quaero project, we proposed a new definition of named entities, based upon an extension of the coverage of named entities as well as the structure of those named entities. In this new definition, the extended named entities we proposed are both hierarchical and compositional. In this paper, we focused on the annotation of a corpus composed of press archives, OCRed from French newspapers of December 1890. We present the methodology we used to produce the corpus and the characteristics of the corpus in terms of named entities annotation. This annotated corpus has been used in an evaluation campaign. We present this evaluation, the metrics we used and the results obtained by the participants."
bouamor-etal-2012-identifying,Identifying bilingual Multi-Word Expressions for Statistical Machine Translation,2012,23,37,3,1,35102,dhouha bouamor,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"MultiWord Expressions (MWEs) repesent a key issue for numerous applications in Natural Language Processing (NLP) especially for Machine Translation (MT). In this paper, we describe a strategy for detecting translation pairs of MWEs in a French-English parallel corpus. In addition we introduce three methods aiming to integrate extracted bilingual MWE S in M OSES, a phrase based Statistical Machine Translation (SMT) system. We experimentally show that these textual units can improve translation quality."
F12-2002,Extraction d{'}information automatique en domaine m{\\'e}dical par projection inter-langue : vers un passage {\\`a} l{'}{\\'e}chelle (Automatic Information Extraction in the Medical Domain by Cross-Lingual Projection) [in {F}rench],2012,0,0,2,1,7146,asma abacha,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
C12-2079,Manual Corpus Annotation: Giving Meaning to the Evaluation Metrics,2012,15,13,9,0,32781,yann mathet,Proceedings of {COLING} 2012: Posters,0,"Computing inter-annotator agreement measures on a manually annotated corpus is necessary to evaluate the reliability of its annotation. However, the interpretation of the obtained results is recognized as highly arbitrary. We describe in this article a method and a tool that we developed which shuffles a reference annotation according to different error paradigms, thereby creating artificial annotations with controlled errors. Agreement measures are computed on these corpora, and the obtained results are used to model the behavior of these measures and understand their actual meaning."
W11-0411,"Proposal for an Extension of Traditional Named Entities: From Guidelines to Evaluation, an Overview",2011,27,36,3,1,5675,cyril grouin,Proceedings of the 5th Linguistic Annotation Workshop,0,"Within the framework of the construction of a fact database, we defined guidelines to extract named entities, using a taxonomy based on an extension of the usual named entities definition. We thus defined new types of entities with broader coverage including substantive-based expressions. These extended named entities are hierarchical (with types and components) and compositional (with recursive type inclusion and metonymy annotation). Human annotators used these guidelines to annotate a 1.3M word broadcast news corpus in French. This article presents the definition and novelty of extended named entity annotation guidelines, the human annotation of a global corpus and of a mini reference corpus, and the evaluation of annotations through the computation of inter-annotator agreements. Finally, we discuss our approach and the computed results, and outline further work."
W11-0207,Medical Entity Recognition: A Comparaison of Semantic and Statistical Methods,2011,21,65,2,1,7146,asma abacha,Proceedings of {B}io{NLP} 2011 Workshop,0,Medical Entity Recognition is a crucial step towards efficient medical texts analysis. In this paper we present and compare three methods based on domain-knowledge and machine-learning techniques. We study two research directions through these approaches: (i) a first direction where noun phrases are extracted in a first step with a chunker before the final classification step and (ii) a second direction where machine learning techniques are used to identify simultaneously entities boundaries and categories. Each of the presented approaches is tested on a standard corpus of clinical texts. The obtained results show that the hybrid approach based on both machine learning and domain knowledge obtains the best performance.
I11-1058,Structured and Extended Named Entity Evaluation in Automatic Speech Transcriptions,2011,25,23,4,1,13778,olivier galibert,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"The evaluation of named entity recognition (NER) methods is an active field of research. This includes the recognition of named entities in speech transcripts. Evaluating NER systems on automatic speech recognition (ASR) output whereas human reference annotation was prepared on clean manual transcripts raises difficult alignment issues. These issues are emphasized when named entities are structured, as is the case in the Quaero NER challenge organized in 2010. This paper describes the structured named entity definition used in this challenge and presents a method to transfer reference annotations to ASR output. This method was used in the Quaero 2010 evaluation of extended named entity annotation on speech transcripts, whose results are given in the paper."
2011.jeptalnrecital-long.6,Acc{\\`e}s au contenu s{\\'e}mantique en langue de sp{\\'e}cialit{\\'e} : extraction des prescriptions et concepts m{\\'e}dicaux (Accessing the semantic content in a specialized language: extracting prescriptions and medical concepts),2011,-1,-1,5,1,5675,cyril grouin,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Pourtant essentiel pour appr{\'e}hender rapidement et globalement l{'}{\'e}tat de sant{\'e} des patients, l{'}acc{\`e}s aux informations m{\'e}dicales li{\'e}es aux prescriptions m{\'e}dicamenteuses et aux concepts m{\'e}dicaux par les outils informatiques se r{\'e}v{\`e}le particuli{\`e}rement difficile. Ces informations sont en effet g{\'e}n{\'e}ralement r{\'e}dig{\'e}es en texte libre dans les comptes rendus hospitaliers et n{\'e}cessitent le d{\'e}veloppement de techniques d{\'e}di{\'e}es. Cet article pr{\'e}sente les strat{\'e}gies mises en oeuvre pour extraire les prescriptions m{\'e}dicales et les concepts m{\'e}dicaux dans des comptes rendus hospitaliers r{\'e}dig{\'e}s en anglais. Nos syst{\`e}mes, fond{\'e}s sur des approches {\`a} base de r{\`e}gles et d{'}apprentissage automatique, obtiennent une F1-mesure globale de 0,773 dans l{'}extraction des prescriptions m{\'e}dicales et dans le rep{\'e}rage et le typage des concepts m{\'e}dicaux."
2011.jeptalnrecital-demonstration.12,Extraction d{'}informations m{\\'e}dicales au {LIMSI} (Medical information extraction at {LIMSI}),2011,-1,-1,10,1,5675,cyril grouin,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,
galibert-etal-2010-named,Named and Specific Entity Detection in Varied Data: The Qu{\\ae}ro Named Entity Baseline Evaluation,2010,9,14,4,1,13778,olivier galibert,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The Qu{\ae}ro program that promotes research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within its context a set of evaluations of Named Entity recognition systems was held in 2009. Four tasks were defined. The first two concerned traditional named entities in French broadcast news for one (a rerun of ESTER 2) and of OCR-ed old newspapers for the other. The third was a gene and protein name extraction in medical abstracts. The last one was the detection of references in patents. Four different partners participated, giving a total of 16 systems. We provide a synthetic descriptions of all of them classifying them by the main approaches chosen (resource-based, rules-based or statistical), without forgetting the fact that any modern system is at some point hybrid. The metric (the relatively standard Slot Error Rate) and the results are also presented and discussed. Finally, a process is ongoing with preliminary acceptance of the partners to ensure the availability for the community of all the corpora used with the exception of the non-Qu{\ae}ro produced ESTER 2 one."
cartoni-zweigenbaum-2010-semi,Semi-Automated Extension of a Specialized Medical Lexicon for {F}rench,2010,7,2,2,0.313652,17415,bruno cartoni,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper describes the development of a specialized lexical resource for a specialized domain, namely medicine. First, in order to assess the linguistic phenomena that need to be adressed, we based our observation on a large collection of more than 300'000 terms, organised around conceptual identifiers. Based on these observations, we highlight the specificities that such a lexicon should take into account, namely in terms of inflectional and derivational knowledge. In a first experiment, we show that general resources lack a large part of the words needed to process specialized language. Secondly, we describe an experiment to feed semi-automatically a medical lexicon and populate it with inflectional information. This experiment is based on a semi-automatic methods that tries to acquire inflectional knowledge from frequent endings of words recorded in existing lexicon. Thanks to this, we increased the coverage of the target vocabulary from 14.1{\%} to 25.7{\%}."
deleger-zweigenbaum-2010-identifying,Identifying Paraphrases between Technical and Lay Corpora,2010,12,3,2,1,17098,louise deleger,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In previous work, we presented a preliminary study to identify paraphrases between technical and lay discourse types from medical corpora dedicated to the French language. In this paper, we test the hypothesis that the same kinds of paraphrases as for French can be detected between English technical and lay discourse types and report the adaptation of our method from French to English. Starting from the constitution of monolingual comparable corpora, we extract two kinds of paraphrases: paraphrases between nominalizations and verbal constructions and paraphrases between neo-classical compounds and modern-language phrases. We do this relying on morphological resources and a set of extraction rules we adapt from the original approach for French. Results show that paraphrases could be identified with a rather good precision, and that these types of paraphrase are relevant in the context of the opposition between technical and lay discourse types. These observations are consistent with the results obtained for French, which demonstrates the portability of the approach as well as the similarity of the two languages as regards the use of those kinds of expressions in technical and lay discourse types."
2010.jeptalnrecital-demonstration.12,{M}e{TAE} : Plate-forme d{'}annotation automatique et d{'}exploration s{\\'e}mantiques pour le domaine m{\\'e}dical,2010,-1,-1,2,1,7146,asma abacha,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,"Nous pr{\'e}sentons une plate-forme d{'}annotation s{\'e}mantique et d{'}exploration de textes m{\'e}dicaux, appel{\'e}e Â« MeTAE Â». Le processus d{'}annotation automatique comporte une premi{\`e}re {\'e}tape de reconnaissance des entit{\'e}s m{\'e}dicales pr{\'e}sentes dans les textes suivie d{'}une {\'e}tape d{'}identification des relations s{\'e}mantiques qui les relient. Cette identification se fonde sur des patrons linguistiques construits manuellement pour chaque type de relation. MeTAE g{\'e}n{\`e}re des annotations RDF {\`a} partir des informations extraites et offre une interface d{'}exploration des textes annot{\'e}s avec des requ{\^e}tes sous forme de formulaire. La plate-forme peut {\^e}tre utilis{\'e}e pour analyser s{\'e}mantiquement les textes m{\'e}dicaux ou interroger la base d{'}annotation disponible pour avoir une/des r{\'e}ponses {\`a} une requ{\^e}te donn{\'e}e (e.g. Â« ?X pr{\'e}vient maladie d{'}Alzheimer Â», {\'e}quivalent {\`a} la question Â« comment pr{\'e}venir la maladie d{'}Alzheimer ? Â»). Cette application peut {\^e}tre la base d{'}un syst{\`e}me de questions-r{\'e}ponses pour le domaine m{\'e}dical."
W09-3102,Extracting Lay Paraphrases of Specialized Expressions from Monolingual Comparable Medical Corpora,2009,19,24,2,1,17098,louise deleger,Proceedings of the 2nd Workshop on Building and Using Comparable Corpora: from Parallel to Non-parallel Corpora ({BUCC}),0,"Whereas multilingual comparable corpora have been used to identify translations of words or terms, monolingual corpora can help identify paraphrases. The present work addresses paraphrases found between two different discourse types: specialized and lay texts. We therefore built comparable corpora of specialized and lay texts in order to detect equivalent lay and specialized expressions. We identified two devices used in such paraphrases: nominalizations and neo-classical compounds. The results showed that the paraphrases had a good precision and that nominalizations were indeed relevant in the context of studying the differences between specialized and lay language. Neo-classical compounds were less conclusive. This study also demonstrates that simple paraphrase acquisition methods can also work on texts with a rather small degree of similarity, once similar text segments are detected."
W09-2701,Knowledge and Reasoning for Medical Question-Answering,2009,18,6,1,1,8591,pierre zweigenbaum,Proceedings of the 2009 Workshop on Knowledge and Reasoning for Answering Questions ({KRAQ} 2009),0,"Restricted domains such as medicine set a context where question-answering is more likely expected to be associated with knowledge and reasoning (Molla and Vicedo, 2007; Ferret and Zweigenbaum, 2007). On the one hand, knowledge and reasoning may be more necessary than in open-domain question-answering because of more specific or more difficult questions. On the other hand, it may also be more manageable, since by definition restricted-domain QA should not have to face the same breadth of questions as open-domain QA. It is therefore interesting to study the role of knowledge and reasoning in restricted-domain question-answering systems. We shall do so in the case of the (bio-)medical domain, which has a long tradition of investigating knowledge representation and reasoning and, more generally, artificial intelligence methods (Shortliffe et al., 1975), and which has seen a growing interest in question-answering systems (Zweigenbaum, 2003; Yu et al., 2005; Demner-Fushman and Lin, 2007; Zweigenbaum et al., 2007)."
E09-1056,Improvements in Analogical Learning: Application to Translating Multi-Terms of the Medical Domain,2009,17,19,3,0,7084,philippe langlais,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Handling terminology is an important matter in a translation workflow. However, current Machine Translation (MT) systems do not yet propose anything proactive upon tools which assist in managing terminological databases. In this work, we investigate several enhancements to analogical learning and test our implementation on translating medical terms. We show that the analogical engine works equally well when translating from and into a morphologically rich language, or when dealing with language pairs written in different scripts. Combining it with a phrase-based statistical engine leads to significant improvements."
2007.jeptalnrecital-long.7,Analyse morphos{\\'e}mantique des compos{\\'e}s savants : transposition du fran{\\c{c}}ais {\\`a} l{'}anglais,2007,-1,-1,3,1,17098,louise deleger,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"La plupart des vocabulaires sp{\'e}cialis{\'e}s comprennent une part importante de lex{\`e}mes morphologiquement complexes, construits {\`a} partir de racines grecques et latines, qu{'}on appelle Â« compos{\'e}s savants Â». Une analyse morphos{\'e}mantique permet de d{\'e}composer et de donner des d{\'e}finitions {\`a} ces lex{\`e}mes, et semble pouvoir {\^e}tre appliqu{\'e}e de fa{\c{c}}on similaire aux compos{\'e}s de plusieurs langues. Cet article pr{\'e}sente l{'}adaptation d{'}un analyseur morphos{\'e}mantique, initialement d{\'e}di{\'e} au fran{\c{c}}ais (D{\'e}riF), {\`a} l{'}analyse de compos{\'e}s savants m{\'e}dicaux anglais, illustrant ainsi la similarit{\'e} de structure de ces compos{\'e}s dans des langues europ{\'e}ennes proches. Nous exposons les principes de cette transposition et ses performances. L{'}analyseur a {\'e}t{\'e} test{\'e} sur un ensemble de 1299 lex{\`e}mes extraits de la terminologie m{\'e}dicale WHO-ART : 859 ont pu {\^e}tre d{\'e}compos{\'e}s et d{\'e}finis, dont 675 avec succ{\`e}s. Outre une simple transposition d{'}une langue {\`a} l{'}autre, la m{\'e}thode montre la potentialit{\'e} d{'}un syst{\`e}me multilingue."
2006.jeptalnrecital-long.14,Productivit{\\'e} quantitative des suffixations par -it{\\'e} et -Able dans un corpus journalistique moderne,2006,-1,-1,10,1,5649,natalia grabar,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans ce travail, nous {\'e}tudions en corpus la productivit{\'e} quantitative des suffixations par -Able et par -it{\'e} du fran{\c{c}}ais, d{'}abord ind{\'e}pendamment l{'}une de l{'}autre, puis lorsqu{'}elles s{'}encha{\^\i}nent d{\'e}rivationnellement (la suffixation en -it{\'e} s{'}applique {\`a} des bases en -Able dans environ 15 {\%} des cas). Nous estimons la productivit{\'e} de ces suffixations au moyen de mesures statistiques dont nous suivons l{'}{\'e}volution par rapport {\`a} la taille du corpus. Ces deux suffixations sont productives en fran{\c{c}}ais moderne : elles forment de nouveaux lex{\`e}mes tout au long des corpus {\'e}tudi{\'e}s sans qu{'}on n{'}observe de saturation, leurs indices de productivit{\'e} montrent une {\'e}volution stable bien qu{'}{\'e}tant d{\'e}pendante des calculs qui leur sont appliqu{\'e}s. On note cependant que, de fa{\c{c}}on g{\'e}n{\'e}rale, de ces deux suffixations, c{'}est la suffixation par -it{\'e} qui est la plus fr{\'e}quente en corpus journalistique, sauf pr{\'e}cis{\'e}ment quand -it{\'e} s{'}applique {\`a} un adjectif en -Able. {\'E}tant entendu qu{'}un adjectif en -Able et le nom en -it{\'e} correspondant expriment la m{\^e}me propri{\'e}t{\'e}, ce r{\'e}sultat indique que la complexit{\'e} de la base est un param{\`e}tre {\`a} prendre en consid{\'e}ration dans la formation du lexique possible."
2005.jeptalnrecital-long.5,Recherche en corpus de r{\\'e}ponses {\\`a} des questions d{\\'e}finitoires,2005,-1,-1,3,1,47585,veronique malaise,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les syst{\`e}mes de questions-r{\'e}ponses, essentiellement focalis{\'e}s sur des questions factuelles en domaine ouvert, testent {\'e}galement d{'}autres t{\^a}ches, comme le travail en domaine contraint ou la recherche de d{\'e}finitions. Nous nous int{\'e}ressons ici {\`a} la recherche de r{\'e}ponses {\`a} des questions Â« d{\'e}finitoires Â» portant sur le domaine m{\'e}dical. La recherche de r{\'e}ponses de type d{\'e}finitoire se fait g{\'e}n{\'e}ralement en utilisant deux types de m{\'e}thodes : celles s{'}appuyant essentiellement sur le contenu du corpus cible, et celles faisant appel {\`a} des connaissances externes. Nous avons choisi de nous limiter au premier de ces deux types de m{\'e}thodes. Nous pr{\'e}sentons une exp{\'e}rience dans laquelle nous r{\'e}utilisons des patrons de rep{\'e}rage d{'}{\'e}nonc{\'e}s d{\'e}finitoires, con{\c{c}}us pour une autre t{\^a}che, pour localiser les r{\'e}ponses potentielles aux questions pos{\'e}es. Nous avons int{\'e}gr{\'e} ces patrons dans une cha{\^\i}ne de traitement que nous {\'e}valuons sur les questions d{\'e}finitoires et le corpus m{\'e}dical du projet EQueR sur l{'}{\'e}valuation de syst{\`e}mes de questions-r{\'e}ponses. Cette {\'e}valuation montre que, si le rappel reste {\`a} am{\'e}liorer, la Â« pr{\'e}cision Â» des r{\'e}ponses obtenue (mesur{\'e}e par la moyenne des inverses de rangs) est honorable. Nous discutons ces r{\'e}sultats et proposons des pistes d{'}am{\'e}lioration."
2005.jeptalnrecital-long.9,Utilisation de corpus de sp{\\'e}cialit{\\'e} pour le filtrage de synonymes de la langue g{\\'e}n{\\'e}rale,2005,-1,-1,2,1,5649,natalia grabar,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les ressources linguistiques les plus facilement disponibles en TAL ressortissent g{\'e}n{\'e}ralement au registre g{\'e}n{\'e}ral d{'}une langue. Lorsqu{'}elles doivent {\^e}tre utilis{\'e}es sur des textes de sp{\'e}cialit{\'e} il peut {\^e}tre utile de les adapter {\`a} ces textes. Cet article est consacr{\'e} {\`a} l{'}adaptation de ressources synonymiques g{\'e}n{\'e}rales {\`a} la langue m{\'e}dicale. L{'}adaptation est obtenue suite {\`a} une s{\'e}rie de filtrages sur un corpus du domaine. Les synonymes originaux et les synonymes filtr{\'e}s sont ensuite utilis{\'e}s comme une des ressources pour la normalisation de variantes de termes dans une t{\^a}che de structuration de terminologie. Leurs apports respectifs sont {\'e}valu{\'e}s par rapport {\`a} la structure terminologique de r{\'e}f{\'e}rence. Cette {\'e}valuation montre que les r{\'e}sultats sont globalement encourageants apr{\`e}s les filtrages, pour une t{\^a}che comme la structuration de terminologies : une am{\'e}lioration de la pr{\'e}cision contre une l{\'e}g{\`e}re diminution du rappel."
2005.jeptalnrecital-long.26,Traduction de termes biom{\\'e}dicaux par inf{\\'e}rence de transducteurs,2005,-1,-1,2,0,5590,vincent claveau,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article propose et {\'e}value une m{\'e}thode de traduction automatique de termes biom{\'e}dicaux simples du fran{\c{c}}ais vers l{'}anglais et de l{'}anglais vers le fran{\c{c}}ais. Elle repose sur une technique d{'}apprentissage artificiel supervis{\'e}e permettant d{'}inf{\'e}rer des transducteurs {\`a} partir d{'}exemples de couples de termes bilingues ; aucune autre ressource ou connaissance n{'}est requise. Ces transducteurs, capturant les grandes r{\'e}gularit{\'e}s de traduction existant dans le domaine biom{\'e}dical, sont ensuite utilis{\'e}s pour traduire de nouveaux termes fran{\c{c}}ais en anglais et vice versa. Les {\'e}valuations men{\'e}es montrent que le taux de bonnes traductions de notre technique se situe entre 52 et 67{\%}. {\`A} travers un examen des erreurs les plus courantes, nous identifions quelques limites inh{\'e}rentes {\`a} notre approche et proposons quelques pistes pour les d{\'e}passer. Nous envisageons enfin plusieurs extensions {\`a} ce travail."
W04-1807,Detecting Semantic Relations between Terms in Definitions,2004,14,27,2,1,47585,veronique malaise,Proceedings of {C}ompu{T}erm 2004: 3rd International Workshop on Computational Terminology,0,"Terminology structuring aims to elicit semantic relations between the terms of a domain. We propose here to exploit definitions found in corpora to obtain such semantic relations. Definition typologies show that definitions can be introduced by different semantic relations, some of these relations being likely to structure terminologies. Our aim is therefore to mine xe2x80x9cdefining expressionsxe2x80x9d in domainspecific corpora, and to detect the semantic relations they involve between their main terms. We use lexico-syntactic markers and patterns to detect at the same time both a definition and its main semantic relation. 46 markers and 74 patterns have been designed and tuned on a first corpus in the field of anthropology. We report on their evaluation on a second corpus in the field of dietetics, where they obtained 4% to 36% recall and from 61 to 66% precision, and discuss the relative accuracy of different subclasses of markers for this task."
2004.jeptalnrecital-long.16,Rep{\\'e}rage et exploitation d{'}{\\'e}nonc{\\'e}s d{\\'e}finitoires en corpus pour l{'}aide {\\`a} la construction d{'}ontologie,2004,-1,-1,2,1,47585,veronique malaise,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Pour construire une ontologie, un mod{\'e}liseur a besoin d{'}objecter des informations s{\'e}mantiques sur les termes principaux de son domaine d{'}{\'e}tude. Les outils d{'}exploration de corpus peuvent aider {\`a} rep{\'e}rer ces types d{'}information, et l{'}identification de couples d{'}hyperonymes a fait l{'}objet de plusieurs travaux. Nous proposons d{'}exploiter des {\'e}nonc{\'e}s d{\'e}finitoires pour extraire d{'}un corpus des informations concernant les trois axes de l{'}ossature ontologique : l{'}axe vertical, li{\'e} {\`a} l{'}hyperonymie, l{'}axe horizontal, li{\'e} {\`a} la co-hyponymie et l{'}axe transversal, li{\'e} aux relations du domaine. Apr{\`e}s un rappel des travaux existants en rep{\'e}rage d{'}{\'e}nonc{\'e}s d{\'e}finitoires en TAL, nous d{\'e}veloppons la m{\'e}thode que nous avons mise en place, puis nous pr{\'e}sentons son {\'e}valuation et les premiers r{\'e}sultats obtenus. Leur rep{\'e}rage atteint de 10{\%} {\`a} 69{\%} de pr{\'e}cision suivant les patrons, celui des unit{\'e}s lexicales varie de 31{\%} {\`a} 56{\%}, suivant le r{\'e}f{\'e}rentiel adopt{\'e}."
2003.jeptalnrecital-long.27,Apprentissage de relations morphologiques en corpus,2003,-1,-1,1,1,8591,pierre zweigenbaum,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous proposons une m{\'e}thode pour apprendre des relations morphologiques d{\'e}rivationnelles en corpus. Elle se fonde sur la cooccurrence en corpus de mots formellement proches et un filtrage compl{\'e}mentaire sur la forme des mots d{\'e}riv{\'e}s. Elle est mise en oeuvre et exp{\'e}riment{\'e}e sur un corpus m{\'e}dical. Les relations obtenues avant filtrage ont une pr{\'e}cision moyenne de 75,6 {\%} au 5000{\`e} rang (fen{\^e}tre de 150 mots). L{'}examen d{\'e}taill{\'e} des d{\'e}riv{\'e}s adjectivaux d{'}un {\'e}chantillon de 633 noms du champ de l{'}anatomie montre une bonne pr{\'e}cision de 85{--}91 {\%} et un rappel mod{\'e}r{\'e} de 32{--}34 {\%}. Nous discutons ces r{\'e}sultats et proposons des pistes pour les compl{\'e}ter."
W02-1403,Lexically-Based Terminology Structuring: Some Inherent Limits,2002,9,15,2,0.810811,5649,natalia grabar,{COLING}-02: {COMPUTERM} 2002: Second International Workshop on Computational Terminology,0,"Terminology structuring has been the subject of much work in the context of terms extracted from corpora: given a set of terms, obtained from an existing resource or extracted from a corpus, identifying hierarchical (or other types of) relations between these terms. The present paper focusses on terminology structuring by lexical methods, which match terms on the basis on their content words, taking morphological variants into account. Experiments are done on a 'flat' list of terms obtained from an originally hierarchically-structured terminology: the French version of the US National Library of Medicine MeSH thesaurus. We compare the lexically-induced relations with the original MeSH relations: after a quantitative evaluation of their congruence through recall and precision metrics, we perform a qualitative, human analysis of the 'new' relations not present in the MeSH. This analysis shows, on the one hand, the limits of the lexical structuring method. On the other hand, it also reveals some specific structuring choices and naming conventions made by the MeSH designers, and emphasizes ontological commitments that cannot be left to automatic structuring."
W02-0304,Accenting unknown words in a specialized language,2002,12,7,1,1,8591,pierre zweigenbaum,Proceedings of the {ACL}-02 Workshop on Natural Language Processing in the Biomedical Domain,0,"We propose two internal methods for accenting unknown words, which both learn on a reference set of accented words the contexts of occurrence of the various accented forms of a given letter. One method is adapted from POS tagging, the other is based on finite state transducers.We show experimental results for letter e on the French version of the Medical Subject Headings thesaurus. With the best training set, the tagging method obtains a precision-recall breakeven point of 84.2xc2xb14.4% and the transducer method 83.8xc2xb14.5% (with a baseline at 64%) for the unknown words that contain this letter. A consensus combination of both increases precision to 92.0xc2xb13.7% with a recall of 75%. We perform an error analysis and discuss further steps that might help improve over the current performance."
C02-2020,"Looking for Candidate Translational Equivalents in Specialized, Comparable Corpora",2002,14,102,2,0,50126,yunchuang chiao,{COLING} 2002: The 17th International Conference on Computational Linguistics: Project Notes,0,"Previous attempts at identifying translational equivalents in comparable corpora have dealt with very large 'general language' corpora and words. We address this task in a specialized domain, medicine, starting from smaller non-parallel, comparable corpora and an initial bilingual medical lexicon. We compare the distributional contexts of source and target words, testing several weighting factors and similarity measures. On a test set of frequently occurring words, for the best combination (the Jaccard similarity measure with or without tf.idf weighting), the correct translation is ranked first for 20% of our test words, and is found in the top 10 candidates for 50% of them. An additional reverse-translation filtering step improves the precision of the top candidate translation up to 74%, with a 33% recall."
2002.jeptalnrecital-long.3,Accentuation de mots inconnus : application au thesaurus biom{\\'e}dical {M}e{SH},2002,10,1,1,1,8591,pierre zweigenbaum,Actes de la 9{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Certaines ressources textuelles ou terminologiques sont {\'e}crites sans signes diacritiques, ce qui freine leur utilisation pour le traitement automatique des langues. Dans un domaine sp{\'e}cialis{\'e} comme la m{\'e}decine, il est fr{\'e}quent que les mots rencontr{\'e}s ne se trouvent pas dans les lexiques {\'e}lectroniques disponibles. Se pose alors la question de l{'}accentuation de mots inconnus : c{'}est le sujet de ce travail. Nous proposons deux m{\'e}thodes d{'}accentuation de mots inconnus fond{\'e}es sur un apprentissage par observation des contextes d{'}occurrence des lettres {\`a} accentuer dans un ensemble de mots d{'}entra{\^\i}nement, l{'}une adapt{\'e}e de l{'}{\'e}tiquetage morphosyntaxique, l{'}autre adapt{\'e}e d{'}une m{\'e}thode d{'}apprentissage de r{\`e}gles morphologiques. Nous pr{\'e}sentons des r{\'e}sultats exp{\'e}rimentaux pour la lettre e sur un thesaurus biom{\'e}dical en fran{\c{c}}ais : le MeSH. Ces m{\'e}thodes obtiennent une pr{\'e}cision de 86 {\`a} 96 {\%} (+-4 {\%}) pour un rappel allant de 72 {\`a} 86 {\%}."
2001.jeptalnrecital-poster.13,L{'}apport de connaissances morphologiques pour la projection de requ{\\^e}tes sur une terminologie normalis{\\'e}e,2001,8,0,1,1,8591,pierre zweigenbaum,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"L{'}apport de connaissances linguistiques {\`a} la recherche d{'}information reste un sujet de d{\'e}bat. Nous examinons ici l{'}influence de connaissances morphologiques (flexion, d{\'e}rivation) sur les r{\'e}sultats d{'}une t{\^a}che sp{\'e}cifique de recherche d{'}information dans un domaine sp{\'e}cialis{\'e}. Cette influence est {\'e}tudi{\'e}e {\`a} l{'}aide d{'}une liste de requ{\^e}tes r{\'e}elles recueillies sur un serveur op{\'e}rationnel ne disposant pas de connaissances linguistiques. Nous observons que pour cette t{\^a}che, flexion et d{\'e}rivation apportent un gain mod{\'e}r{\'e} mais r{\'e}el."
C96-1025,Processing Metonymy- a Domain-Model Heuristic Graph Traversal Approach,1996,12,11,3,0,56016,jacques bouaud,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"We address here the treatment of metonymic expressions from a knowledge representation perspective, that is, in the context of a text understanding system which aims to build a conceptual representation from texts according to a domain model expressed in a knowledge representation formalism. We focus in this paper on the part of the semantic analyser which deals with semantic composition. We explain how we use the domain model to handle metonymy dynamically, and more generally, to underlie semantic composition, using the knowledge descriptions attached to each concept of our ontology as a kind of concept-level, multiple-role qualia structure. We rely for this on a heuristic path search algorithm that exploits the graphic aspects of the conceptual graphs formalism. The methods described have been implemented and applied on French texts in the medical domain."
C90-1019,Deep Sentence Understanding in a Restricted Domain,1990,6,4,1,1,8591,pierre zweigenbaum,{COLING} 1990 Volume 1: Papers presented to the 13th International Conference on Computational Linguistics,0,"We present here the current prototype of the text understanding system HELENE. The objective of this system is to achieve a deep understanding of small reports dealing with a restricted domain. Sentence understanding builds a model of the state of the world described, through the application of several knowledge modules: (i) LFG parsing, (ii) syntactic disambiguation based on lexical entry semantic components, (iii) assembly of semantic components and instantiation of domain entities, and (iv) construction of a world model through activation of common sense and domain knowledge."
