2021.nodalida-main.28,{NLI} Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance,2021,-1,-1,2,0,2672,aarne talman,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences is still unclear. We propose a new diagnostics test suite which allows to assess whether a dataset constitutes a good testbed for evaluating the models{'} meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used benchmarks (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the dataset is likely to contain statistical biases and artefacts that guide prediction. Inversely, a large decrease in model accuracy indicates that the original dataset provides a proper challenge to the models{'} reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks."
2021.naacl-main.370,Scalar Adjective Identification and Multilingual Ranking,2021,-1,-1,2,1,4350,aina soler,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The intensity relationship that holds between scalar adjectives (e.g., nice {\textless} great {\textless} wonderful) is highly relevant for natural language inference and common-sense reasoning. Previous research on scalar adjective ranking has focused on English, mainly due to the availability of datasets for evaluation. We introduce a new multilingual dataset in order to promote research on scalar adjectives in new languages. We perform a series of experiments and set performance baselines on this dataset, using monolingual and multilingual contextual language models. Additionally, we introduce a new binary classification task for English scalar adjective identification which examines the models{'} ability to distinguish scalar from relational adjectives. We probe contextualised representations and report baseline results for future comparison on this task."
2021.blackboxnlp-1.7,{ALL} Dolphins Are Intelligent and {SOME} Are Friendly: Probing {BERT} for Nouns{'} Semantic Properties and their Prototypicality,2021,-1,-1,1,1,2673,marianna apidianaki,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"Large scale language models encode rich commonsense knowledge acquired through exposure to massive data during pre-training, but their understanding of entities and their semantic properties is unclear. We probe BERT (Devlin et al., 2019) for the properties of English nouns as expressed by adjectives that do not restrict the reference scope of the noun they modify (as in {``}red car{''}), but instead emphasise some inherent aspect ({``}red strawberry{''}). We base our study on psycholinguistics datasets that capture the association strength between nouns and their semantic features. We probe BERT using cloze tasks and in a classification setting, and show that the model has marginal knowledge of these features and their prevalence as expressed in these datasets. We discuss factors that make evaluation challenging and impede drawing general conclusions about the models{'} knowledge of noun properties. Finally, we show that when tested in a fine-tuning setting addressing entailment, BERT successfully leverages the information needed for reasoning about the meaning of adjective-noun constructions outperforming previous methods."
2020.semeval-1.18,{MULTISEM} at {S}em{E}val-2020 Task 3: Fine-tuning {BERT} for Lexical Meaning,2020,-1,-1,2,1,4350,aina soler,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"We present the MULTISEM systems submitted to SemEval 2020 Task 3: Graded Word Similarity in Context (GWSC). We experiment with injecting semantic knowledge into pre-trained BERT models through fine-tuning on lexical semantic tasks related to GWSC. We use existing semantically annotated datasets, and propose to approximate similarity through automatically generated lexical substitutes in context. We participate in both GWSC subtasks and address two languages, English and Finnish. Our best English models occupy the third and fourth positions in the ranking for the two subtasks. Performance is lower for the Finnish models which are mid-ranked in the respective subtasks, highlighting the important role of data availability for fine-tuning."
2020.emnlp-main.598,"{BERT} Knows {P}unta {C}ana is not just beautiful, it{'}s gorgeous: Ranking Scalar Adjectives with Contextualised Representations",2020,-1,-1,2,1,4350,aina soler,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources."
2020.blackboxnlp-1.13,Controlling the Imprint of Passivization and Negation in Contextualized Representations,2020,-1,-1,4,0,12485,hande celikkanat,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"Contextualized word representations encode rich information about syntax and semantics, alongside specificities of each context of use. While contextual variation does not always reflect actual meaning shifts, it can still reduce the similarity of embeddings for word instances having the same meaning. We explore the imprint of two specific linguistic alternations, namely passivization and negation, on the representations generated by neural models trained with two different objectives: masked language modeling and translation. Our exploration methodology is inspired by an approach previously proposed for removing societal biases from word vectors. We show that passivization and negation leave their traces on the representations, and that neutralizing this information leads to more similar embeddings for words that should preserve their meaning in the transformation. We also find clear differences in how the respective features generalize across datasets."
W19-5802,{LIMSI}-{MULTISEM} at the {IJCAI} {S}em{D}eep-5 {W}i{C} Challenge: Context Representations for Word Usage Similarity Estimation,2019,0,1,2,1,4350,aina soler,Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5),0,None
W19-5032,Embedding Biomedical Ontologies by Jointly Encoding Network Structure and Textual Node Descriptors,2019,0,0,5,0,20600,sotiris kotitsas,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Network Embedding (NE) methods, which map network nodes to low-dimensional feature vectors, have wide applications in network analysis and bioinformatics. Many existing NE methods rely only on network structure, overlooking other information associated with the nodes, e.g., text describing the nodes. Recent attempts to combine the two sources of information only consider local network structure. We extend NODE2VEC, a well-known NE method that considers broader network structure, to also consider textual node descriptors using recurrent neural encoders. Our method is evaluated on link prediction in two networks derived from UMLS. Experimental results demonstrate the effectiveness of the proposed approach compared to previous work."
W19-0423,A Comparison of Context-sensitive Models for Lexical Substitution,2019,0,2,3,1,4350,aina soler,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"Word embedding representations provide good estimates of word meaning and give state-of-the art performance in semantic tasks. Embedding approaches differ as to whether and how they account for the context surrounding a word. We present a comparison of different word and context representations on the task of proposing substitutes for a target word in context (lexical substitution). We also experiment with tuning contextualized word embeddings on a dataset of sense-specific instances for each target word. We show that powerful contextualized word representations, which give high performance in several semantics-related tasks, deal less well with the subtle in-context similarity relationships needed for substitution. This is better handled by models trained with this objective in mind, where the inter-dependence between word and context representations is explicitly modeled during training."
S19-1002,Word Usage Similarity Estimation with Sentence Representations and Automatic Substitutes,2019,26,0,2,1,4350,aina soler,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"Usage similarity estimation addresses the semantic proximity of word instances in different contexts. We apply contextualized (ELMo and BERT) word and sentence embeddings to this task, and propose supervised models that leverage these representations for prediction. Our models are further assisted by lexical substitute annotations automatically assigned to word instances by context2vec, a neural model that relies on a bidirectional LSTM. We perform an extensive comparison of existing word and sentence representations on benchmark datasets addressing both graded and binary similarity.The best performing models outperform previous methods in both settings."
N19-1317,Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification,2019,0,8,3,1,9708,reno kriz,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Sentence simplification is the task of rewriting texts so they are easier to understand. Recent research has applied sequence-to-sequence (Seq2Seq) models to this task, focusing largely on training-time improvements via reinforcement learning and memory augmentation. One of the main problems with applying generic Seq2Seq models for simplification is that these models tend to copy directly from the original sentence, resulting in outputs that are relatively long and complex. We aim to alleviate this issue through the use of two main techniques. First, we incorporate content word complexities, as predicted with a leveled word complexity model, into our loss function during training. Second, we generate a large set of diverse candidate simplifications at test time, and rerank these to promote fluency, adequacy, and simplicity. Here, we measure simplicity through a novel sentence complexity model. These extensions allow our models to perform competitively with state-of-the-art systems while generating simpler sentences. We report standard automatic and human evaluation metrics."
D19-1618,{SUM}-{QE}: a {BERT}-based Summary Quality Estimation Model,2019,0,3,3,0,27154,stratos xenouleas,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"We propose SUM-QE, a novel Quality Estimation model for summarization based on BERT. The model addresses linguistic quality aspects that are only indirectly captured by content-based approaches to summary evaluation, without involving comparison with human references. SUM-QE achieves very high correlations with human ratings, outperforming simpler models addressing these linguistic aspects. Predictions of the SUM-QE model can be used for system development, and to inform users of the quality of automatically produced summaries and other types of generated text."
N18-2077,Automated Paraphrase Lattice Creation for {H}y{TER} Machine Translation Evaluation,2018,0,1,1,1,2673,marianna apidianaki,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We propose a variant of a well-known machine translation (MT) evaluation metric, HyTER (Dreyer and Marcu, 2012), which exploits reference translations enriched with meaning equivalent expressions. The original HyTER metric relied on hand-crafted paraphrase networks which restricted its applicability to new data. We test, for the first time, HyTER with automatically built paraphrase lattices. We show that although the metric obtains good results on small and carefully curated data with both manually and automatically selected substitutes, it achieves medium performance on much larger and noisier datasets, demonstrating the limits of the metric for tuning and evaluation of current MT systems."
N18-1019,Simplification Using Paraphrases and Context-Based Lexical Substitution,2018,0,1,3,1,9708,reno kriz,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Lexical simplification involves identifying complex words or phrases that need to be simplified, and recommending simpler meaning-preserving substitutes that can be more easily understood. We propose a complex word identification (CWI) model that exploits both lexical and contextual features, and a simplification mechanism which relies on a word-embedding lexical substitution model to replace the detected complex words with simpler paraphrases. We compare our CWI and lexical simplification models to several baselines, and evaluate the performance of our simplification system against human judgments. The results show that our models are able to detect complex words with higher accuracy than other commonly used methods, and propose good simplification substitutes in context. They also highlight the limited contribution of context features for CWI, which nonetheless improve simplification compared to context-unaware models."
N18-1030,Comparing Constraints for Taxonomic Organization,2018,0,1,2,1,24896,anne cocos,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Building a taxonomy from the ground up involves several sub-tasks: selecting terms to include, predicting semantic relations between terms, and selecting a subset of relational instances to keep, given constraints on the taxonomy graph. Methods for this final step {--} taxonomic organization {--} vary both in terms of the constraints they impose, and whether they enable discovery of synonymous terms. It is hard to isolate the impact of these factors on the quality of the resulting taxonomy because organization methods are rarely compared directly. In this paper, we present a head-to-head comparison of six taxonomic organization algorithms that vary with respect to their structural and transitivity constraints, and treatment of synonymy. We find that while transitive algorithms out-perform their non-transitive counterparts, the top-performing transitive algorithm is prohibitively slow for taxonomies with as few as 50 entities. We propose a simple modification to a non-transitive optimum branching algorithm to explicitly incorporate synonymy, resulting in a method that is substantially faster than the best transitive algorithm while giving complementary performance."
D18-2021,"{M}agnitude: A Fast, Efficient Universal Vector Embedding Utility Package",2018,9,0,4,0,11299,ajay patel,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"Vector space embedding models like word2vec, GloVe, and fastText are extremely popular representations in natural language processing (NLP) applications. We present Magnitude, a fast, lightweight tool for utilizing and processing embeddings. Magnitude is an open source Python package with a compact vector storage file format that allows for efficient manipulation of huge numbers of embeddings. Magnitude performs common operations up to 60 to 6,000 times faster than Gensim. Magnitude introduces several novel features for improved robustness like out-of-vocabulary lookups."
D18-1202,Learning Scalar Adjective Intensity from Paraphrases,2018,0,4,4,1,24896,anne cocos,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Adjectives like {``}warm{''}, {``}hot{''}, and {``}scalding{''} all describe temperature but differ in intensity. Understanding these differences between adjectives is a necessary part of reasoning about natural language. We propose a new paraphrase-based method to automatically learn the relative intensity relation that holds between a pair of scalar adjectives. Our approach analyzes over 36k adjectival pairs from the Paraphrase Database under the assumption that, for example, paraphrase pair {``}really hot{''} {\textless}{--}{\textgreater} {``}scalding{''} suggests that {``}hot{''} {\textless} {``}scalding{''}. We show that combining this paraphrase evidence with existing, complementary pattern- and lexicon-based approaches improves the quality of systems for automatically ordering sets of scalar adjectives and inferring the polarity of indirect answers to {``}yes/no{''} questions."
2018.jeptalnrecital-court.34,A comparative study of word embeddings and other features for lexical complexity detection in {F}rench,2018,0,0,2,1,4350,aina soler,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Lexical complexity detection is an important step for automatic text simplification which serves to make informed lexical substitutions. In this study, we experiment with word embeddings for measuring the complexity of French words and combine them with other features that have been shown to be well-suited for complexity prediction. Our results on a synonym ranking task show that embeddings perform better than other features in isolation, but do not outperform frequency-based systems in this language."
W17-1914,Word Sense Filtering Improves Embedding-Based Lexical Substitution,2017,23,3,2,1,24896,anne cocos,"Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications",0,The role of word sense disambiguation in lexical substitution has been questioned due to the high performance of vector space models which propose good substitutes without explicitly accounting for sense. We show that a filtering mechanism based on a sense inventory optimized for substitutability can improve the results of these models. Our sense inventory is constructed using a clustering method which generates paraphrase clusters that are congruent with lexical substitution annotations in a development set. The results show that lexical substitution can still benefit from senses which can improve the output of vector space paraphrase ranking models.
S17-1002,Learning Antonyms with Paraphrases and a Morphology-Aware Neural Network,2017,15,3,3,0,32400,sneha rajana,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Recognizing and distinguishing antonyms from other types of semantic relations is an essential part of language understanding systems. In this paper, we present a novel method for deriving antonym pairs using paraphrase pairs containing negation markers. We further propose a neural network model, AntNET, that integrates morphological features indicative of antonymy into a path-based relation detection algorithm. We demonstrate that our model outperforms state-of-the-art models in distinguishing antonyms from other semantic relations and is capable of efficiently handling multi-word expressions."
S17-1009,Mapping the Paraphrase Database to {W}ord{N}et,2017,18,2,2,1,24896,anne cocos,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"WordNet has facilitated important research in natural language processing but its usefulness is somewhat limited by its relatively small lexical coverage. The Paraphrase Database (PPDB) covers 650 times more words, but lacks the semantic structure of WordNet that would make it more directly useful for downstream tasks. We present a method for mapping words from PPDB to WordNet synsets with 89{\%} accuracy. The mapping also lays important groundwork for incorporating WordNet{'}s relations into PPDB so as to increase its utility for semantic reasoning in applications."
D17-2007,{K}now{Y}our{N}yms? A Game of Semantic Relationships,2017,11,0,9,0,33062,ross mechanic,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"Semantic relation knowledge is crucial for natural language understanding. We introduce {``}KnowYourNyms?{''}, a web-based game for learning semantic relations. While providing users with an engaging experience, the application collects large amounts of data that can be used to improve semantic relation classifiers. The data also broadly informs us of how people perceive the relationships between words, providing useful insights for research in psychology and linguistics."
D17-1152,Learning Translations via Matrix Completion,2017,42,3,6,0,3275,derry wijaya,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Bilingual Lexicon Induction is the task of learning word translations without bilingual parallel corpora. We model this task as a matrix completion problem, and present an effective and extendable framework for completing the matrix. This method harnesses diverse bilingual and monolingual signals, each of which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages."
S16-1002,{S}em{E}val-2016 Task 5: Aspect Based Sentiment Analysis,2016,13,194,12,0,18398,maria pontiki,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015. In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure. From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval. The task attracted 245 submissions from 29 teams."
N16-3006,{T}rans{R}ead: Designing a Bilingual Reading Experience with Machine Translation Technologies,2016,13,0,3,0,837,franccois yvon,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"In this paper, we use multilingual Natural Language Processing (NLP) tools to improve the reading experience of parallel texts on mobile devices. Such enterprise poses multiple challenging issues both from the NLP and from the Human Computer Interaction (HCI) perspectives. We discuss these problems, and report on our own solutions, now implemented in a full-fledged bilingual reading device."
L16-1179,Datasets for Aspect-Based Sentiment Analysis in {F}rench,2016,6,3,1,1,2673,marianna apidianaki,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Aspect Based Sentiment Analysis (ABSA) is the task of mining and summarizing opinions from text about specific entities and their aspects. This article describes two datasets for the development and testing of ABSA systems for French which comprise user reviews annotated with relevant entities, aspects and polarity values. The first dataset contains 457 restaurant reviews (2365 sentences) for training and testing ABSA systems, while the second contains 162 museum reviews (655 sentences) dedicated to out-of-domain evaluation. Both datasets were built as part of SemEval-2016 Task 5 {``}Aspect-Based Sentiment Analysis{''} where seven different languages were represented, and are publicly available for research purposes."
J16-2003,Word Sense Clustering and Clusterability,2016,52,11,2,0,9803,diana mccarthy,Computational Linguistics,0,"Word sense disambiguation and the related field of automated word sense induction traditionally assume that the occurrences of a lemma can be partitioned into senses. But this seems to be a much easier task for some lemmas than others. Our work builds on recent work that proposes describing word meaning in a graded fashion rather than through a strict partition into senses; in this article we argue that not all lemmas may need the more complex graded analysis, depending on their partitionability. Although there is plenty of evidence from previous studies and from the linguistics literature that there is a spectrum of partitionability of word meanings, this is the first attempt to measure the phenomenon and to couple the machine learning literature on clusterability with word usage data used in computational linguistics.n n We propose to operationalize partitionability as clusterability, a measure of how easy the occurrences of a lemma are to cluster. We test two ways of measuring clusterability: 1 existing measures from the machine learning literature that aim to measure the goodness of optimal k-means clusterings, and 2 the idea that if a lemma is more clusterable, two clusterings based on two different views of the same data points will be more congruent. The two views that we use are two different sets of manually constructed lexical substitutes for the target lemma, on the one hand monolingual paraphrases, and on the other hand translations. We apply automatic clustering to the manual annotations. We use manual annotations because we want the representations of the instances that we cluster to be as informative and clean as possible. We show that when we control for polysemy, our measures of clusterability tend to correlate with partitionability, in particular some of the type-1 clusterability measures, and that these measures outperform a baseline that relies on the amount of overlap in a soft clustering."
D16-1215,Vector-space models for {PPDB} paraphrase ranking in context,2016,18,3,1,1,2673,marianna apidianaki,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
2016.jeptalnrecital-demo.12,Lecture bilingue augment{\\'e}e par des alignements multi-niveaux (Augmenting bilingual reading with alignment information),2016,-1,-1,3,0,837,franccois yvon,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 5 : D{\\'e}monstrations,0,"Le travail qui a conduit {\`a} cette d{\'e}monstration combine des outils de traitement des langues multilingues, en particulier l{'}alignement automatique, avec des techniques de visualisation et d{'}interaction. Il vise {\`a} proposer des pistes pour le d{\'e}veloppement d{'}outils permettant de lire simultan{\'e}ment les diff{\'e}rentes versions d{'}un texte disponible en plusieurs langues, avec des applications en lecture de loisir ou en lecture professionnelle."
W15-3048,Alignment-based sense selection in {METEOR} and the {RATATOUILLE} recipe,2015,23,5,2,0,8610,benjamin marie,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes Meteor-WSD and RATATOUILLE, the LIMSI submissions to the WMT15 metrics shared task. MeteorWSD extends synonym mapping to languages other than English based on alignments and gives credit to semantically adequate translations in context. We show that context-sensitive synonym selection increases the correlation of the Meteor metric with human judgments of translation quality on the WMT14 data. RATATOUILLE combines MeteorWSD with nine other metrics for evaluation and outperforms the best metric (BEER) involved in its computation."
W15-1006,{METEOR}-{WSD}: Improved Sense Matching in {MT} Evaluation,2015,9,3,1,1,2673,marianna apidianaki,"Proceedings of the Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,We present an initial experiment in integrating a disambiguation step in MT evaluation. We show that accounting for sense distinctions helps METEOR establish better sense correspondences and improves its correlation with human judgments of translation quality.
S15-2050,{LIMSI}: Translations as Source of Indirect Supervision for Multilingual All-Words Sense Disambiguation and Entity Linking,2015,12,5,1,1,2673,marianna apidianaki,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,We present the LIMSI submission to the Multilingual Word Sense Disambiguation and Entity Linking task of SemEval-2015. The system exploits the parallelism of the multilingual test data and uses translations as source of indirect supervision for sense selection. The LIMSI system gets best results in English in all domains and shows that alignment information can successfully guide disambiguation. This simple but effective method can serve to generate high quality sense annotated data for WSD system training.
apidianaki-etal-2014-semantic,Semantic Clustering of Pivot Paraphrases,2014,24,12,1,1,2673,marianna apidianaki,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Paraphrases extracted from parallel corpora by the pivot method (Bannard and Callison-Burch, 2005) constitute a valuable resource for multilingual NLP applications. In this study, we analyse the semantics of unigram pivot paraphrases and use a graph-based sense induction approach to unveil hidden sense distinctions in the paraphrase sets. The comparison of the acquired senses to gold data from the Lexical Substitution shared task (McCarthy and Navigli, 2007) demonstrates that sense distinctions exist in the paraphrase sets and highlights the need for a disambiguation step in applications using this resource."
F14-1005,Cross-lingual Word Sense Disambiguation for Predicate Labelling of {F}rench,2014,26,2,2,0,12096,lonneke plas,Proceedings of TALN 2014 (Volume 1: Long Papers),0,"We address the problem of transferring semantic annotations, more specifically predicate labellings, from one language to another using parallel corpora. Previous work has transferred these annotations directly at the token level, leading to low recall. We present a global approach to annotation transfer that aggregates information across the whole parallel corpus. We show that this global method outperforms previous results in terms of recall without sacrificing precision too much. Mots-cles : transfert inter-langue, annotation semantique automatique, predicats, desambiguisation lexicale, cor- pus paralleles."
C14-1121,Global Methods for Cross-lingual Semantic Role and Predicate Labelling,2014,29,10,2,0,12096,lonneke plas,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We address the problem of transferring semantic annotations to new languages using parallel corpora. Previous work has transferred these annotations on a token-to-token basis, an approach that is sensitive to alignment errors and translation shifts. We present a global approach to transfer that aggregates information across the whole parallel corpus and leads to more robust labellers. We build two global models, one for predicate labelling and one for role labelling, each tailored to the task at hand. We show that the combination of direct and global methods outperforms previous results."
W13-2501,Cross-lingual {WSD} for Translation Extraction from Comparable Corpora,2013,22,1,1,1,2673,marianna apidianaki,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"We propose a data-driven approach to enhance translation extraction from comparable corpora. Instead of resorting to an external dictionary, we translate source vector features by using a cross-lingual Word Sense Disambiguation method. The candidate senses for a feature correspond to sense clusters of its translations in a parallel corpus and the context used for disambiguation consists of the vector that contains the feature. The translations found in the disambiguation output convey the sense of the features in the source vector, while the use of translation clusters permits to expand their translation with several variants. As a consequence, the translated vectors are less noisy and richer, and allow for the extraction of higher quality lexicons compared to simpler methods."
S13-2032,{LIMSI} : Cross-lingual Word Sense Disambiguation using Translation Sense Clustering,2013,12,5,1,1,2673,marianna apidianaki,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,We describe the LIMSI system for the SemEval-2013 Cross-lingual Word Sense Disambiguation (CLWSD) task. Word senses are represented by means of translation clusters in different languages built by a cross-lingual Word Sense Induction (WSI) method. Our CLWSD classifier exploits the WSI output for selecting appropriate translations for target words in context. We present the design of the system and the obtained results.
W12-4201,{WSD} for n-best reranking and local language modeling in {SMT},2012,22,6,1,1,2673,marianna apidianaki,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"We integrate semantic information at two stages of the translation process of a state-of-the-art SMT system. A Word Sense Disambiguation (WSD) classifier produces a probability distribution over the translation candidates of source words which is exploited in two ways. First, the probabilities serve to rerank a list of n-best translations produced by the system. Second, the WSD predictions are used to build a supplementary language model for each sentence, aimed to favor translations that seem more adequate in this specific sentential context. Both approaches lead to significant improvements in translation performance, highlighting the usefulness of source side disambiguation for SMT."
W12-3141,{LIMSI} @ {WMT}12,2012,23,1,4,0,40944,haison le,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes LIMSI's submissions to the shared translation task. We report results for French-English and German-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams. In this approach, both the translation and target language models are estimated as conventional smoothed n-gram models; an approach we extend here by estimating the translation probabilities in a continuous space using neural networks. Experimental results show a significant and consistent BLEU improvement of approximately 1 point for all conditions. We also report preliminary experiments using an on-the-fly translation model."
apidianaki-sagot-2012-applying,Applying cross-lingual {WSD} to wordnet development,2012,20,3,1,1,2673,marianna apidianaki,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The automatic development of semantic resources constitutes an important challenge in the NLP community. The methods used generally exploit existing large-scale resources, such as Princeton WordNet, often combined with information extracted from multilingual resources and parallel corpora. In this paper we show how Cross-Lingual Word Sense Disambiguation can be applied to wordnet development. We apply the proposed method to WOLF, a free wordnet for French still under construction, in order to fill synsets that did not contain any literal yet and increase its coverage."
gabor-etal-2012-boosting,Boosting the Coverage of a Semantic Lexicon by Automatically Extracted Event Nominalizations,2012,18,0,2,0,22563,kata gabor,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this article, we present a distributional analysis method for extracting nominalization relations from monolingual corpora. The acquisition method makes use of distributional and morphological information to select nominalization candidates. We explain how the learning is performed on a dependency annotated corpus and describe the nominalization results. Furthermore, we show how these results served to enrich an existing lexical resource, the WOLF (Wordnet Libre du Franc{\^A}{\c{}}ais). We present the techniques that we developed in order to integrate the new information into WOLF, based on both its structure and content. Finally, we evaluate the validity of the automatically obtained information and the correctness of its integration into the semantic resource. The method proved to be useful for boosting the coverage of WOLF and presents the advantage of filling verbal synsets, which are particularly difficult to handle due to the high level of verbal polysemy."
C12-2007,Measuring the Adequacy of Cross-Lingual Paraphrases in a Machine Translation Setting,2012,22,0,1,1,2673,marianna apidianaki,Proceedings of {COLING} 2012: Posters,0,"Following the growing trend in the semantics community towards models adapted to specific applications, the SemEval-2 Cross-Lingual Lexical Substitution and Word Sense Disambiguation tasks address the disambiguation needs of Machine Translation (MT). The experiments conducted in this study aim at assessing whether the proposed evaluation protocol and methodology provide a fair estimate of the adequacy of cross-lingual predictions in translations. For this purpose, the gold SemEval paraphrases are fed into a state-of-the-art MT system and the obtained translations are compared to paraphrase quality judgments based on the source context. The results show the strong dependence of cross-lingual paraphrase adequacy on the translation context and cast doubt on the contribution that systems performing well in existing evaluation schemes would have on MT. These empirical findings highlight the importance of complementing the current evaluation schemes with translation information to allow a more accurate estimation of the systems impact on end-to-end applications."
W11-2203,Unsupervised Cross-Lingual Lexical Substitution,2011,28,3,1,1,2673,marianna apidianaki,Proceedings of the First workshop on Unsupervised Learning in {NLP},0,"Cross-Lingual Lexical Substitution (CLLS) is the task that aims at providing for a target word in context, several alternative substitute words in another language. The proposed sets of translations may come from external resources or be extracted from textual data. In this paper, we apply for the first time an unsupervised cross-lingual WSD method to this task. The method exploits the results of a cross-lingual word sense induction method that identifies the senses of words by clustering their translations according to their semantic similarity. We evaluate the impact of using clustering information for CLLS by applying the WSD method to the SemEval-2010 CLLS data set. Our system performs better on the 'out-of-ten' measure than the systems that participated in the SemEval task, and is ranked medium on the other measures. We analyze the results of this evaluation and discuss avenues for a better overall integration of unsupervised sense clustering in this setting."
P11-1148,Latent Semantic Word Sense Induction and Disambiguation,2011,26,43,2,0,5597,tim cruys,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"In this paper, we present a unified model for the automatic induction of word senses from text, and the subsequent disambiguation of particular word instances using the automatically extracted sense inventory. The induction step and the disambiguation step are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. The intuition is that a particular sense is associated with a particular topic, so that different senses can be discriminated through their association with particular topical dimensions; in a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions. The model is evaluated on the semeval-2010 word sense induction and disambiguation task, on which it reaches state-of-the-art results."
2010.iwslt-papers.2,An algorithm for cross-lingual sense-clustering tested in a {MT} evaluation setting,2010,28,15,1,1,2673,marianna apidianaki,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,"Unsupervised sense induction methods offer a solution to the problem of scarcity of semantic resources. These methods automatically extract semantic information from textual data and create resources adapted to specific applications and domains of interest. In this paper, we present a clustering algorithm for cross-lingual sense induction which generates bilingual semantic inventories from parallel corpora. We describe the clustering procedure and the obtained resources. We then proceed to a large-scale evaluation by integrating the resources into a Machine Translation (MT) metric (METEOR). We show that the use of the data-driven sense-cluster inventories leads to better correlation with human judgments of translation quality, compared to precision-based metrics, and to improvements similar to those obtained when a hand-crafted semantic resource is used."
Y09-1007,Capturing Lexical Variation in {MT} Evaluation Using Automatically Built Sense-Cluster Inventories,2009,24,1,1,1,2673,marianna apidianaki,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"The strict character of most of the existing Machine Translation (MT) evaluation metrics does not permit them to capture lexical variation in translation. However, a central issue in MT evaluation is the high correlation that the metrics should have with human judg- ments of translation quality. In order to achieve a higher correlation, the identification of sense correspondences between the compared translations becomes really important. Given that most metrics are looking for exact correspondences, the evaluation results are often mis- leading concerning translation quality. Apart from that, existing metrics do not permit one to make a conclusive estimation of the impact of Word Sense Disambiguation techniques into MT systems. In this paper, we show how information acquired by an unsupervised semantic analysis method can be used to render MT evaluation more sensitive to lexical semantics. The sense inventories built by this data-driven method are incorporated into METEOR: they replace WordNet for evaluation in English and render METEOR's synonymy module operable in French. The evaluation results demonstrate that the use of these inventories gives rise to an increase in the number of matches and the correlation with human judgments of translation quality, compared to precision-based metrics."
E09-1010,Data-Driven Semantic Analysis for Multilingual {WSD} and Lexical Selection in Translation,2009,26,39,1,1,2673,marianna apidianaki,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"A common way of describing the senses of ambiguous words in multilingual Word Sense Disambiguation (WSD) is by reference to their translation equivalents in another language. The theoretical soundness of the senses induced in this way can, however, be doubted. This type of cross-lingual sense identification has implications for multilingual WSD and MT evaluation as well. In this article, we first present some arguments in favour of a more thorough analysis of the semantic information that may be induced by the equivalents of ambiguous words found in parallel corpora. Then, we present an unsupervised WSD method and a lexical selection method that exploit the results of a data-driven sense induction method. Finally, we show how this automatically acquired information can be exploited for a multilingual WSD and MT evaluation more sensitive to lexical semantics."
2009.jeptalnrecital-position.2,"La place de la d{\\'e}sambigu{\\\\\i}sation lexicale dans la Traduction Automatique Statistique""",2009,0,0,1,1,2673,marianna apidianaki,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Prise de position,0,"L{'}{\'e}tape de la d{\'e}sambigu{\""\i}sation lexicale est souvent esquiv{\'e}e dans les syst{\`e}mes de Traduction Automatique Statistique (Statistical Machine Translation (SMT)) car consid{\'e}r{\'e}e comme non n{\'e}cessaire {\`a} la s{\'e}lection de traductions correctes. Le d{\'e}bat autour de cette n{\'e}cessit{\'e} est actuellement assez vif. Dans cet article, nous pr{\'e}sentons les principales positions sur le sujet. Nous analysons les avantages et les inconv{\'e}nients de la conception actuelle de la d{\'e}sambigu{\""\i}sation dans le cadre de la SMT, d{'}apr{\`e}s laquelle les sens des mots correspondent {\`a} leurs traductions dans des corpus parall{\`e}les. Ensuite, nous pr{\'e}sentons des arguments en faveur d{'}une analyse plus pouss{\'e}e des informations s{\'e}mantiques induites {\`a} partir de corpus parall{\`e}les et nous expliquons comment les r{\'e}sultats d{'}une telle analyse pourraient {\^e}tre exploit{\'e}s pour une {\'e}valuation plus flexible et concluante de l{'}impact de la d{\'e}sambigu{\""\i}sation dans la SMT."
apidianaki-2008-translation,Translation-oriented Word Sense Induction Based on Parallel Corpora,2008,34,21,1,1,2673,marianna apidianaki,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Word Sense Disambiguation (WSD) is an intermediate task that serves as a means to an end defined by the application in which it is to be used. However, different applications have varying disambiguation needs which should have an impact on the choice of the method and of the sense inventory used. The tendency towards application-oriented WSD becomes more and more evident, mostly because of the inadequacy of predefined sense inventories and the inefficacy of application-independent methods in accomplishing specific tasks. In this article, we present a data-driven method of sense induction, which combines contextual and translation information coming from a bilingual parallel training corpus. It consists of an unsupervised method that clusters semantically similar translation equivalents of source language (SL) polysemous words. The created clusters are projected on the SL words revealing their sense distinctions. Clustered equivalents describing a sense of a polysemous word can be considered as more or less commutable translations for an instance of the word carrying this sense. The resulting sense clusters can thus be used for WSD and sense annotation, as well as for lexical selection in translation applications."
2007.jeptalnrecital-long.19,"Rep{\\'e}rage de sens et d{\\'e}sambigu{\\\\\i}sation dans un contexte bilingue""",2007,-1,-1,1,1,2673,marianna apidianaki,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les besoins de d{\'e}sambigu{\""\i}sation varient dans les diff{\'e}rentes applications du Traitement Automatique des Langues (TAL). Dans cet article, nous proposons une m{\'e}thode de d{\'e}sambigu{\""\i}sation lexicale op{\'e}ratoire dans un contexte bilingue et, par cons{\'e}quent, ad{\'e}quate pour la d{\'e}sambigu{\""\i}sation au sein d{'}applications relatives {\`a} la traduction. Il s{'}agit d{'}une m{\'e}thode contextuelle, qui combine des informations de cooccurrence avec des informations traductionnelles venant d{'}un bitexte. L{'}objectif est l{'}{\'e}tablissement de correspondances de traduction au niveau s{\'e}mantique entre les mots de deux langues. Cette m{\'e}thode {\'e}tend les cons{\'e}quences de l{'}hypoth{\`e}se contextuelle du sens dans un contexte bilingue, tout en admettant l{'}existence d{'}une relation de similarit{\'e} s{\'e}mantique entre les mots de deux langues en relation de traduction. La mod{\'e}lisation de ces correspondances de granularit{\'e} fine permet la d{\'e}sambigu{\""\i}sation lexicale de nouvelles occurrences des mots polys{\'e}miques de la langue source ainsi que la pr{\'e}diction de la traduction la plus ad{\'e}quate pour ces occurrences."
2006.jeptalnrecital-long.2,Traitement de la polys{\\'e}mie lexicale dans un but de traduction,2006,-1,-1,1,1,2673,marianna apidianaki,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"La d{\'e}sambigu{\""\i}sation lexicale a une place centrale dans les applications de Traitement Automatique des Langues relatives {\`a} la traduction. Le travail pr{\'e}sent{\'e} ici fait partie d{'}une {\'e}tude sur les recouvrements et les divergences entre les espaces s{\'e}mantiques occup{\'e}s par des unit{\'e}s polys{\'e}miques de deux langues. Les correspondances entre ces unit{\'e}s sont rarement biunivoques et l{'}{\'e}tude de ces correspondances aide {\`a} tirer des conclusions sur les possibilit{\'e}s et les limites d{'}utilisation d{'}une autre langue pour la d{\'e}sambigu{\""\i}sation des unit{\'e}s d{'}une langue source. Le but de ce travail est l{'}{\'e}tablissement de correspondances d{'}une granularit{\'e} optimale entre les unit{\'e}s de deux langues entretenant des relations de traduction. Ces correspondances seraient utilisables pour la pr{\'e}diction des {\'e}quivalents de traduction les plus ad{\'e}quats de nouvelles occurrences des {\'e}l{\'e}ments polys{\'e}miques."
