2020.argmining-1.2,D19-1290,0,0.0469679,"Missing"
2020.argmining-1.2,C16-1324,0,0.0584976,"Missing"
2020.argmining-1.2,D17-1218,0,0.045809,"Missing"
2020.argmining-1.2,D15-1267,0,0.0605193,"Missing"
2020.argmining-1.2,J17-1004,0,0.051735,"Missing"
2020.argmining-1.2,N12-1003,0,0.070145,"Missing"
2020.argmining-1.2,W15-0503,0,0.0363498,"Missing"
2020.argmining-1.2,W14-2105,0,0.0526493,"Missing"
2020.argmining-1.2,passonneau-2006-measuring,0,0.180043,"Missing"
2020.argmining-1.2,W16-6613,0,0.0330251,"Missing"
2020.argmining-1.2,C08-1101,0,0.126893,"Missing"
2020.argmining-1.2,D14-1006,0,0.0686345,"Missing"
2020.argmining-1.2,W14-2508,0,0.0748613,"Missing"
2020.calcs-1.6,C16-1234,0,0.0168068,"consisting of the following tags: en (English), hi (Hindi), mixed and univ (e.g., symbols, @ mentions, hashtags). Table 1 shows some examples of the Hinglish code-mixed data, whereas Table 2 lists the statistics of the data set used for the sentiment analysis experiments. Seminal work in sentiment analysis (SA) of Hindi text was done by Joshi et al. (Joshi et al., 2010), who built a system containing a classification, machine translation and sentiment lexicon module. Bakliwal et al. (2012) created a sentiment lexicon for Hindi, and Das and Bandyophadhyay (2010) created the Hindi SentiWordNet. Joshi et al. (2016) introduce a Hindi-English code-mixed dataset for sentiment analysis and propose a system to SA that learns sub-word level representations in LSTM (Long Short-Term Memory) (Subword-LSTM) instead of character- or word-level representations. As mentioned before, the data set contains a mixture of English and romanized or transliterated Hindi. This produces an additional challenge, as this romanized codemixed data contains non-standard spellings like aapke and apke (“your”), non-grammatical constructions like “Wow the amusement never ends even after the election Daily soap bana ke rakh diya” whic"
2020.calcs-1.6,R19-1071,1,0.841433,"sh code-mixed tweets and compared them with monolingual baseline systems, resulting in the following three experimental setups: Cross-lingual embeddings rely on the inherent similarities in language structure and composition to project multiple monolingual embeddings into the same space, enabling tasks which require knowledge of more than one language (Conneau et al., 2018). This kind of embeddings have been used to solve a variety of tasks like wordto-word translation (Chen and Cardie, 2018), evaluating ¨ sentence similarity (Bjerva and Ostling, 2017) and detecting cognates across languages (Labat and Lefever, 2019). Most methods to project two or more monolingual embeddings into a shared space require a parallel seed dictionary to initialize an alignment which can then be improved upon (Upadhyay et al., 2016). The latter approach is not feasible, though, in this particular setting, as we aim to align English words with code-mixed (transliterated) Hinglish words, which often have no standardised spelling, but on the contrary occur with many variations in social media data. In recent research, however, a number of methods have been explored that seek to create a projection without any seed dictionary by r"
2020.calcs-1.6,W15-1521,0,0.0286006,"n as very challenging problems for Hinglish. Similar work has been carried out by Sharma et al. (2016), who developed a shallow parser for Hindi-English code-mixed social media text. Rijhwani et al. (2017) introduce an unsupervised wordlevel language detection technique (using a Hidden Markov Model) for code-switched text on Twitter that can be applied to different languages. Pratapa et al. (2018) compare three bilingual word embedding approaches, bilingual correlation based embeddings (Faruqui and Dyer, 2014), bilingual compositional model (Hermann and Blunsom, 2014) and bilingual Skip-gram (Luong et al., 2015), to perform code-mixed sentiment analysis and Part-of-Speech tagging. In addition, they also train skip gram embeddings on synthetic codemixed text. Their results show that the applied bilingual embeddings do not perform well, and that multilingual embeddings might be a better solution to process code-mixed text. This is mainly due to the fact that code-mixed text contains particular semantic and syntactic structures that do not occur in the respective monolingual corpora. does so using translation systems (Zhou et al., 2016) or cross-lingual signals in another form, such as parallel corpora"
2020.calcs-1.6,S16-1001,0,0.0362634,"beddings aligned using numerals and common tokens like “https” as a bilingual seed dictionary. This methods is especially interesting to look at as there is a decent overlap between the vocabulary of both embeddings as Hinglish is a derivative of English. The classifiers were then trained and tested by means of 5-fold cross-validation on the SemEval 2020 data. 4.3. the sentiment-related information in the shared space in which both languages reside. To test these assumptions, we train a bi-directional LSTM on the English sentiment data of the SemEval-2016 “Sentiment Analysis in Twitter” task (Nakov et al., 2016) using English embeddings in the same shared space as code-mixed Hinglish embeddings. We then evaluate the model on the SemEval-2020 Hinglish data set, using the Hinglish embeddings pre-aligned with English embeddings. Figure 1 illustrates the intuition behind this experiment. Since the model learns to associate particular words to particular sentiments in English during the supervision step, it should ideally also pick up the corresponding words and their sentiments in the code-mixed data due to the shared space, and by consequence be able to perform sentiment analysis with no direct supervis"
2020.calcs-1.6,D13-1084,0,0.0699037,"Missing"
2020.calcs-1.6,D18-1344,0,0.118896,"for Hindi-English from Facebook forums, and performed experiments for language identification, back-transliteration, normalization and part-of-speech tagging on this corpus. They identify normalisation and transliteration as very challenging problems for Hinglish. Similar work has been carried out by Sharma et al. (2016), who developed a shallow parser for Hindi-English code-mixed social media text. Rijhwani et al. (2017) introduce an unsupervised wordlevel language detection technique (using a Hidden Markov Model) for code-switched text on Twitter that can be applied to different languages. Pratapa et al. (2018) compare three bilingual word embedding approaches, bilingual correlation based embeddings (Faruqui and Dyer, 2014), bilingual compositional model (Hermann and Blunsom, 2014) and bilingual Skip-gram (Luong et al., 2015), to perform code-mixed sentiment analysis and Part-of-Speech tagging. In addition, they also train skip gram embeddings on synthetic codemixed text. Their results show that the applied bilingual embeddings do not perform well, and that multilingual embeddings might be a better solution to process code-mixed text. This is mainly due to the fact that code-mixed text contains part"
2020.calcs-1.6,bakliwal-etal-2012-hindi,0,0.0250971,"ositive, negative, or neutral. Besides the sentiment labels, the organisers also provide the language labels at the word level, consisting of the following tags: en (English), hi (Hindi), mixed and univ (e.g., symbols, @ mentions, hashtags). Table 1 shows some examples of the Hinglish code-mixed data, whereas Table 2 lists the statistics of the data set used for the sentiment analysis experiments. Seminal work in sentiment analysis (SA) of Hindi text was done by Joshi et al. (Joshi et al., 2010), who built a system containing a classification, machine translation and sentiment lexicon module. Bakliwal et al. (2012) created a sentiment lexicon for Hindi, and Das and Bandyophadhyay (2010) created the Hindi SentiWordNet. Joshi et al. (2016) introduce a Hindi-English code-mixed dataset for sentiment analysis and propose a system to SA that learns sub-word level representations in LSTM (Long Short-Term Memory) (Subword-LSTM) instead of character- or word-level representations. As mentioned before, the data set contains a mixture of English and romanized or transliterated Hindi. This produces an additional challenge, as this romanized codemixed data contains non-standard spellings like aapke and apke (“your”)"
2020.calcs-1.6,W17-0224,0,0.0286736,"different methods to train our sentiment analysis system for Hinglish code-mixed tweets and compared them with monolingual baseline systems, resulting in the following three experimental setups: Cross-lingual embeddings rely on the inherent similarities in language structure and composition to project multiple monolingual embeddings into the same space, enabling tasks which require knowledge of more than one language (Conneau et al., 2018). This kind of embeddings have been used to solve a variety of tasks like wordto-word translation (Chen and Cardie, 2018), evaluating ¨ sentence similarity (Bjerva and Ostling, 2017) and detecting cognates across languages (Labat and Lefever, 2019). Most methods to project two or more monolingual embeddings into a shared space require a parallel seed dictionary to initialize an alignment which can then be improved upon (Upadhyay et al., 2016). The latter approach is not feasible, though, in this particular setting, as we aim to align English words with code-mixed (transliterated) Hinglish words, which often have no standardised spelling, but on the contrary occur with many variations in social media data. In recent research, however, a number of methods have been explored"
2020.calcs-1.6,Q17-1010,0,0.0503221,"Challenge Dataset Language Labels English Words 27,594 Hindi Words 28,167 Universal Symbols 2,792 Twitter API in both English and transliterated Hindi. For English 141,566 tweets were scraped, while 252,183 tweets were scraped for Hindi. Hinglish tweets were obtained from the API by querying Hindi tweets and then filtering out tweets containing any Devanagari characters. We were left with 138,589 tweets for Hinglish after removing these ‘Devanagari’ tweets. Subsequently, monolingual embeddings were trained for both of the above mentioned corpora with a continuous bag-of-words FastText model (Bojanowski et al., 2017), and used to train a bi-directional LSTM (as explained above). Sentiment Labels Positive Tweets 5,034 Negative Tweets 4,459 Neutral Tweets 5,683 Table 2: Overview of the statistics of the data set used to perform Hinglish code-mixed sentiment analysis. 4.2. mixed data. Since the objective is to demonstrate the viability of cross-lingual embeddings over the simpler, monolingual embeddings, the experimental protocol dictates that the same classifier must be used to evaluate the systems. For the purpose of classification, we opted to use a BiLSTM encoder followed by a Softmax layer. Pre-trained"
2020.calcs-1.6,D18-1024,0,0.0198237,"ion for hyper-parameter optimization. We investigated two different methods to train our sentiment analysis system for Hinglish code-mixed tweets and compared them with monolingual baseline systems, resulting in the following three experimental setups: Cross-lingual embeddings rely on the inherent similarities in language structure and composition to project multiple monolingual embeddings into the same space, enabling tasks which require knowledge of more than one language (Conneau et al., 2018). This kind of embeddings have been used to solve a variety of tasks like wordto-word translation (Chen and Cardie, 2018), evaluating ¨ sentence similarity (Bjerva and Ostling, 2017) and detecting cognates across languages (Labat and Lefever, 2019). Most methods to project two or more monolingual embeddings into a shared space require a parallel seed dictionary to initialize an alignment which can then be improved upon (Upadhyay et al., 2016). The latter approach is not feasible, though, in this particular setting, as we aim to align English words with code-mixed (transliterated) Hinglish words, which often have no standardised spelling, but on the contrary occur with many variations in social media data. In rec"
2020.calcs-1.6,Q18-1039,0,0.0281008,"timent analysis and Part-of-Speech tagging. In addition, they also train skip gram embeddings on synthetic codemixed text. Their results show that the applied bilingual embeddings do not perform well, and that multilingual embeddings might be a better solution to process code-mixed text. This is mainly due to the fact that code-mixed text contains particular semantic and syntactic structures that do not occur in the respective monolingual corpora. does so using translation systems (Zhou et al., 2016) or cross-lingual signals in another form, such as parallel corpora or bilingual dictionaries (Chen et al., 2018). However, since we work with code-mixed (transliterated) Hinglish Twitter data, there are no available resources like parallel corpora or bilingual dictionaries. Moreover, the ever evolving nature of social media text and various spelling alternatives in code-mixed data would make data greedy approaches like parallel corpora redundant. In the proposed research, we thus build upon the recent research in constructing unsupervised cross-lingual embeddings by exploiting the inherent spacial structural similarity of word embeddings. Mulitple approaches use adversarial learning to learn these mappi"
2020.calcs-1.6,D18-1269,0,0.0262736,"and evaluated with 5-fold cross-validation, and an internal 5fold cross-validation was performed on the training partition for hyper-parameter optimization. We investigated two different methods to train our sentiment analysis system for Hinglish code-mixed tweets and compared them with monolingual baseline systems, resulting in the following three experimental setups: Cross-lingual embeddings rely on the inherent similarities in language structure and composition to project multiple monolingual embeddings into the same space, enabling tasks which require knowledge of more than one language (Conneau et al., 2018). This kind of embeddings have been used to solve a variety of tasks like wordto-word translation (Chen and Cardie, 2018), evaluating ¨ sentence similarity (Bjerva and Ostling, 2017) and detecting cognates across languages (Labat and Lefever, 2019). Most methods to project two or more monolingual embeddings into a shared space require a parallel seed dictionary to initialize an alignment which can then be improved upon (Upadhyay et al., 2016). The latter approach is not feasible, though, in this particular setting, as we aim to align English words with code-mixed (transliterated) Hinglish word"
2020.calcs-1.6,W10-3208,0,0.0457655,"Missing"
2020.calcs-1.6,2020.semeval-1.163,0,0.0783773,"Missing"
2020.calcs-1.6,E14-1049,0,0.0450751,"ion, normalization and part-of-speech tagging on this corpus. They identify normalisation and transliteration as very challenging problems for Hinglish. Similar work has been carried out by Sharma et al. (2016), who developed a shallow parser for Hindi-English code-mixed social media text. Rijhwani et al. (2017) introduce an unsupervised wordlevel language detection technique (using a Hidden Markov Model) for code-switched text on Twitter that can be applied to different languages. Pratapa et al. (2018) compare three bilingual word embedding approaches, bilingual correlation based embeddings (Faruqui and Dyer, 2014), bilingual compositional model (Hermann and Blunsom, 2014) and bilingual Skip-gram (Luong et al., 2015), to perform code-mixed sentiment analysis and Part-of-Speech tagging. In addition, they also train skip gram embeddings on synthetic codemixed text. Their results show that the applied bilingual embeddings do not perform well, and that multilingual embeddings might be a better solution to process code-mixed text. This is mainly due to the fact that code-mixed text contains particular semantic and syntactic structures that do not occur in the respective monolingual corpora. does so using tra"
2020.calcs-1.6,N16-1159,0,0.0135371,"ing in English-Spanish (Solorio and Liu, 2008a; Solorio and Liu, 2008b) and TurkishDutch (Nguyen and Seza Dogruoz, 2013) text corpora. More recently, research has been performed to study codeswitching on social media from a computational angle. Vyas et al. (2014) have compiled an annotated corpus for Hindi-English from Facebook forums, and performed experiments for language identification, back-transliteration, normalization and part-of-speech tagging on this corpus. They identify normalisation and transliteration as very challenging problems for Hinglish. Similar work has been carried out by Sharma et al. (2016), who developed a shallow parser for Hindi-English code-mixed social media text. Rijhwani et al. (2017) introduce an unsupervised wordlevel language detection technique (using a Hidden Markov Model) for code-switched text on Twitter that can be applied to different languages. Pratapa et al. (2018) compare three bilingual word embedding approaches, bilingual correlation based embeddings (Faruqui and Dyer, 2014), bilingual compositional model (Hermann and Blunsom, 2014) and bilingual Skip-gram (Luong et al., 2015), to perform code-mixed sentiment analysis and Part-of-Speech tagging. In addition,"
2020.calcs-1.6,D08-1102,0,0.0500413,"the data set used to train and evaluate the system. Section 4. describes our approach to sentiment analysis for code-mixed Hinglish data. In section 5., we report on the results and provide an analysis of the performance, while Section 6. concludes this paper and gives directions for future research. 2. Related Research Related research on computational models for code-mixing is scarce because of the rarity of the phenomenon in conventional text corpora, which makes it hard to apply data-greedy approaches. Previous research, however, has 45 tried to predict code-switching in English-Spanish (Solorio and Liu, 2008a; Solorio and Liu, 2008b) and TurkishDutch (Nguyen and Seza Dogruoz, 2013) text corpora. More recently, research has been performed to study codeswitching on social media from a computational angle. Vyas et al. (2014) have compiled an annotated corpus for Hindi-English from Facebook forums, and performed experiments for language identification, back-transliteration, normalization and part-of-speech tagging on this corpus. They identify normalisation and transliteration as very challenging problems for Hinglish. Similar work has been carried out by Sharma et al. (2016), who developed a shallow"
2020.calcs-1.6,D08-1110,0,0.0439265,"the data set used to train and evaluate the system. Section 4. describes our approach to sentiment analysis for code-mixed Hinglish data. In section 5., we report on the results and provide an analysis of the performance, while Section 6. concludes this paper and gives directions for future research. 2. Related Research Related research on computational models for code-mixing is scarce because of the rarity of the phenomenon in conventional text corpora, which makes it hard to apply data-greedy approaches. Previous research, however, has 45 tried to predict code-switching in English-Spanish (Solorio and Liu, 2008a; Solorio and Liu, 2008b) and TurkishDutch (Nguyen and Seza Dogruoz, 2013) text corpora. More recently, research has been performed to study codeswitching on social media from a computational angle. Vyas et al. (2014) have compiled an annotated corpus for Hindi-English from Facebook forums, and performed experiments for language identification, back-transliteration, normalization and part-of-speech tagging on this corpus. They identify normalisation and transliteration as very challenging problems for Hinglish. Similar work has been carried out by Sharma et al. (2016), who developed a shallow"
2020.calcs-1.6,P16-1157,0,0.0573582,"Missing"
2020.calcs-1.6,D14-1105,0,0.0193717,", while Section 6. concludes this paper and gives directions for future research. 2. Related Research Related research on computational models for code-mixing is scarce because of the rarity of the phenomenon in conventional text corpora, which makes it hard to apply data-greedy approaches. Previous research, however, has 45 tried to predict code-switching in English-Spanish (Solorio and Liu, 2008a; Solorio and Liu, 2008b) and TurkishDutch (Nguyen and Seza Dogruoz, 2013) text corpora. More recently, research has been performed to study codeswitching on social media from a computational angle. Vyas et al. (2014) have compiled an annotated corpus for Hindi-English from Facebook forums, and performed experiments for language identification, back-transliteration, normalization and part-of-speech tagging on this corpus. They identify normalisation and transliteration as very challenging problems for Hinglish. Similar work has been carried out by Sharma et al. (2016), who developed a shallow parser for Hindi-English code-mixed social media text. Rijhwani et al. (2017) introduce an unsupervised wordlevel language detection technique (using a Hidden Markov Model) for code-switched text on Twitter that can b"
2020.calcs-1.6,D17-1207,0,0.0254387,"terated) Hinglish Twitter data, there are no available resources like parallel corpora or bilingual dictionaries. Moreover, the ever evolving nature of social media text and various spelling alternatives in code-mixed data would make data greedy approaches like parallel corpora redundant. In the proposed research, we thus build upon the recent research in constructing unsupervised cross-lingual embeddings by exploiting the inherent spacial structural similarity of word embeddings. Mulitple approaches use adversarial learning to learn these mappings with different ideas for optimization. While Zhang et al. (2017) choose to use Earth Mover’s Distance as a similarity metric between two embedding spaces, Conneau et al. (2017) opt for the Procrustes solution to refine the mappings. In our experiments, we compare the results obtained when applying (1) the approach of Artexte et al. (2018), which uses Singular Value Decomposition and synthetic bilingual dictionary induction using similarity distributions, and (2) the approach of Conneau et al. (2017). We demonstrate that aligning codemixed social media text with an anchor language like English helps to increase the performance in both a supervised and trans"
2020.calcs-1.6,P16-1133,0,0.0206283,"itional model (Hermann and Blunsom, 2014) and bilingual Skip-gram (Luong et al., 2015), to perform code-mixed sentiment analysis and Part-of-Speech tagging. In addition, they also train skip gram embeddings on synthetic codemixed text. Their results show that the applied bilingual embeddings do not perform well, and that multilingual embeddings might be a better solution to process code-mixed text. This is mainly due to the fact that code-mixed text contains particular semantic and syntactic structures that do not occur in the respective monolingual corpora. does so using translation systems (Zhou et al., 2016) or cross-lingual signals in another form, such as parallel corpora or bilingual dictionaries (Chen et al., 2018). However, since we work with code-mixed (transliterated) Hinglish Twitter data, there are no available resources like parallel corpora or bilingual dictionaries. Moreover, the ever evolving nature of social media text and various spelling alternatives in code-mixed data would make data greedy approaches like parallel corpora redundant. In the proposed research, we thus build upon the recent research in constructing unsupervised cross-lingual embeddings by exploiting the inherent sp"
2020.computerm-1.12,W18-4909,0,0.0308939,"Missing"
2020.computerm-1.12,N18-2052,0,0.0520181,"Missing"
2020.computerm-1.12,2020.computerm-1.13,0,0.0350263,"tries, if they are present and annotated in the corpus). We do not discount the importance of ATE systems that handle term variation, but a choice was made to focus on the core task for the first edition of the task. gold standard with both terms and Named Entities. These double scores did not influence the final ranking based on f1-scores. The dataset has been used for more detailed evaluations as well (see section 4.3) and participants were encouraged to report scores on the training domains in their own papers as well. 4.2. Participants Five teams participated in the shared task: TALNLS2N (Hazem et al., 2020), RACAI (Pais and Ion, 2020), e-Terminology (Oliver and V`azquez, 2020), NLPLab UQAM (no system description paper), and NYU (no system description paper but based on previous work in Meyers et al. (2018)). NYU and RACAI participated only in the English track, TALN-LS2N participated in both the English and French tracks, and e-Terminology and NLPLab UQAM participated in all tracks. We refer to their own system description papers for more details, but will There are three different tracks (one per language) and participants could enter in one or multiple tracks. When participants submitted their"
2020.computerm-1.12,C14-1029,0,0.0567379,"Missing"
2020.computerm-1.12,W16-4702,0,0.237528,"Missing"
2020.computerm-1.12,loukachevitch-2012-automatic,0,0.405981,"an ambiguous There are many more ways in which ATE systems can vary. Some can already be deduced from the ways in which the datasets are annotated, such as support for nested terms. Another very fundamental difference is the frequency cutoff: many ATE systems only extract terms which appear above a certain frequency threshold in the corpora. This threshold is extremely variable, with some systems that do not have any threshold, others that only extract candidate terms which appear 15 times or more (Pollak et al., 2019), and still others where only the top-n most frequent terms are extracted (Loukachevitch, 2012). Term length is similarly variable, with systems that don’t place any restrictions, others that extract only single-word terms, only multi-word terms, or those that extract all terms between 1 and n tokens (with n ranging from 2 to 15), where n is sometimes determined by the restrictions of a system, sometimes experimentally set to an optimal value, and at other times directly determined by the maximum term length in a gold standard. There are many other possible differences, such as POS patterns, which will not be discussed in any detail here. More information regarding both datasets for ATE"
2020.computerm-1.12,R15-1062,0,0.343163,"Missing"
2020.computerm-1.12,2020.computerm-1.15,0,0.0606562,"Missing"
2020.computerm-1.12,2020.computerm-1.14,0,0.0372712,"nd annotated in the corpus). We do not discount the importance of ATE systems that handle term variation, but a choice was made to focus on the core task for the first edition of the task. gold standard with both terms and Named Entities. These double scores did not influence the final ranking based on f1-scores. The dataset has been used for more detailed evaluations as well (see section 4.3) and participants were encouraged to report scores on the training domains in their own papers as well. 4.2. Participants Five teams participated in the shared task: TALNLS2N (Hazem et al., 2020), RACAI (Pais and Ion, 2020), e-Terminology (Oliver and V`azquez, 2020), NLPLab UQAM (no system description paper), and NYU (no system description paper but based on previous work in Meyers et al. (2018)). NYU and RACAI participated only in the English track, TALN-LS2N participated in both the English and French tracks, and e-Terminology and NLPLab UQAM participated in all tracks. We refer to their own system description papers for more details, but will There are three different tracks (one per language) and participants could enter in one or multiple tracks. When participants submitted their final results on the test d"
2020.computerm-1.12,W14-6001,0,0.150785,"unsupervised systems, as well as the distinction between sequence labelling approaches and systems that start from a limited list of unique term candidates. Splitting systems by their features is perhaps even more difficult, since research has moved far beyond using simple linguistic and statistical features. Some examples include ˇ the use of topic modelling (Sajatovi´ c et al., 2019; Bolshakova et al., 2013), queries on search engines, Wikipedia, or other external resources (Kessler et al., 2019; Vivaldi and Rodr´ıguez, 2001), and word embeddings (Amjadian et al., 2016; Kucza et al., 2018; Qasemizadeh and Handschuh, 2014; Pollak et al., 2019). Some methods are even called “featureless” (Gao and Yuan, 2019; Wang et al., 2016). ACTER Annotated Corpora for Term Extraction Research ACTER is a collection of domain-specific corpora in which terms have been manually annotated. It covers three languages (English, French, and Dutch) and four domains (corruption, dressage (equitation), heart failure, and wind energy). It has been created in light of some of the perceived difficulties that have been mentioned. A previous version (which did not yet bear the ACTER acronym) has already been elaborately described (Rigouts T"
2020.computerm-1.12,L16-1294,0,0.0310143,"the manual annotation of ATE results, rather than manual annotations in the unprocessed text. A final remark is that some corpora have been annotated with multiple term labels or have even been annotated according to large taxonomies, while others don’t make any distinctions beyond terms. As will be discussed in more detail in section 3, the ACTER dataset has been specifically designed to deal with some of the issues addressed here. Related Research Manually Annotated Gold Standards for ATE Two of the most commonly used annotated datasets are GENIA (Kim et al., 2003), and the ACL RD-TEC 2.0 (Qasemizadeh and Schumann, 2016), both of which are in English. GENIA is a collection of 2000 abstracts from the MEDLINE database in the domain of bio-medicine, specifically “transcription factors in human blood cells”. Over 400k tokens were annotated by two domain experts to obtain 93,293 term annotations. The ACL-RD-TEC 2.0 contains 300 annotated abstracts from the ACL Anthology Reference Corpus. Again, two experts performed the annotation of 33k tokens, which resulted in 6818 term annotations. They claim three main advantages over GENIA: first, the domain (computational linguistics) means that ATE researchers will have a"
2020.computerm-1.12,C10-3015,0,0.0970024,"Missing"
2020.computerm-1.12,R19-1117,1,0.72686,"er two (Macken et al., 2013). These methods typically select candidate terms based on their POS-pattern and rank these candidate terms using the statistical metrics, thus combining the advantages of both techniques. A particular difficulty is defining the cut-off threshold for the term candidates, which can be defined as the top-n terms, the top-n percentage of terms, or all terms above a certain threshold score. Manually predicting the ideal cut-off point is extremely difficult and can result in a skew towards either precision or recall, which can be detrimental to the final f1score (Rigouts Terryn et al., 2019a). While this typology of linguistic, statistical, and hybrid systems is sometimes still used today, in recent years, the advance of machine learning techniques has made such a simple classification of ATE methodologies more complicated (Gao and Yuan, 2019). Methodologies have become so diverse that they are no longer easily captured in such a limited number of clearly delineated categories. For instance, apart from the distinction between statistical and linguistic systems, one could also distinguish between rulebased methods and machine learning methods. However, rather than a simple binary"
2020.computerm-1.12,W11-1816,0,0.0914129,"Missing"
2020.computerm-1.12,U16-1011,0,0.0306596,"e statistical score (Drouin, 2003; Kosa et al., 2020), systems that combine a limited number of features with a voting algorithm (Fedorenko et al., 2013; Vivaldi and Rodr´ıguez, 2001), an evolutionary algorithm that optimises the ROC-curve (Az´e et al., 2005), rule-induction (Foo and Merkel, 2010), supportvector models (Ramisch et al., 2010), logistic regression (Bolshakova et al., 2013; Judea et al., 2014), basic neural networks (H¨atty and Schulte im Walde, 2018a), recursive neural networks (Kucza et al., 2018), siamese neural networks (Shah et al., 2019), and convolutional neural networks (Wang et al., 2016). Within the machine learn86 3. ing systems, there are vast differences between supervised, semi-supervised, and unsupervised systems, as well as the distinction between sequence labelling approaches and systems that start from a limited list of unique term candidates. Splitting systems by their features is perhaps even more difficult, since research has moved far beyond using simple linguistic and statistical features. Some examples include ˇ the use of topic modelling (Sajatovi´ c et al., 2019; Bolshakova et al., 2013), queries on search engines, Wikipedia, or other external resources (Kessl"
2020.computerm-1.12,W19-5118,0,0.036623,"Missing"
2020.lrec-1.504,P18-1073,0,0.0223699,"languages, the independently trained monolingual word embeddings have to be aligned in a common vector space. The development of cross-lingual mappings for monolingual word embeddings has been an 4098 active research area in recent times. While initially, linear mapping method were proposed (see for instance Mikolov et al. (2013)), a lot of different ideas have been explored recently, such as minimization of Earth Mover’s Distance (Zhang et al., 2017) or using the Wasserstein GAN as a means to minimize Sinkhorn distance (Xu et al., 2018). For our experiments, we used the approach proposed by Artetxe et al. (2018), because of its state-of-the-art results on downstream tasks such as word-for-word translation, which starts from the assumption that translations will have similar neighbors in the embedding space. This principle is used to define an initial parallel dictionary which is then iteratively corrected. The iterations involve a novel selflearning approach, which computes the optimal orthogonal mapping for the current dictionaries by means of Singular Value Decomposition (SVD). Subsequently, the dictionaries are improved with a modified version of the nearest neighbor algorithm. The first bilingual"
2020.lrec-1.504,Q17-1010,0,0.0449147,"ta set. 3.2.2. Semantic Information In addition to features modeling formal similarity between the source and target words, we also incorporated semantic information in our classifier. To this end, cross-lingual word embeddings were used, since these have been proven to work well for the cognate detection task in our pilot study on English-Dutch word pairs (Labat and Lefever, 2019). The former approach was improved in the following way. Firstly, standard fastText word embeddings, which were pretrained on Common Crawl and Wikipedia and generated with the standard skip-gram model as proposed by Bojanowski et al. (2017), were extended with domain-specific word embeddings. This was accomplished by incrementally re-training the fastText embeddings with additional sentences from the Dutch Parallel Corpus to accommodate for new, unseen words (Grave et al., 2018). These words are mainly domain-specific and, consequently, absent from the Common Crawl and Wikipedia data. Incremental training of word embeddings is fairly common and has been explored in the past for a variety of models and domains (Kaji and Kobayashi, 2017). Furthermore, in order to calculate similarities between words in the two different languages,"
2020.lrec-1.504,P14-2017,0,0.048386,"Missing"
2020.lrec-1.504,2007.jeptalnrecital-long.8,0,0.463139,"gual word embeddings. Keywords: cognate detection, multi-layer perceptron, orthographic similarity, cross-lingual word embeddings 1. Introduction Cross-lingual similarity of word pairs from different languages concerns both formal and semantic overlap. Whereas the former refers to orthographic and/or phonetic similarity, the latter refers to translation equivalents in different languages (Schepens et al., 2013). Cognates are then defined as word pairs in different languages that have a similar form and meaning, which is often the result of a shared linguistic origin in some ancestor language (Frunza and Inkpen, 2007). Furthermore, true cognates can be distinguished from false friends, which have a similar form but different meaning, and from partial cognates, which share the same meaning for some, but not all contexts. For example, the English-Dutch word pair father – vader is a cognate pair, as both words in the pair have a similar form and meaning. In contrast, the French-Dutch gras – gras and the English-Dutch driving – drijvende are, respectively, instances of false friends and partial cognates. In the first word pair, the French gras means “fat, greasy”, while the Dutch gras stands for “grass”. In th"
2020.lrec-1.504,L18-1550,0,0.0266914,"se have been proven to work well for the cognate detection task in our pilot study on English-Dutch word pairs (Labat and Lefever, 2019). The former approach was improved in the following way. Firstly, standard fastText word embeddings, which were pretrained on Common Crawl and Wikipedia and generated with the standard skip-gram model as proposed by Bojanowski et al. (2017), were extended with domain-specific word embeddings. This was accomplished by incrementally re-training the fastText embeddings with additional sentences from the Dutch Parallel Corpus to accommodate for new, unseen words (Grave et al., 2018). These words are mainly domain-specific and, consequently, absent from the Common Crawl and Wikipedia data. Incremental training of word embeddings is fairly common and has been explored in the past for a variety of models and domains (Kaji and Kobayashi, 2017). Furthermore, in order to calculate similarities between words in the two different languages, the independently trained monolingual word embeddings have to be aligned in a common vector space. The development of cross-lingual mappings for monolingual word embeddings has been an 4098 active research area in recent times. While initiall"
2020.lrec-1.504,D17-1037,0,0.0200625,"ed on Common Crawl and Wikipedia and generated with the standard skip-gram model as proposed by Bojanowski et al. (2017), were extended with domain-specific word embeddings. This was accomplished by incrementally re-training the fastText embeddings with additional sentences from the Dutch Parallel Corpus to accommodate for new, unseen words (Grave et al., 2018). These words are mainly domain-specific and, consequently, absent from the Common Crawl and Wikipedia data. Incremental training of word embeddings is fairly common and has been explored in the past for a variety of models and domains (Kaji and Kobayashi, 2017). Furthermore, in order to calculate similarities between words in the two different languages, the independently trained monolingual word embeddings have to be aligned in a common vector space. The development of cross-lingual mappings for monolingual word embeddings has been an 4098 active research area in recent times. While initially, linear mapping method were proposed (see for instance Mikolov et al. (2013)), a lot of different ideas have been explored recently, such as minimization of Earth Mover’s Distance (Zhang et al., 2017) or using the Wasserstein GAN as a means to minimize Sinkhor"
2020.lrec-1.504,N03-2016,0,0.0864772,"(native English speakers). In comparative linguistics, pairs of cognates can be employed to study language relatedness (Ng et al., 2010) or phylogenetic inference (Atkinson et al., 2010; Rama et al., 2018), whereas in translation studies, cognates and false friends contribute to the notorious problem of source language interference for translators (Mitkov et al., 2007). In NLP, finally, cognate information has been incorporated for various tasks, such as cross-lingual information retrieval (Makin et al., 2007), lexicon induction (Mann and Yarowsky, 2001; Sharoff, 2018) or machine translation (Kondrak et al., 2003; Jha et al., 2018). The remainder of this paper is organized as follows. In Section 2., we present the existing approaches to cognate detection, which can be divided in three different strands: orthographic, phonetic and semantic methods. Section 3. gives a detailed overview of the created data set and the corresponding information sources, viz. orthographic and semantic similarity features, that are used for the classification experiments. In section 4., we report and analyze our experimental results, while Section 5. concludes this paper and gives some directions for future research. 2. Rel"
2020.lrec-1.504,A00-2038,0,0.20638,"Missing"
2020.lrec-1.504,R19-1071,1,0.90538,"e taxonomic structure. In addition, semantic similarity can also be computed by means of distributional information on the words. In this case, the intuition is that semantic similarity can be modelled via word co-occurrences in corpora, as words appearing in similar contexts tend to share similar meanings (Harris, 1954). Once the co-occurrence data is collected, the results are mapped to a vector for each word, and semantic similarity between words is then operationalized by measuring the distance (e.g. cosine distance) between their vectors. In the proposed research, we build on the work of Labat and Lefever (2019) in which preliminary experiments were performed for English-Dutch cognate detection. Their pilot study showed promising results for a classifier combining orthographic similarity information with pretrained fastText word embeddings. In this research, we extend this work by (1) manually creating and annotating a gold standard for French-Dutch pairs of cognates, by (2) extending the word embeddings approach with domain- or corpusspecific information, and by (3) using more advanced methods to project the monolingual embeddings in a common cross-lingual vector space. 3. Cognate Detection System W"
2020.lrec-1.504,W12-0216,0,0.0485195,"Missing"
2020.lrec-1.504,N01-1020,0,0.176259,"ench texts, in order to help second language learners of French (native English speakers). In comparative linguistics, pairs of cognates can be employed to study language relatedness (Ng et al., 2010) or phylogenetic inference (Atkinson et al., 2010; Rama et al., 2018), whereas in translation studies, cognates and false friends contribute to the notorious problem of source language interference for translators (Mitkov et al., 2007). In NLP, finally, cognate information has been incorporated for various tasks, such as cross-lingual information retrieval (Makin et al., 2007), lexicon induction (Mann and Yarowsky, 2001; Sharoff, 2018) or machine translation (Kondrak et al., 2003; Jha et al., 2018). The remainder of this paper is organized as follows. In Section 2., we present the existing approaches to cognate detection, which can be divided in three different strands: orthographic, phonetic and semantic methods. Section 3. gives a detailed overview of the created data set and the corresponding information sources, viz. orthographic and semantic similarity features, that are used for the classification experiments. In section 4., we report and analyze our experimental results, while Section 5. concludes thi"
2020.lrec-1.504,J99-1003,0,0.0898758,"Missing"
2020.lrec-1.504,J03-1002,0,0.0133933,"1. Data To train and evaluate the cognate detection system, we created a context-independent gold standard by manually labelling English-Dutch and French-Dutch pairs of cognates, partial cognates and false friends in bilingual term lists. In this section, we describe how lists of candidate cognate pairs were compiled on the basis of the Dutch Parallel Corpus (Macken et al., 2011) and how a manual annotation was performed to create a gold standard for English-Dutch and French-Dutch cognate pairs. To select a list of candidate cognate pairs, unsupervised statistical word alignment using GIZA++ (Och and Ney, 2003) was applied on the Dutch Parallel Corpus (DPC). This parallel corpus for Dutch, French and English consists of more than ten million words and is sentence-aligned. It contains five different text types and is balanced with respect to text type and translation direction. The automatic word alignment on the English-Dutch part of the DPC resulted in a list containing more than 500,000 translation 4097 equivalents. A first selection was performed by applying the Normalized Levenshtein Distance (NLD) (as implemented by Gries (2004)) on this list of translation equivalents and only considering equi"
2020.lrec-1.504,N18-2063,0,0.0399395,"Missing"
2020.lrec-1.504,L18-1135,0,0.0297062,"elp second language learners of French (native English speakers). In comparative linguistics, pairs of cognates can be employed to study language relatedness (Ng et al., 2010) or phylogenetic inference (Atkinson et al., 2010; Rama et al., 2018), whereas in translation studies, cognates and false friends contribute to the notorious problem of source language interference for translators (Mitkov et al., 2007). In NLP, finally, cognate information has been incorporated for various tasks, such as cross-lingual information retrieval (Makin et al., 2007), lexicon induction (Mann and Yarowsky, 2001; Sharoff, 2018) or machine translation (Kondrak et al., 2003; Jha et al., 2018). The remainder of this paper is organized as follows. In Section 2., we present the existing approaches to cognate detection, which can be divided in three different strands: orthographic, phonetic and semantic methods. Section 3. gives a detailed overview of the created data set and the corresponding information sources, viz. orthographic and semantic similarity features, that are used for the classification experiments. In section 4., we report and analyze our experimental results, while Section 5. concludes this paper and give"
2020.lrec-1.504,D18-1268,0,0.0262075,"e, in order to calculate similarities between words in the two different languages, the independently trained monolingual word embeddings have to be aligned in a common vector space. The development of cross-lingual mappings for monolingual word embeddings has been an 4098 active research area in recent times. While initially, linear mapping method were proposed (see for instance Mikolov et al. (2013)), a lot of different ideas have been explored recently, such as minimization of Earth Mover’s Distance (Zhang et al., 2017) or using the Wasserstein GAN as a means to minimize Sinkhorn distance (Xu et al., 2018). For our experiments, we used the approach proposed by Artetxe et al. (2018), because of its state-of-the-art results on downstream tasks such as word-for-word translation, which starts from the assumption that translations will have similar neighbors in the embedding space. This principle is used to define an initial parallel dictionary which is then iteratively corrected. The iterations involve a novel selflearning approach, which computes the optimal orthogonal mapping for the current dictionaries by means of Singular Value Decomposition (SVD). Subsequently, the dictionaries are improved w"
2020.lrec-1.504,D17-1207,0,0.0271418,"ored in the past for a variety of models and domains (Kaji and Kobayashi, 2017). Furthermore, in order to calculate similarities between words in the two different languages, the independently trained monolingual word embeddings have to be aligned in a common vector space. The development of cross-lingual mappings for monolingual word embeddings has been an 4098 active research area in recent times. While initially, linear mapping method were proposed (see for instance Mikolov et al. (2013)), a lot of different ideas have been explored recently, such as minimization of Earth Mover’s Distance (Zhang et al., 2017) or using the Wasserstein GAN as a means to minimize Sinkhorn distance (Xu et al., 2018). For our experiments, we used the approach proposed by Artetxe et al. (2018), because of its state-of-the-art results on downstream tasks such as word-for-word translation, which starts from the assumption that translations will have similar neighbors in the embedding space. This principle is used to define an initial parallel dictionary which is then iteratively corrected. The iterations involve a novel selflearning approach, which computes the optimal orthogonal mapping for the current dictionaries by me"
2020.semeval-1.135,Q17-1010,0,0.0426542,"est Common Subsequence Ratio (LCSR), Normalized Levenshtein Distance (NLD) and Jaro-Winkler Similarity. As for the semantic features, Hossain et al. (2019) noticed that the editors frequently opted for replacements that are semantically distant from their replaced counterparts. To capture this humor generation technique, we calculated the cosine similarity between the vector of the original token and its replacement, and between the original headline and its replacement. For this purpose, after removing numbers and punctuation marks and lower-casing, word vectors were extracted with fastText (Bojanowski et al., 2017). The vector for a given headline was calculated as the mean of its word vectors. Formula 1 illustrates how we modified cosine similarity to calculate the distance between two vectors A and B as a feature with a positive value to match the other features: 1+ cosine similarity(A, B) = A·B ||A||×||B|| 2 (1) In our modification of cosine similarity, two vectors (word or headline representations) can be exactly the opposite (0), exactly the same (1), or any value in-between. Our final group of features deals with perplexity. Intuitively, an utterance might be considered funny when there is a high"
2020.semeval-1.135,C18-1157,0,0.0172852,"uare error (RMSE) of 0.5253. 1 Introduction Research in the field of computational humor can be divided into three distinct tasks: detecting, assessing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval shared task on Assessing the Funninness of Edited News Headlines (Hossain et al., 2020), Hossain et al. (2019) introduce a novel dataset, named “Humicroedit”, that can be used for"
2020.semeval-1.135,N18-2018,0,0.0171477,"pre-trained language model RoBERTa to learn latent features in the news headlines that are useful to predict the funniness of each headline. The latter system was also our final submission to the competition and is ranked seventh among the 49 participating teams, with a root-mean-square error (RMSE) of 0.5253. 1 Introduction Research in the field of computational humor can be divided into three distinct tasks: detecting, assessing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI"
2020.semeval-1.135,W18-1111,0,0.0360497,"eadline or about the relationship between the original and edited headline. By creating features that model different types of humor generation techniques used by the editors, we want to learn how successful each technique is. In this section, we describe the different types of features that were created and motivate why these features would be useful for the task of humor detection according to existing humor theories. Secondly, we discuss the experimental setup and the machine learning model that was used to perform regression. 3.1.1 Feature engineering Some of our features were inspired by Chhaya et al. (2018), who introduced a supervised learning method to model frustration. Their feature set covers a wide range of features for emotion detection, which intuitively could be useful for our task as well. We implemented three groups of their features, leaving out 1034 those features that are either not relevant for this task or underrepresented in the dataset. The following features were used for the original headline: lexical features (number of uppercase words, number of non-alphanumeric characters, number of punctuation marks, average word length), NER-based features (presence of NER tags ‘person’,"
2020.semeval-1.135,N19-1423,0,0.10084,"e attention mechanism and, building on that, the transformer (Vaswani et al., 2017). Language models are typically trained with unsupervised training objectives which allows them to learn general language patterns from a given dataset. These patterns are encoded in the millions or even billions of parameters of the language model. Transfer learning allows us to transfer these parameters to other down-stream tasks, which is exactly what we did for this task. In our experiments we used RoBERTa (Liu et al., 2019), which is an improvement of the most wellknown currently used language model, BERT (Devlin et al., 2019). RoBERTa was trained on a dataset that is much larger than BERT. Whereas BERT was originally trained on 16 GB of data, RoBERTa utilises 160 GB of corpora. Most notable for our purposes is that a large part of that dataset (76 GB) is their newly created CC-news corpus, containing 63 million English news articles. Even though the current task involves news headlines, we would assume that during pretraining, RoBERTa at least learnt some hidden features that could relate to news-specific text. In our best performing model, we finetuned RoBERTa-base (125M parameters) using a custom head configurat"
2020.semeval-1.135,N19-1172,0,0.0203563,"sing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval shared task on Assessing the Funninness of Edited News Headlines (Hossain et al., 2020), Hossain et al. (2019) introduce a novel dataset, named “Humicroedit”, that can be used for all three tasks described above (see Section 2). The shared task is split into two sub-tasks, namely (i) a regression task to predict the mean"
2020.semeval-1.135,N19-1012,0,0.390217,"each headline. The latter system was also our final submission to the competition and is ranked seventh among the 49 participating teams, with a root-mean-square error (RMSE) of 0.5253. 1 Introduction Research in the field of computational humor can be divided into three distinct tasks: detecting, assessing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval shared task on Assessing"
2020.semeval-1.135,2020.semeval-1.98,0,0.087035,"insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval shared task on Assessing the Funninness of Edited News Headlines (Hossain et al., 2020), Hossain et al. (2019) introduce a novel dataset, named “Humicroedit”, that can be used for all three tasks described above (see Section 2). The shared task is split into two sub-tasks, namely (i) a regression task to predict the mean grade of all annotators for a given edited sentence, and (ii) a binary classification task to select the funnier of two edited headlines. We only participated in the regression sub-task. The remainder of this paper is structured as follows. In Section 2, we provide an overview of the dataset that was used for the competition. Section 3 introduces our two machine"
2020.semeval-1.135,L18-1102,0,0.0548846,"Missing"
2020.semeval-1.135,S17-2005,0,0.145151,"Missing"
2020.semeval-1.135,S17-2004,0,0.084207,"ict the funniness of each headline. The latter system was also our final submission to the competition and is ranked seventh among the 49 participating teams, with a root-mean-square error (RMSE) of 0.5253. 1 Introduction Research in the field of computational humor can be divided into three distinct tasks: detecting, assessing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval s"
2020.semeval-1.135,P18-2124,0,0.0238974,"eplacement Standard perplexity feature, pruned perplexity feature RMSE 0.576 0.579 0.578 0.578 0.577 0.575 0.575 0.576 Table 1: An overview of the different groups of features and their performance in the NuSVR algorithm (without hyperparameter tuning) when trained on train + dev data, and evaluated on test data. 3.2 Transformer-Based System Humor often expects world knowledge and awareness of the context to drive its intent home. Language models are capable of some language understanding (Wang et al., 2018, GLUE, and its successors), and on the popular SQUAD 2.0 question-answering benchmark (Rajpurkar et al., 2018) single (non-ensemble) language models even outperform human participants (Yang et al., 2019; Lan et al., 2019; Clark et al., 2020). The question is, however, if language models can use this information to figure out whether some given input is not only (un)true but also whether it is funny. Even though language models are nothing new, the last few years have seen great improvements in their architecture, most notably the attention mechanism and, building on that, the transformer (Vaswani et al., 2017). Language models are typically trained with unsupervised training objectives which allows th"
2020.semeval-1.135,W18-5446,0,0.0158529,"inkler Similarity Cosine sim. original token and its replacement, cosine sim. original headline and its replacement Standard perplexity feature, pruned perplexity feature RMSE 0.576 0.579 0.578 0.578 0.577 0.575 0.575 0.576 Table 1: An overview of the different groups of features and their performance in the NuSVR algorithm (without hyperparameter tuning) when trained on train + dev data, and evaluated on test data. 3.2 Transformer-Based System Humor often expects world knowledge and awareness of the context to drive its intent home. Language models are capable of some language understanding (Wang et al., 2018, GLUE, and its successors), and on the popular SQUAD 2.0 question-answering benchmark (Rajpurkar et al., 2018) single (non-ensemble) language models even outperform human participants (Yang et al., 2019; Lan et al., 2019; Clark et al., 2020). The question is, however, if language models can use this information to figure out whether some given input is not only (un)true but also whether it is funny. Even though language models are nothing new, the last few years have seen great improvements in their architecture, most notably the attention mechanism and, building on that, the transformer (Vas"
2020.semeval-1.135,D15-1284,0,0.0300313,"with a root-mean-square error (RMSE) of 0.5253. 1 Introduction Research in the field of computational humor can be divided into three distinct tasks: detecting, assessing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval shared task on Assessing the Funninness of Edited News Headlines (Hossain et al., 2020), Hossain et al. (2019) introduce a novel dataset, named “Humicroedit”,"
2020.semeval-1.153,N19-1423,0,0.0398569,"s: 1. Sentiment: Predicting the sentiment (Positive, Neutral or Negative) 2. Humour: Predicting the presence and degree of Humour (Not funny, Funny, Very funny, Hilarious) 3. Sarcasm: Predicting the presence and degree of Humour (Not sarcastic, General, Twisted, Very twisted) 4. Predicting the presence and degree of Offensive Content (Not offensive, Slightly offensive, Very offensive, Hateful) 5. Predicting the presence of Motivation (Not motivational, Motivational) Figure 1 shows the global architecture of our Multi-Modal Multi-Task System. First, the memes are encoded with the help of BERT (Devlin et al., 2019) and visual features oriented at understanding the contents of the image, other than the text (See Section 4). After obtaining these “memebedding” encodings, viz. the joint embeddings from text and image, this information is passed along to the 5 heads for classification for each sub-task. Cross-Entropy Loss is computed for each head independently and the weighted average of the combined loss is jointly optimized with the Adam optimizer. The following section describes the encoding and featurization in more detail. 1157 Figure 1: Architecture of the Multi-Modal Multi-Task Learning Setup 4 Expe"
2020.semeval-1.153,W18-5113,0,0.0262128,"ort, complex nature (Verma et al., 2020). Sentiment classification has been applied to image data for the purposes of automatic tag predictions for images uploaded on social media (Gajarla and Gupta, 2015), which in turn will help optimize image search algorithms by providing a large collection of tagged image data. The importance of joining together the insights gained in the study of the two modalities of memes becomes evident when we consider the complexity such multimodal communication modes add to tasks such as the automatic detection of offensive discourse online (Williams et al., 2016; Lee et al., 2018) when compared to offensive language detection in textual data (Zampieri et al., 2019). The SemEval Memotion task not only aims to contribute to the field of automatic detection of sentiment in internet memes, but also to the extraction of more fine-grained information such as sarcasm, humour and offensiveness. Distinguishing these three types of humour is not an easy task, since even the type of humour typically found in non-offensive memes, tends to skirt the boundaries of what is acceptable. It is in this often politically incorrect capacity that their cohesive potential lies (Proch´azka, 2"
2020.semeval-1.153,D16-1264,0,0.0378458,"ne-tuned on Sub-reddit data Table 1: Overview of the “Memebedding” features used for training 4.1 BERT Features for Text BERT, the Bidirectional Encoder Representations for Transformers (Devlin et al., 2019), leverages the Bidirectional Transformer for Masked Language Modelling. It demonstrates that a language model which is bidirectionally trained has a deeper, more salient understanding of language than the previous uni-directional language models. Fine-tuning BERT has been leveraged in many downstream tasks and is the state-of-the-art for 11 such tasks like GLUE (Wang et al., 2018), SQuAD (Rajpurkar et al., 2016) and MultiNLI (Williams et al., 2017). We use the standard base-uncased BERT pre-trained model available from the Hugging Face Transformers package1 , to encode the meme text into a 768-dimensional vector. 4.2 Visual Feature Extractor While BERT features capture essentially everything about the text of the meme in a broader sense, the visual features aim to add some context from the world of memes and information based on the image itself. To this end, Reddit features were obtained from a classifier trained on reddit memes. We collected around 3980 memes from Reddit, from 8 different subreddit"
2020.semeval-1.153,2020.semeval-1.99,0,0.0699054,"Missing"
2020.semeval-1.153,J18-4010,1,0.870991,"Missing"
2020.semeval-1.153,W18-5446,0,0.0296777,"rained BERT-base-uncased Fine-tuned on Sub-reddit data Table 1: Overview of the “Memebedding” features used for training 4.1 BERT Features for Text BERT, the Bidirectional Encoder Representations for Transformers (Devlin et al., 2019), leverages the Bidirectional Transformer for Masked Language Modelling. It demonstrates that a language model which is bidirectionally trained has a deeper, more salient understanding of language than the previous uni-directional language models. Fine-tuning BERT has been leveraged in many downstream tasks and is the state-of-the-art for 11 such tasks like GLUE (Wang et al., 2018), SQuAD (Rajpurkar et al., 2016) and MultiNLI (Williams et al., 2017). We use the standard base-uncased BERT pre-trained model available from the Hugging Face Transformers package1 , to encode the meme text into a 768-dimensional vector. 4.2 Visual Feature Extractor While BERT features capture essentially everything about the text of the meme in a broader sense, the visual features aim to add some context from the world of memes and information based on the image itself. To this end, Reddit features were obtained from a classifier trained on reddit memes. We collected around 3980 memes from Re"
2020.semeval-1.153,S19-2010,0,0.104762,"Missing"
2020.semeval-1.173,P17-1042,0,0.0313131,"lov et al. (2013) that vector spaces in different languages share a certain similarity. By creating monolingual spaces and then learning a projection from one language to another, there is no need for large parallel corpora. Mikolov et al. (2013) learn a linear mapping from one space to another and optimize the performance by using the most common words from both languages and by using a bilingual lexicon. As large bilingual lexicons are often not available, there was a need to either completely eliminate or drastically reduce the size of the required bilingual lexicon. To address this issue, Artetxe et al. (2017) propose a very simple self-learning approach that exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals. Research by Barnes et al. (2018) attempts to learn bilingual sentiment embeddings, which jointly train the projection and the sentiment component to represent sentiment information in the source and target language. Their method uses a bilingual lexicon, an annotated sentiment corpus in the source language and monolingual embeddings for the source and target language. T"
2020.semeval-1.173,P18-1073,0,0.0483414,"Missing"
2020.semeval-1.173,D08-1014,0,0.0612121,"celona, Spain (Online), December 12, 2020. 2 Related Research A first line of research for sentiment analysis applies supervised machine learning approaches (Joshi et al., 2010; Van Hee et al., 2017). These approaches, however, require large amounts of labeled data, which are often lacking for low(er)-resourced languages. Another important line of research uses machine translation systems to (1) map subjectivity lexicons to other languages (Mihalcea et al., 2007; Meng et al., 2012) or to (2) transfer sentiment information from a high-resource source language to a low-resource target language (Banea et al., 2008). Rasooli et al. (2018) use annotation projection to project supervised labels from the source languages to the target language and a direct transfer approach to develop SA systems. More recently, researchers have started to investigate cross-lingual embeddings for the task of SA. The idea of these embeddings stems from the idea of Mikolov et al. (2013) that vector spaces in different languages share a certain similarity. By creating monolingual spaces and then learning a projection from one language to another, there is no need for large parallel corpora. Mikolov et al. (2013) learn a linear"
2020.semeval-1.173,P18-1231,0,0.018571,"mapping from one space to another and optimize the performance by using the most common words from both languages and by using a bilingual lexicon. As large bilingual lexicons are often not available, there was a need to either completely eliminate or drastically reduce the size of the required bilingual lexicon. To address this issue, Artetxe et al. (2017) propose a very simple self-learning approach that exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals. Research by Barnes et al. (2018) attempts to learn bilingual sentiment embeddings, which jointly train the projection and the sentiment component to represent sentiment information in the source and target language. Their method uses a bilingual lexicon, an annotated sentiment corpus in the source language and monolingual embeddings for the source and target language. Their experimental results show the need for a dedicated high-quality sentiment lexicon in order to achieve a satisfactory performance. More recently, transformer-based approaches (Conneau et al., 2018) have been used for cross-lingual knowledge transfer. These"
2020.semeval-1.173,Q17-1010,0,0.0823707,"e have to find alternative ways to obtain supervision for understanding codemixed Hindi text and ideally combine the information with already available resources for English. We approached the task of analysing Hinglish code-mixed text from two different angles: 1. Hinglish as an independent third language, not inheriting from Hindi or English 2. Hinglish as an extension of English, with an extended vocabulary 4.1 Hinglish as an Independent Language (H-IND) For our first approach, we treat the scraped set of 138,589 Hinglish Tweets as a corpus of monolingual Hinglish data, and train FastText (Bojanowski et al., 2017) word embeddings for this corpus. We opted for FastText because it is fast, efficient and also accounts for sub-word information which could be crucial in this context. Contextualized word-embedding methods like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), although more advanced, are not ideal for this particular task as they typically require more information. To make the model more robust and perceptive to English words which were not present in our original Twitter corpus, we also incorporate pre-trained English FastText word embeddings trained on the vast Common Crawl Corpus2"
2020.semeval-1.173,N19-1423,0,0.038219,"wo different angles: 1. Hinglish as an independent third language, not inheriting from Hindi or English 2. Hinglish as an extension of English, with an extended vocabulary 4.1 Hinglish as an Independent Language (H-IND) For our first approach, we treat the scraped set of 138,589 Hinglish Tweets as a corpus of monolingual Hinglish data, and train FastText (Bojanowski et al., 2017) word embeddings for this corpus. We opted for FastText because it is fast, efficient and also accounts for sub-word information which could be crucial in this context. Contextualized word-embedding methods like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), although more advanced, are not ideal for this particular task as they typically require more information. To make the model more robust and perceptive to English words which were not present in our original Twitter corpus, we also incorporate pre-trained English FastText word embeddings trained on the vast Common Crawl Corpus2 . Since the two sets of embeddings are in separate n-dimensional spaces, they need to be projected in a shared space. For the projection, we resort to the methods presented by Artexte et al. (2018), using similarity distributions between"
2020.semeval-1.173,C16-1234,0,0.0289343,"sually trained on large monolingual corpora (e.g. English or Hindi), and not on mixed data. In addition, social media language is characterized by informal language use (abbreviations, spelling mistakes, flooding, emojis, etc.), which causes a considerable drop in performance for standard NLP approaches that are trained on standard data (Ritter et al., 2011). Related research on computational models for code-mixing is scarce because of the lack of large code-mixed resources, which makes it hard to apply data-greedy approaches. Seminal work in sentiment analysis (SA) of Hindi text was done by (Joshi et al., 2016), who introduce a Hindi-English code-mixed dataset for sentiment analysis and propose a system to SA that learns sub-word level representations in LSTM instead of character- or word-level representations. Pratapa et al. (2018) compare three bilingual word embedding approaches to perform code-mixed sentiment analysis and Part-of-Speech tagging. Their results show that the applied bilingual embeddings do not perform well, and that multilingual embeddings might be a better solution to process code-mixed text. This is mainly because code-mixed text contains particular semantic and syntactic struct"
2020.semeval-1.173,P12-1060,0,0.0345805,"reativecommons.org/licenses/by/4.0/. 1288 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1288–1293 Barcelona, Spain (Online), December 12, 2020. 2 Related Research A first line of research for sentiment analysis applies supervised machine learning approaches (Joshi et al., 2010; Van Hee et al., 2017). These approaches, however, require large amounts of labeled data, which are often lacking for low(er)-resourced languages. Another important line of research uses machine translation systems to (1) map subjectivity lexicons to other languages (Mihalcea et al., 2007; Meng et al., 2012) or to (2) transfer sentiment information from a high-resource source language to a low-resource target language (Banea et al., 2008). Rasooli et al. (2018) use annotation projection to project supervised labels from the source languages to the target language and a direct transfer approach to develop SA systems. More recently, researchers have started to investigate cross-lingual embeddings for the task of SA. The idea of these embeddings stems from the idea of Mikolov et al. (2013) that vector spaces in different languages share a certain similarity. By creating monolingual spaces and then l"
2020.semeval-1.173,P07-1123,0,0.0511738,"ense details: http:// creativecommons.org/licenses/by/4.0/. 1288 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1288–1293 Barcelona, Spain (Online), December 12, 2020. 2 Related Research A first line of research for sentiment analysis applies supervised machine learning approaches (Joshi et al., 2010; Van Hee et al., 2017). These approaches, however, require large amounts of labeled data, which are often lacking for low(er)-resourced languages. Another important line of research uses machine translation systems to (1) map subjectivity lexicons to other languages (Mihalcea et al., 2007; Meng et al., 2012) or to (2) transfer sentiment information from a high-resource source language to a low-resource target language (Banea et al., 2008). Rasooli et al. (2018) use annotation projection to project supervised labels from the source languages to the target language and a direct transfer approach to develop SA systems. More recently, researchers have started to investigate cross-lingual embeddings for the task of SA. The idea of these embeddings stems from the idea of Mikolov et al. (2013) that vector spaces in different languages share a certain similarity. By creating monolingu"
2020.semeval-1.173,2020.semeval-1.100,0,0.077831,"Missing"
2020.semeval-1.173,N18-1202,0,0.0421309,"h as an independent third language, not inheriting from Hindi or English 2. Hinglish as an extension of English, with an extended vocabulary 4.1 Hinglish as an Independent Language (H-IND) For our first approach, we treat the scraped set of 138,589 Hinglish Tweets as a corpus of monolingual Hinglish data, and train FastText (Bojanowski et al., 2017) word embeddings for this corpus. We opted for FastText because it is fast, efficient and also accounts for sub-word information which could be crucial in this context. Contextualized word-embedding methods like BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), although more advanced, are not ideal for this particular task as they typically require more information. To make the model more robust and perceptive to English words which were not present in our original Twitter corpus, we also incorporate pre-trained English FastText word embeddings trained on the vast Common Crawl Corpus2 . Since the two sets of embeddings are in separate n-dimensional spaces, they need to be projected in a shared space. For the projection, we resort to the methods presented by Artexte et al. (2018), using similarity distributions between the embeddings to create a sma"
2020.semeval-1.173,D18-1344,0,0.0162595,"c.), which causes a considerable drop in performance for standard NLP approaches that are trained on standard data (Ritter et al., 2011). Related research on computational models for code-mixing is scarce because of the lack of large code-mixed resources, which makes it hard to apply data-greedy approaches. Seminal work in sentiment analysis (SA) of Hindi text was done by (Joshi et al., 2016), who introduce a Hindi-English code-mixed dataset for sentiment analysis and propose a system to SA that learns sub-word level representations in LSTM instead of character- or word-level representations. Pratapa et al. (2018) compare three bilingual word embedding approaches to perform code-mixed sentiment analysis and Part-of-Speech tagging. Their results show that the applied bilingual embeddings do not perform well, and that multilingual embeddings might be a better solution to process code-mixed text. This is mainly because code-mixed text contains particular semantic and syntactic structures that do not occur in the respective monolingual corpora. Recently, there is a lot of attention for NLP approaches on code-mixed data, as illustrated by the “Fourth Workshop on Computational Approaches to Linguistic Code-s"
2020.semeval-1.173,D11-1141,0,0.0662585,"significant pretraining and a lot of low-resource languages are not accounted for in the pretrained models. Applying sentiment analysis to code-mixed social media data, however, offers a number of challenges for standard NLP approaches. These approaches are usually trained on large monolingual corpora (e.g. English or Hindi), and not on mixed data. In addition, social media language is characterized by informal language use (abbreviations, spelling mistakes, flooding, emojis, etc.), which causes a considerable drop in performance for standard NLP approaches that are trained on standard data (Ritter et al., 2011). Related research on computational models for code-mixing is scarce because of the lack of large code-mixed resources, which makes it hard to apply data-greedy approaches. Seminal work in sentiment analysis (SA) of Hindi text was done by (Joshi et al., 2016), who introduce a Hindi-English code-mixed dataset for sentiment analysis and propose a system to SA that learns sub-word level representations in LSTM instead of character- or word-level representations. Pratapa et al. (2018) compare three bilingual word embedding approaches to perform code-mixed sentiment analysis and Part-of-Speech tagg"
2021.latechclfl-1.15,N19-4010,0,0.0494809,"Missing"
2021.latechclfl-1.15,N19-1423,0,0.0391264,"and would be the first system to perform Part-ofSpeech tagging on Byzantine Greek with such high accuracy. Unfortunately, no such data is publicly available as of yet. taggers, each one trained on a different database, viz. the AGLDT and PROIEL treebanks. A shallow evaluation we performed for these taggers showed that RNNTagger outperformed both of them. 2.2 Language Models for Ancient Greek Recent state-of-the-art PoS-taggers (see for instance Heinzerling and Strube (2019)) often integrate information from a BERT language model. BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) is based on Transformers, a deep learning model where every output node is linked to every input node, whereas the weights between them are dynamically computed during training. As opposed to directional models, which read the input sentence from left-to-right (or right-to-left), the Transformer model is “bidirectional”, i.e. it reads the entire sentence at once. As a result, the model learns the representation of a word based on both its left and right context. Usually, such a BERT neural language model is pre-trained on a huge data set for a particular target language, and then subsequently"
2021.latechclfl-1.15,P19-1027,0,0.0173268,"t on separate output from Morpheus. Additionally, if RNNTagger would be trained on sufficient Byzantine Greek data, it is likely to perform similarly, and would be the first system to perform Part-ofSpeech tagging on Byzantine Greek with such high accuracy. Unfortunately, no such data is publicly available as of yet. taggers, each one trained on a different database, viz. the AGLDT and PROIEL treebanks. A shallow evaluation we performed for these taggers showed that RNNTagger outperformed both of them. 2.2 Language Models for Ancient Greek Recent state-of-the-art PoS-taggers (see for instance Heinzerling and Strube (2019)) often integrate information from a BERT language model. BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) is based on Transformers, a deep learning model where every output node is linked to every input node, whereas the weights between them are dynamically computed during training. As opposed to directional models, which read the input sentence from left-to-right (or right-to-left), the Transformer model is “bidirectional”, i.e. it reads the entire sentence at once. As a result, the model learns the representation of a word based on both its left and right"
2021.latechclfl-1.15,N16-1030,0,0.0458382,"Ancient and Byzantine Greek Ancient Greek corpus using MLM, performance into our morphological analysis system. was significantly increased and the perplexity 15 dropped to 9.8 on the held-out test set. From now https://github.com/pranaydeeps/Ancient-Greek-BERT 133 5.1 Training of the Morphological Analyser As explained in Section 3.1, we extracted all tokens and corresponding fine-grained PoS-tags from the three treebanks. We use the contextual token embeddings from the Expanded Ancient Greek BERT model described in Section 4.2, and stack them with randomly initialized Character Embeddings (Lample et al., 2016), which are then processed by a standard Bi-directional Long Short Term Memory (LSTM) encoder and a Conditional Random Field (CRF) decoder, commonly used in sequential tagging tasks. The Expanded Ancient Greek BERT embeddings are the only frozen part of the model, while all the other fragments are trained. We use the FLAIR framework (Akbik et al., 2019) for this set of experiments. While the Bi-LSTM CRF architecture is a staple of many successful sequential taggers, we elect to stack our BERT embeddings with character embeddings that have found extensive use in models for languages with high m"
2021.latechclfl-1.15,C73-2026,0,0.455793,"Missing"
2021.latechclfl-1.15,W17-4115,0,0.0188527,"ed by a standard Bi-directional Long Short Term Memory (LSTM) encoder and a Conditional Random Field (CRF) decoder, commonly used in sequential tagging tasks. The Expanded Ancient Greek BERT embeddings are the only frozen part of the model, while all the other fragments are trained. We use the FLAIR framework (Akbik et al., 2019) for this set of experiments. While the Bi-LSTM CRF architecture is a staple of many successful sequential taggers, we elect to stack our BERT embeddings with character embeddings that have found extensive use in models for languages with high morphological diversity (Vylomova et al., 2017). We use a hidden size of 256 for our LSTM, and initialize with a starting learning rate of 0.1, which is linearly decreased. 5.2 Evaluation of the Morphological Analyser sights gained during this pilot study. 5.2.1 Validation on the in-domain data The performance of our trained model obtains state-of-the-art results on the very fine-grained PoS scheme incorporating a full morphological analysis as applied in the treebanks. Table 2 shows the accuracy scores for the different treebank validation sets. These validation sets have been randomly selected from the treebanks data, and have not been u"
2021.semeval-1.145,2020.semeval-1.186,0,0.0968851,"Missing"
2021.semeval-1.145,N19-1423,0,0.0156323,"e the task at hand, our approach incorporates information from both domain-related text and visual pre-training, and finally combines the two modalities using Multi-modal Compact Bilinear (MCB) Pooling (Fukui et al., 2016). 2.2 Proposed Models Our multi-modal system is composed of three submodules: 1. The visual pre-processor: the visual module uses a ResNet-51 architecture (He et al., 2016) which is pre-trained to identify subreddits (E.g. /r/motivation, /r/pets /r/politics) from around 6200 Reddit memes. 2. The text pre-processor: the text module uses a pre-trained BERT-large-uncased model (Devlin et al., 2019), fine-tuned on the PTC Corpus (Martino et al., 2020a) from the SemEval 2020 Shared Task. 3. Integration Network: the two sets of embeddings from the first two modules are combined with MCB pooling. 2.2.1 Visual Embeddings We decided to use the Resnet-51 architecture for the Visual pre-processor. This model was trained to predict one of the 18 sub-reddits the memes were scraped from. We hypothesized that the learned embeddings are able to distinguish certain elements of the meme, since the model is forced to encode the sub-reddit it comes from, and the sub-reddits represent the genre (E.g. /r/"
2021.semeval-1.145,2021.semeval-1.7,0,0.154014,"pecific online communities, they have gained popularity very rapidly and are today used by a very large and varied user base. Memes can be used for very different purposes: they can be used as a form of visual rhetoric (Huntington, 2013), for online bullying or trolling (Leaver, 2013), but they can also function as a kind of persuasive device, while the intended message is wrapped in humour (Shifman, 2013). As a result, they form an interesting object of study for automatically detecting propaganda techniques. The goal of the SemEval-2021 shared task on the detection of persuasion techniques (Dimitrov et al., 2021) is to build models for identifying rhetorical and psychological techniques that are used to influence social media users in online disinformation campaigns. This paper reports on our participation in Subtask 3, which is a multi-modal task conceived as a multi-label classification problem: given a meme, the system has to identify which of the 22 techniques are used both in the textual and visual content of the meme. To solve subtask 3, we propose a multi-modal multi-task learning system, which incorporates “memebeddings”, viz. joint text and vision features combined by means of compact bilinea"
2021.semeval-1.145,D16-1044,0,0.0449095,"techniques without complete visual and textual context. Figure 1 shows examples of the two different cases, where in the first image, it is fairly obvious that the text contains all necessary information to predict the propaganda techniques accurately, whereas in the second meme, the textual modality is not sufficient to provide all information required to correctly predict the label. To tackle the task at hand, our approach incorporates information from both domain-related text and visual pre-training, and finally combines the two modalities using Multi-modal Compact Bilinear (MCB) Pooling (Fukui et al., 2016). 2.2 Proposed Models Our multi-modal system is composed of three submodules: 1. The visual pre-processor: the visual module uses a ResNet-51 architecture (He et al., 2016) which is pre-trained to identify subreddits (E.g. /r/motivation, /r/pets /r/politics) from around 6200 Reddit memes. 2. The text pre-processor: the text module uses a pre-trained BERT-large-uncased model (Devlin et al., 2019), fine-tuned on the PTC Corpus (Martino et al., 2020a) from the SemEval 2020 Shared Task. 3. Integration Network: the two sets of embeddings from the first two modules are combined with MCB pooling. 2.2"
2021.semeval-1.145,D16-1264,0,0.0377088,"represent the genre (E.g. /r/politics, /r/sports) or the emotion associated with the meme (E.g. /r/motivation, /r/dankmemes). 2.2.2 Textual Embeddings For the text pre-processor we used a pre-trained bert-large-uncased model from the HuggingFace transformers package1 . We fine-tuned the model with additional linear layers for the multi-label task of predicting propaganda techniques in the PTC Corpus. BERT based fine-tuned models are often used for a lot of text classification tasks and obtain state-of-the-art performances in a large number of NLP tasks like GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016). 2.2.3 Combined Embeddings We train the final model with the combined embeddings from the visual and text pre-processor, 1052 1 https://huggingface.co/transformers/ for the final task of multi-label prediction of the 22 propaganda techniques. While the visual preprocessor and textual pre-processor become excellent feature generators individually, combining the embeddings from two modalities with different dimensions (768d for text and 1024d for images) becomes very complicated. While a dot product will be simple and efficient to compute, it will only encode a linear mapping of features, i.e f"
2021.semeval-1.145,W18-5446,0,0.05098,"Missing"
2021.semeval-1.145,D17-2002,0,0.0647753,"Missing"
C08-1067,P98-1074,0,0.0375799,"ic terminology extraction for all languages taking French as the pivot language 2. improved consistency of the database entries, e.g. through the automatic replacement of synonyms by the preferred term (decided in (1)) c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. This paper presents a novel terminology extraction method applied to the French-English part of the database. There is a long tradition of research into bilingual terminology extraction (Kupiec, 1993), (Gaussier, 1998). In most systems, candidate terms are first identified in the source language based on predefined PoS patterns – for French, N N, N Prep N, and N Adj are typical patterns. In a second step, the translation candidates are extracted from the bilingual corpus based on word alignments. In recent work, Itagaki et al. (2007) use the phrase table derived from the GIZA++ alignments to identify the translations. We use a different and more flexible approach. We developed a sub-sentential alignment system that links linguistically motivated phrases in parallel texts based on lexical correspondences and"
C08-1067,P02-1050,0,0.0843761,"Missing"
C08-1067,P07-2045,0,0.00495923,"ength sentences (8-19 words) and long sentences (&gt; 19 words). Each test corpus contains approximately 4,500 words. We also compiled a development corpus containing sentences of varying sentence length to debug the system and to determine the value of the thresholds used in the system. The formal characteristics of the test corpora and the training corpus are given in Table 2. 3 Sub-sentential alignment Sub-sentential alignments – and the underlying word alignments – are used in the context of Machine Translation to create phrase tables for phrase-based statistical machine translation systems (Koehn et al., 2007). A stand-alone subsentential alignment module however, is also useful for human translators if incorporated in CATtools, e.g. sophisticated bilingual concordance systems, or in sub-sentential translation memory systems (Gotti et al., 2005). A quite obvious application of a sub-sentential alignment system is the creation of bilingual dictionaries and terminology extraction from bilingual corpora (Melamed, 2000), (Itagaki et al., 2007). In the context of statistical machine translation, GIZA++ is one of the most widely used word alignment toolkits. GIZA++ implements the IBM models and is used i"
C08-1067,P93-1003,0,0.234813,"ined: 1. automatic terminology extraction for all languages taking French as the pivot language 2. improved consistency of the database entries, e.g. through the automatic replacement of synonyms by the preferred term (decided in (1)) c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. This paper presents a novel terminology extraction method applied to the French-English part of the database. There is a long tradition of research into bilingual terminology extraction (Kupiec, 1993), (Gaussier, 1998). In most systems, candidate terms are first identified in the source language based on predefined PoS patterns – for French, N N, N Prep N, and N Adj are typical patterns. In a second step, the translation candidates are extracted from the bilingual corpus based on word alignments. In recent work, Itagaki et al. (2007) use the phrase table derived from the GIZA++ alignments to identify the translations. We use a different and more flexible approach. We developed a sub-sentential alignment system that links linguistically motivated phrases in parallel texts based on lexical c"
C08-1067,J00-2004,0,0.0130313,"l alignments – and the underlying word alignments – are used in the context of Machine Translation to create phrase tables for phrase-based statistical machine translation systems (Koehn et al., 2007). A stand-alone subsentential alignment module however, is also useful for human translators if incorporated in CATtools, e.g. sophisticated bilingual concordance systems, or in sub-sentential translation memory systems (Gotti et al., 2005). A quite obvious application of a sub-sentential alignment system is the creation of bilingual dictionaries and terminology extraction from bilingual corpora (Melamed, 2000), (Itagaki et al., 2007). In the context of statistical machine translation, GIZA++ is one of the most widely used word alignment toolkits. GIZA++ implements the IBM models and is used in Moses (Koehn et al., 2007) to generate the initial source-to-target and targetto-source word alignments after which some symmetrization heuristics combine the alignments of both translation directions. We present an alternative – linguistically-based – approach, that starts from a lexical probabilistic bilingual dictionary generated by IBM Model One. 530 3.1 Architecture The basic idea behind our approach is"
C08-1067,moore-2002-fast,0,0.0839103,"kes as input sentence-aligned texts, together with additional linguistic annotations (part-of-speech codes and lemmas) for the source and the target text. In the first step of the process, the source and target sentences are divided into chunks based on PoS information, and lexical correspondences are retrieved from a bilingual dictionary. During anchor chunk alignment, the sub-sentential aligner links chunks based on lexical correspondences and chunk similarity. 3.2 Bilingual Dictionary We used the Perl implementation of IBM Model One that is part of the Microsoft Bilingual Sentence Aligner (Moore, 2002) to derive a bilingual dictionary from a parallel corpus. IBM Model One 1 The more syntax-aware SMT systems assume that to a certain extent syntactic relationships in one language directly map to syntactic relationships in the other, which Hwa (2002) calls the Direct Correspondence Assumption. is a purely lexical model: it only takes into account word frequencies of source and target sentences2 . The IBM models allow only 1:n word mappings, and are therefore asymmetric. To overcome this problem, we ran the model in two directions: from French to English and from English to French. To get high-"
C08-1067,J03-1002,0,0.004563,"English PoS tagger often tags a past participle erroneously as a past tense. 532 We adapted the annotation guidelines of Macken (2007) to the French-English language pair, and used three different types of links: regular links for straightforward correspondences, fuzzy links for translation-specific shifts of various kinds, and null links for words for which no correspondence could be indicated. Figure 2 shows an example. Figure 2: Manual reference: regular links are indicated by x’s, fuzzy links and null links by 0’s To evaluate the system’s performance, we used the evaluation methodology of Och and Ney (2003). Och and Ney distinguished sure alignments (S) and possible alignments (P) and introduced the following redefined precision and recall measures: precision = |A ∩ P | |A ∩ S| , recall = |A| |S| (1) (2) We consider all regular links of the manual reference as sure alignments and all fuzzy and null links as possible alignments to compare the output of our system with the manual reference. We trained statistical translation models using Moses. Moses uses the GIZA++ toolkit (IBM Model 1-4) in both translation directions (source to target, target to source) and allows for different symmetrization h"
C08-1067,W00-0901,0,0.0775748,"is an accurate measure for finding the most surprisingly frequent words in a corpus. Low LL values on the other hand allow to retrieve common vocabulary with high frequencies in both corpora. We have created a frequency list for both corpora and calculated the Log-Likelihood values for each word in this frequency list. In the formula below, N corresponds to the number of words in the corpus, whereas the observed values O correspond to the real frequencies of a word in the corpus. The formula for calculating both the expected values (E) and the Log-Likelihood have been described in detail by (Rayson and Garside, 2000). Log-Likelihood Measure In order to detect single word terms that are distinctive enough to be kept in our bilingual lexicon, we have applied the Log-Likelihood measure (LL). This metric considers frequencies of words weighted over two different corpora (in our case a technical automotive corpus and the more general purpose corpus ”Le Monde”), in order to assign high LL-values to words having much higher or lower frequencies than expected. Daille (1995) Oi ) Ei (4) Manual inspection of the Log-Likelihood figures confirmed our hypothesis that more domainspecific terms in our corpus got high LL"
C08-1067,W00-0726,0,0.271103,"Missing"
C08-1067,C98-1071,0,\N,Missing
C16-1257,W10-2914,0,0.329908,"Missing"
C16-1257,W16-0425,0,0.175134,"Missing"
C16-1257,P11-2008,0,0.172382,"Missing"
C16-1257,S13-2093,0,0.0433394,"Missing"
C16-1257,maynard-greenwood-2014-cares,0,0.12366,"Missing"
C16-1257,D13-1066,0,0.181057,"Missing"
C16-1257,D11-1141,0,0.0670702,"ies (Table 1), as well as the extra non-ironic instances that were added (Section 3.2) are equally distributed among the train and test sets. 4.1 Preprocessing After constructing the corpus, all emoji were replaced by their name or a description using the Python Emoji module2 to facilitate annotation and processing of the data. Furthermore, we normalised hyperlinks and @-replies or mentions to http://someurl and @someuser, respectively. Other preprocessing steps involve tokenisation and PoS-tagging (Gimpel et al., 2011), lemmatisation (Van de Kauter et al., 2013) and named entity recognition (Ritter et al., 2011). 1 Due to a refinement of the annotations, the corpus statistics are slightly different from a first version of the annotated corpus described in Van Hee et al. (2016a). 2 https://github.com/carpedm20/emoji/. 2733 4.2 Information Sources For the automatic irony detection system, we implemented a variety of features that represent every instance within a (sparse) feature vector. • As lexical features, we included bags-of-words (BoW) features that represent a tweet as a ‘bag’ of its words or characters. We incorporated token unigrams and bigrams and character trigrams and fourgrams. Furthermore"
C16-1257,E12-2021,0,0.104504,"Missing"
C16-1257,H05-1044,0,0.0804356,"of interjections was added as a feature. Furthermore, we included a binary feature indicating a ‘clash’ between verb tenses in the tweet (see Reyes et al. (2013)). Finally, we integrated four features indicating the presence of named entities in a tweet: one binary feature and three numeric features, indicating (i) the number of named entities in the text, (ii) the number and (iii) frequency of tokens that are part of a named entity. • Six sentiment lexicon features were implemented based on existing sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (GI) (Stone et al., 1966), MPQA (Wilson et al., 2005), the NRC Emotion Lexicon (Mohammad and Turney, 2013), Liu’s opinion lexicon (Hu and Liu, 2004), and Bounce (K¨okciyan et al., 2013). For each lexicon, five numeric and one binary feature were derived: – the number of positive, negative and neutral lexicon words averaged over text length; – the overall tweet polarity (i.e., the sum of the values of the identified sentiment words); – the difference between the highest positive and lowest negative sentiment values; – a binary feature indicating whether there is a polarity contrast (i.e., at least one positive and one negative sentiment word from"
E09-1057,moore-2002-fast,0,0.0299683,"second, tools to generate additional linguistic information (PoS tagger, lemmatizer and a chunker). The sub-sentential alignment system takes as input sentence-aligned texts, together with the additional linguistic annotations for the source and the target texts. The source and target sentences are divided into chunks based on PoS information, and lexical correspondences are retrieved from a bilingual dictionary. In order to extract bilingual dictionaries from the three parallel corpora, we used the Perl implementation of IBM Model One that is part of the Microsoft Bilingual Sentence Aligner (Moore, 2002). In order to link chunks based on lexical clues and chunk similarity, the following steps are taken for each sentence pair: # words 6,408,693 7,305,151 7,100,585 Preprocessing We PoS-tagged and lemmatized the French, English and Italian corpora with the freely available TreeTagger tool (Schmid, 1994) and we used TadPole (Van den Bosch et al., 2007) to annotate the Dutch corpus. In a next step, chunk information was added by a rule-based language-independent chunker (Macken et al., 2008) that contains distituency rules, which implies that chunk boundaries are added between two PoS codes that c"
E09-1057,J03-1002,0,0.0118533,"f corresponding PoS codes and ’R’ for words linked by language-dependent rules. As the contextual clues (the left and right neigbours of the additional candidate chunks are anchor chunks) provide some extra indication that the chunks can be linked, the similarity test for the final candidates was somewhat relaxed: the percentage of words that have to be linked was lowered to 0.80 and a more relaxed PoS matching function was used. 3.4 Evaluation To test our alignment module, we manually indicated all translational correspondences in the three test corpora. We used the evaluation methodology of Och and Ney (2003) to evaluate the system’s performance. They distinguished sure alignments (S) and possible alignments (P) and introduced the following redefined precision and recall measures (where A refers to the set of alignments): Linking Remaining Chunks In a second step, chunks consisting of one function word – mostly punctuation marks and conjunctions – are linked based on corresponding part-ofspeech codes if their left or right neighbour on the diagonal is an anchor chunk. Corresponding final punctuation marks are also linked. In a final step, additional candidates are constructed by selecting non-anch"
E09-1057,C94-2167,0,0.309216,"t module is particularly well suited for the extraction of complex multiword terms. 1 This article reports on the term extraction experiments for 3 language pairs, i.e. French-Dutch, French-English and French-Italian. The focus was on the extraction of automative lexicons. Introduction Automatic Term Recognition (ATR) systems are usually categorized into two main families. On the one hand, the linguistically-based or rule-based approaches use linguistic information such as PoS tags, chunk information, etc. to filter out stop words and restrict candidate terms to predefined syntactic patterns (Ananiadou, 1994), (Dagan and Church, 1994). On the other hand, the statistical corpus-based approaches select n-gram sequences as candidate terms that are filtered by means of The remainder of this paper is organized as follows: Section 2 describes the corpus. In Section 3 we present our linguistically-based sub-sentential alignment system and in Section 4 we describe how we generate and filter our list of candidate terms. We compare the performance of our system with both bilingual and monolingual state-ofthe-art terminology extraction systems. Section 5 concludes this paper. Proceedings of the 12th Conferen"
E09-1057,W00-0901,0,0.0127159,"e climatisation tableau de commande de climatisation Log-Likelihood Measure The Log-Likehood measure (LL) should allow us to detect single word terms that are distinctive enough to be kept in our bilingual lexicon (Daille, 1995). This metric considers word frequencies weighted over two different corpora (in our case a technical automotive corpus and the more general purpose corpus “Le Monde”1 ), in order to assign high LL-values to words having much higher or lower frequencies than expected. We implemented the formula for both the expected values and the Log-Likelihood values as described by (Rayson and Garside, 2000). Manual inspection of the Log-Likelihood figures confirmed our hypothesis that more domainspecific terms in our corpus were assigned high LL-values. We experimentally defined the threshold for Log-Likelihood values corresponding to distinctive terms on our development corpus. Example (4) shows some translation pairs which are filtered out by applying the LL threshold. air conditioning control (4) air conditioning control panel Fr: cependant – En: however – It: tuttavia – Du: echter Fr: choix – En: choice – It: scelta – Du: keuze 4.1 Fr: continuer – En: continue – It: continuare – Du: Filterin"
E09-1057,A94-1006,0,0.0874688,"larly well suited for the extraction of complex multiword terms. 1 This article reports on the term extraction experiments for 3 language pairs, i.e. French-Dutch, French-English and French-Italian. The focus was on the extraction of automative lexicons. Introduction Automatic Term Recognition (ATR) systems are usually categorized into two main families. On the one hand, the linguistically-based or rule-based approaches use linguistic information such as PoS tags, chunk information, etc. to filter out stop words and restrict candidate terms to predefined syntactic patterns (Ananiadou, 1994), (Dagan and Church, 1994). On the other hand, the statistical corpus-based approaches select n-gram sequences as candidate terms that are filtered by means of The remainder of this paper is organized as follows: Section 2 describes the corpus. In Section 3 we present our linguistically-based sub-sentential alignment system and in Section 4 we describe how we generate and filter our list of candidate terms. We compare the performance of our system with both bilingual and monolingual state-ofthe-art terminology extraction systems. Section 5 concludes this paper. Proceedings of the 12th Conference of the European Chapter"
E09-1057,zhang-etal-2008-comparative,0,0.0537254,"Missing"
E09-1057,P93-1003,0,0.333339,"Missing"
E09-1057,C08-1067,1,0.873831,"Missing"
hoste-etal-2008-learning,J90-1003,0,\N,Missing
hoste-etal-2008-learning,J93-2004,0,\N,Missing
hoste-etal-2008-learning,C94-2167,0,\N,Missing
hoste-etal-2008-learning,W00-0901,0,\N,Missing
hoste-etal-2008-learning,A94-1006,0,\N,Missing
hoste-etal-2008-learning,H90-1021,0,\N,Missing
hoste-etal-2008-learning,C00-1030,0,\N,Missing
hoste-etal-2008-learning,W02-0301,0,\N,Missing
J18-4010,L16-1185,0,0.0641843,"Missing"
J18-4010,L16-1256,0,0.0239491,"glish tweets by searching Twitter with the hashtags #irony, #sarcasm, and #not. For this purpose, we made use of Tweepy,4 a Python library to access the official Twitter API. The tweets were collected between 1 December 2014 and 4 January 2015, represent 2,676 unique Twitter users, and have an average length of 15 tokens. An example tweet is presented in Figure 1. To minimize data noise (see earlier), all tweets were manually labeled using a newly developed fine-grained annotation scheme. Although a number of annotation schemes for irony have been developed recently (e.g., Riloff et al. 2013; Bosco et al. 2016; Stranisci et al. 2016), most of them describe a binary distinction (i.e., ironic vs. not-ironic) or flag irony as part of sentiment annotations. By contrast, Karoui et al. (2017) defined explicit and implicit irony activations based on incongruity in (French, English, and Italian) ironic tweets and they defined eight fine-grained categories of pragmatic devices that realize such an incongruity, including analogy, hyperbole, rhetorical question, oxymoron, and so forth. The typology provides valuable insights into the linguistic realization of irony that could improve 4 https://github.com/twee"
J18-4010,W14-2608,0,0.0302769,"Missing"
J18-4010,C16-1251,0,0.371839,"ledge (i.e., the feeling a concept generally invokes for a person or a group of people) is also referred to as prototypical sentiment (Hoste et al. 2016). Like in Example (2), prototypical sentiment expressions are devoid of subjective words and rely on common sense shared by the speaker and receiver in an interaction. To be able to grasp such implied sentiment, sentiment analysis systems require additional knowledge that provides insight into the world we live in and affective information associated with natural language concepts. Although modeling implicit sentiment is still in its infancy (Cambria et al. 2016), such linking of concepts or situations to implicit sentiment will open new perspectives in natural language processing (NLP) applications, not only for sentiment analysis tasks, but also for any type of tasks that involves semantic text processing, such as automatic irony detection and the detection of cyberharassment (Dinakar et al. 2012; Van Hee et al. 2015). Although automatic systems perform well in deriving particular information from a given text (i.e., the meaning of a word in context, emotions expressed by the author of a text), they often struggle to perform tasks where extratextual"
J18-4010,J96-2004,0,0.16426,"ditional clarifications were recommended prior to annotation of the entire corpus. After adding some refinements to the scheme (Van Hee (2017), a second agreement study was carried out by three Master’s students, who each annotated the same subset (i.e., 100 randomly selected instances) of the corpus. In both rounds, inter-annotator agreement was calculated at different steps in the annotation process. As the metric, we used Fleiss’ Kappa (Fleiss 1971), a widespread statistical measure in the field of computational linguistics for assessing agreement between annotators on categorical ratings (Carletta 1996). The results of the interannotator agreement study are presented in Table 1. With the exception of harshness, which proves to be difficult to judge on, Kappa scores show a moderate to substantial agreement between annotators for the different annotation steps.5 Overall, we see that similar or better agreement was obtained after the refinement of the annotation scheme, which had the largest effect on the irony annotation. An exception, however, is the annotation of a polarity contrast between targets and evaluations, where the agreement drops from 0.66 to 0.55 between the first and second roun"
J18-4010,P15-1038,0,0.0361092,"Missing"
J18-4010,W10-2914,0,0.120007,"Missing"
J18-4010,E14-1040,0,0.0211231,"sentence, causing “a soldier veered his jeep into a crowded market and killed three civilians” to be perceived as more negative toward the soldier than “a soldier’s jeep veered into a crowded market, causing three civilian deaths.” Wilson annotated objective polar utterances or statements that describe positive or negative factual information about something (e.g., “The camera broke the first time I used it”) in meeting report content. A similar study was conducted by Toprak, Jakob, and Gurevych (2010), who annotated objective polar utterances in consumer reviews, but named them polar facts. Deng and Wiebe (2014) detected implicit sentiment by inferencing over explicit sentiment expressions, namely, through implicature rules (i.e., “goodFor” and “badFor”), describing events that have either a positive or negative effect on objects or entities. Ebrahimi 795 Computational Linguistics Volume 44, Number 4 (2013) tackled this problem in the medical domain and exploited disease symptoms as negative implicit sentiment features to predict side effects in drug reviews. Implicit or prototypical sentiment is part of common sense, meaning that the information is not explicitly mentioned, but implicitly shared by"
J18-4010,S16-1173,0,0.061526,"Missing"
J18-4010,P13-1174,0,0.0696005,"was expanded by extracting connotative information from Twitter (i.e., bootstrapping by using seed words like “failure” and “disease”), and using ConceptNet (Speer and Havasi 2013). The development of EmotiNet is grounded in Appraisal Theory (Scherer 1999) and aims to store emotional reactions to real-world contexts (e.g., “I’m going to a family party because my mother obliges me to” → disgust). The knowledge base was used to learn intrinsic properties of entities and situations in journalistic text that trigger certain emotions like “refugees” and “being sentenced” (Balahur and Tanev 2016). Feng et al. (2013) created a connotation lexicon that determines the underlying sentiment of seemingly objective words (e.g., cooperation(+) , overcharge(-) ) and the general connotation of named entities (e.g., Einstein(+) , Osama(-) ). Zhang and Liu (2011) hypothesized that resource phrases (e.g., “this washer uses a lot of electricity”) are important carriers of implicit sentiment. They automatically extracted resource terms like “water” and “money” with resource 1 Game With a Purpose: A computer game which integrates human intervention in a computational process in an entertaining way. 2 International Surve"
J18-4010,W16-0425,0,0.0410396,"ed sequential minimal optimization and logistic regression. Reyes, Rosso, and Veale (2013) defined features based on conceptual descriptions in irony literature, being signatures, unexpectedness, style, and emotional scenarios and experimented with na¨ıve Bayes and decision trees. Kunneman et al. (2015) pioneered irony detection in Dutch tweets using word n-gram features and a Balanced Winnow classifier. Van Hee, Lefever, and Hoste (2016b) combined lexical with sentiment, syntactic, and semantic Word2Vec cluster features for irony detection using a support vector machine (SVM). Recent work by Ghosh and Veale (2016), Poria et al. (2016), and Zhang, Zhang, and Fu (2016) has approached irony detection using (deep) neural networks, which make use of continuous automatic features instead of manually defined ones. Besides text-based features, irony research has explored extratextual features related to the author or context of a tweet, such as previous tweets or topics, author profile information, typical sentiment expressed by an author, and so on (Bamman and Smith 2015; Wang et al. 2015). Riloff et al. (2013), Khattri et al. (2015), and Van Hee (2017) exploited implicit or prototypical sentiment information"
J18-4010,P11-2008,0,0.0419782,"Missing"
J18-4010,P11-2102,0,0.389902,"Missing"
J18-4010,N09-1057,0,0.0588481,"Missing"
J18-4010,P13-2121,0,0.0237069,"rds/hashtag words/interjections, hashtag-to-word ratio, emoticon frequency, and tweet length. The first three are binary features, and the others are numeric and present normalized floats (i.e., divided by the tweet length in tokens), except the tweet length feature. A third set of lexical features include conditional n-gram probabilities based on language model probabilities. Although often exploited in machine translation research (e.g., Bojar et al. 2016), language model information incorporated as features is, to our knowledge, novel in irony detection. The models were created with KENLM (Heafield et al. 2013) and are trained on an ironic and a non-ironic background corpus.8 As 6 http://www.chatslang.com/terms/abbreviations. 7 n-grams based on raw tokens were preferred over lemma forms, as preliminary experiments revealed that better results were obtained with the former. 8 The data (1,126,128 tweets) were collected between April 2016 and January 2017 by crawling Twitter at regular intervals using the Twitter Search API. There is no overlap with the training corpus. 803 Computational Linguistics Volume 44, Number 4 features we extracted log probabilities indicating how probable a tweet is likely to"
J18-4010,P15-2124,0,0.0658214,"Missing"
J18-4010,D16-1104,0,0.0269753,"Missing"
J18-4010,P15-2106,0,0.0334952,"Missing"
J18-4010,E17-1025,0,0.0114016,"e tweets were collected between 1 December 2014 and 4 January 2015, represent 2,676 unique Twitter users, and have an average length of 15 tokens. An example tweet is presented in Figure 1. To minimize data noise (see earlier), all tweets were manually labeled using a newly developed fine-grained annotation scheme. Although a number of annotation schemes for irony have been developed recently (e.g., Riloff et al. 2013; Bosco et al. 2016; Stranisci et al. 2016), most of them describe a binary distinction (i.e., ironic vs. not-ironic) or flag irony as part of sentiment annotations. By contrast, Karoui et al. (2017) defined explicit and implicit irony activations based on incongruity in (French, English, and Italian) ironic tweets and they defined eight fine-grained categories of pragmatic devices that realize such an incongruity, including analogy, hyperbole, rhetorical question, oxymoron, and so forth. The typology provides valuable insights into the linguistic realization of irony that could improve 4 https://github.com/tweepy/tweepy. 799 Computational Linguistics Volume 44, Number 4 Figure 1 Corpus example. its automatic detection (e.g., the correlation between irony markers and irony activation type"
J18-4010,N01-1025,0,0.0255343,"Missing"
J18-4010,W15-2905,0,0.0960545,"es the presentation of a manually annotated irony corpus and details on how we developed our irony detection pipeline and experimented with different information sources for the task. 3.1 State of the Art in Irony Detection Research in NLP has recently seen various attempts to tackle irony detection. As described by Joshi, Bhattacharyya, and Carman (2017), irony modeling approaches can roughly be classified into rule-based and machine learning methods. Whereas rulebased approaches mostly rely on lexical information and require no training (e.g., Veale and Hao 2010; Maynard and Greenwood 2014; Khattri et al. 2015), machine learning does utilize training data and exploits various information sources (or features), including bags of words, syntactic patterns, sentiment information, and semantic relatedness (Davidov, Tsur, and Rappoport 2010; Liebrecht, Kunneman, and van den Bosch 2013; Reyes, Rosso, and Veale 2013). Twitter has been a popular data genre for the task, as 3 When discussing related research, we refer to irony using the terminology utilized by the corresponding researchers (i.e., “sarcasm,” “irony,” or “verbal irony”). 798 Van Hee, Lefever, and Hoste Using Common Sense to Detect Irony on Twi"
J18-4010,W13-1605,0,0.0833337,"Missing"
J18-4010,W06-2915,0,0.0360499,"n 3, we present a state-of-the-art irony detection system and in Section 4 we investigate the feasibility to model implicit sentiment in an automatic way. Exploring whether implicit sentiment information benefits irony detection is the focus of Section 5. Section 6 concludes and suggests some directions for future research. 2. Modeling Implicit Sentiment Modeling implicit sentiment is not a new challenge. Efforts to tackle this problem have been undertaken in different research areas, among others, sentiment analysis, content analysis in journalism, and irony detection. Although early work by Lin et al. (2006) investigated how to identify the perspective from which a document is written automatically, it is Greene (2007) and Wilson (2008) who have pioneered implicit sentiment research. Greene introduced the concept of what he later called syntactic packaging (Greene and Resnik 2009) and demonstrated the influence of syntactic choices on the perceived implicit sentiment of news headlines. He showed, for instance, that the active voice tends to attribute a greater sense of responsibility to the agent of a sentence, causing “a soldier veered his jeep into a crowded market and killed three civilians” t"
J18-4010,maynard-greenwood-2014-cares,0,0.0297326,"proach is given. This includes the presentation of a manually annotated irony corpus and details on how we developed our irony detection pipeline and experimented with different information sources for the task. 3.1 State of the Art in Irony Detection Research in NLP has recently seen various attempts to tackle irony detection. As described by Joshi, Bhattacharyya, and Carman (2017), irony modeling approaches can roughly be classified into rule-based and machine learning methods. Whereas rulebased approaches mostly rely on lexical information and require no training (e.g., Veale and Hao 2010; Maynard and Greenwood 2014; Khattri et al. 2015), machine learning does utilize training data and exploits various information sources (or features), including bags of words, syntactic patterns, sentiment information, and semantic relatedness (Davidov, Tsur, and Rappoport 2010; Liebrecht, Kunneman, and van den Bosch 2013; Reyes, Rosso, and Veale 2013). Twitter has been a popular data genre for the task, as 3 When discussing related research, we refer to irony using the terminology utilized by the corresponding researchers (i.e., “sarcasm,” “irony,” or “verbal irony”). 798 Van Hee, Lefever, and Hoste Using Common Sense"
J18-4010,C16-1151,0,0.0137293,"timization and logistic regression. Reyes, Rosso, and Veale (2013) defined features based on conceptual descriptions in irony literature, being signatures, unexpectedness, style, and emotional scenarios and experimented with na¨ıve Bayes and decision trees. Kunneman et al. (2015) pioneered irony detection in Dutch tweets using word n-gram features and a Balanced Winnow classifier. Van Hee, Lefever, and Hoste (2016b) combined lexical with sentiment, syntactic, and semantic Word2Vec cluster features for irony detection using a support vector machine (SVM). Recent work by Ghosh and Veale (2016), Poria et al. (2016), and Zhang, Zhang, and Fu (2016) has approached irony detection using (deep) neural networks, which make use of continuous automatic features instead of manually defined ones. Besides text-based features, irony research has explored extratextual features related to the author or context of a tweet, such as previous tweets or topics, author profile information, typical sentiment expressed by an author, and so on (Bamman and Smith 2015; Wang et al. 2015). Riloff et al. (2013), Khattri et al. (2015), and Van Hee (2017) exploited implicit or prototypical sentiment information to model a polarity"
J18-4010,D13-1066,0,0.261893,"Missing"
J18-4010,D11-1141,0,0.00761371,"), Table 2 Experimental corpus statistics: Number of instances per annotation category plus non-ironic tweets from a background corpus. ironic by clash 1,728 total 802 other type of irony situational irony other verbal irony 401 2,396 267 not ironic (hashtag corpus) not ironic (backgr. corpus) 604 1,792 2,396 Van Hee, Lefever, and Hoste Using Common Sense to Detect Irony on Twitter which is trained on user-generated content. For lack of a reliable Twitter-specific lemmatizer, we made use of the LeTs Preprocess toolkit (Van de Kauter et al. 2013). We used the Twitter named entity recognizer by Ritter et al. (2011) for named entity recognition. Additionally, all tweets were cleaned (e.g., replacement of HTML-escaped characters) and a number of (shallow) normalization steps were introduced to decrease feature sparseness. In concrete terms, all hyperlinks and @-replies in the tweets were normalized to “http://someurl” and “@someuser,” respectively, and abbreviations were replaced by their full form, based on an English abbreviation dictionary6 (e.g., “w/e” → “whatever”). Furthermore, variations in suspension dots were normalized to three dots (e.g., “.....” → “. . . ”), multiple white spaces were reduced"
J18-4010,L16-1462,0,0.0502238,"Missing"
J18-4010,P10-1059,0,0.143273,"Missing"
J18-4010,S14-2070,1,0.910067,"Missing"
J18-4010,C16-1257,1,0.889789,"Missing"
J18-4010,L16-1283,1,0.901638,"Missing"
J18-4010,R15-1086,1,0.875843,"Missing"
J18-4010,wilson-2008-annotating,0,0.055604,"n an automatic way. Exploring whether implicit sentiment information benefits irony detection is the focus of Section 5. Section 6 concludes and suggests some directions for future research. 2. Modeling Implicit Sentiment Modeling implicit sentiment is not a new challenge. Efforts to tackle this problem have been undertaken in different research areas, among others, sentiment analysis, content analysis in journalism, and irony detection. Although early work by Lin et al. (2006) investigated how to identify the perspective from which a document is written automatically, it is Greene (2007) and Wilson (2008) who have pioneered implicit sentiment research. Greene introduced the concept of what he later called syntactic packaging (Greene and Resnik 2009) and demonstrated the influence of syntactic choices on the perceived implicit sentiment of news headlines. He showed, for instance, that the active voice tends to attribute a greater sense of responsibility to the agent of a sentence, causing “a soldier veered his jeep into a crowded market and killed three civilians” to be perceived as more negative toward the soldier than “a soldier’s jeep veered into a crowded market, causing three civilian deat"
J18-4010,H05-1044,0,0.1139,"Missing"
J18-4010,I11-1131,0,0.0219135,"eory (Scherer 1999) and aims to store emotional reactions to real-world contexts (e.g., “I’m going to a family party because my mother obliges me to” → disgust). The knowledge base was used to learn intrinsic properties of entities and situations in journalistic text that trigger certain emotions like “refugees” and “being sentenced” (Balahur and Tanev 2016). Feng et al. (2013) created a connotation lexicon that determines the underlying sentiment of seemingly objective words (e.g., cooperation(+) , overcharge(-) ) and the general connotation of named entities (e.g., Einstein(+) , Osama(-) ). Zhang and Liu (2011) hypothesized that resource phrases (e.g., “this washer uses a lot of electricity”) are important carriers of implicit sentiment. They automatically extracted resource terms like “water” and “money” with resource 1 Game With a Purpose: A computer game which integrates human intervention in a computational process in an entertaining way. 2 International Survey on Emotion Antecedents and Reactions corpus. 796 Van Hee, Lefever, and Hoste Using Common Sense to Detect Irony on Twitter usage verbs such as “use” and “spend” and found that, when occurring together, they often imply a positive or negat"
J18-4010,C16-1231,0,0.0238262,"Missing"
J18-4010,S14-2009,0,\N,Missing
J18-4010,W16-2301,0,\N,Missing
L16-1051,W06-0901,0,0.107341,". Furthermore, defining a set of strict rules often results in low recall scores, since these lexico-semantic rules usually cover only a portion of the many various ways in which certain information can be lexicalized. Finally, rule-based systems are not easily portable to other languages and domains (or, in the case of event detection, to other types of events). In this paper, we tackle the task of economic event detection by means of a supervised machine learning approach, which we expect will be able to detect a wider variety of lexicalizations of economic events. Whereas many researchers (Ahn, 2006; Hardy et al., 2006; Ji and Grishman, 2008) have successfully applied machine learning techniques for event extraction (and coreference) tasks, we are not aware of studies focusing on economic events that employ machine learning methods without making use of event extraction rules. For this paper, we investigated the viability of a classification-based approach to economic event de330 Figure 1: Annotation of economic events in brat. tection based on an annotated corpus of Dutch financial news articles. We aimed at the detection of 10 types of company-specific events on the sentence level. The"
L16-1051,M93-1019,0,0.553062,"mpted the use of text mining techniques for the automatic detection of economic events in news text. Identifying news published about certain events in an automatic way enables researchers in the field of event studies to process more data in less time, and can consequently lead to new insights into the correlation between events and stock market movements. Furthermore, automatic event detection can be of use for various financial applications such as algorithmic trading (Hogenboom, 2012). Many of the existing approaches to the detection of economic events are pattern-based (i.e. rule-based). Appelt et al. (1993), for instance, apply a system which makes use of a domain pattern recognizer for the detection of joint venture events in English and Japanese text. Drury and Almeida (2011) also make use of phrase extraction patterns for the identification of business event phrases in news stories. Other pattern-based methodologies for the detection of certain types of economic events have been adopted by Arendarenko and Kakkonen (2012) and Hogenboom et al. (2013), who developed resp. the BEECON and SPEED systems. Both systems make use of domain ontologies and manually defined lexicon-semantic rules (e.g. “C"
L16-1051,P08-1030,0,0.0228484,"f strict rules often results in low recall scores, since these lexico-semantic rules usually cover only a portion of the many various ways in which certain information can be lexicalized. Finally, rule-based systems are not easily portable to other languages and domains (or, in the case of event detection, to other types of events). In this paper, we tackle the task of economic event detection by means of a supervised machine learning approach, which we expect will be able to detect a wider variety of lexicalizations of economic events. Whereas many researchers (Ahn, 2006; Hardy et al., 2006; Ji and Grishman, 2008) have successfully applied machine learning techniques for event extraction (and coreference) tasks, we are not aware of studies focusing on economic events that employ machine learning methods without making use of event extraction rules. For this paper, we investigated the viability of a classification-based approach to economic event de330 Figure 1: Annotation of economic events in brat. tection based on an annotated corpus of Dutch financial news articles. We aimed at the detection of 10 types of company-specific events on the sentence level. The use of several lexical, syntactic and seman"
L16-1051,P09-2079,0,0.0374138,"Missing"
L16-1051,lefever-etal-2014-evaluation,1,0.89873,"Missing"
L16-1051,E12-2021,0,0.183269,"Missing"
L16-1051,vossen-etal-2008-integrating,0,0.0390921,"Missing"
L16-1283,barbieri-saggion-2014-modelling-irony,0,0.14584,"Missing"
L16-1283,maynard-greenwood-2014-cares,0,0.146354,"verbal irony in social media texts. Furthermore, we present some statistics on the annotated corpora, from which we can conclude that the detection of contrasting evaluations might be a good indicator for recognizing irony. Keywords: social media, figurative language processing, verbal irony 1. Introduction With the arrival of Web 2.0, technologies like social media have become accessible to a vast amount of people. As a result, they have become valuable sources of information about the public’s opinion. What characterizes social media content is that it is often rich in figurative language (Maynard and Greenwood, 2014; Reyes et al., 2013). Handling figurative language represents, however, one of the most challenging tasks in natural language processing. It is often characterized by linguistic devices such as humor, metaphor and irony, whose meaning goes beyond the literal meaning and is therefore often hard to capture, even for humans. Effectively, understanding figurative language often requires world knowledge and familiarity with the conversational context and the cultural background of the conversation’s participants; information that is difficult to access by machines. Verbal irony is a particular gen"
L16-1283,D13-1066,0,0.0815295,"he results of an inter-annotator agreement study to assess the annotation guidelines. Section 4. elaborates on the annotated corpus; a number of statistics are presented to provide insight into the data. Finally, Section 5. concludes the paper with some prospects for future research. (1) It was so nice of my dad to come to my graduation party. #not 2. Regular sentiment analysis systems will probably classify this tweet as positive, whereas the intended emotion is undeniably a negative one. The hashtag #not indicates the presence of irony in this example. By contrast, in example 2. (taken from Riloff et al. (2013)), there is no explicit indication of irony present. Nevertheless, the irony is noticeable because given our world knowledge, we know that the act of going to the dentist (for a root canal) is typically 1 http://alt.qcri.org/semeval2015/task11 Related Research There are many different theoretical approaches to verbal irony. Traditionally, a distinction is made between situational and verbal irony. Situational irony is often referred to as situations that fail to meet some expectations (Lucariello, 1994; Shelley, 2001). Shelley (2001) illustrates this with firefighters who have a fire in their"
L16-1283,C96-2162,0,0.861352,"Missing"
L18-1284,C10-1070,0,0.0910224,"Missing"
L18-1284,hadi-etal-2004-evalda,0,0.148585,"Missing"
L18-1321,S15-2151,1,0.842906,"Missing"
L18-1321,S16-1168,1,0.849431,"the methods underlying Probase are able to extract approximately 25 million pairs. The interest in the hypernymy extraction task is also illustrated by two shared tasks organised within the SemEval framework: TExEval (Taxonomy Extraction Evaluation) focused on finding hyponym-hypernym relations between a list of domain-specific English terms and subsequent taxonomy construction (Bordea et al., 2015), whereas TExEval-2 introduced a multilingual setting for this task, covering four different languages (English, Dutch, Italian and French) from domains as diverse as environment, food and science (Bordea et al., 2016). Our MIsA is an extension of the WebIsADb framework (Seitner et al., 2016) - a publicly available database with more than 400 million English hypernymy relations extracted from the CommonCrawl web corpus - where: 1. we investigate and evaluate the performance of a collection of existing and new lexico-syntactic patterns for five languages of interest (i.e., English (EN), Spanish (ES), French (FR), Italian (IT), Dutch (NL)); 2. we release a new standalone, language-independent and easy to adapt/configure extractor, which is ready to ex2040 Extractor Document splitting corpus Sentence splitting"
L18-1321,P99-1016,0,0.0412159,"key role for many Natural Language Processing (NLP) tasks, e.g. ontology learning, automatically building or extending knowledge bases, or word sense disambiguation and induction. In fact, hypernymy relations may provide the basis for the construction of more complex structures such as taxonomies, or be used as effective background knowledge for many word understanding applications. In the past, many different methods have been developed for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Dolan et al., 1993; Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009), to name a few. Snow et al. (2004) first search sentences that contain two terms that are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scra"
L18-1321,C92-2082,0,0.386809,"ween a generic term (hypernym) and a specific instance of it (hyponym). These relations play a key role for many Natural Language Processing (NLP) tasks, e.g. ontology learning, automatically building or extending knowledge bases, or word sense disambiguation and induction. In fact, hypernymy relations may provide the basis for the construction of more complex structures such as taxonomies, or be used as effective background knowledge for many word understanding applications. In the past, many different methods have been developed for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Dolan et al., 1993; Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009), to name a few. Snow et al. (2004) first search sentences that contain two terms that are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to"
L18-1321,R11-2016,0,0.0148277,"like &quot;pure-bred dogs such as a bulldog or pug&quot; (where N Pc = &quot;pure-bred dogs&quot; and N Pt = &quot;bulldog&quot;|&quot;pug&quot; is a sequence of concepts) and to produce multiple hypernymy relations from a single match (e.g., (bulldog, pure-bred dogs) and (pug, pure-bred dogs)). Some patterns are directly selected or translated from literature works, such as: i) Ponzetto and Strube (2011), where isa patterns were used to induce a taxonomy from Wikipedia; ii) Orna-Montesinos (2011), where patterns for the term “building” were extracted on a set of specialized textbooks in the field of construction engineering; iii) Klaussner and Zhekova (2011) where the authors extract IsA relations from selected Wikipedia pages and iv) research describing lexico-syntactic patterns for languages other than English, such as Lefever et al. (2014) for Dutch, Séguéla (2001) for French and Galicia-Haro and Gelbukh (2014) and Ortega-Mendoza et al. (2007) for Spanish. The remaining are brand-new experimental patterns, whose selection is dictated mainly from the experience of experts in the field of NLP. In Section 5., we provide a manual assessment of the quality of the most productive patterns that were selected for the five considered languages. 3. Extr"
L18-1321,D10-1108,0,0.0111677,"e parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scratch by extracting hypernyms from a domain corpus and the Web. Definitional sentences such as “lion is a dangerous animal” (where “animal” is the hypernym of “lion”) are recognized by the Word Class Lattices classifier (Navigli and Velardi, 2010) trained on a large set of Wikipedia definitions. Kozareva and Hovy (2010) induce a taxonomy using a particular kind of Hearst-like (Hearst, 1992) lexico-syntactic patterns, i.e. so-called Doubly Anchored Patterns (DAP ). The hypernymy relation extraction consists of two phases. First, the authors bootstrap the terminology harvesting with DAP of the kind “animals such as lions and *”, so it is possible to discover new terms such as “cats”. Next, for each pair of terms in the discovered terminology, e.g. (“lions”,“cats”), they automatically create a DAP −1 of the kind “* such as lions and cats” and discover new hypernyms (e.g. “felines”). The above mentioned works fo"
L18-1321,P10-1134,0,0.0155,", 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scratch by extracting hypernyms from a domain corpus and the Web. Definitional sentences such as “lion is a dangerous animal” (where “animal” is the hypernym of “lion”) are recognized by the Word Class Lattices classifier (Navigli and Velardi, 2010) trained on a large set of Wikipedia definitions. Kozareva and Hovy (2010) induce a taxonomy using a particular kind of Hearst-like (Hearst, 1992) lexico-syntactic patterns, i.e. so-called Doubly Anchored Patterns (DAP ). The hypernymy relation extraction consists of two phases. First, the authors bootstrap the terminology harvesting with DAP of the kind “animals such as lions and *”, so it is possible to discover new terms such as “cats”. Next, for each pair of terms in the discovered terminology, e.g. (“lions”,“cats”), they automatically create a DAP −1 of the kind “* such as lions and cats”"
L18-1321,L16-1056,1,0.809232,"n pairs. The interest in the hypernymy extraction task is also illustrated by two shared tasks organised within the SemEval framework: TExEval (Taxonomy Extraction Evaluation) focused on finding hyponym-hypernym relations between a list of domain-specific English terms and subsequent taxonomy construction (Bordea et al., 2015), whereas TExEval-2 introduced a multilingual setting for this task, covering four different languages (English, Dutch, Italian and French) from domains as diverse as environment, food and science (Bordea et al., 2016). Our MIsA is an extension of the WebIsADb framework (Seitner et al., 2016) - a publicly available database with more than 400 million English hypernymy relations extracted from the CommonCrawl web corpus - where: 1. we investigate and evaluate the performance of a collection of existing and new lexico-syntactic patterns for five languages of interest (i.e., English (EN), Spanish (ES), French (FR), Italian (IT), Dutch (NL)); 2. we release a new standalone, language-independent and easy to adapt/configure extractor, which is ready to ex2040 Extractor Document splitting corpus Sentence splitting POS tagging Pattern matching tuples profile Figure 1: Pipeline for the ext"
L18-1321,N03-1033,0,0.0149757,"ment of the quality of the extracted hypernymy relations for the five most productive patterns per language (30 patterns in total). For each pattern, a random sample of 100 extracted hypernym tuples was manually verified by the annotators, who assigned one of the following three labels to each matched hypernym pattern: 2. Sentence splitting: since the context of the extraction is a single sentence, we split each document in separated one-line sentences; 1. Correct: correctly extracted hypernym tuple. 3. POS tagging: each sentence is processed with a POS tagger (we use the Stanford POS-tagger (Toutanova et al., 2003) for the EN, FR and ES corpora and TreeTagger (Schmid, 1994) for the IT and NL corpora) to allow identification of N P s in the next step; 2. Partially correct: the extracted hypernym tuple is not complete (missing hyponyms, part of the instance/class is missing, e.g. Operation Little Switch was an exchange of sick and wounded prisoners resulting in (Operation Little Switch, exchange)) or is too context-dependent or vague (e.g. John Laurence is a friend resulting in (John Laurence, friend)). 4. Pattern matching: in this final step we find all matches between the lexico-syntactic patterns and t"
L18-1321,J13-3007,1,0.787811,"g techniques (Dolan et al., 1993; Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009), to name a few. Snow et al. (2004) first search sentences that contain two terms that are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)), then parse the sentences, and automatically learn patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. For the ontology learning task, Velardi et al. (2013) induce taxonomies from scratch by extracting hypernyms from a domain corpus and the Web. Definitional sentences such as “lion is a dangerous animal” (where “animal” is the hypernym of “lion”) are recognized by the Word Class Lattices classifier (Navigli and Velardi, 2010) trained on a large set of Wikipedia definitions. Kozareva and Hovy (2010) induce a taxonomy using a particular kind of Hearst-like (Hearst, 1992) lexico-syntactic patterns, i.e. so-called Doubly Anchored Patterns (DAP ). The hypernymy relation extraction consists of two phases. First, the authors bootstrap the terminology ha"
L18-1521,P16-2050,1,0.627018,"Missing"
L18-1521,P14-5010,0,0.00299399,"grape varieties to be predicted. Much variation can be seen in the number of training instances per grape, ranging from 5,706 reviews for chardonnay to 222 reviews for carmen`ere. The third classifier aims at predicting among 47 different countries of origin. Again, the class distribution is unbalanced, with some countries represented very well (e.g., US: 25,104 reviews, Italy: 9,912 reviews, France: 8,568 reviews) to countries only occurring once (Tunisia, South Korea, Montenegro, India) in the training set. All wine reviews were linguistically preprocessed by means of the Stanford toolkit (Manning et al., 2014) involving tokenization, lemmatization and Part-of-Speech tagging. From the preprocessed review text, three different feature types were extracted to model the three classification tasks: lexical, semantic and terminology features. 4.1.1. Lexical features We extracted a list of bag-of-words (BoW) unigram features from the review text containing lowercased lemmas. These BoW features were filtered on Part-of-Speech category to filter out function words and only keep content words (nouns, adjectives, verbs, and adverbs). The BoW features were incorporated as binary features, meaning that each BoW"
lefever-etal-2012-discovering,E06-1002,0,\N,Missing
lefever-etal-2012-discovering,P11-2055,1,\N,Missing
lefever-etal-2012-discovering,P10-1023,0,\N,Missing
lefever-etal-2012-discovering,D08-1080,0,\N,Missing
lefever-etal-2012-discovering,2005.mtsummit-papers.11,0,\N,Missing
lefever-etal-2014-evaluation,bosma-vossen-2010-bootstrapping,0,\N,Missing
lefever-etal-2014-evaluation,J90-1003,0,\N,Missing
lefever-etal-2014-evaluation,W04-1807,0,\N,Missing
lefever-etal-2014-evaluation,C92-2082,0,\N,Missing
lefever-etal-2014-evaluation,P99-1016,0,\N,Missing
lefever-etal-2014-evaluation,J98-1004,0,\N,Missing
lefever-etal-2014-evaluation,P06-1015,0,\N,Missing
lefever-etal-2014-evaluation,W12-3206,0,\N,Missing
lefever-etal-2014-evaluation,P10-1134,0,\N,Missing
lefever-etal-2014-evaluation,R13-1078,1,\N,Missing
lefever-etal-2014-evaluation,N04-1041,0,\N,Missing
lefever-etal-2014-evaluation,E12-2021,0,\N,Missing
lefever-etal-2014-evaluation,S12-1012,0,\N,Missing
lefever-hoste-2010-construction,J93-1004,0,\N,Missing
lefever-hoste-2010-construction,E09-1010,0,\N,Missing
lefever-hoste-2010-construction,C04-1192,0,\N,Missing
lefever-hoste-2010-construction,W09-2413,1,\N,Missing
lefever-hoste-2010-construction,W02-0808,0,\N,Missing
lefever-hoste-2010-construction,W09-2412,0,\N,Missing
lefever-hoste-2010-construction,P03-1058,0,\N,Missing
lefever-hoste-2010-construction,J03-1002,0,\N,Missing
lefever-hoste-2010-construction,2005.mtsummit-papers.11,0,\N,Missing
P11-2055,D07-1007,0,0.0634039,"than English), we decided to take a multilingual approach to WSD, that builds up the sense inventory on the basis of the Europarl parallel corpus (Koehn, 2005). Using 317 translations from a parallel corpus implicitly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the target translations. It also facilitates the integration of WSD in multilingual applications such as multilingual Information Retrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence"
P11-2055,P07-1005,0,0.0312932,"to take a multilingual approach to WSD, that builds up the sense inventory on the basis of the Europarl parallel corpus (Koehn, 2005). Using 317 translations from a parallel corpus implicitly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the target translations. It also facilitates the integration of WSD in multilingual applications such as multilingual Information Retrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004)."
P11-2055,daelemans-hoste-2002-evaluation,1,0.745982,"GIZA++ (Och and Ney, 2003) with its default settings for all focus words. This word alignment output was then considered to be the label for the training instances for the corresponding classifier (e.g. the Dutch translation is the label that is used to train the Dutch classifier). By considering this word alignment output as oracle information, we redefined the CLWSD task as a classification task. To train our five classifiers (English as input language and French, German, Dutch, Italian and Spanish as focus languages), we used the memory-based learning (MBL) algorithm implemented in TIMBL (Daelemans and Hoste, 2002), which has successfully been deployed in previous WSD classification tasks (Hoste et al., 2002). We performed heuristic experiments to define the parameter settings for the classifier, leading to the selection of the Jeffrey Divergence distance metric, Gain Ratio feature weighting and k = 7 as number of nearest neighbours. In future work, we plan to use an optimized word-expert approach in which a genetic algorithm performs joint feature selection and parameter optimization per ambiguous word (Daelemans et al., 2003). For our feature vector creation, we combined a set of English local context"
P11-2055,J94-4003,0,0.332534,"Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). The research described in this paper is novel as it presents a truly multilingual classification-based approach to WSD that directly incorporates evidence from four other languages. To this end, we build further on two well-known research ideas: (1) the possibility to use parallel corpora to extract translation labels and features in an automated way and (2) the assumption that incorporating evidence from multiple languages into the feature vector will be more informative than a"
P11-2055,P02-1033,0,0.319522,"etrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). The research described in this paper is novel as it presents a truly multilingual classification-based approach to WSD that directly incorporates evidence from four other languages. To this end, we build further on two well-known research ideas: (1) the possibility to use parallel corpora to extract translation labels and features in an automated way and (2) the assumption that incorporating evidence from multiple languages into the featu"
P11-2055,J93-1004,0,0.0491088,"tions such as multilingual Information Retrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). The research described in this paper is novel as it presents a truly multilingual classification-based approach to WSD that directly incorporates evidence from four other languages. To this end, we build further on two well-known research ideas: (1) the possibility to use parallel corpora to extract translation labels and features in an automated way and (2) the assumption that incorporating evidenc"
P11-2055,S10-1026,0,0.111609,"participated for Dutch and Spanish) and T3-COLEUR. The UvT-WSD system (van Gompel, 2010), that also uses a k-nearest neighbor classifier and a variety of local and global context features, obtained the best scores for Spanish and Dutch in the SemEval CLWSD competition. Although we also use a memory-based learner, our method is different from this system in the way the feature vectors are constructed. Next to the incorporation of similar local context features, we also include evidence from multiple languages in our feature vector. For French, Italian and German however, the T3-COLEUR system (Guo and Diab, 2010) outperformed the other systems in the SemEval competition. This system adopts a different approach: during the training phase a monolingual WSD system processes the English input sentence and a word alignment module is used to extract the aligned translation. The English senses together with their aligned translations (and probability scores) are then stored in a word sense translation table, in which look-ups are performed during the testing phase. This system also differs from the Uvt-WSD and ParaSense systems in the sense that the word senses are derived from WordNet, whereas the other sys"
P11-2055,W02-0808,0,0.258787,"Missing"
P11-2055,2005.mtsummit-papers.11,0,0.0206818,"nguages. 1 Introduction Word Sense Disambiguation (WSD) is the NLP task that consists in selecting the correct sense of a polysemous word in a given context. Most stateof-the-art WSD systems are supervised classifiers that are trained on manually sense-tagged corpora, which are very time-consuming and expensive to build (Agirre and Edmonds, 2006) . In order to overcome this acquisition bottleneck (sense-tagged corpora are scarce for languages other than English), we decided to take a multilingual approach to WSD, that builds up the sense inventory on the basis of the Europarl parallel corpus (Koehn, 2005). Using 317 translations from a parallel corpus implicitly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the target translations. It also facilitates the integration of WSD in multilingual applications such as multilingual Information Retrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translatio"
P11-2055,lefever-hoste-2010-construction,1,0.806978,"icted set of monolingual or bilingual features. Furthermore, our WSD system does not use any information from external lexical resources such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998). Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 317–322, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Experimental Setup Starting point of the experiments was the six-lingual sentence-aligned Europarl corpus that was used in the SemEval-2010 “Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b). The task is a lexical sample task for twenty English ambiguous nouns that consists in assigning a correct translation in the five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect the relevant translations for each of the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings for all focus words. This word alignment output was then considered to be the label for the training instances for the corresponding classifier (e.g. the Dutch translation is the label that"
P11-2055,S10-1003,1,0.941097,"icted set of monolingual or bilingual features. Furthermore, our WSD system does not use any information from external lexical resources such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998). Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 317–322, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Experimental Setup Starting point of the experiments was the six-lingual sentence-aligned Europarl corpus that was used in the SemEval-2010 “Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b). The task is a lexical sample task for twenty English ambiguous nouns that consists in assigning a correct translation in the five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect the relevant translations for each of the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings for all focus words. This word alignment output was then considered to be the label for the training instances for the corresponding classifier (e.g. the Dutch translation is the label that"
P11-2055,S07-1009,0,0.0100329,"rward accuracy measure. The SemEval metric takes into account the frequency weights of the gold standard translations: translations that were picked by different annotators get a higher weight. For the BEST evaluation, systems 1 http://code.google.com/apis/language/ http://wt.jrc.it/lt/Acquis/ 3 http://www.natcorp.ox.ac.uk/ 2 319 can propose as many guesses as the system believes are correct, but the resulting score is divided by the number of guesses. In this way, systems that output a lot of guesses are not favoured. For a more detailed description of the SemEval scoring scheme, we refer to McCarthy and Navigli (2007). Following variables are used for the SemEval precision formula. Let H be the set of annotators, T the set of test items and hi the set of responses for an item i ∈ T for annotator h ∈ H. Let A be the set of items from T where the system provides at least one answer and ai : i ∈ A the set of guesses from the system for item i. For each i, we calculate the multiset union (Hi ) for all hi for all h ∈ H and for each unique type (res) in Hi that has an associated frequency (f reqres ). P P P rec = ai :i∈A res∈ai f reqres |ai | |Hi | |A| (1) The second metric we use is a straightforward accuracy m"
P11-2055,P03-1058,0,0.0688095,"ual Information Retrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). The research described in this paper is novel as it presents a truly multilingual classification-based approach to WSD that directly incorporates evidence from four other languages. To this end, we build further on two well-known research ideas: (1) the possibility to use parallel corpora to extract translation labels and features in an automated way and (2) the assumption that incorporating evidence from multiple l"
P11-2055,J03-1002,0,0.0020911,"l Linguistics 2 Experimental Setup Starting point of the experiments was the six-lingual sentence-aligned Europarl corpus that was used in the SemEval-2010 “Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b). The task is a lexical sample task for twenty English ambiguous nouns that consists in assigning a correct translation in the five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect the relevant translations for each of the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings for all focus words. This word alignment output was then considered to be the label for the training instances for the corresponding classifier (e.g. the Dutch translation is the label that is used to train the Dutch classifier). By considering this word alignment output as oracle information, we redefined the CLWSD task as a classification task. To train our five classifiers (English as input language and French, German, Dutch, Italian and Spanish as focus languages), we used the memory-based learning (MBL) algorithm implemented in TIMBL (Daelemans and Hoste, 2002),"
P11-2055,C04-1192,0,0.146204,"Missing"
P11-2055,S10-1053,0,0.399631,"Missing"
P11-2055,W09-2413,1,\N,Missing
P16-2050,P14-5010,0,0.0072621,"Missing"
R13-1024,P06-2005,0,0.107264,"proposed a supervised noisy channel model using Hidden Markov Models to calculate the probability of less frequent words. Extensions to this approach were made by studying word processes (Cook and Stevenson, 2009), adapting weighted finite-state machines and rewrite rules (Beaufort et al., 2010) or by adding other elements such as orthographic, phonetic and contextual factors (Xue et al., 2011). Another approach is using statistical machine translation (SMT) techniques for text normalization. Previous work in this field has mostly focused on phrase-based machine translation at the word level. Aw et al. (2006) were the first to compare dictionary substitution using frequencies with phrase-based machine translation. They revealed that SMT improves BLEU scores for English SMS translation. Also working on English text, Raghunathan et al. (2009) confirmed that using an SMT system outperforms a dictionary look-up, most no3 Three Genres of UGC In order to normalize using a machine translation system, and to evaluate the performance, it is essential to build a gold standard data set that can serve as training and test material. As far as we know, no such data set is currently available for Dutch. 3.1 Corp"
R13-1024,P10-1079,0,0.557858,"red to as the spell-checking, machine translation and speech recognition metaphors (Kobus et al., 2008). The most intuitive way of normalizing text would be to approach the problem as a spellchecking one where noisy text has to be transformed to standard text using noisy channel models. Choudhury et al. (2007), for example, proposed a supervised noisy channel model using Hidden Markov Models to calculate the probability of less frequent words. Extensions to this approach were made by studying word processes (Cook and Stevenson, 2009), adapting weighted finite-state machines and rewrite rules (Beaufort et al., 2010) or by adding other elements such as orthographic, phonetic and contextual factors (Xue et al., 2011). Another approach is using statistical machine translation (SMT) techniques for text normalization. Previous work in this field has mostly focused on phrase-based machine translation at the word level. Aw et al. (2006) were the first to compare dictionary substitution using frequencies with phrase-based machine translation. They revealed that SMT improves BLEU scores for English SMS translation. Also working on English text, Raghunathan et al. (2009) confirmed that using an SMT system outperfo"
R13-1024,W09-2010,0,0.0454891,"define three dominant approaches to transfer noisy into standard text. These are referred to as the spell-checking, machine translation and speech recognition metaphors (Kobus et al., 2008). The most intuitive way of normalizing text would be to approach the problem as a spellchecking one where noisy text has to be transformed to standard text using noisy channel models. Choudhury et al. (2007), for example, proposed a supervised noisy channel model using Hidden Markov Models to calculate the probability of less frequent words. Extensions to this approach were made by studying word processes (Cook and Stevenson, 2009), adapting weighted finite-state machines and rewrite rules (Beaufort et al., 2010) or by adding other elements such as orthographic, phonetic and contextual factors (Xue et al., 2011). Another approach is using statistical machine translation (SMT) techniques for text normalization. Previous work in this field has mostly focused on phrase-based machine translation at the word level. Aw et al. (2006) were the first to compare dictionary substitution using frequencies with phrase-based machine translation. They revealed that SMT improves BLEU scores for English SMS translation. Also working on"
R13-1024,P07-1013,0,0.0751251,"Missing"
R13-1024,I11-1109,0,0.171077,"e work by Liu et al. (2011a; 2012). They propose a cognition-driven text normalization system using an unsupervised approach. By observing and simulating human techniques for the normalization task, they avoid dependence on human annotations. They construct a broad-coverage system to enable better word-coverage, using three key components: enhanced letter transformation, visual priming and string/phonetic similarity. If we consider normalization, the task intuitively has a lot in common with transliteration tasks for which character-based SMT systems have proven adequate (Vilar et al., 2007). Pennell and Liu (2011) were the first to study characterbased normalization. They, however, limited their approach by only focusing on abbreviations. In this paper, we propose a cascaded model that follows a machine translation approach and tries to tackle the full range of normalization problems. media genres used, their characteristics and how these have been normalized in Section 3. The setup and experiments are presented in Section 4. We examine the results in Section 5, perform a qualitative error analysis in Section 6 to end with some conclusions and prospects for future work in Section 7. 2 Related Work Trad"
R13-1024,kestemont-etal-2012-netlog,0,0.171929,"Missing"
R13-1024,D11-1141,0,0.0568722,"Missing"
R13-1024,C08-1056,0,0.504823,"Missing"
R13-1024,P07-2045,0,0.00585982,"Missing"
R13-1024,2008.amta-srw.5,0,0.0245086,"which we test this approach on the other genres to see whether it is possible to create a robust system that can process all three UGC genres. To evaluate our approach, both the Word Error Rate (WER) and BLEU scores were calculated. WER, an evaluation metric that is based on edit distance at the word level, is very well suited for the evaluation of NLP tasks where the input and output strings are closely related. As a consequence, the metric is used for the evaluation of optical character recognition (Kolak et al., 2003), grapheme-to-phoneme conversion (Demberg et al., 2007), diacritization (Schlippe et al., 2008) and vocalization of Arabic (K¨ubler and Mohamed, 2008). The BLEU metric, which has been specifically designed for measuring machine translation quality, measures the n-gram overlap between the translation being evaluated and a set of target translations. We therefore believe that BLEU is less appropriate for evaluation in the current set-up, but we include it for comparison’s sake (as other systems mention it such as Aw et al. (2006), Kobus et al. (2008), etc.). 5.1 Results on SMS 10 WER 15 20 25 This is, to our knowledge, the first study on Dutch text normalization, so there is no basis for"
R13-1024,N03-1018,0,0.0399779,"tested on the other two genres, 125 SNS posts and 125 tweets (Section 5.2). SMS genre, after which we test this approach on the other genres to see whether it is possible to create a robust system that can process all three UGC genres. To evaluate our approach, both the Word Error Rate (WER) and BLEU scores were calculated. WER, an evaluation metric that is based on edit distance at the word level, is very well suited for the evaluation of NLP tasks where the input and output strings are closely related. As a consequence, the metric is used for the evaluation of optical character recognition (Kolak et al., 2003), grapheme-to-phoneme conversion (Demberg et al., 2007), diacritization (Schlippe et al., 2008) and vocalization of Arabic (K¨ubler and Mohamed, 2008). The BLEU metric, which has been specifically designed for measuring machine translation quality, measures the n-gram overlap between the translation being evaluated and a set of target translations. We therefore believe that BLEU is less appropriate for evaluation in the current set-up, but we include it for comparison’s sake (as other systems mention it such as Aw et al. (2006), Kobus et al. (2008), etc.). 5.1 Results on SMS 10 WER 15 20 25 Th"
R13-1024,P11-2013,0,0.0954099,"framework of the STEVIN programme, see Spijns and Odijk (2013) for an overview. 179 Proceedings of Recent Advances in Natural Language Processing, pages 179–188, Hissar, Bulgaria, 7-13 September 2013. tably when used on an out-of-domain test set. Kobus et al. (2008) followed the same approach but combined the machine translation features with a speech recognition approach using HMMs on a French corpus. They concluded that the two systems perform better on different aspects of the task and that combining these two modules works best. A different way of approaching normalization is the work by Liu et al. (2011a; 2012). They propose a cognition-driven text normalization system using an unsupervised approach. By observing and simulating human techniques for the normalization task, they avoid dependence on human annotations. They construct a broad-coverage system to enable better word-coverage, using three key components: enhanced letter transformation, visual priming and string/phonetic similarity. If we consider normalization, the task intuitively has a lot in common with transliteration tasks for which character-based SMT systems have proven adequate (Vilar et al., 2007). Pennell and Liu (2011) wer"
R13-1024,P11-1037,0,0.0217483,"framework of the STEVIN programme, see Spijns and Odijk (2013) for an overview. 179 Proceedings of Recent Advances in Natural Language Processing, pages 179–188, Hissar, Bulgaria, 7-13 September 2013. tably when used on an out-of-domain test set. Kobus et al. (2008) followed the same approach but combined the machine translation features with a speech recognition approach using HMMs on a French corpus. They concluded that the two systems perform better on different aspects of the task and that combining these two modules works best. A different way of approaching normalization is the work by Liu et al. (2011a; 2012). They propose a cognition-driven text normalization system using an unsupervised approach. By observing and simulating human techniques for the normalization task, they avoid dependence on human annotations. They construct a broad-coverage system to enable better word-coverage, using three key components: enhanced letter transformation, visual priming and string/phonetic similarity. If we consider normalization, the task intuitively has a lot in common with transliteration tasks for which character-based SMT systems have proven adequate (Vilar et al., 2007). Pennell and Liu (2011) wer"
R13-1024,E12-1015,0,0.0287471,"t into characters and a translation at the character level takes place. This intuitively makes sense, because transformations at the character level are more likely to be reproduced than a combination of possible transformations at the word level. Trying to generalize such character transformations at the word level would probably fail due to data sparseness. We worked with both character unigram and bigram translation models. Bigrams supposedly have the advantage that one character of context across phrase boundaries is used in the selection of translation alternatives from the phrase table (Tiedemann, 2012). This means that more precise translations will be suggested. For our experiments we first focus on the individual performance we can achieve within the annotators were asked to indicate the end of a thought (to account for missing punctuation), regional words, foreign words and named entities. They could also flag words that are ungrammatical, stressed, part of a compound, used as interjections or words that require consecutive normalization operations. To check the reliability of our annotation guidelines, the two annotators each normalized the 1,000 text messages. We estimated the interann"
R13-1024,P12-1109,0,0.0600586,"Missing"
R13-1024,treurniet-etal-2012-collection,1,0.893276,"Missing"
R13-1024,W07-0705,0,0.0499513,"g normalization is the work by Liu et al. (2011a; 2012). They propose a cognition-driven text normalization system using an unsupervised approach. By observing and simulating human techniques for the normalization task, they avoid dependence on human annotations. They construct a broad-coverage system to enable better word-coverage, using three key components: enhanced letter transformation, visual priming and string/phonetic similarity. If we consider normalization, the task intuitively has a lot in common with transliteration tasks for which character-based SMT systems have proven adequate (Vilar et al., 2007). Pennell and Liu (2011) were the first to study characterbased normalization. They, however, limited their approach by only focusing on abbreviations. In this paper, we propose a cascaded model that follows a machine translation approach and tries to tackle the full range of normalization problems. media genres used, their characteristics and how these have been normalized in Section 3. The setup and experiments are presented in Section 4. We examine the results in Section 5, perform a qualitative error analysis in Section 6 to end with some conclusions and prospects for future work in Sectio"
R13-1078,S12-1012,0,0.119364,"a label to each cluster, it is then also possible to extract is − a relations between each cluster member and the cluster label. Caraballo (1999) uses syntactic dependency features (such as conjunction and apposition) to automatically build noun clusters. Pantel and Ravichandran (2004) extended his 594 3.1.1 work by including all syntactic dependency relations for each considered noun. More recent distributional approaches rely on the Distributional Inclusion Hypothesis, according to which semantically narrower terms include a significant number of distributional features of their hypernyms (Lenci and Benotto, 2012). The main advantage of the distributional approaches is that they allow to find semantically related terms, even when they do not explicitly occur in predefined patterns in text. The main disadvantage, however, is that these clustering approaches have difficulties to determine the exact semantic relationship (synonymy, antonymy, hyponymy) between the semantically related concepts. The corpus used in the experiments is a onemillion subcorpus of the 500-million word balanced reference corpus for contemporary (1954present) Dutch texts: SoNaR (Oostdijk et al., 2012). It consists of 38 text types"
R13-1078,P99-1016,0,0.284059,"target word, both cooccurrence and syntactic information can be extracted from the surrounding words. Unsupervised learning methods like clustering to obtain taxonomies, definitions and semantically similar words have been applied by (Widdows, 2003; Pereira et al., 1993; Van de Cruys, 2010). Clustering has also shown to be a valid approach to automatically detect hypernym relations between terms. By clustering words according to their contexts in text and assigning a label to each cluster, it is then also possible to extract is − a relations between each cluster member and the cluster label. Caraballo (1999) uses syntactic dependency features (such as conjunction and apposition) to automatically build noun clusters. Pantel and Ravichandran (2004) extended his 594 3.1.1 work by including all syntactic dependency relations for each considered noun. More recent distributional approaches rely on the Distributional Inclusion Hypothesis, according to which semantically narrower terms include a significant number of distributional features of their hypernyms (Lenci and Benotto, 2012). The main advantage of the distributional approaches is that they allow to find semantically related terms, even when the"
R13-1078,J96-2004,0,0.0118713,"Missing"
R13-1078,P98-2127,0,0.340679,"hyponymy relations in English text. Subsequently, various researchers continued working with this pattern-based approach for English (Cederberg and Widdows, 2003; Pantel and Ravichandran, 2004; Riloff and Shepherd, 1997; Roark and Charniak, 1998) as well as for other languages such as French (Malais´e et al., 2004) or Romanian (Mititelu, 2008). The patterns were further extended through translation and manually searching through texts (Kozareva et al., 2008), or by using more sophisticated methods of clustering related terms, starting from known hypernym pairs and features (Snow et al., 2006; Lin, 1998) or lists of seed words known to have the desired relationship (Roark and Charniak, 1998; Riloff and Shepherd, 1997; Widdows and Borow, 2002). Pantel and Pennacchiotti (2006) used generic patterns (broad coverage noisy patterns) to extract semantic relations and subsequently apply refining techniques to deal with the wide variety of such relations. Similar approaches that combine pattern extraction with post-processing techniques to enrich the system and improve the results have been investigated, for example, with Support Vector Machines and Hidden Markov Models (Ritter et al., 2009). A diffe"
R13-1078,W03-0415,0,0.163526,", Language and Translation Technology Team University College Ghent Gent, Belgium gwendolijn@gmail.com Abstract researchers have started to investigate how semantic resources such as ontologies can be learned from text instead of being created manually. For an overview we refer to (Biemann, 2005). In this paper, we focus on the detection of hypernym relations between nouns and noun phrases. Automatic extraction of nouns or noun phrases which are semantically related has been successfully achieved in prior research, for example using coordination and co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al.,"
R13-1078,W04-1807,0,0.641168,"Missing"
R13-1078,N03-1011,0,0.0519658,"Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al., 2003), which is the part-whole relationship, and synonymy (Lin et al., 2003), which expresses equality. Automatic hypernym detection has been explored in multiple ways. A clear distinction can be made between the pattern-based approaches and the statistical approaches. The aim of the present research is to present a hybrid approach in which distributional information acts as a filter on the pattern-based output. Although our current focus is on hypernym detection of noun-noun pairs, the final goal of this research is to use the automatic hypernym detection system to obtain a hierarchically structur"
R13-1078,C92-2082,0,0.748159,"co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al., 2003), which is the part-whole relationship, and synonymy (Lin et al., 2003), which expresses equality. Automatic hypernym detection has been explored in multiple ways. A clear distinction can be made between the pattern-based approaches and the statistical approaches. The aim of the present research is to present a hybrid approach in which distributional information acts as a filter on the pattern-based output. Although our current focus is on hypernym detection of noun-noun pairs, the final goal of this research is to use the automati"
R13-1078,P10-1134,0,0.146011,"Missing"
R13-1078,P09-1049,0,0.0285254,"olijn Schropp LT3, Language and Translation Technology Team University College Ghent Gent, Belgium gwendolijn@gmail.com Abstract researchers have started to investigate how semantic resources such as ontologies can be learned from text instead of being created manually. For an overview we refer to (Biemann, 2005). In this paper, we focus on the detection of hypernym relations between nouns and noun phrases. Automatic extraction of nouns or noun phrases which are semantically related has been successfully achieved in prior research, for example using coordination and co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference"
R13-1078,P08-1119,0,0.0239374,"attern-based approaches were inspired by the seminal work of Hearst (1992) in which she identified a set of lexico-syntactic patterns for the identification of hyponymy relations in English text. Subsequently, various researchers continued working with this pattern-based approach for English (Cederberg and Widdows, 2003; Pantel and Ravichandran, 2004; Riloff and Shepherd, 1997; Roark and Charniak, 1998) as well as for other languages such as French (Malais´e et al., 2004) or Romanian (Mititelu, 2008). The patterns were further extended through translation and manually searching through texts (Kozareva et al., 2008), or by using more sophisticated methods of clustering related terms, starting from known hypernym pairs and features (Snow et al., 2006; Lin, 1998) or lists of seed words known to have the desired relationship (Roark and Charniak, 1998; Riloff and Shepherd, 1997; Widdows and Borow, 2002). Pantel and Pennacchiotti (2006) used generic patterns (broad coverage noisy patterns) to extract semantic relations and subsequently apply refining techniques to deal with the wide variety of such relations. Similar approaches that combine pattern extraction with post-processing techniques to enrich the syst"
R13-1078,P06-1015,0,0.375589,"ws, 2003; Pantel and Ravichandran, 2004; Riloff and Shepherd, 1997; Roark and Charniak, 1998) as well as for other languages such as French (Malais´e et al., 2004) or Romanian (Mititelu, 2008). The patterns were further extended through translation and manually searching through texts (Kozareva et al., 2008), or by using more sophisticated methods of clustering related terms, starting from known hypernym pairs and features (Snow et al., 2006; Lin, 1998) or lists of seed words known to have the desired relationship (Roark and Charniak, 1998; Riloff and Shepherd, 1997; Widdows and Borow, 2002). Pantel and Pennacchiotti (2006) used generic patterns (broad coverage noisy patterns) to extract semantic relations and subsequently apply refining techniques to deal with the wide variety of such relations. Similar approaches that combine pattern extraction with post-processing techniques to enrich the system and improve the results have been investigated, for example, with Support Vector Machines and Hidden Markov Models (Ritter et al., 2009). A different approach has been used by Navigli et al. (2010), that use word class lattices, or directed acyclic graphs, to Other researchers have applied a distributional approach to"
R13-1078,N04-1041,0,0.546234,"present our results in Section 4. Section 5 concludes the paper with some prospects for future research. 2 Related Research Two main approaches are used to learn hypernym relations from text: pattern-based (or rule-based) approaches and distributional approaches. Most of the pattern-based approaches were inspired by the seminal work of Hearst (1992) in which she identified a set of lexico-syntactic patterns for the identification of hyponymy relations in English text. Subsequently, various researchers continued working with this pattern-based approach for English (Cederberg and Widdows, 2003; Pantel and Ravichandran, 2004; Riloff and Shepherd, 1997; Roark and Charniak, 1998) as well as for other languages such as French (Malais´e et al., 2004) or Romanian (Mititelu, 2008). The patterns were further extended through translation and manually searching through texts (Kozareva et al., 2008), or by using more sophisticated methods of clustering related terms, starting from known hypernym pairs and features (Snow et al., 2006; Lin, 1998) or lists of seed words known to have the desired relationship (Roark and Charniak, 1998; Riloff and Shepherd, 1997; Widdows and Borow, 2002). Pantel and Pennacchiotti (2006) used ge"
R13-1078,N04-3008,0,0.0320259,"sed cooccurring words. In a second step, we applied Pointwise Mutual Information (Church and Hanks, 1990) as a weighting function to discover informative semantic similarity relations between words. As we only want to consider contexts with a high semantic discrimination value, we smoothened the matrix by removing stop words and low frequent words (occurring less than 3 times in the corpus) from the context features. Finally, the cooccurrence matrix was converted into a vector of context features per target word. The matrix and vector construction was performed with the SenseClusters Package (Pedersen and Purandare, 2004). We used the CLUTO clustering toolkit (Karypis, 2002) to group semantically related words into clusters. Similarity between the context vectors was computed by taking their cosine, the cosine of the angle between two vectors being the inner product of the vectors. We used a K-means clustering algorithm and ran experiments with a varying number of output clusters. The impact of the desired number of output clusters is discussed in section 4. 4 Experimental results 4.1 Experimental set-up To evaluate the performance of both the patternbased and combined approach, we extracted a test set from th"
R13-1078,P93-1024,0,0.555766,"tices, or directed acyclic graphs, to Other researchers have applied a distributional approach to automatically extract hypernym pairs from text. The latter approaches start from the distributional hypothesis, stating that words that occur in similar contexts tend to be semantically similar (Harris, 1968). In order to define the context of a given target word, both cooccurrence and syntactic information can be extracted from the surrounding words. Unsupervised learning methods like clustering to obtain taxonomies, definitions and semantically similar words have been applied by (Widdows, 2003; Pereira et al., 1993; Van de Cruys, 2010). Clustering has also shown to be a valid approach to automatically detect hypernym relations between terms. By clustering words according to their contexts in text and assigning a label to each cluster, it is then also possible to extract is − a relations between each cluster member and the cluster label. Caraballo (1999) uses syntactic dependency features (such as conjunction and apposition) to automatically build noun clusters. Pantel and Ravichandran (2004) extended his 594 3.1.1 work by including all syntactic dependency relations for each considered noun. More recent"
R13-1078,C02-1114,0,0.238169,"ollege Ghent Gent, Belgium gwendolijn@gmail.com Abstract researchers have started to investigate how semantic resources such as ontologies can be learned from text instead of being created manually. For an overview we refer to (Biemann, 2005). In this paper, we focus on the detection of hypernym relations between nouns and noun phrases. Automatic extraction of nouns or noun phrases which are semantically related has been successfully achieved in prior research, for example using coordination and co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al., 2003), which is the part-whole relationship, and syn"
R13-1078,W97-0313,0,0.0669935,"4. Section 5 concludes the paper with some prospects for future research. 2 Related Research Two main approaches are used to learn hypernym relations from text: pattern-based (or rule-based) approaches and distributional approaches. Most of the pattern-based approaches were inspired by the seminal work of Hearst (1992) in which she identified a set of lexico-syntactic patterns for the identification of hyponymy relations in English text. Subsequently, various researchers continued working with this pattern-based approach for English (Cederberg and Widdows, 2003; Pantel and Ravichandran, 2004; Riloff and Shepherd, 1997; Roark and Charniak, 1998) as well as for other languages such as French (Malais´e et al., 2004) or Romanian (Mititelu, 2008). The patterns were further extended through translation and manually searching through texts (Kozareva et al., 2008), or by using more sophisticated methods of clustering related terms, starting from known hypernym pairs and features (Snow et al., 2006; Lin, 1998) or lists of seed words known to have the desired relationship (Roark and Charniak, 1998; Riloff and Shepherd, 1997; Widdows and Borow, 2002). Pantel and Pennacchiotti (2006) used generic patterns (broad cover"
R13-1078,N03-1036,0,0.367894,"Translation Technology Team University College Ghent Gent, Belgium gwendolijn@gmail.com Abstract researchers have started to investigate how semantic resources such as ontologies can be learned from text instead of being created manually. For an overview we refer to (Biemann, 2005). In this paper, we focus on the detection of hypernym relations between nouns and noun phrases. Automatic extraction of nouns or noun phrases which are semantically related has been successfully achieved in prior research, for example using coordination and co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al.,"
R13-1078,P98-2182,0,0.0687135,"chnology Team University College Ghent Gent, Belgium gwendolijn@gmail.com Abstract researchers have started to investigate how semantic resources such as ontologies can be learned from text instead of being created manually. For an overview we refer to (Biemann, 2005). In this paper, we focus on the detection of hypernym relations between nouns and noun phrases. Automatic extraction of nouns or noun phrases which are semantically related has been successfully achieved in prior research, for example using coordination and co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al., 2003), which is the part-w"
R13-1078,P06-1101,0,0.150274,"ion can be made between the pattern-based approaches and the statistical approaches. The aim of the present research is to present a hybrid approach in which distributional information acts as a filter on the pattern-based output. Although our current focus is on hypernym detection of noun-noun pairs, the final goal of this research is to use the automatic hypernym detection system to obtain a hierarchically structured term list for any kind of input text. Prior research in hypernym detection suggested the extracted hypernym-hyponym pairs could be used to extend general thesauri like WordNet (Snow et al., 2006; Roark and Charniak, This paper proposes a two-step approach to find hypernym relations between pairs of noun phrases in Dutch text. We first apply a pattern-based approach that combines lexical and shallow syntactic information to extract a list of candidate hypernym pairs from the input text. In a second step, distributional similarity information is used to filter the obtained list of candidate pairs. Evaluation of the system shows encouraging results and reveals that the distributional information particularly helps to improve the precision for context dependent hypernym pairs. The propos"
R13-1078,W09-1122,0,0.041,"Missing"
R13-1078,bosma-vossen-2010-bootstrapping,0,\N,Missing
R13-1078,J90-1003,0,\N,Missing
R13-1078,C98-2177,0,\N,Missing
R13-1078,C98-2122,0,\N,Missing
R15-1086,E09-1046,0,0.0109468,"s. # Posts 3085 181 1671 129 546 23 49 1 • Character n-gram bag-of-words: binary features indicating the presence of character trigrams (without crossing word boundaries), to provide some abstraction from the word level. • Sentiment lexicon features: four numeric features representing the number of positive, negative, and neutral lexicon words (averaged over text length) and the overall post polarity (i.e. the sum of the values of identified sentiment words averaged over text length)4 . The features were calculated based on existing sentiment lexicons for Dutch (De Smedt and Daelemans, 2012b; Jijkoun and Hofmann, 2009). Table 3: Data distribution for the different author roles in cyberbullying events. 4 Experiments This section describes the experiments that were conducted to gain insight into the detection and fine-grained classification of cyberbullying events. 4.1 Features Experimental Setup 5 Two sets of experiments were conducted. Firstly, we explored the detection of cyberbullying posts regardless of the harmfulness score (i.e. we considered posts that were given a score of 1 or 2) and the author’s role. The second set of experiments focuses on a more complex task, the identification of fine-grained t"
R15-1086,de-smedt-daelemans-2012-vreselijk,1,0.415387,"Missing"
R15-1086,desmet-hoste-2014-recognising,1,0.851197,"f cyberbullying posts regardless of the harmfulness score (i.e. we considered posts that were given a score of 1 or 2) and the author’s role. The second set of experiments focuses on a more complex task, the identification of fine-grained text categories related to cyberbullying (see Section 3.2). To this end, a binary classifier was built for each category. Evaluation was done using 10-fold cross-validation. We used Support Vector Machines (SVM) as the classification algorithm since they have proven to work well for high-skew text classification tasks similar to the ones under investigation (Desmet and Hoste, 2014). We used linear kernels and experimentally determined the optimal cost value c to be 1. All experiments were carried out using Pattern (De Smedt and Daelemans, 2012a), a Python Results We implemented different experimental set-ups with various feature groups and hence determined the informativeness of each feature group for the current classification tasks. We explored the contributiveness of the following feature groups in isolation: word unigram bag-of-words (which can be considered as the baseline approach), word bigram bag-of-words, character trigram bag-ofwords, and sentiment lexicon fea"
R15-1086,E12-2021,0,0.0383805,"Missing"
R19-1071,P07-1083,0,0.041255,"false friends are words which have similar forms, but which differ in their meaning. The ability to distinguish cognates from noncognates (and especially) false friends is an important skill for second language learners. Similarly, source language interference is a problem often experienced by translators that is partly caused by the influence of cognates and false friends. Research in natural language processing can address these bottlenecks by, for instance, developing computer tools that aid second language users. Nevertheless, most studies have mainly focused on the detection of cognates (Bergsma and Kondrak, 2007; Hauer and Kondrak, 2011; Ciobanu and Dinu, 2014; Rama, 2016), while relatively little attention has been devoted to false friends (Frunza and Inkpen, 2007; Mitkov et al., 2007; Ljubeˇsi´c and Fiˇser, 2013; Castro et al., 2018). Mitkov (2007) explains that the main goal of investigation is often the cross-lingual identification of equivalent lexical items, as such knowledge can be integrated in other applications. Automatic cognate detection has indeed proven very useful for NLP, e.g. to boost the performance of automatic alignment between related languages or to compile bilingual lexicons (S"
R19-1071,Q17-1010,0,0.482846,"unt-based models count how often a given target word co-occurs with its neighbor words in a large text corpus, after which the resulting counts are mapped to a dense vector for each word. On the other hand, predictive models directly try to predict a word from its neighbors in terms of learned dense embedding vectors (Baroni et al., 2014). Word2vec (Mikolov et al., 2013) is a particularly computationally-efficient and popular example of predictive models for learning word embeddings from raw text. In this research, we will incorporate the more recent fastText word embeddings as implemented by Bojanowski et al. (2017). Related Research Extensive lists of known cognates and false friends are hard to find and expensive to compose, since they require a considerable amount of time and effort from trained lexicographers (Schepens et al., 2012). Especially for low resource languages, this constitutes a serious issue. Therefore, most NLP research on cognates has mainly focused on the automatic detection of such cognate pairs. In the literature, there are three main methods to identify cognates: orthographic, phonetic and semantic approaches. The oldest approaches to tackle this task involve simple string similari"
R19-1071,W13-2411,0,0.0571328,"Missing"
R19-1071,2007.jeptalnrecital-long.8,0,0.902976,"se friends is an important skill for second language learners. Similarly, source language interference is a problem often experienced by translators that is partly caused by the influence of cognates and false friends. Research in natural language processing can address these bottlenecks by, for instance, developing computer tools that aid second language users. Nevertheless, most studies have mainly focused on the detection of cognates (Bergsma and Kondrak, 2007; Hauer and Kondrak, 2011; Ciobanu and Dinu, 2014; Rama, 2016), while relatively little attention has been devoted to false friends (Frunza and Inkpen, 2007; Mitkov et al., 2007; Ljubeˇsi´c and Fiˇser, 2013; Castro et al., 2018). Mitkov (2007) explains that the main goal of investigation is often the cross-lingual identification of equivalent lexical items, as such knowledge can be integrated in other applications. Automatic cognate detection has indeed proven very useful for NLP, e.g. to boost the performance of automatic alignment between related languages or to compile bilingual lexicons (Smith et al., 2017). The aim of this research is twofold: (1) we introduce a context-independent gold standard which can be used to classify English-Dutch pa"
R19-1071,J99-1003,0,0.060422,"and false friends are hard to find and expensive to compose, since they require a considerable amount of time and effort from trained lexicographers (Schepens et al., 2012). Especially for low resource languages, this constitutes a serious issue. Therefore, most NLP research on cognates has mainly focused on the automatic detection of such cognate pairs. In the literature, there are three main methods to identify cognates: orthographic, phonetic and semantic approaches. The oldest approaches to tackle this task involve simple string similarity metrics as the longest common subsequence ratio (Melamed, 1999) or the normalized Levenshtein distance (Levenshtein, 1965). More recently, however, the attention has been drawn to machine learning techniques. For instance, Frunza et al. (2007) combine several orthographic similarity measures to train a machine classifier, while Gomes et al. (2011) design a new similarity metric that is able to learn spelling differences across languages. Different types of approaches can also be combined to distinguish cognates, e.g. Kondrak et al. (2004) join orthographic and phonetic information to distinguish between similar drug names. In order to capture the phonetic"
R19-1071,J03-1002,0,0.0104988,"ade between cognates of which Part-of-Speech (PoS) and meaning are identical in both languages, cognates that differ in PoS (e.g. organisatieorganizing) and cognates that differ in agreement (e.g. organisatie-organisations). Secondly, it is important to note that a successful dictionary lookup never overruled the “proper name” annotation. The resulting gold standard is contextindependent. Hence, it can be used for both the development and the evaluation of machine List of Candidate Cognate Pairs To select a list of candidate cognate pairs, unsupervised statistical word alignment using GIZA++ (Och and Ney, 2003) was applied on the Dutch Parallel Corpus (DPC). This high-quality parallel corpus for Dutch, French and English consists of more than ten million words and is sentencealigned. It contains five different text types and is balanced with respect to text type and translation direction. The automatic word alignment on the English-Dutch part of the DPC resulted in a list containing more than 500,000 translation equivalents. A first selection was performed by applying the Normalized Levenshtein Distance (NLD) (as implemented by Gries (2004)) on this list of translation equivalents and only consideri"
R19-1071,I11-1097,0,0.0197521,"ch have similar forms, but which differ in their meaning. The ability to distinguish cognates from noncognates (and especially) false friends is an important skill for second language learners. Similarly, source language interference is a problem often experienced by translators that is partly caused by the influence of cognates and false friends. Research in natural language processing can address these bottlenecks by, for instance, developing computer tools that aid second language users. Nevertheless, most studies have mainly focused on the detection of cognates (Bergsma and Kondrak, 2007; Hauer and Kondrak, 2011; Ciobanu and Dinu, 2014; Rama, 2016), while relatively little attention has been devoted to false friends (Frunza and Inkpen, 2007; Mitkov et al., 2007; Ljubeˇsi´c and Fiˇser, 2013; Castro et al., 2018). Mitkov (2007) explains that the main goal of investigation is often the cross-lingual identification of equivalent lexical items, as such knowledge can be integrated in other applications. Automatic cognate detection has indeed proven very useful for NLP, e.g. to boost the performance of automatic alignment between related languages or to compile bilingual lexicons (Smith et al., 2017). The a"
R19-1071,A00-2038,0,0.397501,"Missing"
R19-1071,C04-1137,0,0.145509,"Missing"
R19-1117,W16-4702,0,0.285304,"Missing"
R19-1117,W94-0104,0,0.722234,"preliminary list of cts is produced based on partof-speech (POS) patterns. Next, statistical metrics are applied to measure termhood (to what degree a term is related to the domain) and unithood for multi-word terms (whether the individual tokens combine to form a lexical unit) (Kageura and Umino, 1996). These metrics are used to sort the cts based on their likelihood to be actual terms. To filter the list, one can either determine a cut-off value or select the top n or top n percent of terms. As a final step, manual validation is required. This has been a standard methodology for some time (Daille, 1994) and is still used by state-ofthe-art systems such as TermoStat (Drouin, 2003) and TExSIS (Macken et al., 2013). However, the problem with these methodologies is determining the cut-off point (Lopes and Vieira, 2015) and combining multiple features (e.g., separate measures for termhood and unithood). It has become clear that multiple evidence (i.e. combining multiple features) is highly beneficial for ATE (Dobrov and Loukachevitch, 2011; Loukachevitch, 2012). Supervised machine learning (ML) methodologies are now being used in answer to these problems. By automatically learning an optimal comb"
R19-1117,R11-1103,0,0.168259,"her determine a cut-off value or select the top n or top n percent of terms. As a final step, manual validation is required. This has been a standard methodology for some time (Daille, 1994) and is still used by state-ofthe-art systems such as TermoStat (Drouin, 2003) and TExSIS (Macken et al., 2013). However, the problem with these methodologies is determining the cut-off point (Lopes and Vieira, 2015) and combining multiple features (e.g., separate measures for termhood and unithood). It has become clear that multiple evidence (i.e. combining multiple features) is highly beneficial for ATE (Dobrov and Loukachevitch, 2011; Loukachevitch, 2012). Supervised machine learning (ML) methodologies are now being used in answer to these problems. By automatically learning an optimal combination of features and cut-off points, many features can be efficiently combined. One of the biggest hurdles for the progress of ATE technologies has been the data acquisition bottleneck, both for evaluation and now also as training data. Manually annotating terms is a slow and arduous task, with notoriously low interannotator agreement due to the ambiguous nature of terms. This lack of agreement on the basic characteristics of terms i"
R19-1117,loukachevitch-2012-automatic,0,0.668372,"select the top n or top n percent of terms. As a final step, manual validation is required. This has been a standard methodology for some time (Daille, 1994) and is still used by state-ofthe-art systems such as TermoStat (Drouin, 2003) and TExSIS (Macken et al., 2013). However, the problem with these methodologies is determining the cut-off point (Lopes and Vieira, 2015) and combining multiple features (e.g., separate measures for termhood and unithood). It has become clear that multiple evidence (i.e. combining multiple features) is highly beneficial for ATE (Dobrov and Loukachevitch, 2011; Loukachevitch, 2012). Supervised machine learning (ML) methodologies are now being used in answer to these problems. By automatically learning an optimal combination of features and cut-off points, many features can be efficiently combined. One of the biggest hurdles for the progress of ATE technologies has been the data acquisition bottleneck, both for evaluation and now also as training data. Manually annotating terms is a slow and arduous task, with notoriously low interannotator agreement due to the ambiguous nature of terms. This lack of agreement on the basic characteristics of terms is also reflected in th"
R19-1117,W18-4909,0,0.243061,"Missing"
R19-1117,C14-1029,0,0.415532,"Missing"
R19-1117,L16-1294,0,0.131607,"Missing"
R19-1117,2011.eamt-1.31,0,0.0317846,"d and elaborate analysis and comparison of a traditional, state-ofthe-art system (TermoStat) and a new, supervised ML approach (HAMLET), using the results obtained for the same, manually annotated, Dutch corpus about dressage. 1 Introduction Automatic term extraction (ATE), also known as automatic term recognition (ATR), has long been an established task within the field of natural language processing. It can be used both in its own right, to automatically obtain a list of candidate terms (cts) from a specialised corpus, or as a preprocessing step for other tasks, such as machine translation (Wolf et al., 2011). The traditional method for ATE is a hybrid approach, combining both linguistic and statistical information. In a first step, linguistic preprocessing is performed and a preliminary list of cts is produced based on partof-speech (POS) patterns. Next, statistical metrics are applied to measure termhood (to what degree a term is related to the domain) and unithood for multi-word terms (whether the individual tokens combine to form a lexical unit) (Kageura and Umino, 1996). These metrics are used to sort the cts based on their likelihood to be actual terms. To filter the list, one can either det"
S07-1019,S07-1012,0,0.326078,"rices. The classification and clustering experiments, and the final combination of the different outputs are discussed in Section 3. Section 4 gives an overview of the results on the test data and Section 5 summarizes the main findings of the paper. 105 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 105–108, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Data sets and feature construction The data we have used for training our system were made available in the framework of the SemEval (task 13: Web People Search) competition (Artiles et al., 2007). As preliminary training corpus (referred to as “trial data” in our article), we used the WePS corpus (Web People Search corpus), available at http://nlp.uned.es/weps. For the real training set, this trial set was expanded in order to cover different degrees of ambiguity (very common names, uncommon names and celebrity names which tend to monopolize search results). The training corpus is composed of 40 sets of 100 web pages, each set corresponding to the first 100 results for a person name query. The documents were manually clustered. Documents that couldn’t be clustered properly have been p"
S07-1019,P98-1012,0,0.35216,"n Finding information about people on the World Wide Web is one of the most popular activities of Internet users. Given the high ambiguity of person names and the increasing amount of information on the web, it becomes very important to organize this large amount of information into meaningful clusters referring each to one single individual. The problem of resolving name ambiguity on the Internet has been approached from different angles. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) Timur Fayruzov Computational Web Intelligence Ghent University Association Krijgslaan 281, 9000 Gent Timur.Fayruzov@UGent.be have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. Documents get clustered using the standard vector space model. Other researchers have taken this search for distinctive keywords one step further and tried to come up with “concepts” describing the documents. Fleischman and Hovy (2004) introduce the “maximum entropy model”: a binary classifier"
S07-1019,W04-0701,0,0.0273924,"ace combining biographic facts and associated names, whereas Bagga and Baldwin (1998) Timur Fayruzov Computational Web Intelligence Ghent University Association Krijgslaan 281, 9000 Gent Timur.Fayruzov@UGent.be have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. Documents get clustered using the standard vector space model. Other researchers have taken this search for distinctive keywords one step further and tried to come up with “concepts” describing the documents. Fleischman and Hovy (2004) introduce the “maximum entropy model”: a binary classifier determines whether two concept-instance pairs refer to the same individual. Pedersen (2006) presented an unsupervised approach using bigrams in the contexts to be clustered, thus aiming at a concept level semantic space instead of a word level feature space. For the semeval contest, we approached the task from a double supervised and unsupervised perspective. For the supervised classification, the task was redefined in the form of feature vectors containing disambiguating information on pairs of documents. In addition to this, differe"
S07-1019,W03-0405,0,0.11896,"t a combined classification and clustering approach doesn’t always compare favorably to those obtained by the different algorithms separately. 1 Introduction Finding information about people on the World Wide Web is one of the most popular activities of Internet users. Given the high ambiguity of person names and the increasing amount of information on the web, it becomes very important to organize this large amount of information into meaningful clusters referring each to one single individual. The problem of resolving name ambiguity on the Internet has been approached from different angles. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) Timur Fayruzov Computational Web Intelligence Ghent University Association Krijgslaan 281, 9000 Gent Timur.Fayruzov@UGent.be have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. Documents get clustered using the standard vector space model. Other researchers have taken this search for distinctive keywords one step"
S07-1019,C98-1012,0,\N,Missing
S10-1003,W09-2413,1,0.714521,"-lingual WSD task: (a) a parallel corpus on the basis of which the gold standard sense inventory was created and (b) a collection of English sentences containing the lexical sample words annotated with their contextually appropriate translations in five languages. 1 http://www.statmt.org/europarl/ 15 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 15–20, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Below, we provide a short summary of the complete data construction process. For a more detailed description, we refer to Lefever and Hoste (2009; 2010). across languages on the basis of unique sentence IDs. After the selection of all unique translation combinations, the translations were grouped into clusters. The clusters were organized in two levels, in which the top level reflects the main sense categories (e.g. for the word coach we have (1) (sports) manager, (2) bus, (3) carriage and (4) part of a train), and the subclusters represent the finer sense distinctions. Translations that correspond to English multiword units were identified and in case of non-apparent compounds, i.e. compounds which are not marked with a “-”, the diffe"
S10-1003,lefever-hoste-2010-construction,1,0.801637,"Missing"
S10-1003,S07-1009,0,0.0454523,"h.n.de 12 :: Coach 1; Fußbaltrainer 1; P Nationaltrainer 2; Trainer 3; P coach.n.it 12 :: allenatore 3; P rec = coach.n.es 12 :: entrenador 3; 3 3.1 a i :i∈A |A| P P Evaluation Rec = Scoring res∈a i f req res |ai | |H i | (1) res∈a i f req res |ai | a i :i∈T |T | |H i | (2) Out-of-five (Oof ) evaluation For the more relaxed evaluation, systems can propose up to five guesses. For this evaluation, the resulting score is not divided by the number of guesses. To score the participating systems, we use an evaluation scheme which is inspired by the English lexical substitution task in SemEval 2007 (McCarthy and Navigli, 2007). We perform both a best result evaluation and a more relaxed evaluation for the top five results. The evaluation is performed using precision and recall (P rec and Rec in the equations below), and Mode precision (M P ) and Mode recall (M R ), where we calculate precision and recall against the translation that is preferred by the majority of annotators, provided that one translation is more frequent than the others. For the precision and recall formula we use the following variables. Let H be the set of annotators, T the set of test items and hi the set of responses for an item i ∈ T for anno"
S10-1003,S07-1001,0,0.0656999,"Missing"
S10-1003,E09-1010,0,0.0316399,"necks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993; Ide et al., 2002; Ng et al., 2003; Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted 2 Task setup 2.1 Data sets Two types of data sets were used in the Cross-lingual WSD task: (a) a parallel corpus on the basis of which the gold standard sense inventory was created and (b) a collection of English sentences containing the lexical sample words annotated with their contextually appropriate translations in five languages. 1 http://www.statmt.org/europarl/ 15 Proceedings of the 5th International Workshop on Se"
S10-1003,P03-1058,0,0.219148,"ses on two bottlenecks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993; Ide et al., 2002; Ng et al., 2003; Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted 2 Task setup 2.1 Data sets Two types of data sets were used in the Cross-lingual WSD task: (a) a parallel corpus on the basis of which the gold standard sense inventory was created and (b) a collection of English sentences containing the lexical sample words annotated with their contextually appropriate translations in five languages. 1 http://www.statmt.org/europarl/ 15 Proceedings of the 5th Internati"
S10-1003,P91-1023,0,0.279579,"compounds which are not marked with a “-”, the different compound parts were separated by §§ in the clustering file (e.g. the German Post§§kutsche). All clustered translations were also manually lemmatized. The gold standard sense inventory was derived from the Europarl parallel corpus2 , which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus, viz. English (our target language), Dutch, French, German, Italian and Spanish. All data were already sentence-aligned using a tool based on the Gale and Church (1991) algorithm, which was part of the Europarl corpus. We only considered the 1-1 sentence alignments between English and the five other languages. These sentence alignments were made available to the task participants for the five trial words. The sense inventory extracted from the parallel data set (Section 2.2) was used to annotate the sentences in the trial set and the test set, which were extracted from the JRC-ACQUIS Multilingual Parallel Corpus3 and BNC4 . 2.2 2.3 Sense annotation of the test data The resulting sense inventory was used to annotate the sentences in the trial set (20 sentence"
S10-1003,J03-1002,0,0.0101799,"were also allowed to provide fewer. These potentially different translations were used to assign frequency weights (shown in example (2)) to the gold standard translations per sentence. The example (1) below shows the annotation result in both German and Dutch for an English source sentence containing coach. Creation of the sense inventory Two steps were taken to obtain a multilingual sense inventory: (1) word alignment on the sentences to find the set of possible translations for the set of ambiguous nouns and (2) clustering by meaning (per target word) of the resulting translations. GIZA++ (Och and Ney, 2003) was used to generate the initial word alignments, which were manually verified by certified translators in all six involved languages. The human annotators were asked to assign a “NULL” link to words for which no valid translation could be identified. Furthermore, they were also asked to provide extra information on compound translations (e.g. the Dutch word Investeringsbank as a translation of the English multiword Investment Bank ), fuzzy links, or target words with a different PoS (e.g. the verb to bank ). The manually verified translations were clustered by meaning by one annotator. In or"
S10-1003,W09-2412,0,0.123758,"Missing"
S10-1003,W02-0808,0,0.177975,"iguation task focuses on two bottlenecks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993; Ide et al., 2002; Ng et al., 2003; Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted 2 Task setup 2.1 Data sets Two types of data sets were used in the Cross-lingual WSD task: (a) a parallel corpus on the basis of which the gold standard sense inventory was created and (b) a collection of English sentences containing the lexical sample words annotated with their contextually appropriate translations in five languages. 1 http://www.statmt.org/europarl/ 15 Proceedings of"
S10-1003,2005.mtsummit-papers.11,0,0.0996708,"(sports) manager, (2) bus, (3) carriage and (4) part of a train), and the subclusters represent the finer sense distinctions. Translations that correspond to English multiword units were identified and in case of non-apparent compounds, i.e. compounds which are not marked with a “-”, the different compound parts were separated by §§ in the clustering file (e.g. the German Post§§kutsche). All clustered translations were also manually lemmatized. The gold standard sense inventory was derived from the Europarl parallel corpus2 , which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus, viz. English (our target language), Dutch, French, German, Italian and Spanish. All data were already sentence-aligned using a tool based on the Gale and Church (1991) algorithm, which was part of the Europarl corpus. We only considered the 1-1 sentence alignments between English and the five other languages. These sentence alignments were made available to the task participants for the five trial words. The sense inventory extracted from the parallel data set (Section 2.2) was used to annotate the sentences in"
S13-2029,E09-1010,0,0.106415,"y, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task. The remainder of this paper is organized as follows. Section 2 focuses on the task description and briefly recapitalizes the construction of the sense inventory and the annotation procedure of the test sentences. Section 3 presents the participating sy"
S13-2029,S13-2032,0,0.0479872,"cond and best performing - flavor of the system (SnT run) calculates the cosine similarity between the context words of the test and training sentences. The output of the system then contains the translation that results from running word alignment on the focus word in the training corpus. As a fallback, WordNet is again used. The WN senses are sorted by frequency in the SemCor corpus and the corresponding translation is selected from the aligned WordNet in the target language. The third run of the system (merged) combines the output from the other two flavors of the system. The LIMSI system (Apidianaki, 2013) applies an unsupervised CLWSD method that was proposed in (Apidianaki, 2009) for three target languages, viz. Spanish, Italian and French. First, word alignment is applied on the parallel corpus and three bilingual lexicons are built, containing for each focus word the translations in the three target languages. In a next step, a vector is built for each translation of the English focus word, using the cooccurrences of the word in the sentences in which it gets this particular translation. A clustering algorithm then groups the feature vectors using the Weighted Jaccard measure. New instances"
S13-2029,P91-1034,0,0.696471,"owards a specific target domain or application and to train a dedicated CLWSD system using these particular sense inventories. Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task. The remainder of this paper is organized as follows. Section 2 focuses on the task description and briefly recapitalizes"
S13-2029,D07-1007,0,0.0924441,"for a particular word that should fit all possible domains and applications. In addition, the use of domain-specific corpora allows to derive sense inventories that are tailored towards a specific target domain or application and to train a dedicated CLWSD system using these particular sense inventories. Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed co"
S13-2029,S13-2034,0,0.0397537,"uses local context features for a window of three words containing the focus word. Parameters were optimized on the trial data. The second flavor of the system (c1lN) uses the same configuration of the system, but without parameter optimization. The third configuration of the system (var) is heavily optimized on the trial data, selecting the winning configuration per trial word and evaluation metric. In addition to the local context features, also global bag-of-word context features are considered for this version of the system. A completely different approach is taken by the NRC-SMT system (Carpuat, 2013), that uses a statistical machine translation approach to tackle the CLWSD task. The baseline version of the system (SMTbasic) represents a standard phrase-based SMT baseline, that is trained only on the intersected Europarl corpus. Translations for the test instances are extracted from the top hypothesis (for the best evaluation) or from the 100-best list (for the Out-of-five evaluation). The optimized version of the system (SMTadapt2) is trained on the Europarl corpus and additional news data, and uses mixture models that are developed for domain adaptation in SMT. In addition to the five sy"
S13-2029,J93-1004,0,0.584128,"rget domain or application and to train a dedicated CLWSD system using these particular sense inventories. Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task. The remainder of this paper is organized as follows. Section 2 focuses on the task description and briefly recapitalizes the construction of th"
S13-2029,lefever-hoste-2010-construction,1,0.906382,"ate.google.com 3 http://be.bing.com/translator/ 2 language technology today. It requires the construction of an artificial text understanding as the system should detect the correct word sense based on the context of the word. Different methodologies have been investigated to solve the problem; see for instance Agirre and Edmonds (2006) and Navigli (2009) for a detailed overview of WSD algorithms and evaluation. This paper reports on the second edition of the “Cross-Lingual Word Sense Disambiguation” (CLWSD) task, that builds further on the insights we gained from the SemEval-2010 evaluation (Lefever and Hoste, 2010b) and for which new test data were annotated. The task is an unsupervised Word Sense Disambiguation task for English nouns, the sense label of which is composed of translations in different target languages (viz. French, Italian, Spanish, Dutch and German). The sense inventory is built up on the basis of the Europarl parallel corpus; all translations of a polysemous word were manually grouped into clusters, which constitute different senses of that given word. For the test data, native speakers assigned a translation cluster(s) to each test sentence and gave their top three translations from"
S13-2029,S10-1003,1,0.559637,"ate.google.com 3 http://be.bing.com/translator/ 2 language technology today. It requires the construction of an artificial text understanding as the system should detect the correct word sense based on the context of the word. Different methodologies have been investigated to solve the problem; see for instance Agirre and Edmonds (2006) and Navigli (2009) for a detailed overview of WSD algorithms and evaluation. This paper reports on the second edition of the “Cross-Lingual Word Sense Disambiguation” (CLWSD) task, that builds further on the insights we gained from the SemEval-2010 evaluation (Lefever and Hoste, 2010b) and for which new test data were annotated. The task is an unsupervised Word Sense Disambiguation task for English nouns, the sense label of which is composed of translations in different target languages (viz. French, Italian, Spanish, Dutch and German). The sense inventory is built up on the basis of the Europarl parallel corpus; all translations of a polysemous word were manually grouped into clusters, which constitute different senses of that given word. For the test data, native speakers assigned a translation cluster(s) to each test sentence and gave their top three translations from"
S13-2029,S07-1009,0,0.0335916,"osch, 2005). As most classifiers can be initialized with a wide range of parameters, we used a genetic algorithm to optimize the parameter settings for our classification task. 4 Results 4.1 Experimental set up Test set The lexical sample contains 50 English sentences per ambiguous focus word. All instances were manually annotated per language, which resulted in a set of gold standard translation labels per instance. For the construction of the test dataset, we refer to Section 2. 7 http://code.google.com/apis/language/ Evaluation metric The BEST precision and recall metric was introduced by (McCarthy and Navigli, 2007) in the framework of the SemEval-2007 competition. The metric takes into account the frequency weights of the gold standard translations: translations that were picked by different annotators received a higher associated frequency which is incorporated in the formulas for calculating precision and recall. For the BEST precision and recall evaluation, the system can propose as many guesses as the system believes are correct, but the resulting score is divided by the number of guesses. In this way, systems that output many guesses are not favored and systems can maximize their score by guessing"
S13-2029,P03-1058,0,0.075708,"ion and to train a dedicated CLWSD system using these particular sense inventories. Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task. The remainder of this paper is organized as follows. Section 2 focuses on the task description and briefly recapitalizes the construction of the sense inventory"
S13-2029,S13-2031,0,0.0989072,"Missing"
S13-2029,P07-1006,0,0.0305196,"e inventories. Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task. The remainder of this paper is organized as follows. Section 2 focuses on the task description and briefly recapitalizes the construction of the sense inventory and the annotation procedure of the test sentences. Section 3 presents t"
S13-2029,S13-2030,0,0.025878,"uggestions. Both evaluation tasks are explained in more detail in Section 4.1. 3 Systems 3.1 Systems participating to the official CLWSD evaluation campaign Five different research teams participated to the CLWSD task and submitted up to three different runs of their system, resulting in 12 different submissions for the task. All systems took part in both the best and the Out-of-five evaluation tasks. These systems took very different approaches to solve the task, ranging from statistical machine translation, classification and sense clustering to topic model based approaches. The XLING team (Tan and Bond, 2013) submitted three runs of their system for all five target languages. The first version of the system presents a 6 http://www.americannationalcorpus.org/ 160 topic matching and translation approach to CLWSD (TnT run), where LDA is applied on the Europarl sentences containing the ambiguous focus word in order to train topic models. Each sentence in the training corpus is assigned a topic that contains a list of associated words with the topic. The topic of the test sentence is then inferred and compared to the matching training sentences by means of the cosine similarity between the training and"
S13-2029,C04-1192,0,0.0826561,"Missing"
S13-2029,S13-2033,0,0.0269961,"Missing"
S14-2005,S13-2029,1,0.709155,"y, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.” Desired output: “I return home because I am tired.” • Input (L1=Dutch, L2=English): “Workers are facing a massive aanval op their employment and social rights.” Desired output: “Workers are facing a massive attack on their employment and social rights.” The task can be related to two tasks that were offered in previous years of SemEval: Lexical Substitution (Mihalcea et al., 2010) and most notably Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2013). When comparing our task to the Cross-Lingual Word-Sense Disambiguation task, one notable difference is the fact that our task concerns not just words, but also phrases. Another essential difference is the nature of the context; our context is in L2 instead of L1. Unlike the Cross-Lingual Word Sense Disambiguation task, we do not constrain the L1 words or phrases that may be used for translation, except for a maximum length which we set to 6 tokens, whereas Lefever and Hoste (2013) only tested a select number of nouns. Our task emphasizes a correct meaning-preserving choice 37 site1 . We crea"
S14-2005,S10-1002,0,0.0325185,"k to L1 in an L2 context. The full L2 sentence acts as reference sentence. Finally, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.” Desired output: “I return home because I am tired.” • Input (L1=Dutch, L2=English): “Workers are facing a massive aanval op their employment and social rights.” Desired output: “Workers are facing a massive attack on their employment and social rights.” The task can be related to two tasks that were offered in previous years of SemEval: Lexical Substitution (Mihalcea et al., 2010) and most notably Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2013). When comparing our task to the Cross-Lingual Word-Sense Disambiguation task, one notable difference is the fact that our task concerns not just words, but also phrases. Another essential difference is the nature of the context; our context is in L2 instead of L1. Unlike the Cross-Lingual Word Sense Disambiguation task, we do not constrain the L1 words or phrases that may be used for translation, except for a maximum length which we set to 6 tokens, whereas Lefever and Hoste (2013) only tested a select number o"
S14-2005,S14-2030,0,0.0733812,"nd Nanyang Technological University (Singapore) – all language pairs 6. TeamZ - Anubhav Gupta - Universit´e de Franche-Comt´e (France) – English-Spanish, English-German Participants implemented distinct methodologies and implementations. One obvious avenue of tackling the problem is through standard Statistical Machine Translation (SMT). The CNRC team takes a pure SMT approach with few modifications. They employ their own Portage decoder and directly send an L1 fragment in an L2 context, corresponding to a partial translation hypothesis with only one fragment left to decode, to their decoder (Goutte et al., 2014). The UEdin team applies a similar method using the Moses decoder, marking the L2 context so that the decoder leaves this context as is. In addition they add a context similarity feature for every phrase pair in the phrase translation table, which expresses topical similarity with the test context. In order to properly decode, the phrase table is filtered per test sentence (Hasler, 2014). The IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-l"
S14-2005,S14-2110,0,0.0209095,"uesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. CNRC - Cyril Goutte, Michel Simard, Marine Carpuat - National Research Council Canada – All language pairs 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 higher. These scores can be viewed on the result website at http://github.com/proycon/ semeval2014task5/. The result website also holds the system output and evaluation scripts with which all graphs and tables can be reproduced. We observe that the best scoring team in the task (UEdin), as well as the CNRC team, both employ standard Statistical Machine Translation and achieve high results. From this we can conclude that standard SMT techniques are suitable for this task. Teams IUCL and UNAL achieve similarly good results, building on word and phrase alignment data as does SMT, yet not usin"
S14-2005,S14-2060,0,0.037367,"Missing"
S14-2005,S14-2123,0,0.0228464,"tions. They employ their own Portage decoder and directly send an L1 fragment in an L2 context, corresponding to a partial translation hypothesis with only one fragment left to decode, to their decoder (Goutte et al., 2014). The UEdin team applies a similar method using the Moses decoder, marking the L2 context so that the decoder leaves this context as is. In addition they add a context similarity feature for every phrase pair in the phrase translation table, which expresses topical similarity with the test context. In order to properly decode, the phrase table is filtered per test sentence (Hasler, 2014). The IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-linear model: phrase table, L2 Language Model, Multilingual Dictionary, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve t"
S14-2005,S14-2132,0,0.0206542,"IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-linear model: phrase table, L2 Language Model, Multilingual Dictionary, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve this (Silva-Schlenker et al., 2014). The two systems on the lower end of the result spectrum use different techniques altogether. The Sensible team approaches the problem • Best - The system may only output one, its best, translation; • Out of Five - The system may output up to five alternatives, effectively allowing 5 guesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. C"
S14-2005,P07-2045,0,0.0150004,"Missing"
S14-2005,S14-2094,0,0.0289288,"- The system may output up to five alternatives, effectively allowing 5 guesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. CNRC - Cyril Goutte, Michel Simard, Marine Carpuat - National Research Council Canada – All language pairs 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 higher. These scores can be viewed on the result website at http://github.com/proycon/ semeval2014task5/. The result website also holds the system output and evaluation scripts with which all graphs and tables can be reproduced. We observe that the best scoring team in the task (UEdin), as well as the CNRC team, both employ standard Statistical Machine Translation and achieve high results. From this we can conclude that standard SMT techniques are suitable for this task. Teams IUCL and UNAL achieve similarly good results,"
S14-2005,P14-1082,1,0.682767,"Missing"
S14-2005,2005.mtsummit-papers.11,0,0.00919859,"d in a simple XML format that explicitly marks the fragments. System output of participants adheres to the same format. The trial set, released early on in the task, was used by participants to develop and tune their systems on. The test set corresponds to the final data released for the evaluation period; the final evaluation was conducted on this data. The trial data was constructed in an automated fashion in the way described in our pilot study (van Gompel and van den Bosch, 2014). First a phrase-translation table is constructed from a parallel corpus. We used the Europarl parallel corpus (Koehn, 2005) and the Moses tools (Koehn et al., 2007), which in turn makes use of GIZA++ (Och and Ney, 2000). Only strong phrase pairs (exceeding a set threshold) were retained and weaker ones were pruned. This phrase-translation table was then used to create input sentences in which the L2 fragments are swapped for their L1 counterparts, effectively mimicking a fall-back to L1 in an L2 context. The full L2 sentence acts as reference sentence. Finally, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.”"
S14-2070,S13-2053,0,0.0275992,"9 0.8502 (+ 0.0013) 0.8582 (+ 0.0080) 0.8654 (+ 0.0072) 0.8650 (- 0.0004) 0.8660 (+ 0.0010) 0.8660 (=) 0.8654 (- 0.0006) Table 2: F-scores obtained after adding other features for the Twitter and SMS development data (test data SemEval-2013) – subtask A. PMI features: PMI (pointwise mutual information) values indicating the association of a word with positive and negative sentiment. The higher the PMI value, the stronger the word-sentiment association. For each unigram and bigram in the training data, PMI values were extracted from the word-sentiment association lexicon created by NRC Canada (Mohammad et al., 2013). A second PMI feature was considered for each unigram based on the word-sentiment associations found in the SemEval-2014 training dataset. PMI values were calculated as follows: Features lexicons n-grams n-grams + lexicons + normalization n-grams + Part-of-Speech + negation + word shape + named entity + dependency + PMI Dev Twitter 0.5342 0.5896 0.6442 0.6414 (- 0.0028) 0.6466 (+ 0.0052) 0.6542 (+ 0.0076) 0.6581 (+ 0.0039) 0.6559 (- 0.0022) 0.6467 (- 0.0092) 0.6525 (+ 0.0058) Dev SMS 0.5119 0.5628 0.6040 0.6084 (+ 0.0044) 0.6333 (+ 0.0249) 0.6384 (+ 0.0051) 0.6394 (+ 0.0010) 0.6399 (+ 0.0005)"
S14-2070,de-marneffe-etal-2006-generating,0,0.0572289,"Missing"
S14-2070,S13-2052,0,0.0604941,"Missing"
S14-2070,P11-2008,0,0.25856,"Missing"
S14-2070,D11-1141,0,0.0392706,"to Helsinki tomorrow or on the day after tomorrow, yay!). Linguistic Preprocessing First, we performed manual cleaning on the datasets to replace non-UTF-8 characters, and we tokenized all messages using the Carnegie Mellon University Twitter Part-of-Speech Tagger (Gimpel et al., 2011). Subsequently, we Part-of-Speech tagged all instances using the CMU Twitter Partof-Speech Tagger (Gimpel et al., 2011), and performed dependency parsing using a caseless parsing model of the Stanford parser (de Marneffe et al., 2006). Besides that, we also tagged all named entities using the Twitter NLP tools (Ritter et al., 2011) for Named Entity Recognition. As a final preprocessing step, we decided to combine the labels neutral, objective and neutral-OR-objective, thus recasting the task as a three-way classification task. 2.2 (within word tokens) found in the training data. • The number of capitalized words (e.g. SO EXCITED). • The number of hashtags (e.g. #win). Lexicon features: As sentiment lexicons we consulted existing resources: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), MPQA (Wilson et al., 2005), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), Bing Liu (Hu and Liu, 2004)"
S14-2070,P09-2079,0,0.067791,"Missing"
S14-2070,W10-0204,0,0.0630392,"also tagged all named entities using the Twitter NLP tools (Ritter et al., 2011) for Named Entity Recognition. As a final preprocessing step, we decided to combine the labels neutral, objective and neutral-OR-objective, thus recasting the task as a three-way classification task. 2.2 (within word tokens) found in the training data. • The number of capitalized words (e.g. SO EXCITED). • The number of hashtags (e.g. #win). Lexicon features: As sentiment lexicons we consulted existing resources: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), MPQA (Wilson et al., 2005), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), Bing Liu (Hu and Liu, 2004), and Bounce (K¨okciyan et al., 2013) – the latter three are Twitter-specific. Additionally, we created a list of emoticons extracted from the SemEval-2014 training data. Based on these resources, the following features were extracted: Feature Extraction • The number of positive, negative, and neutral lexicon words averaged over text length We implemented a number of lexical and syntactic features that represent every phrase (subtask A) or message (subtask B) within a feature vector: • The overall polarity, which is the sum of the values o"
S14-2070,W11-1709,0,0.312809,"services, social networking sites, and short messaging services have considerably increased the amount of user-generated content produced online. Millions of people rely on these services to send messages, share their views or gather information about others. Simultaneously, companies, marketeers and politicians are anxious to detect sentiment in UGC since these messages might contain valuable information about the public opinion. This explains why sentiment analysis has been a research area of great interest in the last few years (Wiebe et al., 2005; Wilson et al., 2005; Pang and Lee, 2008; Mohammad and Yang, 2011). Though first studies focussed more on product or movie reviews, we see that analyzing sentiment in UGC is currently becoming increasingly popular. The main difference between these two sources of information is that the former is rather long and contains quite formal language whereas the latter one is generally very brief and noisy and thus represents some different challenges (Maynard et al., 2012). In this paper, we describe our contribution to the SemEval-2014 Task 9: Sentiment Analysis in The datasets for training, development and testing were provided by the task organizers. The trainin"
S14-2070,H05-1044,0,\N,Missing
S14-2070,S14-2009,0,\N,Missing
S14-2070,S13-2093,0,\N,Missing
S15-2115,de-marneffe-etal-2006-generating,0,0.00967202,"Missing"
S15-2115,S15-2080,0,0.042618,"Missing"
S15-2115,P11-2008,0,0.04267,"Missing"
S15-2115,maynard-greenwood-2014-cares,0,0.0752971,"Missing"
S15-2115,D11-1141,0,0.055131,"raction. 1 As some tweets were made inaccessible by their creators, we were able to download only 914 of them 685 2.1 Linguistic Preprocessing All tweets were tokenized and PoS-tagged using the Carnegie Mellon University Twitter Part-of-SpeechTagger (Gimpel et al., 2011). Lemmatization was done using the LT3 LeTs Preprocess Toolkit (Van de Kauter et al., 2013). We used a caseless parsing model of the Stanford parser (de Marneffe et al., 2006) for a dependency representation of the messages. As a final step, we tagged all named entities using the Twitter NLP tools for Named Entity Recognition (Ritter et al., 2011). 2.2 Features As a first step, we implemented a set of features that have shown to perform well for sentiment classification in previous research (Van Hee et al., 2014). These include word-based features (e.g. bagof-words), lexical features (e.g. character flooding), sentiment features (e.g. an overall sentiment score per tweet, based on existing sentiment lexicons), and syntactic features (e.g. dependency relation features)2 . To provide some abstraction, we also added PoS n-gram features to the set of bag-of-words features. Nevertheless, as a substantial part of the data we are confronted w"
S15-2115,P14-1024,0,0.0387703,"Missing"
S15-2115,D11-1063,0,0.0543312,"Missing"
S15-2115,S14-2070,1,0.840414,"ntiment (i.e. at least one positive and one negative sentiment word) is contained by the instance. Interjection Count – Numeric feature indicating how many interjections are contained by an instance. This value is normalized by dividing it by the number of tokens in the instance. As stated by (Carvalho et al., 2009), interjections may be potential clues for irony detection. Sarcasm Hashtag – Binary feature indicating whether an instance contains a hashtag that may indicate the presence of sarcasm. To this end, a list of 2 For a detailed description of these features we refer to Van Hee et al. (2014). 3 A number of these features (i.e. contradiction, sudden change, and temporal imbalance) are inspired by Reyes et al. (2013). ≈ 100 sarcasm-related hashtags was extracted from the training data. Punctuation Mark Count – Normalized numeric feature indicating the number of punctuation marks that are contained by an instance. Emoticon count – Normalized numeric feature indicating the number of emoticons that are contained by an instance. Contradiction – Binary feature that indicates whether an instance contains a linguistic contradiction marker (i.e. words like nonetheless, yet, however). Sudde"
S15-2122,J92-4003,0,0.164496,"atures consist of: 1. WordNet features: for each main category, a value is derived indicating the number of (unique) terms annotated as aspect terms from that category in the training data that (1) co-occur in the synset of the candidate term or (2) which are a hyponym/hypernym of a term in the synset. In case the candidate term is a multi-word term whose full term is not found, this value is calculated for all nouns in the multi-word term and the resulting sum is divided by the number of nouns. 2. Cluster features: using the implementation of the Brown hierarchical word clustering algorithm (Brown et al., 1992) by Liang (2005), we derived clusters from the Yelp dataset1 . Then, we derived for each main category a value indicating the number of (unique) terms annotated as aspect terms from that category in the training data that co-occur with the candidate term in the same cluster. Since clusters can only contain single words, we calculate this value for all the nouns in a multi-word term and take the mean of the resulting sum. 3. Linked Open Data (LOD) features: using DBpedia (Lehmann et al., 2013), we included binary values indicating whether a candidate term occurs in one of the following DBpedia"
S15-2122,W11-1902,0,0.0604374,"Missing"
S15-2122,P14-5010,0,0.00564095,"ood of the candidate terms (Kageura and Umino, 1996). To this purpose, we employed Log-likelihood (Rayson and Garside, 2000), C-value (Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including c"
S15-2122,W10-0204,0,0.0225577,"tical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system 720 (Lee et al., 2011). We should also point"
S15-2122,W11-1709,0,0.0123764,"ed using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system 720 (Lee et al., 2011). We should also point out that we only allowed"
S15-2122,W14-1306,0,0.120383,"e annotated with automatically identified <target, aspect category&gt; tuples, the laptop sentences only with the identified aspect categories. In the second phase (Phase B), the gold annotations for the above two datasets, as well as for a hidden domain, were given and the participants had to return the corresponding polarities (positive, negative, neutral). For more information we refer to Pontiki et al. (2015). We tackled the problem by dividing the ABSA task into three incremental subtasks: (i) aspect term extraction, (ii) aspect term classification and (iii) aspect term polarity estimation (Pavlopoulos and Androutsopoulos, 2014). The first two are at the basis of Phase A, whereas the final one constitutes Phase B. For the first step, viz. extracting terms (or targets), we wanted to test our in-house hybrid terminology extraction system (Section 2). Next, we performed a multiclass classification task relying on a feature space containing both lexical and semantic information to aggregate the previously identified terms into the domain-specific and predefined aspects (or aspect categories) (Section 3). Finally, we performed polarity classification by deriving both general and domain-specific lexical features from the r"
S15-2122,S14-2004,0,0.0374204,"t analysis of user-generated content. Until recently, the main research focus has been on discovering the overall polarity of a certain text or phrase. A noticeable shift has occurred to consider a more fine-grained approach, known as aspect-based sentiment analysis (ABSA). For this task the goal is to automatically identify the aspects of given target entities and the sentiment expressed towards each of them. In this paper, we present the LT3 system that participated in this year’s SemEval 2015 ABSA task. Though the focus was on the same domains (restaurants and laptops) as last year’s task (Pontiki et al., 2014), it differed in two ways. This time, entire reviews were to be annotated and for one subtask the systems were confronted with an out-of-domain test set, unknown to the participants. The task ran in two phases. In the first phase (Phase A), the participants were given two test sets (one for the laptops and one for the restaurants domain). The restaurant sentences were to be annotated with automatically identified <target, aspect category&gt; tuples, the laptop sentences only with the identified aspect categories. In the second phase (Phase B), the gold annotations for the above two datasets, as w"
S15-2122,S15-2082,0,0.0932815,"Missing"
S15-2122,W00-0901,0,0.0266193,"tic analysis, TExSIS relies on tokenized, Part-of-Speech tagged, lemmatized and chunked data using the LeTs Preprocess toolkit (Van de Kauter et al., 2013), which is incorporated in the architecture. Subsequently, all words and chunks matching certain Part-of-Speech patterns (i.e. nouns and noun phrases) were considered as candidate terms. In order to determine the specificity of and cohesion between these candidate terms, we combine several statistical filters to represent the termhood and unithood of the candidate terms (Kageura and Umino, 1996). To this purpose, we employed Log-likelihood (Rayson and Garside, 2000), C-value (Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicon"
S15-2122,S14-2009,0,0.0176511,"41.73 obtained by the winning team). This leads us to conclude that there’s quite some room for improvement for the aggregation phase. Normally, the similarity between terms is first computed after which some sort of clustering is performed 4 Phase B In recent years, sentiment analysis has been a popular research strand. An example is last year’s SemEval task 9 Sentiment Analysis in Twitter, which drew over 45 participants. The competition revealed that the best systems use supervised machine learning techniques and rely much on lexical features in the form of n-grams and sentiment lexicons (Rosenthal et al., 2014). For Phase B, in which we had all gold standard terms and aspect categories available, we decided to extend our LT3 system with another classification round where we classify every aspect as positive, negative or neutral. All features are derived from the sentence in which the terms were found and we participated in all three domains. 4.1 Domain Restaurants Laptops Hotels Feature Extraction We implemented a number of lexical features. First of all, we derived bag-of-words token unigram features. Then, we also generated features using two of the more well-known sentiment lexicons: General Inqu"
S15-2122,S14-2070,1,0.799655,"Missing"
S15-2122,H05-1044,0,0.00818665,"s (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system 720 (Lee et al., 2011). We should also point out that we only allowed terms to be identified in th"
S15-2157,baroni-bernardini-2004-bootcat,0,0.181561,"adjective + noun, adjective + noun, etc.). An example of these manually defined patterns is “NP {, NP}* {,} or/and other NP”,1 as in “green beans, carrots, peas and other vegetables”, which results in four hypernym pairs, being (vegetables, green beans), (vegetables, carrots), (vegetables, peas) and (vegetables, onions). Domain specific corpus. As this module aims to find hypernym relations by detecting terms occurring in specific lexico-syntactic constructions, we first needed to compile a domain specific corpus containing these terms. To compile the corpus, we used the the BootCaT toolkit (Baroni and Bernardini, 2004), which can be used to build a specialized web-based corpus starting from a list of seed terms. We considered the different term lists for task 17 as the “seed terms” to build the domain specific corpora, by allowing 10 queries per seed term. Due to technical reasons (the Bing search engine that is used by BootCat only allows 5000 queries per user account), we only compiled corpora for three domains, being equipment, food and science. As a post-processing step, we removed all sentences containing (1) only URL links or (2) no domain specific term, resulting in three corpora containing about 12"
S15-2157,S15-2151,0,0.40511,"such as taxonomies or ontologies can be automatically constructed from text (Biemann, 2005). Different approaches have been proposed to automatically detect hierarchical relations between terms: pattern-based approaches (Hearst, 1992; Pantel and Ravichandran, 2004), statistical and machine learning techniques (Ritter et al., 2009), distributional approaches (Caraballo, 1999; van der Plas and Bouma, 2005; Lenci and Benotto, 2012), morphosyntactic approaches (Tjong Kim Sang et al., 2011) and word class latices (Navigli and Velardi, 2010). The SemEval-2015 “Taxonomy Extraction Evaluation” Task (Bordea et al., 2015) is concerned with automatically finding relations between pairs of terms and organizing them in a hierarchical structure. In this way, the task assumes that a list of domain specific terms is already available in order to focus on the relation detection between these terms. To tackle this SemEval taxonomy learning task, we propose a multi-modular approach that combines lexico-syntactic, morphological and external structured lexical information. We will describe our hypernym detection system in Section 2. The results of the evaluation are presented in Section 3, while Section 4 concludes this"
S15-2157,P99-1016,0,0.425253,"heir own mono- or bilingual taxonomies containing the relevant sector- and company-specific terminology. This clear need for automatisation has encouraged researchers to investigate how terminological and semantically structured resources such as taxonomies or ontologies can be automatically constructed from text (Biemann, 2005). Different approaches have been proposed to automatically detect hierarchical relations between terms: pattern-based approaches (Hearst, 1992; Pantel and Ravichandran, 2004), statistical and machine learning techniques (Ritter et al., 2009), distributional approaches (Caraballo, 1999; van der Plas and Bouma, 2005; Lenci and Benotto, 2012), morphosyntactic approaches (Tjong Kim Sang et al., 2011) and word class latices (Navigli and Velardi, 2010). The SemEval-2015 “Taxonomy Extraction Evaluation” Task (Bordea et al., 2015) is concerned with automatically finding relations between pairs of terms and organizing them in a hierarchical structure. In this way, the task assumes that a list of domain specific terms is already available in order to focus on the relation detection between these terms. To tackle this SemEval taxonomy learning task, we propose a multi-modular approac"
S15-2157,C92-2082,0,0.678297,"the large variety of scientific and technological (sub)domains. In addition to domain-specific terminology, also companies desire to build their own mono- or bilingual taxonomies containing the relevant sector- and company-specific terminology. This clear need for automatisation has encouraged researchers to investigate how terminological and semantically structured resources such as taxonomies or ontologies can be automatically constructed from text (Biemann, 2005). Different approaches have been proposed to automatically detect hierarchical relations between terms: pattern-based approaches (Hearst, 1992; Pantel and Ravichandran, 2004), statistical and machine learning techniques (Ritter et al., 2009), distributional approaches (Caraballo, 1999; van der Plas and Bouma, 2005; Lenci and Benotto, 2012), morphosyntactic approaches (Tjong Kim Sang et al., 2011) and word class latices (Navigli and Velardi, 2010). The SemEval-2015 “Taxonomy Extraction Evaluation” Task (Bordea et al., 2015) is concerned with automatically finding relations between pairs of terms and organizing them in a hierarchical structure. In this way, the task assumes that a list of domain specific terms is already available in"
S15-2157,S12-1012,0,0.052067,"ng the relevant sector- and company-specific terminology. This clear need for automatisation has encouraged researchers to investigate how terminological and semantically structured resources such as taxonomies or ontologies can be automatically constructed from text (Biemann, 2005). Different approaches have been proposed to automatically detect hierarchical relations between terms: pattern-based approaches (Hearst, 1992; Pantel and Ravichandran, 2004), statistical and machine learning techniques (Ritter et al., 2009), distributional approaches (Caraballo, 1999; van der Plas and Bouma, 2005; Lenci and Benotto, 2012), morphosyntactic approaches (Tjong Kim Sang et al., 2011) and word class latices (Navigli and Velardi, 2010). The SemEval-2015 “Taxonomy Extraction Evaluation” Task (Bordea et al., 2015) is concerned with automatically finding relations between pairs of terms and organizing them in a hierarchical structure. In this way, the task assumes that a list of domain specific terms is already available in order to focus on the relation detection between these terms. To tackle this SemEval taxonomy learning task, we propose a multi-modular approach that combines lexico-syntactic, morphological and exte"
S15-2157,P10-1134,0,0.0471196,"researchers to investigate how terminological and semantically structured resources such as taxonomies or ontologies can be automatically constructed from text (Biemann, 2005). Different approaches have been proposed to automatically detect hierarchical relations between terms: pattern-based approaches (Hearst, 1992; Pantel and Ravichandran, 2004), statistical and machine learning techniques (Ritter et al., 2009), distributional approaches (Caraballo, 1999; van der Plas and Bouma, 2005; Lenci and Benotto, 2012), morphosyntactic approaches (Tjong Kim Sang et al., 2011) and word class latices (Navigli and Velardi, 2010). The SemEval-2015 “Taxonomy Extraction Evaluation” Task (Bordea et al., 2015) is concerned with automatically finding relations between pairs of terms and organizing them in a hierarchical structure. In this way, the task assumes that a list of domain specific terms is already available in order to focus on the relation detection between these terms. To tackle this SemEval taxonomy learning task, we propose a multi-modular approach that combines lexico-syntactic, morphological and external structured lexical information. We will describe our hypernym detection system in Section 2. The results"
S15-2157,N04-1041,0,0.191624,"ety of scientific and technological (sub)domains. In addition to domain-specific terminology, also companies desire to build their own mono- or bilingual taxonomies containing the relevant sector- and company-specific terminology. This clear need for automatisation has encouraged researchers to investigate how terminological and semantically structured resources such as taxonomies or ontologies can be automatically constructed from text (Biemann, 2005). Different approaches have been proposed to automatically detect hierarchical relations between terms: pattern-based approaches (Hearst, 1992; Pantel and Ravichandran, 2004), statistical and machine learning techniques (Ritter et al., 2009), distributional approaches (Caraballo, 1999; van der Plas and Bouma, 2005; Lenci and Benotto, 2012), morphosyntactic approaches (Tjong Kim Sang et al., 2011) and word class latices (Navigli and Velardi, 2010). The SemEval-2015 “Taxonomy Extraction Evaluation” Task (Bordea et al., 2015) is concerned with automatically finding relations between pairs of terms and organizing them in a hierarchical structure. In this way, the task assumes that a list of domain specific terms is already available in order to focus on the relation d"
S15-2157,I05-7011,0,0.538682,"Missing"
S16-1168,S16-1205,0,0.102525,"stem introduces hypernym endocentricity as a useful property for hypernym identification (Tan, 2016). Often multi-word hyponyms are endocentric constructions which contains a word that fulfills the same function as one part of its word. E.g. an ”apple pie” is essentially a ”pie”. The number of multi-words terms that are endocentric in English is investigated and whether this endocentric property can be used to generate entity links to connect terms in the Wikipedia list of list. QASSIT A semi-supervised methodology is used for the acquisition of lexical taxonomies based on genetic algorithms (Cleuziou and Moreno, 2016). It is based on the theory of pretopology that offers a powerful formalism to model semantic relations and transforms a list of terms into a structured term space by combining different discriminant criteria. In particular, rare but accurate pieces of knowledge are used to parameterize the different criteria defining the pretopological term space. Then, a structuring algorithm is used to transform the pretopological space into a lexical taxonomy. 5.1 Monolingual Subtasks (English) Table 2 presents the results of the structural analysis for English, giving an overview of the structural measure"
S16-1168,P14-1089,0,0.0222979,"luding monolingual subtasks for hypernym identification and taxonomy construction in English, as well as two corresponding multilingual subtasks that cover Dutch, French and Italian. 3 Dataset Creation We selected three target domains (i.e. Environment, Food and Science) with three root concepts (i.e. “environment”, “food” and “science”, respectively). Then, for each domain we considered different sources for gathering gold standard taxonomies, including a multilingual thesaurus, Eurovoc1 , a large lexical database of English, WordNet, and a general purpose resource, the Wikipedia Bitaxonomy (Flati et al., 2014). We also considered other domainspecific resources including “The Google product taxonomy” 2 for Food, and the “Taxonomy of Fields and their Subfields” 3 for Science. English taxonomies The English gold standard taxonomies are collected from each of the sources 1 Eurovoc: http://eurovoc.europa.eu/drupal/ http://www.google.com/basepages/ producttype/taxonomy.en-US.txt 3 http://sites.nationalacademies.org/PGA/ Resdoc/PGA_044522 2 1082 described above as follows. Gold standards are gathered from WordNet by selecting concepts and relationships in the hypernym-hyponym hierarchy rooted on the corre"
S16-1168,P05-1014,0,0.125948,"duced a multilingual setting for this task, covering four different languages including English, Dutch, Italian and French from domains as diverse as environment, food and science. A total of 62 runs submitted by 5 different teams were evaluated using structural measures, by comparison with gold standard taxonomies and by manual quality assessment of novel relations. 1 Introduction Taxonomies are useful tools for content organisation, navigation, and retrieval, providing valuable input for semantically intensive tasks such as question answering (Harabagiu et al., 2003) and textual entailment (Geffet and Dagan, 2005). In general, a hierarchical relation is any asymmetrical relation that indicates subordination between two terms, but in this task we focus on hyponym-hypernym relations. Taxonomy learning from text is a challenging task that can be divided in several subtasks, including term extraction, hypernym identification and taxonomy construction. Existing approaches for hypernym identification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions"
S16-1168,S15-2152,0,0.120027,"sks such as question answering (Harabagiu et al., 2003) and textual entailment (Geffet and Dagan, 2005). In general, a hierarchical relation is any asymmetrical relation that indicates subordination between two terms, but in this task we focus on hyponym-hypernym relations. Taxonomy learning from text is a challenging task that can be divided in several subtasks, including term extraction, hypernym identification and taxonomy construction. Existing approaches for hypernym identification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pat"
S16-1168,C92-2082,0,0.623288,"trieval, providing valuable input for semantically intensive tasks such as question answering (Harabagiu et al., 2003) and textual entailment (Geffet and Dagan, 2005). In general, a hierarchical relation is any asymmetrical relation that indicates subordination between two terms, but in this task we focus on hyponym-hypernym relations. Taxonomy learning from text is a challenging task that can be divided in several subtasks, including term extraction, hypernym identification and taxonomy construction. Existing approaches for hypernym identification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promisin"
S16-1168,D10-1108,0,0.643764,"Paul Buitelaar* *Insight Centre for Data Analytics National University of Ireland, Galway name.surname@insight-centre.org **LT3, Language and Translation Technology Team, Ghent University, Belgium name.surname@ugent.be Abstract usually produces a large number of noisy, inconsistent relations, which assign multiple parents to a node and contain cycles. Hence, the third stage of taxonomy learning, taxonomy construction, focuses on the overall structure of the resulting graph and aims to organise terms in a hierarchical structure, more specifically a directed acyclic graph (Velardi et al., 2013; Kozareva and Hovy, 2010). This paper describes the second edition of the shared task on Taxonomy Extraction Evaluation organised as part of SemEval 2016. This task aims to extract hypernym-hyponym relations between a given list of domain-specific terms and then to construct a domain taxonomy based on them. TExEval-2 introduced a multilingual setting for this task, covering four different languages including English, Dutch, Italian and French from domains as diverse as environment, food and science. A total of 62 runs submitted by 5 different teams were evaluated using structural measures, by comparison with gold stan"
S16-1168,N15-1098,0,0.027616,"rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pattern-based approaches, including low coverage of domain-specific terms, most participants in this shared task opted for traditional approaches for hypernym identification, with the exception of one system (Pocostales, 2016). TexEval-2 is mainly concerned with automatically extracting hierarchical relations from text and subsequent taxonomy construction, therefore we make the assumption that a list of terms is readily available. This simplifies evaluation by providing a common ground for all the s"
S16-1168,S16-1204,0,0.074038,"icipated in the shared task, but only two systems participated in the multilingual subtasks. Two of the systems that participated in the monolingual subtask alone did not submit runs for the food domain, which has the largest number of nodes. Overall, 62 system runs were submitted by the five teams, 36 for the multilingual subtasks and 26 for the monolingual subtasks. Next, we provide a short description of each approach starting with the two systems that participated in the multilingual subtasks. JUNLP The JUNLP system makes use of an external linguistic resource for hypernym identification (Maitra and Das, 2016). This resource is the BabelNet semantic network that connects concepts and named entities in a very large network of semantic relations, called Babel synsets (Navigli and Ponzetto, 2010). To make sure that no relations that were used to construct the gold standards are considered, only relations that mention Wikipedia as a source were selected, discarding relations from all the other sources. Additionally, the system makes use of two string inclusion heuristics. The first heuristic checks if any of the terms provided by the organisers is included as a substring in another term. The second heu"
S16-1168,N13-1090,0,0.00284317,"with lexico-syntactic patterns. No databases or linguistic resources beyond trial data and raw text corpora mentioned above are used. For the taxonomy construction subtasks, the system makes use of an unsupervised graph pruning approach based on the Tarjan algorithm, connecting the resulting disconnected components to the root of the graph. NUIG-UNLP The system implements a semisupervised method that finds hypernym candidates for the provided noun phrases by representing them as distributional vectors. Roughly, this method assumes that hypernyms may be induced by adding a 1087 vector offset (Mikolov et al., 2013; Rei and Briscoe, 2014) to the corresponding hyponym representation generated by GloVe over a Wikipedia dump. The vector offset is obtained as the average offset between 200 pairs of hyponym-hypernym in the same vector space selected from trial data. USAAR This system introduces hypernym endocentricity as a useful property for hypernym identification (Tan, 2016). Often multi-word hyponyms are endocentric constructions which contains a word that fulfills the same function as one part of its word. E.g. an ”apple pie” is essentially a ”pie”. The number of multi-words terms that are endocentric i"
S16-1168,P10-1023,0,0.0313702,"s for the food domain, which has the largest number of nodes. Overall, 62 system runs were submitted by the five teams, 36 for the multilingual subtasks and 26 for the monolingual subtasks. Next, we provide a short description of each approach starting with the two systems that participated in the multilingual subtasks. JUNLP The JUNLP system makes use of an external linguistic resource for hypernym identification (Maitra and Das, 2016). This resource is the BabelNet semantic network that connects concepts and named entities in a very large network of semantic relations, called Babel synsets (Navigli and Ponzetto, 2010). To make sure that no relations that were used to construct the gold standards are considered, only relations that mention Wikipedia as a source were selected, discarding relations from all the other sources. Additionally, the system makes use of two string inclusion heuristics. The first heuristic checks if any of the terms provided by the organisers is included as a substring in another term. The second heuristic considers terms that have a considerable overlap, for instance Chocolate Pudding and Vanilla Pudding although their hypernym (i.e., Pudding) is not mentioned in the list of terms."
S16-1168,S16-1206,0,0.269983,"Missing"
S16-1168,S16-1202,0,0.203096,"from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pattern-based approaches, including low coverage of domain-specific terms, most participants in this shared task opted for traditional approaches for hypernym identification, with the exception of one system (Pocostales, 2016). TexEval-2 is mainly concerned with automatically extracting hierarchical relations from text and subsequent taxonomy construction, therefore we make the assumption that a list of terms is readily available. This simplifies evaluation by providing a common ground for all the systems, but participants are allowed to add additional nodes, i.e. terms, in the hierarchy as they consider appropriate. To avoid the need for term extraction, terms are extracted from existing taxonomies, providing participants with a domain lexicon that has to be organised in a hierarchical structure. 1081 Proceedings"
S16-1168,W14-1608,0,0.0698067,"from text is a challenging task that can be divided in several subtasks, including term extraction, hypernym identification and taxonomy construction. Existing approaches for hypernym identification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pattern-based approaches, including low coverage of domain-specific terms, most participants in this shared task opted for traditional approaches for hypernym identification, with the exception of one system (Pocostales, 2016). TexEval-2 is mainly concerned with automatically extracting hierarchi"
S16-1168,C14-1097,0,0.021659,"Missing"
S16-1168,E14-4008,0,0.0519906,"ns. Taxonomy learning from text is a challenging task that can be divided in several subtasks, including term extraction, hypernym identification and taxonomy construction. Existing approaches for hypernym identification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pattern-based approaches, including low coverage of domain-specific terms, most participants in this shared task opted for traditional approaches for hypernym identification, with the exception of one system (Pocostales, 2016). TexEval-2 is mainly concerned with automatical"
S16-1168,S16-1203,0,0.215836,"a semisupervised method that finds hypernym candidates for the provided noun phrases by representing them as distributional vectors. Roughly, this method assumes that hypernyms may be induced by adding a 1087 vector offset (Mikolov et al., 2013; Rei and Briscoe, 2014) to the corresponding hyponym representation generated by GloVe over a Wikipedia dump. The vector offset is obtained as the average offset between 200 pairs of hyponym-hypernym in the same vector space selected from trial data. USAAR This system introduces hypernym endocentricity as a useful property for hypernym identification (Tan, 2016). Often multi-word hyponyms are endocentric constructions which contains a word that fulfills the same function as one part of its word. E.g. an ”apple pie” is essentially a ”pie”. The number of multi-words terms that are endocentric in English is investigated and whether this endocentric property can be used to generate entity links to connect terms in the Wikipedia list of list. QASSIT A semi-supervised methodology is used for the acquisition of lexical taxonomies based on genetic algorithms (Cleuziou and Moreno, 2016). It is based on the theory of pretopology that offers a powerful formalis"
S16-1168,J13-3007,0,0.759765,"rdea*, Els Lefever**, Paul Buitelaar* *Insight Centre for Data Analytics National University of Ireland, Galway name.surname@insight-centre.org **LT3, Language and Translation Technology Team, Ghent University, Belgium name.surname@ugent.be Abstract usually produces a large number of noisy, inconsistent relations, which assign multiple parents to a node and contain cycles. Hence, the third stage of taxonomy learning, taxonomy construction, focuses on the overall structure of the resulting graph and aims to organise terms in a hierarchical structure, more specifically a directed acyclic graph (Velardi et al., 2013; Kozareva and Hovy, 2010). This paper describes the second edition of the shared task on Taxonomy Extraction Evaluation organised as part of SemEval 2016. This task aims to extract hypernym-hyponym relations between a given list of domain-specific terms and then to construct a domain taxonomy based on them. TExEval-2 introduced a multilingual setting for this task, covering four different languages including English, Dutch, Italian and French from domains as diverse as environment, food and science. A total of 62 runs submitted by 5 different teams were evaluated using structural measures, by"
S16-1168,C14-1212,0,0.0211349,"ification from text rely on lexico-syntactic patterns (Hearst, 1992; Lefever et al., 2014), cooccurrence information (Grefenstette, 2015), substring inclusion, or exploit semantic relations provided in textual definitions (Velardi et al., 2013). This stage More recently, the hypernym identification subtask has attracted an increased interest from the distributional semantics community (Santus et al., 2014; Rei and Briscoe, 2014; Roller et al., 2014; Yu et al., 2015), as part of a wider effort to distinguish between different semantic relations which exist between distributional similar words (Weeds et al., 2014; Levy et al., 2015). Although this is a promising direction of research, that addresses some of the limitations of pattern-based approaches, including low coverage of domain-specific terms, most participants in this shared task opted for traditional approaches for hypernym identification, with the exception of one system (Pocostales, 2016). TexEval-2 is mainly concerned with automatically extracting hierarchical relations from text and subsequent taxonomy construction, therefore we make the assumption that a list of terms is readily available. This simplifies evaluation by providing a common"
S16-1168,S15-2151,1,\N,Missing
S18-1005,K16-1017,0,0.0646404,"eficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features are and have been used extensively over the past years, deep learning techniques have recently gained increasing popularity for this task. Such systems often rely on semantic relatedness (i.e. through word and character embeddings (e.g. Amir et al., 2016; Ghosh and Veale, 2016)) deduced by the network and reduce feature engineering efforts. Regardless of the methodology and algorithm used, irony detection often involves binary classification where irony is defined as instances that express the opposite of what is meant (e.g. Riloff et al., 2013; Joshi et al., 2017). Twitter has been a popular data genre for this task, as it is easily accessible and provides a rapid and convenient method to find (potentially) ironic messages by looking for hashtags like #irony, #not and #sarcasm. As a consequence, irony detection research often relies on autom"
S18-1005,D17-1169,0,0.0605263,"nent models. They combined (through averaging) pretrained word and emoji embeddings with handcrafted features, including sentiment contrasts between elements in a tweet (i.e. left vs. right sections, hashtags vs. text, emoji vs. text), sentiment intensity and word-based features like flooding and capitalisation). For Task B, they used a slightly altered (i.e. ensemble LR models and concatenated word embeddings instead of averaged) model. NLPRL-IITBHU (Rangwani et al., 2018) ranked fourth and used an XGBoost Classifier to tackle Task A. They combined pre-trained CNN activations using DeepMoji (Felbo et al., 2017) with ten types of handcrafted features. These were based on polarity contrast information, readability metrics, context incongruity, character flooding, punctuation counts, discourse markers/intensifiers/interjections/swear words counts, general token counts, WordNet similarity, polarity scores and URL counts. The fifth best system for Task A was built by NIHRIO (Vu et al., 2018) and consists of a neural-networks-based architecture (i.e. Multilayer Perceptron). The system exploited lexical (word- and character-level unigrams, bigrams and trigrams), syntactic (PoS-tags with tfidf values), sema"
S18-1005,E14-3007,0,0.0779629,".679 precision 0.583 recall 0.666 F1 0.622 0.651 0.649 0.598 0.603 0.556 0.571 0.584 0.643 0.546 0.544 0.496 0.500 0.450 0.455 0.427 0.897 0.714 0.714 0.781 0.556 0.540 0.408 0.142 0.113 0.618 0.618 0.607 0.527 0.491 0.431 0.213 0.200 Table 5: Best unconstrained systems for Task A. In the top five unconstrained (i.e. using additional training data) systems for Task A are #NonDicevoSulSerio, INAOE-UPV, RM@IT, ValenTO and UTMN, with F1 scores ranging between 0.622 and 0.527. #NonDicevoSulserio extended the training corpus with 3,500 tweets from existing irony corpora (e.g. Riloff et al. (2013); Barbieri and Saggion (2014); Pt´acˇ ek et al. (2014) and built an SVM classifier exploiting structural features (e.g. hashtag count, text length), sentiment- (e.g. contrast between text and emoji sentiment), and emotion-based (i.e. emotion lexicon scores) features. INAOE-UPV combined pretrained word embeddings from the Google News corpus with word-based features (e.g. n-grams). They also extended the official training data with benchmark corpora previously used in irony research and trained their system with a total of 165,000 instances. RM@IT approached the task using an ensemble classifier based on attentionbased recu"
S18-1005,S18-1093,0,0.0266587,"HPCC Random BL LDR ECNU NEUROSENTPDI INAOE-UPV Systems and Results for Task B While 43 teams competed in Task A, 31 teams submitted a system for Task B on multiclass irony classification. Table 6 presents the official ranking with each team’s performance in terms of accuracy, precision, recall and F1 score. Similar to Task A, we discuss the top five systems in the overall ranking (Table 6) and then zoom in on the best performing constrained and unconstrained systems (Tables 7 and 8). For Task B, the top five is nearly similar to the top five for Task A and includes the following teams: UCDCC (Ghosh, 2018), NTUASLP (Baziotis et al., 2018), THU NGN (Wu et al., 2018), NLPRL-IITBHU (Rangwani et al., 2018) and NIHRIO (Vu et al., 2018). All of the teams tackled multiclass irony classification by applying (mostly) the same architecture as for Task A (see earlier). Inspired by siamese networks (Bromley et al., 1993) used in image classification, the UCDCC team developed a siamese architecture for irony detection in both subtasks. The neural network architecture makes use of Glove word acc 0.732 0.652 0.605 0.603 precision 0.577 0.496 0.486 0.466 recall 0.504 0.512 0.541 0.506 F1 0.507 0.496 0.495 0.47"
S18-1005,S15-2080,0,0.541466,"corpus. For both tasks, a training corpus of 3,834 tweets was provided, as well as a test set containing 784 tweets. Our shared tasks received submissions from 43 teams for the binary classification Task A and from 31 teams for the multiclass Task B. The highest classification scores obtained for both subtasks are respectively F1 = 0.71 and F1 = 0.51 and demonstrate that fine-grained irony classification is much more challenging than binary irony detection. 1 Introduction The development of the social web has stimulated the use of figurative and creative language, including irony, in public (Ghosh et al., 2015). From a philosophical/psychological perspective, discerning the mechanisms that underlie ironic speech improves our understanding of human reasoning and communication, and more and more, this interest in understanding irony also emerges in the machine learning community (Wallace, 2015). Although an unanimous definition of irony is still lacking in the literature, it is often identified as a trope whose actual meaning differs from what is literally enunciated. Due to its nature, irony has important implications for natural language processing (NLP) tasks, which aim to understand and produce hu"
S18-1005,W16-0425,0,0.473198,"xt of SemEval (Rosenthal et al., 2017)). However, the SemEval-2014 shared task Sentiment Analysis in Twitter (Rosenthal et al., 2014) demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets. The results revealed that, while sentiment classification performance on regular tweets reached up to F1 = 0.71, scores on the ironic tweets varied between F1 = 0.29 and F1 = 0.57. In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g. Liu, 2012; Maynard and Greenwood, 2014; Ghosh and Veale, 2016). Like other types of figurative language, ironic text should not be interpreted in its literal sense; it requires a more complex understanding based on associations with the context or world knowledge. Examples 1 and 2 are sentences that regular sentiment analysis systems would probably classify as positive, whereas the intended sentiment is undeniably negative. This paper presents the first shared task on irony detection: given a tweet, automatic natural language processing systems should determine whether the tweet is ironic (Task A) and which type of irony (if any) is expressed (Task B). T"
S18-1005,J92-4003,0,0.285384,", context incongruity, character flooding, punctuation counts, discourse markers/intensifiers/interjections/swear words counts, general token counts, WordNet similarity, polarity scores and URL counts. The fifth best system for Task A was built by NIHRIO (Vu et al., 2018) and consists of a neural-networks-based architecture (i.e. Multilayer Perceptron). The system exploited lexical (word- and character-level unigrams, bigrams and trigrams), syntactic (PoS-tags with tfidf values), semantic features (word embeddings using GloVe (Pennington et al., 2014), LSI features and Brown cluster features (Brown et al., 1992)) and polarity features derived from the Hu and Liu Opinion Lexicon (Hu and Liu, 2004). Systems and results for Task A In total, 43 teams competed in Task A on binary irony classification. Table 3 presents each team’s performance in terms of accuracy, precision, recall and F1 score. In all tables, the systems are ranked by the official F1 score (shown in the fifth column). Scores from teams that are marked with an asterisk should be interpreted carefully, as the number of predictions they submitted does not correspond to the number of test instances. As can be observed from the table, the SVM"
S18-1005,P11-2102,0,0.247968,"important to note that by a tweet, we understand the actual text it contains, without metadata (e.g. user id, time stamp, location). Although such metadata could help to recognise irony, the objective of this task is to learn, at message level, how irony is linguistically realised. Previous work on irony detection mostly applied supervised machine learning mainly exploiting lexical features. Other features often include punctuation mark/interjection counts (e.g Davidov et al., 2010), sentiment lexicon scores (e.g. Bouazizi and Ohtsuki, 2016; Far´ıas et al., 2016), emoji (e.g. Gonz´alez-Ib´an˜ ez et al., 2011), writing style, emotional scenarios, part of speechpatterns (e.g. Reyes et al., 2013), and so on. Also beneficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features are and have been used extensively over the past years, deep learning techniques have recently gained increasing popularity for th"
S18-1005,C16-1251,0,0.0155516,"eptron, and an optimised (i.e. using feature selection) voting classifier combining Support Vector Machines with Logistic Regression. Neural networkbased systems exploiting word embeddings derived from the training dataset or generated from Wikipedia corpora perform less well for the task. Similar to Task A, the unconstrained systems do not seem to benefit from additional data, as they do not outperform the constrained submissions for the task. 46 lexical features (e.g. n-grams, punctuation and hashtag counts, emoji presence) and sentimentor emotion- lexicon features (e.g. based on SenticNet (Cambria et al., 2016), VADER (Hutto and Gilbert, 2014), aFinn (Nielsen, 2011)). Also important but to a lesser extent were syntactic (e.g. PoS-patterns) and semantic features, based on word, character and emoji embeddings or semantic clusters. The best systems for Task A and Task B obtained an F1 score of respectively 0.705 and 0.507 and clearly outperformed the baselines provided for this task. When looking at the scores per class label in Task B, we observe that high scores were obtained for the non-ironic and ironic by clash classes, and that other irony appears to be the most challenging irony type. Among all"
S18-1005,S17-2103,0,0.025055,"ne learning mainly exploiting lexical features. Other features often include punctuation mark/interjection counts (e.g Davidov et al., 2010), sentiment lexicon scores (e.g. Bouazizi and Ohtsuki, 2016; Far´ıas et al., 2016), emoji (e.g. Gonz´alez-Ib´an˜ ez et al., 2011), writing style, emotional scenarios, part of speechpatterns (e.g. Reyes et al., 2013), and so on. Also beneficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features are and have been used extensively over the past years, deep learning techniques have recently gained increasing popularity for this task. Such systems often rely on semantic relatedness (i.e. through word and character embeddings (e.g. Amir et al., 2016; Ghosh and Veale, 2016)) deduced by the network and reduce feature engineering efforts. Regardless of the methodology and algorithm used, irony detection often involves binary classification where irony is defined as instances"
S18-1005,J96-2004,0,0.681654,"Missing"
S18-1005,W10-2914,0,0.209203,"challenged to automatically determine whether irony is used and which type of irony is expressed. We thus defined two subtasks: It is important to note that by a tweet, we understand the actual text it contains, without metadata (e.g. user id, time stamp, location). Although such metadata could help to recognise irony, the objective of this task is to learn, at message level, how irony is linguistically realised. Previous work on irony detection mostly applied supervised machine learning mainly exploiting lexical features. Other features often include punctuation mark/interjection counts (e.g Davidov et al., 2010), sentiment lexicon scores (e.g. Bouazizi and Ohtsuki, 2016; Far´ıas et al., 2016), emoji (e.g. Gonz´alez-Ib´an˜ ez et al., 2011), writing style, emotional scenarios, part of speechpatterns (e.g. Reyes et al., 2013), and so on. Also beneficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features a"
S18-1005,E17-2068,0,0.034751,"motion lexicon scores) features. INAOE-UPV combined pretrained word embeddings from the Google News corpus with word-based features (e.g. n-grams). They also extended the official training data with benchmark corpora previously used in irony research and trained their system with a total of 165,000 instances. RM@IT approached the task using an ensemble classifier based on attentionbased recurrent neural networks and the FastTable 3: Official (CodaLab) results for Task A, ranked by F1 score. The highest scores in each column are shown in bold and the baselines are indicated in purple. 44 Text (Joulin et al., 2017) library for learning word representations. They enriched the provided training corpus with, on the one hand, the data sets provided for SemEval-2015 Task 11 (Ghosh et al., 2015) and, on the other hand, the sarcasm corpus composed by Pt´acˇ ek et al. (2014). Altogether, this generated a training corpus of approximately 110,000 tweets. ValenTO took advantage of irony corpora previously used in irony detection that were manually annotated or through crowdsourcing (e.g. Riloff et al., 2013; Pt´acˇ ek et al., 2014). In addition, they extended their corpus with an unspecified number of self-collect"
S18-1005,D14-1162,0,0.0811876,"se were based on polarity contrast information, readability metrics, context incongruity, character flooding, punctuation counts, discourse markers/intensifiers/interjections/swear words counts, general token counts, WordNet similarity, polarity scores and URL counts. The fifth best system for Task A was built by NIHRIO (Vu et al., 2018) and consists of a neural-networks-based architecture (i.e. Multilayer Perceptron). The system exploited lexical (word- and character-level unigrams, bigrams and trigrams), syntactic (PoS-tags with tfidf values), semantic features (word embeddings using GloVe (Pennington et al., 2014), LSI features and Brown cluster features (Brown et al., 1992)) and polarity features derived from the Hu and Liu Opinion Lexicon (Hu and Liu, 2004). Systems and results for Task A In total, 43 teams competed in Task A on binary irony classification. Table 3 presents each team’s performance in terms of accuracy, precision, recall and F1 score. In all tables, the systems are ranked by the official F1 score (shown in the fifth column). Scores from teams that are marked with an asterisk should be interpreted carefully, as the number of predictions they submitted does not correspond to the number"
S18-1005,P15-2106,0,0.177051,"lied supervised machine learning mainly exploiting lexical features. Other features often include punctuation mark/interjection counts (e.g Davidov et al., 2010), sentiment lexicon scores (e.g. Bouazizi and Ohtsuki, 2016; Far´ıas et al., 2016), emoji (e.g. Gonz´alez-Ib´an˜ ez et al., 2011), writing style, emotional scenarios, part of speechpatterns (e.g. Reyes et al., 2013), and so on. Also beneficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features are and have been used extensively over the past years, deep learning techniques have recently gained increasing popularity for this task. Such systems often rely on semantic relatedness (i.e. through word and character embeddings (e.g. Amir et al., 2016; Ghosh and Veale, 2016)) deduced by the network and reduce feature engineering efforts. Regardless of the methodology and algorithm used, irony detection often involves binary classification where irony i"
S18-1005,S17-2004,0,0.0651054,"Missing"
S18-1005,S18-1104,0,0.0944065,"s. WLV (Rohanian et al., 2018) developed an ensemble voting classifier with logistic regression (LR) and a support vector machine (SVM) as component models. They combined (through averaging) pretrained word and emoji embeddings with handcrafted features, including sentiment contrasts between elements in a tweet (i.e. left vs. right sections, hashtags vs. text, emoji vs. text), sentiment intensity and word-based features like flooding and capitalisation). For Task B, they used a slightly altered (i.e. ensemble LR models and concatenated word embeddings instead of averaged) model. NLPRL-IITBHU (Rangwani et al., 2018) ranked fourth and used an XGBoost Classifier to tackle Task A. They combined pre-trained CNN activations using DeepMoji (Felbo et al., 2017) with ten types of handcrafted features. These were based on polarity contrast information, readability metrics, context incongruity, character flooding, punctuation counts, discourse markers/intensifiers/interjections/swear words counts, general token counts, WordNet similarity, polarity scores and URL counts. The fifth best system for Task A was built by NIHRIO (Vu et al., 2018) and consists of a neural-networks-based architecture (i.e. Multilayer Perce"
S18-1005,maynard-greenwood-2014-cares,0,0.123957,"the field (e.g. in the context of SemEval (Rosenthal et al., 2017)). However, the SemEval-2014 shared task Sentiment Analysis in Twitter (Rosenthal et al., 2014) demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets. The results revealed that, while sentiment classification performance on regular tweets reached up to F1 = 0.71, scores on the ironic tweets varied between F1 = 0.29 and F1 = 0.57. In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g. Liu, 2012; Maynard and Greenwood, 2014; Ghosh and Veale, 2016). Like other types of figurative language, ironic text should not be interpreted in its literal sense; it requires a more complex understanding based on associations with the context or world knowledge. Examples 1 and 2 are sentences that regular sentiment analysis systems would probably classify as positive, whereas the intended sentiment is undeniably negative. This paper presents the first shared task on irony detection: given a tweet, automatic natural language processing systems should determine whether the tweet is ironic (Task A) and which type of irony (if any)"
S18-1005,D13-1066,0,0.510167,"Missing"
S18-1005,P16-1104,0,0.0317065,"on mark/interjection counts (e.g Davidov et al., 2010), sentiment lexicon scores (e.g. Bouazizi and Ohtsuki, 2016; Far´ıas et al., 2016), emoji (e.g. Gonz´alez-Ib´an˜ ez et al., 2011), writing style, emotional scenarios, part of speechpatterns (e.g. Reyes et al., 2013), and so on. Also beneficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features are and have been used extensively over the past years, deep learning techniques have recently gained increasing popularity for this task. Such systems often rely on semantic relatedness (i.e. through word and character embeddings (e.g. Amir et al., 2016; Ghosh and Veale, 2016)) deduced by the network and reduce feature engineering efforts. Regardless of the methodology and algorithm used, irony detection often involves binary classification where irony is defined as instances that express the opposite of what is meant (e.g. Riloff et al., 2013; Joshi et al., 2"
S18-1005,S17-1007,0,0.136597,"t al., 2015; Van Hee, 2017). 2 3 • Task A describes a binary irony classification task to define, for a given tweet, whether irony is expressed. • Task B describes a multiclass irony classification task to define whether it contains a specific type of irony (verbal irony by means of a polarity clash, situational irony, or another type of verbal irony, see further) or is not ironic. Concretely, participants should define which one out of four categories a tweet contains: ironic by clash, situational irony, other verbal irony or not ironic. Automatic Irony Detection As described by Joshi et al. (2017), recent approaches to irony can roughly be classified as either rule-based or (supervised and unsupervised) machine learning-based. While rule-based approaches mostly rely upon lexical information and require no training, machine learning invariably makes use of training data and exploits different types of information sources (or features), such as bags of words, syntactic patterns, sentiment information or semantic relatedness. Task Description We propose two subtasks A and B for the automatic detection of irony on Twitter, for which we provide more details below. 3.1 Task A: Binary Irony C"
S18-1005,S18-1090,0,0.0147965,"qual weight in the final score. 2 According to magnitude guidelines by Landis and Koch (1977). 42 For both subtasks, two baselines were provided against which to compare the systems’ performance. The first baseline randomly assigns irony labels and the second one is a linear SVM classifier with standard hyperparameter settings exploiting tf-idf word unigram features (implemented with scikit-learn (Pedregosa et al., 2011)). The second baseline system is made available to the task participants via GitHub3 . 6 ble classifier applied majority voting to combine the outcomes of the two models. WLV (Rohanian et al., 2018) developed an ensemble voting classifier with logistic regression (LR) and a support vector machine (SVM) as component models. They combined (through averaging) pretrained word and emoji embeddings with handcrafted features, including sentiment contrasts between elements in a tweet (i.e. left vs. right sections, hashtags vs. text, emoji vs. text), sentiment intensity and word-based features like flooding and capitalisation). For Task B, they used a slightly altered (i.e. ensemble LR models and concatenated word embeddings instead of averaged) model. NLPRL-IITBHU (Rangwani et al., 2018) ranked"
S18-1005,S16-1001,0,0.0419889,"ly 110,000 tweets. ValenTO took advantage of irony corpora previously used in irony detection that were manually annotated or through crowdsourcing (e.g. Riloff et al., 2013; Pt´acˇ ek et al., 2014). In addition, they extended their corpus with an unspecified number of self-collected irony tweets using the hashtags #irony and #sarcasm. Finally, UTMN developed an SVM classifier exploiting binary bag-of-words features. They enriched the training set with 1,000 humorous tweets from SemEval-2017 Task 6 (Potash et al., 2017) and another 1,000 tweets with positive polarity from SemEval-2016 Task 4 (Nakov et al., 2016), resulting in a training corpus of 5,834 tweets. Interestingly, when comparing the best constrained with the best unconstrained system for Task A, we see a difference of 10 points in favour of the constrained system, which indicates that adding more training data does not necessarily improve the classification performance. 7 embeddings as features and creates two identical subnetworks that are each fed with different parts of a tweet. Under the premise that ironic statements are often characterised by a form of opposition or contrast, the architecture captures this incongruity between two par"
S18-1005,S17-2088,0,0.140726,"ts Cynthia Van Hee, Els Lefever and V´eronique Hoste LT3 Language and Translation Technology Team Ghent University Groot-Brittanni¨elaan 45, 9000 Ghent firstname.lastname@ugent.be Abstract detection has a large potential for various applications in the domain of text mining, especially those that require semantic analysis, such as author profiling, detecting online harassment, and, maybe the most well-known example, sentiment analysis. Due to its importance in industry, sentiment analysis research is abundant and significant progress has been made in the field (e.g. in the context of SemEval (Rosenthal et al., 2017)). However, the SemEval-2014 shared task Sentiment Analysis in Twitter (Rosenthal et al., 2014) demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets. The results revealed that, while sentiment classification performance on regular tweets reached up to F1 = 0.71, scores on the ironic tweets varied between F1 = 0.29 and F1 = 0.57. In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g. Liu, 2012; Maynard and Greenwood, 2014; Ghosh and Veale, 2016). Like other typ"
S18-1005,S14-2009,0,0.020285,"am Ghent University Groot-Brittanni¨elaan 45, 9000 Ghent firstname.lastname@ugent.be Abstract detection has a large potential for various applications in the domain of text mining, especially those that require semantic analysis, such as author profiling, detecting online harassment, and, maybe the most well-known example, sentiment analysis. Due to its importance in industry, sentiment analysis research is abundant and significant progress has been made in the field (e.g. in the context of SemEval (Rosenthal et al., 2017)). However, the SemEval-2014 shared task Sentiment Analysis in Twitter (Rosenthal et al., 2014) demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets. The results revealed that, while sentiment classification performance on regular tweets reached up to F1 = 0.71, scores on the ironic tweets varied between F1 = 0.29 and F1 = 0.57. In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g. Liu, 2012; Maynard and Greenwood, 2014; Ghosh and Veale, 2016). Like other types of figurative language, ironic text should not be interpreted in its literal sense; it requi"
S18-1005,E12-2021,0,0.114923,"Missing"
S18-1005,C16-1257,1,0.924617,"Missing"
S18-1005,S18-1085,0,0.0496312,"Missing"
S18-1005,S18-1006,0,0.159345,"Missing"
S19-2077,S19-2007,0,0.039027,"ual Detection of Hate Speech Against Immigrants and Women in Twitter (hatEval) Nina Bauwelinck, Gilles Jacobs, V´eronique Hoste and Els Lefever LT3, Language and Translation Technology Team Department of Translation, Interpreting and Communication – Ghent University Groot-Brittanni¨elaan 45, 9000 Ghent, Belgium firstname.lastname@ugent.be, gillesm.jacobs@ugent.be Abstract standard machine learning approach or deep learning methods (Pitsilis et al., 2018). This was also the approach we took for our hate speech detection system. We participated for both subtasks proposed for English for Task 5 (Basile et al., 2019), being TASK A, which was defined as a binary classification task where systems have to predict whether a tweet with a given target (women or immigrants) is hateful or not hateful, and TASK B, where systems are asked first to classify hateful tweets as aggressive or not aggressive, and second to identify the target harassed as individual or generic (i.e. single human or group). This paper describes our contribution to the SemEval-2019 Task 5 on the detection of hate speech against immigrants and women in Twitter (hatEval). We considered a supervised classification-based approach to detect hate"
S19-2077,H05-1044,0,0.0633562,"and non-linguistic features included. This featurization pipeline is based on work in cyberbullying detection and analysis (Van Hee et al., 2018). The whole set of features listed below was used to build all three classifiers. We did not apply any feature selection. Bag-of-words features: We included binary token unigrams, bigrams and trigrams, along with character trigrams and fourgrams. The latter provide robustness to the spelling variation typically found in social media. Lexicon features: We computed positive and negative opinion word ratio and overall post sentiment using both the MPQA (Wilson et al., 2005) and Hu and Liu’s (Hu and Liu, 2004) opinion lexicons. We added positive, negative and neutral emoji counts based on the BOUNCE emoji sentiment lexicon (K¨okciyan et al., 2013). We also included the relative frequency of all 64 psychometric categories in the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al., 2007). Furthermore, we included diminisher, intensifier, negation, and “allness” lexicons which relate to a negative mindset in the context of suicidality research (Osgood and Walker, 1959; Gottschalk and Gleser, 1960; Shapero, 2011) as well as a proper name gazetteer."
W09-2413,D07-1007,0,0.180514,"d. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. Although WSD has long time been studied as a stand-alone NLP task, there is a growing feeling in the WSD community that WSD should preferably be integrated in real applications such as Machine Translation or multilingual information retrieval (Agirre and Edmonds, 2006). Several studies have demonstrated that for instance Statistical Machine Translation (SMT) benefits from incorporating a dedicated WSD module (Chan et al., 2007; Carpuat and Wu, 2007). Using translations from a corpus instead of human-defined sense labels is one way of facilitating the integration of WSD in multilingual applications. It also implic1 http://www.senseval.org/ Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 82–87, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics itly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the translations. Furthermore, this type of corpus-based approach is languageindependent"
W09-2413,P07-1005,0,0.0576053,"d expensive to build. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. Although WSD has long time been studied as a stand-alone NLP task, there is a growing feeling in the WSD community that WSD should preferably be integrated in real applications such as Machine Translation or multilingual information retrieval (Agirre and Edmonds, 2006). Several studies have demonstrated that for instance Statistical Machine Translation (SMT) benefits from incorporating a dedicated WSD module (Chan et al., 2007; Carpuat and Wu, 2007). Using translations from a corpus instead of human-defined sense labels is one way of facilitating the integration of WSD in multilingual applications. It also implic1 http://www.senseval.org/ Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 82–87, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics itly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the translations. Furthermore, this type of corpus-based approach"
W09-2413,P02-1033,0,0.0423092,"e can infer that the English noun “bill” has at most four different senses. These different senses in turn can be grouped in case of synonymy. In the Dutch-French Europarl, for example, both “rekening” and “kosten”, are translated by the French “frais”, which might indicate that both Dutch words are synonymous. Several WSD studies are based on the idea of cross-lingual evidence. Gale et al. (1993) use a bilingual parallel corpus for the automatic creation of a sense-tagged data set, where target words in the source language are tagged with their translation of the word in the target language. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. Ide et al. (2002) use a multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al., 2003). The 83 propo"
W09-2413,P91-1023,0,0.129323,"e´ tablissement de cr´edit, Bank/ Kreditinstitut, banca, banco Table 2: Example of multi-lingual sense labels for the English noun bank 2.1 Corpus and word selection The document collection which serves as the basis for the gold standard construction and system evaluation is the Europarl parallel corpus2 , which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus: English (our target language), Dutch, French, German, Italian and Spanish. All sentences are aligned using a tool based on the Gale and Church (1991) algorithm. We only consider the 1-1 sentence alignments between English and the five other languages (see also Tufis et al. (2004) for a similar strategy). These 1-1 alignments will be made available to all task participants. Participants are free to use other training corpora, but additional translations which are not present in Europarl will not be included in the sense inventory that is used for evaluation. language. The manual construction of the sense inventory will be discussed in Section 2.2. The test data contains 50 instances for 20 nouns from the test data as used in the Cross-Lingu"
W09-2413,P92-1032,0,0.142822,"Senseval1 and its successor Semeval revealed that supervised approaches to WSD usually achieve better results than unsupervised methods (M`arquez et al., 2006). The former use machine learning techniques to induce a classifier from manually sense-tagged data, where each occurrence of a polysemous word gets assigned a sense label from a predefined sense inventory such as WordNet (Fellbaum, 1998). These supervised methods, however, heavily rely on large sensetagged corpora which are very time consuming and expensive to build. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. Although WSD has long time been studied as a stand-alone NLP task, there is a growing feeling in the WSD community that WSD should preferably be integrated in real applications such as Machine Translation or multilingual information retrieval (Agirre and Edmonds, 2006). Several studies have demonstrated that for instance Statistical Machine Translation (SMT) benefits from incorporating a dedicated WSD module (Chan et al., 2007; Carpuat and Wu, 2007). Using translations from a corpus instead of human-defined sense lab"
W09-2413,W02-0808,0,0.440967,"ated by the French “frais”, which might indicate that both Dutch words are synonymous. Several WSD studies are based on the idea of cross-lingual evidence. Gale et al. (1993) use a bilingual parallel corpus for the automatic creation of a sense-tagged data set, where target words in the source language are tagged with their translation of the word in the target language. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. Ide et al. (2002) use a multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al., 2003). The 83 proposed Cross-lingual Word Sense Disambiguation task differs from earlier work (e.g. Ide et al. (2002)) through its independence from an externally defined sense set. The remainder of this paper is organized as follows. In Se"
W09-2413,2005.mtsummit-papers.11,0,0.0404393,"o people living on the [bank] of the river Language NL,F,D,I,ES Sense label oever/dijk, rives/rivage/bord/bords, Ufer, riva, orilla The [bank] of Scotland ... Language NL,F,D,I,ES Sense label bank/kredietinstelling, banque/ e´ tablissement de cr´edit, Bank/ Kreditinstitut, banca, banco Table 2: Example of multi-lingual sense labels for the English noun bank 2.1 Corpus and word selection The document collection which serves as the basis for the gold standard construction and system evaluation is the Europarl parallel corpus2 , which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus: English (our target language), Dutch, French, German, Italian and Spanish. All sentences are aligned using a tool based on the Gale and Church (1991) algorithm. We only consider the 1-1 sentence alignments between English and the five other languages (see also Tufis et al. (2004) for a similar strategy). These 1-1 alignments will be made available to all task participants. Participants are free to use other training corpora, but additional translations which are not present in Europarl will not be included in t"
W09-2413,S07-1009,0,0.0616228,"in a trilingual setting. 3 System evaluation As stated before, systems can participate in two tasks, i.e. systems can either participate in one or more bilingual evaluation tasks or they can participate in the multilingual evaluation task incorporating the five supported languages. The evaluation of the multilingual evaluation task is simply the average of the system scores on the five bilingual evaluation tasks. 3.1 Evaluation strategies For the evaluation of the participating systems we will use an evaluation scheme which is inspired by the English lexical substitution task in SemEval 2007 (McCarthy and Navigli, 2007). The evaluation will be performed using precision and recall (P and R in the equations that follow). We perform both a best result evaluation and a more relaxed evaluation for the top five results. Let H be the set of annotators, T be the set of test items and hi be the set of responses for an item i ∈ T for annotator h ∈ H. Let A be the set of items from T where the system provides at least one answer and ai : i ∈ A be the set of guesses from the system for item i. For each i, we calculate the multiset union (Hi ) for all hi for all h ∈ H and for each unique type (res) in Hi that has an asso"
W09-2413,P03-1058,0,0.23805,"nguage. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. Ide et al. (2002) use a multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al., 2003). The 83 proposed Cross-lingual Word Sense Disambiguation task differs from earlier work (e.g. Ide et al. (2002)) through its independence from an externally defined sense set. The remainder of this paper is organized as follows. In Section 2, we present a detailed description of the cross-lingual WSD task. It introduces the parallel corpus we used, informs on the development and test data and discusses the annotation procedure. Section 3 gives an overview of the different scoring strategies that will be applied. Section 4 concludes this paper. 2 Task set up The cross-lingual Word Sense Disamb"
W09-2413,J03-1002,0,0.00241474,"lish target word. They are not bound to a predefined parallel corpus, but can freely choose the translations from any available resource. Selecting the target words from the set of nouns thats will be used for the Lexical Substitution Task should make it easier for systems to participate in both tasks. 2.2 The sense inventory for the 5 target nouns in the development data and the 20 nouns in the test data is manually built up in three steps. 1. In the first annotation step, the 5 translations of the English word are identified per sentence ID. In order to speed up this identification, GIZA++ (Och and Ney, 2003) is used to generate the initial word alignments for the 5 languages. All word alignments are manually verified. In this step, we might come across multiword translations, especially in Dutch and German which tend to glue parts of compounds together in one orthographic unit. We decided to keep these translations as such, even if they do not correspond exactly to the English target word. In following sentence, the Dutch translation witboek corresponds in fact to the English compound white paper, and not to the English target word paper: English: the European Commission presented its white paper"
W09-2413,C04-1192,0,0.30313,"Missing"
W09-2413,J93-1004,0,\N,Missing
W09-2413,lefever-hoste-2010-construction,1,\N,Missing
W09-2413,S07-1001,0,\N,Missing
W09-2413,E09-1010,0,\N,Missing
W09-2413,W09-2412,0,\N,Missing
W09-2413,P91-1034,0,\N,Missing
W09-2413,P07-1006,0,\N,Missing
W11-1006,D07-1090,0,0.0109685,"T, Google (that disposes of large computing clusters and a network of data centers for Web search) has very valuable assets at its disposal for this task. We can only speculate about the amount of resources that Google uses to train its translation engine. Part of the training data comes from transcripts of United Nations meetings (in six official languages) and those of the European Parliament (Europarl corpus). Google research papers report on a distributed infrastructure that is used to train on up to two trillion tokens, which result in language models containing up to 300 billion ngrams (Brants et al., 2007). 3 ParaSense This section describes the ParaSense WSD system: a multilingual classification-based approach to Word Sense Disambiguation. Instead of using a predefined monolingual sense-inventory such as WordNet, we use a language-independent framework where the word senses are derived automatically from word alignments on a parallel corpus. We used the sentence-aligned Europarl corpus (Koehn, 2005) for the construction of our WSD module. The following six languages were selected: English (our focus language), Dutch, French, German, Italian and Spanish. We only considered the 1-1 sentence alig"
W11-1006,D07-1007,0,0.0253052,"uning problems of this dedicated WSD feature. Specia (2006) used an inductive logic programming-based WSD system which was tested on seven ambiguous verbs in English-Portuguese translation. The latter systems already present promising results for the use of WSD in MT, but really significant improvements in terms of general machine translation qualProceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 52–60, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics ity were for the first time obtained by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Stroppa et al. (2007) take a completely different approach to perform some sort of implicit Word Sense Disambiguation in MT. They introduce context-information features that exploit source similarity, in addition to target similarity that is modeled by the language model, in an SMT framework. For the estimation of these features that are very similar to"
W11-1006,P07-1005,0,0.0271417,"icated WSD feature. Specia (2006) used an inductive logic programming-based WSD system which was tested on seven ambiguous verbs in English-Portuguese translation. The latter systems already present promising results for the use of WSD in MT, but really significant improvements in terms of general machine translation qualProceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 52–60, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics ity were for the first time obtained by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Stroppa et al. (2007) take a completely different approach to perform some sort of implicit Word Sense Disambiguation in MT. They introduce context-information features that exploit source similarity, in addition to target similarity that is modeled by the language model, in an SMT framework. For the estimation of these features that are very similar to the typical WSD local"
W11-1006,P02-1033,0,0.0357851,"subtle change compared to the reference sentence, but it often drastically affects the global understanding of the sentence. Secondly, we explore the potential benefits of a real multilingual approach to WSD. The idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of a polysemous word are often lexicalized across languages (Resnik and Yarowsky, 2000). Many WSD studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual WSD classifiers (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002) or 53 systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). Our WSD system is different in the sense that it is independent from a predefined sense-inventory (it only uses the parallel corpus at hand) and that it is truly multilingual as it incorporates information from four other languages (French, Dutch, Spanish, Italian and German depending on the target language of the classifier). Although our classifiers are still very preliminary in terms of the feature set and parameters that are used, we obtain interesting results on our test sample of"
W11-1006,J93-1004,0,0.0833292,"slation of an ambiguous word might be a subtle change compared to the reference sentence, but it often drastically affects the global understanding of the sentence. Secondly, we explore the potential benefits of a real multilingual approach to WSD. The idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of a polysemous word are often lexicalized across languages (Resnik and Yarowsky, 2000). Many WSD studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual WSD classifiers (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002) or 53 systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). Our WSD system is different in the sense that it is independent from a predefined sense-inventory (it only uses the parallel corpus at hand) and that it is truly multilingual as it incorporates information from four other languages (French, Dutch, Spanish, Italian and German depending on the target language of the classifier). Although our classifiers are still very preliminary in terms of the feature set and parameters that are used, we obtain i"
W11-1006,P05-1050,0,0.012163,"• local context features related to a window of three words preceding and following the focus word containing for each of these words their full form, lemma, Part-of-Speech and chunk information These local context features are to be considered as a basic feature set. The Senseval evaluation exercises have shown that feeding additional information sources to the classifier results in better system performance (Agirre and Martinez, 2004). In future experiments we plan to integrate a.o. lemma information on the surrounding content words and semantic analysis (e.g. Singular Value Decomposition (Gliozzo et al., 2005)) in order to detect latent correlations between terms. 3.2.2 Translation Features In addition to the commonly deployed local context features, we also extracted a set of binary bagof-words features from the aligned translations that are not the target language of the classifier (e.g. for the French classifier, we extract bag-of-words features from the Italian, Spanish, Dutch and German aligned translations). We preprocessed all aligned translations by means of the Treetagger tool (Schmid, 1994) that outputs Part-of-Speech and lemma information. Per ambiguous focus word, a list of all content"
W11-1006,N03-1017,0,0.00895131,"stical Machine Translation Systems For our experiments, we analyzed the behavior of two phrase-based statistical machine translation (SMT) systems on the translation of ambiguous nouns. SMT generates translations on the basis of statistical models whose parameters are derived from the analysis of sentence-aligned parallel text corpora. Phrase-based SMT is considered as the dominant paradigm in MT research today. It combines a phrase translation model (which is based on the noisy channel model) and a phrase-based decoder in order to find the most probable translation e of a foreign sentence f (Koehn et al., 2003). Usually the Bayes rule is used to reformulate this translation probability: argmaxe p(e|f ) = argmaxe p(f |e)p(e) This allows for a language model p(e) that guarantees the fluency and grammatical correctness of the translation, and a separate translation model p(f |e) that focusses on the quality of the translation. Training of both the language model (on monolingual data) as well as the translation model (on bilingual text corpora) requires large amounts of text data. Research has pointed out that adding more training data, both for the translation as for the language models, results in bet"
W11-1006,P07-2045,0,0.00596872,"(Callison-Burch et al., 2009). Therefore it is important to notice that our comparison of the two SMT systems is somewhat unfair, as we compared the Moses research system (that was trained on the Europarl corpus) with the Google commercial system that is trained on a much larger data set. It remains an interesting exercise though, as we consider the commercial system as the upper bound of how far current SMT can get in case it has unlimited access to text corpora and computational resources. 2.1 Moses The first statistical machine translation system we used is the off-the-shelf Moses toolkit (Koehn et al., 2007). As the Moses system is open-source, well documented, supported by a very lively users forum and reaches state-of-the-art performance, it has quickly been adopted by the community and highly stimulated development in the SMT field. It also features factored translation models, which enable the integration of linguistic and other information at the word level. This makes Moses a good candidate to experiment with for example a dedicated WSD module, that requires more enhanced linguistic information (such as lemmas and Part-of-Speech tags). We trained Moses for English–French and English– Dutch"
W11-1006,2005.mtsummit-papers.11,0,0.0320149,"rpus). Google research papers report on a distributed infrastructure that is used to train on up to two trillion tokens, which result in language models containing up to 300 billion ngrams (Brants et al., 2007). 3 ParaSense This section describes the ParaSense WSD system: a multilingual classification-based approach to Word Sense Disambiguation. Instead of using a predefined monolingual sense-inventory such as WordNet, we use a language-independent framework where the word senses are derived automatically from word alignments on a parallel corpus. We used the sentence-aligned Europarl corpus (Koehn, 2005) for the construction of our WSD module. The following six languages were selected: English (our focus language), Dutch, French, German, Italian and Spanish. We only considered the 1-1 sentence alignments between English and the five other languages. This way we obtained a six-lingual sentence-aligned subcorpus of Europarl, that contains 884.603 sentences per language. For our experiments we used the lexical sample of twenty ambiguous nouns that was also used in the SemEval-2010 ”Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b), which consists in assigning a cor"
W11-1006,lefever-hoste-2010-construction,1,0.866505,". We used the sentence-aligned Europarl corpus (Koehn, 2005) for the construction of our WSD module. The following six languages were selected: English (our focus language), Dutch, French, German, Italian and Spanish. We only considered the 1-1 sentence alignments between English and the five other languages. This way we obtained a six-lingual sentence-aligned subcorpus of Europarl, that contains 884.603 sentences per language. For our experiments we used the lexical sample of twenty ambiguous nouns that was also used in the SemEval-2010 ”Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b), which consists in assigning a correct translation in five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect all relevant translations for the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings on our parallel corpus. The obtained word alignment output was then considered to be the classification label for the training instances for a given classifier (e.g. the French translation resulting from the word alignment is the label that is used to train the French c"
W11-1006,S10-1003,1,0.866454,". We used the sentence-aligned Europarl corpus (Koehn, 2005) for the construction of our WSD module. The following six languages were selected: English (our focus language), Dutch, French, German, Italian and Spanish. We only considered the 1-1 sentence alignments between English and the five other languages. This way we obtained a six-lingual sentence-aligned subcorpus of Europarl, that contains 884.603 sentences per language. For our experiments we used the lexical sample of twenty ambiguous nouns that was also used in the SemEval-2010 ”Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b), which consists in assigning a correct translation in five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect all relevant translations for the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings on our parallel corpus. The obtained word alignment output was then considered to be the classification label for the training instances for a given classifier (e.g. the French translation resulting from the word alignment is the label that is used to train the French c"
W11-1006,P10-1023,0,0.0130162,"ere we only have the English test sentences at our disposal), we had to adopt a different strategy. We decided to use the Google Translate API to automatically generate translations for all English test instances in the five target languages. This automatic translation process can be done using whatever machine translation tool, but we chose the Google API because of its easy integration. Online machine translation tools have already been used before to create artificial parallel corpora that were used for NLP tasks such as for instance Named Entity Recognition (Shah et al., 2010). Similarly, Navigli and Ponzetto (2010) used the Google Translate API to enrich BabelNet, a wide-coverage multilingual semantic network, with lexical information for all languages. Once the automatic aligned translations were generated, we preprocessed them in the same way as we did for the aligned training translations. In a next step, we again selected all content words from these translations and constructed the binary bag-of-words features. 4 Evaluation To evaluate the two machine translation systems as well as the ParaSense system on their performance on the lexical sample of twenty ambiguous words, we used the sense inventory"
W11-1006,P03-1058,0,0.0346299,"word might be a subtle change compared to the reference sentence, but it often drastically affects the global understanding of the sentence. Secondly, we explore the potential benefits of a real multilingual approach to WSD. The idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of a polysemous word are often lexicalized across languages (Resnik and Yarowsky, 2000). Many WSD studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual WSD classifiers (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002) or 53 systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). Our WSD system is different in the sense that it is independent from a predefined sense-inventory (it only uses the parallel corpus at hand) and that it is truly multilingual as it incorporates information from four other languages (French, Dutch, Spanish, Italian and German depending on the target language of the classifier). Although our classifiers are still very preliminary in terms of the feature set and parameters that are used, we obtain interesting result"
W11-1006,J03-1002,0,0.00266907,"s. This way we obtained a six-lingual sentence-aligned subcorpus of Europarl, that contains 884.603 sentences per language. For our experiments we used the lexical sample of twenty ambiguous nouns that was also used in the SemEval-2010 ”Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b), which consists in assigning a correct translation in five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect all relevant translations for the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings on our parallel corpus. The obtained word alignment output was then considered to be the classification label for the training instances for a given classifier (e.g. the French translation resulting from the word alignment is the label that is used to train the French classifier). This way we obtained all class labels (or oracle translations) for all training instances for our five classifiers (English as an input language and French, German, Dutch, Italian and Spanish as target languages). For the experiments described in this paper, we focused on the English– Frenc"
W11-1006,P02-1040,0,0.0827205,"nt from previous research in two aspects. Firstly, we evaluate the performance of two state-of-the-art SMT systems and a dedicated WSD system on the translation of ambiguous words. The comparison is done against a manually constructed gold-standard for two language pairs, viz. English–French and English–Dutch. Although it is crucial to measure the general translation quality after integrating a dedicated WSD module in the SMT system, we think it is equally interesting to conduct a dedicated evaluation of the translation quality on ambiguous nouns. Standard SMT evaluation metrics such as BLEU (Papineni et al., 2002) or edit-distance metrics (e.g. Word Error Rate) measure the global overlap of the translation with a reference, and are thus not very sensitive to WSD errors. The mistranslation of an ambiguous word might be a subtle change compared to the reference sentence, but it often drastically affects the global understanding of the sentence. Secondly, we explore the potential benefits of a real multilingual approach to WSD. The idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of a polysemous word are often lexicalized"
W11-1006,P06-3010,0,0.0238316,"f ambiguous words are gathered from a parallel corpus by means of word alignment. The authors reported improvements on two simplified translation tasks: word translation and blank filling. The evaluation was done on an English-French parallel corpus but is confronted with the important limitation of having only one valid translation (the aligned translation in the parallel corpus) as a gold standard translation. Cabezas and Resnik (2005) tried to improve an SMT system by adding additional translations to the phrase table, but were confronted with tuning problems of this dedicated WSD feature. Specia (2006) used an inductive logic programming-based WSD system which was tested on seven ambiguous verbs in English-Portuguese translation. The latter systems already present promising results for the use of WSD in MT, but really significant improvements in terms of general machine translation qualProceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 52–60, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics ity were for the first time obtained by Carpuat and Wu (2007) and Chan et al. (2007). Both papers d"
W11-1006,2007.tmi-papers.28,0,0.0196963,"of this dedicated WSD feature. Specia (2006) used an inductive logic programming-based WSD system which was tested on seven ambiguous verbs in English-Portuguese translation. The latter systems already present promising results for the use of WSD in MT, but really significant improvements in terms of general machine translation qualProceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 52–60, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics ity were for the first time obtained by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Stroppa et al. (2007) take a completely different approach to perform some sort of implicit Word Sense Disambiguation in MT. They introduce context-information features that exploit source similarity, in addition to target similarity that is modeled by the language model, in an SMT framework. For the estimation of these features that are very similar to"
W11-1006,C04-1192,0,0.0804087,"Missing"
W11-1006,H05-1097,0,0.0185866,"nnotators), whereas very detailed sense distinctions are often irrelevant for practical applications. In addition to this, there is a growing feeling in the community that WSD should be used and evaluated in real application such as Machine Translation (MT) or Information Retrieval (IR) (Agirre and Edmonds, 2006). An important line of research consists in the development of dedicated WSD modules for MT. Instead of assigning a sense label from a monolingual sense-inventory to the ambiguous words, the WSD system has to predict a correct translation for the ambiguous word in a given context. In (Vickrey et al., 2005), the problem was defined as a word translation task. The translation choices of ambiguous words are gathered from a parallel corpus by means of word alignment. The authors reported improvements on two simplified translation tasks: word translation and blank filling. The evaluation was done on an English-French parallel corpus but is confronted with the important limitation of having only one valid translation (the aligned translation in the parallel corpus) as a gold standard translation. Cabezas and Resnik (2005) tried to improve an SMT system by adding additional translations to the phrase"
W11-1006,W09-2413,1,\N,Missing
W11-1006,W09-0401,0,\N,Missing
W17-5218,N10-1122,0,0.0293077,"2 . For banking there are 22 such possible combinations, for retail 24 and for HR 23. Table 2 gives an overview of the three largest main categories per domain. In a next step, sentiment bearing words were selected, assigned a polarity: positive, negative or neutral (OpinEx), and linked to the appropriate aspect term (is about arrow). All annotations were carried out with the BRAT rapid annotation tool (Stenetorp et al., 2012). Datasets and Annotations In the past, ABSA datasets have been annotated comprising movie reviews (Thet et al., 2010), reviews for electronic products(Hu and Liu, 2004; Brody and Elhadad, 2010), and restaurant reviews (Brody and Elhadad, 2010; Ganu et al., 2009). As mentioned above, in the framework of three SemEval shared tasks (Pontiki et al., 2014, 2015, 2016), several benchmark review datasets coming from various domains (electronics, hotels, restaurants, and telecom) and languages (English, Dutch, French, Arabic, Chinese, Spanish, Turkish and Russian) have been made publicly available. For the work presented here, direct customer feedback data written in Dutch was collected in three domains: banking, retail and human resources (HR). The data provider for the first domain, banki"
W17-5218,S16-1044,0,0.0265928,"Missing"
W17-5218,L16-1465,1,0.851826,"Missing"
W17-5218,de-smedt-daelemans-2012-vreselijk,0,0.533722,"Missing"
W17-5218,S15-2082,0,0.0794152,"Missing"
W17-5218,S14-2004,0,0.548154,"Belgium 2 Hello Customer, Belgium {orphee.declercq, els.lefever, gillesm.jacobs, veronique.hoste}@ugent.be tijl@hellocustomer.com Abstract tems do not only try to distinguish the positive from the negative utterances, but also strive to detect the target of the opinion, which comes down to a very fine-grained sentiment analysis task and “almost all real-life sentiment analysis systems in industry should be based on this level of analysis” (Liu, 2015, p10). This fine-grained sentiment analysis task received special attention in the framework of three SemEval shared tasks: SemEval 2014 Task 4 (Pontiki et al., 2014) and SemEval 2015 Task 12 (Pontiki et al., 2015), which focussed on English customer reviews, and SemEval 2016 Task 5 (Pontiki et al., 2016) where seven other languages were also included. Each time the idea was to perform three subtasks: (i) extract all aspect expressions of the entities, (ii) categorize these aspect expressions into predefined categories and (iii) determine whether an opinion on an aspect is positive, negative or neutral. In this paper, we discuss a fine-grained sentiment analysis pipeline to deal with qualitative Dutch feedback data coming from three different domains: bank"
W17-5218,E09-1046,0,0.257749,".2 95.4 Retail F-1 94.9 93.2 93.4 95.2 Prec 89.6 95.6 91.0 95.8 Rec 90.9 95.5 91.4 95.8 HR F-1 89.2 95.6 89.7 95.8 Prec 95.2 94.9 96.5 95.9 Rec 95.4 95.1 96.8 96.2 F-1 95.0 94.4 96.4 95.9 Table 4: Precision, recall, and F-1 scores for aspect term extraction on held-out test sets. tem. As information sources, we implemented the following features: (1) bag-of-words: binary token unigram features, (2) lexicon lookup features based on domain-specific lexicons extracted from the training data, as well as existing sentiment lexicons for Dutch, i.e. Pattern (De Smedt and Daelemans, 2012) and Duoman (Jijkoun and Hofmann, 2009), (3) negator: flips the value of negated lexicon matches and (4) the predicted category of the aspect term. For these experiments, we also envisaged the three different setups: in-domain, crossdomain, and all domain. It is important to mention that for sentiment prediction, the entire sentence is considered for the construction of the features. As a result, conflicting sentiments will be ruled out. In future work, we intend to limit the context window of the detected aspect term. As the polarity detection takes into account the output of the previous two steps, this task was also evaluated by"
W17-5218,S15-2130,0,0.0186697,"ree characters as an approximate suffix; (2) lemma, (3) CGN part-ofspeech (PoS) tag, (4) syntactic chunk, and (5) Named Entity label as provided by the LeTs preprocessing toolkit (Van de Kauter et al., 2013). Both full labels and coarse super-category for PoS, chunk, and NE labels were included as features. 3.2 Aspect Category Classification The aspect category classification subtask requires a system able to label a large variety of classes, in our case 22, 24 and 23 categories. The two systems achieving the best results for SemEval 2015 both used a classification approach (Toh and Su, 2015; Saias, 2015). Furthermore, especially lexical features in the form of bag-of-words have proven successful. The best system (Toh and Su, 2015) also incorporated lexical-semantic features in the form of clusters learned from a large corpus of reference review data, whereas the secondbest (Saias, 2015) applied filtering heuristics on the classification output and thus solely relied on lexical information for the classification. For SemEval 2016 Toh and Su (2016) discovered that when the probability output of a Deep Convolutional Neural Network (Severyn and Moschitti, 2015) was added as additional features, t"
W17-5218,S15-2127,0,0.177309,"Missing"
W17-5218,S15-2079,0,0.0289324,"Missing"
W17-5218,E12-2021,0,0.0209746,"aspect term and assigned it to a predefined aspect category (CatEx). These aspect categories are domain-dependent and consist of a main category (e.g. Personnel) and subcategory (e.g. quality)2 . For banking there are 22 such possible combinations, for retail 24 and for HR 23. Table 2 gives an overview of the three largest main categories per domain. In a next step, sentiment bearing words were selected, assigned a polarity: positive, negative or neutral (OpinEx), and linked to the appropriate aspect term (is about arrow). All annotations were carried out with the BRAT rapid annotation tool (Stenetorp et al., 2012). Datasets and Annotations In the past, ABSA datasets have been annotated comprising movie reviews (Thet et al., 2010), reviews for electronic products(Hu and Liu, 2004; Brody and Elhadad, 2010), and restaurant reviews (Brody and Elhadad, 2010; Ganu et al., 2009). As mentioned above, in the framework of three SemEval shared tasks (Pontiki et al., 2014, 2015, 2016), several benchmark review datasets coming from various domains (electronics, hotels, restaurants, and telecom) and languages (English, Dutch, French, Arabic, Chinese, Spanish, Turkish and Russian) have been made publicly available. F"
W17-5218,S15-2083,0,0.0174104,"e final two and three characters as an approximate suffix; (2) lemma, (3) CGN part-ofspeech (PoS) tag, (4) syntactic chunk, and (5) Named Entity label as provided by the LeTs preprocessing toolkit (Van de Kauter et al., 2013). Both full labels and coarse super-category for PoS, chunk, and NE labels were included as features. 3.2 Aspect Category Classification The aspect category classification subtask requires a system able to label a large variety of classes, in our case 22, 24 and 23 categories. The two systems achieving the best results for SemEval 2015 both used a classification approach (Toh and Su, 2015; Saias, 2015). Furthermore, especially lexical features in the form of bag-of-words have proven successful. The best system (Toh and Su, 2015) also incorporated lexical-semantic features in the form of clusters learned from a large corpus of reference review data, whereas the secondbest (Saias, 2015) applied filtering heuristics on the classification output and thus solely relied on lexical information for the classification. For SemEval 2016 Toh and Su (2016) discovered that when the probability output of a Deep Convolutional Neural Network (Severyn and Moschitti, 2015) was added as addition"
W17-5218,S16-1045,0,0.0283281,"Missing"
W18-3101,W14-2907,0,0.0571157,"Missing"
W18-3101,W06-0901,0,0.140996,"calized. Thus, the need for flexible data-driven approaches, which do not require predefined ontological resources, arises. R¨onnqvist and Sarlin (2017) provide an example of successful datadriven, weakly-supervised distress event detection based on bank entity mentions. Here, bank distress events are conceptualized as mentions of bank entities in a time-window and no typology classification is assigned. We are not aware of any published data-driven, supervised event detection approaches for the economic domain. However, in general domain event extraction, as embodied by projects such as ACE (Ahn, 2006) and ERE/TACKBP (Mitamura et al., 2016), supervised methods for extraction of event structures are predominant because of their promise of improved performance. 2 As discussed in Sprugnoli and Tonelli (2017), the definition of events in the field of information extraction differs widely. In this work, we employ a conceptualization of economic event detection as ‘retrieving textually reported real-world occurrences, actions, relations, and situations involving companies and firms’. Unlike other supervised data-driven ‘event extraction’ tasks such as in the ACE/ERE programs (Aguilar et al., 2014"
W18-3101,L16-1051,1,0.876711,"014), we do not conceptualize events as structured schemata/frames, but more limited as textual mentions of real-world occurrences. The task presented here is often also referred to as event ‘mention’, ‘nugget’, or ‘trigger’ detection. The classification experiments described here are currently at the sentence-level, but our event annotation scheme is token-level. Data Description In this section, we describe the SentiFM economic event dataset collection and annotation. The annotated dataset consists of an English and Dutch news corpus. While in this paper the focus is on English, we refer to Lefever and Hoste (2016) for a pilot study on Dutch event detection and a description of the Dutch event data. A reference to where to download the SentiFM dataset can be found in Section 7. The goal of the SentiFM dataset is to enable supervised data-driven event detection in companyspecific economic news. For English, we downloaded articles from the newspaper The Financial Times using the ProQuest Newsstand by means of keyword-search. The keywords were manually determined based on a subsample of random articles as being indicative to one of the event types. All articles were published between November 2004 and Nove"
W18-3101,D14-1162,0,0.0875982,"ng layer which feeds into an LSTM block. The LSTM block is connected to an output layer with a sigmoid activation function. Bi-directionality of the LSTM-layer is tested in hyper-parameter optimization. We use the Adam optimization algorithm with binary cross-entropy loss function. The embedding layer turns positive integers, in our case hold-in set token indexes, in dense vectors with fixed dimensionality. An existing word embedding matrix can be used in the input-layer which tunes pre-trained word vectors. Three embedded inputs were tested with the multi-label set-up: 200 dimensional GloVe (Pennington et al., 2014) word vectors trained on the hold-in set, 300 dimensional GloVe vectors trained on a 6 billion token corpus of Wikipedia (2014) + Gigawords5B1 (henceforth, 6B corpus), and no pre-trained embeddings. The latter means our classifier trains embedded word-representations (with a fixed dimensionality of 200) itself based on the token sequences of the hold-in set. We evaluated our own GloVe models on an analogy quality assessment task provided with the word2vec source code2 . We picked the highest dimensional word vector model from the top ten ranking on the analogy task. We excluded lower dimension"
W18-3101,E12-2021,0,0.122022,"Missing"
W18-3101,P13-1086,0,0.332235,"enboom et al., 2013; Du et al., 2016). These use rule-sets or ontology knowledge-bases which are largely or fully created by hand. The Stock Sonar project (Feldman et al., 2011) notably uses domain experts to formulate event rules for rule-based stock sentiment analysis. This technology has been successfully used in assessing the impact of events on the stock market (Boudoukh et al., 2016) and in formulating trading strategies (Ben Ami and Feldman, 2017). Other approaches conceptualize economic event detection as the extraction of event tuples (Ding et al., 2015) or as semantic frame parsing (Xie et al., 2013). A drawback of knowledge-based information extraction methods is that creating rules and ontologies is a difficult, time-consuming process. Furthermore, defining a set of strict rules often reThis paper presents a dataset and supervised classification approach for economic event detection in English news articles. Currently, the economic domain is lacking resources and methods for data-driven supervised event detection. The detection task is conceived as a sentence-level classification task for 10 different economic event types. Two different machine learning approaches were tested: a rich fe"
