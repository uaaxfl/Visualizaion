2020.aacl-main.2,D19-1572,0,0.0177822,"hods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual B"
2020.aacl-main.2,Q19-1038,0,0.0286828,"enchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-T"
2020.aacl-main.2,Q18-1039,0,0.0246351,"dge from monolingual pretrained models to multilingual ones. Experimental results on two cross-lingual classification benchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine"
2020.aacl-main.2,D19-1138,0,0.0253137,"nslation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual BERT (Devlin et al., 2019) for multilingual pretrained language models. The pretrained model uses the BERT-style Transformer (Vaswani et al., 2017) architecture, and follows the similar fine-tuning procedure as BERT for text classification, which is illustrated in Figure 1(a). To be specific, the first input token of the models is always a special classification token Contribution during internship at Microso"
2020.aacl-main.2,P19-1493,0,0.0185862,"fication models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual BERT (Devlin et al., 2019) for multilingual pretrained language models. The pretrained model uses the BERT-style Transformer (Vaswani et al., 2017) architecture, and follows the similar fine-tuning procedure as BERT for text classification, which is illustrated in Figure 1(a). To be specific, the first input token of the models is always a special classification token"
2020.aacl-main.2,2020.acl-main.747,0,0.067487,"Missing"
2020.aacl-main.2,P10-1114,0,0.0857345,"Missing"
2020.aacl-main.2,P16-1162,0,0.0418231,"Missing"
2020.aacl-main.2,D19-1077,0,0.0206782,"xploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Background: Multilingual Fine-Tuning We use multilingual BERT (Devlin et al., 2019) for multilingual pretrained language models. The pretrained model uses the BERT-style Transformer (Vaswani et al., 2017) architecture, and follows the similar fine-tuning procedure as BERT for text classification, which is illustrated in Figure 1(a). To be specific, the first input token of the models is always a special classification token Contribution during"
2020.aacl-main.2,P17-1130,0,0.0144372,"sferring the knowledge from monolingual pretrained models to multilingual ones. Experimental results on two cross-lingual classification benchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these"
2020.aacl-main.2,W18-3023,0,0.0238199,"classification benchmarks show that our methods outperform vanilla multilingual fine-tuning. 1 Introduction Supervised text classification heavily relies on manually annotated training data, while the data are usually only available in rich-resource languages, such as English. It requires great effort to make the resources available in other languages. Various methods have been proposed to build cross-lingual classification models by exploiting machine translation systems (Xu and Yang, 2017; Chen et al., 2018; Conneau et al., 2018), and learning multilingual embeddings (Conneau et al., 2018; Yu et al., 2018; Artetxe and Schwenk, 2019; Eisenschlos et al., 2019). Recently, multilingual pretrained language models have shown surprising cross-lingual effectiveness on a wide range of downstream tasks (Devlin et al., 2019; Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2020a,b). Even without using any parallel corpora, the pretrained models can still perform zero-shot cross-lingual classification (Pires et al., 2019; Wu and Dredze, 2019; Keung et al., 2019). That is, these models can be fine-tuned in a source language, and then directly evaluated in other target languages. Despite ∗ 2 Back"
2020.aacl-main.24,D18-1269,0,0.0158392,"the success of our two-stage training strategy, we would like to emphasize two strengths. First, as mentioned before, our easy-to-hard training procedure matches the core idea of Curriculum Learning (Bengio et al., 2009), which smooths the training and help the model generalize better. Second, the two-stage procedure inherently introduces a new self-supervised task, which could take the advantage of Multitask Learning (Caruana, 1993). 5 Related Work Multilingual Representation Learning Learning cross-lingual representations are useful for downstream tasks such as cross-lingual classification (Conneau et al., 2018; Yang et al., 2019b), cross-lingual retrieval (Zweigenbaum et al., 2017; Artetxe and Schwenk, 2019) and cross-lingual QA (Artetxe et al., 2019; Lewis et al., 2019; Clark et al., 2020). Earlier work on multilingual representations exploiting parallel corpora (Luong et al., 2015; Gouws et al., 2015) or a bilingual dictionary to learn a linear mapping (Mikolov et al., 2013; Faruqui and Dyer, 2014). Subsequent methods explored self-training (Artetxe et al., 2017) and unsupervised learning (Zhang et al., 2017; Artetxe et al., 2018; Lample et al., 2018b). Recently, multilingual pretrained encoders"
2020.aacl-main.24,N19-1423,0,0.471701,"台1 风2 是热3 带4 低气5 压6 的一种7 。 EN Typhoon is a type of tropical depression. Table 1: A sentence example in Japanese (JA), Traditional Chinese (T-ZH) and Simplified Chinese (S-ZH) with its English translation (EN). The characters that already share the same Unicode are marked with an underline. In this work, we further match characters with identical meanings but different Unicode, then merge them. Characters eligible to be merged together are marked with the same superscript. Introduction Recently, Pretrained Language Models have shown promising performance on many NLP tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019c; Lan et al., 2020). Many attempts have been made to train a model that supports multiple languages. Among them, Multilingual BERT (mBERT) (Devlin et al., 2019) is released as a part of BERT. It directly adopts the same model architecture and training objective, and is trained on Wikipedia in different languages. XLM (Lample and Conneau, 2019) is proposed with an additional language embedding and a new training ∗ This work was done during Canwen’s internship at Microsoft Research Asia. 1 The code and pretrained weights are available at https: //github.com/"
2020.aacl-main.24,P17-1042,0,0.0115868,"ual Representation Learning Learning cross-lingual representations are useful for downstream tasks such as cross-lingual classification (Conneau et al., 2018; Yang et al., 2019b), cross-lingual retrieval (Zweigenbaum et al., 2017; Artetxe and Schwenk, 2019) and cross-lingual QA (Artetxe et al., 2019; Lewis et al., 2019; Clark et al., 2020). Earlier work on multilingual representations exploiting parallel corpora (Luong et al., 2015; Gouws et al., 2015) or a bilingual dictionary to learn a linear mapping (Mikolov et al., 2013; Faruqui and Dyer, 2014). Subsequent methods explored self-training (Artetxe et al., 2017) and unsupervised learning (Zhang et al., 2017; Artetxe et al., 2018; Lample et al., 2018b). Recently, multilingual pretrained encoders have shown its effec207 Method UnihanLM −cluster −character PAWS-X Wiki ASPEC-JC Shuffled-train ZH JA ZH→JA JA→ZH ZH→JA JA→ZH 82.7 81.5 82.0 80.5 79.2 80.1 33.53 29.33 - 28.70 20.93 - 44.59 42.34 - 40.58 39.24 - Table 8: The results of ablation study on text classification and UMT. “-cluster” and “-character” indicate the model trained without the cluster-level pretraining and character-level pretraining, respectively. The metrics for PAWS-X and ASPEC-JC are a"
2020.aacl-main.24,P18-1073,0,0.0117093,"e useful for downstream tasks such as cross-lingual classification (Conneau et al., 2018; Yang et al., 2019b), cross-lingual retrieval (Zweigenbaum et al., 2017; Artetxe and Schwenk, 2019) and cross-lingual QA (Artetxe et al., 2019; Lewis et al., 2019; Clark et al., 2020). Earlier work on multilingual representations exploiting parallel corpora (Luong et al., 2015; Gouws et al., 2015) or a bilingual dictionary to learn a linear mapping (Mikolov et al., 2013; Faruqui and Dyer, 2014). Subsequent methods explored self-training (Artetxe et al., 2017) and unsupervised learning (Zhang et al., 2017; Artetxe et al., 2018; Lample et al., 2018b). Recently, multilingual pretrained encoders have shown its effec207 Method UnihanLM −cluster −character PAWS-X Wiki ASPEC-JC Shuffled-train ZH JA ZH→JA JA→ZH ZH→JA JA→ZH 82.7 81.5 82.0 80.5 79.2 80.1 33.53 29.33 - 28.70 20.93 - 44.59 42.34 - 40.58 39.24 - Table 8: The results of ablation study on text classification and UMT. “-cluster” and “-character” indicate the model trained without the cluster-level pretraining and character-level pretraining, respectively. The metrics for PAWS-X and ASPEC-JC are accuracy and BLEU, respectively. tiveness for learning deep cross-lin"
2020.aacl-main.24,Q19-1038,0,0.0130377,"t, as mentioned before, our easy-to-hard training procedure matches the core idea of Curriculum Learning (Bengio et al., 2009), which smooths the training and help the model generalize better. Second, the two-stage procedure inherently introduces a new self-supervised task, which could take the advantage of Multitask Learning (Caruana, 1993). 5 Related Work Multilingual Representation Learning Learning cross-lingual representations are useful for downstream tasks such as cross-lingual classification (Conneau et al., 2018; Yang et al., 2019b), cross-lingual retrieval (Zweigenbaum et al., 2017; Artetxe and Schwenk, 2019) and cross-lingual QA (Artetxe et al., 2019; Lewis et al., 2019; Clark et al., 2020). Earlier work on multilingual representations exploiting parallel corpora (Luong et al., 2015; Gouws et al., 2015) or a bilingual dictionary to learn a linear mapping (Mikolov et al., 2013; Faruqui and Dyer, 2014). Subsequent methods explored self-training (Artetxe et al., 2017) and unsupervised learning (Zhang et al., 2017; Artetxe et al., 2018; Lample et al., 2018b). Recently, multilingual pretrained encoders have shown its effec207 Method UnihanLM −cluster −character PAWS-X Wiki ASPEC-JC Shuffled-train ZH J"
2020.aacl-main.24,P17-2096,0,0.0246344,"ccuracy and BLEU, respectively. tiveness for learning deep cross-lingual representations (Eriguchi et al., 2018; Pires et al., 2019; Wu and Dredze, 2019; Lample and Conneau, 2019; Conneau et al., 2019; Huang et al., 2019). Word Segmentation Word segmentation is often formalized as a sequence tagging task. It requires lexical knowledge to split a character sequence into a word list that can be used for downstream tasks. This step is necessary for many earlier NLP systems for Chinese and Japanese. Recent work on Chinese Word Segmentation (Wang and Xu, 2017; Zhou et al., 2017; Yang et al., 2017; Cai et al., 2017; Chen et al., 2017b; Yang et al., 2019a) and Japanese Word Segmentation (Kaji and Kitsuregawa, 2014; Fujinuma and II, 2017; Kitagawa and Komachi, 2018) exploit deep neural networks and focus on building end-to-end sequence tagging models. Unsupervised Machine Translation Recently, machine translation systems have demonstrated near human-level performance on some languages. However, it depends on the availability of large amounts of parallel sentences. Unsupervised Machine Translation addresses this problem by exploiting monolingual corpora which can be easily constructed. Lample et al. (2018a"
2020.aacl-main.24,E14-1049,0,0.00950364,"tage of Multitask Learning (Caruana, 1993). 5 Related Work Multilingual Representation Learning Learning cross-lingual representations are useful for downstream tasks such as cross-lingual classification (Conneau et al., 2018; Yang et al., 2019b), cross-lingual retrieval (Zweigenbaum et al., 2017; Artetxe and Schwenk, 2019) and cross-lingual QA (Artetxe et al., 2019; Lewis et al., 2019; Clark et al., 2020). Earlier work on multilingual representations exploiting parallel corpora (Luong et al., 2015; Gouws et al., 2015) or a bilingual dictionary to learn a linear mapping (Mikolov et al., 2013; Faruqui and Dyer, 2014). Subsequent methods explored self-training (Artetxe et al., 2017) and unsupervised learning (Zhang et al., 2017; Artetxe et al., 2018; Lample et al., 2018b). Recently, multilingual pretrained encoders have shown its effec207 Method UnihanLM −cluster −character PAWS-X Wiki ASPEC-JC Shuffled-train ZH JA ZH→JA JA→ZH ZH→JA JA→ZH 82.7 81.5 82.0 80.5 79.2 80.1 33.53 29.33 - 28.70 20.93 - 44.59 42.34 - 40.58 39.24 - Table 8: The results of ablation study on text classification and UMT. “-cluster” and “-character” indicate the model trained without the cluster-level pretraining and character-level pr"
2020.aacl-main.24,I17-2013,0,0.0608708,"Missing"
2020.aacl-main.24,D19-1252,0,0.216738,"BERT) (Devlin et al., 2019) is released as a part of BERT. It directly adopts the same model architecture and training objective, and is trained on Wikipedia in different languages. XLM (Lample and Conneau, 2019) is proposed with an additional language embedding and a new training ∗ This work was done during Canwen’s internship at Microsoft Research Asia. 1 The code and pretrained weights are available at https: //github.com/JetRunner/unihan-lm. objective (translation language modeling, TLM). XLM-R (Conneau et al., 2019) has a larger size and is trained with more data. Based on XLM, Unicoder (Huang et al., 2019) collects more data and uses multi-task learning to train on three supervised tasks. The census of cross-lingual approaches is to allow lexical information to be shared between languages. XLM and mBERT exploit shared lexical information by Byte Pair Encoding (BPE) (Sennrich et al., 2016) and WordPiece (Wu et al., 2016), respectively. However, these automatically learned shared representations have been criticized by recent work (K et al., 2020), which reveals their limitations in sharing meaningful semantics across languages. Also, words in both Chinese and Japanese are short, which prohibits"
2020.aacl-main.24,P17-1110,0,0.0457722,"Missing"
2020.aacl-main.24,2020.tacl-1.30,0,0.0139913,"ulum Learning (Bengio et al., 2009), which smooths the training and help the model generalize better. Second, the two-stage procedure inherently introduces a new self-supervised task, which could take the advantage of Multitask Learning (Caruana, 1993). 5 Related Work Multilingual Representation Learning Learning cross-lingual representations are useful for downstream tasks such as cross-lingual classification (Conneau et al., 2018; Yang et al., 2019b), cross-lingual retrieval (Zweigenbaum et al., 2017; Artetxe and Schwenk, 2019) and cross-lingual QA (Artetxe et al., 2019; Lewis et al., 2019; Clark et al., 2020). Earlier work on multilingual representations exploiting parallel corpora (Luong et al., 2015; Gouws et al., 2015) or a bilingual dictionary to learn a linear mapping (Mikolov et al., 2013; Faruqui and Dyer, 2014). Subsequent methods explored self-training (Artetxe et al., 2017) and unsupervised learning (Zhang et al., 2017; Artetxe et al., 2018; Lample et al., 2018b). Recently, multilingual pretrained encoders have shown its effec207 Method UnihanLM −cluster −character PAWS-X Wiki ASPEC-JC Shuffled-train ZH JA ZH→JA JA→ZH ZH→JA JA→ZH 82.7 81.5 82.0 80.5 79.2 80.1 33.53 29.33 - 28.70 20.93 -"
2020.aacl-main.24,C14-1027,0,0.022663,". analyze, the criterion for segmenting Chinese characters can be learned with a Japanese corpus and then transferred to CWS. However, since no kana is present in CWS, the model cannot successfully segment kanas, when performing zero-shot inference on JWS. 4.2 Word Segmentation 4.3 Unsupervised Machine Translation Word segmentation is a fundamental task in both Chinese and Japanese NLP. It is often recognized as the first step for further processing in many systems. Thus, we evaluate Chinese Word Segmentation (CWS) and Japanese Word Segmentation (JWS) on PKU dataset (Emerson, 2005) and KWDLC (Kawahara et al., 2014). We use Multilingual BERT and monolingual Chinese BERT (Devlin et al., 2019) as baselines. We use pretrained checkpoints provided by Google6 . Following previous work, we treat the word segmentation task as a sequence labeling task. Note that XLM (Lample and Conneau, 2019) uses pre-segmented sentences as input, making it inapplicable for this task. As shown in Table 5, our proposed UnihanLM outperforms mBERT and monolingual BERT by 1.6 and 0.1 in terms of F1 score on CWS, respectively. On JWS, our model outperforms mBERT by 1.9 on F1. Additionally, we conduct zero-shot transfer experiments to"
2020.aacl-main.24,Y18-1033,0,0.0300615,"redze, 2019; Lample and Conneau, 2019; Conneau et al., 2019; Huang et al., 2019). Word Segmentation Word segmentation is often formalized as a sequence tagging task. It requires lexical knowledge to split a character sequence into a word list that can be used for downstream tasks. This step is necessary for many earlier NLP systems for Chinese and Japanese. Recent work on Chinese Word Segmentation (Wang and Xu, 2017; Zhou et al., 2017; Yang et al., 2017; Cai et al., 2017; Chen et al., 2017b; Yang et al., 2019a) and Japanese Word Segmentation (Kaji and Kitsuregawa, 2014; Fujinuma and II, 2017; Kitagawa and Komachi, 2018) exploit deep neural networks and focus on building end-to-end sequence tagging models. Unsupervised Machine Translation Recently, machine translation systems have demonstrated near human-level performance on some languages. However, it depends on the availability of large amounts of parallel sentences. Unsupervised Machine Translation addresses this problem by exploiting monolingual corpora which can be easily constructed. Lample et al. (2018a) proposed a UMT model by learning to reconstruct in both languages from a shared feature space. Lample et al. (2018c) exploited language modeling and b"
2020.aacl-main.24,P17-4012,0,0.0687952,"oth feedforward network and attention with a drop rate of 0.1. The learning rate for cluster-level pretraining is set to 1 × 10−4 . After 264 hours of clusterlevel pretraining until convergence, we perform character-level pretraining with a smaller learning rate of 5 × 10−5 for another 43 hours. We choose the best model according to its perplexity on validation set. For downstream tasks (to be detailed shortly), we fine-tune UnihanLM with a learning rate of 5 × 10−7 , 1 × 10−4 , 2.5 × 10−5 and a batch 205 5 https://dumps.wikimedia.org/ Method PKU (ZH) Method KWDLC (JA) 95.0 96.5 96.6 OpenNMT (Klein et al., 2017) 96.3 98.2 82.0 85.7 42.12 40.63 Fine-tuned on Wikipedia XLM (Lample and Conneau, 2019) UnihanLM Cross-lingual zero-shot transfer mBERT (2019) UnihanLM JA→ZH Supervised baseline Standard training mBERT (2019) BERT-Mono-ZH (2019) UnihanLM ZH→JA 14.58 33.53 15.06 28.70 Fine-tuned on shuffled ASPEC-JC training set 63.1 74.1 Stroke (Zhang and Komachi, 2019) UnihanLM 33.81 44.59 31.66 40.58 Table 5: F1 scores on Chinese Word Segmentation (CWS) and Japanese Word Segmentation (JWS) tasks. “Cross-lingual zero-shot transfer” indicates that the model is trained on CWS and zero-shot tested on JWS, vice v"
2020.aacl-main.24,P02-1040,0,0.106695,"2019b), including mBERT. 4.5 Table 7: Accuracy scores on PAWS-X dataset. stroke sequence, which is rather unreliable compared to our solution. For example, “丑” (ugly) and “五” (five) have a very similar stroke sequence but completely different meanings. Following (Lample and Conneau, 2019), we use our pretrained weights to initialize the translation model and train the model with denoising auto-encoding loss and online back-translation loss. Note that both baselines use Wikipedia as the unsupervised data and are based on the same UMT method (Lample et al., 2018c). We use character-level BLEU (Papineni et al., 2002) as the evaluation metric. We demonstrate the results in Table 6. As we analyzed, XLM suffers from a severe out-ofvocabulary (OOV) problem on AESPEC-JC, a dataset composed of scientific papers, containing many new terminologies which do not show up in the pretraining corpus of XLM. As a word-based model, XLM is not able to handle these new words and thus yields a rather poor result. When finetuned on unparalleled training set of ASPEC-JC, our model outperforms the previous state-of-the-art model (Zhang and Komachi, 2019) by a large margin of 10.78 and 8.92 in terms of BLEU. Also notably, Uniha"
2020.aacl-main.24,N18-1202,0,0.0180919,"3 帶4 氣5 旋的一種7 。 S-ZH 台1 风2 是热3 带4 低气5 压6 的一种7 。 EN Typhoon is a type of tropical depression. Table 1: A sentence example in Japanese (JA), Traditional Chinese (T-ZH) and Simplified Chinese (S-ZH) with its English translation (EN). The characters that already share the same Unicode are marked with an underline. In this work, we further match characters with identical meanings but different Unicode, then merge them. Characters eligible to be merged together are marked with the same superscript. Introduction Recently, Pretrained Language Models have shown promising performance on many NLP tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019c; Lan et al., 2020). Many attempts have been made to train a model that supports multiple languages. Among them, Multilingual BERT (mBERT) (Devlin et al., 2019) is released as a part of BERT. It directly adopts the same model architecture and training objective, and is trained on Wikipedia in different languages. XLM (Lample and Conneau, 2019) is proposed with an additional language embedding and a new training ∗ This work was done during Canwen’s internship at Microsoft Research Asia. 1 The code and pretrained weights are available at"
2020.aacl-main.24,P19-1493,0,0.020285,"al pretrained encoders have shown its effec207 Method UnihanLM −cluster −character PAWS-X Wiki ASPEC-JC Shuffled-train ZH JA ZH→JA JA→ZH ZH→JA JA→ZH 82.7 81.5 82.0 80.5 79.2 80.1 33.53 29.33 - 28.70 20.93 - 44.59 42.34 - 40.58 39.24 - Table 8: The results of ablation study on text classification and UMT. “-cluster” and “-character” indicate the model trained without the cluster-level pretraining and character-level pretraining, respectively. The metrics for PAWS-X and ASPEC-JC are accuracy and BLEU, respectively. tiveness for learning deep cross-lingual representations (Eriguchi et al., 2018; Pires et al., 2019; Wu and Dredze, 2019; Lample and Conneau, 2019; Conneau et al., 2019; Huang et al., 2019). Word Segmentation Word segmentation is often formalized as a sequence tagging task. It requires lexical knowledge to split a character sequence into a word list that can be used for downstream tasks. This step is necessary for many earlier NLP systems for Chinese and Japanese. Recent work on Chinese Word Segmentation (Wang and Xu, 2017; Zhou et al., 2017; Yang et al., 2017; Cai et al., 2017; Chen et al., 2017b; Yang et al., 2019a) and Japanese Word Segmentation (Kaji and Kitsuregawa, 2014; Fujinuma and"
2020.aacl-main.24,P16-1162,0,0.688063,"his work was done during Canwen’s internship at Microsoft Research Asia. 1 The code and pretrained weights are available at https: //github.com/JetRunner/unihan-lm. objective (translation language modeling, TLM). XLM-R (Conneau et al., 2019) has a larger size and is trained with more data. Based on XLM, Unicoder (Huang et al., 2019) collects more data and uses multi-task learning to train on three supervised tasks. The census of cross-lingual approaches is to allow lexical information to be shared between languages. XLM and mBERT exploit shared lexical information by Byte Pair Encoding (BPE) (Sennrich et al., 2016) and WordPiece (Wu et al., 2016), respectively. However, these automatically learned shared representations have been criticized by recent work (K et al., 2020), which reveals their limitations in sharing meaningful semantics across languages. Also, words in both Chinese and Japanese are short, which prohibits an effective learning of sub-word representations. Different from European languages, Chinese and Japanese naturally share Chinese characters as a subword component. Early work (Chu et al., 2013) shows that shared characters in these two languages can benefit Examplebased Machine Transla"
2020.aacl-main.24,I17-1017,0,0.0277732,", respectively. The metrics for PAWS-X and ASPEC-JC are accuracy and BLEU, respectively. tiveness for learning deep cross-lingual representations (Eriguchi et al., 2018; Pires et al., 2019; Wu and Dredze, 2019; Lample and Conneau, 2019; Conneau et al., 2019; Huang et al., 2019). Word Segmentation Word segmentation is often formalized as a sequence tagging task. It requires lexical knowledge to split a character sequence into a word list that can be used for downstream tasks. This step is necessary for many earlier NLP systems for Chinese and Japanese. Recent work on Chinese Word Segmentation (Wang and Xu, 2017; Zhou et al., 2017; Yang et al., 2017; Cai et al., 2017; Chen et al., 2017b; Yang et al., 2019a) and Japanese Word Segmentation (Kaji and Kitsuregawa, 2014; Fujinuma and II, 2017; Kitagawa and Komachi, 2018) exploit deep neural networks and focus on building end-to-end sequence tagging models. Unsupervised Machine Translation Recently, machine translation systems have demonstrated near human-level performance on some languages. However, it depends on the availability of large amounts of parallel sentences. Unsupervised Machine Translation addresses this problem by exploiting monolingual corpo"
2020.aacl-main.24,P19-1314,0,0.0795465,"e variants of the same characters can be used interchangeably (e.g., “台灣” and “臺灣” for “Taiwan”, in Traditional Chinese). These characters have identical or overlapping meanings. Thus, it is critical to better exploit such information for modeling both cross-lingual (i.e., between Chinese and Japanese), cross-system (i.e., between Traditional and Simplified Chinese) and cross-variant semantics. Both Chinese and Japanese have no delimiter (e.g., white space) to mark the boundaries of words. There have always been debates over whether word segmentation is necessary for Chinese NLP. Recent work (Li et al., 2019) concludes that it is not necessary for various NLP tasks in Chinese. Previous cross-lingual language models use different methods for tokenization. mBERT adds white spaces around Chinese characters and lefts Katakana/Hiragana Japanese (also known as kanas) unprocessed. Different from mBERT, XLM uses Stanford Tokenizer2 and KyTea3 to segment Chinese and Japanese sentences, respectively. After tokenization, mBERT and XLM use WordPiece (Wu et al., 2016) and Byte Pair Encoding (Sennrich et al., 2016) for sub-word encoding, respectively. Nevertheless, both approaches suffer from obvious drawbacks."
2020.aacl-main.24,2021.ccl-1.108,0,0.174219,"Missing"
2020.aacl-main.24,W15-1521,0,0.0165732,"etter. Second, the two-stage procedure inherently introduces a new self-supervised task, which could take the advantage of Multitask Learning (Caruana, 1993). 5 Related Work Multilingual Representation Learning Learning cross-lingual representations are useful for downstream tasks such as cross-lingual classification (Conneau et al., 2018; Yang et al., 2019b), cross-lingual retrieval (Zweigenbaum et al., 2017; Artetxe and Schwenk, 2019) and cross-lingual QA (Artetxe et al., 2019; Lewis et al., 2019; Clark et al., 2020). Earlier work on multilingual representations exploiting parallel corpora (Luong et al., 2015; Gouws et al., 2015) or a bilingual dictionary to learn a linear mapping (Mikolov et al., 2013; Faruqui and Dyer, 2014). Subsequent methods explored self-training (Artetxe et al., 2017) and unsupervised learning (Zhang et al., 2017; Artetxe et al., 2018; Lample et al., 2018b). Recently, multilingual pretrained encoders have shown its effec207 Method UnihanLM −cluster −character PAWS-X Wiki ASPEC-JC Shuffled-train ZH JA ZH→JA JA→ZH ZH→JA JA→ZH 82.7 81.5 82.0 80.5 79.2 80.1 33.53 29.33 - 28.70 20.93 - 44.59 42.34 - 40.58 39.24 - Table 8: The results of ablation study on text classification and"
2020.aacl-main.24,D19-1077,0,0.0193357,"rs have shown its effec207 Method UnihanLM −cluster −character PAWS-X Wiki ASPEC-JC Shuffled-train ZH JA ZH→JA JA→ZH ZH→JA JA→ZH 82.7 81.5 82.0 80.5 79.2 80.1 33.53 29.33 - 28.70 20.93 - 44.59 42.34 - 40.58 39.24 - Table 8: The results of ablation study on text classification and UMT. “-cluster” and “-character” indicate the model trained without the cluster-level pretraining and character-level pretraining, respectively. The metrics for PAWS-X and ASPEC-JC are accuracy and BLEU, respectively. tiveness for learning deep cross-lingual representations (Eriguchi et al., 2018; Pires et al., 2019; Wu and Dredze, 2019; Lample and Conneau, 2019; Conneau et al., 2019; Huang et al., 2019). Word Segmentation Word segmentation is often formalized as a sequence tagging task. It requires lexical knowledge to split a character sequence into a word list that can be used for downstream tasks. This step is necessary for many earlier NLP systems for Chinese and Japanese. Recent work on Chinese Word Segmentation (Wang and Xu, 2017; Zhou et al., 2017; Yang et al., 2017; Cai et al., 2017; Chen et al., 2017b; Yang et al., 2019a) and Japanese Word Segmentation (Kaji and Kitsuregawa, 2014; Fujinuma and II, 2017; Kitagawa an"
2020.aacl-main.24,P17-1078,0,0.0262407,"and ASPEC-JC are accuracy and BLEU, respectively. tiveness for learning deep cross-lingual representations (Eriguchi et al., 2018; Pires et al., 2019; Wu and Dredze, 2019; Lample and Conneau, 2019; Conneau et al., 2019; Huang et al., 2019). Word Segmentation Word segmentation is often formalized as a sequence tagging task. It requires lexical knowledge to split a character sequence into a word list that can be used for downstream tasks. This step is necessary for many earlier NLP systems for Chinese and Japanese. Recent work on Chinese Word Segmentation (Wang and Xu, 2017; Zhou et al., 2017; Yang et al., 2017; Cai et al., 2017; Chen et al., 2017b; Yang et al., 2019a) and Japanese Word Segmentation (Kaji and Kitsuregawa, 2014; Fujinuma and II, 2017; Kitagawa and Komachi, 2018) exploit deep neural networks and focus on building end-to-end sequence tagging models. Unsupervised Machine Translation Recently, machine translation systems have demonstrated near human-level performance on some languages. However, it depends on the availability of large amounts of parallel sentences. Unsupervised Machine Translation addresses this problem by exploiting monolingual corpora which can be easily constructed. La"
2020.aacl-main.24,N19-1278,0,0.308454,"s a type of tropical depression. Table 1: A sentence example in Japanese (JA), Traditional Chinese (T-ZH) and Simplified Chinese (S-ZH) with its English translation (EN). The characters that already share the same Unicode are marked with an underline. In this work, we further match characters with identical meanings but different Unicode, then merge them. Characters eligible to be merged together are marked with the same superscript. Introduction Recently, Pretrained Language Models have shown promising performance on many NLP tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019c; Lan et al., 2020). Many attempts have been made to train a model that supports multiple languages. Among them, Multilingual BERT (mBERT) (Devlin et al., 2019) is released as a part of BERT. It directly adopts the same model architecture and training objective, and is trained on Wikipedia in different languages. XLM (Lample and Conneau, 2019) is proposed with an additional language embedding and a new training ∗ This work was done during Canwen’s internship at Microsoft Research Asia. 1 The code and pretrained weights are available at https: //github.com/JetRunner/unihan-lm. objective (trans"
2020.aacl-main.24,D19-1382,0,0.128372,"s a type of tropical depression. Table 1: A sentence example in Japanese (JA), Traditional Chinese (T-ZH) and Simplified Chinese (S-ZH) with its English translation (EN). The characters that already share the same Unicode are marked with an underline. In this work, we further match characters with identical meanings but different Unicode, then merge them. Characters eligible to be merged together are marked with the same superscript. Introduction Recently, Pretrained Language Models have shown promising performance on many NLP tasks (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019c; Lan et al., 2020). Many attempts have been made to train a model that supports multiple languages. Among them, Multilingual BERT (mBERT) (Devlin et al., 2019) is released as a part of BERT. It directly adopts the same model architecture and training objective, and is trained on Wikipedia in different languages. XLM (Lample and Conneau, 2019) is proposed with an additional language embedding and a new training ∗ This work was done during Canwen’s internship at Microsoft Research Asia. 1 The code and pretrained weights are available at https: //github.com/JetRunner/unihan-lm. objective (trans"
2020.aacl-main.24,E17-1010,0,0.0162878,"3.3 Fine-grained Character-level Pretraining Although the clusters training is effective, there are two problems remaining unsolved. First, Traditional Variant could be ambiguous. As shown in Table 2, a character (most likely one used in Simplified Chinese) may have multiple Traditional Variants. Although it should not have a significant negative effect for understanding the language (since a Simplified Chinese user can disambiguate between different meanings of a character based on its context), it still makes sense to improve the overall performance by distinguish the characters explicitly (Navigli et al., 2017). Also, in tasks involving decoding (e.g., machine translation), they must be processed independently. Thus, character disambiguation can be naturally used as a selfsupervised task. Second, when using the trained model for translation, it would be important for the model to decode the right character for different languages and writing systems. For example, for the word meaning “typhoon”, “台風”, “颱風”, “台 风” should be used in Japanese, Traditional Chinese and Simplified Chinese, respectively. Consequently, we leave these nuances of characters to a fine-grained character-level pretraining. Since"
2020.aacl-main.24,D17-1207,0,0.0138829,"l representations are useful for downstream tasks such as cross-lingual classification (Conneau et al., 2018; Yang et al., 2019b), cross-lingual retrieval (Zweigenbaum et al., 2017; Artetxe and Schwenk, 2019) and cross-lingual QA (Artetxe et al., 2019; Lewis et al., 2019; Clark et al., 2020). Earlier work on multilingual representations exploiting parallel corpora (Luong et al., 2015; Gouws et al., 2015) or a bilingual dictionary to learn a linear mapping (Mikolov et al., 2013; Faruqui and Dyer, 2014). Subsequent methods explored self-training (Artetxe et al., 2017) and unsupervised learning (Zhang et al., 2017; Artetxe et al., 2018; Lample et al., 2018b). Recently, multilingual pretrained encoders have shown its effec207 Method UnihanLM −cluster −character PAWS-X Wiki ASPEC-JC Shuffled-train ZH JA ZH→JA JA→ZH ZH→JA JA→ZH 82.7 81.5 82.0 80.5 79.2 80.1 33.53 29.33 - 28.70 20.93 - 44.59 42.34 - 40.58 39.24 - Table 8: The results of ablation study on text classification and UMT. “-cluster” and “-character” indicate the model trained without the cluster-level pretraining and character-level pretraining, respectively. The metrics for PAWS-X and ASPEC-JC are accuracy and BLEU, respectively. tiveness for l"
2020.aacl-main.24,D17-1079,0,0.0236344,"metrics for PAWS-X and ASPEC-JC are accuracy and BLEU, respectively. tiveness for learning deep cross-lingual representations (Eriguchi et al., 2018; Pires et al., 2019; Wu and Dredze, 2019; Lample and Conneau, 2019; Conneau et al., 2019; Huang et al., 2019). Word Segmentation Word segmentation is often formalized as a sequence tagging task. It requires lexical knowledge to split a character sequence into a word list that can be used for downstream tasks. This step is necessary for many earlier NLP systems for Chinese and Japanese. Recent work on Chinese Word Segmentation (Wang and Xu, 2017; Zhou et al., 2017; Yang et al., 2017; Cai et al., 2017; Chen et al., 2017b; Yang et al., 2019a) and Japanese Word Segmentation (Kaji and Kitsuregawa, 2014; Fujinuma and II, 2017; Kitagawa and Komachi, 2018) exploit deep neural networks and focus on building end-to-end sequence tagging models. Unsupervised Machine Translation Recently, machine translation systems have demonstrated near human-level performance on some languages. However, it depends on the availability of large amounts of parallel sentences. Unsupervised Machine Translation addresses this problem by exploiting monolingual corpora which can be eas"
2020.aacl-main.24,W17-2512,0,0.0126887,"hasize two strengths. First, as mentioned before, our easy-to-hard training procedure matches the core idea of Curriculum Learning (Bengio et al., 2009), which smooths the training and help the model generalize better. Second, the two-stage procedure inherently introduces a new self-supervised task, which could take the advantage of Multitask Learning (Caruana, 1993). 5 Related Work Multilingual Representation Learning Learning cross-lingual representations are useful for downstream tasks such as cross-lingual classification (Conneau et al., 2018; Yang et al., 2019b), cross-lingual retrieval (Zweigenbaum et al., 2017; Artetxe and Schwenk, 2019) and cross-lingual QA (Artetxe et al., 2019; Lewis et al., 2019; Clark et al., 2020). Earlier work on multilingual representations exploiting parallel corpora (Luong et al., 2015; Gouws et al., 2015) or a bilingual dictionary to learn a linear mapping (Mikolov et al., 2013; Faruqui and Dyer, 2014). Subsequent methods explored self-training (Artetxe et al., 2017) and unsupervised learning (Zhang et al., 2017; Artetxe et al., 2018; Lample et al., 2018b). Recently, multilingual pretrained encoders have shown its effec207 Method UnihanLM −cluster −character PAWS-X Wiki"
2020.aacl-main.28,D18-1454,0,0.0229478,"tations. Experimental results show that our framework outperforms knowledge-aware text generation baselines and GPT-2 (Radford et al., 2019) in both automatic and human evaluation. Particularly, our model generates explanations with more informative content and provides reasoning paths on the knowledge graph for concept extraction. To summarize, our contributions are two-fold: knowledge base completion (Bosselut et al., 2019). While the ultimate goals of these tasks are different from ours, we argue that performing explicit commonsense reasoning is also critical to generation. A line of work (Bauer et al., 2018; Lin et al., 2019a) resorts to structured commonsense knowledge and builds graph-aware representations along with the contextualized word embeddings to tackle the commonsense question answering problem. In our work, we focus on reasoning over structured knowledge to explicitly infer discrete bridge concepts that are further used for text generation. Another line of work (Rajani et al., 2019; Khot et al., 2019) identifies the knowledge gap critical for the complete reasoning chain and fills the gap by writing general explanation or acquiring fine-grained annotations with human effort. While sh"
2020.aacl-main.28,P19-1470,0,0.0422763,"Missing"
2020.aacl-main.28,2020.tacl-1.7,1,0.829718,"Text Generation Existing work that utilizes structured knowledge graphs to generate texts mainly lies in conversation generation (Zhou et al., 2018; Tuan et al., 2019; Moon et al., 2019), story generation (Guan et al., 2019) and language modeling (Ahn et al., 2016; Logan et al., 2019; Hayashi et al., 2019). Zhou et al. (2018) and Guan et al. (2019) propose to use graph attention that incorporates the information of neighbouring concepts into context representations to help generate the target sentence. Yang et al. (2019) resort to a dynamic concept memory that updates during essay generation. Guan et al. (2020) conduct post-training on knowledge triples to enhance the GPT-2 with commonsense knowledge. Since one-hop graphs of concepts in the statement have low coverage to the concepts in the explanation, merely leveraging information of individual concepts or triples is not suitable for this task. Another direction that utilizes more complex graph is to model multi-hop reasoning by performing random walk (Moon et al., 2019) on the knowledge graph or simulating a Markov process on the pre-extracted knowledge paths (Tuan et al., 2019). While in our task, we don’t have access to a parallel grounded know"
2020.aacl-main.28,D19-1281,0,0.0245197,"t et al., 2019). While the ultimate goals of these tasks are different from ours, we argue that performing explicit commonsense reasoning is also critical to generation. A line of work (Bauer et al., 2018; Lin et al., 2019a) resorts to structured commonsense knowledge and builds graph-aware representations along with the contextualized word embeddings to tackle the commonsense question answering problem. In our work, we focus on reasoning over structured knowledge to explicitly infer discrete bridge concepts that are further used for text generation. Another line of work (Rajani et al., 2019; Khot et al., 2019) identifies the knowledge gap critical for the complete reasoning chain and fills the gap by writing general explanation or acquiring fine-grained annotations with human effort. While sharing a similar motivation, our method differs from theirs in the sense that we acquire distant supervisions for the bridge concepts to extract reasoning paths and generate plausible explanations without the need of additional human annotation. • We analyze the under-explored commonsense explanation generation task and investigate the challenges in incorporating external knowledge graph to aid the generation pr"
2020.aacl-main.28,D19-1282,0,0.203309,"ng neural language generation models tend to generate trivial and uninformative explanations. For example, one of the existing neural models generates an explanation “The school wasn’t open for summer” to the statement in Figure 1. Although it is sometimes reasonable, simple modification of the statement to the negation form with no additional information cannot explain the reasons why the statement conflicts with commonsense. 2) Noisy commonsense knowledge grounding. It’s still challenging for most existing language generation models to generate explanations that are faithful to commonsense (Lin et al., 2019b). Thus, explicitly incorporating external knowledge sources is necessary for this task. Since the nature of the explanation generation task involves using underlying commonsense knowledge to explain, locating useful commonsense knowledge from large-scale knowledge graph is not trivial and generally requires multi-hop reasoning. 248 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 248–257 c December 4 - 7, 2020. 2020 Association for Computational Ling"
2020.aacl-main.28,W05-0909,0,0.0727217,"English version ConceptNet as our external commonsense knowledge graph. It contains triples in the form of (h, r, t) where h and t represent head and tail concepts and r is the relation type. We follow Lin et al. (2019a) to merge the original 42 relation types into 17 types. We additionally define 17 reverse types corresponding to the original 17 relation types to distinguish the direction of the triples on the graph. 4.2 Automatic Evaluation Metrics To automatically evaluate the performance of the generation models, we use the BLEU-3/4 (Papineni et al., 2001), ROUGE-2/L (Lin, 2004), METEOR (Banerjee and Lavie, 2005) as our main metrics. We also propose Concept F1 to evaluate the accuracy of the unique concepts in the generated explanation that do not occur in the statement. ˆ Specifically, given the generated explanation y and the reference explanation y, we extract a set of concepts Cyˆ and Cy from the generated explanation and the reference explanation respectively using the method in §3.3. We denote the sets of unique concepts in the explanation as Uy = Cy − Cx and Uyˆ = Cyˆ − Cx . Then we can compute the Concept F1 as the harmonic mean of recall and precision. Dataset and Experimental Setup Commonsen"
2020.aacl-main.28,W04-1013,0,0.039496,"dge Graph We use the English version ConceptNet as our external commonsense knowledge graph. It contains triples in the form of (h, r, t) where h and t represent head and tail concepts and r is the relation type. We follow Lin et al. (2019a) to merge the original 42 relation types into 17 types. We additionally define 17 reverse types corresponding to the original 17 relation types to distinguish the direction of the triples on the graph. 4.2 Automatic Evaluation Metrics To automatically evaluate the performance of the generation models, we use the BLEU-3/4 (Papineni et al., 2001), ROUGE-2/L (Lin, 2004), METEOR (Banerjee and Lavie, 2005) as our main metrics. We also propose Concept F1 to evaluate the accuracy of the unique concepts in the generated explanation that do not occur in the statement. ˆ Specifically, given the generated explanation y and the reference explanation y, we extract a set of concepts Cyˆ and Cy from the generated explanation and the reference explanation respectively using the method in §3.3. We denote the sets of unique concepts in the explanation as Uy = Cy − Cx and Uyˆ = Cyˆ − Cx . Then we can compute the Concept F1 as the harmonic mean of recall and precision. Datas"
2020.aacl-main.28,P19-1598,0,0.0288822,"e commonsense explanation generation task in both automatic and human evaluation. 2 2.1 Related Work Machine Commonsense Reasoning Previous work on machine commonsense reasoning mainly focuses on the tasks of inference (Levesque et al., 2011), question answering (Talmor et al., 2018; Sap et al., 2019) and 249 2.2 Knowledge-Grounded Text Generation Existing work that utilizes structured knowledge graphs to generate texts mainly lies in conversation generation (Zhou et al., 2018; Tuan et al., 2019; Moon et al., 2019), story generation (Guan et al., 2019) and language modeling (Ahn et al., 2016; Logan et al., 2019; Hayashi et al., 2019). Zhou et al. (2018) and Guan et al. (2019) propose to use graph attention that incorporates the information of neighbouring concepts into context representations to help generate the target sentence. Yang et al. (2019) resort to a dynamic concept memory that updates during essay generation. Guan et al. (2020) conduct post-training on knowledge triples to enhance the GPT-2 with commonsense knowledge. Since one-hop graphs of concepts in the statement have low coverage to the concepts in the explanation, merely leveraging information of individual concepts or triples is no"
2020.aacl-main.28,P19-1081,0,0.028584,"es the explanation based on these concepts. Our model outperforms state-of-the-art baselines on the commonsense explanation generation task in both automatic and human evaluation. 2 2.1 Related Work Machine Commonsense Reasoning Previous work on machine commonsense reasoning mainly focuses on the tasks of inference (Levesque et al., 2011), question answering (Talmor et al., 2018; Sap et al., 2019) and 249 2.2 Knowledge-Grounded Text Generation Existing work that utilizes structured knowledge graphs to generate texts mainly lies in conversation generation (Zhou et al., 2018; Tuan et al., 2019; Moon et al., 2019), story generation (Guan et al., 2019) and language modeling (Ahn et al., 2016; Logan et al., 2019; Hayashi et al., 2019). Zhou et al. (2018) and Guan et al. (2019) propose to use graph attention that incorporates the information of neighbouring concepts into context representations to help generate the target sentence. Yang et al. (2019) resort to a dynamic concept memory that updates during essay generation. Guan et al. (2020) conduct post-training on knowledge triples to enhance the GPT-2 with commonsense knowledge. Since one-hop graphs of concepts in the statement have low coverage to the"
2020.aacl-main.28,2001.mtsummit-papers.68,0,0.0371618,"4.1 4.1.1 4.1.2 Commonsense Knowledge Graph We use the English version ConceptNet as our external commonsense knowledge graph. It contains triples in the form of (h, r, t) where h and t represent head and tail concepts and r is the relation type. We follow Lin et al. (2019a) to merge the original 42 relation types into 17 types. We additionally define 17 reverse types corresponding to the original 17 relation types to distinguish the direction of the triples on the graph. 4.2 Automatic Evaluation Metrics To automatically evaluate the performance of the generation models, we use the BLEU-3/4 (Papineni et al., 2001), ROUGE-2/L (Lin, 2004), METEOR (Banerjee and Lavie, 2005) as our main metrics. We also propose Concept F1 to evaluate the accuracy of the unique concepts in the generated explanation that do not occur in the statement. ˆ Specifically, given the generated explanation y and the reference explanation y, we extract a set of concepts Cyˆ and Cy from the generated explanation and the reference explanation respectively using the method in §3.3. We denote the sets of unique concepts in the explanation as Uy = Cy − Cx and Uyˆ = Cyˆ − Cx . Then we can compute the Concept F1 as the harmonic mean of reca"
2020.aacl-main.28,P19-1487,0,0.0401311,"e completion (Bosselut et al., 2019). While the ultimate goals of these tasks are different from ours, we argue that performing explicit commonsense reasoning is also critical to generation. A line of work (Bauer et al., 2018; Lin et al., 2019a) resorts to structured commonsense knowledge and builds graph-aware representations along with the contextualized word embeddings to tackle the commonsense question answering problem. In our work, we focus on reasoning over structured knowledge to explicitly infer discrete bridge concepts that are further used for text generation. Another line of work (Rajani et al., 2019; Khot et al., 2019) identifies the knowledge gap critical for the complete reasoning chain and fills the gap by writing general explanation or acquiring fine-grained annotations with human effort. While sharing a similar motivation, our method differs from theirs in the sense that we acquire distant supervisions for the bridge concepts to extract reasoning paths and generate plausible explanations without the need of additional human annotation. • We analyze the under-explored commonsense explanation generation task and investigate the challenges in incorporating external knowledge graph to a"
2020.aacl-main.28,D19-1454,0,0.126624,"Missing"
2020.aacl-main.28,D19-1194,0,0.0127204,"hs and then generates the explanation based on these concepts. Our model outperforms state-of-the-art baselines on the commonsense explanation generation task in both automatic and human evaluation. 2 2.1 Related Work Machine Commonsense Reasoning Previous work on machine commonsense reasoning mainly focuses on the tasks of inference (Levesque et al., 2011), question answering (Talmor et al., 2018; Sap et al., 2019) and 249 2.2 Knowledge-Grounded Text Generation Existing work that utilizes structured knowledge graphs to generate texts mainly lies in conversation generation (Zhou et al., 2018; Tuan et al., 2019; Moon et al., 2019), story generation (Guan et al., 2019) and language modeling (Ahn et al., 2016; Logan et al., 2019; Hayashi et al., 2019). Zhou et al. (2018) and Guan et al. (2019) propose to use graph attention that incorporates the information of neighbouring concepts into context representations to help generate the target sentence. Yang et al. (2019) resort to a dynamic concept memory that updates during essay generation. Guan et al. (2020) conduct post-training on knowledge triples to enhance the GPT-2 with commonsense knowledge. Since one-hop graphs of concepts in the statement have"
2020.aacl-main.28,P19-1393,0,0.0263181,"e concepts like vacation by identifying the relation to the source concepts, i.e. school and summer in the statement. Introduction Machine commonsense reasoning has been widely acknowledged as a crucial component of artificial intelligence and a considerable amount of work has been dedicated to evaluate this ability from various aspects in natural language processing (Levesque et al., 2011; Talmor et al., 2018; Sap et al., 2019). A large proportion of existing tasks frame commonsense reasoning as multi-choice reading comprehension problems, which lack direct assessment to machine commonsense (Wang et al., 2019) and impede its practicability to realistic scenarios (Lin ∗ Corresponding author The source code is available at https://github. com/cdjhz/CommExpGen. 1 et al., 2019b). Recently, Wang et al. (2019) proposed a commonsense explanation generation challenge that directly tests machine’s sense-making capability via commonsense reasoning. In this paper, we focus on the challenging explanation generation task where the goal is to generate a sentence to explain the reasons why the input statement is against commonsense, as shown in Figure 1. Generating a reasonable explanation for a statement faces t"
2020.aacl-main.28,P19-1193,0,0.0211606,", question answering (Talmor et al., 2018; Sap et al., 2019) and 249 2.2 Knowledge-Grounded Text Generation Existing work that utilizes structured knowledge graphs to generate texts mainly lies in conversation generation (Zhou et al., 2018; Tuan et al., 2019; Moon et al., 2019), story generation (Guan et al., 2019) and language modeling (Ahn et al., 2016; Logan et al., 2019; Hayashi et al., 2019). Zhou et al. (2018) and Guan et al. (2019) propose to use graph attention that incorporates the information of neighbouring concepts into context representations to help generate the target sentence. Yang et al. (2019) resort to a dynamic concept memory that updates during essay generation. Guan et al. (2020) conduct post-training on knowledge triples to enhance the GPT-2 with commonsense knowledge. Since one-hop graphs of concepts in the statement have low coverage to the concepts in the explanation, merely leveraging information of individual concepts or triples is not suitable for this task. Another direction that utilizes more complex graph is to model multi-hop reasoning by performing random walk (Moon et al., 2019) on the knowledge graph or simulating a Markov process on the pre-extracted knowledge pa"
2020.acl-main.600,P19-1620,0,0.0412653,"al., 2017), and QANet (Yu et al., 2018). Recently, unsupervised pre-training of language models such as BERT (Devlin et al., 2019), achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct. Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models (Yang et al., 2017; Zhu et al., 2019b; Alberti et al., 2019; Dong et al., 2019). However, the methods require labeled data to train the sequence-to-sequence QG model. Dhingra et al. (2018) propose to collect synthetic context-question-answer triples by generating cloze-style questions from the Wikipedia summary paragraphs in an unsupervised manner. Unsupervised QA Lewis et al. (2019) have explored the unsupervised method for QA. They create synthetic QA data in four steps. i) Sample paragraphs from the English Wikipedia. ii) Use NER or noun chunkers to extract answer candidates from the context. iii) Extract “fill-inthe-blank” cloze-style questions gi"
2020.acl-main.600,N19-1423,0,0.669722,"upervised approaches by a large margin and is competitive with early supervised models. We also show the effectiveness of our approach in the fewshot learning setting. 1 Introduction Extractive question answering aims to extract a span from the given document to answer the question. Rapid progress has been made because of the release of large-scale annotated datasets (Rajpurkar et al., 2016, 2018; Joshi et al., 2017), and well-designed neural models (Wang and Jiang, 2016; Seo et al., 2016; Yu et al., 2018). Recently, unsupervised pre-training of language models on large corpora, such as BERT (Devlin et al., 2019), has brought further performance gains. However, the above approaches heavily rely on the availability of large-scale datasets. The collection of high-quality training data is timeconsuming and requires significant resources, es∗ Contribution during internship at Microsoft Research. The code and data are available at https://github. com/Neutralzz/RefQA. 1 pecially for new domains or languages. In order to tackle the setting in which no training data available, Lewis et al. (2019) leverage unsupervised machine translation to generate synthetic contextquestion-answer triples. The paragraphs are"
2020.acl-main.600,N18-2092,0,0.43499,"19), achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct. Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models (Yang et al., 2017; Zhu et al., 2019b; Alberti et al., 2019; Dong et al., 2019). However, the methods require labeled data to train the sequence-to-sequence QG model. Dhingra et al. (2018) propose to collect synthetic context-question-answer triples by generating cloze-style questions from the Wikipedia summary paragraphs in an unsupervised manner. Unsupervised QA Lewis et al. (2019) have explored the unsupervised method for QA. They create synthetic QA data in four steps. i) Sample paragraphs from the English Wikipedia. ii) Use NER or noun chunkers to extract answer candidates from the context. iii) Extract “fill-inthe-blank” cloze-style questions given the candidate answer and context. iv) Translate cloze-style questions into natural questions by an unsupervised translator. C"
2020.acl-main.600,P17-1147,0,0.340613,"rs, which iteratively refines data over R E F QA. We conduct experiments1 on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data. Our approach outperforms previous unsupervised approaches by a large margin and is competitive with early supervised models. We also show the effectiveness of our approach in the fewshot learning setting. 1 Introduction Extractive question answering aims to extract a span from the given document to answer the question. Rapid progress has been made because of the release of large-scale annotated datasets (Rajpurkar et al., 2016, 2018; Joshi et al., 2017), and well-designed neural models (Wang and Jiang, 2016; Seo et al., 2016; Yu et al., 2018). Recently, unsupervised pre-training of language models on large corpora, such as BERT (Devlin et al., 2019), has brought further performance gains. However, the above approaches heavily rely on the availability of large-scale datasets. The collection of high-quality training data is timeconsuming and requires significant resources, es∗ Contribution during internship at Microsoft Research. The code and data are available at https://github. com/Neutralzz/RefQA. 1 pecially for new domains or languages. In"
2020.acl-main.600,P18-1249,0,0.0280697,"|ci , qi ) a0i ∈ZA + I(a0i (2) 6= ai )log P (a0i |ci , qi0 )], half of the statement tokens are not in the cited document. The article length is limited to 1,000 words for cited documents. Besides, we compute ROUGE-2 (Lin, 2004) as correlation scores between statements and context. We use the score’s median (0.2013) as a threshold, i.e., half of the data with lower scores are discarded. We obtain 303K remaining data to construct our R EF QA. We extract named entities as our answer candidates, using the NER toolkit of Spacy. We split the statements into sub-clauses with Berkeley Neural Parser (Kitaev and Klein, 2018). The questions are generated as in Section 3.2. We also discard sub-clauses that are less than 6 tokens, to prevent losing too much information of original sentences. Finally, we obtain 0.9M R EF QA examples. Algorithm 1: Iterative Data Refinement Input: synthetic context-question-answer triples S = {(ci , qi , ai )}N i=1 , a threshold τ and a decay factor γ. Sample a part of triples SI from S Update the model P parameters by maximizing SI log P (a|c, q) Split unseen triples into {SU1 , SU2 , ..., SUM } for k ← 1 to M do D←φ for (ci , qi , ai ) in SUk do ZA ← {a0i s.t. P (a0i |ci , qi ) ≥ τ }"
2020.acl-main.600,P19-1484,0,0.159185,"., 2016; Yu et al., 2018). Recently, unsupervised pre-training of language models on large corpora, such as BERT (Devlin et al., 2019), has brought further performance gains. However, the above approaches heavily rely on the availability of large-scale datasets. The collection of high-quality training data is timeconsuming and requires significant resources, es∗ Contribution during internship at Microsoft Research. The code and data are available at https://github. com/Neutralzz/RefQA. 1 pecially for new domains or languages. In order to tackle the setting in which no training data available, Lewis et al. (2019) leverage unsupervised machine translation to generate synthetic contextquestion-answer triples. The paragraphs are sampled from Wikipedia. NER and noun chunkers are employed to identify answer candidates. Cloze questions are first extracted from the sentences of the paragraph, and then translated into natural questions. However, there are a lot of lexical overlaps between the generated questions and the paragraph. Similar lexical and syntactic structures render the QA model tend to predict the answer just by word matching. Moreover, the answer category is limited to the named entity or noun p"
2020.acl-main.600,W04-1013,0,0.0162225,"predicted answers and their probabilities. Then we filter the predicted answers with Iterative QA Model Training After refining the dataset, we concatenate them with the filtered examples whose candidate answers agree with the predictions. The new training set is then used to continue to train the QA model. The training objective is defined as: 6722 max X [I(a0i = ai )log P (ai |ci , qi ) a0i ∈ZA + I(a0i (2) 6= ai )log P (a0i |ci , qi0 )], half of the statement tokens are not in the cited document. The article length is limited to 1,000 words for cited documents. Besides, we compute ROUGE-2 (Lin, 2004) as correlation scores between statements and context. We use the score’s median (0.2013) as a threshold, i.e., half of the data with lower scores are discarded. We obtain 303K remaining data to construct our R EF QA. We extract named entities as our answer candidates, using the NER toolkit of Spacy. We split the statements into sub-clauses with Berkeley Neural Parser (Kitaev and Klein, 2018). The questions are generated as in Section 3.2. We also discard sub-clauses that are less than 6 tokens, to prevent losing too much information of original sentences. Finally, we obtain 0.9M R EF QA examp"
2020.acl-main.600,P18-2124,0,0.0491469,"Missing"
2020.acl-main.600,D16-1264,0,0.770515,"extract more appropriate answers, which iteratively refines data over R E F QA. We conduct experiments1 on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data. Our approach outperforms previous unsupervised approaches by a large margin and is competitive with early supervised models. We also show the effectiveness of our approach in the fewshot learning setting. 1 Introduction Extractive question answering aims to extract a span from the given document to answer the question. Rapid progress has been made because of the release of large-scale annotated datasets (Rajpurkar et al., 2016, 2018; Joshi et al., 2017), and well-designed neural models (Wang and Jiang, 2016; Seo et al., 2016; Yu et al., 2018). Recently, unsupervised pre-training of language models on large corpora, such as BERT (Devlin et al., 2019), has brought further performance gains. However, the above approaches heavily rely on the availability of large-scale datasets. The collection of high-quality training data is timeconsuming and requires significant resources, es∗ Contribution during internship at Microsoft Research. The code and data are available at https://github. com/Neutralzz/RefQA. 1 pecially for n"
2020.acl-main.600,W17-2623,0,0.469066,"rs to continue the model training. Thanks to the pretrained linguistic knowledge in the BERTbased QA model, there are more appropriate and diverse answer candidates in the filtered predictions, some of which do not appear in the candidates extracted by NER tools. We also show 6719 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6719–6728 c July 5 - 10, 2020. 2020 Association for Computational Linguistics that iteratively refining the data further improves model performance. We conduct experiments on SQuAD 1.1 (Rajpurkar et al., 2016), and NewsQA (Trischler et al., 2017). Our method yields state-of-the-art results against strong baselines in the unsupervised setting. Specifically, the proposed model achieves 71.4 F1 on the SQuAD 1.1 test set and 45.1 F1 on the NewsQA test set without using annotated data. We also evaluate our method in a few-shot learning setting. Our approach achieves 79.4 F1 on the SQuAD 1.1 dev set with only 100 labeled examples, compared to 63.0 F1 using the method of Lewis et al. (2019). To summarize, the contributions of this paper include: i) R EF QA constructing in an unsupervised manner, which contains more informative context-questi"
2020.acl-main.600,P19-1415,1,0.910983,"16), R-NET (Wang et al., 2017), and QANet (Yu et al., 2018). Recently, unsupervised pre-training of language models such as BERT (Devlin et al., 2019), achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct. Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models (Yang et al., 2017; Zhu et al., 2019b; Alberti et al., 2019; Dong et al., 2019). However, the methods require labeled data to train the sequence-to-sequence QG model. Dhingra et al. (2018) propose to collect synthetic context-question-answer triples by generating cloze-style questions from the Wikipedia summary paragraphs in an unsupervised manner. Unsupervised QA Lewis et al. (2019) have explored the unsupervised method for QA. They create synthetic QA data in four steps. i) Sample paragraphs from the English Wikipedia. ii) Use NER or noun chunkers to extract answer candidates from the context. iii) Extract “fill-inthe-blank” c"
2020.acl-main.600,P17-1018,1,0.931513,"gment the question-answer pairs in R EF QA. 2 Related Work Extractive Question Answering Given a document and question, the task is to predict a continuous sub-span of the document to answer the question. Extractive question answering has garnered a lot of attention over the past few years. Benchmark datasets, such as SQuAD (Rajpurkar et al., 2016, 2018), NewsQA (Trischler et al., 2017) and TriviaQA (Joshi et al., 2017), play an important role in the progress. In order to improve the performance on these benchmarks, several models have been proposed, including BiDAF (Seo et al., 2016), R-NET (Wang et al., 2017), and QANet (Yu et al., 2018). Recently, unsupervised pre-training of language models such as BERT (Devlin et al., 2019), achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct. Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models (Yang et al., 2017; Zhu et al., 2019b; Alberti et"
2020.acl-main.600,K17-1028,0,0.0647072,"Missing"
2020.acl-main.600,P17-1096,0,0.0898366,"DAF (Seo et al., 2016), R-NET (Wang et al., 2017), and QANet (Yu et al., 2018). Recently, unsupervised pre-training of language models such as BERT (Devlin et al., 2019), achieves significant improvement. However, these powerful models rely on the availability of human-labeled data. Large annotated corpora for a specific domain or language are limited and expensive to construct. Semi-Supervised QA Several semi-supervised approaches have been proposed to utilize unlabeled data. Neural question generation (QG) models are used to generate questions from unlabeled passages for training QA models (Yang et al., 2017; Zhu et al., 2019b; Alberti et al., 2019; Dong et al., 2019). However, the methods require labeled data to train the sequence-to-sequence QG model. Dhingra et al. (2018) propose to collect synthetic context-question-answer triples by generating cloze-style questions from the Wikipedia summary paragraphs in an unsupervised manner. Unsupervised QA Lewis et al. (2019) have explored the unsupervised method for QA. They create synthetic QA data in four steps. i) Sample paragraphs from the English Wikipedia. ii) Use NER or noun chunkers to extract answer candidates from the context. iii) Extract “f"
2020.acl-main.600,P14-1090,0,0.105078,"Missing"
2020.coling-main.482,P18-1031,0,0.0207804,"unsupervised settings, such as text clustering. In this paper, we propose a novel method to fine-tune pre-trained models unsupervisedly for text clustering, which simultaneously learns text representations and cluster assignments using a clustering oriented loss. Experiments on three text clustering datasets (namely TREC-6, Yelp, and DBpedia) show that our model outperforms the baseline methods and achieves stateof-the-art results. 1 Introduction Pre-trained language models have shown remarkable progress in many natural language understanding tasks (Radford et al., 2018; Peters et al., 2018; Howard and Ruder, 2018). Especially, BERT (Devlin et al., 2018) applies the fine-tuning approach to achieve ground-breaking performance in a set of NLP tasks. BERT, a deep bidirectional transformer model (Vaswani et al., 2017), utilizes a huge unlabeled data to learn complex features and representations and then fine-tunes its pre-trained model on the downstream tasks with labeled data. Although BERT has achieved great success in many natural language understanding tasks under supervised fine-tuning approaches, relatively little work has been focused on applying pre-trained models in unsupervised settings. In this p"
2020.coling-main.482,N18-1202,0,0.0409681,"pre-trained models in unsupervised settings, such as text clustering. In this paper, we propose a novel method to fine-tune pre-trained models unsupervisedly for text clustering, which simultaneously learns text representations and cluster assignments using a clustering oriented loss. Experiments on three text clustering datasets (namely TREC-6, Yelp, and DBpedia) show that our model outperforms the baseline methods and achieves stateof-the-art results. 1 Introduction Pre-trained language models have shown remarkable progress in many natural language understanding tasks (Radford et al., 2018; Peters et al., 2018; Howard and Ruder, 2018). Especially, BERT (Devlin et al., 2018) applies the fine-tuning approach to achieve ground-breaking performance in a set of NLP tasks. BERT, a deep bidirectional transformer model (Vaswani et al., 2017), utilizes a huge unlabeled data to learn complex features and representations and then fine-tunes its pre-trained model on the downstream tasks with labeled data. Although BERT has achieved great success in many natural language understanding tasks under supervised fine-tuning approaches, relatively little work has been focused on applying pre-trained models in unsuper"
2020.coling-main.492,J05-3002,0,0.11297,"her extracting at sentence level is the best solution for extractive methods. There are several drawbacks of extracting the full sentences. The most obvious issue is that the extracted sentences may contain unnecessary information. Some previous works have also noticed this problem and try to solve it by compressing or rewriting the extracted sentences (Martins and Smith, 2009; Chen and Bansal, 2018; Xu and Durrett, 2019). Furthermore, extracted sentences may contain duplicate contents. Thus, methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and sentence fusion (Barzilay and McKeown, 2005; Lebanoff et al., 2019) are proposed to avoid or merge duplicate contents. The redundancy and unnecessity issues might be caused by extracting full sentences since an important sentence may also contain unnecessary information. Besides, different importance sentences may have duplicate (un)important words. This inspires us that we can perform extraction at a finer granularity so that the important and unimportant contents could be separated. Therefore, we argue that extracting * Contribution done during internship at Microsoft Research Asia while pursuing PhD at Harbin Institute of Technology"
2020.coling-main.492,D19-1307,0,0.023521,"Missing"
2020.coling-main.492,P18-1063,0,0.147971,"Finally, extracted sentences are factually faithful to the input document, compared with abstractive methods (Cao et al., 2018). Despite the success of extractive systems, from previous works, it is still not clear whether extracting at sentence level is the best solution for extractive methods. There are several drawbacks of extracting the full sentences. The most obvious issue is that the extracted sentences may contain unnecessary information. Some previous works have also noticed this problem and try to solve it by compressing or rewriting the extracted sentences (Martins and Smith, 2009; Chen and Bansal, 2018; Xu and Durrett, 2019). Furthermore, extracted sentences may contain duplicate contents. Thus, methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and sentence fusion (Barzilay and McKeown, 2005; Lebanoff et al., 2019) are proposed to avoid or merge duplicate contents. The redundancy and unnecessity issues might be caused by extracting full sentences since an important sentence may also contain unnecessary information. Besides, different importance sentences may have duplicate (un)important words. This inspires us that we can perform extraction at a finer granular"
2020.coling-main.492,P16-1046,0,0.162019,"aluation of both automatic and human evaluations. Hopefully, our work could provide some inspiration of the basic extraction units in extractive summarization for future research. 1 Introduction Automatic text summarization aims to produce a brief piece of text which can preserve the most important information in it. The important contents are identified and then extracted to form the output summary (Nenkova and McKeown, 2011). In recent decades, extractive methods have proven effective in many systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015; Cheng and Lapata, 2016; Zhou et al., 2018; Nallapati et al., 2017). In previous works, extractive summarization systems perform extraction on the sentence level (Mihalcea and Tarau, 2004; Cheng and Lapata, 2016; Nallapati et al., 2017). As the extraction unit, a sentence is a grammatical unit of one or more words that express a statement, question, request, etc. There are several advantages of extracting sentences. First, extractive systems are simpler, easier to develop, and faster during run-time in real application scenarios, compared with abstractive systems. Moreover, original sentences in the input are natura"
2020.coling-main.492,D18-1409,0,0.118128,"Missing"
2020.coling-main.492,D18-1443,0,0.0291087,"2019), we randomly sampled 50 documents from the CNN/Daily Mail test set, which is the same as in §3. 1 https://github.com/abisee/cnn-dailymail 5623 6 6.1 Results Automatic Evaluation Table 3 shows the ROUGE evaluation results. We compare the SSE with the following systems: Abstractive Systems Pointer-Generator Network (PGN) (See et al., 2017) ia a sequence-to-sequence model with copy and coverage mechanisms. FastRewrite (Chen and Bansal, 2018) conducts extraction first then generation. JECS (Xu and Durrett, 2019) first extracts sentences then compresses them to reduce redundancy. Bottom-Up (Gehrmann et al., 2018) applies constrains on the copying probability. Extractive Systems LEAD3 is a commonly used baseline which simply extracts the first three sentences. T EXT R ANK (Mihalcea and Tarau, 2004) is a popular graph-based unsupervised system. S UM MA RU NN ER and NN-SE (Nallapati et al., 2017; Cheng and Lapata, 2016) use hierarchical structure for document encoding and predict sentence extraction probabilities. N EU S UM (Zhou et al., 2018) jointly model the sentence scoring and selection steps. B ERT S UM E XT, B ERT S UM E XT + T RI B LK (with trigram blocking) (Liu, 2019), S ELF -S UPERVISED (Wang"
2020.coling-main.492,N18-1065,0,0.0136492,"/TAC, CNN/Daily Mail, New York Times, etc. In this paper, we take the most commonly used dataset in recent research works (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018; Xu and Durrett, 2019; Lebanoff et al., 2019), CNN/Daily Mail, as our testbed. The statistics of it can be found in Table 2. One of the most distinguishable features of this dataset is that the output summary is in the form of highlights written by the news editors. As shown in the example in Figure 1, the summary (highlights) is a list of bullets. Therefore, extractive methods perform well on this dataset (Grusky et al., 2018). Figure 1: A screenshot example of the document-summary pair in the CNN/Daily Mail dataset. 3.2 The Drawbacks There are two main potential drawbacks of extracting sentences. First, unnecessary information is smuggled with the extracted sentences. Second, duplicate content may appear when extracting multiple sentences. To analyze whether the issues exist, we conduct experiments and analyses with both count-based statistics and human judgments. We consider two different settings to reach our final conclusion, i.e., the extractive oracle and a real extractive system. First, we check the quality"
2020.coling-main.492,P18-1249,0,0.0153127,"622 where σ(·) is the sigmoid function. The training objective of the model is the binary cross-entropy loss 00 ). given the extractive oracle label yi,j and the predicted probability p(Ci,j 5 Experiment 5.1 Dataset Following previous extractive works (Zhou et al., 2018; Xu and Durrett, 2019; Lebanoff et al., 2019; Zhang et al., 2019; Dong et al., 2018), we conduct data preprocessing using the same method1 in See et al. (2017), including sentence splitting and word tokenization. we preprocess the data as same as See et al. (2017). We then use a state-of-the-art BERT-based constituency parser (Kitaev and Klein, 2018) to process the input document whose performance is 95.17 F1 on WSJ test set. The statistic of the original CNN/Daily Mail dataset and the sub-sentential version are listed in Table 2. CNN/Daily Mail Training Dev Test #(Document) 287,227 13,368 11,490 #(Ref / Document) 1 1 1 Doc Len (Sentence) 31.58 26.72 27.05 Doc Len (Word) 791.36 769.26 778.24 Ref Len (Sentence) 3.79 4.11 3.88 Ref Len (Word) 55.17 61.43 58.31 Doc Len (Sub-Sentence) 52.84 51.37 52.02 Table 2: Data statistics of CNN/Daily Mail dataset. 5.2 Implementation Details We found that the tokenizer used in the constituency parser is d"
2020.coling-main.492,P19-1209,0,0.157846,"evel is the best solution for extractive methods. There are several drawbacks of extracting the full sentences. The most obvious issue is that the extracted sentences may contain unnecessary information. Some previous works have also noticed this problem and try to solve it by compressing or rewriting the extracted sentences (Martins and Smith, 2009; Chen and Bansal, 2018; Xu and Durrett, 2019). Furthermore, extracted sentences may contain duplicate contents. Thus, methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and sentence fusion (Barzilay and McKeown, 2005; Lebanoff et al., 2019) are proposed to avoid or merge duplicate contents. The redundancy and unnecessity issues might be caused by extracting full sentences since an important sentence may also contain unnecessary information. Besides, different importance sentences may have duplicate (un)important words. This inspires us that we can perform extraction at a finer granularity so that the important and unimportant contents could be separated. Therefore, we argue that extracting * Contribution done during internship at Microsoft Research Asia while pursuing PhD at Harbin Institute of Technology. This work is licensed"
2020.coling-main.492,W04-1013,0,0.0329308,"Transformer layers are set to 0.1. We train the model for 4 epochs which takes about 6 hours. The final model is picked according to the performance on the development set among the 4 model checkpoints. 00 ) and select the top ones. Since the During inference, we rank the extraction units according to p(Ci,j extraction unit in this paper is shorter than full sentence, we repeatedly select next sub-sentential unit until the summary length reaches the limit. The length limit is set to 60 words according to the statistics on the development set in Table 2. 5.3 Evaluation Metric We employ ROUGE (Lin, 2004) as our evaluation metric. Following previous work, we use ROUGE-1 (unigram), ROUGE-2 (bigram) and ROUGE-L (LCS) as the evaluation metrics in the experimental results. Additionally, we also conduct human evaluation on the output summaries. Following previous works (Cheng and Lapata, 2016; Nallapati et al., 2017; Liu, 2019; Zhang et al., 2019), we randomly sampled 50 documents from the CNN/Daily Mail test set, which is the same as in §3. 1 https://github.com/abisee/cnn-dailymail 5623 6 6.1 Results Automatic Evaluation Table 3 shows the ROUGE evaluation results. We compare the SSE with the follo"
2020.coling-main.492,J93-2004,0,0.0696658,"a relatively complete meaning and be human-readable. Therefore, the clause nodes, such as S and SBAR, become a good choice. In this section, we introduce how to perform extraction on the sub-sentential units, and present a BERT-based model for it. 4.1 The Sub-Sentential Units In order to perform extraction on the sub-sentential units, we need to determine what units can be extracted. The proposed method is based on the constituency parsing tree. The basic idea is based on the sub-sentential clauses in the tree. In our experiments, we adopt the syntactic tagset used in the Penn Treebank (PTB) (Marcus et al., 1993). There are two main types in the PTB tagset, phrase and clause. We use the clause tag since the information in a clause is more complete than a phrase. S S ADVP NP VBD CC VP NP PP VP S , NP VP CC S VP VP …. S SBAR WHNP S VP S Figure 2: Two simplified constituency parsing trees. The nodes in circles are candidates. The final selected node is the on in red solid-lined circle. Given the parsing tree ti of sentence si , we traverse it to determine the boundary of extraction units. Specifically, every clause is treated as the extraction unit candidates. If one of its ancestors is a clause node, we"
2020.coling-main.492,W09-1801,0,0.0464396,"d grammatically correct. Finally, extracted sentences are factually faithful to the input document, compared with abstractive methods (Cao et al., 2018). Despite the success of extractive systems, from previous works, it is still not clear whether extracting at sentence level is the best solution for extractive methods. There are several drawbacks of extracting the full sentences. The most obvious issue is that the extracted sentences may contain unnecessary information. Some previous works have also noticed this problem and try to solve it by compressing or rewriting the extracted sentences (Martins and Smith, 2009; Chen and Bansal, 2018; Xu and Durrett, 2019). Furthermore, extracted sentences may contain duplicate contents. Thus, methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and sentence fusion (Barzilay and McKeown, 2005; Lebanoff et al., 2019) are proposed to avoid or merge duplicate contents. The redundancy and unnecessity issues might be caused by extracting full sentences since an important sentence may also contain unnecessary information. Besides, different importance sentences may have duplicate (un)important words. This inspires us that we can perform extract"
2020.coling-main.492,W04-3252,0,0.758707,"etitively comparing to full sentence extraction under the evaluation of both automatic and human evaluations. Hopefully, our work could provide some inspiration of the basic extraction units in extractive summarization for future research. 1 Introduction Automatic text summarization aims to produce a brief piece of text which can preserve the most important information in it. The important contents are identified and then extracted to form the output summary (Nenkova and McKeown, 2011). In recent decades, extractive methods have proven effective in many systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015; Cheng and Lapata, 2016; Zhou et al., 2018; Nallapati et al., 2017). In previous works, extractive summarization systems perform extraction on the sentence level (Mihalcea and Tarau, 2004; Cheng and Lapata, 2016; Nallapati et al., 2017). As the extraction unit, a sentence is a grammatical unit of one or more words that express a statement, question, request, etc. There are several advantages of extracting sentences. First, extractive systems are simpler, easier to develop, and faster during run-time in real application scenarios, compared with abstractive sys"
2020.coling-main.492,P17-1099,0,0.554056,"= V . 4.2.3 Training Objective With the chunk level representation vectors C 00 , the model predict the output probability of each chunk 00 : Ci,j 00 00 p(Ci,j ) = σ(Wo Ci,j + bo ) (5) 5622 where σ(·) is the sigmoid function. The training objective of the model is the binary cross-entropy loss 00 ). given the extractive oracle label yi,j and the predicted probability p(Ci,j 5 Experiment 5.1 Dataset Following previous extractive works (Zhou et al., 2018; Xu and Durrett, 2019; Lebanoff et al., 2019; Zhang et al., 2019; Dong et al., 2018), we conduct data preprocessing using the same method1 in See et al. (2017), including sentence splitting and word tokenization. we preprocess the data as same as See et al. (2017). We then use a state-of-the-art BERT-based constituency parser (Kitaev and Klein, 2018) to process the input document whose performance is 95.17 F1 on WSJ test set. The statistic of the original CNN/Daily Mail dataset and the sub-sentential version are listed in Table 2. CNN/Daily Mail Training Dev Test #(Document) 287,227 13,368 11,490 #(Ref / Document) 1 1 1 Doc Len (Sentence) 31.58 26.72 27.05 Doc Len (Word) 791.36 769.26 778.24 Ref Len (Sentence) 3.79 4.11 3.88 Ref Len (Word) 55.17 61."
2020.coling-main.492,N06-2046,0,0.385291,"categorized from different perspectives. From the perspective of having supervision or not, there are two major types: unsupervised methods and supervised methods. One of the difficulties in training an extractive system is the lack of extraction labels. The reason is that most of the reference summary is written by human experts, therefore, it is hard to find the exact appearance in the input document. Without natural training labels, unsupervised and supervised methods treat extractive summarization as different problems. Graph-based methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan and Yang, 2006) are very useful unsupervised methods. In these methods, the input document is represented as a connected graph. The vertices represent the sentences, and the edges between vertices have attached weights that show the similarity of the two sentences. The score of a sentence is the importance of its corresponding vertex, which can be computed using graph algorithms. Supervised methods for extractive summarization create training labels manually. (Cao et al., 2015; Ren et al., 2017) directly train regression models using ROUGE scores as the supervision. (Cheng and Lapata, 2016; Nallapati et al.,"
2020.coling-main.492,P19-1214,0,0.0131439,"2018) applies constrains on the copying probability. Extractive Systems LEAD3 is a commonly used baseline which simply extracts the first three sentences. T EXT R ANK (Mihalcea and Tarau, 2004) is a popular graph-based unsupervised system. S UM MA RU NN ER and NN-SE (Nallapati et al., 2017; Cheng and Lapata, 2016) use hierarchical structure for document encoding and predict sentence extraction probabilities. N EU S UM (Zhou et al., 2018) jointly model the sentence scoring and selection steps. B ERT S UM E XT, B ERT S UM E XT + T RI B LK (with trigram blocking) (Liu, 2019), S ELF -S UPERVISED (Wang et al., 2019) and HIBERT (Zhang et al., 2019) use pre-training techniques in extractive document summarization. BERT-SENT is the sentence-level extractive baseline described in section 3.2 . Model PGN FastRewrite JECS Bottom-Up LEAD3 T EXT R ANK ROUGE -1 ROUGE -2 ROUGE -L S UMMA RU NN ER NN-SE N EU S UM B ERT S UM E XT B ERT S UM E XT +T RI B LK S ELF -S UPERVISED HIBERT BERT-SENT SSE 39.53 40.88 41.70 41.22 40.24 40.20 39.60 41.13 41.59 42.61 43.25 41.36 42.10 42.13 42.72 17.28 17.80 18.50 18.68 17.70 17.56 16.20 18.59 19.01 19.99 20.24 19.20 19.70 19.73 20.29 36.38 38.54 37.90 38.34 36.45 36.44 35.30 37."
2020.coling-main.492,P10-1058,0,0.0396597,"cate content may appear when extracting multiple sentences. To analyze whether the issues exist, we conduct experiments and analyses with both count-based statistics and human judgments. We consider two different settings to reach our final conclusion, i.e., the extractive oracle and a real extractive system. First, we check the quality of the sentence level extractive oracle, since it is the upper bound of any extraction system. Two different methods are used in recent extractive summarization research for building the oracle training label. The first one is based on semantic correspondence (Woodsend and Lapata, 2010) of document sentences and reference summary, used in (Cheng and Lapata, 2016). The second one is heuristic, which maximizes the ROUGE score with respect to gold summaries. This one is more broadly used in many recent extractive systems (Nallapati et al., 2017; Zhou et al., 2018; Zhang et al., 2019; Liu, 2019). We adopt the second method since it is more widely used and easy to implement. The extractive oracle is computed with the metric of ROUGE-2 F1 score, which is also the metric used in the final automatic evaluation in these systems. Second, we check the output of a BERT-based sentence le"
2020.coling-main.492,D19-1324,0,0.334292,"ences are factually faithful to the input document, compared with abstractive methods (Cao et al., 2018). Despite the success of extractive systems, from previous works, it is still not clear whether extracting at sentence level is the best solution for extractive methods. There are several drawbacks of extracting the full sentences. The most obvious issue is that the extracted sentences may contain unnecessary information. Some previous works have also noticed this problem and try to solve it by compressing or rewriting the extracted sentences (Martins and Smith, 2009; Chen and Bansal, 2018; Xu and Durrett, 2019). Furthermore, extracted sentences may contain duplicate contents. Thus, methods such as Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998) and sentence fusion (Barzilay and McKeown, 2005; Lebanoff et al., 2019) are proposed to avoid or merge duplicate contents. The redundancy and unnecessity issues might be caused by extracting full sentences since an important sentence may also contain unnecessary information. Besides, different importance sentences may have duplicate (un)important words. This inspires us that we can perform extraction at a finer granularity so that the importa"
2020.coling-main.492,P19-1499,1,0.512227,"sed methods. In these methods, the input document is represented as a connected graph. The vertices represent the sentences, and the edges between vertices have attached weights that show the similarity of the two sentences. The score of a sentence is the importance of its corresponding vertex, which can be computed using graph algorithms. Supervised methods for extractive summarization create training labels manually. (Cao et al., 2015; Ren et al., 2017) directly train regression models using ROUGE scores as the supervision. (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018; Zhang et al., 2019) search the oracle extracted sentences as the training labels. Cheng and Lapata (2016) propose treating document summarization as a sequence labeling task. They first encode the sentences in the document and then classify each sentence into two classes, i.e., extraction or not. Nallapati et al. (2017) propose a system called SummaRuNNer with more features, which also treat extractive document summarization as a sequence labeling task. Zhou et al. (2018) propose using pointer networks (Vinyals et al., 2015) to repeatedly extract sentences. Recently, Reinforcement Learning (RL) is also introduce"
2020.coling-main.492,P18-1061,1,0.620465,"ic and human evaluations. Hopefully, our work could provide some inspiration of the basic extraction units in extractive summarization for future research. 1 Introduction Automatic text summarization aims to produce a brief piece of text which can preserve the most important information in it. The important contents are identified and then extracted to form the output summary (Nenkova and McKeown, 2011). In recent decades, extractive methods have proven effective in many systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015; Cheng and Lapata, 2016; Zhou et al., 2018; Nallapati et al., 2017). In previous works, extractive summarization systems perform extraction on the sentence level (Mihalcea and Tarau, 2004; Cheng and Lapata, 2016; Nallapati et al., 2017). As the extraction unit, a sentence is a grammatical unit of one or more words that express a statement, question, request, etc. There are several advantages of extracting sentences. First, extractive systems are simpler, easier to develop, and faster during run-time in real application scenarios, compared with abstractive systems. Moreover, original sentences in the input are naturally fluent and gram"
2020.coling-main.492,D15-1042,0,\N,Missing
2020.coling-main.492,W01-0100,0,\N,Missing
2020.coling-main.492,D18-1088,1,\N,Missing
2020.coling-main.82,N18-1202,0,0.0316223,"layout variations. Recently, the rapid development of deep learning in computer vision has significantly boosted the data-driven image-based approaches for document layout analysis. Although these approaches have been widely adopted and made significant progress, they usually leverage visual features while neglecting textual features from the documents. Therefore, it is inevitable to explore how to leverage the visual and textual information in a unified way for document layout analysis. Nowadays, the state-of-the-art computer vision and NLP models are often built upon the pre-trained models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Lample and Conneau, 2019; Yang et al., 2019; Dong et al., 2019; Raffel et al., 2019; Xu et al., 2019) followed by fine-tuning on specific downstream tasks, which achieves very promising results. However, pre-trained models not only require large-scale unlabeled data for self-supervised learning, but also need high quality labeled data for task-specific fine-tuning to achieve good performance. For document layout analysis tasks, there have been some image-based document layout datasets, while most of them are built for computer vision approaches and"
2020.coling-main.82,D19-1348,0,0.0586205,"Missing"
2020.emnlp-main.297,N18-1150,0,0.0196461,"d tuned the minimum summary length on the validation set in the range of [30, 80]. The search range of minimum summary length was empirically set according to the summaries of training split of CNNDM, where the average and medium minimum lengths are both around 55. We used step size of 5 to get quick feedback. Similar 3651 to the pre-training process, the datasets with less instances were fine-tuned with smaller batch sizes (i.e., 64 for NYT and 768 for CNNDM). 5 Model Lead3 BERTExt (Liu and Lapata, 2019) PTGen (See et al., 2017) DRM (Paulus et al., 2018) BottomUp (Gehrmann et al., 2018) DCA (Celikyilmaz et al., 2018) BERTAbs (Liu and Lapata, 2019) UniLM (Dong et al., 2019) T RANSFORMER-S2S RO BERTABASE -S2S RO BERTA-S2S RO BERTACONT -S2S Automatic Evaluation We used ROUGE (Lin, 2004) to measure the quality of different summarization model outputs. We reported full-length F1 based ROUGE1, ROUGE-2 and ROUGE-L scores on CNNDM, while we used the limited-length recall based ROUGE-1, ROUGE-2 and ROUGEL on NYT, following Durrett et al. (2016). The ROUGE scores are computed using the ROUGE-1.5.5.pl script6 . Models in Comparison Lead3 is a baseline which simply takes the first three sentences of a document as its"
2020.emnlp-main.297,P18-1063,0,0.0208635,"rage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement learning (Chen and Bansal, 2018), while our model is simpler. Pre-training Pre-training methods draw a lot of attentions recently. Peters et al. (2018) and Radford et al. (2019) pre-trained LSTM and Transformer using language modeling objectives. To leverage the context in both directions, BERT (Devlin et al., 2019) is trained with the masked language modeling and next sentence prediction objectives. SpanBERT (Joshi et al., 2020) applied only the masked language modeling objective that masks contiguous random spans, rather than random tokens. XLNet (Yang et al., 2019) proposed a permutation language modeling objective that r"
2020.emnlp-main.297,D18-1443,0,0.178386,") that have been extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement learning (Chen and Bansal, 2018), while our model is simpler. Pre-training Pre-training methods draw a lot of attentions recently. Peters et al. (2018) and Radford et al. (2019) pre-trained LSTM and Transformer using language modeling objectives. To leverage the context in both directions, BERT (Devlin et al., 2019) is trained with the masked language modeling and next sentence prediction objectives. SpanBERT (Joshi et al., 2020) applied only the masked language modeling objective that masks contiguous random spans, rather than random tokens. XLNet (Yang e"
2020.emnlp-main.297,P16-1154,0,0.0129739,") that lead to another huge performance gain. However, extractive models have their own limitations. For example, the extracted sentences might be too long and redundant. Besides, manually written summaries in their nature are abstractive. Therefore, we focus on abstractive summarization in this paper. Abstractive Summarization This task aims to generate a summary by rewriting a document, which is a SEQ 2 SEQ learning problem. SEQ 2 SEQ attentive LSTMs (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015) are employed in Nallapati et al. (2016) that have been extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement lear"
2020.emnlp-main.297,P18-1013,0,0.0277906,"ed with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2018). Liu and Lapata (2019) used a SEQ 2 SEQ Transformer model with only its encoder initialized with a pre-trained Transformer encoder (i.e., BERT; Devlin et al. 2019). This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model. There is also a line of work that bridges extractive and abstractive models with attention mechanisms (Gehrmann et al., 2018; Hsu et al., 2018) and reinforcement learning (Chen and Bansal, 2018), while our model is simpler. Pre-training Pre-training methods draw a lot of attentions recently. Peters et al. (2018) and Radford et al. (2019) pre-trained LSTM and Transformer using language modeling objectives. To leverage the context in both directions, BERT (Devlin et al., 2019) is trained with the masked language modeling and next sentence prediction objectives. SpanBERT (Joshi et al., 2020) applied only the masked language modeling objective that masks contiguous random spans, rather than random tokens. XLNet (Yang et al., 2019) propos"
2020.emnlp-main.297,N19-4009,0,0.0673284,"Missing"
2020.emnlp-main.54,W05-0909,0,0.397637,"Missing"
2020.emnlp-main.54,D18-1454,0,0.0573109,"Missing"
2020.emnlp-main.54,P19-1470,0,0.0376001,"rporate one-hop knowledge graph for concepts in the story context. In topic-to-essay generation, Yang et al. (2019) augmented the generator with a concept memory that updated dynamically with gate mechanism. Recently, some work also attempted to integrate external commonsense knowledge into generative pretrained language models such as GPT-2 (Radford et al., 2019). Guan et al. (2020) conducted posttraining on sythetic data constructed from commonsense knowledge bases by translating triplets into natural language texts using templates. Bhagavatula et al. (2020) transferred embeddings of COMeT (Bosselut et al., 2019), a GPT-2 model fine-tuned to generate the tail entity of a triple in commonsense knowledge graph, into another GPT-2 model for text generation. In comparison, our model utilizes both structural and semantic information of the commonsense knowledge graph during generation and does not suffers from the catastrophic forgetting problem (Kirkpatrick et al., 2016) caused by implicit knowledge transferring. Our contributions can be summarized as follows: 1) We propose GRF, a novel generation model that utilizes external structural commonsense knowledge to facilitate explicit commonsense reasoning in"
2020.emnlp-main.54,N19-1240,0,0.295331,"e commonsense-aware text generation tasks and show that our model outperforms various selective baselines. We also visualize reasoning paths inferred by the model to demonstrate the effectiveness of the multi-hop reasoning module. 2 2.1 Related Work Commonsense-Aware Neural Text Generation Incorporating commonsense knowledge is essential for text generation to augment the limited textual information. In dialogue generation, Zhou et al. (2018) enriched the context representations of the post with neighbouring concepts on ConceptNet using graph attention. In story ending generation, Guan et al. (2019) proposed incremental encoding with multi-source attention to incorporate one-hop knowledge graph for concepts in the story context. In topic-to-essay generation, Yang et al. (2019) augmented the generator with a concept memory that updated dynamically with gate mechanism. Recently, some work also attempted to integrate external commonsense knowledge into generative pretrained language models such as GPT-2 (Radford et al., 2019). Guan et al. (2020) conducted posttraining on sythetic data constructed from commonsense knowledge bases by translating triplets into natural language texts using temp"
2020.emnlp-main.54,N18-1165,0,0.0287248,"sed by implicit knowledge transferring. Our contributions can be summarized as follows: 1) We propose GRF, a novel generation model that utilizes external structural commonsense knowledge to facilitate explicit commonsense reasoning in text generation. 2) We propose the dynamic multi-hop reasoning module that aggregates evidence along relational paths for grounded gener726 2.2 Multi-Hop Reasoning on Graph Structure Performing explicit multi-hop reasoning on graph structure has been demonstrated to be an effective approach for query answering over incomplete knowledge graphs (Das et al., 2018; Chen et al., 2018; Lin et al., 2018), multi-hop question answering (Bauer et al., 2018; Cao et al., 2019; Qiu et al., 2019) and dialogue generation (Tuan et al., Decoder hidden state Layer Norm Feed Forward LD x Layer Norm Concept distribution H-hops Vocab distribution Masked Self-Attention Word embedding (x1 x2 ... xN [bos] y1 y2 ... yt-1) Graph representations Reasoning module (a) (b) (c) (d) Figure 2: Model architecture. (a) Context modeling with pre-trained transformer (§3.2.2). (b) The model encodes the multi-relational graph with non-parametric operation φ(·) to combine relations and concepts (§3.2.1). ("
2020.emnlp-main.54,P16-1154,0,0.191063,"v)), (11) where ct is the concept of the selected node at the t-th time step. Intuitively, the reasoning module learns to dynamically distribute along the paths by considering the triple evidence according to the current decoder state. 3.2.4 Generation Distribution with Gate Control The final generation distribution combines the distribution over the concepts (Eq. 11) and the distribution over the standard vocabulary (Eq. 7). We use a soft gate probability gt which denotes whether to copy a concept in the generation to control the weight of the two distributions similar to the copy mechanism (Gu et al., 2016; See et al., 2017).   D . (12) gt = σ Wgate hL t 728 The final output distribution is the linear combination of the two distributions weighted by gt and 1 − gt respectively. P (yt |y<t , x, G) = gt+N · P (ct+N |s<t+N , G) + (1 − gt+N ) · P (st+N |s<t+N ), (13) where N is the length of the input text sequence. 3.3 Training and Inference To train the proposed model, we minimize the negative log-likelihood of generating the ground truth target sequence y gold = (y1 , y2 · · · , yM , [eos]). Lgen = M +1 X gold − log P (yt gold |y<t , x, G). Explanation Generation (EG) is to generate an explanat"
2020.emnlp-main.54,2020.tacl-1.7,1,0.939847,"ch as (volcano, MadeOf, lava) besides the story context. Although pre-trained models have been demonstrated to possess commonsense reasoning ability (Trinh and Le, 2018) by implicitly learning some relational patterns from large-scale corpora, they do not fully utilize the commonsense knowledge bases that provide more explicit knowledge grounding. To address this defect, incorporating external commonsense knowledge to enhance models’ reasoning ability has been widely explored (Lin et al., 2019; Ye et al., 2019; Lv et al., 2019). In language generation, previous work (Bhagavatula et al., 2020; Guan et al., 2020) transfers commonsense knowledge into pre-trained language models by utilizing triple information in commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012) and ATOMIC (Sap et al., 2019). However, this approach has two drawbacks. 725 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 725–736, c November 16–20, 2020. 2020 Association for Computational Linguistics First, recovering knowledge triples at the posttraining stage (Guan et al., 2020) hardly enables the model to utilize the encoded knowledge in fine-tuning generation tasks whic"
2020.emnlp-main.54,N16-1014,0,0.11829,"Missing"
2020.emnlp-main.54,D19-1282,0,0.198087,"y the model that provide rationale to the generation.1 “lava” in the story ending by providing background knowledge such as (volcano, MadeOf, lava) besides the story context. Although pre-trained models have been demonstrated to possess commonsense reasoning ability (Trinh and Le, 2018) by implicitly learning some relational patterns from large-scale corpora, they do not fully utilize the commonsense knowledge bases that provide more explicit knowledge grounding. To address this defect, incorporating external commonsense knowledge to enhance models’ reasoning ability has been widely explored (Lin et al., 2019; Ye et al., 2019; Lv et al., 2019). In language generation, previous work (Bhagavatula et al., 2020; Guan et al., 2020) transfers commonsense knowledge into pre-trained language models by utilizing triple information in commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012) and ATOMIC (Sap et al., 2019). However, this approach has two drawbacks. 725 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 725–736, c November 16–20, 2020. 2020 Association for Computational Linguistics First, recovering knowledge triples at the posttraining"
2020.emnlp-main.54,W04-1013,0,0.0182468,"Missing"
2020.emnlp-main.54,D18-1362,0,0.12802,"wledge transferring. Our contributions can be summarized as follows: 1) We propose GRF, a novel generation model that utilizes external structural commonsense knowledge to facilitate explicit commonsense reasoning in text generation. 2) We propose the dynamic multi-hop reasoning module that aggregates evidence along relational paths for grounded gener726 2.2 Multi-Hop Reasoning on Graph Structure Performing explicit multi-hop reasoning on graph structure has been demonstrated to be an effective approach for query answering over incomplete knowledge graphs (Das et al., 2018; Chen et al., 2018; Lin et al., 2018), multi-hop question answering (Bauer et al., 2018; Cao et al., 2019; Qiu et al., 2019) and dialogue generation (Tuan et al., Decoder hidden state Layer Norm Feed Forward LD x Layer Norm Concept distribution H-hops Vocab distribution Masked Self-Attention Word embedding (x1 x2 ... xN [bos] y1 y2 ... yt-1) Graph representations Reasoning module (a) (b) (c) (d) Figure 2: Model architecture. (a) Context modeling with pre-trained transformer (§3.2.2). (b) The model encodes the multi-relational graph with non-parametric operation φ(·) to combine relations and concepts (§3.2.1). (c) The multi-hop re"
2020.emnlp-main.54,D19-1187,0,0.0130561,"Masked Self-Attention Word embedding (x1 x2 ... xN [bos] y1 y2 ... yt-1) Graph representations Reasoning module (a) (b) (c) (d) Figure 2: Model architecture. (a) Context modeling with pre-trained transformer (§3.2.2). (b) The model encodes the multi-relational graph with non-parametric operation φ(·) to combine relations and concepts (§3.2.1). (c) The multi-hop reasoning module aggregates evidence from source concepts Cx along structural paths to all nodes where shade indicates the node score (§3.2.3). (d) The final generation distribution with gate control (§3.2.4). 2019; Moon et al., 2019; Liu et al., 2019). Particularly, reasoning on knowledge graphs to answer relational query typically adopts REINFORCE to learn concrete policies to search for entities or relations. In multi-hop question answering tasks, the reasoning process is augmented with entity graph (Cao et al., 2019; Qiu et al., 2019) or concept paths (Bauer et al., 2018) to enhance semantic connections among document segments. In dialogue generation, Tuan et al. (2019) modeled multiple hops on relationship graphs with a Markov transition matrix. Liu et al. (2019) proposed a twostage architecture that selected information from a knowled"
2020.emnlp-main.54,D17-1159,0,0.0309066,"graph extraction process in §4.2 and describe our proposed model in the next section. 3.2 Static Multi-Relational Graph Encoding Graph Neural Network (GNN) frameworks, such as graph convolution network (GCN) (Kipf and Welling, 2017) and graph attention network (GAT) (Velickovic et al., 2018), have been shown effective at encoding graph-structured data by aggregating node information from local neighbours. To model the relational information in the knowledge graph, R-GCN (Schlichtkrull et al., 2018) generalizes GCN with relationspecific weight matrices but is reported to be over-parameterized (Marcheggiani and Titov, 2017; Schlichtkrull et al., 2018). We follow Vashishth et al. (2020) and use a non-parametric compositional operation φ(·) to combine the node embedding and the relation embedding. Specifically, 727 3.2.1 Generation with Multi-Hop Reasoning Flow given the input graph G = (V, E) and a GCN with LG layer, for each node v ∈ V we update the node embedding at the l + 1-th layer by aggregating information from its local neighbours N (v) which consist of pairs of node u and the connected relation r. X 1 l WN φ(hlu , hlr ), (2) olv = |N (v)| (u,r)∈N (v)   l l l (3) hl+1 = ReLU o + W h v v S v , where h0v"
2020.emnlp-main.54,P02-1040,0,0.106054,"Missing"
2020.emnlp-main.54,P19-1617,0,0.0334703,"a novel generation model that utilizes external structural commonsense knowledge to facilitate explicit commonsense reasoning in text generation. 2) We propose the dynamic multi-hop reasoning module that aggregates evidence along relational paths for grounded gener726 2.2 Multi-Hop Reasoning on Graph Structure Performing explicit multi-hop reasoning on graph structure has been demonstrated to be an effective approach for query answering over incomplete knowledge graphs (Das et al., 2018; Chen et al., 2018; Lin et al., 2018), multi-hop question answering (Bauer et al., 2018; Cao et al., 2019; Qiu et al., 2019) and dialogue generation (Tuan et al., Decoder hidden state Layer Norm Feed Forward LD x Layer Norm Concept distribution H-hops Vocab distribution Masked Self-Attention Word embedding (x1 x2 ... xN [bos] y1 y2 ... yt-1) Graph representations Reasoning module (a) (b) (c) (d) Figure 2: Model architecture. (a) Context modeling with pre-trained transformer (§3.2.2). (b) The model encodes the multi-relational graph with non-parametric operation φ(·) to combine relations and concepts (§3.2.1). (c) The multi-hop reasoning module aggregates evidence from source concepts Cx along structural paths to al"
2020.emnlp-main.54,P19-1081,0,0.0288194,"Vocab distribution Masked Self-Attention Word embedding (x1 x2 ... xN [bos] y1 y2 ... yt-1) Graph representations Reasoning module (a) (b) (c) (d) Figure 2: Model architecture. (a) Context modeling with pre-trained transformer (§3.2.2). (b) The model encodes the multi-relational graph with non-parametric operation φ(·) to combine relations and concepts (§3.2.1). (c) The multi-hop reasoning module aggregates evidence from source concepts Cx along structural paths to all nodes where shade indicates the node score (§3.2.3). (d) The final generation distribution with gate control (§3.2.4). 2019; Moon et al., 2019; Liu et al., 2019). Particularly, reasoning on knowledge graphs to answer relational query typically adopts REINFORCE to learn concrete policies to search for entities or relations. In multi-hop question answering tasks, the reasoning process is augmented with entity graph (Cao et al., 2019; Qiu et al., 2019) or concept paths (Bauer et al., 2018) to enhance semantic connections among document segments. In dialogue generation, Tuan et al. (2019) modeled multiple hops on relationship graphs with a Markov transition matrix. Liu et al. (2019) proposed a twostage architecture that selected informa"
2020.emnlp-main.54,P17-1099,0,0.0148382,"t is the concept of the selected node at the t-th time step. Intuitively, the reasoning module learns to dynamically distribute along the paths by considering the triple evidence according to the current decoder state. 3.2.4 Generation Distribution with Gate Control The final generation distribution combines the distribution over the concepts (Eq. 11) and the distribution over the standard vocabulary (Eq. 7). We use a soft gate probability gt which denotes whether to copy a concept in the generation to control the weight of the two distributions similar to the copy mechanism (Gu et al., 2016; See et al., 2017).   D . (12) gt = σ Wgate hL t 728 The final output distribution is the linear combination of the two distributions weighted by gt and 1 − gt respectively. P (yt |y<t , x, G) = gt+N · P (ct+N |s<t+N , G) + (1 − gt+N ) · P (st+N |s<t+N ), (13) where N is the length of the input text sequence. 3.3 Training and Inference To train the proposed model, we minimize the negative log-likelihood of generating the ground truth target sequence y gold = (y1 , y2 · · · , yM , [eos]). Lgen = M +1 X gold − log P (yt gold |y<t , x, G). Explanation Generation (EG) is to generate an explanation given a counter"
2020.emnlp-main.54,N16-1098,0,0.053806,"resentations for the concepts and the relations (§3.2.1). Then, the multi-hop reasoning module performs dynamic reasoning via aggregating triple evidence along multiple relational paths to generate the salient concept under the context (§3.2.3). Finally, the generation distribution combines the probability of copying concepts from the knowledge graph and that of choosing a word from the standard vocabulary with a gate control (§3.2.4). The overall model architecture is shown in Figure 2. We conduct experiments on three commonsense-aware text generation tasks including story ending generation (Mostafazadeh et al., 2016), abductive natural language generation (Bhagavatula et al., 2020), and explanation generation for sense making (Wang et al., 2019). Results show that our model outperforms strong baselines on these tasks, thereby demonstrating the benefit of multi-hop commonsense reasoning in language generation. ation of some critical concepts. 3) We conduct extensive experiments including automatic and human evaluation on three commonsense-aware text generation tasks and show that our model outperforms various selective baselines. We also visualize reasoning paths inferred by the model to demonstrate the ef"
2020.emnlp-main.54,D19-1194,0,0.0123326,"ng structural paths to all nodes where shade indicates the node score (§3.2.3). (d) The final generation distribution with gate control (§3.2.4). 2019; Moon et al., 2019; Liu et al., 2019). Particularly, reasoning on knowledge graphs to answer relational query typically adopts REINFORCE to learn concrete policies to search for entities or relations. In multi-hop question answering tasks, the reasoning process is augmented with entity graph (Cao et al., 2019; Qiu et al., 2019) or concept paths (Bauer et al., 2018) to enhance semantic connections among document segments. In dialogue generation, Tuan et al. (2019) modeled multiple hops on relationship graphs with a Markov transition matrix. Liu et al. (2019) proposed a twostage architecture that selected information from a knowledge graph for further generating the response. Compared with these generation models that operate on knowledge graphs within a specific domain, our focus is to utilize general commonsense knowledge to supply evidence for text generation. 3 3.1 Methodology Problem Formulation In this paper, we focus on text generation tasks where reasoning over external commonsense knowledge is required. Without loss of generality, the input sou"
2020.emnlp-main.54,P19-1393,0,0.0236239,"riple evidence along multiple relational paths to generate the salient concept under the context (§3.2.3). Finally, the generation distribution combines the probability of copying concepts from the knowledge graph and that of choosing a word from the standard vocabulary with a gate control (§3.2.4). The overall model architecture is shown in Figure 2. We conduct experiments on three commonsense-aware text generation tasks including story ending generation (Mostafazadeh et al., 2016), abductive natural language generation (Bhagavatula et al., 2020), and explanation generation for sense making (Wang et al., 2019). Results show that our model outperforms strong baselines on these tasks, thereby demonstrating the benefit of multi-hop commonsense reasoning in language generation. ation of some critical concepts. 3) We conduct extensive experiments including automatic and human evaluation on three commonsense-aware text generation tasks and show that our model outperforms various selective baselines. We also visualize reasoning paths inferred by the model to demonstrate the effectiveness of the multi-hop reasoning module. 2 2.1 Related Work Commonsense-Aware Neural Text Generation Incorporating commonsens"
2020.emnlp-main.54,P19-1193,0,0.0310046,"onstrate the effectiveness of the multi-hop reasoning module. 2 2.1 Related Work Commonsense-Aware Neural Text Generation Incorporating commonsense knowledge is essential for text generation to augment the limited textual information. In dialogue generation, Zhou et al. (2018) enriched the context representations of the post with neighbouring concepts on ConceptNet using graph attention. In story ending generation, Guan et al. (2019) proposed incremental encoding with multi-source attention to incorporate one-hop knowledge graph for concepts in the story context. In topic-to-essay generation, Yang et al. (2019) augmented the generator with a concept memory that updated dynamically with gate mechanism. Recently, some work also attempted to integrate external commonsense knowledge into generative pretrained language models such as GPT-2 (Radford et al., 2019). Guan et al. (2020) conducted posttraining on sythetic data constructed from commonsense knowledge bases by translating triplets into natural language texts using templates. Bhagavatula et al. (2020) transferred embeddings of COMeT (Bosselut et al., 2019), a GPT-2 model fine-tuned to generate the tail entity of a triple in commonsense knowledge g"
2020.emnlp-main.581,2020.bea-1.16,0,0.122574,"Missing"
2020.emnlp-main.633,L18-1269,0,0.0207178,"is the equivalent learning rate considering all successor modules. Thus, when applying a replacement scheduler, a warm-up mechanism (Popel and Bojar, 2018) is essentially adopted at the same time, which helps the training of a Transformer. 7862 4 Experiments In this section, we introduce the experiments of Theseus Compression for BERT (Devlin et al., 2019) compression. We compare BERT-ofTheseus with other compression methods and further conduct experiments to analyze the results. 4.1 Datasets We evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019; Dolan and Brockett, 2005; Conneau and Kiela, 2018; Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016; Warstadt et al., 2019). Note that we exclude WNLI (Levesque, 2011) following the original BERT paper (Devlin et al., 2019). The accuracy is used as the metric for SST-2, MNLI-m, MNLI-mm, QNLI and RTE. The F1 and accuracy are used for MRPC and QQP. The Pearson correlation and Spearman correlation are used for STS-B. Matthew’s correlation is used for CoLA. The results reported for the test set of GLUE are in the same format as on the official leaderboard. For the sake of comparison with (Sanh et al., 2019), on the development"
2020.emnlp-main.633,N19-1423,0,0.383234,"ge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.1 1 Introduction With the prevalence of deep learning, many huge neural models have been proposed and achieve state-of-the-art performance in various fields (He et al., 2016; Vaswani et al., 2017). Specifically, in Natural Language Processing (NLP), pretraining and fine-tuning have become the new norm of most tasks. Transformer-based pretrained models (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019; Song et al., 2019; Dong et al., 2019) have dominated the field of both Natural Language Understanding (NLU) and Natural Language Generation (NLG). These models benefit from their “overparameterized” nature (Nakkiran et al., 2020) and contain millions or even billions of parameters, making it computationally expensive and inefficient considering both memory consumption and ∗ Equal contribution. Work done during these two authors’ internship at Microsoft Research Asia. 1 The code and pretrained model are available at https: //github.com/JetRunner/BERT-of-T"
2020.emnlp-main.633,I05-5002,0,0.0126639,"r the compression and lr0 is the equivalent learning rate considering all successor modules. Thus, when applying a replacement scheduler, a warm-up mechanism (Popel and Bojar, 2018) is essentially adopted at the same time, which helps the training of a Transformer. 7862 4 Experiments In this section, we introduce the experiments of Theseus Compression for BERT (Devlin et al., 2019) compression. We compare BERT-ofTheseus with other compression methods and further conduct experiments to analyze the results. 4.1 Datasets We evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019; Dolan and Brockett, 2005; Conneau and Kiela, 2018; Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016; Warstadt et al., 2019). Note that we exclude WNLI (Levesque, 2011) following the original BERT paper (Devlin et al., 2019). The accuracy is used as the metric for SST-2, MNLI-m, MNLI-mm, QNLI and RTE. The F1 and accuracy are used for MRPC and QQP. The Pearson correlation and Spearman correlation are used for STS-B. Matthew’s correlation is used for CoLA. The results reported for the test set of GLUE are in the same format as on the official leaderboard. For the sake of comparison with (Sanh et al., 2"
2020.emnlp-main.633,P84-1044,0,0.289138,"Missing"
2020.emnlp-main.633,2021.ccl-1.108,0,0.236556,"Missing"
2020.emnlp-main.633,2020.acl-main.467,0,0.0449486,"Missing"
2020.emnlp-main.633,D13-1170,0,0.0197042,"g rate considering all successor modules. Thus, when applying a replacement scheduler, a warm-up mechanism (Popel and Bojar, 2018) is essentially adopted at the same time, which helps the training of a Transformer. 7862 4 Experiments In this section, we introduce the experiments of Theseus Compression for BERT (Devlin et al., 2019) compression. We compare BERT-ofTheseus with other compression methods and further conduct experiments to analyze the results. 4.1 Datasets We evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019; Dolan and Brockett, 2005; Conneau and Kiela, 2018; Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016; Warstadt et al., 2019). Note that we exclude WNLI (Levesque, 2011) following the original BERT paper (Devlin et al., 2019). The accuracy is used as the metric for SST-2, MNLI-m, MNLI-mm, QNLI and RTE. The F1 and accuracy are used for MRPC and QQP. The Pearson correlation and Spearman correlation are used for STS-B. Matthew’s correlation is used for CoLA. The results reported for the test set of GLUE are in the same format as on the official leaderboard. For the sake of comparison with (Sanh et al., 2019), on the development set of GLUE, the resu"
2020.emnlp-main.633,2020.findings-emnlp.178,1,0.902857,"onding successor module scci by the probability of p. (b) During successor fine-tuning and inference, all successor modules scc1...3 are combined for calculation. tion (Turc et al., 2019) pretrains the student model with a self-supervised masked LM objective on a large corpus first, then performs a standard KD on supervised tasks. TinyBERT (Jiao et al., 2019) conducts the Knowledge Distillation twice with data augmentation. MobileBERT (Sun et al., 2020) devises a more computationally efficient architecture and applies knowledge distillation with a bottom-totop layer training procedure. PABEE (Zhou et al., 2020b) exploits early exiting to dynamically accelerate the inference of BERT. 3 BERT-of-Theseus In this section, we introduce module replacing, the technique proposed for BERT-of-Theseus. Further, we introduce a Curriculum Learning driven scheduler to obtain better performance. The workflow is shown in Figure 1. 3.1 specific loss function (e.g., Cross Entropy), which closely resembles a fine-tuning procedure. Inspired by Dropout (Srivastava et al., 2014), we propose module replacing, a novel technique for model compression. We call the original model and the target model predecessor and successor"
2020.emnlp-main.633,D19-1441,0,0.294124,"Quantization (Gong et al., 2014), Weights Pruning (Han et al., 2016) and Knowledge Distillation (KD) (Hinton et al., 2015). Among them, KD has received much attention for compressing pretrained language models. KD exploits a large teacher model to “teach” a compact student model to mimic the teacher’s behavior. In this way, the knowledge embedded in the teacher model can be transferred into the smaller model. However, the retained performance of the student model relies on a welldesigned distillation loss function which forces the student model to behave as the teacher. Recent studies on KD (Sun et al., 2019; Jiao et al., 2019) even leverage more sophisticated model-specific distillation loss functions for better performance. Different from previous KD studies which explicitly exploit a distillation loss to minimize the distance between the teacher model and the student model, we propose a new genre of model compression. Inspired by the famous thought experiment “Ship of Theseus”2 in Philosophy, where all components of a ship are gradually replaced by new ones until no original component exists, we propose Theseus Compression for BERT (BERT-ofTheseus), which progressively substitutes modules of B"
2020.emnlp-main.633,2020.acl-main.195,0,0.530066,". . , scc3 }. prdi and scci contain two and one layer, respectively. (a) During module replacing training, each predecessor module prdi is replaced with corresponding successor module scci by the probability of p. (b) During successor fine-tuning and inference, all successor modules scc1...3 are combined for calculation. tion (Turc et al., 2019) pretrains the student model with a self-supervised masked LM objective on a large corpus first, then performs a standard KD on supervised tasks. TinyBERT (Jiao et al., 2019) conducts the Knowledge Distillation twice with data augmentation. MobileBERT (Sun et al., 2020) devises a more computationally efficient architecture and applies knowledge distillation with a bottom-totop layer training procedure. PABEE (Zhou et al., 2020b) exploits early exiting to dynamically accelerate the inference of BERT. 3 BERT-of-Theseus In this section, we introduce module replacing, the technique proposed for BERT-of-Theseus. Further, we introduce a Curriculum Learning driven scheduler to obtain better performance. The workflow is shown in Figure 1. 3.1 specific loss function (e.g., Cross Entropy), which closely resembles a fine-tuning procedure. Inspired by Dropout (Srivastav"
2020.emnlp-main.633,Q19-1040,0,0.0431157,"acement scheduler, a warm-up mechanism (Popel and Bojar, 2018) is essentially adopted at the same time, which helps the training of a Transformer. 7862 4 Experiments In this section, we introduce the experiments of Theseus Compression for BERT (Devlin et al., 2019) compression. We compare BERT-ofTheseus with other compression methods and further conduct experiments to analyze the results. 4.1 Datasets We evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019; Dolan and Brockett, 2005; Conneau and Kiela, 2018; Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016; Warstadt et al., 2019). Note that we exclude WNLI (Levesque, 2011) following the original BERT paper (Devlin et al., 2019). The accuracy is used as the metric for SST-2, MNLI-m, MNLI-mm, QNLI and RTE. The F1 and accuracy are used for MRPC and QQP. The Pearson correlation and Spearman correlation are used for STS-B. Matthew’s correlation is used for CoLA. The results reported for the test set of GLUE are in the same format as on the official leaderboard. For the sake of comparison with (Sanh et al., 2019), on the development set of GLUE, the result of MNLI is an average on MNLI-m and MNLI-mm; the results on MRPC and"
2020.emnlp-main.633,N18-1101,0,0.0249639,"l successor modules. Thus, when applying a replacement scheduler, a warm-up mechanism (Popel and Bojar, 2018) is essentially adopted at the same time, which helps the training of a Transformer. 7862 4 Experiments In this section, we introduce the experiments of Theseus Compression for BERT (Devlin et al., 2019) compression. We compare BERT-ofTheseus with other compression methods and further conduct experiments to analyze the results. 4.1 Datasets We evaluate our proposed approach on the GLUE benchmark (Wang et al., 2019; Dolan and Brockett, 2005; Conneau and Kiela, 2018; Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016; Warstadt et al., 2019). Note that we exclude WNLI (Levesque, 2011) following the original BERT paper (Devlin et al., 2019). The accuracy is used as the metric for SST-2, MNLI-m, MNLI-mm, QNLI and RTE. The F1 and accuracy are used for MRPC and QQP. The Pearson correlation and Spearman correlation are used for STS-B. Matthew’s correlation is used for CoLA. The results reported for the test set of GLUE are in the same format as on the official leaderboard. For the sake of comparison with (Sanh et al., 2019), on the development set of GLUE, the result of MNLI is an averag"
2020.emnlp-main.633,D16-1264,0,\N,Missing
2020.findings-emnlp.161,N19-1071,0,0.0751448,"ns are not explicitly modeled in our model and therefore our model is less dependent on sentence positions (as shown in experiments). There are also an interesting line of work on unsupervised abstractive summarization. Yang et al. 1785 (2020) pre-trains a seq2seq Transformer by predicting the first three sentences of news documents and then further tunes the model with semantic classification and denoising auto-encoding objectives. The model described in Wang and Lee (2018) utilizes seq2seq auto-encoding coupled with adversarial training and reinforcement learning. Fevry and Phang (2018) and Baziotis et al. (2019) focus on sentence summarization (i.e., compression). Chu and Liu (2019) proposes yet another denoising auto-encoding based model in multi-document summarization domain. However, the performance of these unsupervised models are still unsatisfactory compared to their extractive counterparts. Pre-training Pre-training methods in NLP learn to encode text by leveraging unlabeled text. Early work mostly concentrate on pre-training word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). Later, sentence encoder can also be pre-trained with language model (or masked l"
2020.findings-emnlp.161,Q17-1010,0,0.0323688,"ed with adversarial training and reinforcement learning. Fevry and Phang (2018) and Baziotis et al. (2019) focus on sentence summarization (i.e., compression). Chu and Liu (2019) proposes yet another denoising auto-encoding based model in multi-document summarization domain. However, the performance of these unsupervised models are still unsatisfactory compared to their extractive counterparts. Pre-training Pre-training methods in NLP learn to encode text by leveraging unlabeled text. Early work mostly concentrate on pre-training word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). Later, sentence encoder can also be pre-trained with language model (or masked language model) objectives (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). Zhang et al. (2019) propose a method to pre-train a hierarchical transformer encoder (document encoder) by predicting masked sentences in a document for supervised summarization, while we focus on unsupervised summarization. In our method, we also propose a new task (sentence shuffling) for pre-training hierarchical transformer encoders. Iter et al. (2020) propose a contrastive pre-training objective to p"
2020.findings-emnlp.161,P16-1046,0,0.0745449,"documents are paired with human written summaries. As mentioned earlier, extractive summarization aims to extract important sentences from documents and it is usually viewed as a (sentence) ranking problem by using scores from classifiers (Kupiec et al., 1995) or sequential labeling models (Conroy and O’leary, 2001). Summarization performance of this class of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin"
2020.findings-emnlp.161,N19-1423,0,0.461437,"n et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents. Zhang et al. (2019) propose an unsupervised"
2020.findings-emnlp.161,K18-1040,0,0.294146,"nd weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents. Zhang et al. (2019) propose an unsupervised method to pre-train a hierarchical transformer model (i.e., HIBERT) for document modeling. The 1784 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1784–1795 c November 16 - 20, 2020. 2020 A"
2020.findings-emnlp.161,W04-1017,0,0.0293505,"d summarization and pre-training. Supervised Summarization Most summarization models require supervision from labeled datasets, where documents are paired with human written summaries. As mentioned earlier, extractive summarization aims to extract important sentences from documents and it is usually viewed as a (sentence) ranking problem by using scores from classifiers (Kupiec et al., 1995) or sequential labeling models (Conroy and O’leary, 2001). Summarization performance of this class of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et a"
2020.findings-emnlp.161,D18-1443,0,0.0434639,"loglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained methods customized for both extractive (Zhang et al., 2019; Liu and Lapata, 2019) and abstractive (Dong et al., 2019; Lewis et al., 2019) summarization again advance the state-of-the-art in supervised summarization. Our model also leverages pre-trained methods and models, but it is unsupervised. Unsupervised Summarization Compared to supervised models, unsupervised models only need unlabeled documents during training. Most unsupervised ext"
2020.findings-emnlp.161,P16-1154,0,0.0227368,"of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained methods customized for both extractive (Zhang et al., 2019; Liu and Lapata, 2019) and abstractive (Dong et al., 2019; Lewis et al., 2019) summarization again advance the state-of-the-art in supervised summarization. Our model also leverages pre-trained methods and models, but it is unsupervised. Unsupervis"
2020.findings-emnlp.161,D13-1158,0,0.160864,"rst author’s internship at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured wit"
2020.findings-emnlp.161,2020.acl-main.439,0,0.0205955,"olov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). Later, sentence encoder can also be pre-trained with language model (or masked language model) objectives (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). Zhang et al. (2019) propose a method to pre-train a hierarchical transformer encoder (document encoder) by predicting masked sentences in a document for supervised summarization, while we focus on unsupervised summarization. In our method, we also propose a new task (sentence shuffling) for pre-training hierarchical transformer encoders. Iter et al. (2020) propose a contrastive pre-training objective to predict relative distances of surrounding sentences to the anchor sentence, while our sentence shuffling task predicts original positions of sentences from a shuffled docuemt. Besides, pre-training methods mentioned above focus on learning good word, sentence or document representations for downstream tasks, while our method focuses on learning sentence level attention distributions (i.e., sentence associations), which is shown in our experiments to be very helpful for unsupervised summarization. 3 Model In this section, we describe our unsuperv"
2020.findings-emnlp.161,2020.acl-main.703,0,0.0754839,"Missing"
2020.findings-emnlp.161,W04-1013,0,0.0376574,"ds as in (Zheng and Lapata, 2019) and finally retain 36,745 for training, 5,531 for validation and 4,375 for test. We segment sentences using the Stanford CoreNLP toolkit (Manning et al., 2014). Sentences are then tokenized with the UTF-8 based BPE tokenizer used in RoBERTa and GPT-2 (Radford et al., 2019) and the resulting vocabulary contains 50,265 subwords. During training, we only leverage articles in CNN/DM or NYT; while we do use both articles and summaries in validation sets to tune hyper-parameters of our models. We evaluated the quality of summaries from different models using ROUGE (Lin, 2004). We report the full length F1 based ROUGE-1, ROUGE-2, ROUGE-L on both CNN/DM and NYT datasets. These ROUGE scores are computed using the ROUGE-1.5.5.pl script4 . 4.2 Implementation Details The main building blocks of S TAS are Transformers (Vaswani et al., 2017). In the following, we describe the sizes of them using the number of layers L, the number of attention heads A, and the hidden size N . As in (Vaswani et al., 2017; Devlin et al., 2019), the hidden size of the feed-forward sublayer is always 4H. S TAS contains one hierarchical encoder (see Section 3.1) and two decoders, where they are"
2020.findings-emnlp.161,P02-1058,0,0.512317,"ely, human labeling for summarization task is expensive and therefore high ∗ Work done during the first author’s internship at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page"
2020.findings-emnlp.161,D19-1387,0,0.473379,"r hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained methods customized for both extractive (Zhang et al., 2019; Liu and Lapata, 2019) and abstractive (Dong et al., 2019; Lewis et al., 2019) summarization again advance the state-of-the-art in supervised summarization. Our model also leverages pre-trained methods and models, but it is unsupervised. Unsupervised Summarization Compared to supervised models, unsupervised models only need unlabeled documents during training. Most unsupervised extractive models are graph based (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). For example,"
2020.findings-emnlp.161,2021.ccl-1.108,0,0.104583,"Missing"
2020.findings-emnlp.161,P14-5010,0,0.00302378,"ticles for training, 13,368 for validation and 11,490 for test. Following Zheng and Lapata (2019), we adopted the splits widely used in abstractive summarization (Paulus et al., 2018) for the NYT dataset, which ranks articles by their publication date and used the first 589,284 for training, the next 32,736 for validation and the remaining 32,739 for test. Then, we filter out documents whose summaries are shorter than 50 words as in (Zheng and Lapata, 2019) and finally retain 36,745 for training, 5,531 for validation and 4,375 for test. We segment sentences using the Stanford CoreNLP toolkit (Manning et al., 2014). Sentences are then tokenized with the UTF-8 based BPE tokenizer used in RoBERTa and GPT-2 (Radford et al., 2019) and the resulting vocabulary contains 50,265 subwords. During training, we only leverage articles in CNN/DM or NYT; while we do use both articles and summaries in validation sets to tune hyper-parameters of our models. We evaluated the quality of summaries from different models using ROUGE (Lin, 2004). We report the full length F1 based ROUGE-1, ROUGE-2, ROUGE-L on both CNN/DM and NYT datasets. These ROUGE scores are computed using the ROUGE-1.5.5.pl script4 . 4.2 Implementation D"
2020.findings-emnlp.161,W04-3252,0,0.634116,"for summarization task is expensive and therefore high ∗ Work done during the first author’s internship at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sen"
2020.findings-emnlp.161,N18-1158,0,0.387507,"s. As mentioned earlier, extractive summarization aims to extract important sentences from documents and it is usually viewed as a (sentence) ranking problem by using scores from classifiers (Kupiec et al., 1995) or sequential labeling models (Conroy and O’leary, 2001). Summarization performance of this class of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained"
2020.findings-emnlp.161,N19-4009,0,0.0146229,"ock. LEAD-3 simply selects the first three sentences as the summary for each document. TEXTRANK (Mihalcea and Tarau, 2004) views a document as a graph with sentences as nodes and edge weights using the sentence similarities. It selects top sentences as summary w.r.t. PageRank (Page et al., 1999) scores. PACSUM (Zheng and Lapata, 2019) is yet another graphbased extractive model using BERT as sentence features. Sentences are ranked using centralities (sum of all out edge weights). They made the ranking criterion positional sensitive by forcing negative 6 We used gradient accumulation technique (Ott et al., 2019) to increase the actual batch size. edge weights for edges between the current sentence and its preceding sentences. Adv-RF (Wang and Lee, 2018) and TED (Yang et al., 2020) are all based on unsupervised seq2seq auto-encoding with additional objectives of adversarial training, reinforcement learning and seq2seq pre-training to predict leading sentences. PACSUM is based on the BERT (Devlin et al., 2019) initialization. RoBERTa (Liu et al., 2019), which extends BERT with better training strategies and more training data, outperforms BERT on many tasks. We therefore re-implemented PACSUM and exten"
2020.findings-emnlp.161,D15-1226,0,0.130693,"hip at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al.,"
2020.findings-emnlp.161,D14-1162,0,0.109516,"q2seq auto-encoding coupled with adversarial training and reinforcement learning. Fevry and Phang (2018) and Baziotis et al. (2019) focus on sentence summarization (i.e., compression). Chu and Liu (2019) proposes yet another denoising auto-encoding based model in multi-document summarization domain. However, the performance of these unsupervised models are still unsatisfactory compared to their extractive counterparts. Pre-training Pre-training methods in NLP learn to encode text by leveraging unlabeled text. Early work mostly concentrate on pre-training word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). Later, sentence encoder can also be pre-trained with language model (or masked language model) objectives (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). Zhang et al. (2019) propose a method to pre-train a hierarchical transformer encoder (document encoder) by predicting masked sentences in a document for supervised summarization, while we focus on unsupervised summarization. In our method, we also propose a new task (sentence shuffling) for pre-training hierarchical transformer encoders. Iter et al. (2020) propose a contrastive p"
2020.findings-emnlp.161,N18-1202,0,0.0116715,"ation (i.e., compression). Chu and Liu (2019) proposes yet another denoising auto-encoding based model in multi-document summarization domain. However, the performance of these unsupervised models are still unsatisfactory compared to their extractive counterparts. Pre-training Pre-training methods in NLP learn to encode text by leveraging unlabeled text. Early work mostly concentrate on pre-training word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). Later, sentence encoder can also be pre-trained with language model (or masked language model) objectives (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019). Zhang et al. (2019) propose a method to pre-train a hierarchical transformer encoder (document encoder) by predicting masked sentences in a document for supervised summarization, while we focus on unsupervised summarization. In our method, we also propose a new task (sentence shuffling) for pre-training hierarchical transformer encoders. Iter et al. (2020) propose a contrastive pre-training objective to predict relative distances of surrounding sentences to the anchor sentence, while our sentence shuffling task predicts original p"
2020.findings-emnlp.161,radev-etal-2004-mead,0,0.0947782,"k on supervised summarization, unsupervised summarization and pre-training. Supervised Summarization Most summarization models require supervision from labeled datasets, where documents are paired with human written summaries. As mentioned earlier, extractive summarization aims to extract important sentences from documents and it is usually viewed as a (sentence) ranking problem by using scores from classifiers (Kupiec et al., 1995) or sequential labeling models (Conroy and O’leary, 2001). Summarization performance of this class of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus"
2020.findings-emnlp.161,W00-0403,0,0.939861,"ummaries. Unfortunately, human labeling for summarization task is expensive and therefore high ∗ Work done during the first author’s internship at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then em"
2020.findings-emnlp.161,P17-1099,0,0.748564,"hen human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained methods customized for both extractive (Zhang et al., 2019; Liu and Lapata, 2019) and abstractive (Dong et al., 2019; Lewis et al., 2019) summarization again advance the state-of-the-art in supervised summarization. Our model also leverages pre-trained methods and models, but it is unsupervised. Unsupervised Summarization Compared to superv"
2020.findings-emnlp.161,2020.findings-emnlp.168,0,0.657384,"ce similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents. Zhang et al. (2019) propose an unsupervised method to pre-train a hierarchical transformer model (i.e., HIBERT) for document modeling. The 1784 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1784–1795 c November 16 - 20, 2020. 2020 Association for Computational Linguistic"
2020.findings-emnlp.161,D18-1088,1,0.882361,"r, extractive summarization aims to extract important sentences from documents and it is usually viewed as a (sentence) ranking problem by using scores from classifiers (Kupiec et al., 1995) or sequential labeling models (Conroy and O’leary, 2001). Summarization performance of this class of methods are greatly improved, when human engineered features (Radev et al., 2004; Nenkova et al., 2006; Filatova and Hatzivassiloglou, 2004) are replaced with convolutional neural networks (CNN) and long shortterm memory networks (LSTM) (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Abstractive summarization on the other hand can generate new words or phrases and are mostly based on sequence to sequence (seq2seq) learning (Bahdanau et al., 2015). To better fit in the summarization task, the original seq2seq model is extended with copy mechanism (Gu et al., 2016), coverage model (See et al., 2017), reinforcement learning (Paulus et al., 2018) as well as bottom-up attention (Gehrmann et al., 2018). Recently, pretrained transformers (Vaswani et al., 2017) achieve tremendous success in many NLP tasks (Devlin et al., 2019; Liu et al., 2019). Pre-trained methods customized fo"
2020.findings-emnlp.161,P19-1499,1,0.831509,"re measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents. Zhang et al. (2019) propose an unsupervised method to pre-train a hierarchical transformer model (i.e., HIBERT) for document modeling. The 1784 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1784–1795 c November 16 - 20, 2020. 2020 Association for Computational Linguistics hierarchical transformer has a token-level transformer to learn sentence representations and a sentence-level transformer to learn interactions between sentences with self-attention. In Zhang et al. (2019), HIBERT is applied to supervised extractive summarization. However, we believe that after pre-training HIBERT"
2020.findings-emnlp.161,P19-1628,0,0.34507,"; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents"
2020.findings-emnlp.161,D08-1079,0,0.902499,"high ∗ Work done during the first author’s internship at Microsoft Research Asia. quality large scale labeled summarization datasets are rear (Hermann et al., 2015) compared to growing web documents created everyday. It is also not possible to create summaries for documents in all text domains and styles. In this paper, we focus on unsupervised summarization, where we only need unlabeled documents during training. Many attempts for unsupervised summarization are extractive (Carbonell and Goldstein, 1998; Radev et al., 2000; Lin and Hovy, 2002; Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wan, 2008; Wan and Yang, 2008; Hirao et al., 2013; Parveen et al., 2015). The core problem is to identify salient sentences in a document. The most popular approaches among these work rank sentences in the document using graph based algorithms, where each node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in ("
2020.findings-emnlp.161,D18-1451,0,0.397504,"node is a sentence and weights of edges are measured by sentence similarities. Then a graph ranking method is employed to estimate sentence importance. For example, TextRank (Mihalcea and Tarau, 2004) utilizes word co-occurrence statistics to compute similarity and then employs PageRank (Page et al., 1997) to rank sentences. Sentence similarities in (Zheng and Lapata, 2019) are measured with BERT (Devlin et al., 2019) and sentences are sorted w.r.t. their centralities in a directed graph. Recently, there has been increasing interest in developing unsupervised abstractive summarization models (Wang and Lee, 2018; Fevry and Phang, 2018; Chu and Liu, 2019; Yang et al., 2020). These models are mostly based on sequence to sequence learning (Sutskever et al., 2014) and sequential denoising auto-encoding (Dai and Le, 2015). Unfortunately, there is no guarantee that summaries produced by these models are grammatical and consistent with facts described original documents. Zhang et al. (2019) propose an unsupervised method to pre-train a hierarchical transformer model (i.e., HIBERT) for document modeling. The 1784 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1784–1795 c Novembe"
2020.findings-emnlp.178,P11-1015,0,\N,Missing
2020.findings-emnlp.178,D17-1070,0,\N,Missing
2020.findings-emnlp.178,P17-1052,0,\N,Missing
2020.findings-emnlp.178,P19-1580,0,\N,Missing
2020.findings-emnlp.178,P16-1162,0,\N,Missing
2020.lrec-1.236,P17-4012,0,0.0342952,"Missing"
2021.acl-long.201,2020.acl-main.721,0,0.0204073,"dge of one document type cannot be easily transferred into another, so that these models often need to be re-trained once the document type is changed. Thereby the local invariance in general document layout (key-value pairs in a left-right layout, tables in a grid layout, etc.) cannot be fully exploited. To this end, the second direction relies on the deep fusion among textual, visual, and layout information from a great number of unlabeled documents in different domains, where pre-training techniques play an important role in learning the cross-modality interaction in an end-to-end fashion (Lockard et al., 2020; Xu et al., 2020). In this way, the pre-trained 2579 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2579–2591 August 1–6, 2021. ©2021 Association for Computational Linguistics models absorb cross-modal knowledge from different document types, where the local invariance among these layouts and styles is preserved. Furthermore, when the model needs to be transferred into another domain with different document formats, only a few labeled samples would be sufficient to fine-t"
2021.acl-long.201,N18-2074,0,0.0284803,"his paper follows the second direction, and we explore how to further improve the pre-training strategies for the VrDU tasks. In this paper, we present an improved version of LayoutLM (Xu et al., 2020), aka LayoutLMv2. Different from the vanilla LayoutLM model where visual embeddings are combined in the fine-tuning stage, we integrate the visual information in the pre-training stage in LayoutLMv2 by taking advantage of the Transformer architecture to learn the cross-modality interaction between visual and textual information. In addition, inspired by the 1-D relative position representations (Shaw et al., 2018; Raffel et al., 2020; Bao et al., 2020), we propose the spatial-aware self-attention mechanism for LayoutLMv2, which involves a 2-D relative position representation for token pairs. Different from the absolute 2-D position embeddings that LayoutLM uses to model the page layout, the relative position embeddings explicitly provide a broader view for the contextual spatial modeling. For the pre-training strategies, we use two new training objectives for LayoutLMv2 in addition to the masked visual-language modeling. The first is the proposed text-image alignment strategy, which aligns the text li"
2021.acl-long.201,2020.acl-main.580,0,0.256687,"y the style and format of each type as well as the document content. Therefore, to accurately recognize the text fields of interest, it is inevitable to take advantage of the cross-modality nature of visually-rich documents, where the textual, visual, and layout information should be jointly modeled and learned end-to-end in a single framework. The recent progress of VrDU lies primarily in two directions. The first direction is usually built on the shallow fusion between textual and visual/layout/style information (Yang et al., 2017; Liu et al., 2019; Sarkhel and Nandi, 2019; Yu et al., 2020; Majumder et al., 2020; Wei et al., 2020; Zhang et al., 2020). These approaches leverage the pre-trained NLP and CV models individually and combine the information from multiple modalities for supervised learning. Although good performance has been achieved, the domain knowledge of one document type cannot be easily transferred into another, so that these models often need to be re-trained once the document type is changed. Thereby the local invariance in general document layout (key-value pairs in a left-right layout, tables in a grid layout, etc.) cannot be fully exploited. To this end, the second direction relie"
2021.acl-long.201,D16-1264,0,0.127266,"Missing"
2021.acl-long.201,D19-1514,0,0.0179133,"ition representation for token pairs. Different from the absolute 2-D position embeddings that LayoutLM uses to model the page layout, the relative position embeddings explicitly provide a broader view for the contextual spatial modeling. For the pre-training strategies, we use two new training objectives for LayoutLMv2 in addition to the masked visual-language modeling. The first is the proposed text-image alignment strategy, which aligns the text lines and the corresponding image regions. The second is the text-image matching strategy popular in previous vision-language pre-training models (Tan and Bansal, 2019; Lu et al., 2019; Su et al., 2020; Chen et al., 2020; Sun et al., 2019), where the model learns whether the document image and textual content are correlated. We select six publicly available benchmark datasets as the downstream tasks to evaluate the performance of the pre-trained LayoutLMv2 model, which are the FUNSD dataset (Jaume et al., 2019) for form understanding, the CORD dataset (Park et al., 2019) and the SROIE dataset (Huang et al., 2019) for receipt understanding, the Kleister-NDA dataset (Grali´nski et al., 2020) for long document understanding with a complex layout, the RVL-CDIP"
2021.acl-long.264,2020.acl-main.421,0,0.0139564,"ge of example consistency R1 . Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, a"
2021.acl-long.264,2020.tacl-1.30,0,0.0152671,"ible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tuned on the concatenation of English t"
2021.acl-long.264,2020.acl-main.747,0,0.0682084,"Missing"
2021.acl-long.264,D18-1269,0,0.0258158,"do not change nword . Thus the three data augmentation strategies will not affect the usage of example consistency R1 . Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on differen"
2021.acl-long.264,2020.acl-main.536,0,0.184978,"arization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method1 significantly improves crosslingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling. 1 Introduction Pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020) have shown great transferability across languages. By fine-tuning on labeled data in a source language, the models can generalize to other target languages, even without any additional training. Such generalization ability reduces the required annotation efforts, which is prohibitively expensive for low-resource languages. Recent work has demonstrated that data augmentation is helpful for cross-lingual transfer, e.g., translating source language training data into target languages (Singh et al., 2019), and generating codeswitch data by randomly replacing input words in the"
2021.acl-long.264,N19-1423,0,0.032175,"Missing"
2021.acl-long.264,E14-1049,0,0.0225658,"ary to the first strand. We focus on the cross-lingual setting, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source"
2021.acl-long.264,2020.acl-main.627,0,0.19623,"s on seven datasets. Experimental results show that our method outperforms conventional fine-tuning with data augmentation. We also demonstrate that X T UNE is flexible to be plugged in various 3403 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3403–3417 August 1–6, 2021. ©2021 Association for Computational Linguistics tasks, such as classification, span extraction, and sequence labeling. We summarize our contributions as follows: the target language. Besides, Qin et al. (2020) finetuned models on multilingual code-switch data, which achieves considerable improvements. • We propose X T UNE, a cross-lingual finetuning method to better utilize data augmentations based on consistency regularization. Consistency Regularization One strand of work in consistency regularization focused on regularizing model predictions to be invariant to small perturbations on image data. The small perturbations can be random noise (Zheng et al., 2016), adversarial noise (Miyato et al., 2019; Carmon et al., 2019) and various data augmentation approaches (Hu et al., 2017; Ye et al., 2019; X"
2021.acl-long.264,P15-1119,1,0.817926,"We focus on the cross-lingual setting, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source language training"
2021.acl-long.264,2020.acl-main.197,0,0.0945311,"Missing"
2021.acl-long.264,P17-1178,0,0.028767,"translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tuned on the concatenation of English training data and its translated data on all target languages"
2021.acl-long.264,2020.acl-main.653,0,0.0247876,". Although word alignment is a possible solution to map the predicted label distributions between translation pairs, the word alignment process will introduce more noise. Therefore, we do not employ machine translation as data augmentation for the example consistency R1 . Experiments 4.1 Experiment Setup Datasets For our experiments, we select three types of cross-lingual understanding tasks from XTREME benchmark (Hu et al., 2020), including two classification datasets: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019), three span extraction datasets: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark et al., 2020), and two sequence labeling datasets: NER (Pan et al., 2017), POS (Nivre et al., 2018). The statistics of the datasets are shown in the supplementary document. Fine-Tuning Settings We consider two typical fine-tuning settings from Conneau et al. (2020a) and Hu et al. (2020) in our experiments, which are (1) cross-lingual transfer: the models are finetuned on English training data without translation available, and directly evaluated on different target languages; (2) translate-train-all: translationbased augmentation is available, and the models are fine-tune"
2021.acl-long.264,2020.acl-main.170,0,0.0340809,"(DA , θ, θ∗ ) where λ1 and λ2 are the corresponding weights of two regularization methods. Notice that the data augmentation strategies A, A0 , and A∗ can be either different or the same, which are tuned as hyper-parameters. 3.2 Gaussian Noise Data Augmentation We consider four types of data augmentation strategies in this work, which are shown in Figure 2. We aim to study the impact of different data augmentation strategies on cross-lingual transferability. 3.2.1 Subword Sampling Representing a sentence in different subword sequences can be viewed as a data augmentation strategy (Kudo, 2018; Provilkov et al., 2020). We utilize XLM-R (Conneau et al., 2020a) as our pre-trained Anchor points have been shown useful to improve cross-lingual transferability. Conneau et al. (2020b) analyzed the impact of anchor points in pre-training cross-lingual language models. Following Qin et al. (2020), we generate code-switch data in multiple languages as data augmentation. We randomly select words in the original text in the source language and replace them with target language words in the bilingual dictionaries to obtain code-switch data. Intuitively, this type of data augmentation explicitly helps pre-trained cross-"
2021.acl-long.264,D19-1575,1,0.837666,"ing, where consistency regularization has not been fully explored. • We study four types of data augmentations that can be easily plugged into cross-lingual fine-tuning. • We give instructions on how to apply X T UNE to various downstream tasks, such as classification, span extraction, and sequence labeling. • We conduct extensive experiments to show that X T UNE consistently improves the performance of cross-lingual fine-tuning. 2 Related Work Cross-Lingual Transfer Besides learning crosslingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Guo et al., 2015; Xu et al., 2018; Wang et al., 2019), most recent work of cross-lingual transfer is based on pre-trained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020a; Chi et al., 2020). These models generate multilingual contextualized word representations for different languages with a shared encoder and show promising cross-lingual transferability. Cross-Lingual Data Augmentation Machine translation has been successfully applied to the cross-lingual scenario as data augmentation. A common way to use machine translation is to finetune models on both source language training data and translated data in all targ"
2021.acl-long.265,2020.acl-main.421,0,0.382399,"Missing"
2021.acl-long.265,J93-2003,0,0.168381,"Missing"
2021.acl-long.265,2021.emnlp-main.125,1,0.795284,"Missing"
2021.acl-long.265,2021.naacl-main.280,1,0.918277,"most applications and resources are still English-centric, making non-English users hard to access. Therefore, it is essential to build cross-lingual transferable models that can learn from the training data in highresource languages and generalize on low-resource languages. Recently, pretrained cross-lingual language models have shown their effectiveness for cross-lingual transfer. By pre-training on monolingual text and parallel sentences, the models provide significant improvements on a wide range of crosslingual end tasks (Conneau and Lample, 2019; Conneau et al., 2020; Liu et al., 2020; Chi et al., 2021b). Cross-lingual language model pre-training is typically achieved by learning various pretext tasks on ∗ Contribution during internship at Microsoft Research. monolingual and parallel corpora. By simply learning masked language modeling (MLM; Devlin et al. 2019) on monolingual text of multiple languages, the models surprisingly achieve competitive results on cross-lingual tasks (Wu and Dredze, 2019; K et al., 2020). Besides, several pretext tasks are proposed to utilize parallel corpora to learn better sentence-level cross-lingual representations (Conneau and Lample, 2019; Chi et al., 2021b;"
2021.acl-long.265,2020.tacl-1.30,0,0.109618,"Missing"
2021.acl-long.265,2020.acl-main.747,0,0.181358,"Missing"
2021.acl-long.265,2020.acl-main.653,0,0.260231,"Missing"
2021.acl-long.265,P19-1124,0,0.0233816,"plicit alignment objective in large-scale pre-training can provide consistent improvements over baseline models. Word alignment The IBM models (Brown et al., 1993) are statistical models for modeling the translation process that can extract word alignments between sentence pairs. A large number of word alignment models are based on the IBM models (Och and Ney, 2003; Mermer and Sarac¸lar, ¨ 2011; Dyer et al., 2013; Ostling and Tiedemann, 2016). Recent studies have shown that word alignments can be extracted from neural machine translation models (Ghader and Monz, 2017; Koehn and Knowles, 2017; Li et al., 2019) or from pretrained cross-lingual LMs (Jalili Sabet et al., 2020; Nagata et al., 2020). 3 Method Figure 1 illustrates an overview of our method for pre-training our cross-lingual LM, which is called XLM-A LIGN. XLM-A LIGN is pretrained in an expectation-maximization manner with two alternating steps, which are word alignment selflabeling and denoising word alignment. We first formulate word alignment as an optimal transport problem, and self-label word alignments of the input translation pair on-the-fly. Then, we update the model parameters with the denoising word alignment task, where the mod"
2021.acl-long.265,P19-1015,0,0.133748,"Missing"
2021.acl-long.348,P18-1073,0,0.162705,"g steps of pre-training encoder and decoder are separated, therefore the training samples of them are not necesarrily the same. (In the figure, the training sample for pre-training the encoder is x1 = x11 x21 ..x61 ) and the training sample for pre-training the decoder is x2 = x12 x22 ..x62 ). For MT fine-tuning, we use the parallel training sample {x1 , y1 } from the parallel corpus or generated from back-translation. connected for MT fine-tuning. We propose two types of semantic interfaces, namely CL-SemFace and VQ-SemFace. The former takes the trained unsupervised cross-lingual embeddings (Artetxe et al., 2018) as the interface for encoder and decoder pretraining. Inspired by the success of neural discrete representation learning (Van Den Oord et al., 2017), the latter uses language-independent vector quantized (VQ) embeddings (semantic unites) as the interface to map encoder outputs and decoder inputs into the shared VQ space. Experiments conducted on both supervised and unsupervised translation tasks demonstrate that SemFace effectively connects the pre-trained encoder and decoder, and achieves a significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively. Our contributions ar"
2021.acl-long.348,N19-1423,0,0.0204389,"the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages: pre-training an encoder and a decoder separately with a large monolingual corpus in a self-supervised manner, and then fine-tuning on specific NMT tasks (Lample and Conneau, 2019). The above method essentially pre-trains a BERTlike (Devlin et al., 2019) Transformer encoder, and uses it to initialize both the encoder and decoder. Although it shows promising results, pre-training decoder benefits little in their results. The potential reason is that the cross-attention between the encoder and decoder is not pre-trained, which is randomly initialized when they are connected for fine-tuning, resulting in a lack of semantic interfaces between the pre-trained encoder and decoder. Another line of work attempts to pre-train a sequence-to-sequence model directly, e.g., MASS (Song et al., 2019) and BART (Lewis et al., 2020). But these methods usually"
2021.acl-long.348,2020.acl-main.703,0,0.419451,"d decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models. 1 Introduction In recent years, pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; Raffel et al., 2020) significantly boost the performances of various natural language processing (NLP) tasks, receiving extensive attention in NLP communities. Following the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages: pre-training an encoder and a decoder separately with a large monolingual corpus in a self-supervised manner, and then fine-tuning on specific NMT tasks (Lample and Conneau, 2019). The above method essentially pre-trains a BERTlike (Devlin et al., 2019) Transformer encoder, and uses it to initialize both the encoder and decoder. Although it shows promising results, pre-trainin"
2021.acl-long.348,2020.emnlp-main.210,0,0.0184456,"wis et al., 2020) adopted a similar framework and trained the model as a denoising auto-encoder. mBART (Liu et al., 2020) trained BART model on large-scale monolingual corpora in many languages. Although the above work can pre-train the cross-attention of decoder, they are learned on monolingual denoising auto-encoding and cannot learn the corss-lingual transformation between source and target languages. There is also some work trying to explicitly introduce cross-lingual information in a code-switch way during the sequence-to-sequence pre-training, such as CSP (Yang et al., 2020b) and mRASP (Lin et al., 2020). However, their methods need a lexicon or phrase translation table, which is inferred from unsupervised cross-lingual embeddings. Therefore, they depend on the quality of the dictionary. The most similar work to ours is probably the one of DALL·E and CLIP (Radford et al., 2020). DALL·E is a transformer language model that receives both the text and the image as a single stream of data. The core idea is to define the cross-modality interface of image and text, which can generate images from text descriptions. In this paper, to address the above limitations of pretraining methods for NMT, we at"
2021.acl-long.348,2020.tacl-1.47,0,0.349601,"s significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models. 1 Introduction In recent years, pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; Raffel et al., 2020) significantly boost the performances of various natural language processing (NLP) tasks, receiving extensive attention in NLP communities. Following the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages: pre-training an encoder and a decoder separately with a large monolingual corpus in a self-supervised manner, and then fine-tuning on specific NMT tasks (Lample and Conneau, 2019). The above method essentially pre-trains a BERTlike (Devlin et al., 2019) Transformer encoder, and uses it to initialize both the encoder and decoder. Although it shows promising results, pre-training decoder benefits li"
2021.acl-long.348,N18-1202,0,0.11426,"eddings as an interface, and VQ-SemFace which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space. We conduct massive experiments on six supervised translation pairs and three unsupervised pairs. Experimental results demonstrate that our proposed SemFace can effectively connect the pre-trained encoder and decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models. 1 Introduction In recent years, pre-trained language models (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019; Raffel et al., 2020) significantly boost the performances of various natural language processing (NLP) tasks, receiving extensive attention in NLP communities. Following the idea of unsupervised pre-training methods in the NLP area, several approaches (Lample and Conneau, 2019; Zhu et al., 2020; Lewis et al., 2020; ∗ Liu et al., 2020) have been proposed to improve neural machine translation (NMT) models with pretraining by leveraging the large-scale monolingual corpora. The typical training process usually consists of two stages:"
2021.acl-long.348,D19-1071,1,0.849776,"(Devlin et al., 2018). The early pre-training techniques mainly focused on the natural language understanding tasks such as the GLUE benchmark (Wang et al., 2018) , and later it was gradually extended to the natural language generation tasks, e.g., NMT. Recently, a prominent line of work has been proposed to improve NMT with pre-training. These techniques can be broadly classified into two categories. The first category usually uses pre-trained models as feature extractors of a source language, or initializes the encoder and decoder with pretrained models separately (Lample and Conneau, 2019; Ren et al., 2019; Yang et al., 2020a; Zhu et al., 2020). For example, Lample and Conneau (2019) proposed a cross-lingual language model with a supervised translation language modeling objective, and used MLM or CLM to pre-train the encoder and decoder of NMT. However, the combined encoder-decoder model, where the crossattention is randomly initialized, often does not work well because of the lack of semantic interfaces between the pre-trained encoder and decoder. There is also some work trying to leverage BERTlike pre-trained models for MT with an adapter (Guo et al., 2020) or an APT framework (Weng et al., 2"
2021.acl-long.348,W18-5446,0,0.0462228,"Missing"
2021.acl-long.348,2020.emnlp-main.208,0,0.0575371,"18). The early pre-training techniques mainly focused on the natural language understanding tasks such as the GLUE benchmark (Wang et al., 2018) , and later it was gradually extended to the natural language generation tasks, e.g., NMT. Recently, a prominent line of work has been proposed to improve NMT with pre-training. These techniques can be broadly classified into two categories. The first category usually uses pre-trained models as feature extractors of a source language, or initializes the encoder and decoder with pretrained models separately (Lample and Conneau, 2019; Ren et al., 2019; Yang et al., 2020a; Zhu et al., 2020). For example, Lample and Conneau (2019) proposed a cross-lingual language model with a supervised translation language modeling objective, and used MLM or CLM to pre-train the encoder and decoder of NMT. However, the combined encoder-decoder model, where the crossattention is randomly initialized, often does not work well because of the lack of semantic interfaces between the pre-trained encoder and decoder. There is also some work trying to leverage BERTlike pre-trained models for MT with an adapter (Guo et al., 2020) or an APT framework (Weng et al., 2020). The former de"
2021.acl-long.462,D19-1435,0,0.0121075,"ang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However, as they rely on many languagedependent edit operations such as the conversion of singular nouns to plurals, it is difficult for them to adapt to other languages. LaserTagger (Malmi et al., 2019) uses the similar method but it is datadriven and language-independent by learning operations from training data. However, its performance is not so desirable as its seq2seq counterpart despite its high efficiency. The only two previous effi"
2021.acl-long.462,W19-4406,0,0.0119897,"ational cost for decoding, we propose to use a shallow decoder, which has proven to be an effective strategy (Kasai et al., 2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC. By combining aggressive decoding with the shallow decoder, we are able to further improve the inference efficiency. 4 Experiments 4.1 Data and Model Configuration We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official e"
2021.acl-long.462,2020.emnlp-main.581,1,0.917671,"nly does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL14 and 72.9 F0.5 in the BEA-19 test set with an almost 10× online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at https://github.com/AutoTemp/ Shallow-Aggressive-Decoding. 1 Introduction The Transformer (Vaswani et al., 2017) has become the most popular model for Grammatical Error Correction (GEC). In practice, however, the sequenceto-sequence (seq2seq) approach has been blamed recently (Chen et al., 2020; Stahlberg and Kumar, ∗ This work was done during the author’s internship at MSR Asia. Contact person: Tao Ge (tage@microsoft.com) † Co-first authors with equal contributions 2020; Omelianchuk et al., 2020) for its poor inference efficiency in modern writing assistance applications (e.g., Microsoft Office Word1 , Google Docs2 and Grammarly3 ) where a GEC model usually performs online inference, instead of batch inference, for proactively and incrementally checking a user’s latest completed sentence to offer instantaneous feedback. To better exploit the Transformer for instantaneous GEC in pra"
2021.acl-long.462,N12-1067,0,0.0292118,"ricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sentences as our validation set. We also test our approach on NLPCC-18 Chinese GEC shared task (Zhao et al., 2018), following their training5 and evaluation setting, to verify the effectiveness of our approach in other languages. To compare with the state-of-the-art approaches in English GEC that pretrain with synthetic data, 4 https://github.com/nusnlp/m2scorer Following Chen et al. (2020), we sample 5,000 training instances as the validation set. 5940"
2021.acl-long.462,W13-1703,0,0.0227589,"ctive strategy (Kasai et al., 2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC. By combining aggressive decoding with the shallow decoder, we are able to further improve the inference efficiency. 4 Experiments 4.1 Data and Model Configuration We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sent"
2021.acl-long.462,P18-1097,1,0.803731,"her languages (e.g., Chinese). As shown in Table 6, our approach consistently performs well in Chinese GEC, showing an around 12.0× online inference speedup over the Transformer-big baseline with comparable performance. 5 Related Work The state-of-the-art of GEC has been significantly advanced owing to the tremendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on ed"
2021.acl-long.462,D19-1633,0,0.0193807,"g and Kumar (2020) which uses span-based edit operations to correct sentences to save the time for copying unchanged tokens, and Chen et al. (2020) which first identifies incorrect spans with a tagging model then only corrects these spans with a generator. However, all the approaches have to extract edit operations and even conduct token alignment in advance from the error-corrected sentence pairs for training the model. In contrast, our proposed shallow aggressive decoding tries to accelerate the model inference through parallel autoregressive decoding which is related to some previous work (Ghazvininejad et al., 2019; Stern et al., 2018) in neural machine translation (NMT), and the imbalanced encoder-decoder architecture which 5944 is recently explored by Kasai et al. (2020) and Li et al. (2021) for NMT. Not only is our approach language-independent, efficient and guarantees that its predictions are exactly the same with greedy decoding, but also does not need to change the way of training, making it much easier to train without so complicated data preparation as in the edit operation based approaches. 6 Conclusion and Future Work In this paper, we propose Shallow Aggressive Decoding (SAD) to accelerate o"
2021.acl-long.462,W19-4427,0,0.0784347,"eam search (beam=5), and generates exactly the same predictions as greedy decoding, as discussed in Section 3.1.2. Since greedy decoding can achieve comparable overall performance (i.e., F0.5 ) with beam search while it tends 6 Our implementation of greedy decoding is simplified for higher efficiency (1.3× ∼ 1.4× speedup over beam=5) than the implementation of beam=1 decoding in fairseq (around 1.1× speedup over beam=5). 7 https://github.com/pytorch/fairseq 35 30 25 Speedup we also synthesize 300M error-corrected sentence pairs for pretraining the English GEC model following the approaches of Grundkiewicz et al. (2019) and Zhang et al. (2019). Note that in the following evaluation sections, the models evaluated are by default trained without the synthetic data unless they are explicitly mentioned. We use the most popular GEC model architecture – Transformer (big) model (Vaswani et al., 2017) as our baseline model which has a 6-layer encoder and 6-layer decoder with 1,024 hidden units. We train the English GEC model using an encoder-decoder shared vocabulary of 32K Byte Pair Encoding (Sennrich et al., 2016) tokens and train the Chinese GEC model with 8.4K Chinese characters. We include more training details"
2021.acl-long.462,2021.findings-acl.11,0,0.083788,"Missing"
2021.acl-long.462,2020.acl-main.391,0,0.0158019,"emendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However, as they rely on many languagedependent edit operations such as the conversion of singular nouns t"
2021.acl-long.462,2020.acl-main.703,0,0.0829372,"Missing"
2021.acl-long.462,N19-1333,0,0.0138028,"with comparable performance. 5 Related Work The state-of-the-art of GEC has been significantly advanced owing to the tremendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operatio"
2021.acl-long.462,2021.ccl-1.108,0,0.061983,"Missing"
2021.acl-long.462,D19-1510,0,0.0157882,"the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However, as they rely on many languagedependent edit operations such as the conversion of singular nouns to plurals, it is difficult for them to adapt to other languages. LaserTagger (Malmi et al., 2019) uses the similar method but it is datadriven and language-independent by learning operations from training data. However, its performance is not so desirable as its seq2seq counterpart despite its high efficiency. The only two previous efficient approaches that are both languageindependent and good-performing are Stahlberg and Kumar (2020) which uses span-based edit operations to correct sentences to save the time for copying unchanged tokens, and Chen et al. (2020) which first identifies incorrect spans with a tagging model then only corrects these spans with a generator. However, all the ap"
2021.acl-long.462,W13-3601,0,0.0149261,"l., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sentences as our validation set. We also test our approach on NLPCC-18 Chinese GEC shared task (Zhao et al., 2018), following their training5 and evaluation setting, to verify the effectiveness of our approach in other languages. To compare with the state-of-the-art approaches in English GEC that pretrain with synthetic data, 4 https://github.com/nusnlp/m2scorer Following Chen et al. (2020), we sample 5,000 training instances as the validation set. 5940 5 Model Transformer-big (beam=5) Transformer-big (greedy) Transformer-big (aggressive) Transformer-big (beam=5) Transform"
2021.acl-long.462,2020.bea-1.16,0,0.0686156,"Missing"
2021.acl-long.462,P16-1162,0,0.0282237,"M error-corrected sentence pairs for pretraining the English GEC model following the approaches of Grundkiewicz et al. (2019) and Zhang et al. (2019). Note that in the following evaluation sections, the models evaluated are by default trained without the synthetic data unless they are explicitly mentioned. We use the most popular GEC model architecture – Transformer (big) model (Vaswani et al., 2017) as our baseline model which has a 6-layer encoder and 6-layer decoder with 1,024 hidden units. We train the English GEC model using an encoder-decoder shared vocabulary of 32K Byte Pair Encoding (Sennrich et al., 2016) tokens and train the Chinese GEC model with 8.4K Chinese characters. We include more training details in the supplementary notes. For inference, we use greedy decoding6 by default. All the efficiency evaluations are conducted in the online inference setting (i.e., batch size=1) as we focus on instantaneous GEC. We perform model inference with fairseq7 implementation using Pytorch 1.5.1 with 1 Nvidia Tesla V100-PCIe of 16GB GPU memory under CUDA 10.2. 20 15 10 5 0 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 Edit Ratio Figure 2: The speedup (over greedy decoding) distribution of all the 1"
2021.acl-long.462,2020.emnlp-main.418,0,0.0927036,"F0.5 Speedup 17.2 29.6 1.0× 15.0 22.0 3.1× 10.5 19.9 38.0× 14.5 28.4 2.7× 20.5 29.4 12.0× Table 6: The performance and online inference efficiency evaluation for the language-independent efficient GEC models in the NLPCC-18 Chinese GEC benchmark. els (e.g., RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019)) with its multi-stage training strategy. Following GECToR’s recipe, we leverage the pretrained model BART (Lewis et al., 2019) to initialize a 12+2 model which proves to work well in NMT (Li et al., 2021) despite more parameters, and apply the multi-stage fine-tuning strategy used in Stahlberg and Kumar (2020). The final single model11 with aggressive decoding achieves the state-of-the-art result – 66.4 F0.5 in the CoNLL-14 test set with a 9.6× speedup over the Transformerbig baseline. Unlike GECToR and PIE that are difficult to adapt to other languages despite their competitive speed because they are specially designed for English GEC with many manually designed languagespecific operations like the transformation of verb forms (e.g., VBD→VBZ) and prepositions (e.g., in→at), our approach is data-driven without depending on language-specific features, and thus can be easily adapted to other language"
2021.acl-long.462,2020.coling-main.200,0,0.0168833,"The state-of-the-art of GEC has been significantly advanced owing to the tremendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However, as they rely on ma"
2021.acl-long.462,I11-1017,0,0.0389356,"which has proven to be an effective strategy (Kasai et al., 2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC. By combining aggressive decoding with the shallow decoder, we are able to further improve the inference efficiency. 4 Experiments 4.1 Data and Model Configuration We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al."
2021.acl-long.462,W14-1701,0,0.0175463,"ncy. 4 Experiments 4.1 Data and Model Configuration We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sentences as our validation set. We also test our approach on NLPCC-18 Chinese GEC shared task (Zhao et al., 2018), following their training5 and evaluation setting, to verify the effectiveness of our approach in other languages. To compare with the state-of-the-art approaches in English GEC that pretrain with synthetic data,"
2021.acl-long.462,P11-1019,0,0.0203876,"2020; Li et al., 2021) in neural machine translation (NMT), instead of using the Transformer with balanced encoder-decoder depth as the previous state-of-the-art Transformer models in GEC. By combining aggressive decoding with the shallow decoder, we are able to further improve the inference efficiency. 4 Experiments 4.1 Data and Model Configuration We follow recent work in English GEC to conduct experiments in the restricted training setting of BEA-2019 GEC shared task (Bryant et al., 2019): We use Lang-8 Corpus of Learner English (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS (Granger; Bryant et al., 2019) as our GEC training data. For facilitating fair comparison in the efficiency evaluation, we follow the previous studies (Omelianchuk et al., 2020; Chen et al., 2020) which conduct GEC efficiency evaluation to use CoNLL-2014 (Ng et al., 2014) dataset that contains 1,312 sentences as our main test set, and evaluate the speedup as well as MaxMatch (Dahlmeier and Ng, 2012) precision, recall and F0.5 using their official evaluation scripts4 . For validation, we use CoNLL-2013 (Ng et al., 2013) that contains 1,381 sentences as our validation set. We al"
2021.acl-long.462,2020.findings-emnlp.30,1,0.708768,"nce. 5 Related Work The state-of-the-art of GEC has been significantly advanced owing to the tremendous success of seq2seq learning (Sutskever et al., 2014) and the Transformer (Vaswani et al., 2017). Most recent work on GEC focuses on improving the performance of the Transformer-based GEC models. However, except for the approaches that add synthetic erroneous data for pretraining (Ge et al., 2018a; Grundkiewicz et al., 2019; Zhang et al., 11 The same model checkpoint also achieves the state-ofthe-art result – 72.9 F0.5 with a 9.3× speedup in the BEA-19 test set. 2019; Lichtarge et al., 2019; Zhou et al., 2020; Wan et al., 2020), most methods that improve performance (Ge et al., 2018b; Kaneko et al., 2020) introduce additional computational cost and thus slow down inference despite the performance improvement. To make the Transformer-based GEC model more efficient during inference for practical application scenarios, some recent studies have started exploring the approaches based on edit operations. Among them, PIE (Awasthi et al., 2019) and GECToR (Omelianchuk et al., 2020) propose to accelerate the inference by simplifying GEC from sequence generation to iterative edit operation tagging. However,"
2021.acl-long.477,D13-1160,0,0.0653481,"is a question answer dataset where the queswe add one hard negative example by randomly tions were real Google search queries and answers sampling from top retrieval results using a BM25 were text spans of Wikipedia articles manually seretriever. More elaborate methods of finding hard lected by annotators. examples such as Xiong et al. (2020) and Ding TriviaQA (Joshi et al., 2017) is a set of trivia et al. (2020) can also be included, but we leave it questions with their answers. We use the unfiltered to future work. version of TriviaQA. 4.6 Removing false negative examples WebQuestions (WQ) (Berant et al., 2013) is a collection of questions from Google Suggest API False negative examples are passages that can with answers from Freebase. match the given question but are falsely labeled ˇ CuratedTREC (TREC) (Baudiˇs and Sediv´ y, as negative examples. In xMoCo formulation, false 2015) composes of questions from both TREC QA negatives can arise if a previous encoded passage tracks and Web sources. p in the queue can answer current question q. It SQuAD v1.1 (Rajpurkar et al., 2016) is original can happen if the some questions share the same used as a benchmark for reading comprehension. passage as answer"
2021.acl-long.477,P17-1171,0,0.0932928,"above works, the questions and passages in our work are not interchangeable and require different encoders, which renders the original MoCo not directly applicable. • We demonstrate the effectiveness of xMoCo in learning a dense passage retrieval model for various open domain question answering datasets. 2 Related Work There are mainly two threads of research work related to this paper. 2.1 Passage Retrieval for QA Retrieving relevance passages is usually the first step in the most QA pipelines. Traditional passage retriever utilizes the keyword-matching based methods such as TF-IDF and BM25 (Chen et al., 2017). Keyword-based approach enjoys its simplicity, but often suffers from term mismatch between questions and passages. Such term mismatch problem can be reduced by either query expansion (Carpineto and Romano, 2012) or appending generated questions to the passages (Nogueira et al., 2019). Dense passage retrieval usually involves learning a dual-encoder to map both questions and passages into dense vectors, where their innerproduct denotes their relevance (Lee et al., 2019). 2.2 3 3.1 Momentum Contrastive Learning Background Task description In this paper, we deal with the task of retrieving rele"
2021.acl-long.477,P17-1147,0,0.0215735,"lits. In this work, we only implement a simple Here is a brief description of the datasets. method of generating hard examples following Natural Questions (NQ) (Kwiatkowski et al., Karpukhin et al. (2020): for each positive pair, 2019) is a question answer dataset where the queswe add one hard negative example by randomly tions were real Google search queries and answers sampling from top retrieval results using a BM25 were text spans of Wikipedia articles manually seretriever. More elaborate methods of finding hard lected by annotators. examples such as Xiong et al. (2020) and Ding TriviaQA (Joshi et al., 2017) is a set of trivia et al. (2020) can also be included, but we leave it questions with their answers. We use the unfiltered to future work. version of TriviaQA. 4.6 Removing false negative examples WebQuestions (WQ) (Berant et al., 2013) is a collection of questions from Google Suggest API False negative examples are passages that can with answers from Freebase. match the given question but are falsely labeled ˇ CuratedTREC (TREC) (Baudiˇs and Sediv´ y, as negative examples. In xMoCo formulation, false 2015) composes of questions from both TREC QA negatives can arise if a previous encoded pass"
2021.acl-long.477,2020.emnlp-main.550,0,0.100511,"e retrieved passages. As recent advancement in machine reading comprehension (MRC) has demonstrated excellent results of finding answers given the correct passages (Wang et al., 2017), the performance of open-domain QA systems now relies heavily on the relevance of the selected passages of the retriever. Traditionally the retrievers usually utilize sparse keywords matching such as TF-IDF or BM25 (Robertson and Zaragoza, 2009), which can be efficiently implemented with an inverted index. With the popularization of neural network in NLP, the dense passage retrieval approach has gained traction (Karpukhin et al., 2020). In this approach, a dual-encoder model is learned to encode questions and passages into a dense, low-dimensional vector space, where the relevance between questions and passages can be calculated by the inner product of their respective vectors. As the vectors of all passages can be pre-computed and indexed, dense passage retrieval can also be done efficiently with vector space search methods during inference time (Shrivastava and Li, 2014). Dense retrieval models are usually trained with contrastive objectives between positive and negative question-passage pairs. As the positive pairs are o"
2021.acl-long.477,Q19-1026,0,0.0222756,"Missing"
2021.acl-long.477,P19-1612,0,0.0583452,"n the most QA pipelines. Traditional passage retriever utilizes the keyword-matching based methods such as TF-IDF and BM25 (Chen et al., 2017). Keyword-based approach enjoys its simplicity, but often suffers from term mismatch between questions and passages. Such term mismatch problem can be reduced by either query expansion (Carpineto and Romano, 2012) or appending generated questions to the passages (Nogueira et al., 2019). Dense passage retrieval usually involves learning a dual-encoder to map both questions and passages into dense vectors, where their innerproduct denotes their relevance (Lee et al., 2019). 2.2 3 3.1 Momentum Contrastive Learning Background Task description In this paper, we deal with the task of retrieving relevant passages given certain natural language questions. Given a question q and a collection of N passages {q1 , q2 , . . . , qN }, a passage retriever aims to return a list of passages {qi1 , qi2 , . . . , qiM } 6121 ranked by their relevance to q. While the number of retrieved passages M is usually in the magnitude of hundreds or thousands, the number of total passages N is typically very large, possibly in millions or billions. Such practical concern places constraints"
2021.acl-long.477,D16-1264,0,0.0485314,"eir answers. We use the unfiltered to future work. version of TriviaQA. 4.6 Removing false negative examples WebQuestions (WQ) (Berant et al., 2013) is a collection of questions from Google Suggest API False negative examples are passages that can with answers from Freebase. match the given question but are falsely labeled ˇ CuratedTREC (TREC) (Baudiˇs and Sediv´ y, as negative examples. In xMoCo formulation, false 2015) composes of questions from both TREC QA negatives can arise if a previous encoded passage tracks and Web sources. p in the queue can answer current question q. It SQuAD v1.1 (Rajpurkar et al., 2016) is original can happen if the some questions share the same used as a benchmark for reading comprehension. passage as answer, or if the same question-passage pair is sampled another time when its previous enWe follow the same procedure in Karpukhin et al. coded vector is still in the queue because the queue (2020) to create positive passages for all datasets. 6124 For TriviaQA, WQ and TREC, we use the highestranked passage from BM25 which contains the answer as positive passage, because these three datasets do not provide answer passages. We discard questions if answer cannot be found at the"
2021.acl-long.477,N19-1423,0,0.192647,"o efficiency and performance reasons. For xMoCo, we also expect our model to be trained in batches. Under the batch training setting, a batch of positive examples are processed together in one training step. The only adaption we need here is to push all vectors computed by slow encoders in one batch into the queues together. It effectively mimics the behavior of the “in-batch negative” strategy employed by previous works such as Karpukhin et al. (2020), where the passages in one batch will serve as negatives examples for their questions. 6123 4.4 Encoders We use pre-trained uncased BERT-base (Devlin et al., 2019) models as our encoders following Karpukhin et al. (2020). The question and passage encoders utilize two sets of different parameters but are initialized from the same BERT-base model. For both question and passage, we use the vectors of the sequence start tokens in the last layer as their representations. Better pre-trained models such as Liu et al. (2019) can lead to better retrieval performance, but we choose the uncased BERT-base model for easier comparison with previous work. 4.5 Incorporating hard negative examples size can be quite large. This is especially important for datasets with s"
2021.acl-long.477,P17-1018,1,0.814432,"effectiveness of the proposed approach. 1 Introduction Retrieving relevant passages given certain query from a large collection of documents is a crucial component in many information retrieval systems such as web search and open domain question answering (QA). Current QA systems often employ a two-stage pipeline: a retriever is firstly used to find relevant passages, and then a fine-grained reader tries to locate the answer in the retrieved passages. As recent advancement in machine reading comprehension (MRC) has demonstrated excellent results of finding answers given the correct passages (Wang et al., 2017), the performance of open-domain QA systems now relies heavily on the relevance of the selected passages of the retriever. Traditionally the retrievers usually utilize sparse keywords matching such as TF-IDF or BM25 (Robertson and Zaragoza, 2009), which can be efficiently implemented with an inverted index. With the popularization of neural network in NLP, the dense passage retrieval approach has gained traction (Karpukhin et al., 2020). In this approach, a dual-encoder model is learned to encode questions and passages into a dense, low-dimensional vector space, where the relevance between que"
2021.acl-short.31,N19-1388,0,0.0333583,"nces among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContr"
2021.acl-short.31,N19-1121,0,0.0224927,"Missing"
2021.acl-short.31,C18-1263,0,0.0176046,"an Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ (a) German Alignment pairs bitrary auxiliary languages to encourage the agreement across different translation directions. As shown in Figure 1, the multilingual baseline is separately trained on French-English and GermanEnglish directio"
2021.acl-short.31,2020.eamt-1.49,0,0.0187435,"Machine Translation Previous works (Zoph et al., 2016; Firat et al., 2016b; Johnson et al., 2017) have explored different settings of the multilingual neural machine translation (MNMT). Recent studies show that MNMT (Blackwood et al., 2018; Platanios et al., 2018; Gu et al., 2018) helps improve the performance of the lowresource or zero-shot translation. Some researchers Agreement-based Learning Many works try to use the agreement-based method (Liang et al., 2007, 2006; Al-Shedivat and Parikh, 2019) to encourage agreement among different translation orders and directions (Liang et al., 2006; Castilho, 2020; Yang et al., 2020a; Cheng et al., 2016; Zhang et al., 2019). Besides, the agreement-based method is also used to minimize the difference between the representation of source and target sentence (Yang et al., 2019). Our method further explores the approach of the multilingual agreement. 7 Conclusion We propose a novel agreement-based framework to encourage multilingual agreement across different translation directions by the agreement term. Experimental results on the multilingual translation task demonstrate that our method effectively minimizes the gaps among different translation direction"
2021.acl-short.31,N13-1073,0,0.221871,"wstest19 Table 3: The statistics of the training, valid, and test sets on WMT datasets of 10 language pairs. Experiment Setup Multilingual Data 10.00M 10.00M 4.60M 4.80M 1.40M 0.70M 0.50M 0.26M 0.18M 0.08M Valid Baselines and Evaluation We compare our method against the following baselines. Bilingual baseline is trained on each language pair separately. One-to-Many and Manyto-One are trained on the En→X and X→En directions respectively. We collect all English sentences (33M) of the bilingual corpora described above and translate them into other languages sentences. We extract alignment pairs (Dyer et al., 2013) across different languages for our method. One-to-Many + Pseudo and Many-to-One + Pseudo are trained on multilingual data combined with the pseudo data. We average the last 5 checkpoints and employ the beam search strategy with a beam size of 5 for evaluation. The evaluation metric is case-sensitive detokenized sacreBLEU2 (Post, 2018). 1 https://github.com/google/ sentencepiece 2 BLEU+case.mixed+lang.{src}{tgt}+numrefs.1+smooth.exp+tok.13a+version.1.4.14 235 3.3 ▁m We adopt the Transformer big architecture as the backbone model for all our experiments, which has 6 layers with an embedding siz"
2021.acl-short.31,N16-1101,0,0.0163994,". To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based meth"
2021.acl-short.31,D16-1026,0,0.0553303,"Missing"
2021.acl-short.31,N18-1032,0,0.0474607,"Missing"
2021.acl-short.31,2020.emnlp-main.204,0,0.0284964,"multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ (a) German Alignment pairs bitrary auxiliary languages to enc"
2021.acl-short.31,Q17-1024,0,0.0520573,"Missing"
2021.acl-short.31,D18-2012,0,0.0136573,"use the same training, valid, and test sets as the previous work (Wang et al., 2020) to evaluate multilingual models by parallel data from multiple WMT datasets with various languages, including English (En), French (Fr), Czech (Cs), German (De), Finnish (Fi), Latvian (Lv), Estonian (Et), Romanian (Ro), Hindi (Hi), Turkish (Tr), and Gujarati (Gu). For each language, we concatenate the WMT data of the latest available year and get at most 10M sentences by randomly sampling. Detailed statistics of datasets are listed in Table 3. All sentences in our experiments are tokenized by SentencePiece1 (Kudo and Richardson, 2018). Test newstest13 newstest16 newstest16 newstest16 newsdev17 newsdev18 newsdev16 newsdev14 newstest16 newsdev19 newstest15 newstest18 newstest18 newstest18 newstest17 newstest18 newstest16 newstest14 newstest18 newstest19 Table 3: The statistics of the training, valid, and test sets on WMT datasets of 10 language pairs. Experiment Setup Multilingual Data 10.00M 10.00M 4.60M 4.80M 1.40M 0.70M 0.50M 0.26M 0.18M 0.08M Valid Baselines and Evaluation We compare our method against the following baselines. Bilingual baseline is trained on each language pair separately. One-to-Many and Manyto-One are"
2021.acl-short.31,N06-1014,0,0.3185,"Missing"
2021.acl-short.31,D18-1039,0,0.0355016,"Missing"
2021.acl-short.31,W18-6319,0,0.0120737,"separately. One-to-Many and Manyto-One are trained on the En→X and X→En directions respectively. We collect all English sentences (33M) of the bilingual corpora described above and translate them into other languages sentences. We extract alignment pairs (Dyer et al., 2013) across different languages for our method. One-to-Many + Pseudo and Many-to-One + Pseudo are trained on multilingual data combined with the pseudo data. We average the last 5 checkpoints and employ the beam search strategy with a beam size of 5 for evaluation. The evaluation metric is case-sensitive detokenized sacreBLEU2 (Post, 2018). 1 https://github.com/google/ sentencepiece 2 BLEU+case.mixed+lang.{src}{tgt}+numrefs.1+smooth.exp+tok.13a+version.1.4.14 235 3.3 ▁m We adopt the Transformer big architecture as the backbone model for all our experiments, which has 6 layers with an embedding size of 1024, a dropout of 0.1, the feed-forward network size of 4096, and 16 attention heads. We train multilingual models with Adam (Kingma and Ba, 2015) (β1 = 0.9, β2 = 0.98). The learning rate is set as 5e4 with a warm-up step of 4,000. The models are trained with the label smoothing cross-entropy with a smoothing ratio of 0.1. The ba"
2021.acl-short.31,P19-1297,0,0.0256514,"thod, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ (a) German Alignmen"
2021.acl-short.31,N19-1044,0,0.0373592,"Missing"
2021.acl-short.31,2020.acl-main.324,0,0.0247877,"(MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ (a) German Alignment pairs bitrary auxiliary languages to encourage the agreement across different translation directions. As shown in Figure 1, the multilingual baseline is separately trained on French-English and GermanEnglish directions and cannot explicitly promote each"
2021.acl-short.31,2020.emnlp-main.75,0,0.123872,"achine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ (a) German Alignment pairs bitrary auxiliary languages to encourage the agreement across different translation directions. As shown in Figure 1, the multilingual baseline is separately trained on French-English and GermanEnglish directions and cannot expli"
2021.acl-short.31,P19-1296,0,0.0208365,"MNMT (Blackwood et al., 2018; Platanios et al., 2018; Gu et al., 2018) helps improve the performance of the lowresource or zero-shot translation. Some researchers Agreement-based Learning Many works try to use the agreement-based method (Liang et al., 2007, 2006; Al-Shedivat and Parikh, 2019) to encourage agreement among different translation orders and directions (Liang et al., 2006; Castilho, 2020; Yang et al., 2020a; Cheng et al., 2016; Zhang et al., 2019). Besides, the agreement-based method is also used to minimize the difference between the representation of source and target sentence (Yang et al., 2019). Our method further explores the approach of the multilingual agreement. 7 Conclusion We propose a novel agreement-based framework to encourage multilingual agreement across different translation directions by the agreement term. Experimental results on the multilingual translation task demonstrate that our method effectively minimizes the gaps among different translation directions and significantly outperforms the multilingual baselines. The analytic experiment about the crosslingual representation shows the effectiveness of our multilingual agreement in minimizing the differences among dif"
2021.acl-short.31,2020.acl-main.148,0,0.0223165,"inimizes the differences among them. We combine the multilingual training objectives with the agreement term by randomly substituting some fragments of the source language with their counterpart translations of auxiliary languages. To examine the effectiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives"
2021.acl-short.31,D16-1163,0,0.024088,"ctiveness of our method, we conduct experiments on the multilingual translation task of 10 language pairs. Experimental results show that our method achieves significant improvements over the previous multilingual baselines. 1 German Multilingual neural machine translation (MNMT) has experienced rapid growth in recent years (Johnson et al., 2017; Zhang et al., 2020; Aharoni et al., 2019; Wang et al., 2019). It is not only capable of translating among multiple language pairs by encouraging the crosslingual knowledge transfer to improve low-resource translation performance (Firat et al., 2016b; Zoph et al., 2016; Sen et al., 2019; Qin et al., 2020; Hedderich et al., 2020; Raffel et al., 2020), but also can handle multiple language pairs in a single model, reducing model parameters and training costs (Firat et al., 2016a; Blackwood et al., 2018; Wang et al., 2020; Sun et al., 2020). Previous works in MNMT simply optimize independent translation objectives and do not use arContribution during internship at Microsoft Research Asia. † Corresponding author. English German + French English (b) Figure 1: Comparison between (a) the multilingual translation and (b) our agreement-based method. Introduction ∗ ("
2021.adaptnlp-1.2,N19-4010,0,0.0410167,"Missing"
2021.adaptnlp-1.2,N19-1078,0,0.0369299,"Missing"
2021.adaptnlp-1.2,C18-1139,0,0.0623864,"Missing"
2021.adaptnlp-1.2,D16-1046,0,0.0277691,"ly, we train the final model with the pseudo-labeled samples. Ruder and Plank, 2018). For example, a named entity segmentation model trained on a news dataset fails to predict correctly on social media data such as Twitter. Because collecting many labeled samples in various domains is expensive, it is important to adapt contextual embedding model to different domains in unsupervised setting. Many domain adaptation methods of neural networks in NLP have been proposed in the past several years (Li, 2012; Ziser and Reichart, 2019, 2018; Cui et al., 2018; Louizos et al., 2015; Ganin et al., 2015; Mou et al., 2016). Our work focuses on unsupervised domain adaptation of contextual embeddings. We aim to fine-tune a pre-trained model that works well on a target domain when provided with labeled source samples and unlabeled target samples. With no access to the labels in target domain data, it is very difficult to adapt when the divergence of the label distribution between the source domain and the target domain is huge. Some of current methods propose a simple unsupervised domain-adaptive method, using a masked language modeling objective over unlabeled text in the target domain (Rochette et al., 2019; Han"
2021.adaptnlp-1.2,D19-1371,0,0.0287304,"labeled text in the target domain (Rochette et al., 2019; Han and Eisenstein, 2019; Gururangan et al., 2020). They first learn discriminative representations for the tarIntroduction Contextualized embeddings have become the foundations of many state-of-the-art natural language processing technologies (Devlin et al., 2018; Han and Eisenstein, 2019; Strakov´a et al., 2019). Pretrained contextualized embeddings can be used for many downstream tasks and be incorporated into an end-to-end system, allowing the embeddings to be fine-tuned from task-specific labeled data (Akbik et al., 2019a,b, 2018; Beltagy et al., 2019). One of the problems with contextual embedding models is that although fine-tuned models perform well on the samples generated from the same distribution as the training samples, they suffer considerably from unlabeled data when applied to a different domain (Saito et al., 2017; Rietzler et al., 2019; ∗ *Corresponding author. 9 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 9–15 April 20, 2021. ©2021 Association for Computational Linguistics get domain and then fine-tune a domain-adapted model with labeled source samples. Although selfsupervised fine-tuning in the targ"
2021.adaptnlp-1.2,P18-1096,0,0.146162,"t samples. We train the final model with those samples. We evaluate our method on named entity segmentation and sentiment analysis tasks. These experiments show that our approach outperforms baseline methods. 1 General Pre-trained Model Labeled Source Samples Unlabeled Target Samples Classifier t Pseudo-labeled Target Samples Figure 1: Overview of our training framework. We jointly fine-tune general pre-trained model and target domain pre-trained model with labeled source data. Then we generate pseudo-labels on target samples. Finally, we train the final model with the pseudo-labeled samples. Ruder and Plank, 2018). For example, a named entity segmentation model trained on a news dataset fails to predict correctly on social media data such as Twitter. Because collecting many labeled samples in various domains is expensive, it is important to adapt contextual embedding model to different domains in unsupervised setting. Many domain adaptation methods of neural networks in NLP have been proposed in the past several years (Li, 2012; Ziser and Reichart, 2019, 2018; Cui et al., 2018; Louizos et al., 2015; Ganin et al., 2015; Mou et al., 2016). Our work focuses on unsupervised domain adaptation of contextual"
2021.adaptnlp-1.2,P19-1527,0,0.050473,"Missing"
2021.adaptnlp-1.2,2020.acl-main.740,0,0.0404887,"Missing"
2021.adaptnlp-1.2,W16-3919,0,0.0613571,"Missing"
2021.adaptnlp-1.2,D19-1433,0,0.0597132,"16). Our work focuses on unsupervised domain adaptation of contextual embeddings. We aim to fine-tune a pre-trained model that works well on a target domain when provided with labeled source samples and unlabeled target samples. With no access to the labels in target domain data, it is very difficult to adapt when the divergence of the label distribution between the source domain and the target domain is huge. Some of current methods propose a simple unsupervised domain-adaptive method, using a masked language modeling objective over unlabeled text in the target domain (Rochette et al., 2019; Han and Eisenstein, 2019; Gururangan et al., 2020). They first learn discriminative representations for the tarIntroduction Contextualized embeddings have become the foundations of many state-of-the-art natural language processing technologies (Devlin et al., 2018; Han and Eisenstein, 2019; Strakov´a et al., 2019). Pretrained contextualized embeddings can be used for many downstream tasks and be incorporated into an end-to-end system, allowing the embeddings to be fine-tuned from task-specific labeled data (Akbik et al., 2019a,b, 2018; Beltagy et al., 2019). One of the problems with contextual embedding models is tha"
2021.adaptnlp-1.2,P19-1591,0,0.0265711,"ined model with labeled source data. Then we generate pseudo-labels on target samples. Finally, we train the final model with the pseudo-labeled samples. Ruder and Plank, 2018). For example, a named entity segmentation model trained on a news dataset fails to predict correctly on social media data such as Twitter. Because collecting many labeled samples in various domains is expensive, it is important to adapt contextual embedding model to different domains in unsupervised setting. Many domain adaptation methods of neural networks in NLP have been proposed in the past several years (Li, 2012; Ziser and Reichart, 2019, 2018; Cui et al., 2018; Louizos et al., 2015; Ganin et al., 2015; Mou et al., 2016). Our work focuses on unsupervised domain adaptation of contextual embeddings. We aim to fine-tune a pre-trained model that works well on a target domain when provided with labeled source samples and unlabeled target samples. With no access to the labels in target domain data, it is very difficult to adapt when the divergence of the label distribution between the source domain and the target domain is huge. Some of current methods propose a simple unsupervised domain-adaptive method, using a masked language mo"
2021.emnlp-main.125,2020.acl-main.421,0,0.147399,"s on an Nvidia DGX-2 Station. Details of the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used for all languages rather than selecting fine-tuned models separately. Details of the fine-tuning hyperparameters are"
2021.emnlp-main.125,Q19-1038,0,0.0291276,". We believe the better-aligned representations potentially improve the cross-lingual transferability. Furthermore, the results also indicate that our pre-training objective is more effective for training the encoder than M T5. 4.6 Word Alignment In addition to cross-lingual sentence retrieval that We analyze the cross-lingual representations pro- evaluates sentence-level representations, we also duced by our M T6 model. Following Chi et al. explore whether the representations produced by (2021a), we evaluate the representations on the M T6 are better-aligned at token-level. Thus, we Tatoeba (Artetxe and Schwenk, 2019) cross-lingual compare our M T6 with M T5 on the word alignsentence retrieval task. The test sets consist of 14 ment task, where the goal is to find corresponding English-centric language pairs covered by the par- word pairs in a translation pair. We use the hidden allel data in our experiments. Figure 4 illustrates vectors from the last encoder layer, and apply the the average accuracy@1 scores of cross-lingual SimAlign (Jalili Sabet et al., 2020) tool to obtain sentence retrieval. The scores are averaged over the resulting word alignments. Table 5 shows the 14 language pairs and both the dir"
2021.emnlp-main.125,2021.naacl-main.280,1,0.930284,"asks and the 1 Introduction training objective. We present three cross-lingual Multilingual pretrained language models, such as tasks for text-to-text Transformer pre-training, i.e., mBERT (Devlin et al., 2019), have attracted in- machine translation, translation pair span corrupcreasing attention. They not only improve the tion, and translation span corruption. In the transperformance on downstream multilingual NLP lation span corruption task, the model is trained to tasks (Conneau and Lample, 2019; Conneau et al., predict the text spans based on the input translation 2020; Liu et al., 2020; Chi et al., 2021c), but pair. The cross-lingual tasks encourage the model also show an impressive cross-lingual transferabil- to align representations of different languages. ity (Wu and Dredze, 2019; K et al., 2020; Hu et al., We also propose a new objective for text-to-text 2020b; Chi et al., 2021a). pre-training, called partially non-autoregressive Multilingual pretrained models are typically (PNAT) decoding. The PNAT objective divides the trained on multilingual unlabeled text with unsu- target sequence into several groups, and constrains pervised language modeling tasks, e.g., masked that the predictions"
2021.emnlp-main.125,2021.acl-long.265,1,0.835814,"asks and the 1 Introduction training objective. We present three cross-lingual Multilingual pretrained language models, such as tasks for text-to-text Transformer pre-training, i.e., mBERT (Devlin et al., 2019), have attracted in- machine translation, translation pair span corrupcreasing attention. They not only improve the tion, and translation span corruption. In the transperformance on downstream multilingual NLP lation span corruption task, the model is trained to tasks (Conneau and Lample, 2019; Conneau et al., predict the text spans based on the input translation 2020; Liu et al., 2020; Chi et al., 2021c), but pair. The cross-lingual tasks encourage the model also show an impressive cross-lingual transferabil- to align representations of different languages. ity (Wu and Dredze, 2019; K et al., 2020; Hu et al., We also propose a new objective for text-to-text 2020b; Chi et al., 2021a). pre-training, called partially non-autoregressive Multilingual pretrained models are typically (PNAT) decoding. The PNAT objective divides the trained on multilingual unlabeled text with unsu- target sequence into several groups, and constrains pervised language modeling tasks, e.g., masked that the predictions"
2021.emnlp-main.125,2020.tacl-1.30,0,0.0611303,"the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used for all languages rather than selecting fine-tuned models separately. Details of the fine-tuning hyperparameters are described in Appendix. As shown in Tabl"
2021.emnlp-main.125,2020.acl-main.747,0,0.151214,"Missing"
2021.emnlp-main.125,2020.acl-main.703,0,0.382809,"put sentence or the hsepi token for the other situations. Classification The goal of the text classification task is to predict the label of a given text. Following T5 (Raffel et al., 2020), we directly use the label text as the output text sequence. We provide an example for the MNLI natural language infer- 4 Experiments ence task (Williams et al., 2018). Given an input sentence pair of “You have access to the facts .” 4.1 Setup and “The facts are accessible to you .”, the goal is Data Following previous work on cross-lingual to classify the input into the relationships of “en- pre-training (Conneau et al., 2020; Chi et al., tailment”, “contradiction”, or “neutral”. The input 2021a), we use the natural sentences from CCand target sequences are constructed as Net (Wenzek et al., 2019) in 94 languages Input: hbosi You have access to the facts. heosi for monolingual text-to-text tasks. For crossThe facts are accessible to you. heosi lingual text-to-text tasks, we use parallel corpora Output: hbosi entailment heosi of 14 English-centric language pairs, collected Since multi-task fine-tuning is not the focus of from MultiUN (Ziemski et al., 2016), IIT Bomthis work, we do not prepend a task prefix in the b"
2021.emnlp-main.125,D19-1252,0,0.0527437,"Missing"
2021.emnlp-main.125,2020.findings-emnlp.147,0,0.0582365,"Missing"
2021.emnlp-main.125,L18-1548,0,0.0349859,"i et al., tailment”, “contradiction”, or “neutral”. The input 2021a), we use the natural sentences from CCand target sequences are constructed as Net (Wenzek et al., 2019) in 94 languages Input: hbosi You have access to the facts. heosi for monolingual text-to-text tasks. For crossThe facts are accessible to you. heosi lingual text-to-text tasks, we use parallel corpora Output: hbosi entailment heosi of 14 English-centric language pairs, collected Since multi-task fine-tuning is not the focus of from MultiUN (Ziemski et al., 2016), IIT Bomthis work, we do not prepend a task prefix in the bay (Kunchukuttan et al., 2018), OPUS (Tiedeinput text. We also adopt a constrained decoding mann, 2012), and WikiMatrix (Schwenk et al., 1674 2019). Details of the pre-training data are described in Appendix. Training Details In the experiments, we consider the small-size Transformer model (Xue et al., 2020), with dmodel = 512, dff = 1, 024, 6 attention heads, and 8 layers for both the encoder and the decoder1 . We use the vocabulary provided by XLMR (Conneau et al., 2020), and extend it with 100 unique mask tokens for the span corruption tasks. We pretrain our M T6 for 0.5M steps with batches of 256 length-512 input seque"
2021.emnlp-main.125,2020.findings-emnlp.360,0,0.0333655,". Besides, under the setting with fewer training data, M T6 shows more improvements over M T5. Model es-en ru-en vi-en tr-en M T5 M T6 11.36 11.83 8.77 9.49 8.98 9.52 10.57 10.80 Table 3: ROUGE-2 scores on Wikilingua cross-lingual summarization. Results are averaged over three runs. Model XQuAD MLQA TyDiQA XNLI PAWS-X M T5 M T6 30.4 28.6 27.5 27.2 27.5 25.9 19.5 14.6 16.0 13.2 Table 4: The cross-lingual transfer gap scores on the XTREME tasks. A lower transfer gap score indicates better cross-lingual transferability. We use the EM scores to compute the gap scores for the QA tasks. Wikilingua (Ladhak et al., 2020) dataset containing passage-summary pairs in four language pairs. We fine-tune the models for 100K steps with a batch size of 32 and a learning rate of 0.0001. We use the greedy decoding for all evaluated models. The Cross-Lingual Summarization The cross- evaluation results are shown in Table 3, where lingual summarization task aims to generate M T6 outperforms M T5 on the test sets of four summaries in a different language. We use the language pairs. 1676 Averaged Accuracy 40 mT5 mT6 30 20 10 4 Layer 6 8 Figure 4: Evaluation results of different layers on Tatoeba cross-lingual sentence retrie"
2021.emnlp-main.125,2020.acl-main.653,0,0.538333,"procedure takes about 2.5 days on an Nvidia DGX-2 Station. Details of the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used for all languages rather than selecting fine-tuned models separately. Details of the"
2021.emnlp-main.125,W04-1013,0,0.0271001,"ion. RG is short for ROUGE. Results of XLM and XNLG are taken from (Chi et al., 2020). Results of M T5 and M T6 are averaged over three runs. lines as the input documents and summaries, respectively. The dataset consists of examples in the languages of English, French, and Chinese. For each language, it contains 500K, 5K, and 5K examples for the training, validation, and test, respectively. We fine-tune the models for 20 epochs with a batch size of 32 and a learning rate of 0.00001. During decoding, we use the greedy decoding for all evaluated models. As shown in Table 2, we report the ROUGE (Lin, 2004) scores of the models on Gigaword multilingual abstractive summarization. We observe that M T6 consistently outperforms M T5 on all the three target languages. Comparing with the XLM (Conneau and Lample, 2019) and XNLG (Chi et al., 2020) models with 800M parameters, our M T6 model achieves a similar performance with only 300M parameters. Besides, under the setting with fewer training data, M T6 shows more improvements over M T5. Model es-en ru-en vi-en tr-en M T5 M T6 11.36 11.83 8.77 9.49 8.98 9.52 10.57 10.80 Table 3: ROUGE-2 scores on Wikilingua cross-lingual summarization. Results are aver"
2021.emnlp-main.125,P19-1015,0,0.0907957,"optimizer (Kingma and Ba, 2015) with a linear learning rate scheduler. The pre-training procedure takes about 2.5 days on an Nvidia DGX-2 Station. Details of the pre-training hyperparameters are described in Appendix. 4.2 4.2.1 Results XTREME Cross-lingual Understanding To validate the performance of M T6, we evaluate the pretrained models on XTREME (Hu et al., 2020b), which is a widely used benchmark for cross-lingual understanding. Following M T5 (Xue et al., 2020), we consider six downstream tasks included by XTREME: the named entity recognition (NER) task on the WikiAnn (Pan et al., 2017; Rahimi et al., 2019) dataset in 40 languages, the question answering (QA) task on MLQA (Lewis et al., 2020b), XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020), the cross-lingual natural language inference task on XNLI (Conneau et al., 2018), and crosslingual paraphrase adversaries on PAWS-X (Yang et al., 2019). The models are evaluated under the cross-lingual transfer setting (Conneau et al., 2020; Hu et al., 2020b). Under this setting, the models should be fine-tuned only on English training data but evaluated on all target languages. Moreover, for each pretrained model, only one model is used"
2021.emnlp-main.125,tiedemann-2012-parallel,0,0.0711811,"Missing"
2021.emnlp-main.125,N18-1101,0,0.0159465,"e hsepi tag means the end of entity span. We use the following constrained decoding rules: (1) The model should decode entity tags or the end-of-sentence tag (heosi) after a hbosi token or a hsepi token; (2) Otherwise, the model should decode the tokens from the input sentence or the hsepi token for the other situations. Classification The goal of the text classification task is to predict the label of a given text. Following T5 (Raffel et al., 2020), we directly use the label text as the output text sequence. We provide an example for the MNLI natural language infer- 4 Experiments ence task (Williams et al., 2018). Given an input sentence pair of “You have access to the facts .” 4.1 Setup and “The facts are accessible to you .”, the goal is Data Following previous work on cross-lingual to classify the input into the relationships of “en- pre-training (Conneau et al., 2020; Chi et al., tailment”, “contradiction”, or “neutral”. The input 2021a), we use the natural sentences from CCand target sequences are constructed as Net (Wenzek et al., 2019) in 94 languages Input: hbosi You have access to the facts. heosi for monolingual text-to-text tasks. For crossThe facts are accessible to you. heosi lingual text"
2021.emnlp-main.2,N19-1388,0,0.0155195,"directly fine-tune all model parameters on WMT19 De-En training set. CRISS and m2m-100 are the state-of-the-art unsupervised and supervised multilingual NMT models, respectively. The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs. m2m-100 is trained with 7.5 billion parallel sentences across 2200 translation directions. The results of CRISS and m2m-100 are listed as reference, because CRISS and m2m-100 are manyto-many NMT models whose performance may degrade due to the competitions among different target languages (Aharoni et al., 2019; Zhang et al., 2020), while SixT is a many-to-one NMT model. The official m2m-100 model has three sizes: small (418M parameters), base (1.2B parameters) and large (12B parameters). The results of m2m-100 20 German De Nl Model # Sents mBART CRISS m2m-100 0.04B 1.8B 7.5B 27.4 28.8 28.0 43.3 47.0 48.5 24.7 32.2 30.0 SixT 0.04B 33.8 54.7 30.1 Es Romance Ro It Fi 28.2 35.4 34.1 29.8 48.9 50.0 18.8 23.9 24.9 33.9 43.0 26.3 Uralic Lv Indo-Aryan Ne Si East Asian Zh Ja Ko Gu Avg. Et Hi 14.2 18.6 19.9 15.7 23.5 25.8 12.3 23.1 21.9 9.6 14.7 3.7 7.2 14.4 10.6 10.3 19.0 0.4 8.3 13.4 19.5 6.0 7.9 11.5 21.1"
2021.emnlp-main.2,P17-1176,1,0.597432,"seen languages. SixT significantly outperforms mBART with an average improvement of 7.1 BLEU on zeroshot any-to-English translation across 14 source languages. Furthermore, with much less training computation cost and training data, the SixT model gets better performance on 15 any-to-English test sets than CRISS and m2m-100, two strong multilingual NMT baselines.1 2 NMT model is directly tested between the unseen language pairs lzi -to-lt in a zero-shot manner. Different from multilingual NMT (Johnson et al., 2017), unsupervised NMT (Lample et al., 2018) or zero-resource NMT through pivoting (Chen et al., 2017, 2018), neither the parallel nor monolingual data in the language lzi is directly accessible in the ZeXT task. The model has to rely on the offthe-shelf MPE to translate from language lzi . The challenge to this task is how to leverage an MPE for machine translation while preserving its crosslingual transferability. In this paper, we utilize XLM-R, which is jointly trained on 100 languages, as the off-the-shelf MPE. The ZeXT task calls for approaches to efficiently build a many-to-one NMT model that can translate from 100 languages supported by XLM-R with parallel dataset of only one language"
2021.emnlp-main.2,2020.acl-main.747,0,0.236641,"th the baselines. SixT gets 18.3 average BLEU and improves over the best baseline by 5.4 average BLEU, showing that SixT successfully learns to translate while preserving the cross-lingual transferability of XLM-R. For all language pairs, SixT obtains better transferring scores. In contrast, vanilla Transformer can hardly transfer and the other baselines do not well transfer to the distant languages. In addition to zero-shot performance, SixT also achieves the best result on De-En test set. Note that the best checkpoint is selected with zero-shot validation set for all methods. Previous work (Conneau et al., 2020; Hu et al., 2020) mainly uses XLM-R for cross-lingual transfer on NLU tasks. The experiments demonstrate that XLM-R can be also utilized for zero-shot neural machine translation if it is fine-tuned properly. We leave the exploration of cross-lingual transfer using XLM-R for other NLG tasks as the future work. Baselines We compare our model with vanilla Transformer and five conventional methods to apply pretrained Transformer encoder on NMT task. The pretrained encoders in these methods are replaced with XLM-R base for fair comparison. • Vanilla Transformer. The encoder is with the same size o"
2021.emnlp-main.2,2020.tacl-1.47,0,0.373676,")) while degrades with Resdrop (see results of (2)→(5) and (4)→(6)). This is expected since Resdrop helps to build a more language-agnostic encoder. Although Resdrop degrades supervised performance, it improves zero-shot translation. The zero-shot performance is related with both supervised performance and model transferability. By either enhancing the supervised performance (with TwoStage and BigDec) or the model transferability (with Resdrop), the overall performance of zero-shot translation can be improved. Analysis Comparison with multilingual NMT In this part, we compare SixT with mBART (Liu et al., 2020), CRISS (Tran et al., 2020) and m2m-100 (Fan et al., 2020) on any-to-English test sets. mBART is a strong pretrained multilingual encoderdecoder based Transformer explicitly designed for NMT. We follow their setting and directly fine-tune all model parameters on WMT19 De-En training set. CRISS and m2m-100 are the state-of-the-art unsupervised and supervised multilingual NMT models, respectively. The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs. m2m-100 is trained with 7.5 billion parallel sentences across 2200 tra"
2021.emnlp-main.2,D18-1398,1,0.602418,"is the layer normalization. Liu et al. (2021) aim at training a language-agnostic encoder for NMT using parallel corpus from scratch. Compared with them, our method shows that it’s possible to make a pretrained multilingual encoder more language-agnostic by relaxing the position constraint during fine-tuning. Capacity-enhanced decoder Some previous work (Zhu et al., 2020; Yang et al., 2020) incorporates BERT into NMT and configures the decoder size as Vaswani et al. (2017). For example, to train an NMT on Europarl De-En training dataset, the default decoder configuration is Transformer base (Gu et al., 2018; Currey et al., 2020). However, our model relies more on the decoder to learn from the labeled data, as the encoder is mainly responsible for cross-lingual transfer. This is also reflected in our training strategy: at the first stage only the decoder parameters are optimized, while at the second stage the encoder is only slightly fine-tuned to preserve its transferability. Therefore, the model capacity of SixT is smaller than vanilla Transformer with the same size. We propose to apply a capacityenhanced decoder that has larger dimension of feed forward network, more layers and more attention"
2021.emnlp-main.2,N19-4009,0,0.02752,"are optimized, while at the second stage the encoder is only slightly fine-tuned to preserve its transferability. Therefore, the model capacity of SixT is smaller than vanilla Transformer with the same size. We propose to apply a capacityenhanced decoder that has larger dimension of feed forward network, more layers and more attention heads at both the first and second training stages. The improvement brought by the big decoder is not simply because of more model parameters. More Model settings We use the XLM-R base model as the off-the-shelf MPE. The model is implemented on fairseq toolkit (Ott et al., 2019). We set Transformer encoder the same size as the XLMR base model. For the decoder, we use the same hyper-parameter setting as the encoder. We denote model with such configuration as SixT and use this configuration for our NMT models through the paper unless otherwise stated. The encoderdecoder attention modules are randomly initialized. We remove the residual connection at the 11-th (penultimate) encoder layer, which is selected on the validation dataset. For the empirical exploration in Table 1, we use two model configurations. For Strategy (1)–(7) where decoder layers are trained from scrat"
2021.emnlp-main.2,D19-5603,0,0.0395317,"rmer. The encoder is with the same size of XLM-R base, the decoder uses the size of BaseDec. All model parameters are randomly initialized. • +XLM-R fine-tune encoder (Conneau and Lample, 2019). The encoder is initialized with XLM-R. All parameters are trained. • +XLM-R fine-tune all (Conneau and Lample, 2019). All parameters except those of cross attention module are initialized with XLM-R and directly fine-tuned. • +XLM-R as encoder embedding (Zhu et al., 2020). The XLM-R output is leveraged as the encoder input of the NMT. The XLM-R model is fixed during training. • +Recycle XLM-R for NMT (Imamura and Sumita, 2019). The method initializes the encoder with XLM-R and only trains decoder at the first step. Then all are trained at the second step. • XLM-R fused model (Zhu et al., 2020). The XLM-R output is fused into encoder and decoder separately with attention mechanism. The encoder embedding is initialized from XLM-R to facilitate 4.3 Ablation Study We conduct an ablation study with the proposed SixT on the Europarl De-En training set, as shown 5 BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a +version.1.5.0 6 19 We use De-En validation dataset this time. Model De Nl Es It Ro Fi Lv Et Hi Ne Zh Avg. Vanilla"
2021.emnlp-main.2,P18-1007,0,0.0646372,"Missing"
2021.emnlp-main.2,D19-1077,0,0.0708843,"even do not have labeled data. Encoder Decoder Given that MPE has achieved great success in crossbody body lingual NLU tasks, a question worthy of research is how to perform zero-shot cross-lingual transfer Encoder Decoder in the NMT task by embed embed leveraging the MPE. Some work (Zhu et al., 2020; Yang et al., 2020; Weng (2) Training the second stageand Sumita, 2019) explores (1) Training in the first stage et al., in 2020; Imamura 1 Introduction approaches to improve NMT performance by inMultilingual pretrained encoders (MPE) such as corporating monolingual pretrained Transformer mBERT (Wu and Dredze, 2019), XLM (Con- encoder such as BERT (Devlin et al., 2019). Howneau and Lample, 2019), and XLM-R (Conneau ever, simply replacing the monolingual pretrained et al., 2020) have shown remarkably strong re- encoder in previous studies with MPE does not sults on zero-shot cross-lingual transfer mainly work well for cross-lingual transfer of NMT (see for natural language understanding (NLU) tasks, baselines in Table 2). Others propose to fine-tune including named entity recognition (NER), ques- the encoder-decoder-based multilingual pretrained tion answering (QA) and natural language infer- model for cr"
2021.emnlp-main.2,2020.emnlp-main.210,0,0.103365,"nneau ever, simply replacing the monolingual pretrained et al., 2020) have shown remarkably strong re- encoder in previous studies with MPE does not sults on zero-shot cross-lingual transfer mainly work well for cross-lingual transfer of NMT (see for natural language understanding (NLU) tasks, baselines in Table 2). Others propose to fine-tune including named entity recognition (NER), ques- the encoder-decoder-based multilingual pretrained tion answering (QA) and natural language infer- model for cross-lingual transfer of NMT (Liu et al., ence (NLI). These methods jointly train a Trans- 2020; Lin et al., 2020). It is still unclear how to former (Vaswani et al., 2017) encoder to perform conduct cross-lingual transfer for NMT model with ∗ existing multilingual pretrained encoders such as Contribution during internship at Microsoft Research. † Corresponding author. XLM-R. 15 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 15–26 c November 7–11, 2021. 2021 Association for Computational Linguistics In this paper, we focus on a Zero-shot crosslingual(X) NMT Transfer task (ZeXT, see Figure 1), which aims at translating multiple unseen languages by leveraging a"
2021.emnlp-main.2,2021.acl-long.101,0,0.37879,"her improve the translation performance by jointly fine-tuning all parameters except encoder embedding of the NMT.2 Since the decoder has been well adapted to the encoder at the first stage, we expect the model can be slightly fine-tuned to improve the translation capacity without losing the Position disentangled encoder The representations from XLM-R initialized encoder have a strong positional correspondence to the source sentence. The word order information inside is language-specific and may hinder the cross-lingual transfer from supervised source language to unseen languages. Inspired by Liu et al. (2021), we propose to relax this structural constraint and make the encoder outputs less position- and languagespecific. More specifically, at the second stage, we remove the residual connection after the selfattention sublayer in one of the encoder layers i 2 According to our preliminary experiment, the average BLEU is 0.2 lower when the encoder embedding is also learned at the second stage. Besides, freezing encoder embedding leads to higher computational efficiency. 17 Encoder body Decoder body Encoder body Decoder body Encoder embed Decoder embed Encoder embed Decoder embed (2) Training at the s"
2021.emnlp-main.2,2020.acl-main.148,0,0.0168613,"model parameters on WMT19 De-En training set. CRISS and m2m-100 are the state-of-the-art unsupervised and supervised multilingual NMT models, respectively. The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs. m2m-100 is trained with 7.5 billion parallel sentences across 2200 translation directions. The results of CRISS and m2m-100 are listed as reference, because CRISS and m2m-100 are manyto-many NMT models whose performance may degrade due to the competitions among different target languages (Aharoni et al., 2019; Zhang et al., 2020), while SixT is a many-to-one NMT model. The official m2m-100 model has three sizes: small (418M parameters), base (1.2B parameters) and large (12B parameters). The results of m2m-100 20 German De Nl Model # Sents mBART CRISS m2m-100 0.04B 1.8B 7.5B 27.4 28.8 28.0 43.3 47.0 48.5 24.7 32.2 30.0 SixT 0.04B 33.8 54.7 30.1 Es Romance Ro It Fi 28.2 35.4 34.1 29.8 48.9 50.0 18.8 23.9 24.9 33.9 43.0 26.3 Uralic Lv Indo-Aryan Ne Si East Asian Zh Ja Ko Gu Avg. Et Hi 14.2 18.6 19.9 15.7 23.5 25.8 12.3 23.1 21.9 9.6 14.7 3.7 7.2 14.4 10.6 10.3 19.0 0.4 8.3 13.4 19.5 6.0 7.9 11.5 21.1 24.8 32.7 18.4 25.0"
2021.emnlp-main.257,2020.emnlp-main.367,0,0.0571655,"Missing"
2021.emnlp-main.257,2020.tacl-1.30,0,0.0249007,"es on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacity each Implementation Details We adapt the Trans- language requires. former architecture from the base model setting Since increasing vocabulary size brings the isin Conneau et al. (2020), i.e., 12 layers and 768 sues of model size and pre-training speed, we study hidd"
2021.emnlp-main.257,2020.acl-main.747,0,0.24302,". In order to address the issues, we propose k-NN-based target sampling to accelerate the expensive softmax. Our experiments show that the multilingual vocabulary learned with VO C AP benefits cross-lingual language model pre-training. Moreover, k-NN-based target sampling mitigates the side-effects of increasing the vocabulary size while achieving comparable performance and faster pre-training speed. The code and the pretrained multilingual vocabularies are available at https://github. com/bozheng-hit/VoCapXLM. 1 Introduction Pretrained cross-lingual language models (Conneau and Lample, 2019; Conneau et al., 2020; Chi et al., 2021b; Xue et al., 2020) have recently shown great success in improving cross-lingual transferability. These models encode texts from different languages into universal representations with a shared multilingual vocabulary and a shared Transformer encoder (Vaswani et al., 2017). By pretraining cross-lingual language models on the largescale multilingual corpus, the models achieve stateof-the-art performance on various downstream tasks, e.g., cross-lingual question answering and cross-lingual sentence classification. Although the Transformer architecture used in most pretrained mo"
2021.emnlp-main.257,P18-1007,0,0.374283,"020). Meanwhile, state-of-the-art pretrained cross-lingual language models use the shared multilingual vocabulary of 250K subword units to represent more than 100 languages (Conneau et al., 2020; Chi et al., 2021b; Xue et al., 2020). Although some subword units are shared across languages, no more than 2.5K language-specific subword units on average are allocated for each language, which is still relatively small. Besides, the multilingual vocabulary is trained on the combined multilingual corpus with subword segmentation algorithms like BPE (Sennrich et al., 2015) and unigram language model (Kudo, 2018). During vocabulary construction, these algorithms tend to select more subword units shared across languages with common scripts like Latin and Cyrillic (Chung et al., 2020b), but have a lower chance to select language-specific subword units. It is hard to determine how much vocabulary capacity a particular language requires and whether the shared multilingual vocabulary has allocated enough vocabulary capacity to represent the language. In this paper, we propose VO C AP, an algorithm to allocate large vocabulary for cross-lingual language model by separately evaluating the required vocabulary"
2021.emnlp-main.257,2020.acl-main.653,0,0.0420862,"uestion answering ness of our methods, we conduct experiments on datasets but degrades on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacity each Implementation Details We adapt the Trans- language requires. former architecture from the base model setting Since increasing vocabulary size brings the isin Conneau et al. (202"
2021.emnlp-main.257,2021.ccl-1.108,0,0.0879979,"Missing"
2021.emnlp-main.257,P17-1178,0,0.0880255,"us. 94 92 −280 −160 −280 −260 −240 −220 ALP −200 −180 −160 −150 Low res. Mid res. High res. Figure 3: F1 score on NER task with different vocabularies versus their ALP on the monolingual corpus. Figure 4: Comparison of vocabulary capacity of different-resourced languages. Shorter bars indicate larger vocabulary capacity. vocabularies for each language on the corresponding monolingual corpus, with vocabulary size ranging from 1K to 30K. Then we pretrain monolingual language models with the corresponding monolingual vocabularies. We evaluate these pretrained models on two downstream tasks: NER (Pan et al., 2017) and POS (Zeman et al., 2019) from the XTREME benchmark since there is annotated task data for a large number of languages. The vocabularies are learned on the reconstructed CommonCrawl corpus (Chi et al., 2021b; Conneau et al., 2020) using SentencePiece (Kudo and Richardson, 2018) with the unigram language model (Kudo, 2018). The unigram distributions are also counted on the CommonCrawl corpus. The Wikipedia corpus is used for all pre-training experiments in this paper since it is easier to run experiments due to its smaller size. More details about the pre-training data can be found in the a"
2021.emnlp-main.257,D19-1382,0,0.0210914,"ulary directly learned on multilingual corpus with 4.1 Setup SentencePiece, i.e., XLM-R250K and J OINT250K , Fine-Tuning Datasets To validate the effectiveour VO C AP250K improves on question answering ness of our methods, we conduct experiments on datasets but degrades on PAWS-X, POS and NER. three types of cross-lingual understanding tasks Then increasing the vocabulary from VO C AP250K from XTREME benchmark (Hu et al., 2020), into VO C AP500K mitigates the gap and bring imcluding two classification datasets: XNLI (Conprovements on six datasets except for PAWS-X, neau et al., 2018), PAWS-X (Yang et al., 2019), which only includes seven high-resource languages. three span extraction datasets: XQuAD (Artetxe However, increasing the size of vocabulary diet al., 2020), MLQA (Lewis et al., 2020), TyDiQArectly learned with Sentencepiece from J OINT250K GoldP (Clark et al., 2020), and two sequence labelto J OINT500K does not improve the performance ing datasets: NER (Pan et al., 2017), POS (Zeman as our VO C AP method does, showing the imporet al., 2019). The statistics of the datasets are shown tance of selecting language-specific subword units in the appendix. and leveraging how much vocabulary capacit"
2021.emnlp-main.389,2020.coling-main.82,1,0.774695,"Missing"
2021.emnlp-main.389,P02-1040,0,0.124372,") n ek 2A where ek is the k-th element in sequence A; I(ek , B) is the index of ek in sequence B; n is the length of sequence A. 5.4 Reading Order Detection We train the models with left-to-right and topto-bottom ordered inputs and report the evaluation results on the test set of ReadingBank in Table 2. We also report the results of the heuristic method. The results show that LayoutReader is superior and achieves the SOTA results compared 5.3 Evaluation Metrics with other baselines. It improves the average pageAverage Page-level BLEU: The BLEU level BLEU by 0.2847 and decreases the ARD score (Papineni et al., 2002) is widely used in by 6.71. Even if we remove some of the input sequence generation. Since LayoutReader is built modalities, there is still 0.16 and 0.27 improveon a sequence-to-sequence model, it is natural to ments of BLEU in LayoutReader (text only) and evaluate our models with BLEU scores. BLEU LayoutReader (layout only), and there is a steady scores measure the n-gram overlaps between the 6.15 reduction of ARD in LayoutReader (layout hypothesis and reference. We report Average only). However, we also see a drop of ARD in Lay7 outReader (text only), mainly because of the severe https://git"
2021.emnlp-main.389,H94-1084,0,0.592416,"Missing"
2021.emnlp-main.45,W05-0909,0,0.0927913,"steps when employed in a continual pre-training setting. This also reduces recent concerns (Strubell et al., 2019; Bender et al., 2021) about the carbon footprint and energy consumption in LM pre-training. 4.2 Tasks and Datasets et al., 2019), we concatenate the passage and an answer as the input of the model to learn to generate the corresponding question in the fine-tuning stage. We use SQUAD (Rajpurkar et al., 2016) dataset to train and test question generation following the data split in (Du and Cardie, 2018). We report evaluation results in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Grammatical Error Correction is a task that rewrites a potentially erroneous input sentence into a fluent sentence that is grammatical error free without changing the original meaning of the input sentence. Following the recent work (Grundkiewicz et al., 2019; Kiyono et al., 2019; Zhou et al., 2020a) in GEC, we use the public Lang-8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS datasets (Bryant et al., 2019; Granger, 1998) for fine-tuning without using any synthetic GEC data, and then evaluate Max-"
2021.emnlp-main.45,N19-1423,0,0.133878,"ovements over T5 or rule-based noising approaches, the T5-based on various natural language generation tasks.1 imperfect span generator can derive various informative text spans that benefit the model to learn 1 Introduction meaningful and diverse rewriting patterns including Text infilling (e.g., masked language modeling) paraphrasing and enhancing the fluency and contexhas become a prevalent learning objective for pre- tual consistency through correcting grammatical, trained language models (PTLMs) (Peters et al., commonsense and factual errors, to improve a text 2018; Radford et al., 2018; Devlin et al., 2019; sequence. These rewriting patterns resemble the Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; goal of various NLG tasks and thus strengthen the Lewis et al., 2020b; Raffel et al., 2019). It provides ability of pre-trained model for downstream appliself-supervision by masking out tokens or spans in cations. text, and trains a model to infill the masked conIn our experiments, we apply SSR to the typitent based on the contexts, accordingly guiding the cal Seq2Seq pre-trained model – T5 (Raffel et al., model for representation learning, as Figure 1(a) 2019) in a continual learning fashi"
2021.emnlp-main.45,P18-1177,0,0.0399452,"Missing"
2021.emnlp-main.45,W19-4427,0,0.0223416,"ut of the model to learn to generate the corresponding question in the fine-tuning stage. We use SQUAD (Rajpurkar et al., 2016) dataset to train and test question generation following the data split in (Du and Cardie, 2018). We report evaluation results in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Grammatical Error Correction is a task that rewrites a potentially erroneous input sentence into a fluent sentence that is grammatical error free without changing the original meaning of the input sentence. Following the recent work (Grundkiewicz et al., 2019; Kiyono et al., 2019; Zhou et al., 2020a) in GEC, we use the public Lang-8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS datasets (Bryant et al., 2019; Granger, 1998) for fine-tuning without using any synthetic GEC data, and then evaluate Max-Match (M2 ) precision, recall, and F0.5 score on the CoNLL-2014 (Ng et al., 2014) test set. Abstractive Summarization aims to rewrite a long document into a short summary. To provide a comparison with the recent work in pretrained models for this task, we present results on two widely used summa"
2021.emnlp-main.45,P84-1044,0,0.174768,"Missing"
2021.emnlp-main.45,2020.tacl-1.5,0,0.0201615,"re diverse training signals. Also, SSR receives complete inputs without artificial masks during pre-training relying solely on monolingual corpus. Pre-training in NLP BERT (Devlin et al., 2019) introduced the masked language modeling objective by masking out certain tokens in a text and predicting them based on their left and right side contexts. Recent work has shown that BERT’s performance can be further improved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2020), and by replacing a consecutive span of tokens with the mask token for MLM training (Joshi et al., 2020). Our approach is also related to ELECTRA (Clark et al., 2020), which uses a pre-trained masked language model to generate fake tokens and train a discriminator to detect them. The key difference is that our approach focuses on span-level texts and trains the model to correct the mistakes instead of simply detecting them, which includes more diverse and informative signals and enables the model to perform text generation tasks in a Seq2Seq fashion. To enable mask language models for natural language generation tasks, Song et al. (2019) used Model Acceleration for PTLMs Recently, a decoder to g"
2021.emnlp-main.45,D16-1139,0,0.0992927,"using transformer models. For example, L=12, H=768 means both the encoder and decoder are built with 12 transformer layers with a hidden size of 768. ∗ The asterisk denotes statistically significant improvement with p-value &lt; 0.05 upon all compared models. • T5-cont: text-to-text transformer initialized by T5 and continually pre-trained with the original text infilling objective with additional training steps. The total number of additional training steps is equal to that of SSR. • DistilT5: the variant that continually pretrains T5 by text infilling with sequencelevel knowledge distillation (Kim and Rush, 2016). This is implemented by using the imperfect text spans generated by T5-large as target outputs for text infilling. DistilT5-small and DistilT5-base are similar to conventional sequence-level knowledge distillation while DistilT5-large can be viewed as continually pre-trained with self-distillation. • DenoiseT5: the variant that injects rule-based noises into plain text and continually pretrain a T5 model to output the original text. The rule-based noises include token shuffling, deletion, and replacement. We adopt the same noise strategy as described in Wang et al. (2019). Then we show additi"
2021.emnlp-main.45,D19-1119,0,0.012307,"generate the corresponding question in the fine-tuning stage. We use SQUAD (Rajpurkar et al., 2016) dataset to train and test question generation following the data split in (Du and Cardie, 2018). We report evaluation results in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Grammatical Error Correction is a task that rewrites a potentially erroneous input sentence into a fluent sentence that is grammatical error free without changing the original meaning of the input sentence. Following the recent work (Grundkiewicz et al., 2019; Kiyono et al., 2019; Zhou et al., 2020a) in GEC, we use the public Lang-8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS datasets (Bryant et al., 2019; Granger, 1998) for fine-tuning without using any synthetic GEC data, and then evaluate Max-Match (M2 ) precision, recall, and F0.5 score on the CoNLL-2014 (Ng et al., 2014) test set. Abstractive Summarization aims to rewrite a long document into a short summary. To provide a comparison with the recent work in pretrained models for this task, we present results on two widely used summarization datasets: CN"
2021.emnlp-main.45,2020.acl-main.703,0,0.368868,"ans that benefit the model to learn 1 Introduction meaningful and diverse rewriting patterns including Text infilling (e.g., masked language modeling) paraphrasing and enhancing the fluency and contexhas become a prevalent learning objective for pre- tual consistency through correcting grammatical, trained language models (PTLMs) (Peters et al., commonsense and factual errors, to improve a text 2018; Radford et al., 2018; Devlin et al., 2019; sequence. These rewriting patterns resemble the Yang et al., 2019; Liu et al., 2019; Lan et al., 2020; goal of various NLG tasks and thus strengthen the Lewis et al., 2020b; Raffel et al., 2019). It provides ability of pre-trained model for downstream appliself-supervision by masking out tokens or spans in cations. text, and trains a model to infill the masked conIn our experiments, we apply SSR to the typitent based on the contexts, accordingly guiding the cal Seq2Seq pre-trained model – T5 (Raffel et al., model for representation learning, as Figure 1(a) 2019) in a continual learning fashion. We show shows. In this paper, we propose to extend the conven- SSR outperforms both the original pre-trained T5 models and their continual training counterparts tional t"
2021.emnlp-main.45,2020.findings-emnlp.165,1,0.749826,"as shown in Figure 1, given a sen- &lt;mask&gt; token during text infilling pre-training. tence “In 2002, Elon Musk founded SpaceX, an For example, for grammatical error correction, the aerospace manufacturer company.”, we randomly input is formatted as “&lt;s1 &gt; I go to school yestersample three text spans (two of them are of length day. &lt;/s1 &gt;” and the output is “&lt;s1 &gt; I went to 1). The masked sentence becomes “In &lt;s1 &gt;, Elon school yesterday.”, which exactly corresponds to Musk &lt;s2 &gt; SpaceX, &lt;s3 &gt; company.” the pre-training format of SSR. 573 In addition, for some constrained text generation tasks (Lin et al., 2020) and controlled text generation (Hu et al., 2017) tasks, we can specify which part of input text to be rewritten with span identifiers. This enables more flexible text generation with Seq2Seq pre-trained models. Taking text attribute transfer as an example, an input example would looks like “Great food &lt;s1 &gt; but very rude &lt;/s1 &gt; waiters.” and the corresponding target sequence is “&lt;s1 &gt; and very friendly”. The inductive bias of span rewriting learned by SSR pre-training naturally benefit these kind of NLG applications. 3.2 Curriculum SSR As mentioned above, we apply SSR as a continual training"
2021.emnlp-main.45,W04-1013,0,0.0429097,"011) and W&I+LOCNESS datasets (Bryant et al., 2019; Granger, 1998) for fine-tuning without using any synthetic GEC data, and then evaluate Max-Match (M2 ) precision, recall, and F0.5 score on the CoNLL-2014 (Ng et al., 2014) test set. Abstractive Summarization aims to rewrite a long document into a short summary. To provide a comparison with the recent work in pretrained models for this task, we present results on two widely used summarization datasets: CNN/DailyMail (Hermann et al., 2015) and XSum (Narayan et al., 2018), and report evaluation results in terms of ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004). Question Generation is to generate valid and flu- 4.3 Compared Models ent questions according to a given passage and We compare SSR with the following models: target answers. It can be considered as rewriting • T5: the original pre-trained text-to-text transa target answer and its surrounding context into a question form. Following previous work (Dong former based on the text infilling objective. 575 Model Architecture Question Generation BLEU-4 METEOR GEC CIDEr P R F0.5 46.68 - 64.9 69.1 26.6 33.7 50.4 57.1 Performance of baseline models without pre-training Zhang and Bansal (2019) Xfmr-big"
2021.emnlp-main.45,D19-1387,0,0.0335607,"Missing"
2021.emnlp-main.45,2021.ccl-1.108,0,0.0660696,"Missing"
2021.emnlp-main.45,I11-1017,0,0.0393082,"(Rajpurkar et al., 2016) dataset to train and test question generation following the data split in (Du and Cardie, 2018). We report evaluation results in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Grammatical Error Correction is a task that rewrites a potentially erroneous input sentence into a fluent sentence that is grammatical error free without changing the original meaning of the input sentence. Following the recent work (Grundkiewicz et al., 2019; Kiyono et al., 2019; Zhou et al., 2020a) in GEC, we use the public Lang-8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS datasets (Bryant et al., 2019; Granger, 1998) for fine-tuning without using any synthetic GEC data, and then evaluate Max-Match (M2 ) precision, recall, and F0.5 score on the CoNLL-2014 (Ng et al., 2014) test set. Abstractive Summarization aims to rewrite a long document into a short summary. To provide a comparison with the recent work in pretrained models for this task, we present results on two widely used summarization datasets: CNN/DailyMail (Hermann et al., 2015) and XSum (Narayan et al., 2018), and report"
2021.emnlp-main.45,D18-1206,0,0.0230991,"ic Lang-8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS datasets (Bryant et al., 2019; Granger, 1998) for fine-tuning without using any synthetic GEC data, and then evaluate Max-Match (M2 ) precision, recall, and F0.5 score on the CoNLL-2014 (Ng et al., 2014) test set. Abstractive Summarization aims to rewrite a long document into a short summary. To provide a comparison with the recent work in pretrained models for this task, we present results on two widely used summarization datasets: CNN/DailyMail (Hermann et al., 2015) and XSum (Narayan et al., 2018), and report evaluation results in terms of ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004). Question Generation is to generate valid and flu- 4.3 Compared Models ent questions according to a given passage and We compare SSR with the following models: target answers. It can be considered as rewriting • T5: the original pre-trained text-to-text transa target answer and its surrounding context into a question form. Following previous work (Dong former based on the text infilling objective. 575 Model Architecture Question Generation BLEU-4 METEOR GEC CIDEr P R F0.5 46.68 - 64.9 69.1 26.6 33.7 50.4 57.1"
2021.emnlp-main.45,W14-1701,0,0.0171311,"is a task that rewrites a potentially erroneous input sentence into a fluent sentence that is grammatical error free without changing the original meaning of the input sentence. Following the recent work (Grundkiewicz et al., 2019; Kiyono et al., 2019; Zhou et al., 2020a) in GEC, we use the public Lang-8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS datasets (Bryant et al., 2019; Granger, 1998) for fine-tuning without using any synthetic GEC data, and then evaluate Max-Match (M2 ) precision, recall, and F0.5 score on the CoNLL-2014 (Ng et al., 2014) test set. Abstractive Summarization aims to rewrite a long document into a short summary. To provide a comparison with the recent work in pretrained models for this task, we present results on two widely used summarization datasets: CNN/DailyMail (Hermann et al., 2015) and XSum (Narayan et al., 2018), and report evaluation results in terms of ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004). Question Generation is to generate valid and flu- 4.3 Compared Models ent questions according to a given passage and We compare SSR with the following models: target answers. It can be considered as rewriting • T"
2021.emnlp-main.45,P02-1040,0,0.115075,"training corpus and optimization steps when employed in a continual pre-training setting. This also reduces recent concerns (Strubell et al., 2019; Bender et al., 2021) about the carbon footprint and energy consumption in LM pre-training. 4.2 Tasks and Datasets et al., 2019), we concatenate the passage and an answer as the input of the model to learn to generate the corresponding question in the fine-tuning stage. We use SQUAD (Rajpurkar et al., 2016) dataset to train and test question generation following the data split in (Du and Cardie, 2018). We report evaluation results in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Grammatical Error Correction is a task that rewrites a potentially erroneous input sentence into a fluent sentence that is grammatical error free without changing the original meaning of the input sentence. Following the recent work (Grundkiewicz et al., 2019; Kiyono et al., 2019; Zhou et al., 2020a) in GEC, we use the public Lang-8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS datasets (Bryant et al., 2019; Granger, 1998) for fine-tuning without using any synthet"
2021.emnlp-main.45,N18-1202,0,0.0621264,"Missing"
2021.emnlp-main.45,D16-1264,0,0.0376143,"omputation cost of using SSR to improve a Seq2Seq pre-trained model is still considerably smaller than the pre-training cost. This is because SSR requires much smaller training corpus and optimization steps when employed in a continual pre-training setting. This also reduces recent concerns (Strubell et al., 2019; Bender et al., 2021) about the carbon footprint and energy consumption in LM pre-training. 4.2 Tasks and Datasets et al., 2019), we concatenate the passage and an answer as the input of the model to learn to generate the corresponding question in the fine-tuning stage. We use SQUAD (Rajpurkar et al., 2016) dataset to train and test question generation following the data split in (Du and Cardie, 2018). We report evaluation results in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Grammatical Error Correction is a task that rewrites a potentially erroneous input sentence into a fluent sentence that is grammatical error free without changing the original meaning of the input sentence. Following the recent work (Grundkiewicz et al., 2019; Kiyono et al., 2019; Zhou et al., 2020a) in GEC, we use the public Lang-8 (Mizumoto et al., 2011), N"
2021.emnlp-main.45,2020.acl-main.593,0,0.0125298,"of a in the input texts and train the models to recover BERT model. DistilBERT (Sanh et al., 2019) and the original texts in an auto-regressive fashion. uses knowledge distillation (Hinton et al., 2015; 572 Romero et al., 2015) to compress BERT. More recently, (Zhou et al., 2021b) proposed Meta Distillation to improve the performance of knowledge distillation for compression BERT. In addition, Xu et al. (2020) introduced progressive module replacing to train more compact BERT models by encouraging the student model to behave similarly with the teacher model. In addition, Zhou et al. (2020c); Schwartz et al. (2020) proposed to accelerate the inference stage of pre-trained models via inputadaptive inference. However, to the best of our knowledge, few studies have been done for accelerating large sequence-to-sequence PTLMs. Our approach can also be used for model compression by using a large pre-trained model as the imperfect span generator. In this way, SSR also exploits the knowledge of a larger model to improve the training of a compact model. Imperfect Span Generation With masked spans, we can generate imperfect text to fill in the spans. Specifically, we feed the masked input into the imperfect span"
2021.emnlp-main.45,P17-1099,0,0.117838,"Missing"
2021.emnlp-main.45,P19-1355,0,0.0120122,"size of 768. ∗ The asterisk denotes statistically significant improvement with p-value &lt; 0.05 upon all compared models. learning rate of 5e-5 with a linear warm-up for the first 8,000 updates. It is noteworthy that although SSR requires using a pre-trained Seq2Seq model for imperfect span generation, the computation cost of using SSR to improve a Seq2Seq pre-trained model is still considerably smaller than the pre-training cost. This is because SSR requires much smaller training corpus and optimization steps when employed in a continual pre-training setting. This also reduces recent concerns (Strubell et al., 2019; Bender et al., 2021) about the carbon footprint and energy consumption in LM pre-training. 4.2 Tasks and Datasets et al., 2019), we concatenate the passage and an answer as the input of the model to learn to generate the corresponding question in the fine-tuning stage. We use SQUAD (Rajpurkar et al., 2016) dataset to train and test question generation following the data split in (Du and Cardie, 2018). We report evaluation results in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Grammatical Error Correction is a task that rewrites"
2021.emnlp-main.45,D19-1412,0,0.01807,"ledge distillation (Kim and Rush, 2016). This is implemented by using the imperfect text spans generated by T5-large as target outputs for text infilling. DistilT5-small and DistilT5-base are similar to conventional sequence-level knowledge distillation while DistilT5-large can be viewed as continually pre-trained with self-distillation. • DenoiseT5: the variant that injects rule-based noises into plain text and continually pretrain a T5 model to output the original text. The rule-based noises include token shuffling, deletion, and replacement. We adopt the same noise strategy as described in Wang et al. (2019). Then we show additional results of SSR-small and SSR-large for further analysis. Summarization Results According to Table 1, it is observed that SSR-base substantially improves the original T5-base model and its continual training variants on both CNN/DM and XSum datasets, and achieves state-of-the-art results for the models of the same size in the abstractive summarization benchmarks. It is notable that our models are only continually pre-trained on a relatively small dataset for only a few number of updates. This confirms the potential of our approach as a general “plugand-play” approach f"
2021.emnlp-main.45,2020.emnlp-main.633,1,0.832781,"t al. et al., 2019) pre-train Seq2Seq models with the text (2019) pruned unnecessary attention heads in the span infilling objective, which removes text spans transformer layers to reduce the parameters of a in the input texts and train the models to recover BERT model. DistilBERT (Sanh et al., 2019) and the original texts in an auto-regressive fashion. uses knowledge distillation (Hinton et al., 2015; 572 Romero et al., 2015) to compress BERT. More recently, (Zhou et al., 2021b) proposed Meta Distillation to improve the performance of knowledge distillation for compression BERT. In addition, Xu et al. (2020) introduced progressive module replacing to train more compact BERT models by encouraging the student model to behave similarly with the teacher model. In addition, Zhou et al. (2020c); Schwartz et al. (2020) proposed to accelerate the inference stage of pre-trained models via inputadaptive inference. However, to the best of our knowledge, few studies have been done for accelerating large sequence-to-sequence PTLMs. Our approach can also be used for model compression by using a large pre-trained model as the imperfect span generator. In this way, SSR also exploits the knowledge of a larger mod"
2021.emnlp-main.45,D19-1253,0,0.0407757,"Missing"
2021.emnlp-main.45,2020.findings-emnlp.30,1,0.914685,"reduce the parameters of a in the input texts and train the models to recover BERT model. DistilBERT (Sanh et al., 2019) and the original texts in an auto-regressive fashion. uses knowledge distillation (Hinton et al., 2015; 572 Romero et al., 2015) to compress BERT. More recently, (Zhou et al., 2021b) proposed Meta Distillation to improve the performance of knowledge distillation for compression BERT. In addition, Xu et al. (2020) introduced progressive module replacing to train more compact BERT models by encouraging the student model to behave similarly with the teacher model. In addition, Zhou et al. (2020c); Schwartz et al. (2020) proposed to accelerate the inference stage of pre-trained models via inputadaptive inference. However, to the best of our knowledge, few studies have been done for accelerating large sequence-to-sequence PTLMs. Our approach can also be used for model compression by using a large pre-trained model as the imperfect span generator. In this way, SSR also exploits the knowledge of a larger model to improve the training of a compact model. Imperfect Span Generation With masked spans, we can generate imperfect text to fill in the spans. Specifically, we feed the masked inpu"
2021.emnlp-main.45,2020.findings-emnlp.136,1,0.845726,"xts and trains the model to correct the mistakes instead of simply detecting them, which includes more diverse and informative signals and enables the model to perform text generation tasks in a Seq2Seq fashion. To enable mask language models for natural language generation tasks, Song et al. (2019) used Model Acceleration for PTLMs Recently, a decoder to generate the masked tokens autore- many attempts have been made to speed up a gressively. UniLM (Dong et al., 2019) multitasks large pre-trained language model (PTLM). To MLM and language modeling objectives. More re- name a few, Shen et al. (2020) quantized BERT cently, BART (Lewis et al., 2020b) and T5 (Raffel to 2-bit using Hessian information; Michel et al. et al., 2019) pre-train Seq2Seq models with the text (2019) pruned unnecessary attention heads in the span infilling objective, which removes text spans transformer layers to reduce the parameters of a in the input texts and train the models to recover BERT model. DistilBERT (Sanh et al., 2019) and the original texts in an auto-regressive fashion. uses knowledge distillation (Hinton et al., 2015; 572 Romero et al., 2015) to compress BERT. More recently, (Zhou et al., 2021b) propo"
2021.emnlp-main.45,P11-1019,0,0.0347076,"generation following the data split in (Du and Cardie, 2018). We report evaluation results in terms of BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). Grammatical Error Correction is a task that rewrites a potentially erroneous input sentence into a fluent sentence that is grammatical error free without changing the original meaning of the input sentence. Following the recent work (Grundkiewicz et al., 2019; Kiyono et al., 2019; Zhou et al., 2020a) in GEC, we use the public Lang-8 (Mizumoto et al., 2011), NUCLE (Dahlmeier et al., 2013), FCE (Yannakoudakis et al., 2011) and W&I+LOCNESS datasets (Bryant et al., 2019; Granger, 1998) for fine-tuning without using any synthetic GEC data, and then evaluate Max-Match (M2 ) precision, recall, and F0.5 score on the CoNLL-2014 (Ng et al., 2014) test set. Abstractive Summarization aims to rewrite a long document into a short summary. To provide a comparison with the recent work in pretrained models for this task, we present results on two widely used summarization datasets: CNN/DailyMail (Hermann et al., 2015) and XSum (Narayan et al., 2018), and report evaluation results in terms of ROUGE-1, ROUGE-2 and ROUGE-L (Lin,"
2021.emnlp-main.771,P17-1176,0,0.0166798,"ode. To facilitate the study of this task, we create a dataset with multiple programming languages. The dataset is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the b"
2021.emnlp-main.771,2020.emnlp-main.728,0,0.0161259,"an appropriate message, while the cascaded model fails may due to the error propagation. More examples on multilingual dataset are shown in Appendix D. 6 Related Work Our work is enlightened from two research lines of studies, which are automated code repair and commit message generation. We discuss these topics in the following. projects with numerous buggy-fixes pairs (Tufano et al., 2018; Chen et al., 2019; Vasic et al., 2019; Yasunaga and Liang, 2020). Tufano et al. (2018) first proposed using end-to-end neural machine translation model for learning bug-fixing patches. Besides, Guo et al. (2020) demonstrated that appropriately incorporating the natural language descriptions into the pre-train model could further improve the performance of code repair. Commit Message Generation Early work on automatic commit message generation translates source code changes (such as feature additions and bug repairs) into natural language based on predefined rules and templates (Buse and Weimer, 2010; Cortés-Coy et al., 2014). To overcome the limitation of high complexity and difficult extensibility, some researchers employ information retrieval methods to generate commit messages, which attempts to r"
2021.emnlp-main.771,D17-1158,0,0.0168867,"a dataset with multiple programming languages. The dataset is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the best performance on code repair task, and the joint method"
2021.emnlp-main.771,P84-1044,0,0.302985,"Missing"
2021.emnlp-main.771,Q17-1024,0,0.0151993,"ination between 9787 cφ = sof tmax zc zTφ √ H ! zφ (9) the output state of commit message decoder and the gated fusion of context representations, which can be calculated as:  T oc = zc + Wo gδ δ + gζ ζ T  Joint Training We jointly train our model in an end-to-end manner, the overall loss is defined as (13) where LR (θ), LC (θ)and LT (θ) are used to optimize the repaired code generation, commit message generation, and binary sequence classification, respectively. When training multilingual model of fixing code and predicting commit message, following multilingual neural machine translation (Johnson et al., 2017), we mix the training corpus and add a special token (e.g., <java>) at the beginning of each input sequence to distinguish from different programming languages. 4 Train Valid Test Total Multi. Python Java Javascript C-sharp Cpp 36682 11129 21446 5424 8510 4585 1391 2680 678 1063 4586 1392 2681 678 1064 45853 13912 26807 6780 10637 Mono. Java 47775 3000 3000 53775 (12) where Wo ∈ RH×H is the learnable weights. denotes the element-wise product. LJ (θ) = LR (θ) + LC (θ) + LT (θ) Languages Data In this section, we describe the creation of the dataset in detail. We first describe how we collect the"
2021.emnlp-main.771,P17-2045,0,0.116157,"ly reduce debugging costs in software development and helps 1 Introduction programmers to understand the high-level rationale Deep learning has been demonstrated remarkably of changes, a lot of great work has been proposed to adept at numerous natural language processing deal with automated program repair (Tufano et al., (NLP) tasks, such as machine translation (Bah- 2018; Chen et al., 2019; Dinella et al., 2020; Yadanau et al., 2014), relation extraction (Zhang et al., sunaga and Liang, 2020; Tang et al., 2021) and 2017), grammar error correction (Ge et al., 2018), commit message generation (Loyola et al., 2017; and so on. The success of deep learning in NLP Liu et al., 2020; Nie et al., 2020), respectively. also promotes the development of which in pro- However, existing work tackles the two tasks ingramming languages (Clement et al., 2020; Lu dependently, ignoring the underlying relationship et al., 2021). Recently, researchers have exploited between these two closely related tasks, e.g., afdeep learning to programming-language related ter fixing the bug, commit message can record the tasks, such as code completion (Svyatkovskiy et al., process of code repair. Therefore it is crucial to 2020), aut"
2021.emnlp-main.771,P16-1162,0,0.561635,"is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the best performance on code repair task, and the joint method behaves better than the cascaded method on commit messag"
2021.emnlp-main.771,2021.findings-acl.111,1,0.811582,"Missing"
2021.emnlp-main.771,P02-1040,0,0.109484,"e, and commit message. The statistics of the dataset used in this paper are summarized in Table 1. More processing details and statistics can be found in Appendix A and Appendix B. We release the datasets at https: //github.com/jqbemnlp/BFCsData. 5 5.1 Experiments Experimental Settings Evaluation Metrics We conduct evaluations on both code repair and commit message generation. For the code repair, we use exact match accuracy (Chen et al., 2018) to measure the percentage of the predicted fixed code that are exactly matching the truth fixed code. In addition, we also introduce the BLEU-4 score (Papineni et al., 2002) as a supplementary metric to evaluate their partial match. For the commit message generation, we use BLEU-4 and Rouge-L (Lin, 2004) to evaluate our model. 3 https://www.githubarchive.org https://docs.github.com/en/ free-pro-team@latest/rest 4 5 https://sites.google.com/view/ learning-fixes/data 9788 Models Naive Method Oracle Method Cascaded Model + Teacher-student + Multitask + Back-translation Joint Model Automated Code Repair BLEU-4 xMatch 87.45 0.00 85.07 3.21 88.23 6.16 87.94 8.33 87.73 5.26 87.61 8.01 Commit Message Generation BLEU-4 ROUGE-L 8.40 7.98 12.64 11.59 9.69 9.41 10.58 10.19 1"
2021.emnlp-main.771,D17-1182,0,0.0279627,"Missing"
2021.emnlp-main.771,W16-2323,0,0.311742,"is collected from commit and buggy-fixed histories of open-source software projects, where each example consists of buggy code, fixed code, and the corresponding commit message. We first introduce the cascaded methods as baseline. The cascaded model employs one model to repair code and the other to generate commit message successively. We enhance this cascaded model with three training approaches inspired by the low-resource machine translation, including the teacher-student method (Chen et al., 2017), the multi-task learning method (Domhan and Hieber, 2017), and the back-translation method (Sennrich et al., 2016a). To deal with the error propagation problem of the cascaded method, we propose a joint model which can achieve both code repair and commit message generation in a single model. We train and evaluate our model using the created triple (buggy-fixed-commit) dataset. The results demonstrate the validity of our proposed methods, which achieve a significant improvement over baseline in both qualities of code and commit messages. Particularly, the enhanced cascaded method obtains the best performance on code repair task, and the joint method behaves better than the cascaded method on commit messag"
2021.emnlp-main.832,2020.findings-emnlp.372,0,0.0600466,"Missing"
2021.emnlp-main.832,2020.acl-main.593,0,0.0402095,"Missing"
2021.emnlp-main.832,N18-1101,0,0.0904588,"Missing"
2021.emnlp-main.832,D13-1170,0,0.00933117,"es. Interestingly, compared to Pure KD where we directly optimize the KL divergence, DistilBERT, TinyBERT and BERT-PKD trade some loyalty in exchange for accuracy. Compared to DistilBERT, TinyBERT digs up higher accuracy by introducing layer-to-layer distillation, with their loyalty remains identical. Also, we do not observe a significant difference between pretraining KD and downstream KD in terms of both loyalty and robustness (p &gt; 0.1). Notably, BERT-of-Theseus has a significantly lower loyalty, suggesting the mechanism behind it is different from KD. We also provide some results on SST-2 (Socher et al., 2013) in Appendix B. 5 Combining the Bag of Tricks Method Speed MNLI L-L P-L AA #Q Teacher 1.0× 84.5 / 83.3 100 100 8.1 89.6 Head Prune +Finetune +KD +KD+PTQ 1.2× 1.2× 1.2× 2.2× 80.9 / 80.6 83.2 / 81.9 84.2 / 83.0 80.8 / 80.4 87.8 89.1 93.3 89.6 85.5 85.5 93.0 86.3 9.1 7.2 8.3 38.4 90.5 83.2 90.5 90.9 Q8-QAT Q8-PTQ +Finetune +KD 1.8× 1.8× 1.8× 1.8× 83.4 / 82.4 80.7 / 80.4 82.9 / 81.9 84.1 / 83.5 89.7 89.6 89.7 94.0 88.2 80.8 84.8 93.9 6.8 40.2 7.1 7.5 82.7 91.6 84.5 86.1 BERT-PKD Theseus +KD +KD+PTQ 2.0× 2.0× 2.0× 3.6× 81.3 / 81.1 81.8 / 80.7 82.6 / 81.7 80.2 / 79.9 88.9 88.1 91.2 89.5 89.0 82.5 91"
2021.emnlp-main.832,P19-1355,0,0.0272379,"ning multiple compression techniques, we provide a practical strategy to achieve better accuracy, loyalty and robustness.1 1 Pruning Knowledge Distillation Module Replacing Original Model (0.7, 0.2, 0.1) Compressed Model Probability Loyalty (0.6, 0.25, 0.15) Predicted probability distribution Entailment Label Introduction Recently, many large pretrained language models (PLMs, Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Shoeybi et al., 2019; Raffel et al., 2020) have been proposed for a variety of Natural Language Processing (NLP) tasks. However, as pointed out in recent studies (Strubell et al., 2019; Schwartz et al., 2020a; Bender et al., 2021), these models suffer from computational inefficiency and high ecological cost. Many attempts have been made to address this problem, including quantization (Zafrir et al., 2019; Shen et al., 2020), pruning (Michel et al., 2019; Sanh et al., 2020), knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019, 2020; Turc et al., 2019; Jiao et al., 2020; Wang et al., 2020; Zhou et al., 2021) and progressive module replacing (Xu et al., 2020). BERT (Devlin et al., 2019) is a representative PLM. Many works compressing BERT use preserved accuracy wi"
2021.emnlp-main.832,D19-1441,0,0.359424,"models (PLMs, Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Shoeybi et al., 2019; Raffel et al., 2020) have been proposed for a variety of Natural Language Processing (NLP) tasks. However, as pointed out in recent studies (Strubell et al., 2019; Schwartz et al., 2020a; Bender et al., 2021), these models suffer from computational inefficiency and high ecological cost. Many attempts have been made to address this problem, including quantization (Zafrir et al., 2019; Shen et al., 2020), pruning (Michel et al., 2019; Sanh et al., 2020), knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019, 2020; Turc et al., 2019; Jiao et al., 2020; Wang et al., 2020; Zhou et al., 2021) and progressive module replacing (Xu et al., 2020). BERT (Devlin et al., 2019) is a representative PLM. Many works compressing BERT use preserved accuracy with computational complexity (e.g., speed-up ratio, FLOPS, number of parameters) as metrics to evaluate compression. This ∗ Equal Contribution. Work done at Microsoft Research Asia. 1 Our code is available at https://github.com/ JetRunner/beyond-preserved-accuracy. Label Loyalty Adversarial Attack Neutral Robustness Figure 1: Three metrics to evaluate the co"
2021.emnlp-main.832,2020.acl-main.195,0,0.0353716,"Missing"
2021.emnlp-main.832,2020.sustainlp-1.11,0,0.0323412,"calculated in the same way as accuracy, but between the student’s prediction and the teacher’s prediction, instead of ground labels: Ll = Accuracy(pred t , pred s ) (1) where pred t and pred s are the predictions of the teacher and student, respectively. 3.1.2 Probability Loyalty Except for the label correspondence, we argue that the predicted probability distribution matters as well. In industrial applications, calibration (Guo et al., 2017; Li et al., 2020), which focuses on the meaningfulness of confidence, is an important issue for deployment. Many dynamic inference acceleration methods (Xin et al., 2020b; Schwartz et al., 2020b; Liu et al., 2020; Xin et al., 2020a; Li et al., 2020) use entropy or the maximum value of the predicted probability distribution as the signal for early exiting. Thus, a shift of predicted probability distribution in a compressed model could break the calibration and invalidate calibrated early exiting pipelines. Kullback–Leibler (KL) divergence is often used to measure how one probability distribution is different from a reference distribution.   X P (x) DKL (P kQ) = P (x) log (2) Q(x) x∈X where X is the probability space; P and Q are predicted probability distrib"
2021.emnlp-main.832,2020.acl-main.204,0,0.0187164,"calculated in the same way as accuracy, but between the student’s prediction and the teacher’s prediction, instead of ground labels: Ll = Accuracy(pred t , pred s ) (1) where pred t and pred s are the predictions of the teacher and student, respectively. 3.1.2 Probability Loyalty Except for the label correspondence, we argue that the predicted probability distribution matters as well. In industrial applications, calibration (Guo et al., 2017; Li et al., 2020), which focuses on the meaningfulness of confidence, is an important issue for deployment. Many dynamic inference acceleration methods (Xin et al., 2020b; Schwartz et al., 2020b; Liu et al., 2020; Xin et al., 2020a; Li et al., 2020) use entropy or the maximum value of the predicted probability distribution as the signal for early exiting. Thus, a shift of predicted probability distribution in a compressed model could break the calibration and invalidate calibrated early exiting pipelines. Kullback–Leibler (KL) divergence is often used to measure how one probability distribution is different from a reference distribution.   X P (x) DKL (P kQ) = P (x) log (2) Q(x) x∈X where X is the probability space; P and Q are predicted probability distrib"
2021.emnlp-main.832,2020.emnlp-main.633,1,0.886173,"for a variety of Natural Language Processing (NLP) tasks. However, as pointed out in recent studies (Strubell et al., 2019; Schwartz et al., 2020a; Bender et al., 2021), these models suffer from computational inefficiency and high ecological cost. Many attempts have been made to address this problem, including quantization (Zafrir et al., 2019; Shen et al., 2020), pruning (Michel et al., 2019; Sanh et al., 2020), knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019, 2020; Turc et al., 2019; Jiao et al., 2020; Wang et al., 2020; Zhou et al., 2021) and progressive module replacing (Xu et al., 2020). BERT (Devlin et al., 2019) is a representative PLM. Many works compressing BERT use preserved accuracy with computational complexity (e.g., speed-up ratio, FLOPS, number of parameters) as metrics to evaluate compression. This ∗ Equal Contribution. Work done at Microsoft Research Asia. 1 Our code is available at https://github.com/ JetRunner/beyond-preserved-accuracy. Label Loyalty Adversarial Attack Neutral Robustness Figure 1: Three metrics to evaluate the compressed models beyond preserved accuracy. For each input, label and probability loyalty measure the shift of label and predicted prob"
2021.findings-acl.188,S17-2001,0,0.0155284,"l., 2019; Conneau et al., 2019) and task-agnostic distillation (Sun et al., 2019b; Jiao et al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92.6 94.0 91.6 91.8 76.4 83.1 93.3 95.9 89.2 91.2 62.3 65.0 90.5 91.3 84.9 87.6 Table 3: Results of our 12×768 models on the dev sets"
2021.findings-acl.188,2021.naacl-main.280,1,0.810564,"Missing"
2021.findings-acl.188,P19-4007,0,0.249501,"en split them according to the desired number of relation heads. We choose to transfer Q-Q, K-K and V-V self-attention relations to achieve a balance between performance and training speed. For large-size teacher, we transfer the self-attention knowledge of an upper middle layer of the teacher. For base-size teacher, using the last layer achieves better performance. 2019a) employ a standard encoder-decoder structure and pretrain the decoder auto-regressively. Besides monolingual pretrained models, multilingual pretrained models (Devlin et al., 2018; Lample and Conneau, 2019; Chi et al., 2019; Conneau et al., 2019; Chi et al., 2020) also advance the state-of-theart on cross-lingual understanding and generation. 2.3 Knowledge Distillation Knowledge distillation has been proven to be a promising way to compress large models while maintaining accuracy. Knowledge of a single or an ensemble of large models is used to guide the training of small models. Hinton et al. (2015) propose to use soft target probabilities to train student models. More fine-grained knowledge such as hidden states (Romero et al., 2015) and attention distributions (Zagoruyko and Komodakis, 2017; Hu et al., 2018) are introduced to impro"
2021.findings-acl.188,D18-1269,0,0.0264397,"Avg M INI LM (Last Layer) + Upper Middle Layer 79.1 80.3 84.7 85.2 91.2 91.5 85.0 85.7 12×384 Ours 80.7 85.7 92.3 86.2 Table 4: Comparison of different methods using BERTLARGE-WWM as the teacher. We report dev results of 12×384 student model with 128 embedding size. et al., 2009) and WNLI (Levesque et al., 2012)). Extractive Question Answering The task aims to predict a continuous sub-span of the passage to answer the question. We evaluate on SQuAD 2.0 (Rajpurkar et al., 2018), which has been served as a major question answering benchmark. Cross-lingual Natural Language Inference (XNLI) XNLI (Conneau et al., 2018) is a cross-lingual classification benchmark. It aims to identity the semantic relationship between two sentences and provides instances in 15 languages. Cross-lingual Question Answering We use MLQA (Lewis et al., 2019b) to evaluate multilingual models. MLQA extends English SQuAD dataset (Rajpurkar et al., 2016) to seven languages. 4.3 Main Results Table 1 presents the dev results of 6×384 and 6×768 models distilled from BERTBASE , BERTLARGE and RoBERTaLARGE on GLUE and SQuAD 2.0. (1) Previous methods (Sanh et al., 2019; Jiao et al., 2019; Sun et al., 2019a; Wang et al., 2020) usually distill"
2021.findings-acl.188,I05-5002,0,0.0319675,"ing (Devlin et al., 2018; Liu et al., 2019; Conneau et al., 2019) and task-agnostic distillation (Sun et al., 2019b; Jiao et al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92.6 94.0 91.6 91.8 76.4 83.1 93.3 95.9 89.2 91.2 62.3 65.0 90.5 91.3 84.9 87.6 Table 3: Results of our 12×7"
2021.findings-acl.188,W07-1401,0,0.0271052,"k and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92.6 94.0 91.6 91.8 76.4 83.1 93.3 95.9 89.2 91.2 62.3 65.0 90.5 91.3 84.9 87.6 Table 3: Results of our 12×768 models on the dev sets of GLUE benchmark and SQuAD 2.0. The fine-tuning results are an average of 4 runs for each task. We report F1 for SQuAD 2.0, Pearson correlation for STS-B, Matthews corre"
2021.findings-acl.188,D18-1232,1,0.896184,"Missing"
2021.findings-acl.188,2020.emnlp-main.242,0,0.0131517,"t probabilities to train student models. More fine-grained knowledge such as hidden states (Romero et al., 2015) and attention distributions (Zagoruyko and Komodakis, 2017; Hu et al., 2018) are introduced to improve the student model. In this work, we focus on task-agnostic knowledge distillation of pretrained Transformers. The distilled task-agnostic model can be fine-tuned to adapt to downstream tasks. It can also be utilized to initialize task-specific distillation (Sun et al., 2019a; Turc et al., 2019; Aguilar et al., 2019; Mukherjee and Awadallah, 2020; Xu et al., 2020; Hou et al., 2020; Li et al., 2020), which uses a finetuned teacher model to guide the training of the student on specific tasks. Knowledge used for distillation and layer mapping function are two key points for task-agnostic distillation of pretrained Transformers. Most previous work uses soft target probabilities, hidden states, self-attention distributions and value-relation to train the student model. For the layer mapping function, TinyBERT (Jiao et al., 2019) uses a uniform strategy to map teacher and student layers. MobileBERT (Sun et al., 2019b) assumes the student has the same number of layers as its teacher to perform"
2021.findings-acl.188,2021.ccl-1.108,0,0.0476769,"Missing"
2021.findings-acl.188,2020.acl-main.202,0,0.139384,"ng of small models. Hinton et al. (2015) propose to use soft target probabilities to train student models. More fine-grained knowledge such as hidden states (Romero et al., 2015) and attention distributions (Zagoruyko and Komodakis, 2017; Hu et al., 2018) are introduced to improve the student model. In this work, we focus on task-agnostic knowledge distillation of pretrained Transformers. The distilled task-agnostic model can be fine-tuned to adapt to downstream tasks. It can also be utilized to initialize task-specific distillation (Sun et al., 2019a; Turc et al., 2019; Aguilar et al., 2019; Mukherjee and Awadallah, 2020; Xu et al., 2020; Hou et al., 2020; Li et al., 2020), which uses a finetuned teacher model to guide the training of the student on specific tasks. Knowledge used for distillation and layer mapping function are two key points for task-agnostic distillation of pretrained Transformers. Most previous work uses soft target probabilities, hidden states, self-attention distributions and value-relation to train the student model. For the layer mapping function, TinyBERT (Jiao et al., 2019) uses a uniform strategy to map teacher and student layers. MobileBERT (Sun et al., 2019b) assumes the student ha"
2021.findings-acl.188,P18-2124,0,0.0738541,"Missing"
2021.findings-acl.188,D16-1264,0,0.303169,"al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92.6 94.0 91.6 91.8 76.4 83.1 93.3 95.9 89.2 91.2 62.3 65.0 90.5 91.3 84.9 87.6 Table 3: Results of our 12×768 models on the dev sets of GLUE benchmark and SQuAD 2.0. The fine-tuning results are an average of 4 runs for each task."
2021.findings-acl.188,D13-1170,0,0.00971792,"ts using 8 V100 GPUs with mixed precision training. 4.2 Downstream Tasks Following previous pre-training (Devlin et al., 2018; Liu et al., 2019; Conneau et al., 2019) and task-agnostic distillation (Sun et al., 2019b; Jiao et al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92"
2021.findings-acl.188,D19-1441,0,0.363617,"rs 6×768 Ours BERTBASE BERTLARGE RoBERTaLARGE 66M 66M 81M ×2.0 ×2.0 ×2.0 76.3 77.7 81.6 84.2 85.0 87.0 90.8 91.4 92.7 91.1 91.1 91.4 72.1 73.0 78.7 92.4 92.5 94.5 88.9 88.9 90.4 52.5 53.9 54.0 81.0 81.7 83.8 BERTBASE RoBERTaBASE BERTSMALL Truncated BERTBASE Truncated RoBERTaBASE DistilBERT TinyBERT M INI LM Table 1: Results of our students distilled from base-size and large-size teachers on the development sets of GLUE and SQuAD 2.0. We report F1 for SQuAD 2.0, Matthews correlation coefficient for CoLA, and accuracy for other datasets. The GLUE results of DistilBERT are taken from Sanh et al. (2019). The rest results of DistilBERT, TinyBERT2 , BERTSMALL , Truncated BERTBASE and M INI LM are taken from Wang et al. (2020). BERTSMALL (Turc et al., 2019) is trained using the MLM objective, without using KD. We also report the results of truncated BERTBASE and truncated RoBERTaBASE , which drops the top 6 layers of the base model. Top-layer dropping has been proven to be a strong baseline (Sajjad et al., 2020). The fine-tuning results are an average of 4 runs. the restriction on the number of attention heads of students, which is required to be the same as its teacher. To introduce more fine-"
2021.findings-acl.188,N18-1101,0,0.0215865,"on (Sun et al., 2019b; Jiao et al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering. The multilingual models are evaluated on cross-lingual natural language inference and cross-lingual question answering. GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classification tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli 2144 Model Teacher #Param SQuAD2 MNLI QNLI QQP RTE SST MRPC CoLA STS Avg BERTBASE RoBERTaBASE - 109M 125M 76.8 83.7 84.5 87.6 91.7 92.8 91.3 91.9 68.6 78.7 93.2 94.8 87.3 90.2 58.9 63.6 89.5 91.2 82.4 86.1 12×768 Ours 12×768 Ours BERTLARGE RoBERTaLARGE 109M 125M 81.8 86.6 86.5 89.4 92.6 94.0 91.6 91.8 76.4 83.1 93.3 95.9 89.2 91.2 62.3 65.0 90.5 91.3 84.9 87.6 Table 3: Results of our 12×768 models on the dev sets of GLUE benchmark and SQuAD 2.0. The fine-tuning results are an a"
2021.findings-acl.188,2020.emnlp-main.633,1,0.858797,"l. (2015) propose to use soft target probabilities to train student models. More fine-grained knowledge such as hidden states (Romero et al., 2015) and attention distributions (Zagoruyko and Komodakis, 2017; Hu et al., 2018) are introduced to improve the student model. In this work, we focus on task-agnostic knowledge distillation of pretrained Transformers. The distilled task-agnostic model can be fine-tuned to adapt to downstream tasks. It can also be utilized to initialize task-specific distillation (Sun et al., 2019a; Turc et al., 2019; Aguilar et al., 2019; Mukherjee and Awadallah, 2020; Xu et al., 2020; Hou et al., 2020; Li et al., 2020), which uses a finetuned teacher model to guide the training of the student on specific tasks. Knowledge used for distillation and layer mapping function are two key points for task-agnostic distillation of pretrained Transformers. Most previous work uses soft target probabilities, hidden states, self-attention distributions and value-relation to train the student model. For the layer mapping function, TinyBERT (Jiao et al., 2019) uses a uniform strategy to map teacher and student layers. MobileBERT (Sun et al., 2019b) assumes the student has the same number"
2021.findings-acl.188,D19-1374,0,0.0164205,"inilm. 1 ing in real-life applications due to the restrictions of computation resources and latency. Knowledge distillation (KD; Hinton et al. 2015, Romero et al. 2015) has been widely employed to compress pretrained Transformers, which transfers knowledge of the large model (teacher) to the small model (student) by minimizing the differences between teacher and student features. Soft target probabilities (soft labels) and intermediate representations are usually utilized to perform KD training. In this work, we focus on task-agnostic compression of pretrained Transformers (Sanh et al., 2019; Tsai et al., 2019; Jiao et al., 2019; Sun et al., 2019b; Wang et al., 2020). The student models are distilled from large pretrained Transformers using large-scale text corpora. The distilled task-agnostic model can be directly finetuned on downstream tasks, and can be utilized to initialize task-specific distillation. DistilBERT (Sanh et al., 2019) uses soft target probabilities for masked language modeling predictions and embedding outputs to train the student. The student model is initialized from the teacher by taking one layer out of two. TinyBERT (Jiao et al., 2019) utilizes hidden states and self-attenti"
2021.findings-acl.372,D18-1045,0,0.017158,"bserved in So 4260 BLEU with different model size WMT'14 En-De BLEU 30 28 26 24 22 0 25 50 75 100 125 Model size (million) Transformer DARTSformer 150 175 200 Figure 5: BLEU comparison between DARTSformer and standard Transformers with different model sizes. et al. (2019). Based on this observation, DARTSformer is more pronounced for environments with resource limitations, such as mobile phones. A possible reason for the decreased performance gap at larger model sizes is that the effect of overfitting becomes more important. We expect that some data augmentation skills (Sennrich et al., 2015; Edunov et al., 2018; Qu et al., 2020) might be of help. 4.4 e d BLEU Tiny Small Medium DARTSformer 128 128 256 256 120 240 480 960 24.2 26.3 27.5 28.4 DARTS + Transformer Transformer 320 - 320 - 27.7 27.7 Search Settings The Impact of Search Hidden Size The main motivation for our presented method is that we want to search in a large hidden size to reduce the performance gap between searching and re-training. However, whether this gap exists needs rigorous validation. Otherwise, it would suffice to instead use a small hidden size d in architecture search, and then increase d after search for training the actual"
2021.findings-acl.372,D19-1367,0,0.0244248,"0 GPU. Limited by GPU memory, DARTS in Transformers has to search in small sizes while evaluating in large sizes, which will cause performance gaps (Chen et al., 2019). Introduction Current neural architecture search (NAS) studies have produced models that surpass the performance of those designed by humans (Real et al., 2019; Lu et al., 2020). For sequence tasks, efforts are made in reinforcement learning-based (Pham et al., 2018) and evolution-based (So et al., 2019; Wang et al., 2020) methods, which suffer from the huge computational cost. Instead, gradient-based methods (Liu et al., 2018; Jiang et al., 2019; Yang et al., 2020) are less demanding in computing resources and easy to implement, attracting many attentions recently. The idea of gradient-based NAS is to train a super network covering all candidate operations. Different sub-graphs of the super network form the search space. To find a well-performing subgraph, Liu et al. (2018) (DARTS) introduced search parameters jointly optimized with the network weights. Operations corresponding to the largest search parameters are kept for each intermediate node after searching. A limitation of DARTS is its memory inefficiency because it needs to sto"
2021.findings-acl.372,D19-1437,0,0.0271931,"l. (2019) invented different reversible architectures based on the ResNet (He et al., 2016). MacKay et al. (2018) extended RevNets to the recurrent network, which is particularly memory-efficient. Bai et al. (2019, 2020) conducted experiments with reversible Transformers by fixed point iteration. Kitaev et al. (2020) combined local sensitive hashing attention with reversible transformers to save memory in training with long sequences. An important application of reversible networks is the flow-based models (Kingma and Dhariwal, 2018; Huang et al., 2018; Tran et al., 2019). For sequence tasks, Ma et al. (2019) achieved success in non-autoregressive machine translation. 6 Conclusion We have proposed a memory-efficient differentiable architecture search (DARTS) method on sequence-to-sequence tasks. In particular, we have first devised a multi-split reversible network whose intermediate layer outputs can be reconstructed from top to bottom by the last layer’s output. We have then combined this reversible network with DARTS and developed a backpropagation-withreconstruction algorithm to significantly relieve the memory burden during the gradient-based architecture search process. We have validated the"
2021.findings-acl.372,C18-1250,0,0.0273577,"the network weights simultaneously. After searching, one can have a ready-to-run network. We use a simple one-stage NAS algorithm (Guo et al., 2020) as a baseline in Section 4.1. (Z190001), National Key Research and Development Project of China (No. 2018AAA0101004), and Beijing Academy of Artificial Intelligence (BAAI). References Reversible networks The idea of reversible networks is first introduced by RevNets (Gomez et al., 2017). Later on, Jacobsen et al. (2018); Chang et al. (2018); Behrmann et al. (2019) invented different reversible architectures based on the ResNet (He et al., 2016). MacKay et al. (2018) extended RevNets to the recurrent network, which is particularly memory-efficient. Bai et al. (2019, 2020) conducted experiments with reversible Transformers by fixed point iteration. Kitaev et al. (2020) combined local sensitive hashing attention with reversible transformers to save memory in training with long sequences. An important application of reversible networks is the flow-based models (Kingma and Dhariwal, 2018; Huang et al., 2018; Tran et al., 2019). For sequence tasks, Ma et al. (2019) achieved success in non-autoregressive machine translation. 6 Conclusion We have proposed a memo"
2021.findings-acl.372,P02-1040,0,0.114487,"sists of 36 million training sentence pairs. (3) WMT’18 English-Czech (En-Cs), again without ParaCrawl, which consists of 15.8 million training sentence pairs. Tokenization is done by Moses2 . We employ BPE (Sennrich et al., 2016) to generate a shared vocabulary for each language pair. The BPE merge operation numbers are 32K (WMT’18 En-De), 40K (WMT’14 En-Fr), 32K (WMT’18 En-Cs). We discard sentences longer than 250 tokens. For the retraining validation set, we randomly choose 3300 2 https://github.com/moses-smt/mosesdecoder sentence pairs from the training set. The evaluation metric is BLEU (Papineni et al., 2002). We use beam search for test sets with a beam size of 5, and we tune the length penalty parameter from 0.5 to 1.0. Suppose the input length is m, and the maximum output length is 1.2m + 10. 3.2 Search Configuration The architecture searches are all run on WMT’14 En-De. DARTS is a bilevel optimization process, which updates network weights θ on one dataset and search parameters α on another dataset. We split the 4.5 million sentence pairs into 2.5/2.0 million for θ and α. Both Ltrain and Lval are cross entropy loss with a label smoothing factor of 0.1. The split number n is 2 for the encoder a"
2021.findings-acl.372,P16-1009,0,0.0703353,"Missing"
2021.findings-acl.372,P16-1162,0,0.0342308,"he search process is the most memory intensive part, such that we use BP-with-reconstruction as shown in Line 2-5 of Algorithm 2. 3 Experiment Setup 3.1 Datasets We use three standard datasets to perform our experiments as So et al. (2019): (1) WMT’18 EnglishGerman (En-De) without ParaCrawl, which consists of 4.5 million training sentence pairs. (2) WMT’14 French-English (En-Fr), which consists of 36 million training sentence pairs. (3) WMT’18 English-Czech (En-Cs), again without ParaCrawl, which consists of 15.8 million training sentence pairs. Tokenization is done by Moses2 . We employ BPE (Sennrich et al., 2016) to generate a shared vocabulary for each language pair. The BPE merge operation numbers are 32K (WMT’18 En-De), 40K (WMT’14 En-Fr), 32K (WMT’18 En-Cs). We discard sentences longer than 250 tokens. For the retraining validation set, we randomly choose 3300 2 https://github.com/moses-smt/mosesdecoder sentence pairs from the training set. The evaluation metric is BLEU (Papineni et al., 2002). We use beam search for test sets with a beam size of 5, and we tune the length penalty parameter from 0.5 to 1.0. Suppose the input length is m, and the maximum output length is 1.2m + 10. 3.2 Search Config"
2021.findings-acl.372,P19-1355,0,0.0673408,"Missing"
2021.findings-acl.372,2020.acl-main.686,0,0.17934,"bone model of DARTS. Experiments are run on a single step of forward-backward pass on a batch of 3584 tokens with a NVIDIA P100 GPU. Limited by GPU memory, DARTS in Transformers has to search in small sizes while evaluating in large sizes, which will cause performance gaps (Chen et al., 2019). Introduction Current neural architecture search (NAS) studies have produced models that surpass the performance of those designed by humans (Real et al., 2019; Lu et al., 2020). For sequence tasks, efforts are made in reinforcement learning-based (Pham et al., 2018) and evolution-based (So et al., 2019; Wang et al., 2020) methods, which suffer from the huge computational cost. Instead, gradient-based methods (Liu et al., 2018; Jiang et al., 2019; Yang et al., 2020) are less demanding in computing resources and easy to implement, attracting many attentions recently. The idea of gradient-based NAS is to train a super network covering all candidate operations. Different sub-graphs of the super network form the search space. To find a well-performing subgraph, Liu et al. (2018) (DARTS) introduced search parameters jointly optimized with the network weights. Operations corresponding to the largest search parameters"
2021.findings-acl.394,W07-1401,0,0.155731,"8 66.7 67.3 91.0 90.9 Model Table 1: Comparisons between our models and previous pretrained models on GLUE dev set. Reported results are medians over five random seeds. on both the base-size and small-size model, the best configuration is γ = 1. The detailed pre-training configurations are provided in the supplemental materials. 5.2 Results on GLUE Benchmark The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) is a collection of diverse natural language understanding (NLU) tasks, including inference tasks (MNLI, QNLI, RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018; Rajpurkar et al. 2016b), similarity and paraphrase tasks (MRPC, QQP, STSB; Dolan and Brockett 2005; Cer et al. 2017), and single-sentence tasks (CoLA, SST-2; Warstadt et al. 2018; Socher et al. 2013). The detailed descriptions of GLUE datasets are provided in the supplementary materials. The evaluation metrics are Spearman correlation for STS-B, Matthews correlation for CoLA, and accuracy for the other GLUE tasks. For small-size settings, we use the hyperparameter configuration as suggested in (Clark et al., 2020a). For base-size settings, we con"
2021.findings-acl.394,S17-2001,0,0.0337325,"both the base-size and small-size model, the best configuration is γ = 1. The detailed pre-training configurations are provided in the supplemental materials. 5.2 Results on GLUE Benchmark The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) is a collection of diverse natural language understanding (NLU) tasks, including inference tasks (MNLI, QNLI, RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018; Rajpurkar et al. 2016b), similarity and paraphrase tasks (MRPC, QQP, STSB; Dolan and Brockett 2005; Cer et al. 2017), and single-sentence tasks (CoLA, SST-2; Warstadt et al. 2018; Socher et al. 2013). The detailed descriptions of GLUE datasets are provided in the supplementary materials. The evaluation metrics are Spearman correlation for STS-B, Matthews correlation for CoLA, and accuracy for the other GLUE tasks. For small-size settings, we use the hyperparameter configuration as suggested in (Clark et al., 2020a). For base-size settings, we consider a limited hyperparameter searching for each task, with learning rates ∈ {5e-5, 1e-4, 1.5e-4} and training epochs ∈ {3, 4, 5}. The remaining hyperparameters ar"
2021.findings-acl.394,2020.emnlp-main.20,0,0.211754,"t al., 2020; Raffel et al., 2020). The most representative pretext task is masked language modeling (MLM), which is introduced to pretrain a bidirectional BERT (Devlin et al., 2019) encoder. RoBERTa (Liu et al., 2019) apply several strategies to enhance the BERT performance, including training with more data and dynamic masking. UniLM (Dong et al., 2019; Bao et al., 2020) extend the mask prediction to generation tasks by adding the auto-regressive objectives. XLNet (Yang et al., 2019) propose the permuted language modeling to learn the dependencies among the masked tokens. Besides, E LEC TRA (Clark et al., 2020a) propose a novel training objective called replaced token detection which is defined over all input tokens. Moreover, E LEC TRIC (Clark et al., 2020b) extends the idea of E LEC TRA by energy-based cloze models. Some prior efforts demonstrate that sampling more hard examples is conducive to more effective training. Lin et al. (2017) propose the focal loss in order to focus on more hard examples. Generative adversarial networks (Goodfellow et al., 2014) is trained to maximize the probability of the discriminator making a mistake, which is closely related to E LECTRA’s training framework. In th"
2021.findings-acl.394,2021.ccl-1.108,0,0.0456304,"Missing"
2021.findings-acl.394,N18-1202,0,0.0406331,"to E LECTRA for training from scratch. Experimental results on various tasks show that our methods outperform E LECTRA despite the simplicity. Specifically, under the small-size setting, our model performance is 0.9 higher than E LECTRA on MNLI (Williams et al., 2018) and 4.2 higher on SQuAD 2.0 (Rajpurkar et al., 2016a), respectively. Under the base-size setting, our model performance is 0.26 higher than E LECTRA on MNLI and 0.52 higher on SQuAD 2.0, respectively. 2 Related Work State-of-the-art NLP models are mostly pretrained on a large unlabeled corpus with the self-supervised objectives (Peters et al., 2018; Lan et al., 2020; Raffel et al., 2020). The most representative pretext task is masked language modeling (MLM), which is introduced to pretrain a bidirectional BERT (Devlin et al., 2019) encoder. RoBERTa (Liu et al., 2019) apply several strategies to enhance the BERT performance, including training with more data and dynamic masking. UniLM (Dong et al., 2019; Bao et al., 2020) extend the mask prediction to generation tasks by adding the auto-regressive objectives. XLNet (Yang et al., 2019) propose the permuted language modeling to learn the dependencies among the masked tokens. Besides, E LE"
2021.findings-acl.394,D16-1264,0,0.269278,"ing cross-entropy loss. The method adaptively downweights the well-predicted replacements for MLM, which avoids sampling too many correct tokens as replacements. We conduct pre-training experiments on the WikiBooks corpus for both small-size and base-size models. The proposed techniques are plugged into E LECTRA for training from scratch. Experimental results on various tasks show that our methods outperform E LECTRA despite the simplicity. Specifically, under the small-size setting, our model performance is 0.9 higher than E LECTRA on MNLI (Williams et al., 2018) and 4.2 higher on SQuAD 2.0 (Rajpurkar et al., 2016a), respectively. Under the base-size setting, our model performance is 0.26 higher than E LECTRA on MNLI and 0.52 higher on SQuAD 2.0, respectively. 2 Related Work State-of-the-art NLP models are mostly pretrained on a large unlabeled corpus with the self-supervised objectives (Peters et al., 2018; Lan et al., 2020; Raffel et al., 2020). The most representative pretext task is masked language modeling (MLM), which is introduced to pretrain a bidirectional BERT (Devlin et al., 2019) encoder. RoBERTa (Liu et al., 2019) apply several strategies to enhance the BERT performance, including training"
2021.findings-acl.394,D13-1170,0,0.00344594,"etailed pre-training configurations are provided in the supplemental materials. 5.2 Results on GLUE Benchmark The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) is a collection of diverse natural language understanding (NLU) tasks, including inference tasks (MNLI, QNLI, RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018; Rajpurkar et al. 2016b), similarity and paraphrase tasks (MRPC, QQP, STSB; Dolan and Brockett 2005; Cer et al. 2017), and single-sentence tasks (CoLA, SST-2; Warstadt et al. 2018; Socher et al. 2013). The detailed descriptions of GLUE datasets are provided in the supplementary materials. The evaluation metrics are Spearman correlation for STS-B, Matthews correlation for CoLA, and accuracy for the other GLUE tasks. For small-size settings, we use the hyperparameter configuration as suggested in (Clark et al., 2020a). For base-size settings, we consider a limited hyperparameter searching for each task, with learning rates ∈ {5e-5, 1e-4, 1.5e-4} and training epochs ∈ {3, 4, 5}. The remaining hyperparameters are the same as E LECTRA. We report the median performance on the dev set over five d"
2021.findings-acl.394,N18-1101,0,0.240925,", 2017) for the generator’s MLM task, rather than using cross-entropy loss. The method adaptively downweights the well-predicted replacements for MLM, which avoids sampling too many correct tokens as replacements. We conduct pre-training experiments on the WikiBooks corpus for both small-size and base-size models. The proposed techniques are plugged into E LECTRA for training from scratch. Experimental results on various tasks show that our methods outperform E LECTRA despite the simplicity. Specifically, under the small-size setting, our model performance is 0.9 higher than E LECTRA on MNLI (Williams et al., 2018) and 4.2 higher on SQuAD 2.0 (Rajpurkar et al., 2016a), respectively. Under the base-size setting, our model performance is 0.26 higher than E LECTRA on MNLI and 0.52 higher on SQuAD 2.0, respectively. 2 Related Work State-of-the-art NLP models are mostly pretrained on a large unlabeled corpus with the self-supervised objectives (Peters et al., 2018; Lan et al., 2020; Raffel et al., 2020). The most representative pretext task is masked language modeling (MLM), which is introduced to pretrain a bidirectional BERT (Devlin et al., 2019) encoder. RoBERTa (Liu et al., 2019) apply several strategies"
2021.findings-acl.40,2020.findings-emnlp.414,0,0.0284982,"-words embedding to initialize. As shown in Figure 3, the word ‘lymphoma’ is not included in BERT vocabulary. We tokenize it into three subwords (lym, ##pho, ##ma). The embedding 3.3 Vocabulary Expansion Vocabulary expansion is the core module of AdaLM. It augments domain-specific terms or subword units to leverage domain knowledge. The size of the incremental vocabulary is a vital parameter for vocabulary expansion. Considering that unigram language modeling (Kudo, 2018) aligns more closely with morphology and avoids problems stemming from BPE’s greedy construction procedure, as proposed in (Bostrom and Durrett, 2020), we followed Kudo (2018) and introduced a corpus occurrence probability as a metric to optimize the size of incremental vocabulary automati462 -200 Initial from Original BERT -210 Transformer Encoder -220 -230 Expanded Embedding [29709 × 768] Original Embedding [30522 × 768] -240 -250 recently 3728 lymphoma 30735 entity 9178 developed 2764 quickly 2856 -260 30 recently lymphoma entity developed quickly Figure 3: Concatenate original embedding with expanded embedding. cally. We assume that each subword occurs independently and we assign to each subword in the corpus a probability equal to its"
2021.findings-acl.40,W04-1213,0,0.190167,"/arxiv 4 https://microsoft.github.io/BLURB/ 3 Test 8,662 16,364 15,745 Train 1,688 3,219 Task 114 455 Test 139 974 Classes 6 7 Table 2: Computer science dataset used in our experiment. We use the same train, development, and test splits as Gururangan et al. (2020) Fine-tuning tasks: For the biomedical domain, we choose three tasks: named entity recognition (NER), evidence-based medical information extraction (PICO), and relation extraction (RE). We perform entity-level F1 in NER task and wordlevel macro-F1 in the PICO task. The RE task uses the micro-F1 of positive classes evaluation. JNLPBA (Collier and Kim, 2004) NER dataset contains 6,892 disease mentions, which are mapped to 790 unique disease concepts with BIO tagging (Ramshaw and Marcus, 1995). EBM PICO (Nye et al., 2018) datasets annotates text spans with four tags: Participants, Intervention, Comparator and Outcome. ChemProt (Krallinger et al., 2017) dataset consists of five interactions between chemical and protein entities. We list the statistics of those tasks in Table 1. We fine-tune two downstream tasks in the computer science domain. They are both classification tasks. The ACL-ARC (Jurgens et al., 2018) dataset mainly focuses on analyzing"
2021.findings-acl.40,N19-1423,0,0.0259702,"probability to choose the size of incremental vocabulary automatically. Then we systematically explore different strategies to compress the large pretrained models for specific domains. We conduct our experiments in the biomedical and computer science domain. The experimental results demonstrate that our approach achieves better performance over the BERTBASE model in domain-specific tasks while 3.3× smaller and 5.1× faster than BERTBASE . The code and pretrained models are available at https://aka.ms/adalm. 1 Domain Corpus Pre-trained language models, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and UniLM (Dong et al., 2019) have achieved impressive success in many natural language processing tasks. These models usually have hundreds of millions of parameters. They are pre-trained on a large corpus of general domain and fine-tuned on target domain tasks. However, it is not optimal to deploy these models directly to edge devices in specific domains. First, heavy model size and high latency makes it difficult Contribution during internship at Microsoft Research. (b) Distill-then-Adapt (c) Adapt-then-Distill (d) Adapt-and-Distill Figure 1: The four alternativ"
2021.findings-acl.40,2020.acl-main.740,0,0.0362375,"Missing"
2021.findings-acl.40,2020.findings-emnlp.372,0,0.0428914,"in and fine-tune the domainspecific small models on different downstream tasks. Experiments demonstrate that Adapt-andDistill achieves state-of-the-art results for domainspecific tasks. Specifically, the 6-layer model of 384 hidden dimensions outperforms the BERTBASE model while 3.3× smaller and 5.1× faster than BERTBASE . 2 unseen words. Task-agnostic knowledge distillation In recent years, tremendous progress has been made in model compression (Cheng et al., 2017). Knowledge distillation has proven to be a promising way to compress large models while maintaining accuracy (Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2020). In this paper, we focus on task-agnostic knowledge distillation approaches, where a distilled small pre-trained model can be directly fine-tuned on downstream tasks. DistilBERT (Sanh et al., 2019) employs the soft label and embedding outputs to supervise the student. TinyBERT (Jiao et al., 2020) and MobileBERT (Sun et al., 2020) introduce self-attention distributions and hidden states to train the student model. MiniLM (Wang et al., 2020) avoids restrictions on the number of student layers and employs the self-attention distributions and value relation o"
2021.findings-acl.40,Q18-1028,0,0.0138259,"ositive classes evaluation. JNLPBA (Collier and Kim, 2004) NER dataset contains 6,892 disease mentions, which are mapped to 790 unique disease concepts with BIO tagging (Ramshaw and Marcus, 1995). EBM PICO (Nye et al., 2018) datasets annotates text spans with four tags: Participants, Intervention, Comparator and Outcome. ChemProt (Krallinger et al., 2017) dataset consists of five interactions between chemical and protein entities. We list the statistics of those tasks in Table 1. We fine-tune two downstream tasks in the computer science domain. They are both classification tasks. The ACL-ARC (Jurgens et al., 2018) dataset mainly focuses on analyzing how scientific works frame their contributions through different types of citations. SCIERC (Luan et al., 2018) dataset includes annotations for scientific entities, their relations, and coreference clusters. The statistics are available in Table 2. 2 Dev 4551 85321 11268 Table 1: Biomedical dataset used in our experiment. All selected from BLURB4 Domain corpus: For the biomedical domain, we collect a 16GB corpus from PubMed2 abstracts to adapt our model. We use the latest collection and pre-process the corpora with the same process as PubMedBERT (we omit a"
2021.findings-acl.40,P18-1007,0,0.148257,"is tokenized into [l, ##ym, ##ph, ##oma]). Gu et al.(2020) mentions that domainspecific vocabularies play a vital role in domain adaptation of pre-trained models. Specifically, we propose a domain-specific vocabulary expansion in the adaptation stage, which augments in-domain terms or subword units automatically given indomain text. Also, it is critical to decide the size of incremental vocabulary. Motivated by subword reg460 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 460–470 August 1–6, 2021. ©2021 Association for Computational Linguistics ularization (Kudo, 2018), AdaLM introduces a corpus occurrence probability as a metric to optimize the size of incremental vocabulary automatically. We systematically explore different strategies to compress general BERT models to specific domains (Figure 1): (a) From scratch: pre-training domain-specific small model from scratch with domain corpus; (b) Distill-then-Adapt: first distilling large model into small model, then adapting it into a specific domain; (c) Adapt-then-Distill: first adapting BERT into a specific domain, then distilling model into small size; (d) Adapt-and-Distill: adapting both the large and sm"
2021.findings-acl.40,2021.ccl-1.108,0,0.0636689,"Missing"
2021.findings-acl.40,D18-1360,0,0.0192212,"with BIO tagging (Ramshaw and Marcus, 1995). EBM PICO (Nye et al., 2018) datasets annotates text spans with four tags: Participants, Intervention, Comparator and Outcome. ChemProt (Krallinger et al., 2017) dataset consists of five interactions between chemical and protein entities. We list the statistics of those tasks in Table 1. We fine-tune two downstream tasks in the computer science domain. They are both classification tasks. The ACL-ARC (Jurgens et al., 2018) dataset mainly focuses on analyzing how scientific works frame their contributions through different types of citations. SCIERC (Luan et al., 2018) dataset includes annotations for scientific entities, their relations, and coreference clusters. The statistics are available in Table 2. 2 Dev 4551 85321 11268 Table 1: Biomedical dataset used in our experiment. All selected from BLURB4 Domain corpus: For the biomedical domain, we collect a 16GB corpus from PubMed2 abstracts to adapt our model. We use the latest collection and pre-process the corpora with the same process as PubMedBERT (we omit any abstracts with less than 128 words to reduce noise.). For the computer science domain, we use the abstracts text from the arXiv3 Dataset. We sele"
2021.findings-acl.40,P18-1019,0,0.0185183,"r experiment. We use the same train, development, and test splits as Gururangan et al. (2020) Fine-tuning tasks: For the biomedical domain, we choose three tasks: named entity recognition (NER), evidence-based medical information extraction (PICO), and relation extraction (RE). We perform entity-level F1 in NER task and wordlevel macro-F1 in the PICO task. The RE task uses the micro-F1 of positive classes evaluation. JNLPBA (Collier and Kim, 2004) NER dataset contains 6,892 disease mentions, which are mapped to 790 unique disease concepts with BIO tagging (Ramshaw and Marcus, 1995). EBM PICO (Nye et al., 2018) datasets annotates text spans with four tags: Participants, Intervention, Comparator and Outcome. ChemProt (Krallinger et al., 2017) dataset consists of five interactions between chemical and protein entities. We list the statistics of those tasks in Table 1. We fine-tune two downstream tasks in the computer science domain. They are both classification tasks. The ACL-ARC (Jurgens et al., 2018) dataset mainly focuses on analyzing how scientific works frame their contributions through different types of citations. SCIERC (Luan et al., 2018) dataset includes annotations for scientific entities,"
2021.findings-acl.40,W95-0107,0,0.0313798,": Computer science dataset used in our experiment. We use the same train, development, and test splits as Gururangan et al. (2020) Fine-tuning tasks: For the biomedical domain, we choose three tasks: named entity recognition (NER), evidence-based medical information extraction (PICO), and relation extraction (RE). We perform entity-level F1 in NER task and wordlevel macro-F1 in the PICO task. The RE task uses the micro-F1 of positive classes evaluation. JNLPBA (Collier and Kim, 2004) NER dataset contains 6,892 disease mentions, which are mapped to 790 unique disease concepts with BIO tagging (Ramshaw and Marcus, 1995). EBM PICO (Nye et al., 2018) datasets annotates text spans with four tags: Participants, Intervention, Comparator and Outcome. ChemProt (Krallinger et al., 2017) dataset consists of five interactions between chemical and protein entities. We list the statistics of those tasks in Table 1. We fine-tune two downstream tasks in the computer science domain. They are both classification tasks. The ACL-ARC (Jurgens et al., 2018) dataset mainly focuses on analyzing how scientific works frame their contributions through different types of citations. SCIERC (Luan et al., 2018) dataset includes annotati"
2021.findings-acl.40,2020.acl-main.195,0,0.0235285,"e domainspecific small models on different downstream tasks. Experiments demonstrate that Adapt-andDistill achieves state-of-the-art results for domainspecific tasks. Specifically, the 6-layer model of 384 hidden dimensions outperforms the BERTBASE model while 3.3× smaller and 5.1× faster than BERTBASE . 2 unseen words. Task-agnostic knowledge distillation In recent years, tremendous progress has been made in model compression (Cheng et al., 2017). Knowledge distillation has proven to be a promising way to compress large models while maintaining accuracy (Sanh et al., 2019; Jiao et al., 2020; Sun et al., 2020; Wang et al., 2020). In this paper, we focus on task-agnostic knowledge distillation approaches, where a distilled small pre-trained model can be directly fine-tuned on downstream tasks. DistilBERT (Sanh et al., 2019) employs the soft label and embedding outputs to supervise the student. TinyBERT (Jiao et al., 2020) and MobileBERT (Sun et al., 2020) introduce self-attention distributions and hidden states to train the student model. MiniLM (Wang et al., 2020) avoids restrictions on the number of student layers and employs the self-attention distributions and value relation of the teacher’s la"
2021.findings-acl.40,2020.findings-emnlp.129,0,0.0347896,"text. Gururangan et al. (2020) also employ continual pre-training to adapt pre-trained models into different domains including biomedical, computer science and news. However, many specialized domains contain their own specific words that are not included in pre-trained language model vocabulary. Gu et al.(2020) propose a biomedical pre-trained model PubMedBERT, where the vocabulary was created from scratch and the model is pre-trained from scratch. Furthermore, in many specialized domains, large enough corpora may not be available to support pre-training from scratch. Zhang et al. (2020) and Tai et al. (2020) extend the open-domain vocabulary with top frequent in-domain words to resolve this out-of-vocabulary issue. This approach ignores domain-specific sub-word units (e.g., blasto-, germin- in biomedical domain). These subword units help generalize domain knowledge and avoid 3 3.1 Methods Overview We systematically explore different strategies to achieve an effective and efficient small model in specific domains. We summarize them into four strategies: from scratch, distill-then-adapt, adaptthen-distill and adapt-and-distill. 461 Pretrain-from-scratch Domain-specific pretraining from scratch empl"
2021.naacl-main.172,2020.emnlp-main.703,0,0.0508362,"Missing"
2021.naacl-main.172,N19-1423,0,0.0774097,"Missing"
2021.naacl-main.172,L18-1550,0,0.0522884,"Missing"
2021.naacl-main.172,2020.pam-1.10,0,0.0546558,"Missing"
2021.naacl-main.172,P18-2023,0,0.0630005,"Missing"
2021.naacl-main.172,C18-1166,0,0.0541641,"Missing"
2021.naacl-main.172,2021.ccl-1.108,0,0.0592445,"Missing"
2021.naacl-main.172,S18-2023,0,0.0608721,"Missing"
2021.naacl-main.172,2020.acl-main.467,0,0.049519,"Missing"
2021.naacl-main.172,S12-1049,0,0.0893999,"Missing"
2021.naacl-main.172,N18-2028,0,0.0678343,"Missing"
2021.naacl-main.172,D17-1082,0,0.0457702,"Missing"
2021.naacl-main.172,2020.aacl-main.24,1,0.844222,"Missing"
2021.naacl-main.172,2020.acl-main.330,1,0.87885,"Missing"
2021.naacl-main.172,2020.emnlp-main.633,1,0.891427,"Missing"
2021.naacl-main.172,D18-1009,0,0.0419089,"Missing"
2021.naacl-main.172,P14-2115,0,0.0763014,"Missing"
2021.naacl-main.172,P15-1057,0,0.019293,"Missing"
2021.naacl-main.172,P19-1435,0,0.0490671,"Missing"
2021.naacl-main.280,D19-1252,1,0.753364,"Missing"
2021.naacl-main.280,L18-1548,0,0.0642253,"oss-lingual understanding tasks. We also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inference corpus (XNLI; Conneau et al. 2018"
2021.naacl-main.280,2020.acl-main.653,0,0.715277,"h monolingual and parallel corpora. Contribution during internship at Microsoft Research. Contact person: Li Dong and Furu Wei. We jointly train I NFOXLM with MMLM, TLM 3576 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3576–3588 June 6–11, 2021. ©2021 Association for Computational Linguistics and X L C O. We conduct extensive experiments on several cross-lingual understanding tasks, including cross-lingual natural language inference (Conneau et al., 2018), cross-lingual question answering (Lewis et al., 2020), and cross-lingual sentence retrieval (Artetxe and Schwenk, 2019). Experimental results show that I NFOXLM outperforms strong baselines on all the benchmarks. Moreover, the analysis indicates that I NFOXLM achieves better cross-lingual transferability. 2 2.1 Related Work between the sampled positive and negative pairs. In addition to the estimators, various view pairs are employed in these methods. The view pair can be the local and global features of an image (Hjelm et al., 2019; Bachman et al., 2019), the random data augmentations of the same image (Tian et al., 2019; He et al., 2020; Chen"
2021.naacl-main.280,L16-1561,0,0.0179259,"OXLM with previous work on three cross-lingual understanding tasks. We also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inf"
2021.naacl-main.280,2020.findings-emnlp.147,0,0.110105,"Missing"
2021.naacl-main.280,tiedemann-2012-parallel,0,0.0390072,"e also conduct ablation studies to understand the major components of I NFOXLM. 4.1 Setup Corpus We use the same pre-training corpora as previous models (Conneau et al., 2020a; Conneau and Lample, 2019). Specifically, we reconstruct CC-100 (Conneau et al., 2020a) for MMLM, which remains 94 languages by filtering the language code larger than 0.1GB. Following (Conneau and Lample, 2019), for the TLM and X L C O tasks, we employ 14 language pairs of parallel data that involves English. We collect translation pairs from MultiUN (Ziemski et al., 2016), IIT Bombay (Kunchukuttan et al., 2018), OPUS (Tiedemann, 2012), and WikiMatrix (Schwenk et al., 2019). The size of parallel corpora is about 42GB. More details about the pre-training data are described in the appendix. Model Size We follow the model configurations of XLM-R (Conneau et al., 2020a). For the Transformer (Vaswani et al., 2017) architecture, we use 12 layers and 768 hidden states for I NFOXLM (i.e., base size), and 24 layers and 1,024 hidden states for I NFOXLM LARGE (i.e., large size). 4.2 Evaluation Cross-Lingual Natural Language Inference The Cross-Lingual Natural Language Inference corpus (XNLI; Conneau et al. 2018) is a widely used cross"
C08-1062,W04-3247,0,0.249297,"(4) rank SB given that SA is provided. Among them, (4) is of most concern. It should be noting that both (2) and (4) need to consider the influence from the sentences in the same and different collections. In this study, we made an attempt to capture the intuition that “A sentence receives a positive influence from the sentences that correlate to it in the same collection, whereas a sentence receives a negative influence from the sentences that correlates to it in the different collection.” We represent the sentences in A or B as a text graph constructed using the same approach as was used in Erkan and Radev (2004a, 2004b). Different from the existing PageRank-like algorithms adopted in document summarization, we propose a novel sentence ranking algorithm, called PNR2 (Ranking with Positive and Negative Reinforcement). While PageRank models the positive mutual reinforcement among the sentences in the graph, PNR2 is capable of modeling both positive and negative reinforcement in the ranking process. The remainder of this paper is organized as follows. Section 2 introduces the background of the work presented in this paper, including existing graph-based summarization models, descriptions of update summa"
C08-1062,P06-1047,1,0.861538,"hich was then used as the criterion to rank and select summary sentences. Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year. Besides, they reported experimental comparison of three different graph-based sentence ranking algorithms obtained from Positional Power Function, HITS and PageRank (Mihalcea and Tarau, 2005). Both HITS and PageRank performed excellently. Likewise, the use of PageRank family was also very popular in event-based summarization approaches (Leskovec et al., 2004; Vanderwende et al., 2004; Yoshioka and Haraguchi, 2004; Li et al., 2006). In contrast to conventional sentencebased approaches, newly emerged event-based approaches took event terms, such as verbs and action nouns and their associated named entities as graph nodes, and connected nodes according to their co-occurrence information or semantic dependency relations. They were able to provide finer text representation and thus could be in favor of sentence compression which was targeted to include more informative contents in a fixed-length summary. Nevertheless, these advantages lied on appropriately defining and selecting event terms. All above-mentioned representati"
C08-1062,N03-1020,0,0.205294,"summary the highest ranked sentence of concern if it doesn’t significantly repeat the information already included in the summary until the word limitation is reached. Average number of documents Average number of sentences A 10 237.6 B 10 177.3 Table 1. Basic Statistics of DUC2007 Update Data Set As for the evaluation metric, it is difficult to come up with a universally accepted method that can measure the quality of machine-generated summaries accurately and effectively. Many literatures have addressed different methods for automatic evaluations other than human judges. Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. Given the fact that judgments by humans are timeconsuming and labor-intensive, and more important, ROUGE has been officially adopted for the DUC evaluations since 2005, like the other researchers, we also choose it as the evaluation criteria. In the following experiments, the sentences and the queries are all represented as the vectors of words. The relevance of a sentence to the query is calculated by cosine similarity. Notice that the word weights are normally measured by the document-level TF*IDF sche"
C08-1062,P04-3020,0,0.0697616,"Missing"
C08-1062,W04-3252,0,\N,Missing
C08-1062,H05-1115,0,\N,Missing
C12-1047,P11-5003,0,0.0437679,"Missing"
C12-1047,P07-2015,0,0.024705,"tion 4 shows the experiments and results, and we conclude this work with directions for future study in section 5. 2 Related work 2.1 Extractive document summarization A substantial amount of work has been done on extractive text summarization (Lloret and Palomar, 2012). Many text features, such as term frequency, sentence position, query relevance and sentence dependency structure, have been investigated for sentence salience estimation. They are usually weighted automatically by applying certain learning-based mechanisms or tuned experimentally to build a feature-based summarization system (Fuentes et al., 2007) (Wong et al., 2008). Previous research shows that a combination of sentence position, fixedphrase and sentence length give the best results in learning-based sentence selection (Ani Nenkova, 2011). Meanwhile, feature-based approaches have been widely used in the top five participating systems in DUC3 2005-2007. In addition, different types of links among sentences and documents are employed by graphbased approaches to measure sentence salience, such as LexRank (Erkan and Radev, 2004), TextRank (Mihalcea, 2004), and Mutual Reinforcement Chain(MRC) (Wei et al., 2008). LexRank and TextRank make"
C12-1047,N03-1020,0,0.0933039,"del, the contribution of social influence and content quality to the model, the correctness of sub-topic segmentation, and the accuracy of the content quality estimation model. The performance of the social influence of users has not been evaluated due to the difficulty of data acquisition. It is hard to label the influence of users, or to collect such information automatically. Therefore, in this paper, we evaluate the end-to-end contribution of social influence under the whole summarization framework. 4.2 Evaluation metric We evaluate the performance of our summarization system using ROUGE (Lin and Hovy, 2003), which is widely-used in summarization evaluation. It measures the overlap of N-grams between the predicted summary and the reference, which is defined as, ROU GE = Σ t∈Sr e f Σ g r amn ∈t C ount mat ch (g r amn ) Σ t∈Sr e f Σ g r amn ∈t C ount(g r amn ) where n is the word length of n-gram, S r e f denotes the reference summary, C ount(g r amn ) is the number of n-grams comprising sentences in the reference summary and C ount mat ch (g r amn ) computes the maximum number of n-grams appearing both in the summary generated by our system and the reference summary. 772 4.3 Evaluation of the rein"
C12-1047,W11-0709,0,0.206308,"mary. Both feature-based and graph-based approaches are exploited to measure the salience of posts under an extractive summarization framework. Taking into consideration 3 http://www-nlpir.nist.gov/projects/duc/data.html 765 the evolutionary characteristic of topics along time line, researchers have also started to explore the evolutionary summarization of events in micro-blog. In feature-based approaches, a variety of statistical and linguistic features have been extensively investigated, such as, language model (O’Connor et al., 2010), tweet frequency (Shiells et al., 2010), term frequency (Liu et al., 2011) (Takamura et al., 2011) (Parthasarathy, 2012), TF-IDF (Frederking, 2011) (Chakrabarti and Punera, 2011), hybrid TF-IDF (Sharifi et al., 2010b), KLdivergence (Zubiaga et al., 2012), time delay (Takamura et al., 2011), and topic relevance (Long et al., 2011). Among them, simple term frequency has proven to be extremely extraordinary for topic-sensitive micro-blog summarization because of the unstructured and short characteristics of micro-blog posts according to Inouye and Kalita (2011). As for micro-blog summarization, some micro-blog specific features such as text normalization, the content o"
C12-1047,P04-3020,0,0.0390607,"anisms or tuned experimentally to build a feature-based summarization system (Fuentes et al., 2007) (Wong et al., 2008). Previous research shows that a combination of sentence position, fixedphrase and sentence length give the best results in learning-based sentence selection (Ani Nenkova, 2011). Meanwhile, feature-based approaches have been widely used in the top five participating systems in DUC3 2005-2007. In addition, different types of links among sentences and documents are employed by graphbased approaches to measure sentence salience, such as LexRank (Erkan and Radev, 2004), TextRank (Mihalcea, 2004), and Mutual Reinforcement Chain(MRC) (Wei et al., 2008). LexRank and TextRank make use of pairwise similarity between sentences, hypothesizing that the sentences similar to most of the other sentences in a cluster are more salient. In contrast to the single level PageRank in LexRank and TextRank, MRC considers both internal and external constraints on three different levels, document, sentence, and term and achieves promising improvement. 2.2 Micro-blog summarization Recently, researchers have conducted a number of investigations on micro-blog(e.g. Twitter) summarization. Instead of ranking s"
C12-1047,N10-1100,0,0.569622,"richness, as a measure of the regularity of written language and the pointless degree of the content. The above information is jointly employed in a graph-based ranking algorithm. In order to avoid redundancy in the result, the final summary is generated by selecting tweets from the previous ranking results with the traditional Maximal Marginal Relevance(MMR) algorithm (Carbonell and Goldstein, 1998). We conduct experiments on a real data set containing 3.9 million tweets. Compared with two popular graph-based summarization approaches, namely Lexrank (Erkan and Radev, 2004) and phrase graph (Sharifi et al., 2010a), the experimental results show that: • The reinforcement summarization model integrating social influence and content quality achieves a considerable performance and outperforms the standard LexRank and the phrase graph summarization approaches. • The social influence of users and the content quality of tweets help to more effectively measure the salience of tweets. The rest of this paper is organized as follows. Related work is introduced in Section 2. Next, we present a detailed introduction of our approach in Section 3. Section 4 shows the experiments and results, and we conclude this wo"
C12-1047,P07-1070,0,0.0122818,"y to dominate the topic. The last is set against the informal writing style of Twitter. Inspired by Wei et al. (2008), we propose a unified mutual reinforcement summarization model taking advantage of relations among tweets, words, and users for tweet salience measurement. Figure 1 shows the overview of the proposed model. The similarity of tweets to the sub-topic benefits from both the content similarity among tweets and the word coverage in the sub-topic cluster. In document summarization, the contribution of relationships among sentences to the performance improvements has been recognized (Wan et al., 2007). Sharifi et al. (2010a) found that sequences of words that encompassed the topic phrase highly overlapped when considering a large number of tweets for a single topic. The social influence of users contributes to salience measurement via author relation. And the content quality of tweets is incorporated at the tweet level. Figure 1: The Unified Mutual Reinforcement Graph Model The mutual reinforcement model is formed with three PageRank-like models for word, tweet, and user respectively, but in a unified and interrelated way. The ranking of one of them is derived not only from the relationshi"
C12-1047,C08-1124,0,0.0269199,"ments and results, and we conclude this work with directions for future study in section 5. 2 Related work 2.1 Extractive document summarization A substantial amount of work has been done on extractive text summarization (Lloret and Palomar, 2012). Many text features, such as term frequency, sentence position, query relevance and sentence dependency structure, have been investigated for sentence salience estimation. They are usually weighted automatically by applying certain learning-based mechanisms or tuned experimentally to build a feature-based summarization system (Fuentes et al., 2007) (Wong et al., 2008). Previous research shows that a combination of sentence position, fixedphrase and sentence length give the best results in learning-based sentence selection (Ani Nenkova, 2011). Meanwhile, feature-based approaches have been widely used in the top five participating systems in DUC3 2005-2007. In addition, different types of links among sentences and documents are employed by graphbased approaches to measure sentence salience, such as LexRank (Erkan and Radev, 2004), TextRank (Mihalcea, 2004), and Mutual Reinforcement Chain(MRC) (Wei et al., 2008). LexRank and TextRank make use of pairwise simi"
C12-1104,H01-1065,0,0.0106409,"ls our method and Section 6 evaluates our method. Section 7 demonstrates the application of the proposed method to the Twitter search. Finally, Section 8 concludes with a discussion of future work. 2 Related Work Two categories of research are highly related to our work: multi-document summarization and recent studies of tweets. 2.1 Multi-document Summarization Abstraction and selection are two strategies employed for multi-document summarization. The former involves information fusion (Barzilay et al., 1999; Xie et al., 2008), sentence compression (Knight and Marcu, 2002), and reformulation (Barzilay et al., 2001; Saggion, 2011); while the latter requires computing salience scores of some units (e.g., sentences, paragraphs) and extracting those with highest scores with redundancy removed. NewsBlaster3 and our method are examples of abstraction and selection based methods, respectively. We choose the selection strategy because it is relatively simpler, e.g., not requiring 3 http://newsblaster.cs.columbia.edu 1701 language generation to produce a grammatical and coherent summary, and better suites the scenario of tweet summarization. Note that our method considers each tweet as the unit for summarizatio"
C12-1104,C10-1034,1,0.723844,"dies aiming to help people to efficiently access tweets. For instance, Finin et al. (2010) annotate named entities in tweets by exploiting Amazon’s Mechanical Turk service4 and CrowdFlower5 ; Liu et al. (2011) propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to recognize named entities in tweets; Liu et al. (2010) conduct a pilot study of Semantic Role Labeling on tweets; Sankaranarayanan et al. (2009) extract breaking news from tweets to build a news processing system, called TwitterStand; Duan et al. (2010) give an empirical study on learning to rank of tweets; Weng et al. (2010) propose TwitterRank, an extension of the PageRank algorithm to identify influential twitter accounts; O’Connor et al. (2010) present TweetMotif which groups tweets by frequent significant terms; Inouye and Kalita (2011) compare several tweet summarization algorithms that use text features like TFIDF to compute the similarity between any two tweet; Sharifi et al. (2010) exploit the Phrase Reinforcement Algorithm to find the most commonly used phrases that encompass the given topic phrase, based on which salient sentences"
C12-1104,W10-0713,0,0.031121,"es Congress (Golbeck et al., 2010), by city police departments in large U.S. cities (Heverin and Zach, 2010), and by scholars (Priem and Costello, 2010); Jansen et al. (2009) report research results investigating microblogging as a form of electronic word-of-mouth for sharing consumer opinions concerning brands; Heverin and Zach (2010) give insights into why particular events resonate with the population. All the above studies indicate the critical role of tweets as a dynamic information source. There is another line of studies aiming to help people to efficiently access tweets. For instance, Finin et al. (2010) annotate named entities in tweets by exploiting Amazon’s Mechanical Turk service4 and CrowdFlower5 ; Liu et al. (2011) propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to recognize named entities in tweets; Liu et al. (2010) conduct a pilot study of Semantic Role Labeling on tweets; Sankaranarayanan et al. (2009) extract breaking news from tweets to build a news processing system, called TwitterStand; Duan et al. (2010) give an empirical study on learning to rank of tweets; Weng et al. (201"
C12-1104,C10-1079,1,0.787012,"(2010) give insights into why particular events resonate with the population. All the above studies indicate the critical role of tweets as a dynamic information source. There is another line of studies aiming to help people to efficiently access tweets. For instance, Finin et al. (2010) annotate named entities in tweets by exploiting Amazon’s Mechanical Turk service4 and CrowdFlower5 ; Liu et al. (2011) propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to recognize named entities in tweets; Liu et al. (2010) conduct a pilot study of Semantic Role Labeling on tweets; Sankaranarayanan et al. (2009) extract breaking news from tweets to build a news processing system, called TwitterStand; Duan et al. (2010) give an empirical study on learning to rank of tweets; Weng et al. (2010) propose TwitterRank, an extension of the PageRank algorithm to identify influential twitter accounts; O’Connor et al. (2010) present TweetMotif which groups tweets by frequent significant terms; Inouye and Kalita (2011) compare several tweet summarization algorithms that use text features like TFIDF to compute the similarity"
C12-1104,P11-1037,1,0.779704,"lars (Priem and Costello, 2010); Jansen et al. (2009) report research results investigating microblogging as a form of electronic word-of-mouth for sharing consumer opinions concerning brands; Heverin and Zach (2010) give insights into why particular events resonate with the population. All the above studies indicate the critical role of tweets as a dynamic information source. There is another line of studies aiming to help people to efficiently access tweets. For instance, Finin et al. (2010) annotate named entities in tweets by exploiting Amazon’s Mechanical Turk service4 and CrowdFlower5 ; Liu et al. (2011) propose to combine a K-Nearest Neighbors (KNN) classifier with a linear Conditional Random Fields (CRF) model under a semi-supervised learning framework to recognize named entities in tweets; Liu et al. (2010) conduct a pilot study of Semantic Role Labeling on tweets; Sankaranarayanan et al. (2009) extract breaking news from tweets to build a news processing system, called TwitterStand; Duan et al. (2010) give an empirical study on learning to rank of tweets; Weng et al. (2010) propose TwitterRank, an extension of the PageRank algorithm to identify influential twitter accounts; O’Connor et al"
C12-1104,C08-1087,0,0.0587079,"Missing"
C12-1104,P10-1057,0,0.0172924,"ssifier, is learnt statistically from the training data. There are methods between those categories. For example, Wan and Yang (2008) consider cluster level information, i.e., the importance of the cluster and the relevance of sentence to the cluster, for computing sentence salience score. Motivated by LexRank (Erkan and Radev, 2004), we adopt graph based methods. Differently, our system incorporates rich social network features and considers readability to compute salience score of every tweet. Most existing studies focus on formal texts such as news. However, exceptions exist. For instance, Qazvinian and Radev (2010, 2008) study the problem of summarizing a scientific paper. They propose a clustering approach where communities in the citation summary’s lexical network are formed and sentences are extracted from separate clusters. Sharifi et al. (2010) use the Phrase Reinforcement algorithm to generate one-line summary for a collection of tweets related to a topic. Though our method is also designed for tweets, there are several significant differences. Firstly, our method does not assume that the input tweets are about a topic. Secondly, our method selects representative tweets by exploiting social netwo"
C12-1104,N10-1100,0,0.456337,"argely attributed to the short and noise prone nature of tweets, which causes a single tweet to be insufficient to provide reliable information to compute its salience score. We develop a graph-based summarization system that aggregates social signals, i.e., re-tweeted times and follower numbers to handle this challenge. More specifically, the translation probability from one tweet to the other depends on both the similarity between the two tweets and the social network features associated with the second tweet. This largely differentiates our system from existing studies, such as the work of Sharifi et al. (2010), which uses only tweet-level content features (e.g., keywords) to select representative sentences. 1 2 http://www.twitter.com Noise in tweets means ill-formed words or sentences in tweets. 1700 Besides utilizing social signals, our system has two additional features. Firstly, the readability feature is introduced to the graph model to reduce the chance of tweets hard to read to appear in the summarization. Several factors are considered while computing a tweet’s readability, including: 1) The number of out-of-vocabulary (OOV) words; 2) the number of words; and 3) the number of abnormal symbol"
C12-1104,xie-etal-2008-extracting,0,0.0150853,"d work. Section 3 defines the task. Section 4 describes the baseline. Section 5 details our method and Section 6 evaluates our method. Section 7 demonstrates the application of the proposed method to the Twitter search. Finally, Section 8 concludes with a discussion of future work. 2 Related Work Two categories of research are highly related to our work: multi-document summarization and recent studies of tweets. 2.1 Multi-document Summarization Abstraction and selection are two strategies employed for multi-document summarization. The former involves information fusion (Barzilay et al., 1999; Xie et al., 2008), sentence compression (Knight and Marcu, 2002), and reformulation (Barzilay et al., 2001; Saggion, 2011); while the latter requires computing salience scores of some units (e.g., sentences, paragraphs) and extracting those with highest scores with redundancy removed. NewsBlaster3 and our method are examples of abstraction and selection based methods, respectively. We choose the selection strategy because it is relatively simpler, e.g., not requiring 3 http://newsblaster.cs.columbia.edu 1701 language generation to produce a grammatical and coherent summary, and better suites the scenario of tw"
C12-1104,P99-1071,0,\N,Missing
C12-2081,esuli-sebastiani-2006-sentiwordnet,0,0.177866,"Missing"
C12-2081,P11-2104,0,0.169438,"Missing"
C12-2081,N10-1119,0,0.0289978,"Missing"
C12-2081,P07-1123,0,0.374643,"Missing"
C12-2081,E09-1077,0,0.249258,"Missing"
C12-2081,D08-1058,0,0.169823,"oreover, in many cases, two or more English sentiment words often are translated to the same foreign word. Both factors lead to smaller translated sentiment lexicons than the original ones. (Mihalcea et al., 2007) study the efectiveness of translating English sentiment lexicon to Romanian using two bilingual dictionaries. The original English sentiment lexicon contains 6,856 entries; after translation, only 4,983 entries are left in the Romanian sentiment lexicon. About 2000 entries are lost or conlated into other entries during the translation process. The translation method is also used in (Wan, 2008, 2011). On the other hand, though bootstrapping methods don't use bilingual dictionaries and hence are not subject to the limitation of the translation methods, they have relatively high demands for semantic resources such as WordNet (Fellbaum, 1998). Bootstrapping methods enlarge the sentiment lexicons from English sentiment seed words. (Hassan et al., 2011) present a method to identify the sentiment polarity of foreign words by using WordNet (or similar semantic resources) in the target foreign language. (Ku and Chen, 2007) create a Chinese Lexicon by translating the General Inquirer, combi"
C12-2081,J11-3005,0,0.0428357,"Missing"
C12-2081,N10-2012,0,0.014788,"eriod or question mark, at the end of the English word. We use this simple rule to limit the possible parts-of-speech of the translations. For example, “efusive.” is translated to “热情洋溢”, while “efusive” is translated to “感情奔放的” ; after adding punctuation context, “efusive” is translated to words that have diferent parts-of-speech. We can also combine this technique with the coordinated phrase technique. Concretely, We use a bi-gram language model for generating possible collocations. Instead of creating our own language model from large corpora, we leverage the Microsoft Web N-gram Services (Wang et al., 2010)3 , an online N-gram corpus that built from Web documents. We choose the bi-gram language model trained on document titles. Given each English polarity word w1 , we use the language model to generate up to the 1000 most frequent bi-grams w1 w2 . To create coordinated phrases, we irst translate all sentiment words using Google Translate. And then we create coordinated phrases for the English sentiment words which are translated into the same Chinese word. We select those English words and join them with the word “and”. The punctuation context are generated by appending a period after the given"
C12-2081,H05-1044,0,0.0366647,"ent dictionaries in other languages as well. Depending on the target language, we might need to make some small modiications. Word segmentation is unnecessary for most European languages. And in some languages, we need to consider the word order issues when extracting the sentiment words from the translation results, since translation engine might reorder the queries. For example, in Arabic, the modifying adjectives are placed before the nouns, which is diferent from English; and also in Arabic, the words are written from right to left. 3 Experimental Study We use the MPQA subjective lexicon (Wilson et al., 2005) as the English lexicon. We only keep the strong subjective entries, which include 1,481 positive and 3,080 negative entries. For the purpose of comparison, we implemented the following baseline approaches. The irst three baselines rely on a bilingual dictionary. We use the LDC (Linguistic Data Consortium) English-Chinese bilingual wordlists6 , which is also used in (Wan, 2008). This dictionary contains 18,195 entries. Each English entry is mapped to a list of Chinese words or expressions. As shown in Table 2, the irst baseline (DICT) looks up the English entry in the bilingual dictionary and"
C12-2081,C10-1136,1,0.894878,"Missing"
C14-1018,baccianella-etal-2010-sentiwordnet,0,0.227345,"or negative score reflecting its sentiment polarity and strength. Sentiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale se"
C14-1018,J81-4005,0,0.756294,"Missing"
C14-1018,P07-1054,0,0.0130165,"component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym"
C14-1018,P12-1092,0,0.137724,"ons between phrases and sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase embedding and propose a representation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment"
C14-1018,C04-1200,0,0.143042,"ng the sentiment-specific phrase embedding from massive tweets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are"
C14-1018,P13-2087,0,0.0738275,"hrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words an"
C14-1018,P12-1043,0,0.0177108,"with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item."
C14-1018,C94-1079,0,0.0290494,"nt lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al. (2010) represent words and phrases with their syntactic contexts within a window size from the web documents. Unlike the dominated propagation based methods, we explore the classification framework based on representation learning for building large-scale sentiment lexicon from Twitter. To construct the Twitter-specific sentim"
C14-1018,P11-1015,0,0.623804,"ntation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase em"
C14-1018,S13-2053,0,0.722061,"(2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al. (2010) represent words and phrases with their syntactic contexts within a window size from the web documents. Unlike the dominated propagation based methods, we explore the classification framework based on representation learning for building large-scale sentiment lexicon from Twitter. To construct the Twitter-specific sentiment lexicon, Mohammad et al. (2013) use pointwise mutual information (PMI) between each phrase and hashtag/emoticon seed words, such as #good, #bad, :) and :(. Chen et al. (2012) utilize the Urban Dictionary and extract the target-dependent sentiment expressions from Twitter. Unlike Mohammad et al. (2013) that only capture the relations between phrases and sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase embedding and propose a representation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of word"
C14-1018,S13-2052,0,0.0305225,"Missing"
C14-1018,W02-1011,0,0.0230923,"m tweets, leveraging massive tweets containing positive and negative emoticons as training set without any manual annotation. To obtain more training data for building the phrase-level sentiment classifier, we exploit the similar words from Urban Dictionary 2 , which is a crowd-sourcing resource, to expand a small list of sentiment seeds. Finally, we utilize the classifier to predict the sentiment score of each phrase in the vocabulary of SSPE, resulting in the sentiment lexicon. We evaluate the effectiveness of our sentiment lexicon (TS-Lex) by applying it in a supervised learning framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the topperformed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding learning algorithms. T"
C14-1018,J11-1002,0,0.0117166,"timent information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation learning approach, as illustrated in Figure 1. We cast sentiment lexicon learning as a phrase-level cla"
C14-1018,E09-1077,0,0.0168758,"uilt manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resource"
C14-1018,D11-1014,0,0.0867976,"an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013) that learn the compositionality of sentences. 3 Methodology In this section, we describe our method for building large-scale sentiment lexicon from Twitter within a classification framework, as illustrated in Figure 1. We leverage the continuous representation of phrases as features, without parsers or hand-crafted rules, and automatically obtain the training data by seed expansion from Urban Dictionary. After the classifier is built, we employ it to predict the sentiment distribution of each phrase in the embedding vocabulary, resulting in the sentiment lexicon. To encode the sentimen"
C14-1018,D13-1170,0,0.0317488,"Missing"
C14-1018,P14-1146,1,0.621391,"d in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013) that learn the compositionality of sentences. 3 Methodolo"
C14-1018,P02-1053,0,0.0288544,"eets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment sco"
C14-1018,N10-1119,0,0.0358758,"entiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation learning approac"
C14-1018,H05-1044,0,0.940027,"eness of our sentiment lexicon (TS-Lex) by applying it in a supervised learning framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the topperformed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding learning algorithms. The main contributions of this work are as follows: • To our best knowledge, this is the first work that leverages the continuous representation of phrases for building large-scale sentiment lexicon from Twitter; • We propose a tailored neural architecture for learning the sentiment-specific phrase embedding from massive tweets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework fo"
C14-1018,P13-1173,0,0.0128539,"negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this directi"
C14-1018,P10-1040,0,\N,Missing
C16-1004,D10-1047,0,0.0521323,"Missing"
C16-1004,P15-2136,1,0.701611,"a set of selected sentences S. Specifically, we present a new framework to conduct regression with respect to the relative gain of s given S calculated by the ROUGE metric. Besides the single sentence features, additional features derived from the sentence relations are incorporated. Experiments on the DUC 2001, 2002 and 2004 multi-document summarization datasets show that the proposed method outperforms state-of-the-art extractive summarization approaches. 1 Introduction Sentence regression is one of the branches of extractive summarization methods that achieves state-ofthe-art performances (Cao et al., 2015b; Wan et al., 2015) and is commonly used in practical systems (Hu and Wan, 2013; Wan and Zhang, 2014; Hong and Nenkova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ra"
C16-1004,C12-1056,0,0.0164822,"Missing"
C16-1004,W06-1643,0,0.0131876,"ion Sentence regression is one of the branches of extractive summarization methods that achieves state-ofthe-art performances (Cao et al., 2015b; Wan et al., 2015) and is commonly used in practical systems (Hu and Wan, 2013; Wan and Zhang, 2014; Hong and Nenkova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and Favre, 2009; Li et al., 2013). Then, in the sentence selection process, they discard the redundant sentences that are similar to the already selected sentences. In this paper, we propose a novel regression framework to directly model the relative importance f (s|S) of a sentence s given the sentences S. Specifically, we evaluate the relativ"
C16-1004,W09-1802,0,0.0313306,"kova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and Favre, 2009; Li et al., 2013). Then, in the sentence selection process, they discard the redundant sentences that are similar to the already selected sentences. In this paper, we propose a novel regression framework to directly model the relative importance f (s|S) of a sentence s given the sentences S. Specifically, we evaluate the relative importance f (s|S) with a regression model where additional features involving the sentence relations are incorporated. Then we generate the summary by greedily selecting the next sentence which maximizes f (s|S) with respect to the current selected sentences S. Our"
C16-1004,W00-0405,0,0.0341864,"Missing"
C16-1004,P07-2049,0,0.0156763,"Missing"
C16-1004,E14-1075,0,0.0755947,"ive gain of s given S calculated by the ROUGE metric. Besides the single sentence features, additional features derived from the sentence relations are incorporated. Experiments on the DUC 2001, 2002 and 2004 multi-document summarization datasets show that the proposed method outperforms state-of-the-art extractive summarization approaches. 1 Introduction Sentence regression is one of the branches of extractive summarization methods that achieves state-ofthe-art performances (Cao et al., 2015b; Wan et al., 2015) and is commonly used in practical systems (Hu and Wan, 2013; Wan and Zhang, 2014; Hong and Nenkova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and"
C16-1004,D15-1011,0,0.037119,"Missing"
C16-1004,P13-1099,0,0.0831969,"tence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and Favre, 2009; Li et al., 2013). Then, in the sentence selection process, they discard the redundant sentences that are similar to the already selected sentences. In this paper, we propose a novel regression framework to directly model the relative importance f (s|S) of a sentence s given the sentences S. Specifically, we evaluate the relative importance f (s|S) with a regression model where additional features involving the sentence relations are incorporated. Then we generate the summary by greedily selecting the next sentence which maximizes f (s|S) with respect to the current selected sentences S. Our method improves th"
C16-1004,N10-1134,0,0.0287194,"Missing"
C16-1004,P11-1052,0,0.0967887,"Missing"
C16-1004,C00-1072,0,0.02429,"013; Wan and Zhang, 2014; Hong and Nenkova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and Favre, 2009; Li et al., 2013). Then, in the sentence selection process, they discard the redundant sentences that are similar to the already selected sentences. In this paper, we propose a novel regression framework to directly model the relative importance f (s|S) of a sentence s given the sentences S. Specifically, we evaluate the relative importance f (s|S) with a regression model where additional features involving the sentence relations are incorporated. Then we generate the summary by greedily selecting the next sentence which maximizes f (s|S) with respec"
C16-1004,W04-1013,0,0.0300013,"Missing"
C16-1004,P04-3020,0,0.0219873,"Missing"
C16-1004,W02-0401,0,0.0259134,"ummarization approaches. 1 Introduction Sentence regression is one of the branches of extractive summarization methods that achieves state-ofthe-art performances (Cao et al., 2015b; Wan et al., 2015) and is commonly used in practical systems (Hu and Wan, 2013; Wan and Zhang, 2014; Hong and Nenkova, 2014). Existing sentence regression methods usually model sentence importance and sentence redundancy in two separate processes, namely sentence ranking and sentence selection. Specifically, in the sentence ranking process, they evaluate the importance f (s) of each sentence s with a ranking model (Osborne, 2002; Conroy et al., 2004; Galley, 2006; Li et al., 2007) through either directly measuring the salience of sentences (Li et al., 2007; Ouyang et al., 2007) or firstly ranking words (or bi-grams) and then combining these scores to rank sentences (Lin and Hovy, 2000; Yih et al., 2007; Gillick and Favre, 2009; Li et al., 2013). Then, in the sentence selection process, they discard the redundant sentences that are similar to the already selected sentences. In this paper, we propose a novel regression framework to directly model the relative importance f (s|S) of a sentence s given the sentences S. Sp"
C16-1004,W12-2601,0,0.0356117,"Missing"
C16-1004,W00-0403,0,0.208521,"Missing"
C16-1004,P13-2024,0,0.0184591,"Missing"
C16-1004,D08-1079,0,0.0155828,"Missing"
C16-1004,P11-1155,0,0.0294679,"Missing"
C16-1004,W04-3252,0,\N,Missing
C16-1053,P15-2136,1,0.147777,"y useless in the summary because it is unable to answer the query need. Therefore, the surface features are inadequate to measure the query relevance, which further augments the error of the whole summarization system. This drawback partially explains why it might achieve acceptable performance to adopt generic summarization models in the query-focused summarization task (e.g., (Gillick and Favre, 2009)). Intuitively, the isolation problem can be solved with a joint model. Meanwhile, neural networks have shown to generate better representations than surface features in the summarization task (Cao et al., 2015b; Yin and Pei, 2015). Thus, a joint neural network model should be a nice solution to extractive query-focused summarization. To this end, we propose a novel summarization system called AttSum, which joints query relevance ranking and sentence saliency ranking with a neural attention model. The attention mechanism has been successfully applied to learn alignment between various modalities (Chorowski et al., 2014; Xu et al., 2015; Bahdanau et al., 2014). In addition, the work of (Kobayashi et al., 2015) demonstrates that it is reasonably good to use the similarity between the sentence embeddin"
C16-1053,P16-1046,0,0.0295447,"th manual and system summaries for the task of summary evaluation. Their method, however, did not surpass ROUGE. Recently, some works (Cao et al., 2015a; Cao et al., 2015b) have tried to use neural networks to complement sentence ranking features. Although these models achieved the state-of-the-art performance, they still heavily relied on hand-crafted features. A few researches explored to directly measure similarity based on distributed representations. (Yin and Pei, 2015) trained a language model based on convolutional neural networks to project sentences onto distributed representations. (Cheng and Lapata, 2016) treated single document summarization as a sequence labeling task and modeled it by the recurrent neural networks. Others like (Kobayashi et al., 2015; K˚ageb¨ack et al., 2014) just used the sum of trained word embeddings to represent sentences or documents. In addition to extractive summarization, deep learning technologies have also been applied to compressive and abstractive summarization. (Filippova et al., 2015) used word embeddings and Long Short Term Memory models (LSTMs) to output readable and informative sentence compressions. (Rush et al., 2015; Hu et al., 2015) leveraged the neural"
C16-1053,D15-1042,0,0.00650885,"based on distributed representations. (Yin and Pei, 2015) trained a language model based on convolutional neural networks to project sentences onto distributed representations. (Cheng and Lapata, 2016) treated single document summarization as a sequence labeling task and modeled it by the recurrent neural networks. Others like (Kobayashi et al., 2015; K˚ageb¨ack et al., 2014) just used the sum of trained word embeddings to represent sentences or documents. In addition to extractive summarization, deep learning technologies have also been applied to compressive and abstractive summarization. (Filippova et al., 2015) used word embeddings and Long Short Term Memory models (LSTMs) to output readable and informative sentence compressions. (Rush et al., 2015; Hu et al., 2015) leveraged the neural attention model (Bahdanau et al., 2014) in the machine translation area to generate one-sentence summaries. We have described these methods in Section 2.2. 6 Conclusion and Future Work This paper proposes a novel query-focused summarization system called AttSum which jointly handles saliency ranking and relevance ranking. It automatically generates distributed representations for sentences as well as the document clu"
C16-1053,W06-1643,0,0.0186371,"o find the optimal solution (McDonald, 2007; Gillick and Favre, 2009). Graph-based models played a leading role in the extractive summarization area, due to its ability to reflect various sentence relationships. For example, (Wan and Xiao, 2009) adopted manifold ranking to make use of the within-document sentence relationships, the cross-document sentence relationships and the sentence-to-query relationships. In contrast to these unsupervised approaches, there are also various learning-based summarization systems. Different classifiers have been explored, e.g., conditional random field (CRF) (Galley, 2006), Support Vector Regression (SVR) (Ouyang et al., 2011), and Logistic Regression (Li et al., 2013), etc. Many query-focused summarizers are heuristic extensions of generic summarization methods by incorporating the information of the given query. A variety of query-dependent features were defined to measure the relevance, including TF-IDF cosine similarity (Wan and Xiao, 2009), WordNet similarity (Ouyang et al., 2011), and word co-occurrence (Prasad Pingali and Varma, 2007), etc. However, these features usually reward sentences similar to the query, which fail to meet the query need. 5.2 Deep"
C16-1053,W09-1802,0,0.148632,"document cluster embeddings. “⊕” stands for a pooling operation, while “⊗” represents a relevance measurement function. for reference. Apparently, even if a sentence is exactly the same as the query, it is still totally useless in the summary because it is unable to answer the query need. Therefore, the surface features are inadequate to measure the query relevance, which further augments the error of the whole summarization system. This drawback partially explains why it might achieve acceptable performance to adopt generic summarization models in the query-focused summarization task (e.g., (Gillick and Favre, 2009)). Intuitively, the isolation problem can be solved with a joint model. Meanwhile, neural networks have shown to generate better representations than surface features in the summarization task (Cao et al., 2015b; Yin and Pei, 2015). Thus, a joint neural network model should be a nice solution to extractive query-focused summarization. To this end, we propose a novel summarization system called AttSum, which joints query relevance ranking and sentence saliency ranking with a neural attention model. The attention mechanism has been successfully applied to learn alignment between various modaliti"
C16-1053,D15-1229,0,0.108359,"and, if a sentence is salient in the document cluster, its embedding should be representative. As a result, the weighted-sum pooling generates the document representation which is automatically biased to embeddings of sentences match both documents and the query. AttSum simulates human attentive reading behavior, and the attention mechanism in it has actual meaning. The experiments to be presented in Section 4.6 will demonstrate its strong ability to catch query relevant sentences. Actually, the attention mechanism has been applied in one-sentence summary generation before (Rush et al., 2015; Hu et al., 2015). The success of these works, however, heavily depends on the hand-crafted features. We believe that the attention mechanism may not be able to play its anticipated role if it is not used appropriately. 2.3 Ranking Layer Since the semantics directly lies in sentence and document embeddings, we rank a sentence according to its embedding similarity to the document cluster, following the work of (Kobayashi et al., 2015). Here we adopt cosine similarity: v(s) • v(d|q)T cos(d, s|q) = (5) ||v(s) ||• ||v(d|q)|| Compared with Euclidean distance, one advantage of cosine similarity is that it is automat"
C16-1053,W14-1504,0,0.158097,"Missing"
C16-1053,D15-1232,0,0.0981039,"rks have shown to generate better representations than surface features in the summarization task (Cao et al., 2015b; Yin and Pei, 2015). Thus, a joint neural network model should be a nice solution to extractive query-focused summarization. To this end, we propose a novel summarization system called AttSum, which joints query relevance ranking and sentence saliency ranking with a neural attention model. The attention mechanism has been successfully applied to learn alignment between various modalities (Chorowski et al., 2014; Xu et al., 2015; Bahdanau et al., 2014). In addition, the work of (Kobayashi et al., 2015) demonstrates that it is reasonably good to use the similarity between the sentence embedding and document embedding for saliency measurement, where the document embedding is derived from the sum pooling of sentence embeddings. In order to consider the relevance and saliency simultaneously, we introduce the weighted-sum pooling over sentence embeddings to represent the document, where the weight is the automatically learned query relevance of a sentence. In this way, the document representation will be biased to the sentence embeddings which match the meaning of both query and documents. The w"
C16-1053,P13-1099,0,0.185236,"Introduction Query-focused summarization (Dang, 2005) aims to create a brief, well-organized and fluent summary that answers the need of the query. It is useful in many scenarios like news services and search engines, etc. Nowadays, most summarization systems are under the extractive framework which directly selects existing sentences to form the summary. Basically, there are two major tasks in extractive query-focused summarization, i.e., to measure the saliency of a sentence and its relevance to a user’s query. After a long period of research, learning-based models like Logistic Regression (Li et al., 2013) etc. have become growingly popular in this area. However, most current supervised summarization systems often perform the two tasks in isolation. Usually, they design query-dependent features (e.g., query word overlap) to learn the relevance ranking, and query-independent features (e.g., term frequency) to learn the saliency ranking. Then, the two types of features are combined to train an overall ranking model. Note that the only supervision available is the reference summaries. Humans write summaries with the trade-off between relevance and saliency. Some salient content may not appear in r"
C16-1053,W04-1013,0,0.0254933,"according to its embedding similarity to the document cluster, following the work of (Kobayashi et al., 2015). Here we adopt cosine similarity: v(s) • v(d|q)T cos(d, s|q) = (5) ||v(s) ||• ||v(d|q)|| Compared with Euclidean distance, one advantage of cosine similarity is that it is automatically scaled. According to (K˚ageb¨ack et al., 2014), cosine similarity is the best metrics to measure the embedding similarity for summarization. In the training process, we apply the pairwise ranking strategy (Collobert et al., 2011) to tune model parameters. Specifically, we calculate the ROUGE-2 scores (Lin, 2004) of all the sentences in the training dataset. Those sentences with high ROUGE-2 scores are regarded as positive samples, and the rest as negative samples. Afterwards, we randomly choose a pair of positive and negative sentences which are denoted as s+ and s− , respectively. Through the CNN Layer and Pooling Layer, we generate the embeddings of v(s+ ), v(s− ) and v(d|q). We can then obtain the ranking scores of s+ and s− according to Eq. 5. With the pairwise ranking criterion, AttSum should give a positive sample a higher score in comparison with a negative sample. The cost function is defined"
C16-1053,W12-2601,0,0.0619964,"= 50 for all the rest experiments. It is the same dimension as the word embeddings. During the training of pairwise ranking, we set the margin Ω = 0.5. The initial learning rate is 0.1 and batch size is 100. 4.3 Evaluation Metric For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2004) 3 . It measures the summary quality by counting the overlapping units such as the n-grams, word sequences and word pairs between the peer summary and reference summaries. We take ROUGE-2 as the main measures due to its high capability of evaluating automatic summarization systems (Owczarzak et al., 2012). During the training data of pairwise ranking, we also rank the sentences according to ROUGE-2 scores. 4.4 Baselines To evaluate the summarization performance of AttSum, we implement rich extractive summarization methods. Above all, we introduce two common baselines. The first one just selects the leading sentences to form a summary. It is often used as an official baseline of DUC, and we name it “LEAD”. The other system is called “QUERY SIM”, which directly ranks sentences according to its TF-IDF cosine similarity to the query. In addition, we implement two popular extractive query-focused s"
C16-1053,D15-1044,0,0.141032,"rge. On the other hand, if a sentence is salient in the document cluster, its embedding should be representative. As a result, the weighted-sum pooling generates the document representation which is automatically biased to embeddings of sentences match both documents and the query. AttSum simulates human attentive reading behavior, and the attention mechanism in it has actual meaning. The experiments to be presented in Section 4.6 will demonstrate its strong ability to catch query relevant sentences. Actually, the attention mechanism has been applied in one-sentence summary generation before (Rush et al., 2015; Hu et al., 2015). The success of these works, however, heavily depends on the hand-crafted features. We believe that the attention mechanism may not be able to play its anticipated role if it is not used appropriately. 2.3 Ranking Layer Since the semantics directly lies in sentence and document embeddings, we rank a sentence according to its embedding similarity to the document cluster, following the work of (Kobayashi et al., 2015). Here we adopt cosine similarity: v(s) • v(d|q)T cos(d, s|q) = (5) ||v(s) ||• ||v(d|q)|| Compared with Euclidean distance, one advantage of cosine similarity is"
D14-1054,D08-1083,0,0.0209071,"Sentiment Analysis Duyu Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contai"
D14-1054,P14-2009,1,0.766966,"hou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does"
D14-1054,P13-2087,0,0.0212728,"using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is"
D14-1054,P11-2008,0,0.131527,"Missing"
D14-1054,P11-1015,0,0.3622,"cial Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic ch"
D14-1054,D13-1171,0,0.046832,"Missing"
D14-1054,J11-2001,0,0.0176143,"notated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like e"
D14-1054,S13-2053,0,0.183839,"such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does not capture the phrasal information. The segmentations based on syntactic chunkers typically aim to identify noun groups, verb groups or named entities from a sentence. However, many sentiment indicators are phrases constituted of adjectives, negations, adverbs or idioms (Liu, 2012; Mohammad et al., 2013a), which are splitted by syntactic chunkers. Besides, a better approach would be to utilize the sentiment information to improve the segmentor. Accordingly, the sentiment-specific segmentor will enhance the performance of sentiment classification in turn. In this paper, we propose a joint segmentation and classification framework (JSC) for sentiment analysis, which simultaneous conducts sentence segmentation and sentence-level sentiment classification. The framework is illustrated in FigIn this paper, we propose a joint segmentation and classification framework for sentiment analysis. Existin"
D14-1054,C14-1018,1,0.854768,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,N10-1120,0,0.161612,"Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, b"
D14-1054,P14-1146,1,0.670131,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,pak-paroubek-2010-twitter,0,0.0427636,"pically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like emoticons (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Zhao et al., 2012). Majority of existing approaches follow Pang et al. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learnin"
D14-1054,P10-1141,0,0.148379,"Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-w"
D14-1054,P02-1053,0,0.0058165,"the joint model from sentences annotated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corp"
D14-1054,W02-1011,0,0.0553792,"fication performance. The joint model is trained only based on the annotated sentiment polarity of sentences, without any segmentation annotations. Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that, our joint model performs comparably with the state-of-the-art methods. 1 Introduction Sentiment classification, which classifies the sentiment polarity of a sentence (or document) as positive or negative, is a major research direction in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Majority of existing approaches follow Pang et al. (2002) and treat sen∗ This work was partly done when the first and fourth authors were visiting Microsoft Research. 477 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477–487, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics CG SC SEG SEG SC that is not bad -1 <+1,-1> NO 0.6 0.6 that is not bad that is not bad -1 <+1,-1> NO 0.4 0.4 Polarity: +1 that is not bad +1 <+1,+1> YES 2.3 2.3 that is not bad +1 <+1,+1> YES 1.6 1.6 Segmentations Polarity Update Rank Update Input Top K Figure 1: The joint segmentation and c"
D14-1054,P12-2018,0,0.141338,"l. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learning effective features to obtain better classification performance. On movie or product reviews, Wang and Manning (2012) present NBSVM, which trades-off • To our knowledge, this is the first work that automatically produces sentence segmentation for sentiment classification within a joint framework. • We show that the joint model yields comparable performance with the state-of-the-art methods on the benchmark Twitter sentiment classification datasets in SemEval 2013. 478 overview of the proposed joint segmentation and classification model (JSC) for sentiment analysis. The segmentation candidate generation model and the segmentation ranking model are described in Section 4. The details of the sentiment classific"
D14-1054,H05-1044,0,0.0828802,"Missing"
D14-1054,D11-1014,0,0.256554,"ytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a sepa"
D14-1054,D13-1016,0,0.0546336,"Missing"
D14-1054,D12-1110,0,0.0455462,"Missing"
D14-1054,D11-1016,0,0.0130106,"the document feature. On Twitter, Mohammad et al. (2013b) develop a state-of-the-art Twitter sentiment classifier in SemEval 2013, using a variety of sentiment lexicons and hand-crafted features. With the revival of deep learning (representation learning (Hinton and Salakhutdinov, 2006; Bengio et al., 2013; Jones, 2014)), more recent studies focus on learning the low-dimensional, dense and real-valued vector as text features for sentiment classification. Glorot et al. (2011) investigate Stacked Denoising Autoencoders to learn document vector for domain adaptation in sentiment classification. Yessenalina and Cardie (2011) represent each word as a matrix and compose words using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabili"
D14-1054,P14-1011,1,0.830375,"ords or phrases of variable length. Under this scenario, phrase embedding is highly suitable as it is capable to represent phrases with different length into a consistent distributed vector space (Mikolov et al., 2013). For each phrase, phrase embedding is a dense, real-valued and continuous vector. After the phrase embedding is trained, the nearest neighbors in the embedding space are favored to have similar grammatical usages and semantic meanings. The effectiveness of phrase embedding has been verified for building large-scale sentiment lexicon (Tang et al., 2014a) and machine translation (Zhang et al., 2014). We learn phrase embedding with Skip-Gram model (Mikolov et al., 2013), which is the state-of(2) k where φij is the segmentation score of Ωij ; sf eijk is the k-th segmentation feature of Ωij ; w and b are the parameters of the segmentation ranking model. During training, given a sentence si and its gold sentiment polarity polig , the optimization objec2 j∈Hi Segmentation-Specific Feature We empirically design four segmentation-specific features to reflect the information of each segmentation, as listed in Table 3. The objective of the segmentation ranking model is to assign a scalar to each"
D14-1054,D13-1170,0,0.203308,"ng Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal"
D14-1054,J13-3004,0,\N,Missing
D14-1054,P11-1016,1,\N,Missing
D16-1081,W14-1605,0,0.0311354,"lack ones are associated with clues. Compared with our riddle task, the clues in the CPs are derived from each question where the radicals in solution are derived from the metaphors in the riddles. Proverb (Littman et al., 2002) is the first system for the automatic resolution of CPs. Ernandes et al. (2005) utilize a web-search module to find sensible candidates to questions expressed in natural language and get the final answer by ranking the candidates. And the rule-based module and the dictionary module are mentioned in his work. The tree kernel is used to rerank the candidates proposed by Barlacchi et al. (2014) for automatic resolution of crossword puzzles. From another perspective, there are a few projects Offline Learning Riddle/Solution Pairs Phrase-Radical Alignment and Rule Learning Alignment Table Rule Table Riddle Solving Solution Solution Ranking Solution Candidate Generation Riddle Description Riddle Ranking Riddle Candidate Generation Solution (Chinese Character) Riddle Generation Riddle Description Figure 3: The pipeline of offline learning, riddle solving and riddle generation on Chinese language cultures, such as the couplet generation and the poem generation. A statistical machine tran"
D16-1081,C92-4176,0,0.132808,"Missing"
D16-1081,C08-1048,1,0.756656,"on Pairs Phrase-Radical Alignment and Rule Learning Alignment Table Rule Table Riddle Solving Solution Solution Ranking Solution Candidate Generation Riddle Description Riddle Ranking Riddle Candidate Generation Solution (Chinese Character) Riddle Generation Riddle Description Figure 3: The pipeline of offline learning, riddle solving and riddle generation on Chinese language cultures, such as the couplet generation and the poem generation. A statistical machine translation (SMT) framework is proposed to generate Chinese couplets and classic Chinese poetry (He et al., 2012; Zhou et al., 2009; Jiang and Zhou, 2008). Jiang and Zhou (2008) use a phrasebased SMT model with linguistic filters to generate Chinese couplets satisfied couplet constraints, using both human judgments and BLEU scores as the evaluation. Zhou et al. (2009) use the SMT model to generate quatrain with a human evaluation. He et al. (2012) generate Chinese poems with the given topic words by combining a statistical machine translation model with an ancient poetic phrase taxonomy. Following the approaches in SMT framework, it is valid to regard the metaphors with its radicals as the alignments. There are several works using neural networ"
D16-1081,J03-1002,0,0.00555444,"d to extract the metaphors based on the phrase-radical alignments and rules. We exploit the phrase-radical alignments as to de848 scribe the simple metaphors, e.g. “千 里” aligns “马”, which aligns the phrase and the radical by the meaning. We employ a statistical framework with a word alignment algorithm to automatically mine phrase-radical metaphors from riddle dataset. Considering the alignment is often represented as the matching between successive words in the riddle and a radical in the solution, we propose two methods specifically to extract alignments. The first method in according with (Och and Ney, 2003) is described as follows. With a riddle description q and corresponding solution s, we tokenize the input riddle q to character as (w1 , w2 , . . . , wn ) and decompose the solution s into radicals as (r1 , r2 , . . . , rm ). We count all ([wi , wj ], rk )(i, j ∈ [1, n], k ∈ [1, m]) as alignments. The second method takes into account more structural information of characters. Let (w1 , w2 ) denote two successive characters in the riddle q. If w1 is a radical of w2 and the rest parts of w2 as r appear in the solution q, we strongly support that ((w1 , w2 ), r) is a alignment. It is identical if"
D16-1081,D14-1074,0,0.0239932,"a phrasebased SMT model with linguistic filters to generate Chinese couplets satisfied couplet constraints, using both human judgments and BLEU scores as the evaluation. Zhou et al. (2009) use the SMT model to generate quatrain with a human evaluation. He et al. (2012) generate Chinese poems with the given topic words by combining a statistical machine translation model with an ancient poetic phrase taxonomy. Following the approaches in SMT framework, it is valid to regard the metaphors with its radicals as the alignments. There are several works using neural network to generate Chinese poems(Zhang and Lapata, 2014; Yi et al., 2016). Due to the limited data and strict rules, it is hard to transfer to the riddle generation. 3 Phrase-Radical Alignments and Rules The metaphor is one of the key components in both solving and generation. On the one hand we need to identify these metaphors since each of them aligns a radical in the final solution. On the other hand, we need to integrate these metaphors into the riddle descriptions to generate riddles. Thus, how to extract the metaphors of riddles becomes a big challenge in our task. Below we introduce our method to extract the metaphors based on the phrase-ra"
D16-1081,Y09-1006,1,0.90926,"rning Riddle/Solution Pairs Phrase-Radical Alignment and Rule Learning Alignment Table Rule Table Riddle Solving Solution Solution Ranking Solution Candidate Generation Riddle Description Riddle Ranking Riddle Candidate Generation Solution (Chinese Character) Riddle Generation Riddle Description Figure 3: The pipeline of offline learning, riddle solving and riddle generation on Chinese language cultures, such as the couplet generation and the poem generation. A statistical machine translation (SMT) framework is proposed to generate Chinese couplets and classic Chinese poetry (He et al., 2012; Zhou et al., 2009; Jiang and Zhou, 2008). Jiang and Zhou (2008) use a phrasebased SMT model with linguistic filters to generate Chinese couplets satisfied couplet constraints, using both human judgments and BLEU scores as the evaluation. Zhou et al. (2009) use the SMT model to generate quatrain with a human evaluation. He et al. (2012) generate Chinese poems with the given topic words by combining a statistical machine translation model with an ancient poetic phrase taxonomy. Following the approaches in SMT framework, it is valid to regard the metaphors with its radicals as the alignments. There are several wo"
D16-1081,J92-1011,0,\N,Missing
D17-1007,D11-1072,0,0.100881,"Missing"
D17-1007,Q15-1023,0,0.0938631,"Missing"
D17-1007,P14-5010,0,0.00445278,"tities based on the sentence search instead of the common method using entity search. There are some issues in the original annotations because of the annotation regulation. First, entities in their own pages are usually not annotated. Thus we annotate these entities with matching between the text and the page title. Second, entities are usually annotated only in their first appearance. We annotate these entities if they are annotated in previous sentences in the page. Moreover, pronouns are widely used in Wikipedia sentences and are usually not annotated. We use the Stanford CoreNLP toolkit (Manning et al., 2014) to do the coreference resolution. In addition, we use the content in the disambiguation page and the infobox. Although these two kinds of information may have incomplete grammatical structure, it contains enough context information for the sentence search in our task. We use the Wikipedia snapshot of May 1, 2016, which contains 4.45 million pages and 120 million sentences. We extract sentences that contain at least one anchor in the Wikipedia articles, and Our work is different from using search engines to generate candidates. We firstly propose to search Wikipedia sentences and take advantag"
D17-1007,D07-1074,0,0.211261,"lake Shelton]]. 2 Related Work Recognizing entity mentions in text and linking them to the corresponding entries helps to understand documents and queries. Most work uses the knowledge base including Freebase (Chiu et al., 2014), YAGO (Yosef et al., 2011) and Dbpedia (Olieman et al., 2014). Wikify (Mihalcea and Csomai, 2007) is the very early work on linking anchor texts to Wikipedia pages. It extracts all ngrams that match Wikipedia concepts such as anchors and titles as candidates. They implement a voting scheme based on the knowledge-based and data-driven method to disambiguate candidates. Cucerzan (2007) uses four recourses to generate candidates, namely entity pages, redirecting pages, disambiguation pages, and list pages. Then they disambiguate candidates by calculating the similarity between the contextual information and the document as well as category tags on Wikipedia pages. Milne and Witten (2008) generate candidates by gathering all n-grams in the document, and retaining those whose probability exceeds a low threshold. Then they define commonness and relatedness on the hyper-link structure of Wikipedia to disambiguate candidates. The work on linking entities in queries has been Table"
D18-1088,N18-1150,0,0.059276,"017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-based methods (Cheng and Lapata, 2016) or by maximizing the ROUGE score (Lin, 2004) between a subset of sentences and the human written summaries (Nallapati et al., 2017). These methods do not fully exploit the human summaries, they only create True/False labels which might be suboptimal. Extractive summari"
D18-1088,P16-1046,1,0.940453,"Zhou† † Microsoft Research Asia, Beijing, China ‡ Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh, UK {xizhang,fuwei,mingzhou}@microsoft.com,mlap@inf.ed.ac.uk Abstract (Mihalcea, 2005), and integer linear programming (Woodsend and Lapata, 2010). The successful application of neural network models to a variety of NLP tasks and the availability of large scale summarization datasets (Hermann et al., 2015; Nallapati et al., 2016) has provided strong impetus to develop data-driven approaches which take advantage of continuousspace representations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 201"
D18-1088,W04-1017,0,0.0245229,"001 and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention: extractive approaches generate summaries by copying parts of the source document (usually whole sentences), while abstractive methods may generate new words or phrases which are not in the document. A great deal of previous work has focused on extractive summarization which is usually modeled as a sentence ranking or binary classification problem (i.e., sentences which are top ranked or predicted as True are selected as summaries). Early attempts mostly leverage human-engineered features (Filatova and Hatzivassiloglou, 2004) coupled with binary classifiers (Kupiec et al., 1995), hidden Markov models (Conroy and O’leary, 2001), graph based methods 779 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 779–784 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics sum sent1 Latent Variables 1 0 sent1 sent2 sum sent2 1 0 sent3 sent4 ment are given (methods for obtaining these labels are discussed in Section 3). As shown in the lower part of Figure 1, our extractive model has three parts: a sentence encoder to convert each sentence"
D18-1088,P15-1162,0,0.072593,"Missing"
D18-1088,E17-2068,0,0.0115852,"3.30 17.28 15.75 15.82 16.20 17.52 18.20 18.45 18.77 15.43 R-L 36.57 35.50 32.65 36.38 39.08 36.90 35.30 36.39 36.60 37.14 37.54 34.33 Table 1: Results of different models on the CNN/Dailymail test set using full-length F1 ROUGE -1 (R-1), ROUGE -2 (R-2), and ROUGE -L (R-L). regularized all LSTMs with a dropout rate of 0.3 (Srivastava et al., 2014; Zaremba et al., 2014). We also applied word dropout (Iyyer et al., 2015) at rate 0.2. We set the hidden unit size d = 300 for both word-level and sentence-level LSTMs and all LSTMs had one layer. We used 300 dimensional pre-trained FastText vectors (Joulin et al., 2017) to initialize our word embeddings. The latent model was initialized from the extractive model (thus both models have the same size) and we set the weight in Equation (7) to α = 0.5. The latent model was trained with SGD, with learning rate 0.01 for 5 epochs. During inference, for both extractive and latent models, we rank sentences with p(yi = True|y1:i−1 , D) and select the top three as summary (see also Equation (3)). Experiments Dataset and Evaluation We conducted experiments on the CNN/Dailymail dataset (Hermann et al., 2015; See et al., 2017). We followed the same pre-processing steps as"
D18-1088,D15-1044,0,0.0753765,"to learn sentence representations, while they use convolutional neural network coupled with max pooling (Kim et al., 2016). 2.2 Sentence Compression We train a sentence compression model to map a sentence selected by the extractive model to a sentence in the summary. The model can be used to evaluate the quality of a selected sentence with respect to the summary (i.e., the degree to which it is similar) or rewrite an extracted sentence according to the style of the summary. For our compression model we adopt a standard attention-based sequence-to-sequence architecture (Bahdanau et al., 2015; Rush et al., 2015). The training set for this model is generated from the same summarization dataset used to train the exractive model. Let D = (S1 , S2 , . . . , S|D |) denote a document and H = (H1 , H2 , . . . , H|H |) its summary. We view each sentence Hi in the summary as a target sentence and assume that its corresponding source is a sentence in D most similar to it. We measure the similarity between source sentences and candidate targets using ROUGE, i.e., Sj = argmaxSj ROUGE(Sj , Hi ) and hSj , Hi i is a training instance for the compresˆi besion model. The probability of a sentence H ˆ ˆ ˆ ing the comp"
D18-1088,W04-1013,0,0.496949,"ver et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-based methods (Cheng and Lapata, 2016) or by maximizing the ROUGE score (Lin, 2004) between a subset of sentences and the human written summaries (Nallapati et al., 2017). These methods do not fully exploit the human summaries, they only create True/False labels which might be suboptimal. Extractive summarization models require sentence-level labels, which are usually created heuristically (e.g., with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to i"
D18-1088,P17-1099,0,0.866109,"esentations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually obtained by rule-b"
D18-1088,N10-1134,0,0.0358833,"nd H = (H1 , H2 , . . . , H|H |) its human summary (Hk is a sentence in H). We assume that there is a latent variable zi ∈ {0, 1} for each sentence Si indicating whether Si should be selected, and zi = 1 entails it should. We use the extractive model from Section 2.1 to produce probability distributions for latent variables (see Equation (3)) and obtain them by sampling zi ∼ p(zi |z1:i−1 , hD i−1 ) (see R(C, H) = α Rp (C, H) + (1 − α) Rr (C, H) (7) Our use of the terms “precision” and “recall” is reminiscent of relevance and coverage in other summarization work (Carbonell and Goldstein, 1998; Lin and Bilmes, 2010; See et al., 2017). We train the model by minimizing the negative expected R(C, H): L(θ) = −E(z1 ,...,z|D |)∼p(·|D) [R(C, H)] (8) where p(·|D) is the distribution produced by the neural extractive model (see Equation (3)). Unfortunately, computing the expectation term is prohibitive, since the possible latent variable combinations are exponential. In practice, we approximate this expectation with a single sample from 1 We also experimented with unnormalized probabilities (i.e., excluding the exp in Equation (4)), however we obtained inferior results. 781 Model L EAD 3 L EAD 3 (Nallapati et al"
D18-1088,W01-0100,0,0.709066,"are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes directly from gold summaries. Experiments on the CNN/Dailymail dataset show that our model improves over a strong extractive baseline trained on heuristically approximated labels and also performs competitively to several recent models. 1 Introduction Document summarization aims to automatically rewrite a document into a shorter version while retaining its most important content. Of the many summarization paradigms that have been identified over the years (see Mani 2001 and Nenkova and McKeown 2011 for comprehensive overviews), two have consistently attracted attention: extractive approaches generate summaries by copying parts of the source document (usually whole sentences), while abstractive methods may generate new words or phrases which are not in the document. A great deal of previous work has focused on extractive summarization which is usually modeled as a sentence ranking or binary classification problem (i.e., sentences which are top ranked or predicted as True are selected as summaries). Early attempts mostly leverage human-engineered features (Fil"
D18-1088,P05-3013,0,0.0543716,"Missing"
D18-1088,P10-1058,1,0.908096,"Missing"
D18-1088,K16-1028,0,0.373973,"of continuousspace representations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which are not included in most summarization datasets (only document and gold summary pairs are available). Sentence labels are usually"
D18-1088,N18-1158,1,0.825962,"(Hermann et al., 2015; Nallapati et al., 2016) has provided strong impetus to develop data-driven approaches which take advantage of continuousspace representations. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to learn context dependent sentence representations for a document and then use yet another LSTM decoder to predict a binary label for each sentence. Nallapati et al. (2017) adopt a similar approach, they differ in their neural architecture for sentence encoding and the features used during label prediction, while Narayan et al. (2018) equip the same architecture with a training algorithm based on reinforcement learning. Abstractive models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) are based on sequenceto-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015), however, most of them underperform or are on par with the baseline of simply selecting the leading sentences in the document as summaries (but see Paulus et al. 2017 and Celikyilmaz et al. 2018 for exceptions). Although seemingly more successful than their abstractive counterparts, extractive models require sentence-level labels, which"
D18-1232,D15-1237,0,\N,Missing
D18-1232,D16-1264,0,\N,Missing
D18-1232,D16-1180,0,\N,Missing
D18-1232,P17-1147,0,\N,Missing
D18-1232,D17-1215,0,\N,Missing
D18-1232,P17-1018,1,\N,Missing
D18-1232,D17-1264,0,\N,Missing
D18-1232,P18-1158,0,\N,Missing
D18-1232,P18-1078,0,\N,Missing
D18-1271,C16-1171,0,0.0159595,"nu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates,"
D18-1271,D14-1092,1,0.890876,"Missing"
D18-1271,D12-1025,1,0.84361,"s languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsuperv"
D18-1271,D14-1061,1,0.825032,"ast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsupervised, effective, in"
D18-1271,N07-2008,0,0.0207658,"0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and"
D18-1271,P98-1069,0,0.239043,"slation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel co"
D18-1271,D16-1075,1,0.854091,"nowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge. Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment. 2496 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2"
D18-1271,C16-1309,1,0.941152,"nowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge. Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment. 2496 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2"
D18-1271,N13-1056,0,0.0349868,"Missing"
D18-1271,W09-3107,1,0.81454,"ed alignment, exploring a paradigm for language knowledge acquisition. • We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches. • We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task. 2 Burst Information Network A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections. A BINet is originally defined as G = hV, E, ωi in (Ge et al., 2016a). Each node v ∈ V is a burst element defined as a burst word1 during one of its burst periods hw, Pi where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge  ∈ E indicates the connection between two burst elements with the weight ω which is defined as the number of doc"
D18-1271,D15-1015,0,0.0135832,"10; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods,"
D18-1271,P06-1103,0,0.0425456,"i and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast"
D18-1271,W02-0902,0,0.0611221,"tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are"
D18-1271,W11-2125,0,0.0196081,"guage knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Ro"
D18-1271,N16-1132,0,0.031503,"Missing"
D18-1271,P08-1088,0,0.0159272,"d (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of t"
D18-1271,D14-1198,1,0.826094,"nt, exploring a paradigm for language knowledge acquisition. • We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches. • We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task. 2 Burst Information Network A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections. A BINet is originally defined as G = hV, E, ωi in (Ge et al., 2016a). Each node v ∈ V is a burst element defined as a burst word1 during one of its burst periods hw, Pi where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge  ∈ E indicates the connection between two burst elements with the weight ω which is defined as the number of documents where these"
D18-1271,W11-2206,1,0.807265,"y for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and"
D18-1271,D11-1006,0,0.0252557,", 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By"
D18-1271,J05-4003,0,0.0814633,"ments with a low score (&lt; 0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016;"
D18-1271,D14-1162,0,0.0810082,"ge. Given that our approach is unsupervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access. For future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach’s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications. Acknowledgments We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji’s work has been suppo"
D18-1271,E09-1091,0,0.0212751,"lignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the"
D18-1271,C10-1124,0,0.0211684,"eans it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining"
D18-1271,P15-2118,0,0.0147857,"s and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narro"
D18-1271,N18-1202,0,0.0218069,"pervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access. For future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach’s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications. Acknowledgments We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji’s work has been supported by the U.S. DARPA AIDA Pro"
D18-1271,P99-1067,0,0.288414,"ing and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. Howev"
D18-1271,N04-1033,0,0.054922,"37 documents. We removed stopwords, conducted lemmatization and name tagging for the English stream, and did word segmentation and name tagging for the Chinese stream using the Stanford CoreNLP toolkit (Manning et al., 2014). 4 Due to the upper bound of Conf (Gc , Ge ), the algorithm must terminate after several iterations. We detected bursts and constructed the BINets5 for the Chinese and English stream based on (Ge et al., 2016a). The constructed Chinese BINet has 7,360 nodes and 33,892 edges while the English one has 8,852 nodes and 85,125 edges. Our seed bi-lingual lexicon is released by (Zens and Ney, 2004), containing 81,990 Chinese word entries, each of which has an English translation. Among the 7,360 nodes in the Chinese BINet, 2,281 nodes need to be deciphered since their words are not in the bi-lingual lexicon. 4.1.2 Evaluation Setting We evaluate our approach in an end-to-end fashion. For a node c in the Chinese BINet, we choose the node e∗ which has the highest score as c’s counterpart in the English BINet: e∗ = arg max Score(c, e) e∈Cand(c) We rank the aligned node pairs by the score and manually evaluate the quality of the top K pairs. A pair hc,ei is annotated as correct if e is a cor"
D18-1271,P11-1002,0,0.0330427,"n (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that ou"
D18-1271,P10-1115,0,0.0274153,"rger than that used in our experiment and they are endlessly updated. 2503 10 The alignments with a low score (&lt; 0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Ki"
D18-1271,W02-2026,0,0.132146,"Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for genera"
D18-1271,P17-1179,0,0.0253021,"Missing"
D18-1271,D17-1207,0,0.0444246,"Missing"
D18-1271,C04-1089,0,0.0807467,"ingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpor"
D18-1271,N10-1063,0,0.0216071,"endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et a"
D18-1271,P06-1010,0,0.229533,"Chinese BINet, its candidate nodes in the English BINet can be derived as: Cand(c) = {e|P(e) ∩ P(c) 6= ∅} where e ∈ Ve , and P(c) and P(e) are the burst periods of c and e respectively. 3.3 Candidate Verification For the candidate list for c (i.e., Cand(c)), we need to verify each node e ∈ Cand(c) and choose the most probable one as c’s counterpart. Formally, we define Score(c, e) as the credibility score of e being the correct counterpart of c and propose the following novel clues for verification. Pronunciation Inspired by previous work on name translation mining (e.g., (Schafer III, 2006; Sproat et al., 2006; Ji, 2009)), for a node e ∈ Cand(c), if its pronunciation is similar to c, then e is likely to be the translation of c. For a Chinese node c and an English node e, we define Sp as its scaled pronunciation score to measure their pronunciation similarity whose range is [0, 1]: 1 Sp ∈ [0, 1] ∝ LD where LD is the normalized (by e’s length) Levenshtein edit distance between c’s pinyin3 string and e’s word string. Translation For a node e ∈ Cand(c), it is possible that e’s word exists or partially exists in the bi-lingual lexicon. We can exploit the translation clue to verify if e is c’s counterpar"
D18-1271,D12-1003,0,0.0225396,"De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across la"
D19-1217,P11-1020,0,0.0148984,"Missing"
D19-1217,D18-1012,0,0.0401083,"Missing"
D19-1217,D14-1162,0,0.0862165,"Missing"
D19-1424,D19-1539,0,0.0219444,"Missing"
D19-1424,I05-5002,0,0.0145753,"tes the angle between the current optimization direction and the final optimization direction. Then we get the projection values dαi and dβi by computing the deviation degrees between the optimization direction δ i and the axes. 4 Experimental Setup We conduct experiments on four datasets: Multi-genre Natural Language Inference Corpus (MNLI; Williams et al. 2018), Recognizing Textual Entailment (RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009), Stanford Sentiment Treebank (SST-2; Socher et al. 2013), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett 2005). We use the same data split as in (Wang et al., 2019). The accuracy metric is used for evaluation. We employ the pre-trained BERT-large model in our experiments. The cased version of tokenizer is used. We follow the settings and the hyperparameters suggested in (Devlin et al., 2018). The Adam (Kingma and Ba, 2015) optimizer is used for fine-tuning. The number of fine-tuning epochs is selected from {3, 4, 5}. For RTE and MRPC, we set the batch size to 32, and the learning rate to Pre-training Gets a Good Initial Point Across Downstream Tasks Fine-tuning BERT on the usually performs significant"
D19-1424,W07-1401,0,0.0278316,"the cross product of two vectors, and k·k denotes the Euclidean norm. To be specific, we first compute cosine similarity between δ i and δ1 , which indicates the angle between the current optimization direction and the final optimization direction. Then we get the projection values dαi and dβi by computing the deviation degrees between the optimization direction δ i and the axes. 4 Experimental Setup We conduct experiments on four datasets: Multi-genre Natural Language Inference Corpus (MNLI; Williams et al. 2018), Recognizing Textual Entailment (RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009), Stanford Sentiment Treebank (SST-2; Socher et al. 2013), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett 2005). We use the same data split as in (Wang et al., 2019). The accuracy metric is used for evaluation. We employ the pre-trained BERT-large model in our experiments. The cased version of tokenizer is used. We follow the settings and the hyperparameters suggested in (Devlin et al., 2018). The Adam (Kingma and Ba, 2015) optimizer is used for fine-tuning. The number of fine-tuning epochs is selected from {3, 4, 5}. For RTE and MRPC, we set the ba"
D19-1424,P18-1031,0,0.357239,"to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language. 1 Introduction Language model pre-training has achieved strong performance in many NLP tasks (Peters et al., 2018; Howard and Ruder, 2018a; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Dong et al., 2019). A neural encoder is trained on a large text corpus by using language modeling objectives. Then the pre-trained model either is used to extract vector representations for input, or is fine-tuned on the specific datasets. Recent work (Tenney et al., 2019b; Liu et al., 2019a; Goldberg, 2019; Tenney et al., 2019a) has shown that the pre-trained models can encode syntactic and semantic information of language. However, it is unclear why pre-training ∗ Contribution during internship at Microsoft Research. is effe"
D19-1424,P18-1132,0,0.0733257,"Missing"
D19-1424,Q16-1037,0,0.0469084,"ons of language, which makes them more invariant across tasks. Moreover, high layers seem to play a more important role in learning task-specific information during fine-tuning. 8 Related Work Pre-trained contextualized word representations learned from language modeling objectives, such as CoVe (McCann et al., 2017), ELMo (Peters et al., 2018), ULMFit (Howard and Ruder, 2018b), GPT (Radford et al., 2018, 2019), and BERT (Devlin et al., 2018), have shown strong performance on a variety of natural language processing tasks. Recent work of inspecting the effectiveness of the pre-trained models (Linzen et al., 2016; Kuncoro et al., 2018; Tenney et al., 2019b; Liu et al., 2019a) focuses on analyzing the syntactic and semantic properties. Tenney et al. (2019b) and Liu et al. (2019a) suggest that pre-training helps the models to encode much syntactic information and many transferable features through evaluating models on several probing tasks. Goldberg (2019) assesses the syntactic abilities of BERT and draws the similar conclusions. Our work explores the effectiveness of pre-training from another angle. We propose to visualize the loss landscapes and optimization trajectories of the BERT fine-tuning proce"
D19-1424,N19-1112,0,0.333702,"ine-tuning, which suggests that the layers that are close to input learn more transferable representations of language. 1 Introduction Language model pre-training has achieved strong performance in many NLP tasks (Peters et al., 2018; Howard and Ruder, 2018a; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Dong et al., 2019). A neural encoder is trained on a large text corpus by using language modeling objectives. Then the pre-trained model either is used to extract vector representations for input, or is fine-tuned on the specific datasets. Recent work (Tenney et al., 2019b; Liu et al., 2019a; Goldberg, 2019; Tenney et al., 2019a) has shown that the pre-trained models can encode syntactic and semantic information of language. However, it is unclear why pre-training ∗ Contribution during internship at Microsoft Research. is effective on downstream tasks in terms of both trainability and generalization capability. In this work, we take BERT (Devlin et al., 2018) as an example to understand the effectiveness of pretraining. We visualize the loss landscapes and the optimization procedure of fine-tuning on specific datasets in three ways. First, we compute the one-dimensional (1D) los"
D19-1424,P19-1441,0,0.309652,"ine-tuning, which suggests that the layers that are close to input learn more transferable representations of language. 1 Introduction Language model pre-training has achieved strong performance in many NLP tasks (Peters et al., 2018; Howard and Ruder, 2018a; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Dong et al., 2019). A neural encoder is trained on a large text corpus by using language modeling objectives. Then the pre-trained model either is used to extract vector representations for input, or is fine-tuned on the specific datasets. Recent work (Tenney et al., 2019b; Liu et al., 2019a; Goldberg, 2019; Tenney et al., 2019a) has shown that the pre-trained models can encode syntactic and semantic information of language. However, it is unclear why pre-training ∗ Contribution during internship at Microsoft Research. is effective on downstream tasks in terms of both trainability and generalization capability. In this work, we take BERT (Devlin et al., 2018) as an example to understand the effectiveness of pretraining. We visualize the loss landscapes and the optimization procedure of fine-tuning on specific datasets in three ways. First, we compute the one-dimensional (1D) los"
D19-1424,N18-1202,0,0.248143,"g procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language. 1 Introduction Language model pre-training has achieved strong performance in many NLP tasks (Peters et al., 2018; Howard and Ruder, 2018a; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Dong et al., 2019). A neural encoder is trained on a large text corpus by using language modeling objectives. Then the pre-trained model either is used to extract vector representations for input, or is fine-tuned on the specific datasets. Recent work (Tenney et al., 2019b; Liu et al., 2019a; Goldberg, 2019; Tenney et al., 2019a) has shown that the pre-trained models can encode syntactic and semantic information of language. However, it is unclear why pre-training ∗ Contribution during internship at Mic"
D19-1424,N18-1101,0,0.0313096,"= vcos = kδ1 k kδ1 k2 s kδ i k 2 dβi = ( ) − (dαi )2 kδ1 k vcos = kδ i k (3) (4) (5) where × denotes the cross product of two vectors, and k·k denotes the Euclidean norm. To be specific, we first compute cosine similarity between δ i and δ1 , which indicates the angle between the current optimization direction and the final optimization direction. Then we get the projection values dαi and dβi by computing the deviation degrees between the optimization direction δ i and the axes. 4 Experimental Setup We conduct experiments on four datasets: Multi-genre Natural Language Inference Corpus (MNLI; Williams et al. 2018), Recognizing Textual Entailment (RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009), Stanford Sentiment Treebank (SST-2; Socher et al. 2013), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett 2005). We use the same data split as in (Wang et al., 2019). The accuracy metric is used for evaluation. We employ the pre-trained BERT-large model in our experiments. The cased version of tokenizer is used. We follow the settings and the hyperparameters suggested in (Devlin et al., 2018). The Adam (Kingma and Ba, 2015) optimizer is used for fi"
D19-1424,W19-4302,0,0.0223545,"1 2 3 4 2.0 1.5 1.0 0.5 0.0 1.0 0.5 0.0 Figure 7: Layer-wise training loss surfaces on the MNLI dataset (top) and the MRPC dataset (bottom). The dotshaped point represents the fine-tuned model. The star-shaped point represents the model with rollbacking different layer groups. Low layers correspond to the 0th-7th layers of BERT, middle layers correspond to the 8th-15th layers, and high layers correspond to the 16th-23rd layers. BERT can achieve better generalization capability than training from scratch. Liu et al. (2019a) find that different layers of BERT exhibit different transferability. Peters et al. (2019) show that the classification tasks build up information mainly in the intermediate and last layers of BERT. Tenney et al. (2019a) observe that low layers of BERT encode more local syntax, while high layers capture more complex semantics. Zhang et al. (2019) also show that not all layers of a deep neural model have equal contributions to model performance. We draw the similar conclusion by visualizing layer-wise loss surface of BERT on downstream tasks. Besides, we find that low layers of BERT are more invariant and transferable across datasets. In the computer vision community, many efforts h"
D19-1424,D13-1170,0,0.042939,"we first compute cosine similarity between δ i and δ1 , which indicates the angle between the current optimization direction and the final optimization direction. Then we get the projection values dαi and dβi by computing the deviation degrees between the optimization direction δ i and the axes. 4 Experimental Setup We conduct experiments on four datasets: Multi-genre Natural Language Inference Corpus (MNLI; Williams et al. 2018), Recognizing Textual Entailment (RTE; Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009), Stanford Sentiment Treebank (SST-2; Socher et al. 2013), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett 2005). We use the same data split as in (Wang et al., 2019). The accuracy metric is used for evaluation. We employ the pre-trained BERT-large model in our experiments. The cased version of tokenizer is used. We follow the settings and the hyperparameters suggested in (Devlin et al., 2018). The Adam (Kingma and Ba, 2015) optimizer is used for fine-tuning. The number of fine-tuning epochs is selected from {3, 4, 5}. For RTE and MRPC, we set the batch size to 32, and the learning rate to Pre-training Gets a Good Initial Point Ac"
D19-1424,P19-1452,0,0.436118,"ore invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language. 1 Introduction Language model pre-training has achieved strong performance in many NLP tasks (Peters et al., 2018; Howard and Ruder, 2018a; Radford et al., 2018; Devlin et al., 2018; Baevski et al., 2019; Dong et al., 2019). A neural encoder is trained on a large text corpus by using language modeling objectives. Then the pre-trained model either is used to extract vector representations for input, or is fine-tuned on the specific datasets. Recent work (Tenney et al., 2019b; Liu et al., 2019a; Goldberg, 2019; Tenney et al., 2019a) has shown that the pre-trained models can encode syntactic and semantic information of language. However, it is unclear why pre-training ∗ Contribution during internship at Microsoft Research. is effective on downstream tasks in terms of both trainability and generalization capability. In this work, we take BERT (Devlin et al., 2018) as an example to understand the effectiveness of pretraining. We visualize the loss landscapes and the optimization procedure of fine-tuning on specific datasets in three ways. First, we compute the one-d"
D19-5802,D14-1179,0,0.0154774,"Missing"
D19-5802,N18-2074,0,0.0275226,"toolkit to preprocess data. We use 300-dimensional GloVe embeddings (Pennington et al., 2014) to initialize word vectors of both questions and passages, and keep them fixed during training. A special trainable token <UNK> is used to represent out-of-vocabulary words. We randomly mask some words in the passage to <UNK> with 0.2 probability while training. The dimension of character embedding and segment embedding is 64 and 128, respectively. The number of Transformer layers used in our model is 12. For each Transformer layer, we set the hidden size to 128, and use relative position embedding (Shaw et al., 2018) whose clipping distance is 16. The number of the attention heads is 8. During training, the batch size is 32 and the number of the max training epochs is 80. We use 3.2 Results Exact match (EM) and F1 scores are two evaluation metrics for SQuAD. EM measures the percentage of the prediction that matches the groundtruth answer exactly, while F1 measures the overlap between the predicted answer answer and the ground-truth answer. The scores on the development set are evaluated by the official script. As shown in Table 1, the unified model outperforms previous state-of-the-art models and the base"
D19-5802,P82-1020,0,0.673204,"Missing"
D19-5802,P17-1147,0,0.024587,"er for the MRC task. Experimental results on SQuAD show that the unified model outperforms previous networks that separately treat encoding and matching. We also introduce a metric to inspect whether a Transformer layer tends to perform encoding or matching. The analysis results show that the unified model learns different modeling strategies compared with previous manually-designed models. 1 Introduction In spite of different neural network structures, encoding and matching components are two basic building blocks for many NLP tasks like machine reading comprehension (Rajpurkar et al., 2016; Joshi et al., 2017). A widely-used paradigm is that the input texts are encoded into vectors, and then these vectors are aggregated to model interactions between them by matching layers. Figure 1(a) shows a typical machine reading comprehension model, encoding components separately encode question and passage to vector representations. Then, we obtain context-sensitive representations for input words by considering the interactions between question and passage. Finally, an output layer is used to predict the prob∗ Contribution during internship at Microsoft Research 14 Proceedings of the Second Workshop on Machi"
D19-5802,P17-1018,1,0.922662,"soft Research hangbobao@gmail.com,piaosh@hit.edu.cn lidong1,fuwei,wenwan,nanya,lecu,mingzhou@microsoft.com Abstract ability of each token being the start or end position of the answer span. The encoding layers are usually built upon recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), and self-attention networks (Yu et al., 2018). For the matching component, various model components have been developed to fuse question and passage vector representations, such as match-LSTM (Wang and Jiang, 2016), coattention (Seo et al., 2016; Xiong et al., 2016), and self-matching (Wang et al., 2017). Recently, Devlin et al. (2018) employ Transformer networks to pretrain a bidirectional language model (called BERT), and then fine-tune the layers on specific tasks, which obtains state-of-the-art results on MRC. A research question is: apart from the benefits of pretraining, how many performance gain comes from the unified network architecture. In this paper, we evaluate and analyze unifying encoding and matching components with Transformer layers (Vaswani et al., 2017), using MRC as a case study. As shown in Figure 1(b), compared with previous specially-designed MRC networks, we do not exp"
D19-5802,D14-1162,0,0.0867023,"re Transformer layers used to compute the question-sensitive passage representations. To make a fair comparison, we only compare with the models that do not rely on pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Experiments Experimental Setup Dataset Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is composed of over 100,000 instances created by crowdworkers. Every answer is constrained to be a continuous sub-span of the passage. Settings We employ the spaCy toolkit to preprocess data. We use 300-dimensional GloVe embeddings (Pennington et al., 2014) to initialize word vectors of both questions and passages, and keep them fixed during training. A special trainable token <UNK> is used to represent out-of-vocabulary words. We randomly mask some words in the passage to <UNK> with 0.2 probability while training. The dimension of character embedding and segment embedding is 64 and 128, respectively. The number of Transformer layers used in our model is 12. For each Transformer layer, we set the hidden size to 128, and use relative position embedding (Shaw et al., 2018) whose clipping distance is 16. The number of the attention heads is 8. Duri"
D19-5802,N18-1202,0,0.0240129,"Apart from comparing with previous state-of-the-art models (Seo et al., 2016; Wang et al., 2017; Yu et al., 2018), we implement a baseline model that separately perform encoding and matching. The same settings as above are used. The first three Transformer layers are utilized to encode passage and question separately. Then we add a passage-question matching layer following Yu et al. (2018), with nine more Transformer layers used to compute the question-sensitive passage representations. To make a fair comparison, we only compare with the models that do not rely on pre-trained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). Experiments Experimental Setup Dataset Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is composed of over 100,000 instances created by crowdworkers. Every answer is constrained to be a continuous sub-span of the passage. Settings We employ the spaCy toolkit to preprocess data. We use 300-dimensional GloVe embeddings (Pennington et al., 2014) to initialize word vectors of both questions and passages, and keep them fixed during training. A special trainable token <UNK> is used to represent out-of-vocabulary words. We randomly ma"
E17-1059,P12-1039,1,0.752484,"our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content planning, sentence planning, and surface realization. Mei et al. (2016) treat database records and output texts as sequences, and use recurrent neural networks to encode and decode them. In contrast, our input is a set of discrete attributes instead of database records or sequences. In addition, the contents of database records are strong constraints on results in concept-to-text generation. However, in our setting, user and product infor"
E17-1059,P16-1004,1,0.841718,"model in order to generate reviews conditioned on input attributes. • We create a dataset based on Amazon book 624 predict results by conditioning outputs on the encoding vectors. This general framework is flexible because different neural networks can be used for encoders and decoders depending on the nature of inputs and outputs, which has been used to address various tasks. For example, recurrent neural networks are used to model sequences, such as machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), syntactic parsing (Vinyals et al., 2015b), and semantic parsing (Dong and Lapata, 2016). Additionally, convolutional neural networks are employed for image data, such as image caption generation (Vinyals et al., 2015a), and video description generation (Donahue et al., 2015; Venugopalan et al., 2015). Our model employs multilayer perceptron to encode attribute information, and uses recurrent neural networks to decode product reviews. In order to better handle alignments between inputs and outputs, the attention mechanism is introduced for the encoder-decoder model. The attention model boosts performance for various tasks (Bahdanau et al., 2015; Luong et al., 2015; Xu et al., 201"
E17-1059,D15-1166,0,0.0304329,"Missing"
E17-1059,W15-2922,0,0.026596,"ries. Recently, deep learning has achieved promising results on sentiment analysis (Socher et al., 2011; Dong et al., 2014; Kim, 2014). Lipton et al. (2015) use character-level concatenated input recurrent neural networks as a generative model to predict rating and category for reviews. In contrast, our model is mainly evaluated on the review generation task rather than classification. Moreover, we use an attention mechanism in our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content"
E17-1059,P82-1020,0,0.85057,"Missing"
E17-1059,D13-1176,0,0.0263763,"valuable for sentiment analysis, but not well studied previously. • We propose an attention-enhanced attributeto-sequence model in order to generate reviews conditioned on input attributes. • We create a dataset based on Amazon book 624 predict results by conditioning outputs on the encoding vectors. This general framework is flexible because different neural networks can be used for encoders and decoders depending on the nature of inputs and outputs, which has been used to address various tasks. For example, recurrent neural networks are used to model sequences, such as machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), syntactic parsing (Vinyals et al., 2015b), and semantic parsing (Dong and Lapata, 2016). Additionally, convolutional neural networks are employed for image data, such as image caption generation (Vinyals et al., 2015a), and video description generation (Donahue et al., 2015; Venugopalan et al., 2015). Our model employs multilayer perceptron to encode attribute information, and uses recurrent neural networks to decode product reviews. In order to better handle alignments between inputs and outputs, the attention mechanism is introduced for the encoder-decoder model. T"
E17-1059,N16-1086,0,0.0159003,"nd hidden Markov models for text generation on review data. However, we focus on generating product reviews conditioned on input attributes. Park et al. (2015) propose to retrieve relevant opinion sentences using product specifications as queries, while we work on generation instead of retrieval. Our task definition is also related to concept-totext generation (Konstas and Lapata, 2012; Konstas and Lapata, 2013), such as generating weather forecast or sportscasting from database records. A typical system contains three main stages: content planning, sentence planning, and surface realization. Mei et al. (2016) treat database records and output texts as sequences, and use recurrent neural networks to encode and decode them. In contrast, our input is a set of discrete attributes instead of database records or sequences. In addition, the contents of database records are strong constraints on results in concept-to-text generation. However, in our setting, user and product information implicitly indicates the style of generated reviews, which makes the results extremely diverse. Another line of related work is the encoderdecoder model with neural networks. Specifically, an encoder is employed to encode"
E17-1059,P02-1040,0,0.117645,"employs distributed representations to avoid using sparse indicator features. Then, we evaluate the NN methods that use different attributes to retrieve reviews, which is a strong baseline for the generation task. The results show that our method outperforms the baseline methods. Moreover, the improvements of the attention mechanism are significant with p < 0.05 according to the bootstrap resampling test (Koehn, 2004). We further show some examples to analyze the attention model in Section 4.4. we use the greedy search algorithm to generate reviews. 4.3 MELM 59.00 Evaluation Results The BLEU (Papineni et al., 2002) score is used for automatic evaluation, which has been shown to correlate well with human judgment on many generation tasks. The BLEU score measures the precision of n-gram matching by comparing the generated results with references, and penalizes length using a brevity penalty term. We compute BLEU-1 (unigram) and BLEU-4 (up to 4 grams) in experiments. 4.3.1 Comparison with Baseline Methods We describe the comparison methods as follows: Rand. The predicted results are randomly sampled from all the reviews in the T RAIN set. This baseline method suggests the expected lower bound for this task"
E17-1059,D14-1181,0,0.00361927,"nce against baseline methods. Moreover, we demonstrate that the attention mechanism significantly improves the performance of our model. The contributions of this work are three-fold: 2 Related Work Sentiment analysis and opinion mining aim to identify and extract subjective content in text (Liu, 2015). Most previous work focuses on using rulebased methods or machine learning techniques for sentiment classification, which classifies reviews into different sentiment categories. Recently, deep learning has achieved promising results on sentiment analysis (Socher et al., 2011; Dong et al., 2014; Kim, 2014). Lipton et al. (2015) use character-level concatenated input recurrent neural networks as a generative model to predict rating and category for reviews. In contrast, our model is mainly evaluated on the review generation task rather than classification. Moreover, we use an attention mechanism in our encoderdecoder model, which has been proved very helpful in various tasks (Bahdanau et al., 2015; Xu et al., 2015), to generate user- and product-specific reviews. Maqsud (2015) compare latent Dirichlet allocation, Markov chains, and hidden Markov models for text generation on review data. However"
J15-1002,baccianella-etal-2010-sentiwordnet,0,0.0149854,"e only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the polarity orientations of the words in WordNet. They select seven positive words and seven negative words and expand them through the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classifier to predict the sentiment polarities of all the words in WordNet and use the glosses (textual definitions of the words in WordNet) as the features of classification. The sentiment lexicon generated is the well-known SentiWordNet.2 2.2 Cross-Lingual Sentimen"
J15-1002,C10-1004,0,0.0343341,"Missing"
J15-1002,D10-1005,0,0.149484,"a. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for a single word. Without sufficient features, it is difficult for these approaches to perform well in learning. Another line of cross-lingual sentiment classification uses Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003) or its variants, like Boyd-Graber and Resnik (2010) or He, Alani, and Zhou (2010). These studies assume that each review is a mixture of sentiments and each sentiment is a probability over words. Then they apply the LDA-like approach to model the sentiment polarity of each review. Nonetheless, this assumption may not be applicable in sentiment lexicon learning because a single word can be regarded as the minimal semantic unit, and it is difficult, if not impossible, to infer the latent topics from a single word. Recall that different from the sentiment classification of product reviews where the instances are normally independent, words in sen"
J15-1002,P11-1061,0,0.0255781,"essary because it is relatively easy to collect from the Web. Consequentially, the novel sentiment information inferred from the parallel corpus can easily update the existing sentiment lexicons. These advantages can greatly improve the coverage of the generated sentiment lexicon, as demonstrated later in our experiments. 3.2 Bilingual Word Graph Label Propagation As commonly used semi-supervised approaches, label propagation (Zhu and Ghahramani 2002) and its variants (Zhu, Ghahramani, and Lafferty 2003; Zhou et al. 2004) have been applied to many applications, such as part-of-speech tagging (Das and Petrov 2011; Li, Graca, and Taskar 2012), image annotation (Wang, Huang, and Ding 2011), protein function prediction (Jiang 2011; Jiang and McQuay 2012), and so forth. 4 http://www.statmt.org/moses/giza/GIZA++.html. 5 http://nlp.cs.berkeley.edu. 27 Computational Linguistics Volume 41, Number 1 The underlying idea of label propagation is that the connected nodes in the graph tend to share the same sentiment labels. In bilingual word graph label propagation, the words tend to share same sentiment labels if they are connected by synonym relations or word alignment and tend to belong to different sentiment l"
J15-1002,P11-2075,0,0.0525109,"Missing"
J15-1002,esuli-sebastiani-2006-sentiwordnet,0,0.0560392,"Missing"
J15-1002,P11-2104,0,0.0862085,"ords to the words in the target language. The few existing approaches first build word relations between English and the target language. Then, based on the word relation and English sentiment seed words, they determine the sentiment polarities of the words in the target language. In these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages. Two approaches are often used to connect the words in different languages in the literature. One is based on translation entries in cross-lingual dictionaries (Hassan et al. 2011). The other relies on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language (Steinberger et al. 2011). The two approaches in Duh, Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (2007) tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language. To solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning. Specifically, we model this task with a bilingual word graph,"
J15-1002,P97-1023,0,0.0418632,"ely leverages the inter-language relations and both types (synonym and antonym) of the intra-language relations in sentiment lexicon learning. 3. We leverage the word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. We build the inter-language relation in the bilingual word graph upon word alignment, and achieve significant results. 2. Related Work 2.1 English Sentiment Lexicon Learning In general, the work on sentiment lexicon learning focuses mainly on English and can be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy 2004). The co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predefined sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the p"
J15-1002,W10-4116,0,0.0466903,"Missing"
J15-1002,C04-1200,0,0.955319,"mputing, the Hong Kong Polytechnic University. E-mail: gaodehong polyu@163.com, cswjli@comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Oku"
J15-1002,N06-1014,0,0.0196801,"Missing"
J15-1002,P11-1033,0,0.014246,"gle Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for a single word. Wit"
J15-1002,P12-1060,1,0.855189,"They manually produce two high-level gold-standard sentiment lexicons for two languages (e.g., English and Spanish) and then translate them into the third language (e.g., Italian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task"
J15-1002,C12-2081,1,0.923795,"They manually produce two high-level gold-standard sentiment lexicons for two languages (e.g., English and Spanish) and then translate them into the third language (e.g., Italian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task"
J15-1002,P07-1123,0,0.137069,"expand them through the see-also and antonym relations in WordNet. These expanded words are then used for training. They train a ternary classifier to predict the sentiment polarities of all the words in WordNet and use the glosses (textual definitions of the words in WordNet) as the features of classification. The sentiment lexicon generated is the well-known SentiWordNet.2 2.2 Cross-Lingual Sentiment Lexicon Learning The work on cross-lingual sentiment lexicon learning is still at an early stage and can be categorized into two types, according to how they bridge the words in two languages. Mihalcea et al. (2007) generate sentiment lexicon for Romanian by directly translating the English sentiment words into Romanian through bilingual English–Romanian dictionaries. When confronting multiword translations, they translate the multiwords word by word. Then the validated translations must occur at least three times on the Web. The approach proposed by Hassan et al. (2011) learns sentiment words based on English WordNet and WordNets in the target languages (e.g., Hindi and Arabic). Crosslingual dictionaries are used to connect the words in two languages and the polarity of a given word is determined by the"
J15-1002,J05-4003,0,0.0143604,"garded as negative. 4. Experiment 4.1 Data Sets We conduct experiments on Chinese sentiment lexicon learning. As in previous work (Baccianella, Esuli, and Sebastiani 2010), the sentiment words in General Inquirer lexicon are selected as the English seeds (Stone 1997). From the GI lexicon we collect 2,005 positive words and 1,635 negative words. To build the bilingual word graph, we adopt the Chinese–English parallel corpus, which is obtained from the news articles published by Xinhua News Agency in Chinese and English collections, using the automatic parallel sentence identification approach (Munteanu and Marcu 2005). Altogether, we collect more than 25M parallel sentence pairs in English and (of) and am) Chinese. We remove all the stopwords in Chinese and English (e.g., together with the low-frequency words that occur fewer than 5 times. After preprocessing, we finally have more than 174,000 English words, among which 3,519 words have sentiment labels and more than 146,000 Chinese words for which we need to predict the sentiment labels. To transfer sentiment information to Chinese unlabeled words more efficiently, we remove the unlabeled English words in the word graph (i.e., XU E = Φ). The unsupervised"
J15-1002,J11-1002,0,0.101363,"both types (synonym and antonym) of the intra-language relations in sentiment lexicon learning. 3. We leverage the word alignment information derived from a large number of parallel sentences in sentiment lexicon learning. We build the inter-language relation in the bilingual word graph upon word alignment, and achieve significant results. 2. Related Work 2.1 English Sentiment Lexicon Learning In general, the work on sentiment lexicon learning focuses mainly on English and can be categorized as co-occurrence–based approaches (Hatzivassiloglou and McKeown 1997; Riloff, Wiebe, and Wilson 2003; Qiu et al. 2011) and semantic-based approaches (Mihalcea, Banea, and Wiebe 2007; Takamura, Inui, and Okumura 2005; Kim and Hovy 2004). The co-occurrence-based approaches determine the sentiment polarity of a given word according to the statistical information, like the co-occurrence of the word to predefined sentiment seed words or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. Fo"
J15-1002,E09-1077,0,0.0937027,"n summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Takamura, Inui, and Okumura 2005; Rao and Ravichandran 2009). However, current work mainly focuses on English sentiment lexicon generation or expansion, while sentiment lexicon learning for other languages has not been well studied. In this article, we address the issue of cross-lingual sentiment lexicon learning, which aims to generate sentiment lexicons for a non-English language (hereafter referred to as “the target language”) with the help of the available English sentiment lexicons. The underlying motivation of this task is to leverage the existing English sentiment lexicons and substantial linguistic resources to label the sentiment polarities of"
J15-1002,W03-0404,0,0.0971793,"ds or the co-occurrence to product features. The statistical information is mainly derived from certain corpora. One of the earliest work conducted by Hatzivassiloglou and McKeown (1997) assumes that the conjunction words can convey the polarity relation of the two words they connect. For example, the conjunction word and tends to link two words with the same polarity, whereas the conjunction word but is likely to link two words with opposite polarities. Their approach only considers adjectives, not nouns or verbs, and it is unable to extract adjectives that are not conjoined by conjunctions. Riloff et al. (2003) define several pattern templates and extract sentiment words by two bootstrapping approaches. Turney and Littman (2003) calculate the pointwise mutual information (PMI) of a given word with positive and negative sets of sentiment words. The sentiment polarity of the word is determined by average PMI values of the positive and negative sets. To obtain PMI, they provide queries (consisting of the given word and the sentiment word) to the search engine. The number of hits and the position (if the given word is near the sentiment word) are used to estimate the association of the given word to the"
J15-1002,W11-1704,0,0.104368,"ord relation and English sentiment seed words, they determine the sentiment polarities of the words in the target language. In these two steps, relation-building plays a fundamental role because it is responsible for the transfer of sentiment information between the two languages. Two approaches are often used to connect the words in different languages in the literature. One is based on translation entries in cross-lingual dictionaries (Hassan et al. 2011). The other relies on a machine translation (MT) engine as a black box to translate the sentiment words in English to the target language (Steinberger et al. 2011). The two approaches in Duh, Fujino, and Nagata (2011) and Mihalcea, Banea, and Weibe (2007) tend to use a small set of vocabularies to translate the natural language, which leads to a low coverage of generated sentiment lexicons for the target language. To solve this problem, we propose a generic approach to addressing the task of cross-lingual sentiment lexicon learning. Specifically, we model this task with a bilingual word graph, which is composed of two intra-language subgraphs and an interlanguage subgraph. The intra-language subgraphs are used to model the semantic relations among the w"
J15-1002,P05-1017,0,0.0288596,"gative) word are positive (negative) and its antonyms are negative (positive). Initializing with a set of sentiment words, they expand sentiment lexicons based on these two kinds of word relations. Kamps et al. (2004) build a synonym graph according to the synonym relation (synset) derived from WordNet. The sentiment polarity of a word is calculated by the shortest path to two sentiment words good and bad. However, the shortest path cannot precisely describe the sentiment orientation, considering there are only five steps between the word good and the word bad in WordNet (Hassan et al. 2011). Takamura et al. (2005) construct a word graph with the gloss of WordNet. Words are connected if a word appears in the gloss of another. The word sentiment polarity is determined by the weight of its connections on the word graph. Based on WordNet, Rao and Ravichandran (2009) exploit several graph-based semi-supervised learning methods like Mincuts and Label Propagation. The word polarity orientations are induced by initializing some sentiment seed words in the WordNet graph. Esuli et al. (2006, 2007) and Baccianella et al. (2010) treat sentiment word learning as a machine learning problem, that is, to classify the"
J15-1002,P09-1027,0,0.0759673,"ian) via Google Translator. They believe that those words in the third language that appear in both translation lists are likely to be sentiment words. These approaches connect the words in two languages based on MT engines. The main concern of these approaches is the low overlapping between the vocabularies of natural documents and the vocabularies of the documents translated by MT engines (Duh, Fujino, and Nagata 2011; Meng et al. 2012a). The shortcoming of these MT-based approaches inevitably leads to low coverage. Our task resembles the task of cross-lingual sentiment classification, like Wan (2009), Lu et al. (2011), and Meng et al. (2012a), which classifies the sentiment polarities of product reviews. Generally, these studies use semi-supervised learning approaches and regard translations from labeled English sentiment reviews as the training data. The terms in each review are leveraged as the features for training, which has proven to be effective in sentiment classification (Pang and Lee 2008). We can regard the task of sentiment lexicon learning as word-level sentiment classification. However, for wordlevel sentiment classification, it is not straightforward to extract features for"
J15-1002,W03-1017,0,0.230886,"are from the Department of Computing, the Hong Kong Polytechnic University. E-mail: gaodehong polyu@163.com, cswjli@comp.polyu.edu.hk. Submission received: 28 April 2013; revised submission received: 25 February 2014; accepted for publication: 12 May 2014. doi:10.1162/COLI a 00207 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 1 1. Introduction A sentiment lexicon is regarded as the most valuable resource for sentiment analysis (Pang and Lee 2008), and lays the groundwork of much sentiment analysis research, for example, sentiment classification (Yu and Hatzivassiloglou 2003; Kim and Hovy 2004) and opinion summarization (Hu and Liu 2004). To avoid manually annotating sentiment words, an automatically learning sentiment lexicon has attracted considerable attention in the community of sentiment analysis. The existing work determines word sentiment polarities either by the statistical information (e.g., the co-occurrence of words with predefined sentiment seed words) derived from a large corpus (Riloff, Wiebe, and Wilson 2003; Hu and Liu 2006) or by the word semantic information (e.g., synonym relations) found in existing human-created resources (e.g., WordNet) (Tak"
J15-1002,D12-1127,0,\N,Missing
J15-1002,kamps-etal-2004-using,0,\N,Missing
J15-2004,W11-0705,0,0.0607269,"structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level ¨ and McDonald (2011a) presented a latent variable model for prediction. T¨ackstrom only using document-level annotations to learn sentence-level sentiment labels, and ¨ and McDonald (2011b) improved it by using a semi-supervised latent variT¨ackstrom able model to utilize manually crafted sentence labels. Agarwal et al. (2011) and Tu et al. (2012) explored part-of-speech tag features and tree-kernel. Wang and Manning (2012) used SVM built over Na¨ıve Bayes log-count ratios as feature values to classify polarity. They showed that SVM was better at full-length reviews, and Multinomial Na¨ıve Bayes was better at short-length reviews. Liu, Agam, and Grossman (2012) proposed a set of heuristic rules based on dependency structure to detect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to t"
J15-2004,Q13-1005,0,0.00541006,"al of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a natural language sentence to its semantic representation without annotated logical form. In this work, we build a sentiment parser. Specifically, we use a modified version of the"
J15-2004,P14-1091,1,0.77232,"h iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a natural language sentence to its semantic representation without annotated logical form. In this work, we build a sentiment parser. Specifically, we use a modified version of the CYK algorithm that parses sentences in a bottom–up fashion. We use the log-linear model to score candidates generated by beam search. Instead of using question-answ"
J15-2004,P05-1022,0,0.023269,"ssification compositions by defining sentiment grammar and borrowing some techniques in the parsing research field. Moreover, our method uses symbolic representations instead of vector spaces. 2.2 Syntactic Parsing and Semantic Parsing The work presented in this article is close to traditional statistical parsing, as we borrow some algorithms to build the sentiment parser. Syntactic parsers are learned from the Treebank corpora, and find the most likely parse tree with the largest probability. In this article, we borrow some well-known techniques from syntactic parsing methods (Charniak 1997; Charniak and Johnson 2005; McDonald, Crammer, and Pereira 2005; ¨ Kubler, McDonald, and Nivre 2009), such as the CYK algorithm and Context-Free Grammar. These techniques are used to build the sentiment grammar and parsing model. They provide a natural way of defining the structure of sentiment trees and parse sentences to trees. The key difference lies in that our task is to calculate the polarity label of a sentence, instead of obtaining the parse tree. We only have sentencepolarity pairs as our training instances instead of annotated tree structures. Moreover, in the decoding process, our goal is to compute correct"
J15-2004,J07-2003,0,0.0181793,"represent the form of an inference rule as: (r) H1 . . . [i, X, j] HK (8) where, if all the terms r and Hk are true, then we can infer [i, X, j] as true. Here, r denotes a sentiment rule, and Hk denotes an item. When we refer to both rules and items, we employ the word terms. Theoretically, we can convert the sentiment rules to CNF versions, and then use the CYK algorithm to conduct parsing. Because the maximum number of non-terminal symbols in a rule is already restricted to two, we formulate the statistical sentiment parsing based on a customized CYK algorithm that is similar to the work of Chiang (2007). Let X, X1 , X2 represent the non-terminals N or P; the inference rules for the statistical sentiment parsing are summarized in Figure 3. 3.3 Ranking Model The parsing model generates many candidate parse trees T(s) for a sentence s. The goal of the ranking model is to score and rank these parse trees. The sentiment tree with the highest score is treated as the best representation for sentence s. We extract a feature vector φ(s, t) ∈ Rd for the specific sentence-tree pair (s, t), where t ∈ T(s) is the parse tree. Let ψ ∈ Rd be the parameter vector for the features. We use the log-linear model"
J15-2004,D08-1083,0,0.669511,"contrast] The negation expressions, intensification modifiers, and the contrastive conjunction can change the polarity (Examples (1), (3), (4), (5)), strength (Examples (2), (3), (5)), or both (Examples (3), (5)) of the sentiment of the sentences. We do not need any detailed explanations here as they can be commonly found and easily understood in people’s 1 http://twitter.com. 2 http://www.imdb.com. 266 Dong et al. A Statistical Parsing Framework for Sentiment Classification daily lives. Existing works to address these issues usually rely on syntactic parsing results either used as features (Choi and Cardie 2008; Moilanen, Pulman, and Zhang 2010) in learning-based methods or hand-crafted rules (Moilanen and Pulman 2007; Jia, Yu, and Meng 2009; Klenner, Petrakis, and Fahrni 2009; Liu and Seneff 2009) in lexiconbased methods. However, even with the difficulty and feasibility of deriving the sentiment structure from syntactic parsing results put aside, it is an even more challenging task to generate stable and reliable parsing results for text that is ungrammatical in nature and has a high ratio of out-of-vocabulary words. The accuracy of the linguistic parsers trained on standard data sets (e.g., the P"
J15-2004,D09-1062,0,0.0171966,"Missing"
J15-2004,P10-2050,0,0.0145972,"tect negations and sentimentbearing expressions. Most of these methods are built on bag-of-words features, and sentiment compositions are handled by manually crafted rules. In contrast to these models, we derive polarity labels from tree structures parsed by the sentiment grammar. There have been several attempts to assume that the problem of sentiment analysis is compositional. Sentiment classification can be solved by deriving the sentiment of a complex constituent (sentence) from the sentiment of small units (words and phrases) (Moilanen and Pulman 2007; Klenner, Petrakis, and Fahrni 2009; Choi and Cardie 2010; Nakagawa, Inui, and Kurohashi 2010). Moilanen and Pulman (2007) proposed using delicate written linguistic patterns as heuristic decision rules when computing the sentiment from individual words to phrases and finally to the sentence. The manually compiled rules were powerful enough to discriminate between the different sentiments in effective remedies (positive) / effective torture (negative), and in too colorful (negative) and too sad (negative). Nakagawa, Inui, and Kurohashi (2010) leveraged a conditional random field model to calculate the sentiment of all the parsed elements in the depe"
J15-2004,W10-2903,0,0.0205801,"parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without a"
J15-2004,W10-3110,0,0.0535975,"Missing"
J15-2004,C10-2028,0,0.0707075,"Missing"
J15-2004,P10-1018,0,0.0204095,"Missing"
J15-2004,J99-4004,0,0.0826633,"In order to tackle the OOV problem, we treat a text span that consists of OOV words as empty text span, and derive them to E. The OOV text spans are combined with other text spans without considering their sentiment information. Finally, each sentence is derived to the symbol S using the start rules that are the beginnings of derivations. We can use the sentiment grammar to compactly describe the derivation process of a sentence. 3.2 Parsing Model We present the formal description of the statistical sentiment parsing model following deductive proof systems (Shieber, Schabes, and Pereira 1995; Goodman 1999) as used in traditional syntactic parsing. For a concrete example, (A → BC) 274 [i, B, k] [i, A, j] [k, C, j] (6) Dong et al. A Statistical Parsing Framework for Sentiment Classification ∗ ∗ j ∗ which represents if we have the rule A → BC and B ⇒ wki and C ⇒ wk (⇒ is used to represent the reflexive and transitive closure of immediate derivation), then we can ∗ j obtain A ⇒ wi . By adding a unary rule j (A → w i ) [i, A, j] (7) with the binary rule in Equation (6), we can express the standard CYK algorithm for CFG in Chomsky Normal Form (CNF). And the goal is [0, S, n], in which S is the start"
J15-2004,P14-1022,0,0.0365465,"Missing"
J15-2004,P97-1023,0,0.045005,"rage and domain adaption problems. Moreover, lexicons are often built and used without considering the context (Wilson, Wiebe, and Hoffmann 2009). Also, hand-crafted rules are often matched heuristically. The sentiment dictionaries used for lexicon-based sentiment analysis can be created manually, or automatically using seed words to expand the list of words. Kamps et al. (2004) and Williams and Anand (2009) used various lexical relations (such as synonym and antonym relations) in WordNet to expend a set of seed words. Some other methods learn lexicons from data directly. Hatzivassiloglou and McKeown (1997) used a log-linear regression model with conjunction constraints to predict whether conjoined adjectives have similar or different polarities. Combining conjunction constraints across many adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polar"
J15-2004,D07-1115,0,0.0357653,"akamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2 -based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas, Zhang, and Levene (2012) used word count as a feature template and trained a classifier using Support Vector Machines with linear kernel. They then regarded the weights as polarity strengths. Krestel and Siersdorfer (2013) generated topicdependent lexicons from review articles by incorporating topic and rating probabilities and defined the polarity stren"
J15-2004,kamps-etal-2004-using,0,0.0896409,"Missing"
J15-2004,W06-1642,0,0.0313257,"adjectives, a clustering algorithm separated the adjectives into groups of different polarity. Finally, adjectives were labeled as positive or negative. Velikovich et al. (2010) constructed a term similarity graph using the cosine similarity of context vectors. They performed graph propagation from seeds on the graph, obtaining polarity words and phrases. Takamura, Inui, and Okumura (2005) regarded the polarity of words as spins of electrons, using the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. Kanayama and Nasukawa (2006) used tendencies for similar polarities to appear successively in contexts. They defined density and precision of coherency to filter neutral phrases and uncertain candidates. Choi and Cardie (2009a) and Lu et al. (2011) transformed the lexicon learning to an optimization problem, and used integer linear programming to solve it. Kaji and Kitsuregawa (2007) defined the χ2 -based polarity value and PMI-based polarity value as a polarity strength to filter neutral phrases. de Marneffe, Manning, and Potts (2010) utilized review data to define polarity strength as the expected rating value. Mudinas"
J15-2004,P06-1115,0,0.0399416,"latent sentiment trees. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) pr"
J15-2004,P03-1054,0,0.0210265,"ey regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most previous methods are either rigid in terms of handcrafted rules, or sensitive to the performance of existing synt"
J15-2004,R09-1034,0,0.0605772,"Missing"
J15-2004,D12-1069,0,0.0126019,"ount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the parameters of structural SVM. Liang, Jordan, and Klein (2013) learned a semantic parser from question-answer pairs, where the logical form was modeled as a latent tree-based semantic representation. Krishnamurthy and Mitchell (2012) presented a method for training a semantic parser using a knowledge base and an unlabeled text corpus, without any individually annotated sentences. Artzi and Zettlemoyer (2013) used various types of weak supervision to learn a grounded Combinatory Categorial Grammar semantic parser, which took context into consideration. Bao et al. (2014) presented a translation-based weakly supervised semantic parsing method to translate questions to answers based on CYK parsing. A log-linear model is defined to score derivations. All these weakly supervised semantic parsing methods learned to transform a n"
J15-2004,J13-2005,0,0.0371933,"Missing"
J15-2004,D09-1017,0,0.0285956,"Missing"
J15-2004,C12-2072,0,0.0397133,"Missing"
J15-2004,P11-1015,0,0.111172,"en widely recognized that sentiment expressions are colloquial and evolve over time very frequently. Taking tweets from Twitter1 and movie reviews on IMDb2 as examples, people use very casual language as well as informal and new vocabulary to comment on general topics and movies. In practice, it is not feasible to create and maintain sentiment lexicons to capture sentiment expressions with high coverage. On the other hand, the learning-based approach relies on large annotated samples to overcome the vocabulary coverage and deals with variations of words in sentences. Human ratings in reviews (Maas et al. 2011) and emoticons in tweets (Davidov, Tsur, and Rappoport 2010; Zhao et al. 2012) are extensively used to collect a large number of training corpora to train the sentiment classifier. However, it is usually not easy to design effective features to build the classifier. Among others, unigrams have been reported as the most effective features (Pang, Lee, and Vaithyanathan 2002) in sentiment classification. Handling complicated expressions delivering people’s opinions is one of the most challenging problems in sentiment analysis. Compositionalities such as negation, intensification, contrast, and th"
J15-2004,J93-2004,0,0.0523085,"Missing"
J15-2004,P05-1012,0,0.100479,"Missing"
J15-2004,P07-1055,0,0.014092,"ated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve better sentiment classification performance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to 269 Computational Linguistics Volume 41, Number 2 extract frequent subsequences and dependency subtrees, and used them as features of SVM. McDonald et al. (2007) investigated a global structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina, Yue, and Cardie (2010) used sentence-level latent variables to improve document-level ¨ and McDonald (2011a) presented a latent variable model for prediction. T¨ackstrom only using document-level annotations to learn sentence-level sentiment labels, and ¨ and McDonald (2011b) improved it by using a semi-supervised latent variT¨ackstrom able model to utilize manually cra"
J15-2004,N10-1120,0,0.417582,"Missing"
J15-2004,P04-1035,0,0.1445,"r of classes in sentiment classification is smaller than that in topic-based text classification (Pang and Lee 2008). Pang, Lee, and Vaithyanathan (2002) investigated three machine learning methods to produce automated classifiers to generate class labels for movie reviews. They tested them on Na¨ıve Bayes, Maximum Entropy, and Support Vector Machine (SVM), and evaluated the contribution of different features including unigrams, bigrams, adjectives, and part-of-speech tags. Their experimental results suggested that a SVM classifier with unigram presence features outperforms other competitors. Pang and Lee (2004) separated subjective portions from the objective by finding minimum cuts in graphs to achieve better sentiment classification performance. Matsumoto, Takamura, and Okumura (2005) used text mining techniques to 269 Computational Linguistics Volume 41, Number 2 extract frequent subsequences and dependency subtrees, and used them as features of SVM. McDonald et al. (2007) investigated a global structured model for jointly classifying polarity at different levels of granularity. This model allowed classification decisions from one level in the text to influence decisions at another. Yessenalina,"
J15-2004,P05-1015,0,0.976359,"over, the Stanford Sentiment Treebank5 contains polarity labels of all syntactically plausible phrases. In addition, we use the MPQA6 data set for the phrase-level task. We describe these data sets as follows. RT-C: 436,000 critic reviews from Rotten Tomatoes. It consists of 218,000 negative and 218,000 positive critic reviews. The average review length is 23.2 words. Critic reviews from Rotten Tomatoes contain a label (Rotten: Negative, Fresh: Positive) to indicate the polarity, which we use directly as the polarity label of corresponding reviews. PL05-C: The sentence polarity data set v1.0 (Pang and Lee 2005) contains 5,331 positive and 5,331 negative snippets written by critics from Rotten Tomatoes. This data set is widely used as the benchmark data set in the sentence-level polarity classification task. The data source is the same as RT-C. SST: The Stanford Sentiment Treebank (Socher et al. 2013) is built upon PL05-C. The sentences are parsed to parse trees. Then, 215,154 syntactically plausible phrases are extracted and annotated by workers from Amazon Mechanical Turk. The experimental settings of positive/negative classification for sentences are the same as in Socher et al. (2013). RT-U: 737,"
J15-2004,W02-1011,0,0.0223195,"Missing"
J15-2004,P06-2034,0,0.0277218,"s. Recently, Hall, Durrett, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration,"
J15-2004,D12-1110,0,0.0833857,"Missing"
J15-2004,D11-1014,0,0.2693,"Missing"
J15-2004,D13-1170,0,0.308991,"egation for sentiment analysis. Some other methods model sentiment compositionality in the vector space. They regard the composition operator as a matrix, and use matrix-vector multiplication to obtain the transformed vector representation. Socher et al. (2012) proposed a recursive neural network model that learned compositional vector representations for phrases and sentences. Their model assigned a vector and a matrix to every node in a parse tree. The vector captured the inherent meaning of the constituent, and the matrix captured how it changes the meaning of neighboring words or phrases. Socher et al. (2013) recently introduced a sentiment treebank based on the results of the Stanford parser (Klein and Manning 2003). The sentiment treebank included polarity labels of phrases that are annotated using Amazon Mechanical Turk. The authors trained recursive neural tensor networks on the sentiment treebank. For a new sentence, the model predicted polarity labels based on the syntactic parse tree, and used tensors to handle compositionality in the vector space. Dong et al. (2014) proposed utilizing multiple composition functions in recursive neural models and learning to select them adaptively. Most pre"
J15-2004,J11-2001,0,0.865765,"28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identification as a special text classification task and focuses on building classifiers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classification approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created manually by experts. I"
J15-2004,P11-2100,0,0.0312767,"Missing"
J15-2004,P05-1017,0,0.0398978,"Missing"
J15-2004,P12-2066,0,0.204703,"Missing"
J15-2004,P02-1053,0,0.0173161,"publication: 28 January 2015. doi:10.1162/COLI a 00221 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 2 1. Introduction Sentiment analysis (Pang and Lee 2008; Liu 2012) has received much attention from both research and industry communities in recent years. Sentiment classification, which identifies sentiment polarity (positive or negative) from text (sentence or document), has been the most extensively studied task in sentiment analysis. Until now, there have been two mainstream approaches for sentiment classification. The lexicon-based approach (Turney 2002; Taboada et al. 2011) aims to aggregate the sentiment polarity of a sentence from the polarity of words or phrases found in the sentence, and the learning-based approach (Pang, Lee, and Vaithyanathan 2002) treats sentiment polarity identification as a special text classification task and focuses on building classifiers from a set of sentences (or documents) annotated with their corresponding sentiment polarity. The lexicon-based sentiment classification approach is simple and interpretable, but suffers from scalability and is inevitably limited by sentiment lexicons that are commonly created"
J15-2004,J09-3003,0,0.0514213,"Missing"
J15-2004,D10-1102,0,0.041237,"Missing"
J15-2004,W03-1017,0,0.152178,"n this article. Regardless of what granularity the task is performed on, existing approaches deriving sentiment polarity from text fall into two major categories, namely, lexicon-based and learning-based approaches. The lexicon-based sentiment analysis uses dictionary matching on a predefined sentiment lexicon to derive sentiment polarity. These methods often use a set of manually defined rules to deal with the negation of polarity. Turney (2002) proposed using the average sentiment orientation of phrases, which contains adjectives or adverbs, in a review to predict its sentiment orientation. Yu and Hatzivassiloglou (2003) calculated a modified log-likelihood ratio for every word by the co-occurrences with positive and negative seed words. To determine the polarity of a sentence, they compare the average log-likelihood value with threshold. Taboada et al. (2011) presented a lexicon-based approach for extracting sentiment from text. They used dictionaries of words with annotated sentiment orientation (polarity and strength) while incorporating intensification and negation. The lexicon-based methods often achieve high precisions and do not need 268 Dong et al. A Statistical Parsing Framework for Sentiment Classif"
J15-2004,D07-1071,0,0.0408997,"t, and Klein (2014) developed a discriminative constituency parser using rich surface features, adapting it to sentiment analysis. Besides extracting unigrams and bigrams as features, they learned interactions between tags and words located at the beginning or the end of spans. However, their method relies on phrase-level polarity annotations. Semantic parsing is another body of work related to this article. A semantic parser is used to parse meaning representations for given sentences. Most existing semantic parsing works (Zelle and Mooney 1996; Kate and Mooney 2006; Raymond and Mooney 2006; Zettlemoyer and Collins 2007, 2009; Li, Liu, and Sun 2013) relied on fine-grained annotations of target logical forms, which required the supervision of experts and are relatively expensive. To balance the performance and the amount of human annotation, some works used only question-answer pairs or even binary correct/incorrect signals as their input. Clarke et al. (2010) used a binary correct/incorrect signal of a database query to map sentences to logical forms. It worked with FunQL language and transformed semantic parsing as an integer linear programming (ILP) problem. In each iteration, it solved ILP and updated the"
J15-2004,P09-1110,0,0.026688,"Missing"
J15-2004,N10-1119,0,\N,Missing
J15-2004,P12-2018,0,\N,Missing
J15-2004,P11-1060,0,\N,Missing
L18-1079,reschke-etal-2014-event,0,0.0969942,"n 2011 Tohoku earthquake and 869 Sanriku earthquake is expressed by the following text in 2011 Tohoku earthquake: infobox generation problem can be divided into three subtasks: event classification, event schema extraction and slot filling. As mentioned before, EventWiki can provide rich information for event classification and event schema extraction. Moreover, it is extremely useful for training a slot filling model for event extraction. Slot and value pairs in the infobox (intra-event information) in EventWiki can be used as weak (distant) supervision for training a slot filling model, as (Reschke et al., 2014) did. For example, for the slot value pair “magnitude: 9.0” in 2011 Tohoku earthquake, we first find out the sentences which “9.0” appears in. The context information of “9.0” can be used as features and “magnitude” is used as the label of “9.0” for training a slot filling model. 2.3.2. Event-event relation extraction and inference As slot filling for event extraction, we can also use interevent information in EventWiki to train an event-event relation extraction model using distant supervision strategy. For an event relation triple, we can find out the sentences that mention both events in th"
P08-2023,H05-1091,0,0.0999416,"ck of necessary co-referenced mentions might be the main reason. 2 Related Work Many approaches have been proposed in the literature of relation extraction. Among them, feature-based and kernel-based approaches are most popular. Kernel-based approaches exploit the structure of the tree that connects two entities. Zelenko et al (2003) proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. The above two work was further advanced by Bunescu and Mooney (2005) who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tr"
P08-2023,P04-1054,0,0.1814,"less effort than applying deep natural language processing. But unfortunately, entity co-reference does not help as much as we have expected. The lack of necessary co-referenced mentions might be the main reason. 2 Related Work Many approaches have been proposed in the literature of relation extraction. Among them, feature-based and kernel-based approaches are most popular. Kernel-based approaches exploit the structure of the tree that connects two entities. Zelenko et al (2003) proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. The above two work was further advanced by Bunescu and Mooney (2005) who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME"
P08-2023,N07-1015,0,0.0958831,"y determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree. Based on his work, Zhou et al (2005) 89 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89–92, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list. Jiang and Zhai (2007) then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence, syntactic parse tree and dependency parse tree. Their experiments showed that using only the basic unit features within each feature subspace can already achieve state-of-art performance, while over-inclusion of complex features might hurt the performance. Previous approaches mainly focused on English relations. Most of them were evaluated on the ACE 2004 data set (or a sub set of it) which defined 7 relation types and 23 subtypes. Although Chinese p"
P08-2023,P05-1053,0,0.169738,"formation to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree. Based on his work, Zhou et al (2005) 89 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89–92, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list. Jiang and Zhai (2007) then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence, syntactic parse tree and dependency parse tree. Their experiments showed that using only the basic unit features within each"
P08-2023,D07-1076,0,0.0581352,"ects two entities. Zelenko et al (2003) proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. The above two work was further advanced by Bunescu and Mooney (2005) who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph. Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree. Based on his work, Zhou et al (2005) 89 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89–92, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics further incorporated the base phrase chunking information and semi-automatically collecte"
P08-2023,I05-2023,0,0.101769,"Missing"
P08-2023,P06-1104,0,\N,Missing
P09-2030,C00-1072,0,0.0223101,"elaborately designed to characterize the different aspects of the sentences. They have been extensively investigated in the past due to their easy implementation and the ability to achieve promising results. The use of featurebased ranking has led to many successful (e.g. top five) systems in DUC 2005-2007 queryfocused summarization (Over et al., 2007). A variety of statistical and linguistic features, such as term distribution, sentence length, sentence position, and named entity, etc., can be found in literature. Among them, query relevance, centroid (Radev et al., 2004) and signature term (Lin and Hovy, 2000) are most remarkable. There are two alternative approaches to integrate the features. One is to combine features into a unified representation first, and then use it to rank the sentences. The other is to utilize rank fusion or rank aggregation techniques to combine the ranking results (orders, ranks or scores) produced by the multiple ranking functions into a unified rank. The most popular implementation of the latter approaches is to linearly combine the features to obtain an overall score which is then used as the ranking criterion. The weights of the features are either experimentally tune"
P09-2030,N03-1020,0,0.315524,"Missing"
P11-1037,J92-4003,0,0.149212,"d on instance re-weighting, which allows the small amount of the bootstrapped training sets to have an equal weight to the large source domain training set. Wu et al. (2009) propose another bootstrapping algorithm that selects bridging instances from an unlabeled target domain, which are informative about the target domain and are also easy to be correctly labeled. We adopt bootstrapping as well, but use human labeled tweets as seeds. Another representative of semi-supervised learning is learning a robust representation of the input from unlabeled data. Miller et al. (2004) use word clusters (Brown et al., 1992) learned from unlabeled text, resulting in a performance improvement of NER. Guo et al. (2009) introduce Latent Semantic Association (LSA) for NER. In our pilot study of NER for tweets, we adopt bag-of-words models to represent a word in tweet, to concentrate our efforts on combining global evidence with local information and semi-supervised learning. We leave it to our future work to explore which is the best input representation for our task. 3 Task Definition We first introduce some background about tweets, then give a formal definition of the task. 3.1 The Tweets A tweet is a short text me"
P11-1037,D10-1098,0,0.0778136,"Missing"
P11-1037,W02-1001,0,0.23149,"Missing"
P11-1037,W10-0713,0,0.546486,"elines and that both the combination with KNN and the semi-supervised learning strategy are effective. The rest of our paper is organized as follows. In the next section, we introduce related work. In Section 3, we formally define the task and present the challenges. In Section 4, we detail our method. In Section 5, we evaluate our method. Finally, Section 6 concludes our work. 2 Related Work Related work can be roughly divided into three categories: NER on tweets, NER on non-tweets (e.g., news, bio-logical medicine, and clinical notes), and semi-supervised learning for NER. 2.1 NER on Tweets Finin et al. (2010) use Amazons Mechanical Turk service 2 and CrowdFlower 3 to annotate named entities in tweets and train a CRF model to evaluate the effectiveness of human labeling. In contrast, our work aims to build a system that can automatically identify named entities in tweets. To achieve this, a KNN classifier with a CRF model is combined to leverage cross tweets information, and the semisupervised learning is adopted to leverage unlabeled tweets. 2.2 NER on Non-Tweets NER has been extensively studied on formal text, such as news, and various approaches have been proposed. For example, Krupka and Hausma"
P11-1037,D09-1015,0,0.00692716,"Missing"
P11-1037,P05-1045,0,0.0670203,"argmaxc (w ′ ,c′ )∈nb δ(c, c ) · cos(w,  w  ). 3: Calculate the labeling confidence cf : cf ∑ ′ (w  ,c ∑ ′ ′ ′ δ(c,c )·cos(w,  w  ) )∈nb ′ ′ (w  ,c )∈nb cos(w,  w ′) = = = . 4: return The predicted label c∗ and its confidence cf . model, which is good at encoding the subtle interactions between words and their labels, compensates for KNN’s incapability to capture fine-grained evidence involving multiple decision points. The Linear CRF model is used as the fine model, with the following considerations: 1) It is wellstudied and has been successfully used in state-ofthe-art NER systems (Finkel et al., 2005; Wang, 2009); 2) it can output the probability of a label sequence, which can be used as the labeling confidence that is necessary for the semi-supervised learning framework. In our experiments, the CRF++ 5 toolkit is used to train a linear CRF model. We have written a Viterbi decoder that can incorporate partially observed labels to implement the crf function in Algorithm 1. Algorithm 2 KNN Training. Require: Training tweets ts. 1: Initialize the classifier lk :lk = ∅. 2: for Each tweet t ∈ ts do 3: for Each word,label pair (w, c) ∈ t do 4: Get the feature vector w:  w  5: 6: 7: 8: = reprw"
P11-1037,N09-1032,0,0.0124944,"ave an equal weight to the large source domain training set. Wu et al. (2009) propose another bootstrapping algorithm that selects bridging instances from an unlabeled target domain, which are informative about the target domain and are also easy to be correctly labeled. We adopt bootstrapping as well, but use human labeled tweets as seeds. Another representative of semi-supervised learning is learning a robust representation of the input from unlabeled data. Miller et al. (2004) use word clusters (Brown et al., 1992) learned from unlabeled text, resulting in a performance improvement of NER. Guo et al. (2009) introduce Latent Semantic Association (LSA) for NER. In our pilot study of NER for tweets, we adopt bag-of-words models to represent a word in tweet, to concentrate our efforts on combining global evidence with local information and semi-supervised learning. We leave it to our future work to explore which is the best input representation for our task. 3 Task Definition We first introduce some background about tweets, then give a formal definition of the task. 3.1 The Tweets A tweet is a short text message containing no more than 140 characters in Twitter, the biggest micro-blog service. Here"
P11-1037,W02-1041,0,0.128934,"Missing"
P11-1037,P07-1034,0,0.0228947,"Inside and Outside of a chunk). In contrast to the above work, our study focuses on NER for tweets, a new genre of texts, which are short, noise prone and ungrammatical. 2.3 Semi-supervised Learning for NER Semi-supervised learning exploits both labeled and un-labeled data. It proves useful when labeled data is scarce and hard to construct while unlabeled data is abundant and easy to access. Bootstrapping is a typical semi-supervised learning method. It iteratively adds data that has been 361 confidently labeled but is also informative to its training set, which is used to re-train its model. Jiang and Zhai (2007) propose a balanced bootstrapping algorithm and successfully apply it to NER. Their method is based on instance re-weighting, which allows the small amount of the bootstrapped training sets to have an equal weight to the large source domain training set. Wu et al. (2009) propose another bootstrapping algorithm that selects bridging instances from an unlabeled target domain, which are informative about the target domain and are also easy to be correctly labeled. We adopt bootstrapping as well, but use human labeled tweets as seeds. Another representative of semi-supervised learning is learning"
P11-1037,P03-1054,0,0.00537236,"Missing"
P11-1037,P06-1141,0,0.201162,"Missing"
P11-1037,M98-1015,0,0.0194458,"nin et al. (2010) use Amazons Mechanical Turk service 2 and CrowdFlower 3 to annotate named entities in tweets and train a CRF model to evaluate the effectiveness of human labeling. In contrast, our work aims to build a system that can automatically identify named entities in tweets. To achieve this, a KNN classifier with a CRF model is combined to leverage cross tweets information, and the semisupervised learning is adopted to leverage unlabeled tweets. 2.2 NER on Non-Tweets NER has been extensively studied on formal text, such as news, and various approaches have been proposed. For example, Krupka and Hausman (1998) use manual rules to extract entities of predefined types; Zhou and Ju (2002) adopt Hidden Markov Models (HMM) while Finkel et al. (2005) use CRF to train a sequential NE labeler, in which the BIO (meaning Beginning, the Inside and the Outside of 2 1 3 http://sourceforge.net/projects/opennlp/ 360 https://www.mturk.com/mturk/ http://crowdflower.com/ an entity, respectively) schema is applied. Other methods, such as classification based on Maximum Entropy models and sequential application of Perceptron or Winnow (Collins, 2002), are also practiced. The state-of-the-art system, e.g., the Stanford"
P11-1037,H05-1056,0,0.123381,"Missing"
P11-1037,W09-1119,0,0.899478,"ike an <PRODUCT >iphone</PRODUCT>without apps, <PERSON>Justin Bieber</PERSON>without his hair,<PERSON>Lady gaga</PERSON> without her telephone, it just wouldn...”, meaning that “iphone” is a product, while “Justin Bieber” and “Lady gaga” are persons. 4 Our Method NER task can be naturally divided into two subtasks, i.e., boundary detection and type classification. Following the common practice , we adopt a sequential labeling approach to jointly resolve these sub-tasks, i.e., for each word in the input tweet, a label is assigned to it, indicating both the boundary and entity type. Inspired by Ratinov and Roth (2009), we use the BILOU schema. Algorithm 1 outlines our method, where: trains and traink denote two machine learning processes to get the CRF labeler and the KNN classifier, respectively; reprw converts a word in a tweet into a bag-of-words vector; the reprt function transforms a tweet into a feature matrix that is later fed into the CRF model; the knn function predicts the class of a word; the update function applies the predicted class by KNN to the inputted tweet; the crf function conducts word level NE labeling;τ and γ represent the minimum labeling confidence of KNN and CRF, respectively, whi"
P11-1037,N10-1009,0,0.0111834,"Missing"
P11-1037,P08-1076,0,0.0926995,"Missing"
P11-1037,P09-3003,0,0.0814909,"b δ(c, c ) · cos(w,  w  ). 3: Calculate the labeling confidence cf : cf ∑ ′ (w  ,c ∑ ′ ′ ′ δ(c,c )·cos(w,  w  ) )∈nb ′ ′ (w  ,c )∈nb cos(w,  w ′) = = = . 4: return The predicted label c∗ and its confidence cf . model, which is good at encoding the subtle interactions between words and their labels, compensates for KNN’s incapability to capture fine-grained evidence involving multiple decision points. The Linear CRF model is used as the fine model, with the following considerations: 1) It is wellstudied and has been successfully used in state-ofthe-art NER systems (Finkel et al., 2005; Wang, 2009); 2) it can output the probability of a label sequence, which can be used as the labeling confidence that is necessary for the semi-supervised learning framework. In our experiments, the CRF++ 5 toolkit is used to train a linear CRF model. We have written a Viterbi decoder that can incorporate partially observed labels to implement the crf function in Algorithm 1. Algorithm 2 KNN Training. Require: Training tweets ts. 1: Initialize the classifier lk :lk = ∅. 2: for Each tweet t ∈ ts do 3: for Each word,label pair (w, c) ∈ t do 4: Get the feature vector w:  w  5: 6: 7: 8: = reprw (w, t). Add"
P11-1037,D09-1158,0,0.00983723,"t proves useful when labeled data is scarce and hard to construct while unlabeled data is abundant and easy to access. Bootstrapping is a typical semi-supervised learning method. It iteratively adds data that has been 361 confidently labeled but is also informative to its training set, which is used to re-train its model. Jiang and Zhai (2007) propose a balanced bootstrapping algorithm and successfully apply it to NER. Their method is based on instance re-weighting, which allows the small amount of the bootstrapped training sets to have an equal weight to the large source domain training set. Wu et al. (2009) propose another bootstrapping algorithm that selects bridging instances from an unlabeled target domain, which are informative about the target domain and are also easy to be correctly labeled. We adopt bootstrapping as well, but use human labeled tweets as seeds. Another representative of semi-supervised learning is learning a robust representation of the input from unlabeled data. Miller et al. (2004) use word clusters (Brown et al., 1992) learned from unlabeled text, resulting in a performance improvement of NER. Guo et al. (2009) introduce Latent Semantic Association (LSA) for NER. In our"
P11-1037,W07-1033,0,0.00577536,"Missing"
P11-1037,W03-0434,0,0.0545152,"inear CRF model. We have written a Viterbi decoder that can incorporate partially observed labels to implement the crf function in Algorithm 1. Algorithm 2 KNN Training. Require: Training tweets ts. 1: Initialize the classifier lk :lk = ∅. 2: for Each tweet t ∈ ts do 3: for Each word,label pair (w, c) ∈ t do 4: Get the feature vector w:  w  5: 6: 7: 8: = reprw (w, t). Add the w  and c pair to the classifier: lk = lk ∪ {(w,  c)}. end for end for return KNN classifier lk . 4.3 Features Given a word in a tweet, the KNN classifier considers a text window of size 5 with the word in the middle (Zhang and Johnson, 2003), and extracts bag-ofword features from the window as features. For each word, our CRF model extracts similar features as Wang (2009) and Ratinov and Roth (2009), namely, orthographic features, lexical features and gazetteers related features. In our work, we use the gazetteers provided by Ratinov and Roth (2009). Two points are worth noting here. One is that before feature extraction for either the KNN or the CRF, stop words are removed. The stop words used here are mainly from a set of frequently-used words 6 . The other is that tweet meta data is normalized, that is, every link becomes *LIN"
P11-1037,P02-1060,0,0.065803,"Missing"
P11-1037,N04-1043,0,\N,Missing
P11-1037,W03-0430,0,\N,Missing
P11-1037,W03-0419,0,\N,Missing
P11-1037,M98-1004,0,\N,Missing
P12-1055,D10-1098,0,0.0503916,"Missing"
P12-1055,W05-1303,0,0.0825989,"ons 2 . As an illustrative example, we show “Anneke Gronloh”, which may occur as “Mw.,Gronloh”, “Anneke Kronloh” or “Mevrouw G”. We thus propose NEN for tweets, which plays an important role in entity retrieval, trend detection, and event and entity tracking. For example, Khalid et al. (2008) show that even a simple normalization method leads to improvements of early precision, for both document and passage retrieval, and better normalization results in better retrieval performance. Traditionally, NEN is regarded as a septated task, which takes the output of NER as its input (Li et al., 2002; Cohen, 2005; Jijkoun et al., 2008; Dai et al., 2011). One limitation of this cascaded approach is that errors propagate from NER to NEN and there is no feedback from NEN to NER. As demonstrated by Khalid et al. (2008), most NEN errors are caused 2 This data set consists of 12,245 randomly sampled tweets within five days. 526 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 526–535, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics by recognition errors. Another challenge of NEN is the dearth of information in a single t"
P12-1055,D07-1074,0,0.0414343,"y kappa is 0.72. Any inconsistent case is discussed by the two annotators till a consensus is reached. 2, 245 tweets are used for development, and the remainder are used for 5-fold cross validation. 5.2 Evaluation Metrics We adopt the widely-used Precision, Recall and F1 to measure the performance of NER for a particular type of entity, and the average Precision, Recall and F1 to measure the overall performance of NER (Liu et al., 2011; Ritter et al., 2011). As for NEN, we adopt the widely-used Accuracy, i.e., to what percentage the outputted canonical forms are correct (Jijkoun et al., 2008; Cucerzan, 2007; Li et al., 2002). 12 Two native English speakers. 532 5.3 Baseline Tables 1- 2 show the overall performance of the baseline and ours (denoted by SRN ). It can be seen that, our method yields a significantly higher F1 (with p < 0.01) than SBR , and a moderate improvement of accuracy as compared with SBN (with p < 0.05). As a case study, we show that our system successfully identified “jaxon11 ” as a PERSON in the tweet “· · · come to see jaxon11 someday· · · ”, which is mistakenly labeled as a LOCATION by SBR . This is largely owing to the fact that our system aligns “jaxon11 ” with “Jaxson12"
P12-1055,I11-1095,0,0.0222717,"we show “Anneke Gronloh”, which may occur as “Mw.,Gronloh”, “Anneke Kronloh” or “Mevrouw G”. We thus propose NEN for tweets, which plays an important role in entity retrieval, trend detection, and event and entity tracking. For example, Khalid et al. (2008) show that even a simple normalization method leads to improvements of early precision, for both document and passage retrieval, and better normalization results in better retrieval performance. Traditionally, NEN is regarded as a septated task, which takes the output of NER as its input (Li et al., 2002; Cohen, 2005; Jijkoun et al., 2008; Dai et al., 2011). One limitation of this cascaded approach is that errors propagate from NER to NEN and there is no feedback from NEN to NER. As demonstrated by Khalid et al. (2008), most NEN errors are caused 2 This data set consists of 12,245 randomly sampled tweets within five days. 526 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 526–535, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics by recognition errors. Another challenge of NEN is the dearth of information in a single tweet, due to the short and noise-prone na"
P12-1055,W10-0713,0,0.104261,"Missing"
P12-1055,D09-1015,0,0.00911895,"otated data set, and show that our method compares favorably with the baseline, achieving better performance in both tasks. Our paper is organized as follows. In the next section, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding sc"
P12-1055,W06-1643,0,0.0711415,"Missing"
P12-1055,P11-1038,0,0.0565987,"Missing"
P12-1055,W02-1041,0,0.0298758,"baseline, achieving better performance in both tasks. Our paper is organized as follows. In the next section, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly outperforms the BIO schema (Beginning, the Inside a"
P12-1055,M98-1015,0,0.0119543,"m each other. 2. We evaluate our method on a human annotated data set, and show that our method compares favorably with the baseline, achieving better performance in both tasks. Our paper is organized as follows. In the next section, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findin"
P12-1055,C02-1127,0,0.0487485,"ge of 3.3 variations 2 . As an illustrative example, we show “Anneke Gronloh”, which may occur as “Mw.,Gronloh”, “Anneke Kronloh” or “Mevrouw G”. We thus propose NEN for tweets, which plays an important role in entity retrieval, trend detection, and event and entity tracking. For example, Khalid et al. (2008) show that even a simple normalization method leads to improvements of early precision, for both document and passage retrieval, and better normalization results in better retrieval performance. Traditionally, NEN is regarded as a septated task, which takes the output of NER as its input (Li et al., 2002; Cohen, 2005; Jijkoun et al., 2008; Dai et al., 2011). One limitation of this cascaded approach is that errors propagate from NER to NEN and there is no feedback from NEN to NER. As demonstrated by Khalid et al. (2008), most NEN errors are caused 2 This data set consists of 12,245 randomly sampled tweets within five days. 526 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 526–535, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics by recognition errors. Another challenge of NEN is the dearth of information"
P12-1055,P11-1037,1,0.398518,"ss than 140 characters shared through the Twitter service 1 , have become an important source of fresh information. As a result, the task of named entity recognition (NER) for tweets, which aims to identify mentions of rigid designators from tweets belonging to named-entity types such as persons, organizations and locations (2007), has attracted increasing research interest. For example, Ritter et al. (2011) develop a system that exploits a CRF model to segment named 1 http://www.twitter.com entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities. Liu et al. (2011) combine a classifier based on the k-nearest neighbors algorithm with a CRFbased model to leverage cross tweets information, and adopt the semi-supervised learning to leverage unlabeled tweets. However, named entity normalization (NEN) for tweets, which transforms named entities mentioned in tweets to their unambiguous canonical forms, has not been well studied. Owing to the informal nature of tweets, there are rich variations of named entities in them. According to our investigation on the data set provided by Liu et al. (2011), every named entity in tweets has an average of 3.3 variations 2"
P12-1055,W07-0804,0,0.0222233,"earest neighbors algorithm with a CRF-based model to leverage cross tweets information, and adopt the semi-supervised learning to leverage unlabeled tweets. Our method leverages redundance in similar tweets, using a factor graph rather than a two-stage labeling strategy. One advantage of our method is that local and global information can interact with each other. 2.2 NEN There is a large body of studies into normalizing various types of entities for formally written texts. For instance, Cohen (2005) normalizes gene/protein names using dictionaries automatically extracted from gene databases; Magdy et al. (2007) address cross-document Arabic name normalization using a machine learning approach, a dictionary of person names and frequency information for names in a collection; Cucerzan (2007) demostrates a largescale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results; Dai et al. (2011) employ a Markov logic network to model interweaved con3 4 https://www.mturk.com/mturk/ http://crowdflower.com/ straints in a setting of gene mention normalization. Jijkoun et al. (2008) study NEN for UGC. They"
P12-1055,W03-0430,0,0.0793044,"in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly outperforms the BIO schema (Beginning, the Inside and Outside of a chunk). A handful of work on other genres of texts exists. For example, Yoshida and Tsujii build a biomedical NER system (2007) using lexical features, orthographic features, semantic features and syntactic features, such as part-of-speech (POS) and shallow par"
P12-1055,H05-1056,0,0.0223589,"next section, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly outperforms the BIO schema (Beginning, the Inside and Outside of a chunk). A handful of work on other genres of texts exists. For example, Yosh"
P12-1055,W09-1119,0,0.712807,"ER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly outperforms the BIO schema (Beginning, the Inside and Outside of a chunk). A handful of work on other genres of texts exists. For example, Yoshida and Tsujii build a biomedical NER system (2007) using lexical features, orthographic features, semantic features and syntactic features, such as part-of-speech (POS) and shallow parsing; Downey et al. (2007) employ capitalization cues and n-gram statistics to lo"
P12-1055,D11-1141,0,0.172463,"Yn / m ∩ j+1 ∩ j−1 i+1 i−1 Ym Yn / Ym Yn is empty; whether the dominating label/entity type in Ymi is the same as that in Ynj . We develop a cascaded system as the baseline, which conducts NER and NEN sequentially. Its NER module, denoted by SBR , is based on the stateof-the-art method introduced by Liu et al. (2011); and its NEN model , denoted by SBN , follows the NEN system for user-generated news comments proposed by Jijkoun et al. (2008), which uses handcrafted rules to improve a typical NEN system that normalizes surface forms to Wikipedia page titles. We use the POS tagger developed by Ritter et al. (2011) to extract POS related features, and the OpenNLP toolkit to get lemma related features. 5 5.4 Results Experiments We manually annotate a data set to evaluate our method. We show that our method outperforms the baseline, a cascaded system that conducts NER and NEN individually. 5.1 Data Preparation We use the data set provided by Liu et al. (2011), which consists of 12,245 tweets with four types of entities annotated: PERSON, LOCATION, ORGANIZATION and PRODUCT. We enrich this data set by adding entity normalization information. Two annotators 12 are involved. For any entity mention, two annota"
P12-1055,N10-1009,0,0.0195022,"that our method compares favorably with the baseline, achieving better performance in both tasks. Our paper is organized as follows. In the next section, we introduce related work. In Section 3 and 4, we formally define the task and present our method. In Section 5, we evaluate our method. And finally we conclude our work in Section 6. 2 Related Work Related work can be divided into two categories: NER and NEN. 2.1 NER NER has been well studied and its solutions can be divided into three categories: 1) Rule-based (Krupka and Hausman, 1998); 2) machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). Owing to the availability of annotated corpora, such as ACE05, Enron (Minkov et al., 2005) and CoNLL03 (Tjong Kim Sang and De Meulder, 2003), data driven methods are now dominant. Current studies of NER mainly focus on formal text such as news articles (Mccallum and Li, 2003; Etzioni et al., 2005). A representative work is that of Ratinov and Roth (2009), in which they systematically study the challenges of NER, compare several solutions, and show some interesting findings. For example, they show that the BILOU encoding scheme significantly ou"
P12-1055,P09-3003,0,0.00719362,"same entity; 2) set zmn to 1, if the similarity between tm and tn is above a threshold (0.8 in our work), or tm and tn share one hash tag; and 3)zmn ij = −1, if the similarity between tm and tn is below a threshold (0.3 in work). To compute 4.3 Features (1) 1 A feature in {ϕk }K k=1 involves a pair of neighbori−1 and y i , while a feaing NE-type labels, i.e., ym m (2) K2 ture in {ϕk }k=1 concerns a pair of distant NE-type labels and its associated normalization label, i.e., i ,y j and z ij . Details are given below. ym n mn (1) 1 4.3.1 Feature Set One: {ϕk }K k=1 We adopts features similar to Wang (2009), and Ratinov and Roth (2009), i.e., orthographic features, lexical features and gazetteer-related features. These features are defined on the observation. Combining i−1 and y i constitutes {ϕ(1) }K1 . them with ym m k k=1 Orthographic features: Whether tim is capitalized or upper case; whether it is alphanumeric or contains any slashes; wether it is a stop word; word prefixes and suffixes. Lexical features: Lemma of tim , ti−1 and ti+1 m m , i respectively; whether tm is an out-of-vocabulary i−1 and ti+1 , respec(OOV) word 11 ; POS of tim , tm m i tively; whether tm is a hash tag, a link, or"
P12-1055,W07-1033,0,0.0618195,"Missing"
P12-1055,W03-0419,0,\N,Missing
P12-1055,M98-1004,0,\N,Missing
P12-1060,C10-2028,0,0.00911262,"studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creatin"
P12-1060,P11-2075,0,0.118846,"t approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et al., 2011). Although the machine-translation-based methods are intuitive, they have certain limitations. First, the vocabulary covered by the translated labeled data is limited, hence many sentiment indicative words can not be learned from the translated labeled data. Duh et al. (2011) report low overlapping between vocabulary of natural English documents and the vocabulary of documents translated to English from Japanese, and the experiments of Duh 1 http://research.nii.ac.jp/ntcir/index-en.html 572 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572–581, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics et al. (2011) show that vocabulary coverage has a strong correlation with sentiment classification accuracy. Second, machine translation may change the sentiment polarity of the original"
P12-1060,C04-1121,0,0.3955,"sifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creating or using sentiment lexicons. Turney (2002) derives sentiment scores for phrases by measuring the mutual information between the given phrase and the words “excellent” and “poor”, and then uses the average scores of the phrases in a document as the sentiment of the document. Corpus-based methods are often built upon machine learning models. Pang et al. (2002) compare the performance of three commonly used machine learning models (Naive Bayes, Maximum Entropy and SVM). Gamon (2004) shows that introducing deeper linguistic features into SVM can help to improve the performance. The interested readers are referred to (Pang and Lee, 2008) for a comprehensive review of sentiment classification. 2.2 Cross-Lingual Sentiment Classification Cross-lingual sentiment classification, which aims to conduct sentiment classification in the target language (e.g. Chinese) with labeled data in the source language (e.g. English), has been extensively studied in the very recent years. The basic idea is to explore the abundant labeled sentiment data in source language to alleviate the shorta"
P12-1060,P09-1028,0,0.0398786,"then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al. (2011) focus on the task of jointly improving the performance of sentiment classification on two languages (e"
P12-1060,N06-1014,0,0.01986,"third term on the right hand side (L(θ|Dt )) is optional. 2 For simplicity, we assume the prior distribution P (C) is uniform and drop it from the formulas. 3.3 Parameter Estimation Instead of estimating word projection probability (P (ws |wt ) and P (wt |ws )) and conditional probability of word to class (P (wt |c) and P (ws |c)) simultaneously in the training procedure, we estimate them separately since the word projection probability stays invariant when estimating other parameters. We estimate word projection probability using word alignment probability generated by the Berkeley aligner (Liang et al., 2006). The word alignment probabilities serves two purposes. First, they connect the corresponding words between the source language and the target language. Second, they adjust the strength of influences between the corresponding words. Figure 2 gives an example of word alignment probability. As is shown, the three words “tour de force” altogether express a positive meaning, while in Chinese the same meaning is expressed with only one word “杰作” (masterpiece). CLMM use word alignment probability to decrease the influences from “杰作” (masterpiece) to “tour”, “de” and “force” individually, using the w"
P12-1060,P11-1033,0,0.531802,". (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual setting. Instead of using machine translation engines to translate labeled text, the authors use it to construct the word translation oracle for pivot words translation. Lu et al. (2011) focus on the task of jointly improving the performance of sentiment classification on two languages (e.g. English and Chinese) . the authors use an unlabeled parallel corpus instead of machine translation engines. They assume parallel sentences in the corpus should have the same sentiment polarity. Besides, they assume labeled data in both language are available. They propose a method of training two classifiers based on maximum entropy formulation to maximize their prediction agreement on the parallel corpus. However, this method requires labeled data in both the source language and the targ"
P12-1060,J05-4003,0,0.0182661,"Missing"
P12-1060,W02-1011,0,0.0302846,"d data in the target language are also available. 1 Introduction Sentiment Analysis (also known as opinion mining), which aims to extract the sentiment information from text, has attracted extensive attention in recent years. Sentiment classification, the task of determining the sentiment orientation (positive, negative or neutral) of text, has been the most extensively studied task in sentiment analysis. There is ∗ Contribution during internship at Microsoft Research Asia. already a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is des"
P12-1060,J11-2001,0,0.018128,"w of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment orientation detection as a conventional classification task and focuses on building classifier from a set of sentences (or documents) labeled with sentiment orientations. Dictionary-based methods involve in creating or using sentiment lexicons. Turney (2002) derives sentiment scores for phrases by measuring the mutual information between the given phrase and the"
P12-1060,P02-1053,0,0.015491,"abeled data in the target language. The paper is organized as follows. We review related work in Section 2, and present the cross-lingual mixture model in Section 3. Then we present the ex573 perimental studies in Section 4, and finally conclude the paper and outline the future plan in Section 5. 2 Related Work In this section, we present a brief review of the related work on monolingual sentiment classification and cross-lingual sentiment classification. 2.1 Sentiment Classification Early work of sentiment classification focuses on English product reviews or movie reviews (Pang et al., 2002; Turney, 2002; Hu and Liu, 2004). Since then, sentiment classification has been investigated in various domains and different languages (Zagibalov and Carroll, 2008; Seki et al., 2007; Seki et al., 2008; Davidov et al., 2010). There exist two main approaches to extracting sentiment orientation automatically. The Dictionary-based approach (Turney, 2002; Taboada et al., 2011) aims to aggregate the sentiment orientation of a sentence (or document) from the sentiment orientations of words or phrases found in the sentence (or document), while the corpus-based approach (Pang et al., 2002) treats the sentiment or"
P12-1060,D08-1058,0,0.154858,"directly adapt labeled data from the source language to target language. Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. English Labeled data are first translated to Chinese, and then two SVM classifiers are trained on English and Chinese labeled data respectively. After that, co-training (Blum and Mitchell, 1998) approach is adopted to leverage Chinese unlabeled data and their English translation to improve the SVM classifier for Chinese sentiment classification. The same idea is used in (Wan, 2008), but the ensemble techniques used are various voting methods and the individual classifiers used are dictionary-based classifiers. Instead of ensemble methods, Pan et al. (2011) use matrix factorization formulation. They extend Nonnegative Matrix Tri-Factorization model (Li et al., 2009) to bilingual view setting. Their bilingual view is also constructed by using machine translation engines to translate original documents. Prettenhofer and Stein (2011) use machine translation engines in a different way. They generalize Structural Correspondence Learning (Blitzer et al., 2006) to multilingual"
P12-1060,P09-1027,0,0.805218,"(Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other languages. One direct approach to leveraging the labeled data in English is to use machine translation engines as a black box to translate the labeled data from English to the target language (e.g. Chinese), and then using the translated training data directly for the development of the sentiment classifier in the target language (Wan, 2009; Pan et al., 2011). Although the machine-translation-based methods are intuitive, they have certain limitations. First, the vocabulary covered by the translated labeled data is limited, hence many sentiment indicative words can not be learned from the translated labeled data. Duh et al. (2011) report low overlapping between vocabulary of natural English documents and the vocabulary of documents translated to English from Japanese, and the experiments of Duh 1 http://research.nii.ac.jp/ntcir/index-en.html 572 Proceedings of the 50th Annual Meeting of the Association for Computational Linguisti"
P12-1060,C08-1135,0,0.125358,"lso known as opinion mining), which aims to extract the sentiment information from text, has attracted extensive attention in recent years. Sentiment classification, the task of determining the sentiment orientation (positive, negative or neutral) of text, has been the most extensively studied task in sentiment analysis. There is ∗ Contribution during internship at Microsoft Research Asia. already a large amount of work on sentiment classification of text in various genres and in many languages. For example, Pang et al. (2002) focus on sentiment classification of movie reviews in English, and Zagibalov and Carroll (2008) study the problem of classifying product reviews in Chinese. During the past few years, NTCIR1 organized several pilot tasks for sentiment classification of news articles written in English, Chinese and Japanese (Seki et al., 2007; Seki et al., 2008). For English sentiment classification, there are several labeled corpora available (Hu and Liu, 2004; Pang et al., 2002; Wiebe et al., 2005). However, labeled resources in other languages are often insufficient or even unavailable. Therefore, it is desirable to use the English labeled data to improve sentiment classification of documents in other"
P12-1060,W06-1615,0,\N,Missing
P12-3003,D09-1015,0,0.0176951,"R. NER is the task of identifying mentions of rigid designators from text belonging to namedentity types such as persons, organizations and locations. Existing solutions fall into three categories: 1) 6 http://lucene.apache.org/java/docs/index.html Intelr Xeonr 2.33 CPU 5140 @2.33GHz, 4G of RAM, OS of Windows Server 2003 Enterprise X64 version 7 Table 1: Current deployment of QuickView. Workstation #1 #2, 3 #4 #5 Hosted components Crawler,Raw tweet buffer Process pipeline Indexing Buffer, Indexer/Querier Web application the rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron and CoNLL03, the data-driven methods become the dominating methods. However, because of domain mismatch, current systems trained on non-tweets perform poorly on tweets. Our NER system takes three steps to address this problem. Firstly, it defines those recently labeled tweets that are similar to the current tweet as its recognition context, under which a KNNbased classifier is used to conduct word level classification. Following the two-stage prediction aggre"
P12-3003,P05-1045,0,0.0131915,"Missing"
P12-3003,W02-1041,0,0.0318638,"ext belonging to namedentity types such as persons, organizations and locations. Existing solutions fall into three categories: 1) 6 http://lucene.apache.org/java/docs/index.html Intelr Xeonr 2.33 CPU 5140 @2.33GHz, 4G of RAM, OS of Windows Server 2003 Enterprise X64 version 7 Table 1: Current deployment of QuickView. Workstation #1 #2, 3 #4 #5 Hosted components Crawler,Raw tweet buffer Process pipeline Indexing Buffer, Indexer/Querier Web application the rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron and CoNLL03, the data-driven methods become the dominating methods. However, because of domain mismatch, current systems trained on non-tweets perform poorly on tweets. Our NER system takes three steps to address this problem. Firstly, it defines those recently labeled tweets that are similar to the current tweet as its recognition context, under which a KNNbased classifier is used to conduct word level classification. Following the two-stage prediction aggregation methods (Krishnan and Manning, 2006), such pre-labeled results,"
P12-3003,P11-1016,1,0.74586,"alization model identifies and corrects ill-formed words. For example, after normalization, “loooove” in “· · · I loooove my icon· · · ” will be transformed to “love”. A phrase-based translation system without re-ordering is used to implement this model. The translation table includes manually compiled ill/good form pairs, and the language model is a trigram trained on LDC data 4 using SRILM (Stolcke, 2002). The OpenNLP 5 toolkit is directly used to implement the parsing model. In future, the parsing model will be re-trained using annotated tweets. The SA component is implemented according to Jiang et al. (2011), which incorporates target-dependent features and considers related tweets by utilizing a graph-based optimization. The classification model is a KNN-based classifier that caches confidently labeled results to re-train itself, which also recognizes and drops noisy tweets. 4 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?catalogId=LDC2005T12 5 http://sourceforge.net/projects/opennlp/ 16 Each processed tweet, if not identified as noise, is put into a shared buffer for indexing. The third part is responsible for indexing and querying. It constantly takes from the indexing buffer a processed"
P12-3003,W05-0625,0,0.0116131,"poral expressions, etc. These gazetteers represent general knowledge across domains, and help to make up for the lack of training data. SRL. Given a sentence, the SRL component identifies every predicate, and for each predicate further identifies its arguments. This task has been extensively studied on well-written corpora like news, and a couple of solutions exist. Examples include: 1) the pipelined approach, i.e., dividing the task into several successive components such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue, 2004; Koomen et al., 2005); 2) sequentially labeling 17 based approach (M`arquez et al., 2005), i.e., labeling the words according to their positions relative to an argument (i.e., inside, outside, or at the beginning); and 3) Markov Logic Networks (MLN) based approach (Meza-Ruiz and Riedel, 2009), i.e., simultaneously resolving all the sub-tasks using learnt weighted formulas. Unsurprisingly, the performance of the state-of-the-art SRL system (MezaRuiz and Riedel, 2009) drops sharply when applied to tweets. The SRL component of QuickView is based on CRF, and uses the recently labeled tweets that are similar to the cur"
P12-3003,P06-1141,0,0.0293496,"2010); and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron and CoNLL03, the data-driven methods become the dominating methods. However, because of domain mismatch, current systems trained on non-tweets perform poorly on tweets. Our NER system takes three steps to address this problem. Firstly, it defines those recently labeled tweets that are similar to the current tweet as its recognition context, under which a KNNbased classifier is used to conduct word level classification. Following the two-stage prediction aggregation methods (Krishnan and Manning, 2006), such pre-labeled results, together with other conventional features used by the state-of-the-art NER systems, are fed into a linear CRF models, which conducts fine-grained tweet level NER. Secondly, the KNN and CRF model are repeatedly retrained with an incrementally augmented training set, into which highly confidently labeled tweets are added. Finally, following Lev Ratinov and Dan Roth (2009), 30 gazetteers are used, which cover common names, countries, locations, temporal expressions, etc. These gazetteers represent general knowledge across domains, and help to make up for the lack of tr"
P12-3003,M98-1015,0,0.0808344,"discuss two core components of QuickView: NER and SRL. NER. NER is the task of identifying mentions of rigid designators from text belonging to namedentity types such as persons, organizations and locations. Existing solutions fall into three categories: 1) 6 http://lucene.apache.org/java/docs/index.html Intelr Xeonr 2.33 CPU 5140 @2.33GHz, 4G of RAM, OS of Windows Server 2003 Enterprise X64 version 7 Table 1: Current deployment of QuickView. Workstation #1 #2, 3 #4 #5 Hosted components Crawler,Raw tweet buffer Process pipeline Indexing Buffer, Indexer/Querier Web application the rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron and CoNLL03, the data-driven methods become the dominating methods. However, because of domain mismatch, current systems trained on non-tweets perform poorly on tweets. Our NER system takes three steps to address this problem. Firstly, it defines those recently labeled tweets that are similar to the current tweet as its recognition context, under which a KNNbased classifier is used to conduct word level"
P12-3003,W05-0628,0,0.0454103,"Missing"
P12-3003,N09-1018,0,0.0129393,"s task has been extensively studied on well-written corpora like news, and a couple of solutions exist. Examples include: 1) the pipelined approach, i.e., dividing the task into several successive components such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue, 2004; Koomen et al., 2005); 2) sequentially labeling 17 based approach (M`arquez et al., 2005), i.e., labeling the words according to their positions relative to an argument (i.e., inside, outside, or at the beginning); and 3) Markov Logic Networks (MLN) based approach (Meza-Ruiz and Riedel, 2009), i.e., simultaneously resolving all the sub-tasks using learnt weighted formulas. Unsurprisingly, the performance of the state-of-the-art SRL system (MezaRuiz and Riedel, 2009) drops sharply when applied to tweets. The SRL component of QuickView is based on CRF, and uses the recently labeled tweets that are similar to the current tweet as the broader context. Algorithm 1 outlines its implementation, where: train denotes a machine learning process to get a labeler l, which in our work is a linear CRF model; the cluster function puts the new tweet into a cluster; the label function generates pr"
P12-3003,W09-1119,0,0.0692759,"Missing"
P12-3003,H05-1088,0,0.0416791,"Missing"
P12-3003,N10-1009,0,0.0313196,"tifying mentions of rigid designators from text belonging to namedentity types such as persons, organizations and locations. Existing solutions fall into three categories: 1) 6 http://lucene.apache.org/java/docs/index.html Intelr Xeonr 2.33 CPU 5140 @2.33GHz, 4G of RAM, OS of Windows Server 2003 Enterprise X64 version 7 Table 1: Current deployment of QuickView. Workstation #1 #2, 3 #4 #5 Hosted components Crawler,Raw tweet buffer Process pipeline Indexing Buffer, Indexer/Querier Web application the rule-based (Krupka and Hausman, 1998); 2) the machine learning based (Finkel and Manning, 2009; Singh et al., 2010); and 3) hybrid methods (Jansche and Abney, 2002). With the availability of annotated corpora, such as ACE05, Enron and CoNLL03, the data-driven methods become the dominating methods. However, because of domain mismatch, current systems trained on non-tweets perform poorly on tweets. Our NER system takes three steps to address this problem. Firstly, it defines those recently labeled tweets that are similar to the current tweet as its recognition context, under which a KNNbased classifier is used to conduct word level classification. Following the two-stage prediction aggregation methods (Krish"
P12-3003,W04-3212,0,0.0482433,"ations, temporal expressions, etc. These gazetteers represent general knowledge across domains, and help to make up for the lack of training data. SRL. Given a sentence, the SRL component identifies every predicate, and for each predicate further identifies its arguments. This task has been extensively studied on well-written corpora like news, and a couple of solutions exist. Examples include: 1) the pipelined approach, i.e., dividing the task into several successive components such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue, 2004; Koomen et al., 2005); 2) sequentially labeling 17 based approach (M`arquez et al., 2005), i.e., labeling the words according to their positions relative to an argument (i.e., inside, outside, or at the beginning); and 3) Markov Logic Networks (MLN) based approach (Meza-Ruiz and Riedel, 2009), i.e., simultaneously resolving all the sub-tasks using learnt weighted formulas. Unsurprisingly, the performance of the state-of-the-art SRL system (MezaRuiz and Riedel, 2009) drops sharply when applied to tweets. The SRL component of QuickView is based on CRF, and uses the recently labeled tweets that"
P12-3003,C10-1079,1,\N,Missing
P12-3003,M98-1004,0,\N,Missing
P13-1128,P12-1055,1,0.377711,", and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method. 1 Introduction Twitter is a widely used social networking service. With millions of active users and hundreds of millions of new published tweets every day1 , it has become a popular platform to capture and transmit the human experiences of the moment. Many tweet related researches are inspired, from named entity recognition (Liu et al., 2012), topic detection (Mathioudakis and Koudas, 2010), clustering (Rosa et al., 2010), to event extraction (Grinev et al., 2009). In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i.e., an entry ID of a knowledge base like Wikipedia. Entity 1 http://siteanalytics.compete.com/twitter.com/ linking task is generally considered as a bridge between unstructured text and structured machinereadable knowledge base, and represents a critical role in machine reading program (Singh et al., 2011). Entity linking for tweets is particularly"
P13-1128,P11-1080,0,0.0359615,"are inspired, from named entity recognition (Liu et al., 2012), topic detection (Mathioudakis and Koudas, 2010), clustering (Rosa et al., 2010), to event extraction (Grinev et al., 2009). In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i.e., an entry ID of a knowledge base like Wikipedia. Entity 1 http://siteanalytics.compete.com/twitter.com/ linking task is generally considered as a bridge between unstructured text and structured machinereadable knowledge base, and represents a critical role in machine reading program (Singh et al., 2011). Entity linking for tweets is particularly meaningful, considering that tweets are often hard to read owing to its informal written style and length limitation of 140 characters. Current entity linking methods are built on top of a large scale knowledge base such as Wikipedia. A knowledge base consists of a set of entities, and each entity can have a variation list2 . To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea an"
P13-1128,P10-1006,0,0.0140507,"inking methods are built on top of a large scale knowledge base such as Wikipedia. A knowledge base consists of a set of entities, and each entity can have a variation list2 . To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea and Csomai, 2007; Han and Zhao, 2009); 2) the coherence among the mapped entities for a set of related mentions, e.g, multiple mentions in a document (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Zhao, 2010; Han et al., 2011). Tweets pose special challenges to entity linking. First, a tweet is often too concise and too noisy to provide enough information for similarity computing, owing to its short and grass root nature. Second, tweets have rich variations of named entities3 , and many of them fall out of the scope of the existing dictionaries mined from Wikipedia (called OOV mentions hereafter). On 2 Entity variation lists can be extracted from the entity resolution pages of Wikipedia. For example, the link “http://en.wikipedia.org/wiki/Svm” will lead us to a resolution page, where “Svm” are li"
P14-1146,C10-2005,0,0.00563043,"aches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focu"
P14-1146,P07-1056,0,0.114896,"plement this system because the codes are not publicly available 3 . NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model. The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases. Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007). Results and Analysis. Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets. Distant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier. The results of bagof-ngram (uni/bi/tri-gram) features are not satisfied because the one-hot word representation cannot capture the latent connections between words. NBSVM and RAE perform comparably and have 3 For 3-class sentiment classification in SemEval 2013, our re-implementation"
P14-1146,C10-2028,0,0.00814251,"t classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter senti"
P14-1146,P11-2008,0,0.0123158,"Missing"
P14-1146,P13-1088,0,0.0113325,"Missing"
P14-1146,P11-1016,1,0.370167,"their loss functions. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment cl"
P14-1146,P13-2087,0,0.792293,"utoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and"
P14-1146,P11-1015,0,0.646787,"(2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from sc"
P14-1146,D12-1110,0,0.368522,"Missing"
P14-1146,P13-1045,0,0.149339,"p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad"
P14-1146,S13-2053,0,0.231338,"on has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases"
P14-1146,S13-2052,0,0.0193943,"to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the w"
P14-1146,pak-paroubek-2010-twitter,0,0.047259,"low traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based me"
P14-1146,W02-1011,0,0.132502,"dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the lea"
P14-1146,D11-1014,0,0.926164,"Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch. 3 Sentiment-Specific Word Embedding for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification. We propose incorporat"
P14-1146,D13-1170,0,0.34155,"p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad"
P14-1146,J11-2001,0,0.172252,"ion, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy"
P14-1146,P02-1053,0,0.0615851,"asks. 2 Related Work In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011;"
P14-1146,P12-2018,0,0.321669,"ve/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013. Baseline Methods. We compare our method with the following sentiment classification algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available"
P14-1146,H05-1044,0,0.100893,"he sentiment lexicon, P#Lex PN j=1 β(wi , cij ) i=1 Accuracy = (10) #Lex × N where #Lex is the number of words in the sentiment lexicon, wi is the i-th word in the lexicon, cij is the j-th closest word to wi in the lexicon with cosine similarity, β(wi , cij ) is an indicator function that is equal to 1 if wi and cij have the same sentiment polarity and 0 for the opposite case. The higher accuracy refers to a better polarity consistency of words in the sentiment lexicon. We set N as 100 in our experiment. Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding. For each lexicon, we remove the words that do not appear in the lookup table of word embedding. We only use unigram embedding in this section because these sentiment lexicons do not contain phrases. The distribution of the lexicons used in this paper is listed in Table 4. Lexicon HL MPQA Joint Positive 1,331 1,932 1,051 Negative 2,647 2,817 2,024 Total 3,978 4,749 3,075 Table 4: Statistics of the sentiment lexicons. Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity. Results. Table 5"
P14-1146,D11-1016,0,0.0249497,"the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors."
P14-1146,D13-1061,0,0.152584,"representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors. It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity. In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis. We encode the sentiment information in1555 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics to the continuous representation of words, so that it is able to separate good and bad to opposite ends"
P14-1146,P10-1040,0,\N,Missing
P14-2009,W02-1011,0,0.0546421,"Missing"
P14-2009,D12-1110,0,0.0247666,"Missing"
P14-2009,P11-2008,0,0.0271014,"Missing"
P14-2009,D13-1170,0,0.118265,"Missing"
P14-2009,J11-2001,0,0.042909,"respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗ Contribution during internship at Microsoft Research. 49 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 3 determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena (Taboada et al., 2011) in sentiment analysis. In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it. The experimental results suggest that our approach yields better performances than the baseline methods. 2 We use the dependency parsing results to find the words syntactically connected with the interested target. Adaptive Recursive Neural Network is proposed to propagate the sentiments of words to the target node. We model the adaptive sentiment propagations as semantic compositions. The computation process is conducted in a bottom-up manner, and the vector representations"
P14-2009,P11-1016,1,0.747726,"is to classify their sentiments for a given target as positive, negative, and neutral. People may mention several entities (or targets) in one tweet, which affects the availabilities for most of existing methods. For example, the tweet “@ballmer: windows phone is better than ios!” has three targets (@ballmer, windows phone, and ios). The user expresses neutral, positive, and negative sentiments for them, respectively. If target information is ignored, it is difficult to obtain the correct sentiment for a specified target. For target-dependent sentiment classification, the manual evaluation of Jiang et al. (2011) ∗ Contribution during internship at Microsoft Research. 49 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 49–54, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 3 determines how to propagate the sentiments towards the target and handles the negation or intensification phenomena (Taboada et al., 2011) in sentiment analysis. In addition, we introduce a manually annotated dataset, and conduct extensive experiments on it. The experimental results suggest that our approach yields better perfo"
P14-2009,P03-1054,0,0.0549664,"Missing"
P14-2009,N10-1120,0,0.0373418,"Missing"
P15-1026,P13-1042,0,0.20894,"itive interpretations for MCCNNs by developing a method to detect salient question words in the different column networks. 2 Related Work The state-of-the-art methods for question answering over a knowledge base can be classified into two classes, i.e., semantic parsing based and information retrieval based. Semantic parsing based approaches aim at learning semantic parsers which parse natural language questions into logical forms and then query knowledge base to lookup answers. The most important step is mapping questions into predefined logical forms, such as combinatory categorial grammar (Cai and Yates, 2013) and dependencybased compositional semantics (Liang et al., Another line of related work is applying deep learning techniques for the question answering task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity"
P15-1026,D11-1142,0,0.0529251,"task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on R E V ERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in F REEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 cess method does not ease the task because W E B Q UESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on W IKI A NSWERS and used them as question paraphrases. There are 350,000 paraphrase clusters which contain about two million questions. They are used"
P15-1026,P14-1091,1,0.92368,"crosoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. Then, a score layer is employed to rank candi"
P15-1026,P13-1158,0,0.250666,"The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on R E V ERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in F REEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 cess method does not ease the task because W E B Q UESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on W IKI A NSWERS and used them as question paraphrases. There are 350,000 paraphrase clusters which contain about two million questions. They are used to generalize for unseen words and question patterns. 4 Methods The overview of our framework is shown in Figure 1. For instance, for the question when did Avatar release in UK, the related nodes of the entity Avatar are queried from F REEBASE. These related nodes are regarded as candidate answers (Cq ). Then, for every candidate answer a, the model predicts a score S (q, a) to determine whether it is a correct"
P15-1026,P14-1133,0,0.653202,"during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. Then, a score layer is empl"
P15-1026,D13-1160,0,0.755738,"r, how ∗ Contribution during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. The"
P15-1026,D14-1070,0,0.0457346,"Missing"
P15-1026,D12-1069,0,0.0673897,"Missing"
P15-1026,D14-1067,0,0.859341,"Dong†∗ Furu Wei‡ Ming Zhou‡ Ke Xu† † SKLSDE Lab, Beihang University, Beijing, China ‡ Microsoft Research, Beijing, China donglixp@gmail.com {fuwei,mingzhou}@microsoft.com kexu@nlsde.buaa.edu.cn Abstract to understand questions and bridge the gap between natural languages and structured semantics of knowledge bases is still very challenging. Up to now, there are two mainstream methods for this task. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability. I"
P15-1026,D10-1119,0,0.0063562,"rgescale knowledge bases, such as F REEBASE (Bollacker et al., 2008), provides a rich resource to answer open-domain questions. However, how ∗ Contribution during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities"
P15-1026,D13-1161,0,0.0182072,"n-domain questions. However, how ∗ Contribution during internship at Microsoft Research. 260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dim"
P15-1026,C02-1150,0,0.255307,"2, we compute salience scores for several questions, and normalize them by the max values in different columns. We clearly see that these words play different roles in a question. The overall conclusion is that the wh- words (such as what, who and where) tend to be important for question understanding. Moreover, nouns dependent of the wh- words and verbs are important clues to obtain question representations. For instance, the figure demonstrates that the nouns type/country/leader and the verbs speak/located are salient in the columns of networks. These observations agree with previous works (Li and Roth, 2002). Some manually defined rules (Yao and Van Durme, 2014) used in the question answering task are also based on them. Salient Words Detection In order to analyze the model, we detect salient words in questions. The salience score of a question word depends on how much the word affects the computation of question representation. In other words, if a word plays more important role in the model, its salience score should be larger. We compute several salience scores for a same word to illustrate its importance in different columns of networks. For the i-th column, the salience score of word wj in t"
P15-1026,P11-1060,0,0.168473,"sk. The first one is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014) and the other relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b). The semantic parsers learn to understand natural language questions by converting them into logical forms. Then, the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of them still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability capability. In addition, they need to manually design features for semantic parsers. The second approach uses information extraction techniques for open question answering. These methods retrieve a set of candidate answers from the knowledge base, and the extract features for the question and these candidates to rank them. However, the method proposed by Yao and Van Durme (2014) relies on rules and dependenc"
P15-1026,W12-3016,0,0.00897462,"ll the answers can be found in F REEBASE. Freebase It is a large-scale knowledge base that consists of general facts (Bollacker et al., 2008). These facts are organized as subject-propertyobject triples. For example, the fact Avatar is directed by James Cameron is represented by (/m/0bth54, film.film.directed by, /m/03 gd) in RDF format. The preprocess method presented in (Bordes et al., 2014a) was used to make F REE BASE fit in memory. Specifically, we kept the triples where one of the entities appeared in the training/development set of W EB Q UESTIONS or C LUE W EB extractions provided in (Lin et al., 2012), and removed the entities appearing less than five times. Then, we obtained 18M triples that contained 2.9M entities and 7k relation types. As described in (Bordes et al., 2014a), this preproS (q, a) = f1 (q)T g1 (a) + f2 (q)T g2 (a) + f3 (q)T g3 (a) | {z } | {z } | {z } answer path answer context answer type (1) where fi (q) and gi (a) have the same dimension. As shown in Figure 1, the score layer computes scores and adds them together. 4.1 Candidate Generation The first step is to retrieve candidate answers from F REEBASE for a question. Questions should contain an identified entity that ca"
P15-1026,Q14-1030,0,0.218962,"260 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 260–269, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2011). Some semantic parsing based systems required manually annotated logical forms to train the parsers (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These annotations are relatively expensive. So recent works (Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014; Bao et al., 2014; Reddy et al., 2014) mainly aimed at using weak supervision (question-answer pairs) to effectively train semantic parsers. These methods achieved comparable results without using logical forms annotated by experts. However, some methods relied on lexical triggers or manually defined features. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base (namely F REE BASE in our experiments) are also represented as low-dimensional vectors. Then, a score layer is employed to rank candidate answers accordin"
P15-1026,P10-1040,0,0.0222071,"q2 , q3 ) (8) P q1 ,q2 ∈P q3 ∈RP where RP contains kp questions which are randomly sampled from other clusters. The same optimization algorithm described in Section 4.4 is used to update parameters. 5 F1 31.4 39.9 37.5 33.0 39.2 29.7 40.8 Experiments In order to evaluate the model, we use the dataset W EB Q UESTIONS (Section 3) to conduct experiments. Settings The development set is used to select hyper-parameters in the experiments. The nonlinearity function f = tanh is employed. The dimension of word vectors is set to 25. They are initialized by the pre-trained word embeddings provided in (Turian et al., 2010). The window size of MCCNNs is 5. The dimension of the pooling layers and the dimension of answer embeddings are set to 64. The parameters are initialized by the techniques described in (Bengio, 2012). The max value used for max-norm regularization is 3. The initial learning rate used in AdaGrad is set to 0.01. A mini-batch consists of 10 question-answer pairs, and every question-answer pair has k negative samples that are randomly sampled from its candidate set. The margin values in Equation (4) and Equation (7) is set to m = 0.5 and mp = 0.1. 5.2 Model Analysis We also conduct ablation exper"
P15-1026,P14-1090,0,0.797611,"Missing"
P15-1026,P14-2105,0,0.262001,"ng et al., Another line of related work is applying deep learning techniques for the question answering task. Grefenstette et al. (2014) proposed a deep architecture to learn a semantic parser from annotated logic forms of questions. Iyyer et al. (2014) introduced dependency-tree recursive neural networks for the quiz bowl game which asked players to answer an entity for a given paragraph. Yu et 261 al. (2014) proposed a bigram model based on convolutional neural networks to select answer sentences from text data. The model learned a similarity function between questions and answer sentences. Yih et al. (2014) used convolutional neural networks to answer single-relation questions on R E V ERB (Fader et al., 2011). However, the system worked on relation-entity triples instead of more structured knowledge bases. For instance, the question shown in Figure 1 is answered by using several triples in F REEBASE. Also, we can utilize richer information (such as entity types) in structured knowledge bases. 3 cess method does not ease the task because W E B Q UESTIONS only contains about 2k entities. WikiAnswers Fader et al. (2013) extracted the similar questions on W IKI A NSWERS and used them as question pa"
P15-2047,P03-1054,0,0.0500541,"output. Parameters are learned using the back-propagation method (Rumelhart et al., 1988). 4 Experiments We compare DepNN against multiple baselines on SemEval-2010 dataset (Hendrickx et al., 2010). The training set includes 8000 sentences, and the test set includes 2717 sentences. There are 9 287 Model relation types, and each type has two directions. Instances which don’t fall in any of these classes are labeled as Other. The official evaluation metric is the macro-averaged F1-score (excluding Other) and the direction is considered. We use dependency trees generated by the Stanford Parser (Klein and Manning, 2003) with the collapsed option. 4.1 SVM MV-RNN CNN FCM DT-RNN DepNN Contributions of different components baseline (Path words) +Depedency relations +Attached subtrees +Lexical features 50-d 73.8 80.3 81.2 82.7 F1 200-d 75.5 81.8 82.8 83.6 We start with a baseline model using a CNN with only the words on the shortest path. We then add dependency relations and attached subtrees. The results indicate that both parts are effective for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the"
P15-2047,N07-2032,0,0.0416795,"Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2 for detailed examples). However, how to uniformly and efficiently combine these two components is still an open problem. In this paper, we propose a novel structure named Augmented Dependency Path"
P15-2047,S10-1057,0,0.346496,"ve for relation classification. The rich linguistic information embedded in the dependency relations and subtrees can on one hand, help distinguish different functions of the same word, and on the other hand infer an unseen word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indi"
P15-2047,D12-1110,0,0.0907578,"word’s role in the sentence. Finally, the lexical features are added and DepNN achieves state-of-the-art results. Comparison with Baselines In this subsection, we compare DepNN with several baseline relation classification approaches. Here, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings."
P15-2047,Q14-1017,0,0.023951,"n learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combination but not a subtree embedding. We adapt the augmented dependency path into a dependency subtree and apply DT-RNN. As shown in Table 2, DepNN achieves the best result (83.6) using NER features. WordNet features can also improve the performance of DepNN, but not as obvious as NER. Yu et al. (2014) had similar observations, since the larger number of WordNet tags may cause overfitting. SVM achieves a comparable result, though the quality of feature engineering highly rel"
P15-2047,H05-1091,0,0.250704,"n Li1,2 Heng Ji4 Ming Zhou3 Houfeng Wang1,2 1 Key Laboratory of Computational Linguistics, Peking University, MOE, China 2 Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China 3 Microsoft Research, Beijing, China 4 Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA {cs-ly, lisujian, wanghf}@pku.edu.cn {furu, mingzhou}@microsoft.com jih@rpi.edu Abstract in different ways. Culotta and Sorensen (2004) designed a dependency tree kernel and attached more information including Part-of-Speech tag, chunking tag of each node in the tree. Interestingly, Bunescu and Mooney (2005) provided an important insight that the shortest path between two entities in a dependency graph concentrates most of the information for identifying the relation between them. Nguyen et al. (2007) developed these ideas by analyzing multiple subtrees with the guidance of pre-extracted keywords. Previous work showed that the most useful dependency information in relation classification includes the shortest dependency path and dependency subtrees. These two kinds of information serve different functions and their collaboration can boost the performance of relation classification (see Section 2"
P15-2047,I08-2119,0,0.124479,"Missing"
P15-2047,C14-1220,0,0.431582,"re, DepNN and the baselines are all based on the 200-d embeddings trained on Gigaword due to the larger corpus and higher dimensions. SVM (Rink and Harabagiu, 2010): This is the top performed system in SemEval-2010. It utilizes many external corpora to extract features from the sentence to build an SVM classifier. 1 82.2 81.82 82.7 83.0 73.1 83.0 83.6 MV-RNN (Socher et al., 2012): This model finds the path between the two entities in the constituent parse tree and then learns the distributed representation of its highest node with a matrix for each word to make the compositions specific. CNN: Zeng et al. (2014) build a convolutional model over the tokens of a sentence to learn the sentence level feature vector. It uses a special position vector that indicates the relative distances of current input word to two marked entities. FCM (Yu et al., 2014): FCM decomposes the sentence into substructures and extracts features for each of them, forming substructure embeddings. These embeddings are combined by sumpooling and input into a sof tmax classifier. DT-RNN (Socher et al., 2014) : This is an RNN for modeling dependency trees. It combines node’s word embedding with its children through a linear combinat"
P15-2047,P05-1053,0,\N,Missing
P15-2047,W08-1301,0,\N,Missing
P15-2047,P04-1054,0,\N,Missing
P15-2047,D14-1070,0,\N,Missing
P15-2047,P06-1104,0,\N,Missing
P15-2136,D14-1181,0,0.00276888,"Missing"
P15-2136,W04-1013,0,0.0525643,"ver, he reserves all the representations generated by filters to a fully connected output layer. This practice greatly enlarges following parameters and ignores the relation among phrases with different lengths. Hence we use the two-stage max-over-time pooling to associate all these filters. Besides the features xp obtained through the CNNs, we also extract several documentdependent features notated as xe , shown in Table 1. In the end, xp is combined with xe to conduct sentence ranking. Here we follow the regression framework of Li et al. (2007). The sentence saliency y is scored by ROUGE-2 (Lin, 2004) (stopwords removed) and the model tries to estimate this saliency. φ = [xp , xe ] (3) wrT (4) yˆ = ×φ AVG-CF Description The position of the sentence. The averaged term frequency values of words in the sentence. The averaged cluster frequency values of words in the sentence. 3.2 Comparison with Baseline Methods To evaluate the summarization performance of PriorSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in Table 2) participating DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowl"
P15-2136,W02-0401,0,0.431087,"res beyond word level (e.g., phrases) are seldom involved in current research. The CTSUM system developed by Wan and Zhang (2014) is the most relevant to ours. It attempted to explore a context-free measure named certainty which is critical to ranking sentences in summarization. To calculate the certainty score, four dictionaries are manually built as features and a corpus is annotated to train the feature weights using Support Vector Regression (SVR). HowIntroduction Sentence ranking, the vital part of extractive summarization, has been extensively investigated. Regardless of ranking models (Osborne, 2002; Galley, 2006; Conroy et al., 2004; Li et al., 2007), feature engineering largely determines the final summarization performance. Features often fall into two types: document-dependent features (e.g., term frequency or position) and documentindependent features (e.g., stopword ratio or word polarity). The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in. Take the following two sentences as an example: 1. Hurricane Emily slammed into Dominica on September"
P15-2136,P14-2105,0,0.013617,"The underlined phrases greatly reduce the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summar"
P15-2136,C14-1220,0,0.00457144,"the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summarization datasets. The experimental resu"
P15-2136,W06-1643,0,\N,Missing
P15-2136,E14-1075,0,\N,Missing
P17-1018,P16-1086,0,0.0145181,"t that passage parts are of different importance to the particular question for reading comprehension and question answering. are real anonymized queries issued through Bing or Cortana and the passages are related web pages. For each question in the dataset, several related passages are provided. However, the answers are human generated, which is different from SQuAD where answers must be a span of the passage. End-to-end Neural Networks for Reading Comprehension Along with cloze-style datasets, several powerful deep learning models (Hermann et al., 2015; Hill et al., 2016; Chen et al., 2016; Kadlec et al., 2016; Sordoni et al., 2016; Cui et al., 2016; Trischler et al., 2016; Dhingra et al., 2016; Shen et al., 2016) have been introduced to solve this problem. Hermann et al. (2015) first introduce attention mechanism into reading comprehension. Hill et al. (2016) propose a windowbased memory network for CBT dataset. Kadlec et al. (2016) introduce pointer networks with one attention step to predict the blanking out entities. Sordoni et al. (2016) propose an iterative alternating attention mechanism to better model the links between question and passage. Trischler et al. (2016) solve cloze-style questio"
P17-1018,D14-1159,0,0.00946933,"odel for encoding evidence from the passage, we draw the align194 Result Analysis (a) (b) (c) (d) Figure 3: Model performance on different question types (a), different answer lengths (b), different passage lengths (c), different question lengths (d). The point on the x-axis of figure (c) and (d) represent the datas whose passages length or questions length are between the value of current point and last point. ing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality (Richardson et al., 2013; Berant et al., 2014; Yang et al., 2015), but are too small for training modern data-intensive models. Those that are automatically generated from natural occurring data can be very large (Hill et al., 2016; Hermann et al., 2015), which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover, Chen et al. (2016) have shown that the CNN / Daily News dataset (Hermann et al., 2015) requires less reasoning than previously thought, and conclude that performance is almost saturated. model and its ablati"
P17-1018,D15-1161,0,0.0171807,"yen et al., 2016) is also a large-scale dataset. The questions in the dataset Related Work Reading Comprehension and Question Answering Dataset Benchmark datasets play an important role in recent progress in reading comprehension and question answering research. Exist195 self-matching attention in our model. It dynamically refines the passage representation by looking over the whole passage and aggregating evidence relevant to the current passage word and question, allowing our model make full use of passage information. Weightedly attending to word context has been proposed in several works. Ling et al. (2015) propose considering window-based contextual words differently depending on the word and its relative position. Cheng et al. (2016) propose a novel LSTM network to encode words in a sentence which considers the relation between the current token being processed and its past tokens in the memory. Parikh et al. (2016) apply this method to encode words in a sentence according to word form and its distance. Since passage information relevant to question is more helpful to infer the answer in reading comprehension, we apply self-matching based on question-aware representation and gated attention-ba"
P17-1018,P16-1223,0,0.37859,"t. ing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality (Richardson et al., 2013; Berant et al., 2014; Yang et al., 2015), but are too small for training modern data-intensive models. Those that are automatically generated from natural occurring data can be very large (Hill et al., 2016; Hermann et al., 2015), which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover, Chen et al. (2016) have shown that the CNN / Daily News dataset (Hermann et al., 2015) requires less reasoning than previously thought, and conclude that performance is almost saturated. model and its ablation models. As we can see, both four models show the same trend. The questions are split into different groups based on a set of question words we have defined, including “what”, “how”, “who”, “when”, “which”, “where”, and “why”. As we can see, our model is better at “when” and “who” questions, but poorly on “why” questions. This is mainly because the answers to why questions can be very diverse, and they are"
P17-1018,D16-1053,0,0.675629,"ive Innovation Center for Language Ability, Xuzhou, 221009, China {wangwenhui,chbb}@pku.edu.cn {nanya,fuwei,mingzhou}@microsoft.com Abstract 2015; Hill et al., 2016) in which answers are single words or entities. Moreover, SQuAD requires different forms of logical reasoning to infer the answer (Rajpurkar et al., 2016). Rapid progress has been made since the release of the SQuAD dataset. Wang and Jiang (2016b) build question-aware passage representation with match-LSTM (Wang and Jiang, 2016a), and predict answer boundaries in the passage with pointer networks (Vinyals et al., 2015). Seo et al. (2016) introduce bi-directional attention flow networks to model question-passage pairs at multiple levels of granularity. Xiong et al. (2016) propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions. Lee et al. (2016) and Yu et al. (2016) predict answers by ranking continuous text spans within passages. Inspired by Wang and Jiang (2016b), we introduce a gated self-matching network, illustrated in Figure 1, an end-to-end neural network model for reading comprehension and question answering. Our model consists of four parts:"
P17-1018,P14-5010,0,0.00543631,"Missing"
P17-1018,D14-1179,0,0.0126271,"Missing"
P17-1018,D16-1244,0,0.102298,"Missing"
P17-1018,N16-1170,0,0.257789,"d Question Answering Wenhui Wang†§∗ Nan Yang‡§ Furu Wei‡ Baobao Chang† Ming Zhou‡ † Key Laboratory of Computational Linguistics, Peking University, MOE, China ‡ Microsoft Research, Beijing, China  Collaborative Innovation Center for Language Ability, Xuzhou, 221009, China {wangwenhui,chbb}@pku.edu.cn {nanya,fuwei,mingzhou}@microsoft.com Abstract 2015; Hill et al., 2016) in which answers are single words or entities. Moreover, SQuAD requires different forms of logical reasoning to infer the answer (Rajpurkar et al., 2016). Rapid progress has been made since the release of the SQuAD dataset. Wang and Jiang (2016b) build question-aware passage representation with match-LSTM (Wang and Jiang, 2016a), and predict answer boundaries in the passage with pointer networks (Vinyals et al., 2015). Seo et al. (2016) introduce bi-directional attention flow networks to model question-passage pairs at multiple levels of granularity. Xiong et al. (2016) propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions. Lee et al. (2016) and Yu et al. (2016) predict answers by ranking continuous text spans within passages. Inspired by Wang and Jiang ("
P17-1018,D14-1162,0,0.122829,"Missing"
P17-1018,D16-1264,0,0.76541,"tation is that it has very limited knowledge of context. One answer candidate is often oblivious to important (5) 191 When predicting the start position, hat−1 represents the initial hidden state of the answer recurrent network. We utilize the question vector rQ as the initial state of the answer recurrent network. rQ = att(uQ , VrQ ) is an attention-pooling vector of the question based on the parameter VrQ : cues in the passage outside its surrounding window. Moreover, there exists some sort of lexical or syntactic divergence between the question and passage in the majority of SQuAD dataset (Rajpurkar et al., 2016). Passage context is necessary to infer the answer. To address this problem, we propose directly matching the question-aware passage representation against itself. It dynamically collects evidence from the whole passage for words in passage and encodes the evidence relevant to the current passage word and its matching question information into the passage representation hPt : hPt = BiRNN(hPt−1 , [vtP , ct ]) Q Q sj = vT tanh(WuQ uQ j + Wv V r ) ai = exp(si )/Σm j=1 exp(sj ) Q rQ = Σm i=1 ai ui To train the network, we minimize the sum of the negative log probabilities of the ground truth start"
P17-1018,D13-1020,0,0.104396,"show the ability of the model for encoding evidence from the passage, we draw the align194 Result Analysis (a) (b) (c) (d) Figure 3: Model performance on different question types (a), different answer lengths (b), different passage lengths (c), different question lengths (d). The point on the x-axis of figure (c) and (d) represent the datas whose passages length or questions length are between the value of current point and last point. ing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality (Richardson et al., 2013; Berant et al., 2014; Yang et al., 2015), but are too small for training modern data-intensive models. Those that are automatically generated from natural occurring data can be very large (Hill et al., 2016; Hermann et al., 2015), which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover, Chen et al. (2016) have shown that the CNN / Daily News dataset (Hermann et al., 2015) requires less reasoning than previously thought, and conclude that performance is almost saturated."
P17-1018,D15-1237,0,0.122175,"Missing"
P17-1018,D16-1013,0,0.0821462,"cular question for reading comprehension and question answering. are real anonymized queries issued through Bing or Cortana and the passages are related web pages. For each question in the dataset, several related passages are provided. However, the answers are human generated, which is different from SQuAD where answers must be a span of the passage. End-to-end Neural Networks for Reading Comprehension Along with cloze-style datasets, several powerful deep learning models (Hermann et al., 2015; Hill et al., 2016; Chen et al., 2016; Kadlec et al., 2016; Sordoni et al., 2016; Cui et al., 2016; Trischler et al., 2016; Dhingra et al., 2016; Shen et al., 2016) have been introduced to solve this problem. Hermann et al. (2015) first introduce attention mechanism into reading comprehension. Hill et al. (2016) propose a windowbased memory network for CBT dataset. Kadlec et al. (2016) introduce pointer networks with one attention step to predict the blanking out entities. Sordoni et al. (2016) propose an iterative alternating attention mechanism to better model the links between question and passage. Trischler et al. (2016) solve cloze-style question answering task by combining an attentive model with a rerankin"
P17-1101,P00-1041,0,0.847188,"ling the salient information from the highlight to generate a fluent sentence. We model the distilling process with selective encoding. Introduction Sentence summarization aims to shorten a given sentence and produce a brief summary of it. This is different from document level summarization task since it is hard to apply existing techniques in extractive methods, such as extracting sentence level features and ranking sentences. Early works propose using rule-based methods (Zajic et al., 2007), syntactic tree pruning methods (Knight and Marcu, 2002), statistical machine translation techniques (Banko et al., 2000) and so on for this task. We focus on abstractive sentence summarization task in this paper. Recently, neural network models have been applied in this task. Rush et al. (2015) use autoconstructed sentence-headline pairs to train a neu∗ Contribution during internship at Microsoft Research. All the above works fall into the encodingdecoding paradigm, which first encodes the input sentence to an abstract representation and then decodes the intended output sentence based on the encoded information. As an extension of the encoding-decoding framework, attentionbased approach (Bahdanau et al., 2015)"
P17-1101,P16-1046,0,0.0503931,"Gigaword and DUC 2004 test sets show that the above models achieve state-of-theart results. Gu et al. (2016) and Gulcehre et al. (2016) come up similar ideas that summarization task can benefit from copying words from input sentences. Gu et al. (2016) propose CopyNet to model the copying action in response generation, which also applies for summarization task. Gulcehre et al. (2016) propose a switch gate to control whether to copy from source or generate from decoder vocabulary. Zeng et al. (2016) also propose using copy mechanism and add a scalar weight on the gate of GRU/LSTM for this task. Cheng and Lapata (2016) use an RNN based encoder-decoder for extractive summarization of documents. Yu et al. (2016) propose a segment to segment neural transduction model for sequence-tosequence framework. The model introduces a latent segmentation which determines correspondences between tokens of the input sequence and the output sequence. Experiments on this task show that the proposed transduction model per1096 forms comparable to the ABS model. Shen et al. (2016) propose to apply Minimum Risk Training (MRT) in neural machine translation to directly optimize the evaluation metrics. Ayana et al. (2016) apply MRT"
P17-1101,D14-1179,0,0.00739871,"Missing"
P17-1101,N16-1012,0,0.72835,"y statistical machine translation techniques by modeling headline generation as a translation task and use 8000 article-headline pairs to train the system. Rush et al. (2015) propose leveraging news data in Annotated English Gigaword (Napoles et al., 2012) corpus to construct large scale parallel data for sentence summarization task. They propose an ABS model, which consists of an attentive Convolutional Neural Network encoder and an neural network language model (Bengio et al., 2003) decoder. On this Gigaword test set and DUC 2004 test set, the ABS model produces the state-of-theart results. Chopra et al. (2016) extend this work, which keeps the CNN encoder but replaces the decoder with recurrent neural networks. Their experiments showes that the CNN encoder with RNN decoder model performs better than Rush et al. (2015). Nallapati et al. (2016) further change the encoder to an RNN encoder, which leads to a full RNN sequence-to-sequence model. Besides, they enrich the encoder with lexical and statistic features which play important roles in traditional feature based summarization systems, such as NER and POS tags, to improve performance. Experiments on the Gigaword and DUC 2004 test sets show that the"
P17-1101,W03-0501,0,0.313208,"2 recall and 10.63 ROUGE-2 F1 on these test sets respectively, which improves performance compared to the state-of-the-art methods. 2 Related Work Abstractive sentence summarization, also known as sentence compression and similar to headline generation, is used to help compress or fuse the selected sentences in extractive document summarization systems since they may inadvertently include unnecessary information. The sentence summarization task has been long connected to the headline generation task. There are some previous methods to solve this task, such as the linguistic rule-based method (Dorr et al., 2003). As for the statistical machine learning based methods, Banko et al. (2000) apply statistical machine translation techniques by modeling headline generation as a translation task and use 8000 article-headline pairs to train the system. Rush et al. (2015) propose leveraging news data in Annotated English Gigaword (Napoles et al., 2012) corpus to construct large scale parallel data for sentence summarization task. They propose an ABS model, which consists of an attentive Convolutional Neural Network encoder and an neural network language model (Bengio et al., 2003) decoder. On this Gigaword tes"
P17-1101,P16-1154,0,0.212515,"but replaces the decoder with recurrent neural networks. Their experiments showes that the CNN encoder with RNN decoder model performs better than Rush et al. (2015). Nallapati et al. (2016) further change the encoder to an RNN encoder, which leads to a full RNN sequence-to-sequence model. Besides, they enrich the encoder with lexical and statistic features which play important roles in traditional feature based summarization systems, such as NER and POS tags, to improve performance. Experiments on the Gigaword and DUC 2004 test sets show that the above models achieve state-of-theart results. Gu et al. (2016) and Gulcehre et al. (2016) come up similar ideas that summarization task can benefit from copying words from input sentences. Gu et al. (2016) propose CopyNet to model the copying action in response generation, which also applies for summarization task. Gulcehre et al. (2016) propose a switch gate to control whether to copy from source or generate from decoder vocabulary. Zeng et al. (2016) also propose using copy mechanism and add a scalar weight on the gate of GRU/LSTM for this task. Cheng and Lapata (2016) use an RNN based encoder-decoder for extractive summarization of documents. Yu et al"
P17-1101,P16-1014,0,0.192029,"der with recurrent neural networks. Their experiments showes that the CNN encoder with RNN decoder model performs better than Rush et al. (2015). Nallapati et al. (2016) further change the encoder to an RNN encoder, which leads to a full RNN sequence-to-sequence model. Besides, they enrich the encoder with lexical and statistic features which play important roles in traditional feature based summarization systems, such as NER and POS tags, to improve performance. Experiments on the Gigaword and DUC 2004 test sets show that the above models achieve state-of-theart results. Gu et al. (2016) and Gulcehre et al. (2016) come up similar ideas that summarization task can benefit from copying words from input sentences. Gu et al. (2016) propose CopyNet to model the copying action in response generation, which also applies for summarization task. Gulcehre et al. (2016) propose a switch gate to control whether to copy from source or generate from decoder vocabulary. Zeng et al. (2016) also propose using copy mechanism and add a scalar weight on the gate of GRU/LSTM for this task. Cheng and Lapata (2016) use an RNN based encoder-decoder for extractive summarization of documents. Yu et al. (2016) propose a segment"
P17-1101,N16-1082,0,0.00759375,"ovements compared to the s2s+att model. Overall, these improvements on all groups indicate that the selective encoding method benefits the abstractive sentence summarization task. ROUGE-2 F1 Scores Models 20 15 10 5 0 10 20 30 40 Input Sentence Length 50 60 Figure 4: ROUGE-2 F1 score on different groups of input sentences in terms of their length for s2s+att baseline and our SEASS model on English Gigaword test sets. Saliency Heat Map of Selective Gate Since the output of the selective gate network is a high dimensional vector, it is hard to visualize all the gate values. We use the method in Li et al. (2016) to visualize the contribution of the selective gate to the final output, which can be approximated by the first derivative. Given sentence words x with associated output summary y, the trained model associates the pair (x, y) with a score Sy (x). The goal is to decide which gate g associated with a specific word makes the most significant contribution to Sy (x). We approximate the Sy (g) by computing the first-order Taylor expansion since the score Sy (x) is a highly non-linear function in the deep neural network models: Sy (g) ≈ w(g)T g + b (19) where w(g) is first the derivative of Sy with"
P17-1101,W04-1013,0,0.0638597,"proximately 6,000 source text sentences with multiple manually-created summaries (about 26,000 sentence-summary pairs in total). Toutanova et al. (2016) provide a standard split of the data into training, development, and test sets, with 4,936, 448 and 785 input sentences respectively. Since the training set is too small, we only use the test set as one of our test sets. We denote this dataset as MSR-ATC (Microsoft Research Abstractive Text Compression) test set in the following. Table 2 summarizes the statistic information of the three datasets we used. 5.2 Evaluation Metric We employ ROUGE (Lin, 2004) as our evaluation metric. ROUGE measures the quality of summary by computing overlapping lexical units, such as unigram, bigram, trigram, and longest common subsequence (LCS). It becomes the standard evaluation metric for DUC shared tasks and popular for summarization evaluation. Following previous work, we use ROUGE-1 (unigram), ROUGE-2 (bi2 Thanks to Rush et al. (2015), we acquired the test set they used. Following Chopra et al. (2016), we remove pairs with empty titles resulting in slightly different accuracy compared to Rush et al. (2015) for their systems. The cleaned test set contains 1"
P17-1101,D15-1166,0,0.0301138,"r and the selective gate network, we use GRU with attention as the decoder to produce the output summary. At each decoding time step t, the GRU reads the previous word embedding wt−1 and previous context vector ct−1 as inputs to compute the new hidden state st . To initialize the GRU hidden state, we use a linear layer with the last backward encoder hidden state h~1 as input: st = GRU(wt−1 , ct−1 , st−1 ) s0 = tanh(Wd h~1 + b) (10) (11) where Wd is the weight matrix and b is the bias vector. The context vector ct for current time step t is computed through the concatenate attention mechanism (Luong et al., 2015), which matches the current decoder state st with each encoder hidden state h0i to get an importance score. The importance scores are then normalized to get the current context vector by weighted sum: (8) (9) where Ws and Us are weight matrices, b is the bias vector, σ denotes sigmoid activation function, and is element-wise multiplication. After the selective gate network, we obtain another sequence of vectors (h01 , h02 , . . . , h0n ). This new sequence is then used as the input sentence representation for the decoder to generate the summary. Summary Decoder et,i = va&gt; tanh(Wa st−1 + Ua h0i"
P17-1101,K16-1028,0,0.26566,"Missing"
P17-1101,W12-3018,0,0.0538818,"Missing"
P17-1101,D16-1033,0,0.0242887,"hat except for the empty titles, this test set has some invalid lines like the input sentence containing only one word. Therefore, we further sample 2000 pairs as our internal test set and release it for future works3 . DUC 2004 Test Set We employ DUC 2004 data for tasks 1 & 2 (Over et al., 2007) in our experiments as one of the test sets since it is too small to train a neural network model on. The dataset pairs each document with 4 different human-written reference summaries which are capped at 75 bytes. It has 500 input sentences with each sentence paired with 4 summaries. MSR-ATC Test Set Toutanova et al. (2016) release a new dataset for sentence summarization task by crowdsourcing. This dataset contains approximately 6,000 source text sentences with multiple manually-created summaries (about 26,000 sentence-summary pairs in total). Toutanova et al. (2016) provide a standard split of the data into training, development, and test sets, with 4,936, 448 and 785 input sentences respectively. Since the training set is too small, we only use the test set as one of our test sets. We denote this dataset as MSR-ATC (Microsoft Research Abstractive Text Compression) test set in the following. Table 2 summarizes"
P17-1101,D16-1138,0,0.0411574,". (2016) and Gulcehre et al. (2016) come up similar ideas that summarization task can benefit from copying words from input sentences. Gu et al. (2016) propose CopyNet to model the copying action in response generation, which also applies for summarization task. Gulcehre et al. (2016) propose a switch gate to control whether to copy from source or generate from decoder vocabulary. Zeng et al. (2016) also propose using copy mechanism and add a scalar weight on the gate of GRU/LSTM for this task. Cheng and Lapata (2016) use an RNN based encoder-decoder for extractive summarization of documents. Yu et al. (2016) propose a segment to segment neural transduction model for sequence-tosequence framework. The model introduces a latent segmentation which determines correspondences between tokens of the input sequence and the output sequence. Experiments on this task show that the proposed transduction model per1096 forms comparable to the ABS model. Shen et al. (2016) propose to apply Minimum Risk Training (MRT) in neural machine translation to directly optimize the evaluation metrics. Ayana et al. (2016) apply MRT on abstractive sentence summarization task and the results show that optimizing for ROUGE im"
P17-1101,D15-1044,0,0.425453,"to shorten a given sentence and produce a brief summary of it. This is different from document level summarization task since it is hard to apply existing techniques in extractive methods, such as extracting sentence level features and ranking sentences. Early works propose using rule-based methods (Zajic et al., 2007), syntactic tree pruning methods (Knight and Marcu, 2002), statistical machine translation techniques (Banko et al., 2000) and so on for this task. We focus on abstractive sentence summarization task in this paper. Recently, neural network models have been applied in this task. Rush et al. (2015) use autoconstructed sentence-headline pairs to train a neu∗ Contribution during internship at Microsoft Research. All the above works fall into the encodingdecoding paradigm, which first encodes the input sentence to an abstract representation and then decodes the intended output sentence based on the encoded information. As an extension of the encoding-decoding framework, attentionbased approach (Bahdanau et al., 2015) has been broadly used: the encoder produces a list of vectors for all tokens in the input, and the decoder uses an attention mechanism to dynamically extract encoded informati"
P17-1101,P16-1159,0,0.0161797,"ate from decoder vocabulary. Zeng et al. (2016) also propose using copy mechanism and add a scalar weight on the gate of GRU/LSTM for this task. Cheng and Lapata (2016) use an RNN based encoder-decoder for extractive summarization of documents. Yu et al. (2016) propose a segment to segment neural transduction model for sequence-tosequence framework. The model introduces a latent segmentation which determines correspondences between tokens of the input sequence and the output sequence. Experiments on this task show that the proposed transduction model per1096 forms comparable to the ABS model. Shen et al. (2016) propose to apply Minimum Risk Training (MRT) in neural machine translation to directly optimize the evaluation metrics. Ayana et al. (2016) apply MRT on abstractive sentence summarization task and the results show that optimizing for ROUGE improves the test performance. 3 Input: Output: South Korean President Kim Young-Sam left here Wednesday on a week - long state visit to Russia and Uzbekistan for talks on North Korea ’s nuclear confrontation and ways to strengthen bilateral ties . Kim leaves for Russia for talks on NKorea nuclear standoff Table 1: An abstractive sentence summarization exam"
P17-4017,D14-1162,0,0.11618,"e “No. The keyboard is extra and functions as the cover”. Therefore, the question ranking problem is essential for an FAQ search engine. Formally, given two questions q and q0, the question ranker will learn a mapping function f where f (q, q0) → [0, 1], so f is actually a semantic similarity metric between two questions, indicating whether they convey the same meaning. We train a regression forest model (Meinshausen, 2006) and use the following features: monolingual word aligner (Sultan et al., 2014), DSSM model (Huang et al., 2013), word embedding composition (max, sum, idf-sum) with GloVe (Pennington et al., 2014), n-gram overlap, subsequence matching, PairingWords (Han et al., 2013), word mover’s distance (Kusner et al., 2015). The model is evaluated on the dataset for SemEval-2016 Task 1. The mean accuracy (Pearson Correlation) of our model on five datasets (0.78455) significantly outperforms the 1st place team (0.77807)1 . Specifically, the accuracy on the question-to-question dataset is 0.75773, surpassing the 1st place team (0.68705) by a large margin. This confirms the effectiveness of our model on the FAQ search task. Q: What’s the CPU? A: The processor is 3 GHz Intel Core i5 for Microsoft Surfa"
P17-4017,J11-1002,0,0.0440437,"e with a keyboard Customer reviews provide rich information for different aspects of the product from users’ perA: No. The keyboard is extra and functions as the cover 1 http://alt.qcri.org/semeval2016/ task1/index.php?id=results 99 spective. They are very important resources for answering opinion-oriented questions. To this end, we first split the review text into sentences and run opinion mining modules to extract the aspects and corresponding opinions. We then use the text question answering module to generate responses (i.e. review sentences). For opinion mining, we use a hybrid approach (Qiu et al., 2011) to extract the aspects from review sentences. We also run the sentiment classifier (Tang et al., 2014) to determine the polarity (i.e. positive, negative, and neutral) of the sentence regarding the specific aspect mentioned. The aspects and polarity are indexed together with keywords using the Lucene toolkit2 . For text QA, given an input query, it outputs the answer based on the following three steps: Q: Is the screen size of surface pro 4 appropriate for reading? R: The screen is not too small like the iPad, neither is it bulky like a laptop. 2.5 The chit-chat engine is mainly designed to r"
P17-4017,Q14-1018,0,0.0226224,"sks “does it have a keyboard”, the question “does it come with a keyboard” will be matched and the answer should be “No. The keyboard is extra and functions as the cover”. Therefore, the question ranking problem is essential for an FAQ search engine. Formally, given two questions q and q0, the question ranker will learn a mapping function f where f (q, q0) → [0, 1], so f is actually a semantic similarity metric between two questions, indicating whether they convey the same meaning. We train a regression forest model (Meinshausen, 2006) and use the following features: monolingual word aligner (Sultan et al., 2014), DSSM model (Huang et al., 2013), word embedding composition (max, sum, idf-sum) with GloVe (Pennington et al., 2014), n-gram overlap, subsequence matching, PairingWords (Han et al., 2013), word mover’s distance (Kusner et al., 2015). The model is evaluated on the dataset for SemEval-2016 Task 1. The mean accuracy (Pearson Correlation) of our model on five datasets (0.78455) significantly outperforms the 1st place team (0.77807)1 . Specifically, the accuracy on the question-to-question dataset is 0.75773, surpassing the 1st place team (0.68705) by a large margin. This confirms the effectivene"
P17-4017,P14-1146,1,0.225227,"users’ perA: No. The keyboard is extra and functions as the cover 1 http://alt.qcri.org/semeval2016/ task1/index.php?id=results 99 spective. They are very important resources for answering opinion-oriented questions. To this end, we first split the review text into sentences and run opinion mining modules to extract the aspects and corresponding opinions. We then use the text question answering module to generate responses (i.e. review sentences). For opinion mining, we use a hybrid approach (Qiu et al., 2011) to extract the aspects from review sentences. We also run the sentiment classifier (Tang et al., 2014) to determine the polarity (i.e. positive, negative, and neutral) of the sentence regarding the specific aspect mentioned. The aspects and polarity are indexed together with keywords using the Lucene toolkit2 . For text QA, given an input query, it outputs the answer based on the following three steps: Q: Is the screen size of surface pro 4 appropriate for reading? R: The screen is not too small like the iPad, neither is it bulky like a laptop. 2.5 The chit-chat engine is mainly designed to reply to greeting queries such as “hello“ and “thank you”, as well as queries that cannot be answered by"
P17-4017,S13-1005,0,0.031184,"ion ranking problem is essential for an FAQ search engine. Formally, given two questions q and q0, the question ranker will learn a mapping function f where f (q, q0) → [0, 1], so f is actually a semantic similarity metric between two questions, indicating whether they convey the same meaning. We train a regression forest model (Meinshausen, 2006) and use the following features: monolingual word aligner (Sultan et al., 2014), DSSM model (Huang et al., 2013), word embedding composition (max, sum, idf-sum) with GloVe (Pennington et al., 2014), n-gram overlap, subsequence matching, PairingWords (Han et al., 2013), word mover’s distance (Kusner et al., 2015). The model is evaluated on the dataset for SemEval-2016 Task 1. The mean accuracy (Pearson Correlation) of our model on five datasets (0.78455) significantly outperforms the 1st place team (0.77807)1 . Specifically, the accuracy on the question-to-question dataset is 0.75773, surpassing the 1st place team (0.68705) by a large margin. This confirms the effectiveness of our model on the FAQ search task. Q: What’s the CPU? A: The processor is 3 GHz Intel Core i5 for Microsoft Surface Pro 4 (128 GB, 4 GB RAM, Intel Core i5) Q: What is the pre-installed"
P17-4017,N16-1170,0,0.0184876,"h ranks all candidate sentences with a regression based ranking framework. • Candidate triggering, which decides whether it is confident enough to output the candidate. Specifically, for candidate retrieval, we use the default ranker in Lucene to retrieve the top 20 candidate sentences. For candidate ranking, we build a regression based framework to rank all candidate sentences based on features designed at different levels of granularity. Our feature set consists of WordCnt, translation model, type matching, WordNet, and two neural network based methods BiCNN (Yu et al., 2014) and MatchLSTM (Wang and Jiang, 2016). We conduct experiments on the WikiQA (Yang et al., 2015) dataset. The results show that we achieve state-of-the-art results with 0.7164 in terms of MAP and 0.7332 in terms of MRR. For candidate triggering, as the ranking model outputs a regression score for each candidate sentence, we only output the candidate sentence whose score is higher than the threshold selected on the development set. The engine’s output examples are shown as follows: Q: hello R: hey how are you? Q: thank you R: you’re very welcome sir Q: you are so cute R: u r more 2.6 Meta Engine For each query, SuperAgent will call"
P17-4017,D15-1237,0,0.0138675,"ng framework. • Candidate triggering, which decides whether it is confident enough to output the candidate. Specifically, for candidate retrieval, we use the default ranker in Lucene to retrieve the top 20 candidate sentences. For candidate ranking, we build a regression based framework to rank all candidate sentences based on features designed at different levels of granularity. Our feature set consists of WordCnt, translation model, type matching, WordNet, and two neural network based methods BiCNN (Yu et al., 2014) and MatchLSTM (Wang and Jiang, 2016). We conduct experiments on the WikiQA (Yang et al., 2015) dataset. The results show that we achieve state-of-the-art results with 0.7164 in terms of MAP and 0.7332 in terms of MRR. For candidate triggering, as the ranking model outputs a regression score for each candidate sentence, we only output the candidate sentence whose score is higher than the threshold selected on the development set. The engine’s output examples are shown as follows: Q: hello R: hey how are you? Q: thank you R: you’re very welcome sir Q: you are so cute R: u r more 2.6 Meta Engine For each query, SuperAgent will call the abovementioned sub-engines in parallel. The meta engi"
P18-1015,P16-1154,0,0.0665386,"(Banko et al., 2000). Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags. These features have played important roles in traditional feature based summarization systems. Gu et al. (2016) found that a large proportion of the words in the summary were copied from the source text. Therefore, they proposed CopyNet which considered the copying mechanism during generation. Recently, See et al. (2017) used the coverage mechanism to discourage repetition. Cao et al. (2017b) encoded facts extracted from the source sentence to enhance the summary faithfulness. There were also studies to modify the loss function to fit the evaluation metrics. For instance, Ayana et al. (2016) applied the Minimum Risk Training strategy to maximize the ROUGE scores of generated sum5 Conclusion and Future"
P18-1015,W17-3204,0,0.0762296,"Missing"
P18-1015,W04-1013,0,0.062659,"seq model to generate more faithful and informative summaries. Specifically, since the input of our system consists of both the sentence and soft template, we use the concatenation function3 to combine the hidden states of the sentence and template: 2.2.1 Rerank In Retrieve, the template candidates are ranked according to the text similarity between the corresponding indexed sentences and the input sentence. However, for the summarization task, we expect the soft template r resembles the actual summary y∗ as much as possible. Here we use the widely-used summarization evaluation metrics ROUGE (Lin, 2004) to measure the actual saliency s∗ (r, y∗ ) (see Section 3.2). We utilize the hidden states of x and r to predict the saliency s of the template. Specifically, we regard the output of the BiRNN as the representation of the sentence or template: ← − → − hx = [ h x1 ; h x−1 ] ← − → − hr = [ h r1 ; h r−1 ] Hc = [hx1 ; · · · ; hx−1 ; hr1 ; · · · ; hr−1 ] The combined hidden states are fed into the prevailing attentional RNN decoder (Bahdanau et al., 2014) to generate the decoding hidden state at the position t: st = Att-RNN(st−1 , yt−1 , Hc ), s(r, x) = (2) ot = sof tmax(st Wo ), (3) + bs ), (6) w"
P18-1015,D15-1166,0,0.0773642,"eNLP (Manning et al., 2014) to recognize named entities. 3.3 the sentence. ABS+ Rush et al. (2015a) further tuned the ABS model with additional hand-crafted features to balance between abstraction and extraction. RAS-Elman As the extension of the ABS model, it used a convolutional attention-based encoder and a RNN decoder (Chopra et al., 2016). Featseq2seq Nallapati et al. (2016) used a complete seq2seq RNN model and added the hand-crafted features such as POS tag and NER, to enhance the encoder representation. Luong-NMT Chopra et al. (2016) implemented the neural machine translation model of Luong et al. (2015) for summarization. This model contained two-layer LSTMs with 500 hidden units in each layer. OpenNMT We also implement the standard attentional seq2seq model with OpenNMT. All the settings are the same as our system. It is noted that OpenNMT officially examined the Gigaword dataset. We distinguish the official result6 and our experimental result with suffixes “O” and “I” respectively. FTSum Cao et al. (2017b) encoded the facts extracted from the source sentence to improve both the faithfulness and informativeness of generated summaries. In addition, to evaluate the effectiveness of our joint"
P18-1015,P00-1041,0,0.523097,"of neural text generation. However, they handled the task of Language Modeling and randomly picked an existing sentence in the training corpus. In comparison, we develop an IR system to find proper existing summaries as soft templates. Moreover, Guu et al. (2017) used a general seq2seq framework while we extend the seq2seq framework to conduct template reranking and template-aware summary generation simultaneously. tion include template-based methods (Zhou and Hovy, 2004), syntactic tree pruning (Knight and Marcu, 2002; Clarke and Lapata, 2008) and statistical machine translation techniques (Banko et al., 2000). Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags. These features have played important roles in traditional feature based summarization systems. Gu et al. (2016) fou"
P18-1015,P14-5010,0,0.00357672,"LESS 3 The number of the generated summaries, which contains less than three tokens. These extremely short summaries are usually unreadable. COPY The proportion of the summary words (without stopwords) copied from the source sentence. A seriously large copy ratio indicates that the summarization system pays more attention to compression rather than required abstraction. NEW NE The number of the named entities that do not appear in the source sentence or actual summary. Intuitively, the appearance of new named entities in the summary is likely to bring unfaithfulness. We use Stanford CoreNLP (Manning et al., 2014) to recognize named entities. 3.3 the sentence. ABS+ Rush et al. (2015a) further tuned the ABS model with additional hand-crafted features to balance between abstraction and extraction. RAS-Elman As the extension of the ABS model, it used a convolutional attention-based encoder and a RNN decoder (Chopra et al., 2016). Featseq2seq Nallapati et al. (2016) used a complete seq2seq RNN model and added the hand-crafted features such as POS tag and NER, to enhance the encoder representation. Luong-NMT Chopra et al. (2016) implemented the neural machine translation model of Luong et al. (2015) for sum"
P18-1015,K16-1028,0,0.407324,"ng online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning. It can be used to design or refine appealing headlines. Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). 152 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 152–161 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics and Rewrite. Given the input sentence x, the Retrieve module filters candidate soft templates C = {ri } from the training corpus. For validation and test, we regard the candidate template with the highest predicted saliency (a.k.a informativeness) score as the actual soft template r. For training, we choose the one with the maximal actual saliency score in C, which speeds up converge"
P18-1015,P16-1223,0,0.0259771,"Missing"
P18-1015,D15-1044,0,0.0589636,"and templateaware summary generation (Rewriting). Experiments show that, in terms of informativeness, our model significantly outperforms the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness. In addition, the import of high-quality external summaries improves the stability and readability of generated summaries. 1 Introduction The exponentially growing online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning. It can be used to design or refine appealing headlines. Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). 152 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 152–161 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computati"
P18-1015,N16-1012,0,0.53363,"e exponentially growing online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization (Rush et al., 2015a), which generates a shorter version of a given sentence while attempting to preserve its original meaning. It can be used to design or refine appealing headlines. Recently, the application of the attentional sequence-to-sequence (seq2seq) framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). 152 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 152–161 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics and Rewrite. Given the input sentence x, the Retrieve module filters candidate soft templates C = {ri } from the training corpus. For validation and test, we regard the candidate template with the highest predicted saliency (a.k.a informativeness) score as the actual soft template r. For training, we choose the one with the maximal actual saliency score in C,"
P18-1015,P17-1099,0,0.146846,"16; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as named entities and POS tags. These features have played important roles in traditional feature based summarization systems. Gu et al. (2016) found that a large proportion of the words in the summary were copied from the source text. Therefore, they proposed CopyNet which considered the copying mechanism during generation. Recently, See et al. (2017) used the coverage mechanism to discourage repetition. Cao et al. (2017b) encoded facts extracted from the source sentence to enhance the summary faithfulness. There were also studies to modify the loss function to fit the evaluation metrics. For instance, Ayana et al. (2016) applied the Minimum Risk Training strategy to maximize the ROUGE scores of generated sum5 Conclusion and Future Work This paper proposes to introduce soft templates as additional input to guide the seq2seq summarization. We use the popular IR platform Lucene to retrieve proper existing summaries as candidate soft template"
P18-1015,W04-1000,0,0.59971,"ctive function of likelihood and ROUGE scores. Guu et al. (2017) also proposed to encode human-written sentences to improvement the performance of neural text generation. However, they handled the task of Language Modeling and randomly picked an existing sentence in the training corpus. In comparison, we develop an IR system to find proper existing summaries as soft templates. Moreover, Guu et al. (2017) used a general seq2seq framework while we extend the seq2seq framework to conduct template reranking and template-aware summary generation simultaneously. tion include template-based methods (Zhou and Hovy, 2004), syntactic tree pruning (Knight and Marcu, 2002; Clarke and Lapata, 2008) and statistical machine translation techniques (Banko et al., 2000). Recently, the application of the attentional seq2seq framework has attracted growing attention and achieved state-of-the-art performance on this task (Rush et al., 2015a; Chopra et al., 2016; Nallapati et al., 2016). In addition to the direct application of the general seq2seq framework, researchers attempted to integrate various properties of summarization. For example, Nallapati et al. (2016) enriched the encoder with hand-crafted features such as na"
P18-1061,P15-2136,1,0.953154,"nh,mingzhou}@microsoft.com Abstract Sentence scoring aims to assign an importance score to each sentence, and has been broadly studied in many previous works. Feature-based methods are popular and have proven effective, such as word probability, TF*IDF weights, sentence position and sentence length features (Luhn, 1958; Hovy and Lin, 1998; Ren et al., 2017). Graph-based methods such as TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) measure sentence importance using weighted-graphs. In recent years, neural network has also been applied to sentence modeling and scoring (Cao et al., 2015a; Ren et al., 2017). For the second step, sentence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary. Integer Linear Programming based methods (McDonald, 2007) treat sentence selection as an optimization problem under some constraints such as summary length. Submodular functions (Lin and Bilmes, 2011) have also been applied to solving the optimization problem of finding the"
P18-1061,P16-1046,0,0.136097,"a sentence. Sentence selection is based on the scores of 655 sidering the DUC tasks have byte length limit for summaries. In this work, we adopt the CNN/Daily Mail dataset to train the neural network model, which does not have this length limit. To prevent the tendency of choosing longer sentences, we use ROUGE F1 as the evaluation function r(·), and set the length limit l as a fixed number of sentences. Therefore, the proposed model is trained to learn a scoring function g(·) of the ROUGE F1 gain, specifically: a two-level attention mechanism to measure the contextual relations of sentences. Cheng and Lapata (2016) propose treating document summarization as a sequence labeling task. They first encode the sentences in the document and then classify each sentence into two classes, i.e., extraction or not. Nallapati et al. (2017) propose a system called SummaRuNNer with more features, which also treat extractive document summarization as a sequence labeling task. The two works are both in the separated paradigm, as they first assign a probability of being extracted to each sentence, and then select sentences according to the probability until reaching the length limit. Ren et al. (2016) train two neural ne"
P18-1061,W09-1802,0,0.104755,"these methods, the input document is represented as a connected graph. The vertices represent the sentences, and the edges between vertices have attached weights that show the similarity of the two sentences. The score of a sentence is the importance of its corresponding vertex, which can be computed using graph algorithms. Machine learning techniques are also widely used for better sentence modeling and importance estimation. Kupiec et al. (1995) use a Naive Bayes classifier to learn feature combinations. Conroy and O’leary (2001) further use a Hidden Markov Model in document summarization. Gillick and Favre (2009) find that using bigram features consistently yields better performance than unigrams or trigrams for ROUGE (Lin, 2004) measures. Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) method as a heuristic in sentence selection. Systems using MMR select the sentence which has the maximal score and is minimally redundant with previous selected sentences. McDonald (2007) treats sentence selection as an optimization problem under some constraints such as summary length. Therefore, he uses Integer Linear Programming (ILP) to solve this optimization problem. Sentence selectio"
P18-1061,P84-1044,0,0.585141,"Missing"
P18-1061,X98-1026,0,0.345759,"Summarization by Jointly Learning to Score and Select Sentences Qingyu Zhou†∗, Nan Yang‡ , Furu Wei‡ , Shaohan Huang‡ , Ming Zhou‡ , Tiejun Zhao† † Harbin Institute of Technology, Harbin, China ‡ Microsoft Research, Beijing, China {qyzhou,tjzhao}@hit.edu.cn {nanya,fuwei,shaohanh,mingzhou}@microsoft.com Abstract Sentence scoring aims to assign an importance score to each sentence, and has been broadly studied in many previous works. Feature-based methods are popular and have proven effective, such as word probability, TF*IDF weights, sentence position and sentence length features (Luhn, 1958; Hovy and Lin, 1998; Ren et al., 2017). Graph-based methods such as TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) measure sentence importance using weighted-graphs. In recent years, neural network has also been applied to sentence modeling and scoring (Cao et al., 2015a; Ren et al., 2017). For the second step, sentence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary"
P18-1061,D14-1162,0,0.0899533,"Missing"
P18-1061,W04-1013,0,0.342842,"ertices have attached weights that show the similarity of the two sentences. The score of a sentence is the importance of its corresponding vertex, which can be computed using graph algorithms. Machine learning techniques are also widely used for better sentence modeling and importance estimation. Kupiec et al. (1995) use a Naive Bayes classifier to learn feature combinations. Conroy and O’leary (2001) further use a Hidden Markov Model in document summarization. Gillick and Favre (2009) find that using bigram features consistently yields better performance than unigrams or trigrams for ROUGE (Lin, 2004) measures. Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) method as a heuristic in sentence selection. Systems using MMR select the sentence which has the maximal score and is minimally redundant with previous selected sentences. McDonald (2007) treats sentence selection as an optimization problem under some constraints such as summary length. Therefore, he uses Integer Linear Programming (ILP) to solve this optimization problem. Sentence selection can also be seen as finding the optimal subset of sentences in a document. Lin and Bilmes (2011) propose using submod"
P18-1061,C16-1004,1,0.938489,"entence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary. Integer Linear Programming based methods (McDonald, 2007) treat sentence selection as an optimization problem under some constraints such as summary length. Submodular functions (Lin and Bilmes, 2011) have also been applied to solving the optimization problem of finding the optimal subset of sentences in a document. Ren et al. (2016) train two neural networks with handcrafted features. One is used to rank sentences, and the other one is used to model redundancy during sentence selection. In this paper, we present a neural extractive document summarization (N EU S UM) framework which jointly learns to score and select sentences. Different from previous methods that treat sentence scoring and sentence selection as two tasks, our method integrates the two steps into one endto-end trainable model. Specifically, N EU S UM is a neural network model without any handcrafted features that learns to identify the relative importance"
P18-1061,P11-1052,0,0.101239,"years, neural network has also been applied to sentence modeling and scoring (Cao et al., 2015a; Ren et al., 2017). For the second step, sentence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary. Integer Linear Programming based methods (McDonald, 2007) treat sentence selection as an optimization problem under some constraints such as summary length. Submodular functions (Lin and Bilmes, 2011) have also been applied to solving the optimization problem of finding the optimal subset of sentences in a document. Ren et al. (2016) train two neural networks with handcrafted features. One is used to rank sentences, and the other one is used to model redundancy during sentence selection. In this paper, we present a neural extractive document summarization (N EU S UM) framework which jointly learns to score and select sentences. Different from previous methods that treat sentence scoring and sentence selection as two tasks, our method integrates the two steps into one endto-end trainable mo"
P18-1061,P17-1099,0,0.785313,"bel the sentences in a given document similar to Nallapati et al. (2017). Specifically, we construct training data by maximizing the ROUGE-2 F1 score. Since it is computationally expensive to find the global optimal combination of sentences, we employ a greedy approach. Given a document with n sentences, we n enumerate the candidates  from 1-combination 1 n to n-combination n . We stopsearching if the n highest ROUGE-2  F1 score in k is less than the n best one in k−1 . Table 1 shows the data statistics of the CNN/Daily Mail dataset. We conduct data preprocessing using the same method2 in See et al. (2017), including sentence splitting and word tokenization. Both Nallapati et al. (2016, 2017) use the anonymized version of the data, where the named entities are replaced by identifiers such as entity4. Following See et al. (2017), we use the non-anonymized version so we can directly operate on the original text. Si ∈D 4.3 (20) Experiments 5.1 whereWm and bm are learnable parameters, and s~1 is the last backward state of the document level encoder BiGRU. Since we do not have any sentences extracted yet, we use a zero vector to represent the previous extracted sentence, i.e., s0 = 0. With the score"
P18-1061,W04-3252,0,0.790016,"∗, Nan Yang‡ , Furu Wei‡ , Shaohan Huang‡ , Ming Zhou‡ , Tiejun Zhao† † Harbin Institute of Technology, Harbin, China ‡ Microsoft Research, Beijing, China {qyzhou,tjzhao}@hit.edu.cn {nanya,fuwei,shaohanh,mingzhou}@microsoft.com Abstract Sentence scoring aims to assign an importance score to each sentence, and has been broadly studied in many previous works. Feature-based methods are popular and have proven effective, such as word probability, TF*IDF weights, sentence position and sentence length features (Luhn, 1958; Hovy and Lin, 1998; Ren et al., 2017). Graph-based methods such as TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) measure sentence importance using weighted-graphs. In recent years, neural network has also been applied to sentence modeling and scoring (Cao et al., 2015a; Ren et al., 2017). For the second step, sentence selection adopts a particular strategy to choose content sentence by sentence. Maximal Marginal Relevance (Carbonell and Goldstein, 1998) based methods select the sentence that has the maximal score and is minimally redundant with sentences already included in the summary. Integer Linear Programming based methods (McDonald, 2007) treat sentence selection"
P18-1061,N06-2046,0,0.175732,"for Computational Linguistics sentences to determine which sentence should be extracted, which is usually done heuristically. Many techniques have been proposed to model and score sentences. Unsupervised methods do not require model training or data annotation. In these methods, many surface features are useful, such as term frequency (Luhn, 1958), TF*IDF weights (Erkan and Radev, 2004), sentence length (Cao et al., 2015a) and sentence positions (Ren et al., 2017). These features can be used alone or combined with weights. Graph-based methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Wan and Yang, 2006) are also applied broadly to ranking sentences. In these methods, the input document is represented as a connected graph. The vertices represent the sentences, and the edges between vertices have attached weights that show the similarity of the two sentences. The score of a sentence is the importance of its corresponding vertex, which can be computed using graph algorithms. Machine learning techniques are also widely used for better sentence modeling and importance estimation. Kupiec et al. (1995) use a Naive Bayes classifier to learn feature combinations. Conroy and O’leary (2001) further use"
P18-1061,K16-1028,0,0.224849,"Missing"
P18-1097,I17-2058,0,0.0228356,"el to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Adapting grammatical error correction based on the native language of writers with neural network joint models. In EMNLP. Conclusion We propose a novel fluency boost learning and inference mechanism to o"
P18-1097,W07-1604,0,0.121354,"Missing"
P18-1097,W17-5037,0,0.211908,"0 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation. 5.3.3 Towards the state-of-the-art for GEC Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the stateof-the-art result. We first compare our best models – dual-boost learning (+native) with fluency boost inference and shallow fusion LM – to top-performing GEC systems evaluated on CoNLL-2014 dataset: We give more details about disfluency candidates, inclu"
P18-1097,D11-1010,0,0.0136033,"G test set). System Source CAMB14 CAMB16SMT CAMB16NMT CAMB17 (CAMB16SMT based) CAMB17 (AMU16 based) NUS16 NUS17 AMU16∗ Nested-seq2seq Sakaguchi et al. (2017)∗ Ours Ours (with non-public Lang-8 data) Human JFLEG Dev GLEU 38.21 42.81 46.10 47.20 47.72 43.26 46.27 51.01 49.74 48.93 49.82 51.35 52.93 55.26 JFLEG Test GLEU 40.54 46.04 52.05 50.13 56.78 51.46 53.41 53.98 56.33 57.74 62.37 Table 6: JFLEG Leaderboard. Ours denote the single dual-boost models in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Brisco"
P18-1097,D12-1052,0,0.184701,"use for fluency boost learning is native data which is proven to be useful for GEC. English Wikipedia that contains 61,677,453 senAs discussed in Section 3.1, when there is no tences. additional native data, S in Algorithm 1–3 is idenWe use CoNLL-2014 shared task dataset with tical to S ∗ . In the case where additional native data original annotations (Ng et al., 2014), which conis available to help model learning, S becomes: tains 1,312 sentences, as our main test set for evalS = S∗ ∪ C uation. We use MaxMatch (M2 ) precision, recall c c where C = {(x , x )} denotes the set of selfand F0.5 (Dahlmeier and Ng, 2012b) as our evaluacopied sentence pairs from native data. tion metrics. As previous studies, we use CoNLL2013 test data as our development set. 10: 11: 12: 4 Fluency boost inference As we discuss in Section 1, some sentences with multiple grammatical errors usually cannot be perfectly corrected through normal seq2seq inference 5.2 Experimental setting We set up experiments in order to answer the following questions: 1059 Model normal seq2seq back-boost self-boost dual-boost back-boost (+native) self-boost (+native) dual-boost (+native) back-boost (+native)? self-boost (+native)? dual-boost (+nat"
P18-1097,N12-1067,0,0.644898,"use for fluency boost learning is native data which is proven to be useful for GEC. English Wikipedia that contains 61,677,453 senAs discussed in Section 3.1, when there is no tences. additional native data, S in Algorithm 1–3 is idenWe use CoNLL-2014 shared task dataset with tical to S ∗ . In the case where additional native data original annotations (Ng et al., 2014), which conis available to help model learning, S becomes: tains 1,312 sentences, as our main test set for evalS = S∗ ∪ C uation. We use MaxMatch (M2 ) precision, recall c c where C = {(x , x )} denotes the set of selfand F0.5 (Dahlmeier and Ng, 2012b) as our evaluacopied sentence pairs from native data. tion metrics. As previous studies, we use CoNLL2013 test data as our development set. 10: 11: 12: 4 Fluency boost inference As we discuss in Section 1, some sentences with multiple grammatical errors usually cannot be perfectly corrected through normal seq2seq inference 5.2 Experimental setting We set up experiments in order to answer the following questions: 1059 Model normal seq2seq back-boost self-boost dual-boost back-boost (+native) self-boost (+native) dual-boost (+native) back-boost (+native)? self-boost (+native)? dual-boost (+nat"
P18-1097,W13-1703,0,0.396373,"h of them 5 Experiments are dynamically updated, which improves each 5.1 Dataset and evaluation other: the disfluency candidates produced by error generation model can benefit training the error As previous studies (Ji et al., 2017), we use the correction model, while the disfluency candidates public Lang-8 Corpus (Mizumoto et al., 2011; created by error correction model can be used as Tajiri et al., 2012), Cambridge Learner Cortraining data for the error generation model. We pus (CLC) (Nicholls, 2003) and NUS Corpus summarize this learning approach in Algorithm 3. of Learner English (NUCLE) (Dahlmeier et al., 2013) as our original error-corrected training data. 3.4 Fluency boost learning with large-scale Table 1 shows the stats of the datasets. In addinative data tion, we also collect 2,865,639 non-public errorOur proposed fluency boost learning strategies can corrected sentence pairs from Lang-8.com. The be easily extended to utilize the huge volume of native data we use for fluency boost learning is native data which is proven to be useful for GEC. English Wikipedia that contains 61,677,453 senAs discussed in Section 3.1, when there is no tences. additional native data, S in Algorithm 1–3 is idenWe us"
P18-1097,I13-1122,0,0.0200246,"CAMB17 (AMU16 based) NUS16 NUS17 AMU16∗ Nested-seq2seq Sakaguchi et al. (2017)∗ Ours Ours (with non-public Lang-8 data) Human JFLEG Dev GLEU 38.21 42.81 46.10 47.20 47.72 43.26 46.27 51.01 49.74 48.93 49.82 51.35 52.93 55.26 JFLEG Test GLEU 40.54 46.04 52.05 50.13 56.78 51.46 53.41 53.98 56.33 57.74 62.37 Table 6: JFLEG Leaderboard. Ours denote the single dual-boost models in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 20"
P18-1097,W11-2838,0,0.0978,"Missing"
P18-1097,P06-1032,0,0.245061,"ult (GLEU=61.50 on JFLEG test set). System Source CAMB14 CAMB16SMT CAMB16NMT CAMB17 (CAMB16SMT based) CAMB17 (AMU16 based) NUS16 NUS17 AMU16∗ Nested-seq2seq Sakaguchi et al. (2017)∗ Ours Ours (with non-public Lang-8 data) Human JFLEG Dev GLEU 38.21 42.81 46.10 47.20 47.72 43.26 46.27 51.01 49.74 48.93 49.82 51.35 52.93 55.26 JFLEG Test GLEU 40.54 46.04 52.05 50.13 56.78 51.46 53.41 53.98 56.33 57.74 62.37 Table 6: JFLEG Leaderboard. Ours denote the single dual-boost models in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq"
P18-1097,C08-1022,0,0.0710524,"Missing"
P18-1097,P17-1074,0,0.164354,"which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Adapting grammatical error correction based on the native language of writers with neural network joint models. In EMNLP. Conclusion We propose a novel fluency boost learning and inf"
P18-1097,E14-3013,0,0.0794372,"kis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier"
P18-1097,W14-1702,0,0.451367,". It is clear that our approach can generate less fluent sentences with various grammatical errors and most of them are typical mistakes that a human learner tends to make. Therefore, they can be used to establish high-quality training data with their correct sentence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Ch"
P18-1097,P11-2089,0,0.0269671,"al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Adapting grammatical error correction based on the native language o"
P18-1097,N18-2046,0,0.427784,"t our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets. 6 Related work Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et al., 2010; Tetreault et al., 2010a; Dale and Kilgarriff, 2011) 7 The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chollampatt and Ng (2018) (F0.5 =54.79) and Grundkiewicz and Junczys-Dowmunt (2018) (F0.5 =56.25), which are contemporaneous to this paper. In contrast to the basic seq2seq model in this paper, they used advanced approaches for modeling (e.g., convolutional seq2seq with pre-trained word embedding, using edit operation features, ensemble decoding and advanced model combinations). It should be noted that their approaches are orthogonal to ours, making it possible to apply our fluency boost learning and inference mechanism to their models. 8 The recently proposed SMT-NMT hybrid system (Grundkiewicz and Junczys-Dowmunt, 2018), which is tuned towards GLEU on JFLEG Dev set, report"
P18-1097,I11-1017,0,0.618333,"f (x ) {xok |xok ∈ Yn (xr ; Θcrt ) ∪ Yn (xc ; Θgen ) ∧ ≥ σ} input to generate the next output x . The prof (xok ) o cess will not terminate unless x t does not im(7) prove xot−1 in terms of fluency. Moreover, the error correction model and the error generation model are dual and both of them 5 Experiments are dynamically updated, which improves each 5.1 Dataset and evaluation other: the disfluency candidates produced by error generation model can benefit training the error As previous studies (Ji et al., 2017), we use the correction model, while the disfluency candidates public Lang-8 Corpus (Mizumoto et al., 2011; created by error correction model can be used as Tajiri et al., 2012), Cambridge Learner Cortraining data for the error generation model. We pus (CLC) (Nicholls, 2003) and NUS Corpus summarize this learning approach in Algorithm 3. of Learner English (NUCLE) (Dahlmeier et al., 2013) as our original error-corrected training data. 3.4 Fluency boost learning with large-scale Table 1 shows the stats of the datasets. In addinative data tion, we also collect 2,865,639 non-public errorOur proposed fluency boost learning strategies can corrected sentence pairs from Lang-8.com. The be easily extended"
P18-1097,N16-1133,0,0.0116492,"in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovsk"
P18-1097,han-etal-2010-using,0,0.0762136,"Missing"
P18-1097,P17-1070,0,0.569229,"st night. (b) She sees Tom caught by a policeman in the park last night. seq2seq inference She saw Tom caught by a policeman in the park last night. (c) Figure 1: (a) an error-corrected sentence pair; (b) if the sentence becomes slightly different, the model fails to correct it perfectly; (c) single-round seq2seq inference cannot perfectly correct the sentence, but multi-round inference can. Introduction Sequence-to-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) for grammatical error correction (GEC) have drawn growing attention (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Schmaltz et al., 2017; Sakaguchi et al., 2017; Chollampatt and Ng, 2018) in recent years. However, most of the seq2seq models for GEC have two flaws. First, the seq2seq models are trained with only limited error-corrected sentence pairs like Figure 1(a). Limited by the size of training data, the models with millions of parameters may not be well generalized. Thus, it is common that the models fail to correct a sentence perfectly even if the sentence is slightly different from the training instance, as illustrated by Figure 1(b). Second, the seq2seq models usually cannot perfectly correct a s"
P18-1097,W14-1703,0,0.0209873,"ke. Therefore, they can be used to establish high-quality training data with their correct sentence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation. 5.3.3 Towards the state-of-the-art for GEC Now, we a"
P18-1097,D16-1161,0,0.415141,"Ours denote the single dual-boost models in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Fost"
P18-1097,I17-1005,0,0.0311243,"nd Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Ad"
P18-1097,P15-2097,0,0.523053,"introducing the non-public training data, our single and ensemble system obtain 52.72 and 54.51 F0.5 score respectively, which is a stateof-the-art result7 on CoNLL-2014 dataset. Moreover, we evaluate our approach on JFLEG corpus (Napoles et al., 2017). JFLEG is the latest released dataset for GEC evaluation and it contains 1,501 sentences (754 in dev set and 747 in test set). To test our approach’s generalization ability, we evaluate our single models used for CoNLL evaluation (in Table 5) on JFLEG without re-tuning. Table 6 shows the JFLEG leaderboard. Instead of M2 score, JFLEG uses GLEU (Napoles et al., 2015) as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets. 6 Related work Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et"
P18-1097,D16-1228,0,0.0192413,"luency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Adapting grammatical error correction based on the native language of writers with neural network joint models. In EMNLP. Conclusion We propose a novel fluency b"
P18-1097,E17-2037,0,0.285943,"7 (AMU16 based) and NUS17. It should be noted, however, that the CAMB17 and NUS17 are actually re-rankers built on top of an SMTbased GEC system (AMU16’s framework); thus, they are ensemble models. When we build our approach on top of AMU16 (i.e., we take AMU16’s outputs as the input to our GEC system to edit on top of its outputs), we achieve 53.30 F0.5 score. With introducing the non-public training data, our single and ensemble system obtain 52.72 and 54.51 F0.5 score respectively, which is a stateof-the-art result7 on CoNLL-2014 dataset. Moreover, we evaluate our approach on JFLEG corpus (Napoles et al., 2017). JFLEG is the latest released dataset for GEC evaluation and it contains 1,501 sentences (754 in dev set and 747 in test set). To test our approach’s generalization ability, we evaluate our single models used for CoNLL evaluation (in Table 5) on JFLEG without re-tuning. Table 6 shows the JFLEG leaderboard. Instead of M2 score, JFLEG uses GLEU (Napoles et al., 2015) as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform we"
P18-1097,W14-1701,0,0.737801,"of the datasets. In addinative data tion, we also collect 2,865,639 non-public errorOur proposed fluency boost learning strategies can corrected sentence pairs from Lang-8.com. The be easily extended to utilize the huge volume of native data we use for fluency boost learning is native data which is proven to be useful for GEC. English Wikipedia that contains 61,677,453 senAs discussed in Section 3.1, when there is no tences. additional native data, S in Algorithm 1–3 is idenWe use CoNLL-2014 shared task dataset with tical to S ∗ . In the case where additional native data original annotations (Ng et al., 2014), which conis available to help model learning, S becomes: tains 1,312 sentences, as our main test set for evalS = S∗ ∪ C uation. We use MaxMatch (M2 ) precision, recall c c where C = {(x , x )} denotes the set of selfand F0.5 (Dahlmeier and Ng, 2012b) as our evaluacopied sentence pairs from native data. tion metrics. As previous studies, we use CoNLL2013 test data as our development set. 10: 11: 12: 4 Fluency boost inference As we discuss in Section 1, some sentences with multiple grammatical errors usually cannot be perfectly corrected through normal seq2seq inference 5.2 Experimental settin"
P18-1097,P02-1040,0,0.103417,"tively, which is a stateof-the-art result7 on CoNLL-2014 dataset. Moreover, we evaluate our approach on JFLEG corpus (Napoles et al., 2017). JFLEG is the latest released dataset for GEC evaluation and it contains 1,501 sentences (754 in dev set and 747 in test set). To test our approach’s generalization ability, we evaluate our single models used for CoNLL evaluation (in Table 5) on JFLEG without re-tuning. Table 6 shows the JFLEG leaderboard. Instead of M2 score, JFLEG uses GLEU (Napoles et al., 2015) as its evaluation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets. 6 Related work Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et al., 2010; Tetreault et al., 2010a; Dale and Kilgarriff, 2011) 7 The state-of-the-art result on CoNLL-2014 dataset"
P18-1097,W17-5032,0,0.151049,"GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015;"
P18-1097,P16-1112,0,0.0226045,"ockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and"
P18-1097,W14-1704,0,0.178962,"which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation. 5.3.3 Towards the state-of-the-art for GEC Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the"
P18-1097,D15-1166,0,0.0469225,"g. • Whether is fluency boost learning mechanism helpful for training the error correction model, and which of the strategies (back-boost, selfboost, dual-boost) is the most effective? • Whether does our fluency boost inference improve normal seq2seq inference for GEC? • Whether can our approach improve neural GEC to achieve state-of-the-art results? The training details for our seq2seq error correction model and error generation model are as follows: the encoder of the seq2seq models is a 2-layer bidirectional GRU RNN and the decoder is a 2-layer GRU RNN with the general attention mechanism (Luong et al., 2015). Both the dimensionality of word embeddings and the hidden size of GRU cells are 500. The vocabulary sizes of the encoder and decoder are 100,000 and 50,000 respectively. The models’ parameters are uniformly initialized in [-0.1,0.1]. We train the models with an Adam optimizer with a learning rate of 0.0001 up to 40 epochs with batch size = 128. Dropout is applied to non-recurrent connections at a ratio of 0.15. For fluency boost learning, we generate disfluency candidates from 10-best outputs. During model inference, we set beam size to 5 and decode 1-best result with a 2-layer GRU RNN langu"
P18-1097,N10-1018,0,0.0270497,"o, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tet"
P18-1097,P11-1093,0,0.0615507,"Missing"
P18-1097,P16-1208,0,0.142982,"the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation. 5.3.3 Towards the state-of-the-art for GEC Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the stateof-the-art result. We first compare our best models – dual-boost learning (+native) with fluenc"
P18-1097,W12-2032,0,0.0199865,"et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017; Chollampatt and Ng, 2018) have caught much attention. Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017). Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani e"
P18-1097,Q16-1013,0,0.0185555,". Moreover, we propose fluency boost inference which allows the model to repeatedly edit a sentence as long as the sentence’s fluency can be improved. To the best of our knowledge, it is the first to conduct multi-round seq2seq inference for GEC, while similar ideas have been proposed for NMT (Xia et al., 2017). In addition to the studies on GEC, there is also much research on grammatical error detection 1062 (Leacock et al., 2010; Rei and Yannakoudakis, 2016; Kaneko et al., 2017) and GEC evaluation (Tetreault et al., 2010b; Madnani et al., 2011; Dahlmeier and Ng, 2012c; Napoles et al., 2015; Sakaguchi et al., 2016; Napoles et al., 2016; Bryant et al., 2017; Asano et al., 2017). We do not introduce them in detail because they are not much related to this paper’s contributions. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder–decoder for statistical machine translation. In EMNLP. 7 Shamil Chollampatt, Duc Tam Hoang, and Hwee Tou Ng. 2016a. Adapting grammatical error correction based on the native language of writers with neural network joint models. In EMNLP. Conclusion We pro"
P18-1097,I17-2062,0,0.100917,"Missing"
P18-1097,D17-1298,0,0.549359,"sees Tom caught by a policeman in the park last night. seq2seq inference She saw Tom caught by a policeman in the park last night. (c) Figure 1: (a) an error-corrected sentence pair; (b) if the sentence becomes slightly different, the model fails to correct it perfectly; (c) single-round seq2seq inference cannot perfectly correct the sentence, but multi-round inference can. Introduction Sequence-to-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) for grammatical error correction (GEC) have drawn growing attention (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Schmaltz et al., 2017; Sakaguchi et al., 2017; Chollampatt and Ng, 2018) in recent years. However, most of the seq2seq models for GEC have two flaws. First, the seq2seq models are trained with only limited error-corrected sentence pairs like Figure 1(a). Limited by the size of training data, the models with millions of parameters may not be well generalized. Thus, it is common that the models fail to correct a sentence perfectly even if the sentence is slightly different from the training instance, as illustrated by Figure 1(b). Second, the seq2seq models usually cannot perfectly correct a sentence with many gramm"
P18-1097,N16-1042,0,0.564323,"s Tom caught by a policeman in the park last night. (b) She sees Tom caught by a policeman in the park last night. seq2seq inference She saw Tom caught by a policeman in the park last night. (c) Figure 1: (a) an error-corrected sentence pair; (b) if the sentence becomes slightly different, the model fails to correct it perfectly; (c) single-round seq2seq inference cannot perfectly correct the sentence, but multi-round inference can. Introduction Sequence-to-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) for grammatical error correction (GEC) have drawn growing attention (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Schmaltz et al., 2017; Sakaguchi et al., 2017; Chollampatt and Ng, 2018) in recent years. However, most of the seq2seq models for GEC have two flaws. First, the seq2seq models are trained with only limited error-corrected sentence pairs like Figure 1(a). Limited by the size of training data, the models with millions of parameters may not be well generalized. Thus, it is common that the models fail to correct a sentence perfectly even if the sentence is slightly different from the training instance, as illustrated by Figure 1(b). Second, the seq2seq models u"
P18-1097,W16-0530,0,0.298583,"r approach can generate less fluent sentences with various grammatical errors and most of them are typical mistakes that a human learner tends to make. Therefore, they can be used to establish high-quality training data with their correct sentence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a chara"
P18-1097,W13-3607,0,0.0939701,"AMB17 (CAMB16SMT based) CAMB17 (AMU16 based) NUS16 NUS17 AMU16∗ Nested-seq2seq Sakaguchi et al. (2017)∗ Ours Ours (with non-public Lang-8 data) Human JFLEG Dev GLEU 38.21 42.81 46.10 47.20 47.72 43.26 46.27 51.01 49.74 48.93 49.82 51.35 52.93 55.26 JFLEG Test GLEU 40.54 46.04 52.05 50.13 56.78 51.46 53.41 53.98 56.33 57.74 62.37 Table 6: JFLEG Leaderboard. Ours denote the single dual-boost models in Table 5. The systems with bold fonts are based on seq2seq models. ∗ denotes the system is tuned on JFLEG. or MT-based (Brockett et al., 2006; Dahlmeier and Ng, 2011, 2012a; Yoshimoto et al., 2013; Yuan and Felice, 2013; Behera and Bhattacharyya, 2013). For example, top-performing systems (Felice et al., 2014; Rozovskaya et al., 2014; JunczysDowmunt and Grundkiewicz, 2014) in CoNLL2014 shared task (Ng et al., 2014) use either of the methods. Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC. Among them, seq2seq models (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Sakaguchi"
P18-1097,P16-1009,0,0.0318795,"uring training, as Figure 2(a) illustrates, so that these pairs can further help model learning. In this section, we present three fluency boost learning strategies: back-boost, self-boost, and 2 Fluency of a sentence in this paper refers to how likely the sentence is written by a native speaker. In other words, if a sentence is very likely to be written by a native speaker, it should be regarded highly fluent. dual-boost that generate fluency boost sentence pairs in different ways, as illustrated in Figure 3. 3.1 Back-boost learning Back-boost learning borrows the idea from back translation (Sennrich et al., 2016) in NMT, referring to training a backward model (we call it error generation model, as opposed to error correction model) that is used to convert a fluent sentence to a less fluent sentence with errors. Since the less fluent sentences are generated by the error generation seq2seq model trained with error-corrected data, they usually do not change the original sentence’s meaning; thus, they can be paired with their correct sentences, establishing fluency boost sentence pairs that can be used as training instances for error correction models, as Figure 3(a) shows. Specifically, we first train a"
P18-1097,D14-1102,0,0.129719,"40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses a rule-based method to synthesize errors for data augmentation. 5.3.3 Towards the state-of-the-art for GEC Now, we answer the last question raised in Section 5.2 by testing if our approaches achieve the stateof-the-art result. We first compare our best models – dual-boost learning (+native) with fluency boost inference and shallow fusion LM – to top-performing GEC systems evaluated on CoNLL-2014 dataset: W"
P18-1097,P12-2039,0,0.452191,"erate the next output x . The prof (xok ) o cess will not terminate unless x t does not im(7) prove xot−1 in terms of fluency. Moreover, the error correction model and the error generation model are dual and both of them 5 Experiments are dynamically updated, which improves each 5.1 Dataset and evaluation other: the disfluency candidates produced by error generation model can benefit training the error As previous studies (Ji et al., 2017), we use the correction model, while the disfluency candidates public Lang-8 Corpus (Mizumoto et al., 2011; created by error correction model can be used as Tajiri et al., 2012), Cambridge Learner Cortraining data for the error generation model. We pus (CLC) (Nicholls, 2003) and NUS Corpus summarize this learning approach in Algorithm 3. of Learner English (NUCLE) (Dahlmeier et al., 2013) as our original error-corrected training data. 3.4 Fluency boost learning with large-scale Table 1 shows the stats of the datasets. In addinative data tion, we also collect 2,865,639 non-public errorOur proposed fluency boost learning strategies can corrected sentence pairs from Lang-8.com. The be easily extended to utilize the huge volume of native data we use for fluency boost lea"
P18-1097,P10-2065,0,0.224949,"luation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets. 6 Related work Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et al., 2010; Tetreault et al., 2010a; Dale and Kilgarriff, 2011) 7 The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chollampatt and Ng (2018) (F0.5 =54.79) and Grundkiewicz and Junczys-Dowmunt (2018) (F0.5 =56.25), which are contemporaneous to this paper. In contrast to the basic seq2seq model in this paper, they used advanced approaches for modeling (e.g., convolutional seq2seq with pre-trained word embedding, using edit operation features, ensemble decoding and advanced model combinations). It should be noted that their approaches are orthogonal to ours, making it possible to apply our fluency b"
P18-1097,W10-1006,0,0.11465,"luation metric, which is a fluencyoriented GEC metric based on a variant of BLEU (Papineni et al., 2002) and has several advantages over M2 for GEC evaluation. It is observed that our single models consistently perform well on JFLEG, outperforming most of the CoNLL-2014 top-performing systems and yielding a state-ofthe-art result8 on this benchmark, demonstrating that our models are well generalized and perform stably on multiple datasets. 6 Related work Most of advanced GEC systems are classifierbased (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Leacock et al., 2010; Tetreault et al., 2010a; Dale and Kilgarriff, 2011) 7 The state-of-the-art result on CoNLL-2014 dataset has been recently advanced by Chollampatt and Ng (2018) (F0.5 =54.79) and Grundkiewicz and Junczys-Dowmunt (2018) (F0.5 =56.25), which are contemporaneous to this paper. In contrast to the basic seq2seq model in this paper, they used advanced approaches for modeling (e.g., convolutional seq2seq with pre-trained word embedding, using edit operation features, ensemble decoding and advanced model combinations). It should be noted that their approaches are orthogonal to ours, making it possible to apply our fluency b"
P18-1097,D17-1297,0,0.439115,"es with various grammatical errors and most of them are typical mistakes that a human learner tends to make. Therefore, they can be used to establish high-quality training data with their correct sentence, which will be helpful for increasing the size of training data to numbers of times, accounting for the improvement by fluency boost learning. F0.5 25.25 37.33 37.33 39.90 42.44 51.08 35.01 49.49 52.21 36.79 47.40 39.39 44.27 53.14 40.56 45.15 41.37 50.04 53.30 52.72 54.51 • CAMB14, CAMB16SMT , CAMB16NMT and CAMB17: GEC systems (Felice et al., 2014; Yuan et al., 2016; Yuan and Briscoe, 2016; Yannakoudakis et al., 2017) developed by Cambridge University. • AMU14 and AMU16: SMT-based GEC systems (Junczys-Dowmunt and Grundkiewicz, 2014, 2016) developed by AMU. • CUUI and VT16: the former system (Rozovskaya et al., 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach. • NUS14, NUS16 and NUS17: GEC systems (Susanto et al., 2014; Chollampatt et al., 2016a; Chollampatt and Ng, 2017) that combine SMT with other techniques (e.g., classifiers). • Char-seq2seq: a character-level seq2seq model (Xie et al., 2016). It uses"
P18-2065,D14-1179,0,0.0380087,"Missing"
P18-2065,P16-1014,0,0.0145111,"nitial learning rate is set to 1. We trained the model for 40 epochs and started learning rate decay from the 11th epoch with a decay rate 0.7. The dropout rate is set to 0.3. We split the data into 20 partitions and used data sampling in OpenNMT to train the model. This reduces the length of the epochs for more frequent learning rate updates and validation perplexity computation. Copying Mechanism Since most encoder-decoder methods maintain a fixed vocabulary of frequent words and convert a large number of long-tail words into a special symbol “hunki”, the copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; See et al., 2017; Meng et al., 2017) is designed to copy words from the input sequence to the output sequence, thus enlarging the vocabulary and reducing the proportion of generated unknown words. For the neural Open IE task, the copying mechanism is more important because the output vocabulary is directly from the input vocabulary except for the placeholder symbols. We simplify the copying method in (See et al., 2017), the probability of generating the word yt comes from two parts as follows: ( p(yt |y1 , y2 , ..., yt−1 ; X) if yt ∈ V p(yt ) = P t otherwise i:xi =yt ai (5) where V is the ta"
P18-2065,W10-0907,0,0.035619,"n phrases. The first and second generation Open IE systems extract only relations that are mediated by verbs and ignore contexts. To alleviate these limitations, the third generation O LLIE (Mausam et al., 2012) was developed, which achieves better performance by extracting relations mediated by nouns, adjectives, and more. In addition, contextual information is also leveraged to improve the precision of extractions. All the three generations only consider binary extractions from the text, while binary extractions are not always enough for their semantics representations. Therefore, S RL I E (Christensen et al., 2010) was developed to include an attribute context with a tuple when it is available. O PENIE4 was built on S RL I E with a rule-based extraction system R EL N OUN (Pal and Mausam, 2016) for extracting noun-mediated relations. Recently, O PENIE5 improved upon extractions from numerical sentences (Saha et al., 2017) and broke conjunctions in arguments to generate multiple extractions. During this period, there were also some other Open IE systems emerged and successfully applied in different scenarios, such as ClausIE (Del Corro and Gemulla, 2013) Stanford O PENIE (Angeli et al., 2015), PropS (Stan"
P18-2065,P17-4012,0,0.0385064,"the-art baselines including O LLIE, ClausIE, Stanford O PENIE, PropS and O PENIE4. The evaluation metrics are precision and recall. αij hj j=1 exp(eij ) αij = Pn k=1 exp(eik ) (4) eij = a(si−1 , hj ) where a is an alignment model that scores how well the inputs around position j and the output at position i match, which is measured by the encoder hidden state hj and the decoder hidden state si−1 . The encoder and decoder are jointly optimized to maximize the log probability of the output sequence conditioned on the input sequence. 2.3 3.2 We implemented the neural Open IE model using OpenNMT (Klein et al., 2017), which is an open source encoder-decoder framework. We used 4 M60 GPUs for parallel training, which takes 3 days. The encoder is a 3-layer bidirectional LSTM and the decoder is another 3-layer LSTM. Our model has 256-dimensional hidden states and 256-dimensional word embeddings. A vocabulary of 50k words is used for both the source and target sides. We optimized the model with SGD and the initial learning rate is set to 1. We trained the model for 40 epochs and started learning rate decay from the 11th epoch with a decay rate 0.7. The dropout rate is set to 0.3. We split the data into 20 part"
P18-2065,D15-1166,0,0.150582,"Missing"
P18-2065,D11-1142,0,0.950754,"a. For instance, given the sentence “deep learning is a subfield of machine learning”, the triple (deep learning; is a subfield of ; machine learning) can be extracted, where the relation phrase “is a subfield of ” indicates the semantic relationship between two arguments. Open IE plays a key role in natural language understanding and fosters many downstream NLP applications such as knowledge base construction, question answering, text comprehension, and others. The Open IE system was first introduced by T EXT RUNNER (Banko et al., 2007), followed by several popular systems such as R E V ERB (Fader et al., 2011), O LLIE (Mausam 1 2 https://github.com/allenai/openie-standalone https://github.com/dair-iitd/OpenIE-standalone 407 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 407–413 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics encoder decoder ?1 copying ?2 … ?? + 3-layer LSTM ?1 ?2 … ?? embedding attention ? ?1 ?2 … Figure 1: The encoder-decoder model architecture for the neural Open IE system learning h/arg2i”. We obtain the input and output sequence pairs from highly confident tuples bootstrapp"
P18-2065,D12-1048,0,0.755908,"r knowledge, this is the first time that the Open IE task is addressed using an end-to-end neural approach, bypassing the handcrafted patterns and alleviating error propagation. precision-recall curve. Following these efforts, the second generation known as R2A2 (Etzioni et al., 2011) was developed based on R E V ERB and an argument identifier, A RG L EARNER, to better extract the arguments for the relation phrases. The first and second generation Open IE systems extract only relations that are mediated by verbs and ignore contexts. To alleviate these limitations, the third generation O LLIE (Mausam et al., 2012) was developed, which achieves better performance by extracting relations mediated by nouns, adjectives, and more. In addition, contextual information is also leveraged to improve the precision of extractions. All the three generations only consider binary extractions from the text, while binary extractions are not always enough for their semantics representations. Therefore, S RL I E (Christensen et al., 2010) was developed to include an attribute context with a tuple when it is available. O PENIE4 was built on S RL I E with a rule-based extraction system R EL N OUN (Pal and Mausam, 2016) for"
P18-2065,D17-1278,0,0.446789,"Missing"
P18-2065,P17-1054,0,0.0620639,"ial to solve the problems of cascading errors to alleviate extracting incorrect tuples. To this end, we propose a neural Open IE approach with an encoder-decoder framework. The encoder-decoder framework is a text generation technique and has been successfully applied to many tasks, such as machine translation (Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), image caption (Vinyals et al., 2014), abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) and recently keyphrase extraction (Meng et al., 2017). Generally, the encoder encodes the input sequence to an internal representation called ‘context vector’ which is used by the decoder to generate the output sequence. The lengths of input and output sequences can be different, as there is no one on one relation between the input and output sequences. In this work, Open IE is cast as a sequence-to-sequence generation problem, where the input sequence is the sentence and the output sequence is the tuples with special placeholders. For instance, given the input sequence “deep learning is a subfield of machine learning”, the output sequence will"
P18-2065,W16-1307,0,0.153303,"LIE (Mausam et al., 2012) was developed, which achieves better performance by extracting relations mediated by nouns, adjectives, and more. In addition, contextual information is also leveraged to improve the precision of extractions. All the three generations only consider binary extractions from the text, while binary extractions are not always enough for their semantics representations. Therefore, S RL I E (Christensen et al., 2010) was developed to include an attribute context with a tuple when it is available. O PENIE4 was built on S RL I E with a rule-based extraction system R EL N OUN (Pal and Mausam, 2016) for extracting noun-mediated relations. Recently, O PENIE5 improved upon extractions from numerical sentences (Saha et al., 2017) and broke conjunctions in arguments to generate multiple extractions. During this period, there were also some other Open IE systems emerged and successfully applied in different scenarios, such as ClausIE (Del Corro and Gemulla, 2013) Stanford O PENIE (Angeli et al., 2015), PropS (Stanovsky et al., 2016), and more. 5 Conclusion and Future Work We proposed a neural Open IE approach using an encoder-decoder framework. The neural Open IE model is trained with highly"
P18-2065,P16-1154,0,0.0231749,"ith SGD and the initial learning rate is set to 1. We trained the model for 40 epochs and started learning rate decay from the 11th epoch with a decay rate 0.7. The dropout rate is set to 0.3. We split the data into 20 partitions and used data sampling in OpenNMT to train the model. This reduces the length of the epochs for more frequent learning rate updates and validation perplexity computation. Copying Mechanism Since most encoder-decoder methods maintain a fixed vocabulary of frequent words and convert a large number of long-tail words into a special symbol “hunki”, the copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; See et al., 2017; Meng et al., 2017) is designed to copy words from the input sequence to the output sequence, thus enlarging the vocabulary and reducing the proportion of generated unknown words. For the neural Open IE task, the copying mechanism is more important because the output vocabulary is directly from the input vocabulary except for the placeholder symbols. We simplify the copying method in (See et al., 2017), the probability of generating the word yt comes from two parts as follows: ( p(yt |y1 , y2 , ..., yt−1 ; X) if yt ∈ V p(yt ) = P t otherwise i:xi =yt a"
P18-2065,D15-1044,0,0.0472825,"(Banko et al., 2007; Gashteovski et al., 2017; Schneider et al., 2017). Therefore, it is essential to solve the problems of cascading errors to alleviate extracting incorrect tuples. To this end, we propose a neural Open IE approach with an encoder-decoder framework. The encoder-decoder framework is a text generation technique and has been successfully applied to many tasks, such as machine translation (Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), image caption (Vinyals et al., 2014), abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) and recently keyphrase extraction (Meng et al., 2017). Generally, the encoder encodes the input sequence to an internal representation called ‘context vector’ which is used by the decoder to generate the output sequence. The lengths of input and output sequences can be different, as there is no one on one relation between the input and output sequences. In this work, Open IE is cast as a sequence-to-sequence generation problem, where the input sequence is the sentence and the output sequence is the tuples with special placeholders. For instance, give"
P18-2065,P17-2050,0,0.167661,"more. In addition, contextual information is also leveraged to improve the precision of extractions. All the three generations only consider binary extractions from the text, while binary extractions are not always enough for their semantics representations. Therefore, S RL I E (Christensen et al., 2010) was developed to include an attribute context with a tuple when it is available. O PENIE4 was built on S RL I E with a rule-based extraction system R EL N OUN (Pal and Mausam, 2016) for extracting noun-mediated relations. Recently, O PENIE5 improved upon extractions from numerical sentences (Saha et al., 2017) and broke conjunctions in arguments to generate multiple extractions. During this period, there were also some other Open IE systems emerged and successfully applied in different scenarios, such as ClausIE (Del Corro and Gemulla, 2013) Stanford O PENIE (Angeli et al., 2015), PropS (Stanovsky et al., 2016), and more. 5 Conclusion and Future Work We proposed a neural Open IE approach using an encoder-decoder framework. The neural Open IE model is trained with highly confident binary extractions bootstrapped from a state-of-the-art Open IE system, therefore it can generate highquality tuples wit"
P18-2065,W17-5402,0,0.116001,"Missing"
P18-2065,P17-1099,0,0.0534503,"017; Schneider et al., 2017). Therefore, it is essential to solve the problems of cascading errors to alleviate extracting incorrect tuples. To this end, we propose a neural Open IE approach with an encoder-decoder framework. The encoder-decoder framework is a text generation technique and has been successfully applied to many tasks, such as machine translation (Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), image caption (Vinyals et al., 2014), abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) and recently keyphrase extraction (Meng et al., 2017). Generally, the encoder encodes the input sequence to an internal representation called ‘context vector’ which is used by the decoder to generate the output sequence. The lengths of input and output sequences can be different, as there is no one on one relation between the input and output sequences. In this work, Open IE is cast as a sequence-to-sequence generation problem, where the input sequence is the sentence and the output sequence is the tuples with special placeholders. For instance, given the input sequence “deep learning is a su"
P18-2065,D16-1252,0,0.365457,"g n-ary extractions and 408 Open IE system. Both the arguments and relations are sub-spans that correspond to the input sequence. We leverage the attention method proposed by Bahdanau et al. to calculate the context vector c as follows: ci = n X with binary relations. To further obtain highquality tuples, we only kept the tuples whose confidence score is at least 0.9. Finally, there are a total of 36,247,584 hsentence, tuplei pairs extracted. The training data is released for public use at https://1drv.ms/u/s!ApPZx_ TWwibImHl49ZBwxOU0ktHv. For the test data, we used a large benchmark dataset (Stanovsky and Dagan, 2016) that contains 3,200 sentences with 10,359 extractions4 . We compared with several state-of-the-art baselines including O LLIE, ClausIE, Stanford O PENIE, PropS and O PENIE4. The evaluation metrics are precision and recall. αij hj j=1 exp(eij ) αij = Pn k=1 exp(eik ) (4) eij = a(si−1 , hj ) where a is an alignment model that scores how well the inputs around position j and the output at position i match, which is measured by the encoder hidden state hj and the decoder hidden state si−1 . The encoder and decoder are jointly optimized to maximize the log probability of the output sequence condit"
P19-1328,S07-1091,0,0.202177,"that not only are semantically consistent with the original target word and fits in the ∗ This work was done during the first author’s internship at Microsoft Research Asia. context but also preserve the sentence’s meaning. Most previous approaches to this challenge first obtain substitute candidates by picking synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy"
P19-1328,D16-1215,0,0.0947329,"a. context but also preserve the sentence’s meaning. Most previous approaches to this challenge first obtain substitute candidates by picking synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; a"
P19-1328,D10-1113,0,0.0318685,"uthor’s internship at Microsoft Research Asia. context but also preserve the sentence’s meaning. Most previous approaches to this challenge first obtain substitute candidates by picking synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of wor"
P19-1328,D08-1094,0,0.135637,"Missing"
P19-1328,P16-1012,0,0.161898,"ontext; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; and supervised learning approaches (Biemann, 2013; Szarvas et al., 2013a,b; Hintz and Biemann, 2016) uses delexicalized features to rank substitute candidates. Although these approaches work well in some cases, they have two key limitations: (1) they rely heavily on lexical resources. While the resources can offer synonyms for substitution, they are not perfect and they are likely to overlook some good candidates, as Figure 1(a) shows. (2) most previous approaches only measure the substitution candidates’ fitness given the context but they do not consider whether the substitution changes the sentence’s meaning. Take Figure 1(b) as an example, although tough may fit in the context as well as"
P19-1328,E14-1057,0,0.0765349,"Missing"
P19-1328,S07-1009,0,0.439037,"target word’s embedding for partially masking the word, allowing BERT to take balanced consideration of the target word’s semantics and contexts for proposing substitute candidates, and then validates the candidates based on their substitution’s influence on the global contextualized representation of the sentence. Experiments show our approach performs well in both proposing and ranking substitute candidates, achieving the state-of-the-art results in both LS07 and LS14 benchmarks. 1 The wine he sent to me as my birthday gift is too strong to drink. WordNet Introduction Lexical substitution (McCarthy and Navigli, 2007) aims to replace a target word in a sentence with a substitute word without changing the meaning of the sentence, which is useful for many Natural Language Processing (NLP) tasks like text simplification and paraphrase generation. One main challenge in this task is proposing substitutes that not only are semantically consistent with the original target word and fits in the ∗ This work was done during the first author’s internship at Microsoft Research Asia. context but also preserve the sentence’s meaning. Most previous approaches to this challenge first obtain substitute candidates by picking"
P19-1328,N15-1050,0,0.240569,"synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; and supervised learning approaches (Biemann, 2013; Szarvas et al., 2013a,b; Hintz and Biemann, 2016) uses delexicalized features to rank substitu"
P19-1328,K16-1006,0,0.142733,"oach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; and supervised learning approaches (Biemann, 2013; Szarvas et al., 2013a,b; Hintz and Biemann, 2016) uses delexicalized features to rank substitute candidates. Although these approaches work well in some cases, they have two key limitations: (1) they rely heavily on lexical resources. While the resources can offer synonyms for substitution, they are not perfect and they are likely to overlook some good candidates, as Figure 1(a) shows. (2) most previous approaches only measure the substitution candidates’ fitness given the conte"
P19-1328,W15-1501,0,0.395774,"synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; and supervised learning approaches (Biemann, 2013; Szarvas et al., 2013a,b; Hintz and Biemann, 2016) uses delexicalized features to rank substitu"
P19-1328,N16-1131,0,0.0480592,"Missing"
P19-1328,N13-1133,0,0.175153,"based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–3373 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2016; Melamud et al., 2016) uses the similarity of word embeddings to rank substitute words; and supervised learning approaches (Biemann, 2013; Szarvas et al., 2013a,b; Hintz and Biemann, 2016) uses delexicalized features to rank substitute candidates. Although these approaches work well in some cases, they have two key limitations: (1) they rely heavily on lexical resources. While the resources can offer synonyms for substitution, they are not perfect and they are likely to overlook some good candidates, as Figure 1(a) shows. (2) most previous approaches only measure the substitution candidates’ fitness given the context but they do not consider whether the substitution changes the sentence’s meaning. Take Figure 1(b) as an example, although tough may f"
P19-1328,D13-1198,0,0.0475952,"Missing"
P19-1328,P10-1097,0,0.0821976,"Missing"
P19-1328,S07-1044,0,0.354603,"g substitutes that not only are semantically consistent with the original target word and fits in the ∗ This work was done during the first author’s internship at Microsoft Research Asia. context but also preserve the sentence’s meaning. Most previous approaches to this challenge first obtain substitute candidates by picking synonyms from manually curated lexical resources as candidates, and then rank them based on their appropriateness in context, or instead ranking all words in the vocabulary to avoid the usage of lexical resources. For example, knowledge-based lexical substitution systems (Yuret, 2007; Hassan et al., 2007) use pre-defined rules to score substitute candidates; vector space modeling approach (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2010; Apidianaki, 2016) uses distributional sparse vector representations based on the syntactic context; substitute vector approach (Yuret, 2012; Melamud et al., 2015b) comprises the potential fillers for the target word slot in that context; word/context embedding similarity approach (Melamud et al., 2015a; Roller and Erk, 3368 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3368–"
P19-1366,J05-3002,0,0.053984,"Missing"
P19-1366,C10-3004,1,0.580884,"nt retrieved is always the one whose message is exactly the training message and responses contain the ground-truth response. We thus remove the document from the retrieved result before re-ranking to make sure that the N-best response candidates are different from the groundtruth response. 4 Experiments 4.1 Data We use the NTCIR corpus4 in our experiments. Its data are collected from a Chinese microblogging service, Sina Weibo5 , where users can both post messages and make comments (responses) on other users’ messages. First, we tokenize each utterance using the Language Technology Platform (Che et al., 2010) and remove samples whose responses are shorter than 5, which is helpful in relieving the generic response problem (Li et al., 2017). Then, we randomly select 10,000 messages associated with responses to form a validation set and another 10,000 messages with responses as a test set. Table 2 shows some statistics of the datasets. 4.2 Baselines Rtr: The retrieval-based method searches the index for response candidates and subsequently re4 5 http://research.nii.ac.jp/ntcir/data/data-en.html https://weibo.com turns the one that best matches the message after re-ranking (see Sec. 3.4 for details)."
P19-1366,P18-1139,0,0.0947042,"roach is 500. The batch size is set to 64. The discriminator and the generator are trained alternately, where the discriminator is optimized for 10 batches, then switch to the generator for 20 batches. We use ADAM optimizer whose learning rate is initialized to 0.0001. In the inference process, we generate responses using beam search with beam size set to 5. 5 Results 5.1 Evaluation Metrics Human Evaluation We randomly sampled 200 messages from the test set to conduct the human evaluation as it is extremely time-consuming. Five annotators8 are recruited to judge a response from three aspects (Ke et al., 2018): 6 https://github.com/MarkWuNLP/ResponseEdit https://code.google.com/archive/p/word2vec/ 8 All annotators are well-educated students and have Bachelor or higher degree. 3768 7 Mean Rtr S2S MS2S Edit AL Ours 0.63 0.76 0.85 0.85 0.98 1.10 Appropriateness +2 +1 0 24.8 27.9 31.9 31.4 36.8 41.5 12.9 20.0 21.5 21.9 24.0 26.8 62.3 52.1 46.6 46.7 39.2 31.7  Mean 0.71 0.58 0.63 0.66 0.57 0.65 0.92 0.51 0.62 0.67 0.77 0.88 Informativeness +2 +1 0 41.1 10.2 14.1 15.9 21.8 31.2 10.1 30.5 33.8 34.9 33.6 25.9 48.8 59.3 52.1 49.2 44.6 42.9  Mean 0.67 0.69 0.73 0.68 0.66 0.72 1.93 1.74 1.74 1.92 1.88 1.87"
P19-1366,P17-4012,0,0.0344183,"tor representing lexical differences between retrieved contexts and the message. AL: The adversarial learning for neural response generation (Li et al., 2017) is also an adversarial method but is not retrieval-enhanced. Here, we do not employ the REGS (reward for every generation step) setting as the Monte-Carlo roll-out is quite time-consuming and the accuracy of the discriminator trained on partially decoded sequences is not as good as that on complete sequences. 4.3 Experiment Settings We use the published code6 for Edit and implement other approaches by an open source framework: Open-NMT (Klein et al., 2017). The vocabulary table consists of the most frequent 30,000 words, whose 300-dimensional word embeddings are pre-trained on the training set by Word2Vec 7 . The number of hidden units for all LSTM in our approach is 500. The batch size is set to 64. The discriminator and the generator are trained alternately, where the discriminator is optimized for 10 batches, then switch to the generator for 20 batches. We use ADAM optimizer whose learning rate is initialized to 0.0001. In the inference process, we generate responses using beam search with beam size set to 5. 5 Results 5.1 Evaluation Metrics"
P19-1366,W06-1303,0,0.542045,"systems, personal assistants, and chatbots. Early dialogue systems are often built using the rule-based method (Weizenbaum, 1966) or template-based method (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007), which are usually labor-intensive and difficult to scale up. Recently, with the rise of social networking, conversational data have accumulated to a considerable scale. This promoted the development of data-driven methods, including retrieval-based methods (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Wen et al., 2017) and generation-based methods (Leuski et al., 2006; Ji et al., 2014; Yan et al., 2016). Retrieval-based methods reply to users by searching and re-ranking response candidates ⇤ Corresponding author. from a pre-constructed response set. Written mainly by humans, these responses are always diverse and informative, but may be inappropriate to input messages due to their being prepared in advance and thus incapable of being customized (Shang et al., 2015). In contrast, generation-based methods can produce responses tailored to the messages. The most common method of this category in recent years is the sequence to sequence (Seq2Seq) model (Sutske"
P19-1366,N16-1014,0,0.387164,"set. Written mainly by humans, these responses are always diverse and informative, but may be inappropriate to input messages due to their being prepared in advance and thus incapable of being customized (Shang et al., 2015). In contrast, generation-based methods can produce responses tailored to the messages. The most common method of this category in recent years is the sequence to sequence (Seq2Seq) model (Sutskever et al., 2014; Shang et al., 2015; Vinyals and Le, 2015). In practice, it usually suffers from the problem of generating generic responses, such as “I don’t know” and “Me, too” (Li et al., 2016a; Serban et al., 2016). While the contents of retrieved responses, apart from the irrelevant parts, are of great diversity, making it a potential resource for tailoring appropriate and informative responses. Therefore, it is natural to enhance the response generation approach with retrieved responses. Previous work has been proposed to extend the input of a Seq2Seq model with N-best response candidates (or their contexts) (Song et al., 2018; Pandey et al., 2018). On one hand, these approaches are trained using MLE objective, which correlates weakly with true quality of responses thus limits t"
P19-1366,D16-1127,0,0.613432,"set. Written mainly by humans, these responses are always diverse and informative, but may be inappropriate to input messages due to their being prepared in advance and thus incapable of being customized (Shang et al., 2015). In contrast, generation-based methods can produce responses tailored to the messages. The most common method of this category in recent years is the sequence to sequence (Seq2Seq) model (Sutskever et al., 2014; Shang et al., 2015; Vinyals and Le, 2015). In practice, it usually suffers from the problem of generating generic responses, such as “I don’t know” and “Me, too” (Li et al., 2016a; Serban et al., 2016). While the contents of retrieved responses, apart from the irrelevant parts, are of great diversity, making it a potential resource for tailoring appropriate and informative responses. Therefore, it is natural to enhance the response generation approach with retrieved responses. Previous work has been proposed to extend the input of a Seq2Seq model with N-best response candidates (or their contexts) (Song et al., 2018; Pandey et al., 2018). On one hand, these approaches are trained using MLE objective, which correlates weakly with true quality of responses thus limits t"
P19-1366,D17-1230,0,0.430029,"sponses, such as keywords (Mou et al., 2016; Serban et al., 2017a), topic information (Xing et al., 2017) and knowledge triples (Zhou et al., 2018). Another way is to improve the Seq2Seq architecture. Li et al.(2016b) introduce the Maximum Mutual Information as the objective function. Serban et al.(2017b) add a latent variable to inject variability. The training of Seq2Seq can be formulated as a reinforcement learning problem (Li et al., 2016b; Zhang et al., 2017). To avoid manually defining reward functions, a discriminator can be introduced and trained synchronously by adversarial learning (Li et al., 2017). After that, Xu et al. (2018) propose a language model based discriminator to better distinguish novel responses from repeated responses. In a similar adversarial setting, Zhang et al. (2018) optimize a Variational Information Maximization Objective to improve informativeness. Our approach is also an adversarial model, the difference is that we employ the N-best response candidates to enhance the generation. Taking advantages of the two methods, retrieval-enhanced response generation approaches make use of the informative content in retrieved results to generate new responses. Typically, gene"
P19-1366,W00-0304,0,0.00841286,"y favorite dessert. It’s so delicious. Table 1: An example of a message (MSG), a groundtruth response (GT), a generated response (RSP) and N-best response candidates (C#1 and C#2) during the training process. Similar contents in the response and candidates are in boldface. Introduction Dialogue systems intend to converse with humans with a coherent structure. They have been widely used in real-world applications, including customer service systems, personal assistants, and chatbots. Early dialogue systems are often built using the rule-based method (Weizenbaum, 1966) or template-based method (Litman et al., 2000; Schatzmann et al., 2006; Williams and Young, 2007), which are usually labor-intensive and difficult to scale up. Recently, with the rise of social networking, conversational data have accumulated to a considerable scale. This promoted the development of data-driven methods, including retrieval-based methods (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Wen et al., 2017) and generation-based methods (Leuski et al., 2006; Ji et al., 2014; Yan et al., 2016). Retrieval-based methods reply to users by searching and re-ranking response candidates ⇤ Corresponding author. from a p"
P19-1366,W05-1612,0,0.0246684,". (2018) optimize a Variational Information Maximization Objective to improve informativeness. Our approach is also an adversarial model, the difference is that we employ the N-best response candidates to enhance the generation. Taking advantages of the two methods, retrieval-enhanced response generation approaches make use of the informative content in retrieved results to generate new responses. Typically, generating responses from retrieved candidates can be seen as a text-to-text system, which produces meaningful text from meaningful text rather than from abstract meaning representations (Marsi and Krahmer, 2005). Barzilay 3764 Retrieval-based Method Policy Gradient Generator Prob ( human-generated ) Response How did you make it? It looks delicious. Encoder Training Set MLP Message: I made strawberry shortcake. Candidate LSTM Candidate#1 Could you tell me how this thing is cooked? Candidate#2Tiramisu is my favorite dessert. It’s so delicious. … N-best Response Candidates Zx Concatenation Response LSTM Message LSTM Index Decoder Average 1 Zc 2 Zc … Discriminator Figure 1: An overview of our proposed approach. The discriminator is enhanced by the N-best response candidates returned by a retrieval-"
P19-1366,C16-1316,0,0.129093,"information retrieval techniques to rank response candidates. In addition, the matching and ranking methods can also be implemented using neural networks (Yan et al., 2016; Qiu et al., 2017; Wu et al., 2017). Based on that, Yang et al. (2018) propose a deep matching network which could model external knowledge. Generation-based methods can be cast as a sequence to sequence (Seq2Seq) process (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015) but suffers from generating generic responses. One way to address the problem is to introduce new content into responses, such as keywords (Mou et al., 2016; Serban et al., 2017a), topic information (Xing et al., 2017) and knowledge triples (Zhou et al., 2018). Another way is to improve the Seq2Seq architecture. Li et al.(2016b) introduce the Maximum Mutual Information as the objective function. Serban et al.(2017b) add a latent variable to inject variability. The training of Seq2Seq can be formulated as a reinforcement learning problem (Li et al., 2016b; Zhang et al., 2017). To avoid manually defining reward functions, a discriminator can be introduced and trained synchronously by adversarial learning (Li et al., 2017). After that, Xu et al. (20"
P19-1366,P18-1123,0,0.0372555,"ls and Le, 2015). In practice, it usually suffers from the problem of generating generic responses, such as “I don’t know” and “Me, too” (Li et al., 2016a; Serban et al., 2016). While the contents of retrieved responses, apart from the irrelevant parts, are of great diversity, making it a potential resource for tailoring appropriate and informative responses. Therefore, it is natural to enhance the response generation approach with retrieved responses. Previous work has been proposed to extend the input of a Seq2Seq model with N-best response candidates (or their contexts) (Song et al., 2018; Pandey et al., 2018). On one hand, these approaches are trained using MLE objective, which correlates weakly with true quality of responses thus limits the effectiveness of the candidates in producing the responses. Table 1 shows an exam3763 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3763–3773 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ple during the training process. Related contents of the candidates are appropriately integrated into the response, but the model is discouraged as the response is different from th"
P19-1366,P17-2079,0,0.013461,"aselines in both automatic and human evaluations. 2 Related Work Data-driven dialogue systems can be roughly divided into two categories: retrieval-based and generation-based. Retrieval-based methods respond to users by selecting the response that best matches an input message from a pre-constructed response set. Leuski et al. (2006) match a response with a message using a statistical language model. Ji et al.(2014) employ information retrieval techniques to rank response candidates. In addition, the matching and ranking methods can also be implemented using neural networks (Yan et al., 2016; Qiu et al., 2017; Wu et al., 2017). Based on that, Yang et al. (2018) propose a deep matching network which could model external knowledge. Generation-based methods can be cast as a sequence to sequence (Seq2Seq) process (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015) but suffers from generating generic responses. One way to address the problem is to introduce new content into responses, such as keywords (Mou et al., 2016; Serban et al., 2017a), topic information (Xing et al., 2017) and knowledge triples (Zhou et al., 2018). Another way is to improve the Seq2Seq architecture. Li et al.(2016b)"
P19-1366,P15-1152,0,0.0615908,"Missing"
P19-1366,N15-1020,0,0.0779876,"Missing"
P19-1366,E17-1042,0,0.0278622,"Missing"
P19-1366,D18-1428,0,0.0597753,"et al., 2016; Serban et al., 2017a), topic information (Xing et al., 2017) and knowledge triples (Zhou et al., 2018). Another way is to improve the Seq2Seq architecture. Li et al.(2016b) introduce the Maximum Mutual Information as the objective function. Serban et al.(2017b) add a latent variable to inject variability. The training of Seq2Seq can be formulated as a reinforcement learning problem (Li et al., 2016b; Zhang et al., 2017). To avoid manually defining reward functions, a discriminator can be introduced and trained synchronously by adversarial learning (Li et al., 2017). After that, Xu et al. (2018) propose a language model based discriminator to better distinguish novel responses from repeated responses. In a similar adversarial setting, Zhang et al. (2018) optimize a Variational Information Maximization Objective to improve informativeness. Our approach is also an adversarial model, the difference is that we employ the N-best response candidates to enhance the generation. Taking advantages of the two methods, retrieval-enhanced response generation approaches make use of the informative content in retrieved results to generate new responses. Typically, generating responses from retrieve"
P19-1415,D14-1179,0,0.0545077,"Missing"
P19-1415,P18-1078,0,0.0736195,"eparate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for humans to the paragraph. Clark and Gardner (2018) and Tan et al. (2018) use questions to retrieve paragraphs that do not contain the answer as adversarial examples. Rajpurkar et al. (2018) create unanswerable questions through rigid rules, which swap entities, numbers and antonyms of answerable questions. It has been shown that adversarial examples generated by rule-based systems are much easier to detect than ones in the SQuAD 2.0 dataset. Data Augmentation for MRC Several attempts have been made to augment training data for machine reading comprehension. We catego4239 SEQ2SEQ tween inputs. The decoder of two models generates unanswerable q"
P19-1415,N19-1423,0,0.183228,") = Interac+on |˜ q| Y P (˜ qt |˜ q&lt;t , q, p, a) (1) t=1 Encoder W W W C … C C P A P W Encoder W W W W C … C C … C C C … C C C Q P A P Q Q Q Paragraph + Ques+on W Encoder P Paragraph W P W W W … where q˜&lt;t = q˜1 . . . q˜t−1 . W C C Q Q 3.1 Ques+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and it"
P19-1415,P18-1177,0,0.049005,"Missing"
P19-1415,P17-1123,0,0.0430977,"l., 2018; Yu et al., 2018; Wang et al., 2018) have outperformed humans on this task in terms of automatic metrics. The SQuAD 2.0 dataset (Rajpurkar et al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. I"
P19-1415,P16-1154,0,0.0669924,"Missing"
P19-1415,N10-1086,0,0.0764351,"t al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable quest"
P19-1415,P82-1020,0,0.820425,"Missing"
P19-1415,D17-1215,0,0.184369,"xtra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for humans to the paragraph. Clark and Gardner (2018) and Tan et al. (2018) use questions to retrieve paragraphs that do not contain the answer as adversarial examples. Rajpurkar et al. (2018) create unanswerable questions through rigid rules, which swap entities, numbers and antonyms of answerable questions. It has been shown that adversarial examples generated by rule-based systems are much easier to detect than ones in the SQuAD 2.0 dataset. Data Augmentation for MRC Several attempts hav"
P19-1415,P17-1147,0,0.0420112,"W W W C … C C … C C C … C C C Q P A P Q Q Q Paragraph + Ques+on W Encoder P Paragraph W P W W W … where q˜&lt;t = q˜1 . . . q˜t−1 . W C C Q Q 3.1 Ques+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and its corresponding paragraph p that contains the answer a, we aim to generate unanswerable questio"
P19-1415,P17-4012,0,0.0800098,"Missing"
P19-1415,Q18-1023,0,0.0474809,"Missing"
P19-1415,K17-1034,0,0.0257042,"ast example shows that inserting negation words in different positions (“n’t public” versus “not in victoria”) can express different meanings. Such cases are critical for generated questions’ answerability, which is hard to handle in a rule-based system. 4.2 4.2.1 Data Augmentation for Machine Reading Comprehension Question Answering Models We apply our automatically generated unanswerable questions as augmentation data to the following reading comprehension models: BiDAF-No-Answer (BNA) BiDAF (Seo et al., 2017) is a benchmark model on extractive machine reading comprehension. Based on BiDAF, Levy et al. (2017) propose the BiDAF-No-Answer model to predict the distribution of answer candidates and the probability of a question being unanswerable at the same time. DocQA Clark and Gardner (2018) propose the DocQA model to address document-level reading comprehension. The no-answer probability is also predicted jointly. BERT Fine-Tuning It is the state-of-the-art model on unanswerable machine reading comprehension. We adopt the uncased version of BERT (Devlin et al., 2019) for fine-tuning. The batch sizes of BERT-base and BERT-large are set to 12 and 24 respectively. The rest hyperparameters are kept un"
P19-1415,W04-1013,0,0.0112197,"d embeddings. The hidden state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation task. We also conduc"
P19-1415,P18-1157,0,0.0277041,"Missing"
P19-1415,D15-1166,0,0.0405388,"rent neural networks with long short-term memory units (LSTM; Hochreiter and Schmidhuber, 1997) to produce encoder hidden states hi = fBiLSTM (hi−1 , ei ). On each decoding step t, the hidden states of decoder (a single-layer unidirectional LSTM network) are computed by st = fLSTM (st−1 , [yt−1 ; ct−1 ]), where yt−1 is the word embedding of previously predicted token and ct−1 is the encoder context vector of previous step. Besides, we use an attention mechanism to summarize the encoder-side information into ct for current step. The attention distribution γt over source words is computed as in Luong et al. (2015): score(hi , st ) = hT i Wγ s t γi,t = exp(score(hi , st ))/Zt ct = |x| X γi,t hi (2) (3) (4) i P|x| where Zt = k exp(score(hk , st )), Wγ in score function is a learnable parameter. Next, st is concatenated with ct to produce the vocabulary distribution Pv : 4240 Pv = softmax(Wv [st ; ct ] + bv ) (5) where Wv and bv are learnable parameters. Copy mechanism (See et al., 2017) is incorporated to directly copy words from inputs, because words in paragraphs or source questions are of great value for unanswerable question generation. Specifically, we use st and ct to produce a gating probability g"
P19-1415,P15-2097,0,0.0231868,"den state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation task. We also conduct human evaluation on 100 sample"
P19-1415,P02-1040,0,0.104033,"are the same vocabulary and word embeddings. The hidden state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation t"
P19-1415,D14-1162,0,0.0826805,"Missing"
P19-1415,P18-2124,0,0.555636,"aset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during internship at Microsoft Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve question answering models. The generated unanswerable questions should not be too easy for the question answering model so that data augmentation can better help the model. For example, a simple baseline method is randomly choosing a question asked for an"
P19-1415,D16-1264,0,0.0603667,"ion on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model. 1 Ans. Question: What organization runs the public schools in Victoria? UnAns. Question: What organization runs the waste management in Victoria? (Plausible) Answer: Victoria Department of Education Figure 1: An example taken from the SQuAD 2.0 dataset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during int"
P19-1415,P17-1099,0,0.0281374,"s the encoder context vector of previous step. Besides, we use an attention mechanism to summarize the encoder-side information into ct for current step. The attention distribution γt over source words is computed as in Luong et al. (2015): score(hi , st ) = hT i Wγ s t γi,t = exp(score(hi , st ))/Zt ct = |x| X γi,t hi (2) (3) (4) i P|x| where Zt = k exp(score(hk , st )), Wγ in score function is a learnable parameter. Next, st is concatenated with ct to produce the vocabulary distribution Pv : 4240 Pv = softmax(Wv [st ; ct ] + bv ) (5) where Wv and bv are learnable parameters. Copy mechanism (See et al., 2017) is incorporated to directly copy words from inputs, because words in paragraphs or source questions are of great value for unanswerable question generation. Specifically, we use st and ct to produce a gating probability gt : gt = sigmoid(Wg [st ; ct ] + bg ) (6) where Wg and bg are learnable parameters. The gate gt determines whether generating a word from the vocabulary or copying a word from inputs. Finally, we obtain the probability of generating q˜t by: X P (˜ qt |˜ q&lt;t , q, p, a) = gt Pv (˜ qt ) + (1 − gt ) γˆi,t i∈ζq˜t where ζq˜t denotes all the occurrence of q˜t in inputs, and the copy"
P19-1415,N18-2090,0,0.0319965,"document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for h"
P19-1415,N19-1270,0,0.0353075,"Missing"
P19-1415,D18-1427,0,0.0242532,"answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do"
P19-1415,P18-1158,0,0.0133866,"ith BERT-large model. 2 Related Work Machine Reading Comprehension (MRC) Various large-scale datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2018; Kocisky et al., 2018) have spurred rapid progress on machine reading comprehension in recent years. SQuAD (Rajpurkar et al., 2016) is an extractive benchmark whose questions and answers spans are annotated by humans. Neural reading comprehension systems (Wang and Jiang, 2017; Seo et al., 2017; Wang et al., 2017; Hu et al., 2018; Huang et al., 2018; Liu et al., 2018; Yu et al., 2018; Wang et al., 2018) have outperformed humans on this task in terms of automatic metrics. The SQuAD 2.0 dataset (Rajpurkar et al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models base"
P19-1415,P17-1018,1,0.86445,"1 Ans. Question: What organization runs the public schools in Victoria? UnAns. Question: What organization runs the waste management in Victoria? (Plausible) Answer: Victoria Department of Education Figure 1: An example taken from the SQuAD 2.0 dataset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during internship at Microsoft Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve"
P19-1415,P17-1096,0,0.0389431,"ues+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and its corresponding paragraph p that contains the answer a, we aim to generate unanswerable questions q˜ that fulfills certain requirements. First, it cannot be answered by paragraph p. Second, it must be relevant to both answerable question q"
P19-1415,P13-1171,0,0.0355318,"Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve question answering models. The generated unanswerable questions should not be too easy for the question answering model so that data augmentation can better help the model. For example, a simple baseline method is randomly choosing a question asked for another paragraph, and using it as an unanswerable question. However, it would be trivial to determine whether the retrieved question is answerable by using wordoverlap heuristics, because the question is irrelevant to the context (Yih et al., 2013). In this work, we propose to generate unanswerable questions by editing an answerable question and conditioning on the corresponding paragraph that contains the answer. So the generated unanswerable questions are more lexically similar and relevant to the context. Moreover, by using the answerable question as a prototype and its answer span as a plausible answer, the generated examples can provide more discriminative training signal to the question answering model. 4238 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4238–4248 c Florence, Italy,"
P19-1499,Q17-1010,0,0.046088,"n (Gehrmann et al., 2018). Our model, which is a very good extractive model, can be used as the sentence extraction component in these models and potentially improves their performance. Pre-trained NLP Models Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the surrounding words within a fixed size window to predict the word in the middle with a log bilinear model. The resulting word embedding table can be used in other downstream tasks. There are other word embedding pre-training methods using similar techniques (Pennington et al., 2014; Bojanowski et al., 2017). Peters et al. (2018) and Radford et al. (2018) find even a sentence encoder 5060 Figure 1: The architecture of H IBERT during training. senti is a sentence in the document above, which has four sentences in total. sent3 is masked during encoding and the decoder predicts the original sent3 . (not just word embeddings) can also be pre-trained with language model objectives (i.e., predicting the next or previous word). Language model objective is unidirectional, while many tasks can leverage the context in both directions. Therefore, Devlin et al. (2018) propose the naturally bidirectional mask"
P19-1499,N18-1150,0,0.0511629,"ticipants were presented with a document and a list of summaries produced by different systems. We asked subjects to rank these summaries (ties allowed) by taking informativeness (is the summary capture the important information from the document?) and fluency (is the summary grammatical?) into account. Each document is annotated by three different subjects. 4.4 Results Our main results on the CNNDM dataset are shown in Table 1, with abstractive models in the top block and extractive models in the bottom block. Pointer+Coverage (See et al., 2017), Abstract-ML+RL (Paulus et al., 2017) and DCA (Celikyilmaz et al., 2018) are all sequence to sequence learning based models with copy and coverage modeling, reinforcement learning and deep communicating agents extensions. SentRewrite (Hsu et al., 2018) and InconsisLoss (Chen and Bansal, 2018) all try to decompose the word by word summary generation into sentence selection from document and “sentence” level summarization (or compression). Bottom-Up (Gehrmann et al., 2018) generates summaries by combines a word prediction model with the decoder attention model. The extractive models are usually based on hierarchical encoders (SummaRuNNer; Nallapati et al. 2017 and N"
P19-1499,D18-1442,0,0.0154095,"04b). As the very successful applications of neural networks to a wide range of NLP tasks, the manually engineered features (for document encoding) are replaced with hierarchical LSTMs/CNNs and the sequence labeling (or classification) model is replaced with an LSTM decoder (Cheng and Lapata, 2016; Nallapati et al., 2017). The architecture is widely adopted in recent neural extractive models and is extended with reinforcement learning (Narayan et al., 2018; Dong et al., 2018), latent variable models (Zhang et al., 2018), joint scoring (Zhou et al., 2018) and iterative document representation (Chen et al., 2018). Recently, transformer networks (Vaswani et al., 2017) achieves good performance in machine translation (Vaswani et al., 2017) and a range of NLP tasks (Devlin et al., 2018; Radford et al., 2018). Different from the extractive models above, we adopt a hierarchical Transformer for document encoding and also propose a method to pre-train the document encoder. Abstractive Summarization Abstractive summarization aims to generate the summary of a document with rewriting. Most recent abstractive models (Nallapati et al., 2016) are based on neural sequence to sequence learning (Bahdanau et al., 2015"
P19-1499,P18-1063,0,0.214733,"are based on neural sequence to sequence learning (Bahdanau et al., 2015; Sutskever et al., 2014). However, the generated summaries of these models can not be controlled (i.e., their meanings can be quite different from the original and contents can be repeated). Therefore, copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning model optimizing ROUGE (Paulus et al., 2017) are introduced. These problems are alleviated but not solved. There is also an interesting line of work combining extractive and abstractive summarization with reinforcement learning (Chen and Bansal, 2018), fused attention (Hsu et al., 2018) and bottom-up attention (Gehrmann et al., 2018). Our model, which is a very good extractive model, can be used as the sentence extraction component in these models and potentially improves their performance. Pre-trained NLP Models Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the surrounding words within a fixed size window to predict the word in the middle with a log bilinear model. The resulting word embedding table can be used in other downstream tasks. There are other word embedding pre-training"
P19-1499,P16-1046,0,0.399477,"tences), while abstractive methods may generate new words or phrases which are not in the original document. Extractive summarization is usually modeled as a sentence ranking problem with length constraints (e.g., max number of words or sentences). Top ranked sentences (under constraints) are selected as summaries. Early attempts mostly leverage manually engineered features (Filatova and Hatzivassiloglou, 2004a). Based on these sparse features, sentence are selected using a classifier or a regression model. Later, the feature engineering part in this paradigm is replaced with neural networks. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to encode a document and then use another LSTM to predict binary labels for each sentence in the document. This architecture is widely adopted recently (Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Our model also employs a hierarchical document encoder, but we adopt a hierarchical transformer (Vaswani et al., 2017) rather a hierarchical LSTM. Because recent studies (Vaswani et al., 2017; Devlin et al., 2018) show the transformer model performs better than LSTM in many tasks. Abs"
P19-1499,D18-1409,0,0.306318,"Missing"
P19-1499,P16-1188,0,0.0847286,"el can be trained by minimizing the negative loglikelihood of all sentence labels given their paired documents. 4 Experiments In this section we assess the performance of our model on the document summarization task. We 5063 first introduce the dataset we used for pre-training and the summarization task and give implementation details of our model. We also compare our model against multiple previous models. 4.1 Datasets We conducted our summarization experiments on the non-anonymous version CNN/Dailymail (CNNDM) dataset (Hermann et al., 2015; See et al., 2017), and the New York Times dataset (Durrett et al., 2016; Xu and Durrett, 2019). For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017)3 . The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following (Xu and Durrett, 2019; Durrett et al., 2016), we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in Xu and Durrett (2019), which contain 137,778 documents for training, 17,222 for validation and 17,223 for t"
P19-1499,W04-1017,0,0.455222,"or an overview). The most popular two among them are extractive approaches and abstractive approaches. As the name implies, extractive approaches generate summaries by extracting parts of the original document (usually sentences), while abstractive methods may generate new words or phrases which are not in the original document. Extractive summarization is usually modeled as a sentence ranking problem with length constraints (e.g., max number of words or sentences). Top ranked sentences (under constraints) are selected as summaries. Early attempts mostly leverage manually engineered features (Filatova and Hatzivassiloglou, 2004a). Based on these sparse features, sentence are selected using a classifier or a regression model. Later, the feature engineering part in this paradigm is replaced with neural networks. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to encode a document and then use another LSTM to predict binary labels for each sentence in the document. This architecture is widely adopted recently (Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Our model also employs a hierarchical document encoder, but we adopt a hier"
P19-1499,D18-1443,0,0.403009,"et al., 2014). However, the generated summaries of these models can not be controlled (i.e., their meanings can be quite different from the original and contents can be repeated). Therefore, copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning model optimizing ROUGE (Paulus et al., 2017) are introduced. These problems are alleviated but not solved. There is also an interesting line of work combining extractive and abstractive summarization with reinforcement learning (Chen and Bansal, 2018), fused attention (Hsu et al., 2018) and bottom-up attention (Gehrmann et al., 2018). Our model, which is a very good extractive model, can be used as the sentence extraction component in these models and potentially improves their performance. Pre-trained NLP Models Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the surrounding words within a fixed size window to predict the word in the middle with a log bilinear model. The resulting word embedding table can be used in other downstream tasks. There are other word embedding pre-training methods using similar techniques (Pennington et al., 2014; Bojanowski et al., 2017)."
P19-1499,P16-1154,0,0.274615,"2018). Our model also employs a hierarchical document encoder, but we adopt a hierarchical transformer (Vaswani et al., 2017) rather a hierarchical LSTM. Because recent studies (Vaswani et al., 2017; Devlin et al., 2018) show the transformer model performs better than LSTM in many tasks. Abstractive models do not attract much attention until recently. They are mostly based on sequence to sequence (seq2seq) models (Bahdanau et al., 2015), where a document is viewed a sequence and its summary is viewed as another sequence. Although seq2seq based summarizers can be equipped with copy mechanism (Gu et al., 2016; See et al., 2017), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2017), there is still no guarantee that the generated summaries are grammatical and convey the same meaning as the original document does. It seems that extractive models are more reliable than their abstractive counterparts. However, extractive models require sentence level labels, which are usually not included in most summarization datasets (most datasets only contain document-summary pairs). Sentence labels are usually obtained by rule-based methods (e.g., maximizing the ROUGE score between a"
P19-1499,P18-1013,0,0.127264,"learning (Bahdanau et al., 2015; Sutskever et al., 2014). However, the generated summaries of these models can not be controlled (i.e., their meanings can be quite different from the original and contents can be repeated). Therefore, copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning model optimizing ROUGE (Paulus et al., 2017) are introduced. These problems are alleviated but not solved. There is also an interesting line of work combining extractive and abstractive summarization with reinforcement learning (Chen and Bansal, 2018), fused attention (Hsu et al., 2018) and bottom-up attention (Gehrmann et al., 2018). Our model, which is a very good extractive model, can be used as the sentence extraction component in these models and potentially improves their performance. Pre-trained NLP Models Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the surrounding words within a fixed size window to predict the word in the middle with a log bilinear model. The resulting word embedding table can be used in other downstream tasks. There are other word embedding pre-training methods using similar techniques (Pe"
P19-1499,W04-1013,0,0.464357,"mmaries for training, 13,368 for validation and 11,490 for test. Following (Xu and Durrett, 2019; Durrett et al., 2016), we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in Xu and Durrett (2019), which contain 137,778 documents for training, 17,222 for validation and 17,223 for test. To create sentence level labels for extractive summarization, we used a strategy similar to Nallapati et al. (2017). We label the subset of sentences in a document that maximizes ROUGE (Lin, 2004) (against the human summary) as True and all other sentences as False. To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which includes 6,339,616 documents sampled from the English Gigaword4 dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well. As in See et al. (2017), documents and summaries in CNNDM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit (Manning et al.,"
P19-1499,W01-0100,0,0.708557,"ich stands for HIerachical Bidirectional Encoder Representations from Transformers. We design an unsupervised method to pre-train H IBERT for document modeling. We apply the pre-trained H IBERT to the task of document summarization and achieve state-of-the-art performance on both the CNN/Dailymail and New York Times dataset. 2 Related Work In this section, we introduce work on extractive summarization, abstractive summarization and pre-trained natural language processing models. For a more comprehensive review of summarization, we refer the interested readers to Nenkova and McKeown (2011) and Mani (2001). Extractive Summarization Extractive summarization aims to select important sentences (sometimes other textual units such as elementary discourse units (EDUs)) from a document as its summary. It is usually modeled as a sentence ranking problem by using the scores from classifiers (Kupiec et al., 1995), sequential labeling models (Conroy and O’leary, 2001) as well as integer linear programmers (Woodsend and Lapata, 2010). Early work with these models above mostly leverage human engineered features such as sentence position and length (Radev et al., 2004), word frequency (Nenkova et al., 2006)"
P19-1499,P14-5010,0,0.00389708,"UGE (Lin, 2004) (against the human summary) as True and all other sentences as False. To unsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which includes 6,339,616 documents sampled from the English Gigaword4 dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well. As in See et al. (2017), documents and summaries in CNNDM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit (Manning et al., 2014). To reduce the vocabulary size, we applied byte pair encoding (BPE; Sennrich et al. 2016) to all of our datasets. To limit the memory consumption during training, we limit the length of each sentence to be 50 words (51th word and onwards are removed) and split documents with more than 30 sentences into smaller documents with each containing at most 30 sentences. 3 Scripts publicly available at https://github.com/ abisee/cnn-dailymail 4 https://catalog.ldc.upenn.edu/LDC2012T21 4.2 Implementation Details Our model is trained in three stages, which includes two pre-training stages and one finetu"
P19-1499,K16-1028,0,0.0606688,"18), joint scoring (Zhou et al., 2018) and iterative document representation (Chen et al., 2018). Recently, transformer networks (Vaswani et al., 2017) achieves good performance in machine translation (Vaswani et al., 2017) and a range of NLP tasks (Devlin et al., 2018; Radford et al., 2018). Different from the extractive models above, we adopt a hierarchical Transformer for document encoding and also propose a method to pre-train the document encoder. Abstractive Summarization Abstractive summarization aims to generate the summary of a document with rewriting. Most recent abstractive models (Nallapati et al., 2016) are based on neural sequence to sequence learning (Bahdanau et al., 2015; Sutskever et al., 2014). However, the generated summaries of these models can not be controlled (i.e., their meanings can be quite different from the original and contents can be repeated). Therefore, copy mechanism (Gu et al., 2016), coverage model (See et al., 2017) and reinforcement learning model optimizing ROUGE (Paulus et al., 2017) are introduced. These problems are alleviated but not solved. There is also an interesting line of work combining extractive and abstractive summarization with reinforcement learning ("
P19-1499,N18-1158,0,0.503838,"elected as summaries. Early attempts mostly leverage manually engineered features (Filatova and Hatzivassiloglou, 2004a). Based on these sparse features, sentence are selected using a classifier or a regression model. Later, the feature engineering part in this paradigm is replaced with neural networks. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to encode a document and then use another LSTM to predict binary labels for each sentence in the document. This architecture is widely adopted recently (Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Our model also employs a hierarchical document encoder, but we adopt a hierarchical transformer (Vaswani et al., 2017) rather a hierarchical LSTM. Because recent studies (Vaswani et al., 2017; Devlin et al., 2018) show the transformer model performs better than LSTM in many tasks. Abstractive models do not attract much attention until recently. They are mostly based on sequence to sequence (seq2seq) models (Bahdanau et al., 2015), where a document is viewed a sequence and its summary is viewed as another sequence. Although seq2seq based summarizers can be equipped with c"
P19-1499,D14-1162,0,0.0908302,"8) and bottom-up attention (Gehrmann et al., 2018). Our model, which is a very good extractive model, can be used as the sentence extraction component in these models and potentially improves their performance. Pre-trained NLP Models Most model pretraining methods in NLP leverage the natural ordering of text. For example, word2vec uses the surrounding words within a fixed size window to predict the word in the middle with a log bilinear model. The resulting word embedding table can be used in other downstream tasks. There are other word embedding pre-training methods using similar techniques (Pennington et al., 2014; Bojanowski et al., 2017). Peters et al. (2018) and Radford et al. (2018) find even a sentence encoder 5060 Figure 1: The architecture of H IBERT during training. senti is a sentence in the document above, which has four sentences in total. sent3 is masked during encoding and the decoder predicts the original sent3 . (not just word embeddings) can also be pre-trained with language model objectives (i.e., predicting the next or previous word). Language model objective is unidirectional, while many tasks can leverage the context in both directions. Therefore, Devlin et al. (2018) propose the na"
P19-1499,N18-1202,0,0.560136,"August 2, 2019. 2019 Association for Computational Linguistics cently (Cheng and Lapata, 2016; Nallapati et al., 2017) employ hierarchical document encoders and even have neural decoders, which are complex. Training such complex neural models with inaccurate binary labels is challenging. We observed in our initial experiments on one of our dataset that our extractive model (see Section 3.3 for details) overfits to the training set quickly after the second epoch, which indicates the training set may not be fully utilized. Inspired by the recent pre-training work in natural language processing (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018), our solution to this problem is to first pre-train the “complex”’ part (i.e., the hierarchical encoder) of the extractive model on unlabeled data and then we learn to classify sentences with our model initialized from the pre-trained encoder. In this paper, we propose H IBERT, which stands for HIerachical Bidirectional Encoder Representations from Transformers. We design an unsupervised method to pre-train H IBERT for document modeling. We apply the pre-trained H IBERT to the task of document summarization and achieve state-of-the-art performance o"
P19-1499,radev-etal-2004-mead,0,0.322522,"Missing"
P19-1499,P17-1099,0,0.626391,"also employs a hierarchical document encoder, but we adopt a hierarchical transformer (Vaswani et al., 2017) rather a hierarchical LSTM. Because recent studies (Vaswani et al., 2017; Devlin et al., 2018) show the transformer model performs better than LSTM in many tasks. Abstractive models do not attract much attention until recently. They are mostly based on sequence to sequence (seq2seq) models (Bahdanau et al., 2015), where a document is viewed a sequence and its summary is viewed as another sequence. Although seq2seq based summarizers can be equipped with copy mechanism (Gu et al., 2016; See et al., 2017), coverage model (See et al., 2017) and reinforcement learning (Paulus et al., 2017), there is still no guarantee that the generated summaries are grammatical and convey the same meaning as the original document does. It seems that extractive models are more reliable than their abstractive counterparts. However, extractive models require sentence level labels, which are usually not included in most summarization datasets (most datasets only contain document-summary pairs). Sentence labels are usually obtained by rule-based methods (e.g., maximizing the ROUGE score between a set of sentences an"
P19-1499,P16-1162,0,0.0555691,"nsupervisedly pre-train our document model H IBERT (see Section 3.2 for details), we created the GIGA-CM dataset (totally 6,626,842 documents and 2,854 million words), which includes 6,339,616 documents sampled from the English Gigaword4 dataset and the training split of the CNNDM dataset. We used the validation set of CNNDM as the validation set of GIGA-CM as well. As in See et al. (2017), documents and summaries in CNNDM, NYT50 and GIGA-CM are all segmented and tokenized using Stanford CoreNLP toolkit (Manning et al., 2014). To reduce the vocabulary size, we applied byte pair encoding (BPE; Sennrich et al. 2016) to all of our datasets. To limit the memory consumption during training, we limit the length of each sentence to be 50 words (51th word and onwards are removed) and split documents with more than 30 sentences into smaller documents with each containing at most 30 sentences. 3 Scripts publicly available at https://github.com/ abisee/cnn-dailymail 4 https://catalog.ldc.upenn.edu/LDC2012T21 4.2 Implementation Details Our model is trained in three stages, which includes two pre-training stages and one finetuning stage. The first stage is the open-domain pretraining and in this stage we train H IB"
P19-1499,D18-1548,0,0.0537113,"Missing"
P19-1499,P10-1058,0,0.0552205,"tractive summarization and pre-trained natural language processing models. For a more comprehensive review of summarization, we refer the interested readers to Nenkova and McKeown (2011) and Mani (2001). Extractive Summarization Extractive summarization aims to select important sentences (sometimes other textual units such as elementary discourse units (EDUs)) from a document as its summary. It is usually modeled as a sentence ranking problem by using the scores from classifiers (Kupiec et al., 1995), sequential labeling models (Conroy and O’leary, 2001) as well as integer linear programmers (Woodsend and Lapata, 2010). Early work with these models above mostly leverage human engineered features such as sentence position and length (Radev et al., 2004), word frequency (Nenkova et al., 2006) and event features (Filatova and Hatzivassiloglou, 2004b). As the very successful applications of neural networks to a wide range of NLP tasks, the manually engineered features (for document encoding) are replaced with hierarchical LSTMs/CNNs and the sequence labeling (or classification) model is replaced with an LSTM decoder (Cheng and Lapata, 2016; Nallapati et al., 2017). The architecture is widely adopted in recent n"
P19-1499,D19-1324,0,0.667328,"inimizing the negative loglikelihood of all sentence labels given their paired documents. 4 Experiments In this section we assess the performance of our model on the document summarization task. We 5063 first introduce the dataset we used for pre-training and the summarization task and give implementation details of our model. We also compare our model against multiple previous models. 4.1 Datasets We conducted our summarization experiments on the non-anonymous version CNN/Dailymail (CNNDM) dataset (Hermann et al., 2015; See et al., 2017), and the New York Times dataset (Durrett et al., 2016; Xu and Durrett, 2019). For the CNNDM dataset, we preprocessed the dataset using the scripts from the authors of See et al. (2017)3 . The resulting dataset contains 287,226 documents with summaries for training, 13,368 for validation and 11,490 for test. Following (Xu and Durrett, 2019; Durrett et al., 2016), we created the NYT50 dataset by removing the documents whose summaries are shorter than 50 words from New York Times dataset. We used the same training/validation/test splits as in Xu and Durrett (2019), which contain 137,778 documents for training, 17,222 for validation and 17,223 for test. To create sentence"
P19-1499,D18-1088,1,0.949714,"Early attempts mostly leverage manually engineered features (Filatova and Hatzivassiloglou, 2004a). Based on these sparse features, sentence are selected using a classifier or a regression model. Later, the feature engineering part in this paradigm is replaced with neural networks. Cheng and Lapata (2016) propose a hierarchical long short-term memory network (LSTM; Hochreiter and Schmidhuber 1997) to encode a document and then use another LSTM to predict binary labels for each sentence in the document. This architecture is widely adopted recently (Nallapati et al., 2017; Narayan et al., 2018; Zhang et al., 2018). Our model also employs a hierarchical document encoder, but we adopt a hierarchical transformer (Vaswani et al., 2017) rather a hierarchical LSTM. Because recent studies (Vaswani et al., 2017; Devlin et al., 2018) show the transformer model performs better than LSTM in many tasks. Abstractive models do not attract much attention until recently. They are mostly based on sequence to sequence (seq2seq) models (Bahdanau et al., 2015), where a document is viewed a sequence and its summary is viewed as another sequence. Although seq2seq based summarizers can be equipped with copy mechanism (Gu et"
P19-1499,P18-1061,1,0.915666,"006) and event features (Filatova and Hatzivassiloglou, 2004b). As the very successful applications of neural networks to a wide range of NLP tasks, the manually engineered features (for document encoding) are replaced with hierarchical LSTMs/CNNs and the sequence labeling (or classification) model is replaced with an LSTM decoder (Cheng and Lapata, 2016; Nallapati et al., 2017). The architecture is widely adopted in recent neural extractive models and is extended with reinforcement learning (Narayan et al., 2018; Dong et al., 2018), latent variable models (Zhang et al., 2018), joint scoring (Zhou et al., 2018) and iterative document representation (Chen et al., 2018). Recently, transformer networks (Vaswani et al., 2017) achieves good performance in machine translation (Vaswani et al., 2017) and a range of NLP tasks (Devlin et al., 2018; Radford et al., 2018). Different from the extractive models above, we adopt a hierarchical Transformer for document encoding and also propose a method to pre-train the document encoder. Abstractive Summarization Abstractive summarization aims to generate the summary of a document with rewriting. Most recent abstractive models (Nallapati et al., 2016) are based on n"
P19-1609,W17-4773,0,0.0183401,"cation, many grammatical improvements do not bring task-oriented improvements. The reason is that the parts GEC edits are not the content that should be kept in the results. Also, it is notable that except for Formal→Informal style transfer whose target sentences should be in an informal style, GEC brings much more improvements than adverse effects on the tasks, demonstrating the potential of GEC for NLG. 4 Related Work and Discussion The most related work to ours is the automatic post editing (APE) (Bojar et al., 2016) which has been extensively studied for MT (e.g., (Pal et al., 2016, 2017; Chatterjee et al., 2017; Hokamp, 2017; Tan et al., 2017)) in the past few years. These APE approaches are usually trained with source language input data, target language MT output and target language post editing (PE) data. Although these APE models and systems have proven to be successful in improving MT results, they are taskspecific and cannot be used for other NLG tasks. In contrast, we propose a general post editing approach by applying the current state-of-the-art GEC system to editing the outputs of NLG systems. To the best of our knowledge, this is the first attempt to explore improving seq2seq based NLG mo"
P19-1609,D15-1042,0,0.0608229,"Missing"
P19-1609,D13-1155,0,0.0342569,"entence compression dataset1 (GoogComp). For sentence simplification, we use the state-of-the-art deep reinforcement model DRESS (Zhang and Lapata, 2017) as our base model and test on Newsela text simplification dataset. Table 6 shows the results for the effects of GEC on sentence compression and simplification. For sentence compression, BLEU decreases from 60.38 to 58.77 after GEC post editing. We manually analyze the results and find there are many grammatical errors in the reference sentences. This is not surprising, since the reference sentences are constructed with an automatic approach (Filippova and Altun, 2013). The grammatical errors in the references affect the BLEU evaluation and make it less reliable. The BLEU decrease is also observed in sentence simplification task but for a different reason. In the Newsela dataset, the reference sentences are written by humans and therefore have much fewer grammatical errors compared to GoogComp. In contrast to sentence compression where reference errors are the main reason for the BLEU decrease, the BLEU decrease in sentence simplification usually happens in the cases where the correction of grammatical errors reduces the sentence’s n-gram overlap with the r"
P19-1609,P18-1097,1,0.909232,"al., 2014; Sutskever et al., 2014) has attracted growing attention in natural language processing (NLP). Despite various advantages of seq2seq models, they tend to have a weakness: there is no guarantee that they can always generate sentences without grammatical errors. Table 1 shows examples generated by seq2seq models in various tasks with grammatical errors. One valid solution to this challenge is conducting grammatical error correction (GEC) for machine generated sentences. Recent GEC systems (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Ge et al., 2018a,b) can achieve human-level performance in GEC benchmarks. We are curious whether they can help improve seq2seq based natural language generation (NLG) models. We therefore propose an empirical study on GEC post editing for various text generation tasks (i.e., machine translation, style transfer, sentence compres• We present an empirical study on GEC post editing for seq2seq text generation. To the best of our knowledge, it is the first work to study improving seq2seq based NLG models using GEC. • We show some interesting results by thoroughly comparing and analyzing GEC post editing for vari"
P19-1609,N18-2046,0,0.0616355,"quence (seq2seq) text generation (Cho et al., 2014; Sutskever et al., 2014) has attracted growing attention in natural language processing (NLP). Despite various advantages of seq2seq models, they tend to have a weakness: there is no guarantee that they can always generate sentences without grammatical errors. Table 1 shows examples generated by seq2seq models in various tasks with grammatical errors. One valid solution to this challenge is conducting grammatical error correction (GEC) for machine generated sentences. Recent GEC systems (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Ge et al., 2018a,b) can achieve human-level performance in GEC benchmarks. We are curious whether they can help improve seq2seq based natural language generation (NLG) models. We therefore propose an empirical study on GEC post editing for various text generation tasks (i.e., machine translation, style transfer, sentence compres• We present an empirical study on GEC post editing for seq2seq text generation. To the best of our knowledge, it is the first work to study improving seq2seq based NLG models using GEC. • We show some interesting results by thoroughly comparing and analyzing GEC post"
P19-1609,W17-4775,0,0.0272998,"improvements do not bring task-oriented improvements. The reason is that the parts GEC edits are not the content that should be kept in the results. Also, it is notable that except for Formal→Informal style transfer whose target sentences should be in an informal style, GEC brings much more improvements than adverse effects on the tasks, demonstrating the potential of GEC for NLG. 4 Related Work and Discussion The most related work to ours is the automatic post editing (APE) (Bojar et al., 2016) which has been extensively studied for MT (e.g., (Pal et al., 2016, 2017; Chatterjee et al., 2017; Hokamp, 2017; Tan et al., 2017)) in the past few years. These APE approaches are usually trained with source language input data, target language MT output and target language post editing (PE) data. Although these APE models and systems have proven to be successful in improving MT results, they are taskspecific and cannot be used for other NLG tasks. In contrast, we propose a general post editing approach by applying the current state-of-the-art GEC system to editing the outputs of NLG systems. To the best of our knowledge, this is the first attempt to explore improving seq2seq based NLG models with a st"
P19-1609,N18-1055,0,0.0185797,"d: Introduction Sequence-to-sequence (seq2seq) text generation (Cho et al., 2014; Sutskever et al., 2014) has attracted growing attention in natural language processing (NLP). Despite various advantages of seq2seq models, they tend to have a weakness: there is no guarantee that they can always generate sentences without grammatical errors. Table 1 shows examples generated by seq2seq models in various tasks with grammatical errors. One valid solution to this challenge is conducting grammatical error correction (GEC) for machine generated sentences. Recent GEC systems (Chollampatt and Ng, 2018; Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018; Ge et al., 2018a,b) can achieve human-level performance in GEC benchmarks. We are curious whether they can help improve seq2seq based natural language generation (NLG) models. We therefore propose an empirical study on GEC post editing for various text generation tasks (i.e., machine translation, style transfer, sentence compres• We present an empirical study on GEC post editing for seq2seq text generation. To the best of our knowledge, it is the first work to study improving seq2seq based NLG models using GEC. • We show some interesting results by tho"
P19-1609,C18-1086,0,0.125372,"Missing"
P19-1609,P16-2046,0,0.0687993,"Missing"
P19-1609,E17-2056,0,0.0440796,"Missing"
P19-1609,N18-1012,0,0.108733,"Missing"
P19-1609,stymne-ahrenberg-2010-using,0,0.0912647,"Missing"
P19-1609,W17-4776,0,0.0250691,"o not bring task-oriented improvements. The reason is that the parts GEC edits are not the content that should be kept in the results. Also, it is notable that except for Formal→Informal style transfer whose target sentences should be in an informal style, GEC brings much more improvements than adverse effects on the tasks, demonstrating the potential of GEC for NLG. 4 Related Work and Discussion The most related work to ours is the automatic post editing (APE) (Bojar et al., 2016) which has been extensively studied for MT (e.g., (Pal et al., 2016, 2017; Chatterjee et al., 2017; Hokamp, 2017; Tan et al., 2017)) in the past few years. These APE approaches are usually trained with source language input data, target language MT output and target language post editing (PE) data. Although these APE models and systems have proven to be successful in improving MT results, they are taskspecific and cannot be used for other NLG tasks. In contrast, we propose a general post editing approach by applying the current state-of-the-art GEC system to editing the outputs of NLG systems. To the best of our knowledge, this is the first attempt to explore improving seq2seq based NLG models with a state-of-the-art neur"
P19-1609,D17-1062,1,0.895997,"Missing"
S14-2033,S13-2053,0,0.595629,"utomatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn SSWE, we develop a tailored neural network, which incorporates the supervision from sentiment polarity of tweets in the hybrid loss function. We learn SSWE from tweets, leveraging massive tweets with emoticons as distantsupervised corpora without any manual annotations. We evaluate the deep learning system on the test set of Twitter Sentiment Analysis Track in SemEval 2014 2 . Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along with the SemEval 2013 participants o"
S14-2033,P14-2009,1,0.80825,"afted features. We develop a neural network with hybrid loss function 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creat"
S14-2033,S13-2052,0,0.118118,"Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn SSWE, we develop a tailored neural network, which incorporates the supervision from sentiment polarity of tweets in the hybrid loss function. We learn SSWE from tweets, leveraging massive tweets with emoticons as distantsupervised corpora without any manual annotations. We evaluate the deep learning system on the test set of Twitter Sentiment Analysis Track in SemEval 2014 2 . Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along with the SemEval 2013 participants owning larger training data than us. The performance of only using SSWE as features is comparable to th"
S14-2033,W02-1011,0,0.0200027,"nction 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 http://alt.qcri.org/semeval20"
S14-2033,P11-2008,0,0.291901,"Missing"
S14-2033,P14-1146,1,0.727439,"uru Wei‡ , Bing Qin† , Ting Liu† , Ming Zhou‡ Research Center for Social Computing and Information Retrieval Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China {dytang, qinb, tliu}@ir.hit.edu.cn {fuwei, mingzhou}@microsoft.com Abstract designing effective features to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). For example, Mohammad et al. (2013) implement diverse sentiment lexicons and a variety of hand-crafted features. To leverage massive tweets containing positive and negative emoticons for automatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment c"
S14-2033,P11-1016,1,0.797284,"res with the state-of-the-art hand-crafted features. We develop a neural network with hybrid loss function 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the orga"
S14-2033,H05-1044,0,0.204982,"Missing"
S14-2033,P14-1062,0,0.00858127,"omputing and Information Retrieval Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China {dytang, qinb, tliu}@ir.hit.edu.cn {fuwei, mingzhou}@microsoft.com Abstract designing effective features to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). For example, Mohammad et al. (2013) implement diverse sentiment lexicons and a variety of hand-crafted features. To leverage massive tweets containing positive and negative emoticons for automatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To le"
S15-2086,S14-2115,0,0.0131346,"l (Mikolov et al., 2013) to learn 40-dimensional word embeddings from a twitter dataset. Then, we employ K-means algorithm and L2 distance of word vectors to cluster the 255, 657 words to 4960 classes. The clusters are used to represent words. We extract unigrams and bigrams as features, and use them in sub/obj classifier. The word2vec clustering results are publicly available3 for research purposes. As shown in Table 1, similar words are clustered into the same clusters. This feature template is used in sub/obj classification. CNN predicted distribution The convolutional neural networks (dos Santos, 2014) are used to predict the probabilities of three sentiment classes, and the predicted distribution is used as a threedimension feature template. As illustrated in Figure 2, we use the network architecture proposed by Collobert et al. (2011). The dimension of word vectors is 50, and the window size is 5. Then the concatenated word vectors are fed into a convolutional layer. The vector representation of a sentence is obtained by a max pooling layer, and is used to predict the probabilities of three classes by the softmax layer. We employ stochastic gradient descent to minimize the cross-entropy l"
S15-2086,D14-1181,0,0.0112281,"Missing"
S15-2086,S13-2052,0,0.0610536,"performance of our system. 1 Introduction In the task 10 of SemEval-2015, submitted systems are required to categorize tweets to positive, negative, and neutral classes (Rosenthal et al., 2015). There are six testing sets in SemEval2015. Four of them are tweets: Twitter13, Twitter14, Twitter14Sarcasm, and Twitter15. The TwitterSarcasm14 consists of the tweets which express sarcasm. In order to evaluate the performance on out-of-domain data, the other two datasets are LiveJournal14 and SMS13 that are from web blogs and SMS messages respectively. The details of these datasets are described in (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). ∗ Contribution during internship at Microsoft Research. 2 2.1 System Description Overview As shown in Figure 1, our sentiment analysis system is a two-stage sentiment classifier which consists of a subjective/objective (sub/obj) classifier and a positive/negative (pos/neg) classifier. By using this artitacture, we can design different feature sets for the two classification steps. Notably, the predicted values of pos/neg classifier is employed to help classify tweets to sub/obj classes. We employ the LIBLINEAR (Fan et al., 2008) with option “-"
S15-2086,N13-1039,0,0.0449785,"ize. Word ngrams We use unigrams and bigrams for words. Character ngrams For each word, character ngrams are extracted. We use four-grams and fivegrams in our system. Word skip-grams For all the trigrams and fourgrams, one of the words is replaced by ∗ to indicate the presence of non-contiguous words. This feature template is used in sub/obj classification. Brown cluster ngrams We use Brown clusters1 to represent words, and extract unigrams and bigrams as features. POS The presence or absence of part-of-speech tags are used as binary features. We use the CMU ARK Twitter Part-of-Speech Tagger (Owoputi et al., 2013) in our implementation. Lexicons The NRC Hashtag Sentiment Lexicon 1 http://www.ark.cs.cmu.edu/TweetNLP/clusters/50mpaths2 516 2.3 Deep Learning Features In order to automatically extract features, we explore using some deep learning techniques in our system. These features and the basic features described in Section 2.2 are used together to learn classifiers. Word2vec cluster ngrams We use the word2vec tool (Mikolov et al., 2013) to learn 40-dimensional word embeddings from a twitter dataset. Then, we employ K-means algorithm and L2 distance of word vectors to cluster the 255, 657 words to 49"
S15-2086,S14-2009,0,0.0279426,"system. 1 Introduction In the task 10 of SemEval-2015, submitted systems are required to categorize tweets to positive, negative, and neutral classes (Rosenthal et al., 2015). There are six testing sets in SemEval2015. Four of them are tweets: Twitter13, Twitter14, Twitter14Sarcasm, and Twitter15. The TwitterSarcasm14 consists of the tweets which express sarcasm. In order to evaluate the performance on out-of-domain data, the other two datasets are LiveJournal14 and SMS13 that are from web blogs and SMS messages respectively. The details of these datasets are described in (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). ∗ Contribution during internship at Microsoft Research. 2 2.1 System Description Overview As shown in Figure 1, our sentiment analysis system is a two-stage sentiment classifier which consists of a subjective/objective (sub/obj) classifier and a positive/negative (pos/neg) classifier. By using this artitacture, we can design different feature sets for the two classification steps. Notably, the predicted values of pos/neg classifier is employed to help classify tweets to sub/obj classes. We employ the LIBLINEAR (Fan et al., 2008) with option “-s 1” as our classifier."
S15-2086,S15-2078,0,0.0805037,"he sentiment labels for tweets, which enables us to design different features for subjective/objective classification and positive/negative classification. In addition to n-grams, lexicons, word clusters, and twitter-specific features, we develop several deep learning methods to automatically extract features for the message-level sentiment classification task. Moreover, we propose a polarity boosting trick which improves the performance of our system. 1 Introduction In the task 10 of SemEval-2015, submitted systems are required to categorize tweets to positive, negative, and neutral classes (Rosenthal et al., 2015). There are six testing sets in SemEval2015. Four of them are tweets: Twitter13, Twitter14, Twitter14Sarcasm, and Twitter15. The TwitterSarcasm14 consists of the tweets which express sarcasm. In order to evaluate the performance on out-of-domain data, the other two datasets are LiveJournal14 and SMS13 that are from web blogs and SMS messages respectively. The details of these datasets are described in (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). ∗ Contribution during internship at Microsoft Research. 2 2.1 System Description Overview As shown in Figure 1, our sentiment"
S15-2086,D13-1170,0,0.0263646,"Missing"
S15-2086,P14-1146,1,0.837196,"lovve, love/miss, luuuvvv, lubb, lurve Table 1: Examples of word2vec clusters. Similar words are clustered to the same cluster. 3 300 300 300 Softmax layer Hidden layer ... ... Max pooling layer Convolutional layer 5x50 Figure 2: Architecture of convolutional neural network used in our system. The lines represent vectors, and the numbers indicate the vector dimensions. vent overfitting, a L2-norm constraint for the column vectors of weight matrices is used. The backpropagation algorithm (Rumelhart et al., 1986) is employed to compute the gradients for parameters. The word vectors provided by Tang et al. (2014) are used for initialization. Sentiment-specific embedding Tang et al. (2014) improve the word2vec model to learn sentimentspecific word embeddings from tweets annotated by emoticons. We use element-wise max, min, and avg operations for the word vectors to extract features. 2.4 Polarity Boosting Trick Predicted scores indicate the confidence of classifier. If the pos/neg classifier has a high confidence to classify a tweet to positive or negative, it is less likely that this tweet is objective. Consequently, the absolute value of output of pos/neg classifier is used as a feature in sub/obj cla"
S15-2086,H05-1044,0,0.0384794,"ributes to the performances. It provides more explicit sentiment information than word2vec vectors. As shown in Table 2, the polarity boosting trick also contributes to the performance of our system on all the six datasets. Table 2: We compare the macro-averaged F1-scores of our system (Spp) with the best results of other teams in SemEval-2015. Our system achieves the highest F1scores on three out of six datasets. LvJn14 and Sarc14 become better. Moreover, the automatically learned lexicons play a positive role in our system. We also try some manually annotated lexicons (such as MPQA Lexicon (Wilson et al., 2005), and Bing Liu Lexicon (Hu and Liu, 2004)), but the performance drops on the dev data. It illustrates the coverage of lexicons is important for the informal text data. The cluster features are also useful in this task, because the clusters reduce the feature sparsity and have the ability to deal with out-ofvocabulary words. The deep learning significantly improves test results on all the datasets except on the sarcastic tweets. Using the clustering results of word2vec performs better and more stable than directly using the vectors as features. This feature template contributes more than other"
zhang-etal-2008-exploiting,N07-1015,0,\N,Missing
zhang-etal-2008-exploiting,P05-1053,0,\N,Missing
zhang-etal-2008-exploiting,P06-2060,0,\N,Missing
zhang-etal-2008-exploiting,P04-1054,0,\N,Missing
zhang-etal-2008-exploiting,N06-1038,0,\N,Missing
zhang-etal-2008-exploiting,D07-1076,0,\N,Missing
zhang-etal-2008-exploiting,H05-1091,0,\N,Missing
zhang-etal-2008-exploiting,P06-1104,0,\N,Missing
