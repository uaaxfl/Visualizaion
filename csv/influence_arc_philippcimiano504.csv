2020.ldl-1.5,L18-1383,0,0.14009,"roperability and sharing of existing language technologies and data sets. In order to contribute to the development of common language technologies and support these sharing initiatives, as a proof-of-concept we have developed an approach to integrate our Terme-`a-LLOD approach into two language infrastructures, namely Teanga and ELG. Furthermore, such an integration proves the interoperability of our approach which relies on virtualization services. Teanga is a linked data based platform for natural language processing (NLP) which enables the use of many NLP services from a single interface (Ziad et al., 2018). Many platforms have been developed to improve the interoperability among different NLP services and, consequently, reThe grid platform has been built with robust, scalable, reliable, widely used technologies that are constantly developed further. It presents the ability to scale with the growing demand and supply of resources through an interactive modern web interface, providing the base technology for a catalogue or directory of functional services, data sets, tools, technologies, models and LT companies, research organisations, research projects, service and application types, languages."
2020.ldl-1.5,ehrmann-etal-2014-representing,1,0.785544,"ive your terminology, ii) make your terminology interoperable, iii) link your terminology to a network. As datamodel, the document proposes to use the SKOS model rather than the Ontolex-lemon model. The approach proposed in this paper has focused on the transformation of terminological resources; yet, the principled approach of simplifying the work of transforming resources into RDF would apply to other data formats as well. There has been some work on transforming lexicographic resources as well as WordNets into Linked Data using lemon-Ontolex (McCrae et al., 2012; Eckle-Kohler et al., 2015; Ehrmann et al., 2014; McCrae et al., 2014). There has been work on transforming corpora into RDF (Chiarcos and F¨ath, 2017). The approach described here could be applied to those data formats as well. We have integrated our transformation component into the Teanga (Ziad et al., 2018) and ELG (Rehm et al., Forthcoming) infrastructrues. There are other NLP architecures into which TAL container could be integtated. WebLicht23 is an environment for building, executing, and visualizing the results of NLP pipelines, which is integrated into the CLARIN infrastructure (Hinrichs and Krauwer, 2014). NLP tools are implement"
2020.ldl-1.5,hinrichs-krauwer-2014-clarin,0,0.0303608,"012; Eckle-Kohler et al., 2015; Ehrmann et al., 2014; McCrae et al., 2014). There has been work on transforming corpora into RDF (Chiarcos and F¨ath, 2017). The approach described here could be applied to those data formats as well. We have integrated our transformation component into the Teanga (Ziad et al., 2018) and ELG (Rehm et al., Forthcoming) infrastructrues. There are other NLP architecures into which TAL container could be integtated. WebLicht23 is an environment for building, executing, and visualizing the results of NLP pipelines, which is integrated into the CLARIN infrastructure (Hinrichs and Krauwer, 2014). NLP tools are implemented as web services that consume and produce the Text Corpus Format (TCF)24 , an XML format designed for use as an internal data exchange format for WebLicht processing tools. It ensures semantic interoperability among all WebLicht tools and resources by defining a common vocabulary for linguistic concepts in TCF schema. The services and resources are developed as web services in the CLARIN framework. The services are exposed using metadata descriptions Component Metadata 6. Conclusion We have proposed a virtualization approach to support the conversion and hosting of t"
2020.lrec-1.695,P18-1073,0,0.016647,"ersion of 5 terminological resources in TBX to RDF. 5.3. Linking Finally, the project is developing (semi-)automated linking mechanisms. This concerns both the conceptual level of language descriptions as also the lexical data. We are working both in a mono- and in a cross-lingual set up. Since this work is still in progress, at this time we can only report on preliminary approaches. In the context of cross-lingual concept matching, we are updating an already existent ontology matching tool, CIDERCL (Gracia and Asooja, 2013) with contemporary techniques based on cross-lingual word embeddings (Artetxe et al., 2018). Regarding linking cross-lingual lexical data, the project has laid the groundwork for research in the topic of ”translation inference across dictionaries” by organising the TIAD’19 shared task (Gracia et al., 2019), in which a benchmark and evaluation framework were provided to allow for systematic comparisons between systems. Such systems were able to infer indirect translations between language pairs that were initially disconnected in the Apertium RDF graph (Gracia et al., 2018), showing promising results but also the need of further research. Ontology lexicalisation aims at developing te"
2020.lrec-1.695,2019.gwc-1.34,1,0.826147,"Missing"
2020.lrec-1.695,E09-2008,0,0.0376885,"ons for morphology (Klimek et al., 2019) and the representation of frequency, attestation and corpus information (Chiarcos and Ionov, 2019). The morphology module is specifically important for the cross-linguistic applicability of OntoLex-Lemon, as it aims to support languages with a lot of stem internal alternations: By using regular expressions to represent morphology generation rules, it provides implementation-independent means to generate inflected forms from lemma information, that can be subsequently incorporated in conventional morphology frameworks such as XFST (Ranta, 1998) or FOMA (Hulden, 2009). Specifications for phonological processes and mor3 See https://www.w3.org/2016/05/ontolex/ See (McCrae et al., 2012) and (Cimiano et al., 2016). 5 SKOS stands for “Simple Knowledge Organization System”. SKOS provides “a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary” (https://www. w3.org/TR/skos-primer/). 6 See http://www.elex.is/ for more detail. 4 Figure 2: The core Modules of OntoLex-Lemon. Graphic taken from https://www.w3.or"
2020.lrec-1.695,L16-1386,1,0.78537,"Missing"
2020.lrec-1.695,W98-1308,0,0.29918,"d emerging specifications for morphology (Klimek et al., 2019) and the representation of frequency, attestation and corpus information (Chiarcos and Ionov, 2019). The morphology module is specifically important for the cross-linguistic applicability of OntoLex-Lemon, as it aims to support languages with a lot of stem internal alternations: By using regular expressions to represent morphology generation rules, it provides implementation-independent means to generate inflected forms from lemma information, that can be subsequently incorporated in conventional morphology frameworks such as XFST (Ranta, 1998) or FOMA (Hulden, 2009). Specifications for phonological processes and mor3 See https://www.w3.org/2016/05/ontolex/ See (McCrae et al., 2012) and (Cimiano et al., 2016). 5 SKOS stands for “Simple Knowledge Organization System”. SKOS provides “a model for expressing the basic structure and content of concept schemes such as thesauri, classification schemes, subject heading lists, taxonomies, folksonomies, and other similar types of controlled vocabulary” (https://www. w3.org/TR/skos-primer/). 6 See http://www.elex.is/ for more detail. 4 Figure 2: The core Modules of OntoLex-Lemon. Graphic taken"
2020.nlpcss-1.15,Q17-1010,0,0.0117306,". We pre-process the text data using standard pre-processing operations such as lemmatization, stemming and stopword removal. We use spaCy (Honnibal and Montani, 2017) to obtain the lemmata and POS-tags. All features are represented as tf.idf values in the models. POS-tags are glued to the words in a pre-processing step for this purpose. We compute embeddings specific for our dataset using all 295,812 documentations and all 636 unique texts for measures of care applied4 (the wording over all data is very similar) to train our own word embeddings for our domain with the python module fasttext (Bojanowski et al., 2017). Also, we up-sample the datasets for training to have an equal distribution of the classes. As a baseline, we consider models trained using unigram features weighted with tf.idf. We experimentally test different feature combinations including the following features for our models: trigrams, lemmata, stemmed words, word embeddings (only in CNN), POS-tags. This leads to 15 feature combinations (8 without POS tags and word embeddings for CNN). 5 F1 (macro) 0.86 0.85 0.85 0.85 0.81 0.80 0.69 0.48 0.47 Table 4: Baseline scores with tf.idf of Bag-of-words : F-Measure (macro) (Median of 5-fold cross"
2020.nlpcss-1.15,C18-1179,0,0.0450775,"Missing"
2020.nlpcss-1.15,S16-1003,0,0.0379926,"Missing"
2020.nlpcss-1.15,W17-5203,0,0.0468836,"Missing"
2020.spnlp-1.4,P13-1012,0,0.0265566,"mctc-spnlp2020.zip 3 https://github.com/ag-sc/ SCIOExtraction 23 to model the tasks of entity recognition, relation extraction and co-reference resolution jointly, in their approach the interaction between relation extraction and co-reference resolution is not modelled directly, only via entity tags. In our approach we model the joint interaction between inducing equivalence classes (resolving co-references) while extracting the properties of entities/individuals as a basis to inform the decision about whether two individuals are the same (thus co-refer) given their properties. Durret et al. (Durrett et al., 2013) propose a global inference entity-level modeling for classical co-reference resolution based on a rich factor graph. In the unrolled factor graph, each factor refers to one entity property defined on a semantic or syntactic linguistic basis. In contrast to this work where properties of an individual/entity are pre-defined by the semantic model. Thus, our focus lies in their joint exploration while learning their interplay during inference in order to decide whether the properties belong to the same individual or not. Haghighi et al. (Haghighi and Klein, 2010) propose an unsupervised generativ"
2020.spnlp-1.4,W16-6112,0,0.0420281,"investigation and analysis of different approximative inference strategies based on Gibbs sampling. We present and evaluate our models on the task of extracting key parameters from scientific full text articles describing pre-clinical studies in the domain of spinal cord injury. 1 Introduction While there has been significant progress on information extraction tasks with a comparably low level of structural complexity such as entity recognition (Goulart et al., 2011; Nadeau and Sekine, 2007), relation extraction (Zhou et al., 2014; Kumar, 2017), and co-reference resolution (Soon et al., 2001; Ferracane et al., 2016), there is not much progress on capturing the comprehensive meaning of a text with respect to a given semantic model in terms of a given vocabulary of classes and properties. We refer to this task as model-complete text comprehension (MCTC) which requires to put all 1 We refer to mentions of entities in a text as entities and to the denotation of such entities in a given model of the text as individuals 22 Proceedings of 4th Workshop on Structured Prediction for NLP, pages 22–32 c November 20, 2020. 2020 Association for Computational Linguistics with the prediction of the properties of each in"
2020.spnlp-1.4,W18-5204,0,0.0190977,"oposed to rely on a maximum entropy classifier jointly extracting fine grained PICO elements from abstracts. Brujin et al. (De Bruijn et al., 2008) combined an SVM-based text classifier with regular expressions to extract PICO elements. Further, Ferracane et al. (Ferracane et al., 2016) aim to leverage co-reference resolution to identify experimental groups (patients) from medical abstracts. However, none of these works aims at deeper extraction of arms/experimental groups and their properties. In general, most approaches in the literature focus on sentence extraction and classification only (Mayer et al., 2018; Zhao et al., 2012; Wallace et al., 2016) rather than on predicting a semantic structure. 3 Method Structured prediction describes a variety of tasks with the goal of predicting a pre-defined target structure that is extracted from an unstructured input text (Smith, 2011). We formulate the MCTC problem as a structured prediction task, where the structure to be predicted is an instance of the semantic model capturing the meaning of a text. This involves the task of predicting the number of individuals of each class (cardinality prediction) as well as predicting the values of the key properties"
2020.spnlp-1.4,C96-1079,0,0.598061,"to be predicted is an instance of the semantic model capturing the meaning of a text. This involves the task of predicting the number of individuals of each class (cardinality prediction) as well as predicting the values of the key properties of each individual. Our proposed method relies on probabilistic graphical models i.e. conditional random fields (CRFs; (Lafferty et al., 2001; Sutton et al., 2012)) as their application is well established in many structured prediction tasks in the context of NLP. The task of slot-filling (SF) was first introduced in the Message Understanding Conference (Grishman and Sundheim, 1996). It is concerned with predicting an entity-centric structure having a set of relations to other entities as it can be found e.g. in ontology-based information extraction (SanchezCisneros and Aparicio Gali, 2013; Buitelaar et al., 2006) or extracting info-boxes from Wikipedia articles (Lange et al., 2010). Contrary to MCTC, classical slot-filling requires the prediction of a single structure per document only, which heavily reduces relational complexity and does not include nested individuals. There are many approaches to SF ranging from relying on distant supervision as described by Surdeanu"
2020.spnlp-1.4,N10-1061,0,0.0443933,"iven their properties. Durret et al. (Durrett et al., 2013) propose a global inference entity-level modeling for classical co-reference resolution based on a rich factor graph. In the unrolled factor graph, each factor refers to one entity property defined on a semantic or syntactic linguistic basis. In contrast to this work where properties of an individual/entity are pre-defined by the semantic model. Thus, our focus lies in their joint exploration while learning their interplay during inference in order to decide whether the properties belong to the same individual or not. Haghighi et al. (Haghighi and Klein, 2010) propose an unsupervised generative model incorporating several linguistic properties of the entity and its mention. In contrast, our work does not rely on entities that are explicitly mentioned in text. Instead, our model follows the schema of a semantic model to reason about the existence of individuals that can be inferred from the text and groups these individuals into groups by way of inferring the properties of these individuals. Our work is highly related to information extraction systems in the (bio-) medical field. When it comes e.g. to the prediction of key parameters of clinical stu"
2020.spnlp-1.4,P18-4012,1,0.825825,"ing the Cardinality: Aiming at cardinality prediction, we measure the compatibility of cardinality values in dependence of other random variables in ~y . For this, we make the choice of a cardinality dependent on n-grams appearing in the surface forms of property values. In addition, we also consider features implementing a prior for the cardinalities of classes as well as for the number of different values for multi-value properties. By this, the model is able to learn a class/property-specific distribution of cardinality values. For example, assuming that the cardinality 27 SANTO framework (Hartung et al., 2018). Annotations are available on the full level of relevant concepts of SCIO. Each document can be seen as a data point that is annotated with an instance of the semantic model containing a list of experimental groups and their properties. While annotations for the hasName property are linked to specific textual phrases in the document, all other properties are annotated in a distantly supervised fashion. In a preliminary step, we apply a named entity recognition heuristic based on automatically generated regular expressions to compute a set of documentbased annotations for all classes and prope"
2020.spnlp-1.4,S13-2104,0,0.077147,"Missing"
2020.spnlp-1.4,W16-1300,0,0.0745152,"Missing"
2020.spnlp-1.4,J01-4004,0,0.237526,"s on the empirical investigation and analysis of different approximative inference strategies based on Gibbs sampling. We present and evaluate our models on the task of extracting key parameters from scientific full text articles describing pre-clinical studies in the domain of spinal cord injury. 1 Introduction While there has been significant progress on information extraction tasks with a comparably low level of structural complexity such as entity recognition (Goulart et al., 2011; Nadeau and Sekine, 2007), relation extraction (Zhou et al., 2014; Kumar, 2017), and co-reference resolution (Soon et al., 2001; Ferracane et al., 2016), there is not much progress on capturing the comprehensive meaning of a text with respect to a given semantic model in terms of a given vocabulary of classes and properties. We refer to this task as model-complete text comprehension (MCTC) which requires to put all 1 We refer to mentions of entities in a text as entities and to the denotation of such entities in a given model of the text as individuals 22 Proceedings of 4th Workshop on Structured Prediction for NLP, pages 22–32 c November 20, 2020. 2020 Association for Computational Linguistics with the prediction of"
2020.spnlp-1.4,D15-1104,0,0.0291509,"iomedical field focusing on entities such as genes, diseases, treatments, etc. (Goulart et al., 2011). NER+L is an important preliminary step in many downstream applications as it identifies core informational units that are needed for more complex analysis levels including relation extraction, slot filling, and MCTC. Relation Extraction (RE) describes the task of detecting relations between entities mentioned in a text (Giuliano et al., 2007). While many models rely on a pipeline architecture predicting entities first and then predicting relations, more recent works model both tasks jointly (Luo et al., 2015). Although there has been notable progress on RE in the last years, the task has been typically restricted to extracting binary relations within single sentence boundaries only (Zhou et al., 2014). With our work, we strive to go beyond such simplifications towards document-level text interpretation with respect to a more complex model. Co-reference resolution (CRR) describes originally the task of finding nouns and pronouns that refer to the same underlying entity (Soon et al., 2001). When applying CRR to the medical field, the task shifts towards the resolution of mentions of diseases, tests,"
2020.spnlp-1.4,D17-1004,0,0.0136359,"ntities as it can be found e.g. in ontology-based information extraction (SanchezCisneros and Aparicio Gali, 2013; Buitelaar et al., 2006) or extracting info-boxes from Wikipedia articles (Lange et al., 2010). Contrary to MCTC, classical slot-filling requires the prediction of a single structure per document only, which heavily reduces relational complexity and does not include nested individuals. There are many approaches to SF ranging from relying on distant supervision as described by Surdeanu et al. (Surdeanu et al., 2010) to, more recently, neural approaches as described by Zhang et al. (Zhang et al., 2017). Finally, SF can be seen as an upstream process for (cold-start) knowledge base population as described by ter Horst et al. (ter Horst et al., 2018). Encoding Semantic Models: An instance of the semantic model is encoded as a nested vector ~y containing as many elements as there are classes and properties in the model. Thus, given a set of classes {C1 , . . . , Cn } and a set of properties P = 24 (MAP) inference as shown in Equation (1): ~yˆ = argmax p(~y |~x; θ) (1) ~ y ∈Y As inference in high dimensional vector spaces is often intractable, conditional random fields decompose the joint proba"
2021.argmining-1.19,D14-1226,0,0.0748302,"Missing"
2021.argmining-1.19,D17-1218,0,0.0271879,"as the duced by Bar-Haim et al. (2020a) is key point anal- summary of an argument (Petasis and Karkaletsis, ysis where the goal is to map a collection of argu1 https://2021.argmining.org/shared_task_ibm, last accessed: ments to a set of salient key points (say, high-level 2021-08-08 2 arguments) to provide a quantitative summary of The code is available under https://github.com/webis-de/ these arguments. ArgMining-21 184 Proceedings of The 8th Workshop on Argument Mining, pages 184–189 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics 2016; Daxenberger et al., 2017). Wang and Ling (2016) used a sequence-to-sequence model for the abstractive summarization of arguments from online debate portals. A complementary task of generating conclusions as informative argument summaries was introduced by Syed et al. (2021). Similar to Alshomary et al. (2020b) who inferred a conclusion’s target with a triplet neural network, we rely on contrastive learning here, using a siamese network though. Also, we build upon ideas of Alshomary et al. (2020a) who proposed a graph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main su"
2021.argmining-1.19,W16-2816,0,0.0243861,"ontrastive learning here, using a siamese network though. Also, we build upon ideas of Alshomary et al. (2020a) who proposed a graph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main supporting reason as an extractive summary. All these works represent the single-document summarization paradigm where only one argument is summarized at a time, whereas the given shared task is a multi-document summarization setting. The first approaches to multi-document argument summarization aimed to identify the main points of online discussions. Among these, Egan et al. (2016) grouped verb frames into pattern clusters that serve as input to a structured summarization pipeline, whereas Misra et al. (2016) proposed a more condensed approach by directly extracting argumentative sentences, summarized by similarity clustering. Bar-Haim et al. (2020a) continued this line of research by introducing the notion of key points and contributing the ArgsKP corpus, a collection of arguments mapped to manually-created key points. These key points are concise and selfcontained sentences that capture the gist of the arguments. Later, Bar-Haim et al. (2020b) proposed a quantitative"
2021.argmining-1.19,W04-1013,0,0.0128467,"ph-based Summarization Aspect Clustering 19.8 18.9 3.5 4.7 18.0 17.1 For the graph-based summarization model, we employed Spacy (Honnibal et al., 2020) to split the arguments into sentences. Similar to (Bar-Haim et al., 2020b), only sentences with a minimum of 5 and a maximum of 20 tokens, and not starting with a pronoun, were used for building the graph. Argument quality scores for each sentence were obtained from Project Debater’s API (Toledo et al., 2019)3 . We selected the thresholds for the parameters d, qual and match in Equation 1 as 0.2, 0.8 and 0.4 respectively, optimizing for ROUGE (Lin, 2004). In particular, we computed ROUGE-L between the ground-truth key points and the top 10 ranked sentences as our predictions, averaged over all the topic and stance combinations in the training split. We excluded sentences with a matching score higher than 0.8 with the selected candidates to minimize redundancy. For aspect clustering, we created 15 clusters per topic and stance combination. After greedy approximation of the candidate sentences, we removed redundant ones using a threshold of 0.65 for the normalized BERTScore (Zhang et al., 2020) with the previously selected candidates. Table 1:"
2021.argmining-1.19,D19-1387,0,0.0191228,"ss, matching findings of Syed et al. (2021). For the aspect clustering, we observe that the key points are more focused on specific aspects such as “disease” (for Pro) and “effectiveness” (for Con). In a real-world application, this may provide the flexibility to choose key points by aspects of interest to the end-user, especially with further improvement of aspect tagger by avoiding non-essential extracted phrases as “mandatory”. Hence, given the task of generating a quantitative summary of a collection of arguments, we believe that the graph-based summary provides We employed RoBERTa-large (Liu and Lapata, 2019) for encoding the tokens of the two inputs of key point matching to the siamese neural network, which acts as a mean-pooling layer and projects the encoder outputs (matrix of token embeddings) into a sentence embedding of size 768. We used Sentence-BERT (Reimers and Gurevych, 2019) to train our model for 10 epochs, with batch size 32, and maximum input length of 70, leaving all other parameters to their defaults. For automatic evaluation, we computed both strict and relaxed mean Average Precision (mAP) following Friedman et al. (2021). In cases where there is no majority label for matching, th"
2021.argmining-1.19,W16-3636,0,0.0248737,"aph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main supporting reason as an extractive summary. All these works represent the single-document summarization paradigm where only one argument is summarized at a time, whereas the given shared task is a multi-document summarization setting. The first approaches to multi-document argument summarization aimed to identify the main points of online discussions. Among these, Egan et al. (2016) grouped verb frames into pattern clusters that serve as input to a structured summarization pipeline, whereas Misra et al. (2016) proposed a more condensed approach by directly extracting argumentative sentences, summarized by similarity clustering. Bar-Haim et al. (2020a) continued this line of research by introducing the notion of key points and contributing the ArgsKP corpus, a collection of arguments mapped to manually-created key points. These key points are concise and selfcontained sentences that capture the gist of the arguments. Later, Bar-Haim et al. (2020b) proposed a quantitative argument summarization framework that automatically extracts key points from a set of arguments. Building upon this research, our"
2021.argmining-1.19,W16-2811,0,0.045117,"Missing"
2021.argmining-1.19,D19-1410,0,0.0185596,"oints by aspects of interest to the end-user, especially with further improvement of aspect tagger by avoiding non-essential extracted phrases as “mandatory”. Hence, given the task of generating a quantitative summary of a collection of arguments, we believe that the graph-based summary provides We employed RoBERTa-large (Liu and Lapata, 2019) for encoding the tokens of the two inputs of key point matching to the siamese neural network, which acts as a mean-pooling layer and projects the encoder outputs (matrix of token embeddings) into a sentence embedding of size 768. We used Sentence-BERT (Reimers and Gurevych, 2019) to train our model for 10 epochs, with batch size 32, and maximum input length of 70, leaving all other parameters to their defaults. For automatic evaluation, we computed both strict and relaxed mean Average Precision (mAP) following Friedman et al. (2021). In cases where there is no majority label for matching, the relaxed mAP considers them to be a match while the strict mAP considers them as not matching. In the development phase, we trained our model on the training split and evaluated on the validation split provided by the organizers. The strict and relaxed mAP on the validation set we"
2021.argmining-1.19,2021.naacl-main.34,0,0.0187054,"sentence to the final set of key points if its maximum matching score with the already selected candidates is below a certain threshold. Aspect Clustering Extracting key points is conceptually similar to identifying aspects (Bar-Haim Graph-based Summarization Following the et al., 2020a), which inspired our clustering apwork of Alshomary et al. (2020a), we first construct proach that selects representative sentences from an undirected graph with the arguments’ sentences multiple aspect clusters as the final key points. We as nodes. As a filtering step, we compute argument employ the tagger of Schiller et al. (2021) to exquality scores for each sentence as Toledo et al. tract the arguments’ aspects (on average, 2.1 as(2019) and exclude low-quality arguments from pects per argument). To tackle the lack of diversity, the graph. Next, we employ our key point match- we follow Heinisch and Cimiano (2021) and creing model (Section 4.1) to compute the edge weight ate k diverse aspect clusters by projecting the exbetween two nodes as the pairwise matching score tracted aspect phrases to an embedding space. Next, of the corresponding sentences. Only nodes with a we model the candidate selection of argument sensco"
2021.argmining-1.19,2020.coling-main.470,1,0.82388,"Missing"
2021.argmining-1.19,2021.findings-acl.306,1,0.802537,"Missing"
2021.argmining-1.19,D19-1564,0,0.0281845,"selection of argument senscore above a defined threshold are connected via tences as the set cover problem. Specifically, the 186 Approach R-1 R-2 R-L 5.2 Graph-based Summarization Aspect Clustering 19.8 18.9 3.5 4.7 18.0 17.1 For the graph-based summarization model, we employed Spacy (Honnibal et al., 2020) to split the arguments into sentences. Similar to (Bar-Haim et al., 2020b), only sentences with a minimum of 5 and a maximum of 20 tokens, and not starting with a pronoun, were used for building the graph. Argument quality scores for each sentence were obtained from Project Debater’s API (Toledo et al., 2019)3 . We selected the thresholds for the parameters d, qual and match in Equation 1 as 0.2, 0.8 and 0.4 respectively, optimizing for ROUGE (Lin, 2004). In particular, we computed ROUGE-L between the ground-truth key points and the top 10 ranked sentences as our predictions, averaged over all the topic and stance combinations in the training split. We excluded sentences with a matching score higher than 0.8 with the selected candidates to minimize redundancy. For aspect clustering, we created 15 clusters per topic and stance combination. After greedy approximation of the candidate sentences, we r"
2021.argmining-1.19,N16-1007,0,0.0282445,"t al. (2020a) is key point anal- summary of an argument (Petasis and Karkaletsis, ysis where the goal is to map a collection of argu1 https://2021.argmining.org/shared_task_ibm, last accessed: ments to a set of salient key points (say, high-level 2021-08-08 2 arguments) to provide a quantitative summary of The code is available under https://github.com/webis-de/ these arguments. ArgMining-21 184 Proceedings of The 8th Workshop on Argument Mining, pages 184–189 Punta Cana, Dominican Republic, November 10–11, 2021. ©2021 Association for Computational Linguistics 2016; Daxenberger et al., 2017). Wang and Ling (2016) used a sequence-to-sequence model for the abstractive summarization of arguments from online debate portals. A complementary task of generating conclusions as informative argument summaries was introduced by Syed et al. (2021). Similar to Alshomary et al. (2020b) who inferred a conclusion’s target with a triplet neural network, we rely on contrastive learning here, using a siamese network though. Also, we build upon ideas of Alshomary et al. (2020a) who proposed a graph-based model using PageRank (Page et al., 1999) that extracts the argument’s conclusion and the main supporting reason as an"
2021.argmining-1.3,W17-5112,0,0.0260673,"s conclusions and argument similarity ratings: we i) assess predictors of human argument similarity ratings to investigate the criteria that correlate with human ratings of argument similarity; ii) discuss potential advantages of using AMR for graph-based argumentation tasks in a concrete example, and iii) investigate how interpretable argument similarity computation can help assess the quality and usefulness of conclusions drawn from arguments in a reference-less conclusion evaluation setup.1 2 the analysis of linguistic aspects (Lauscher et al., 2021), e.g., by extracting selected features (Aker et al., 2017; Lugini and Litman, 2018) or leveraging discourse knowledge in language models (Opitz, 2019), others exploit large background knowledge graphs (Kobbe et al., 2019; Paul et al., 2020; Yuan et al., 2021) such as ConceptNet (Liu and Singh, 2004; Speer et al., 2017) or DBpedia (Mendes et al., 2012). An advantage of our approach is the explicit graph alignment between two arguments’ meaning graphs that better marks related structures, and that can help explain argument similarity judgements. Related work Argument similarity and search Assessing argument similarity is a key task in argument mining"
2021.argmining-1.3,D17-1070,0,0.0255921,"2 MATCHStruct . The first metric variant focuses on conceptual overlap (Fig. 1, middle), i.e. the more concrete semantic aspects, by putting a triple weight on concept matches. The second variant focuses on structural matches (Fig. 1, bottom), i.e., the more abstract semantic aspects, by putting triple weight on relation matches. Baselines We compare to previously established unsupervised baselines (Reimers et al., 2019): i) Tfidf calculates cosine similarity between Tfidf-weighted bag-of-word vectors; i) InferSent(FastText|Glove) leverages sentence embeddings produced by the InferSent model (Conneau et al., 2017) based on either FastText (Bojanowski et al., 2016) or GloVe (Pennington et al., 2014) vectors, which are compared with cosine similarity; iii) (GloVe|ELMo|BERT) Embedding uses averaged GloVe embeddings or averaged contextualized embeddings from ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) language models. 5.2 Results Best system Table 1 shows our main results. The AMR-based approach that is based on conceptfocused S2 MATCH scores, taking both the argument and its inferred conclusion into account, obtains rank 1 (68.70 macro F1) and outperforms all baselines, including the BERT ba"
2021.argmining-1.3,E17-1051,0,0.208134,"alcohol leads to depression vs. depression leads to consumption of alcohol are clearly distinct, while sharing the same concepts. Other AMR facets may also be useful. E.g., AMR captures coreferences and resolving them in different ways can induce significant meaning differences, Finally, AMR includes key semantic relations (location, cause, possession, etc.) that are often implicit or underspecified in language, hence their explicit representation in AMR provides a rich basis for assessing arguments. Arguments represented with AMR can be compared with AMR graph metrics (Cai and Knight, 2013; Damonte et al., 2017; Opitz et al., 2020) that also induce an explicit alignment between two argument graphs. As a complementary example, the similarity of two arguments may be reinforced by the similarity of their inferred conclusions, as shown below: i) Fracking can contaminate water and water wells and suck towns dry. ii) As a water-poor state, fracking and its toxic wastewater presents a serious danger to our communities and ecosystems. Arguments i) and ii) are rated as similar, presumably because they point at detrimental ramifications of fracking related to water issues. This similarity is likely to be refl"
2021.argmining-1.3,2021.eacl-main.17,0,0.0947016,"Missing"
2021.argmining-1.3,N19-1423,0,0.0048565,"weight on relation matches. Baselines We compare to previously established unsupervised baselines (Reimers et al., 2019): i) Tfidf calculates cosine similarity between Tfidf-weighted bag-of-word vectors; i) InferSent(FastText|Glove) leverages sentence embeddings produced by the InferSent model (Conneau et al., 2017) based on either FastText (Bojanowski et al., 2016) or GloVe (Pennington et al., 2014) vectors, which are compared with cosine similarity; iii) (GloVe|ELMo|BERT) Embedding uses averaged GloVe embeddings or averaged contextualized embeddings from ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) language models. 5.2 Results Best system Table 1 shows our main results. The AMR-based approach that is based on conceptfocused S2 MATCH scores, taking both the argument and its inferred conclusion into account, obtains rank 1 (68.70 macro F1) and outperforms all baselines, including the BERT baseline. The difference is significant with p &lt; 0.005 (Student t-test). This system is closely followed by other AMR-based systems, e.g., using concept-focused S2 MATCH that sees only the argument (68.17 macro Conclusion generator We generate conclusions from arguments using the T5 model (Raffel et al.,"
2021.argmining-1.3,2020.acl-main.399,0,0.0892293,"as mechanisms for explaining functions, strengths and weaknesses of arguments. Other research aims at studying the computational and formal aspects of argumentation, e.g. abstract argumentation (Dung, 1995) and Bayesian argumentation (Zenker, 2013). Research in empirical argument mining led researchers to investigate practical methods for explanations (Lawrence, 2021; Becker et al., 2021; Gunning et al., 2019; Rago et al., 2021; Vassiliades et al., 2021). While most approaches focus on Generation of argumentative conclusions The task of conclusion generation has been recently investigated by Alshomary et al. (2020, 2021), and allows us to infer conclusions from given premises. Conclusion generation can be seen as the inverse of argument generation (Sato et al., 2015; Schiller et al., 2020). In this work, we show that by considering conclusions inferred from pairs of arguments, we can improve our argument similarity ratings. 3 Hypotheses We base our models for explanatory argument similarity assessment on two hypotheses. Hypothesis I: Abstract Meaning Representation (Banarescu et al., 2013) of arguments supports explainable argument similarity assessment AMRs are directed, rooted and acyclic 1 https://g"
2021.argmining-1.3,W13-2322,0,0.283129,"ers et al., 2019; Lenz et al., 2019) and can enhance argument search (Maturana, 1988; Rissland et al., 1993; Wachsmuth et al., 2017; Ajjour et al., 2019; Chesnevar and Maguitman, 2004). Yet, while delivering solid performance on benchmarks, current methods fail to provide any deeper rationale for their predictions. It is thus not clear whether and to what extent spurious clues or other artifacts may influence the similarity decision (Opitz and Frank, 2019; Niven and Kao, 2019). In this paper, we aim at alleviating these issues by i) representing arguments with Abstract Meaning Representation (Banarescu et al., 2013) and conducting similarity assessment using well-defined graph metrics that provide explanatory AMR structure alignments; and ii) by investigating to what extent argument similarity can be projected to inferred conclusions. Argument mining with graphs There is growing interest in extracting graph structures from natural language arguments. Lenz et al. (2020), e.g., propose a pipeline for detecting and linking argumentative discourse units (ADUs). Al-Khatib et al. (2020) detect textual phrases and link them with POS/NEG relations, where POS indicates a positive influence and NEG a negative infl"
2021.argmining-1.3,2021.deelio-1.2,1,0.65493,".g., offers a theory of what is needed to make an argument complete (Toulmin, 2003). Argumentation schemes, which develop taxonomies of argument types and argumentation fallacies (Walton, 2005; Walton et al., 2008) can be viewed as mechanisms for explaining functions, strengths and weaknesses of arguments. Other research aims at studying the computational and formal aspects of argumentation, e.g. abstract argumentation (Dung, 1995) and Bayesian argumentation (Zenker, 2013). Research in empirical argument mining led researchers to investigate practical methods for explanations (Lawrence, 2021; Becker et al., 2021; Gunning et al., 2019; Rago et al., 2021; Vassiliades et al., 2021). While most approaches focus on Generation of argumentative conclusions The task of conclusion generation has been recently investigated by Alshomary et al. (2020, 2021), and allows us to infer conclusions from given premises. Conclusion generation can be seen as the inverse of argument generation (Sato et al., 2015; Schiller et al., 2020). In this work, we show that by considering conclusions inferred from pairs of arguments, we can improve our argument similarity ratings. 3 Hypotheses We base our models for explanatory argu"
2021.argmining-1.3,P13-2131,0,0.224136,"claims: consumption of alcohol leads to depression vs. depression leads to consumption of alcohol are clearly distinct, while sharing the same concepts. Other AMR facets may also be useful. E.g., AMR captures coreferences and resolving them in different ways can induce significant meaning differences, Finally, AMR includes key semantic relations (location, cause, possession, etc.) that are often implicit or underspecified in language, hence their explicit representation in AMR provides a rich basis for assessing arguments. Arguments represented with AMR can be compared with AMR graph metrics (Cai and Knight, 2013; Damonte et al., 2017; Opitz et al., 2020) that also induce an explicit alignment between two argument graphs. As a complementary example, the similarity of two arguments may be reinforced by the similarity of their inferred conclusions, as shown below: i) Fracking can contaminate water and water wells and suck towns dry. ii) As a water-poor state, fracking and its toxic wastewater presents a serious danger to our communities and ecosystems. Arguments i) and ii) are rated as similar, presumably because they point at detrimental ramifications of fracking related to water issues. This similarit"
2021.argmining-1.3,W19-4503,1,0.940331,"inisch2 1 Anette Frank1 lastname@cl.uni-heidelberg.de, {pheinisch,cimiano}@techfak.uni-bielefeld.de Abstract language models such as BERT (Devlin et al., 2019) or InferSent (Conneau et al., 2017). Two key advantages of such approaches are due to their unsupervised setup: First, unsupervised methods do not rely on human annotations, which are expensive and can be subject to noise and biases. Second, it has been shown for previous supervised methods that they have learned less about argumentation tasks than had been assumed, by exploiting spurious clues and artifacts from manually created data (Opitz and Frank, 2019; Niven and Kao, 2019). This has led to a recent interest in solving argumentation tasks in an unsupervised manner, e.g., by logical reasoning (Jo et al., 2021). In this paper we will highlight that previous methods for rating argument similarity suffer from a common flaw: beyond shallow statistics (word matches in bag-of-word models, or word similarities in distributional space), they do not provide any rationale for their predictions, and the prediction process is in general not transparent. Therefore, we know only little about the following question: When assessing the similarity of argumen"
2021.argmining-1.3,2021.eacl-main.129,1,0.841402,"Missing"
2021.argmining-1.3,2020.tacl-1.34,1,0.924772,"ession vs. depression leads to consumption of alcohol are clearly distinct, while sharing the same concepts. Other AMR facets may also be useful. E.g., AMR captures coreferences and resolving them in different ways can induce significant meaning differences, Finally, AMR includes key semantic relations (location, cause, possession, etc.) that are often implicit or underspecified in language, hence their explicit representation in AMR provides a rich basis for assessing arguments. Arguments represented with AMR can be compared with AMR graph metrics (Cai and Knight, 2013; Damonte et al., 2017; Opitz et al., 2020) that also induce an explicit alignment between two argument graphs. As a complementary example, the similarity of two arguments may be reinforced by the similarity of their inferred conclusions, as shown below: i) Fracking can contaminate water and water wells and suck towns dry. ii) As a water-poor state, fracking and its toxic wastewater presents a serious danger to our communities and ecosystems. Arguments i) and ii) are rated as similar, presumably because they point at detrimental ramifications of fracking related to water issues. This similarity is likely to be reflected in conclusions"
2021.argmining-1.3,W18-5208,0,0.0205579,"rgument similarity ratings: we i) assess predictors of human argument similarity ratings to investigate the criteria that correlate with human ratings of argument similarity; ii) discuss potential advantages of using AMR for graph-based argumentation tasks in a concrete example, and iii) investigate how interpretable argument similarity computation can help assess the quality and usefulness of conclusions drawn from arguments in a reference-less conclusion evaluation setup.1 2 the analysis of linguistic aspects (Lauscher et al., 2021), e.g., by extracting selected features (Aker et al., 2017; Lugini and Litman, 2018) or leveraging discourse knowledge in language models (Opitz, 2019), others exploit large background knowledge graphs (Kobbe et al., 2019; Paul et al., 2020; Yuan et al., 2021) such as ConceptNet (Liu and Singh, 2004; Speer et al., 2017) or DBpedia (Mendes et al., 2012). An advantage of our approach is the explicit graph alignment between two arguments’ meaning graphs that better marks related structures, and that can help explain argument similarity judgements. Related work Argument similarity and search Assessing argument similarity is a key task in argument mining (Reimers et al., 2019; Len"
2021.argmining-1.3,D14-1162,0,0.0855337,"ith 4 folds. In every iteration, 7 topics serve as testing data, while the other 21 topics serve to tune a decision threshold of the metric score.6 As in Reimers et al. (2019), we evaluate the F1 score for each of the two labels and the arithmetic F1 mean (macro F1). ARG1 ARG1 water endanger-01 ARG1 ARG1 ARG0 fracking ecosystem Figure 1: Standard, concept-focus and structure focus. AMR metric We use S2 MATCH (Opitz et al., 2020), which is based on the AMR graph matching metric S MATCH (Cai and Knight, 2013), but admits graded concept similarity by matching concept nodes with GloVe embeddings (Pennington et al., 2014) and cosine similarity4 . To find an optimal graph mapping, exactly like S MATCH, it leverages a hill-climber to approximate the NPhard problem of aligning AMR graphs. Following the alignment step, the (soft) matching of propositions (triples) are scored with an F1 score. Since, so-far, little is known about the trade-off and interface between concrete and abstract semantics in human mental representations (Mkrtychian et al., 2019), we introduce two more variants that assess similarity from complementary perspectives: S2 MATCHConcept and S2 MATCHStruct . The first metric variant focuses on con"
2021.argmining-1.3,N18-1202,0,0.00760878,"tic aspects, by putting triple weight on relation matches. Baselines We compare to previously established unsupervised baselines (Reimers et al., 2019): i) Tfidf calculates cosine similarity between Tfidf-weighted bag-of-word vectors; i) InferSent(FastText|Glove) leverages sentence embeddings produced by the InferSent model (Conneau et al., 2017) based on either FastText (Bojanowski et al., 2016) or GloVe (Pennington et al., 2014) vectors, which are compared with cosine similarity; iii) (GloVe|ELMo|BERT) Embedding uses averaged GloVe embeddings or averaged contextualized embeddings from ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) language models. 5.2 Results Best system Table 1 shows our main results. The AMR-based approach that is based on conceptfocused S2 MATCH scores, taking both the argument and its inferred conclusion into account, obtains rank 1 (68.70 macro F1) and outperforms all baselines, including the BERT baseline. The difference is significant with p &lt; 0.005 (Student t-test). This system is closely followed by other AMR-based systems, e.g., using concept-focused S2 MATCH that sees only the argument (68.17 macro Conclusion generator We generate conclusions from arguments usi"
2021.argmining-1.3,mendes-etal-2012-dbpedia,0,0.0171408,"Missing"
2021.argmining-1.3,P19-1459,0,0.306852,"lastname@cl.uni-heidelberg.de, {pheinisch,cimiano}@techfak.uni-bielefeld.de Abstract language models such as BERT (Devlin et al., 2019) or InferSent (Conneau et al., 2017). Two key advantages of such approaches are due to their unsupervised setup: First, unsupervised methods do not rely on human annotations, which are expensive and can be subject to noise and biases. Second, it has been shown for previous supervised methods that they have learned less about argumentation tasks than had been assumed, by exploiting spurious clues and artifacts from manually created data (Opitz and Frank, 2019; Niven and Kao, 2019). This has led to a recent interest in solving argumentation tasks in an unsupervised manner, e.g., by logical reasoning (Jo et al., 2021). In this paper we will highlight that previous methods for rating argument similarity suffer from a common flaw: beyond shallow statistics (word matches in bag-of-word models, or word similarities in distributional space), they do not provide any rationale for their predictions, and the prediction process is in general not transparent. Therefore, we know only little about the following question: When assessing the similarity of arguments, researchers typica"
2021.argmining-1.3,P19-1054,0,0.19062,"ements. Our approach provides significant performance improvements over strong baselines in a fully unsupervised setting. Finally, we make first steps to address the problem of reference-less evaluation of argumentative conclusion generations. 1 Philipp Cimiano2 Dept. of Computational Linguistics, Heidelberg University 2 CITEC, Bielefeld University 1 2 Philipp Wiesenbach1 • Which argument features correlate with human argument similarity decisions? In this work, we undertake a first attempt at answering this question, by testing two hypotheses: Introduction Rating the similarity of arguments (Reimers et al., 2019) is a core task in argument mining and argument search (Maturana, 1988; Wachsmuth et al., 2017; Ajjour et al., 2019). Argument similarity ratings are also needed for (case-based) argument retrieval (Rissland et al., 1993; Chesnevar and Maguitman, 2004), data exploration via argument clustering, and even automated debaters (Slonim et al., 2021): to counter an opponent’s argument, one may retrieve an argument similar to theirs, but of opposite stance to the topic (Wachsmuth et al., 2018). Typically, argument similarity ratings are computed over ‘bag-of-word’ argument representations, or else ove"
2021.argmining-1.3,2021.findings-acl.203,0,0.0298737,"cuss potential advantages of using AMR for graph-based argumentation tasks in a concrete example, and iii) investigate how interpretable argument similarity computation can help assess the quality and usefulness of conclusions drawn from arguments in a reference-less conclusion evaluation setup.1 2 the analysis of linguistic aspects (Lauscher et al., 2021), e.g., by extracting selected features (Aker et al., 2017; Lugini and Litman, 2018) or leveraging discourse knowledge in language models (Opitz, 2019), others exploit large background knowledge graphs (Kobbe et al., 2019; Paul et al., 2020; Yuan et al., 2021) such as ConceptNet (Liu and Singh, 2004; Speer et al., 2017) or DBpedia (Mendes et al., 2012). An advantage of our approach is the explicit graph alignment between two arguments’ meaning graphs that better marks related structures, and that can help explain argument similarity judgements. Related work Argument similarity and search Assessing argument similarity is a key task in argument mining (Reimers et al., 2019; Lenz et al., 2019) and can enhance argument search (Maturana, 1988; Rissland et al., 1993; Wachsmuth et al., 2017; Ajjour et al., 2019; Chesnevar and Maguitman, 2004). Yet, while"
2021.argmining-1.3,J17-3005,0,0.0271019,"conclusion into account, obtains rank 1 (68.70 macro F1) and outperforms all baselines, including the BERT baseline. The difference is significant with p &lt; 0.005 (Student t-test). This system is closely followed by other AMR-based systems, e.g., using concept-focused S2 MATCH that sees only the argument (68.17 macro Conclusion generator We generate conclusions from arguments using the T5 model (Raffel et al., 2020) pre-trained on summarization tasks. To encourage the model to generate informative conclusions (as opposed to summaries), we further finetune it on premise-conclusion samples from Stab and Gurevych (2017), which contain intelligible 4 Setup 5 For further detail on this fine-tuning step see Appendix. Strictly speaking, this is not a fully unsupervised setup, however, we stick to this framing of the task to facilitate comparison to the previous work (Reimers et al., 2019). 6 If the cosine similarity exceeds τ = 0.95. 27 Baselines Ours metric model type macro F1 score sim not sim rank human ? 78.34 74.74 81.94 0 random Tf-Idf InfSnt-fText InfSnt-GloVe GloVe Emb. ELMo Emb. BERT Embe. f (a, a0 ) f (a, a0 ) f (a, a0 ) f (a, a0 ) f (a, a0 ) f (a, a0 ) 48.01 61.18 66.21 64.94 64.68 64.47 65.39 34.31 5"
2021.argmining-1.3,W17-5106,0,0.174565,"fully unsupervised setting. Finally, we make first steps to address the problem of reference-less evaluation of argumentative conclusion generations. 1 Philipp Cimiano2 Dept. of Computational Linguistics, Heidelberg University 2 CITEC, Bielefeld University 1 2 Philipp Wiesenbach1 • Which argument features correlate with human argument similarity decisions? In this work, we undertake a first attempt at answering this question, by testing two hypotheses: Introduction Rating the similarity of arguments (Reimers et al., 2019) is a core task in argument mining and argument search (Maturana, 1988; Wachsmuth et al., 2017; Ajjour et al., 2019). Argument similarity ratings are also needed for (case-based) argument retrieval (Rissland et al., 1993; Chesnevar and Maguitman, 2004), data exploration via argument clustering, and even automated debaters (Slonim et al., 2021): to counter an opponent’s argument, one may retrieve an argument similar to theirs, but of opposite stance to the topic (Wachsmuth et al., 2018). Typically, argument similarity ratings are computed over ‘bag-of-word’ argument representations, or else over argument representations inferred with i) Representing arguments with Abstract Meaning Repre"
2021.argmining-1.3,P18-1023,0,0.0230075,"attempt at answering this question, by testing two hypotheses: Introduction Rating the similarity of arguments (Reimers et al., 2019) is a core task in argument mining and argument search (Maturana, 1988; Wachsmuth et al., 2017; Ajjour et al., 2019). Argument similarity ratings are also needed for (case-based) argument retrieval (Rissland et al., 1993; Chesnevar and Maguitman, 2004), data exploration via argument clustering, and even automated debaters (Slonim et al., 2021): to counter an opponent’s argument, one may retrieve an argument similar to theirs, but of opposite stance to the topic (Wachsmuth et al., 2018). Typically, argument similarity ratings are computed over ‘bag-of-word’ argument representations, or else over argument representations inferred with i) Representing arguments with Abstract Meaning Representations (AMRs) and using AMR graph metrics improves argument similarity rating and provides explanatory information. ii) Extending arguments with inferred conclusions can improve argument similarity rating. In the following §2 we discuss related work. §3 introduces our two key hypotheses, and §4 presents our argument similarity rating model and its implementation. In §5 we compare our model"
2021.starsem-1.10,2020.emnlp-main.11,0,0.0186345,"values from mentions of absolute ones. Similarly, the Open Book QA (Mihaylov et al., 2018) requires models to involve common sense knowledge to solve the task successfully. There are different ways to frame the task of answering questions by machine reading, including sentence retrieval (Momtazi and Klakow, 2015), multi-hop reasoning (Khot et al., 2020), and reasoning about multiple paragraphs or documents at the same time (Dua et al., 2019; Cao et al., 2019). Recent work has considered the development of reasoning-based QA systems (Weber et al., 2019) as well as the integration of external (Banerjee and Baral, 2020) and commonsense knowledge (Clark et al., 2020) into the QA process. Other machine reading based QA datasets focus on answering questions on the basis of strucDataset SQuAD 1.0, 2.0 NewsQA TriviaQA RACE Open Book QA LC-QuAD 2.0 WikiHop MedHop QASC QALD-9 SciQA DROP Domain Open Open Open Open Open Open Open Closed Open Open Closed Both Task Type Extractive QA Extractive QA Extractive QA Multiple Choice Multiple Choice Graph Retrieval Extractive QA Extractive QA Multiple Choice Graph Retrieval Document Retrieval Extractive QA Samples ~ 150,000 120,000 95,000 100,000 6,000 30,000 50,000 2,500 10,"
2021.starsem-1.10,N19-1240,0,0.0385061,"Missing"
2021.starsem-1.10,2020.lrec-1.677,0,0.0329518,"n-ended extractive QA include SQuAD 1.0 (Rajpurkar et al., 2016) and 2.0 (Rajpurkar et al., 2018), which have enjoyed wide popularity in the research community. Together with NewsQA (Trischler et al., 2017), these datasets represent the largest, crowd-sourced, extractive QA datasets available. Questions here are answered by correctly identifying a span in a context paragraph, with version 2.0 introducing a subclass of unanswerable questions. More recently, SQuAD versions in languages other than English have been developed (Croce et al., 2018; Mozannar et al., 2019; d’Hoffschmidt et al., 2020; Carrino et al., 2020). The TriviaQA (Joshi et al., 2017) dataset integrates the notion of external evidence (Wikipedia articles) for trivia and open domain question answering and broadens the task to information retrieval (IR) settings. Related Work Datasets on machine reading and question answering tasks can be characterized by broad categories: 106 The RACE dataset (Lai et al., 2017) relies on a multiple choice setting and leverages data used for the assessment of reading comprehension by humans. It features simple reasoning challenges such as deducing relative values from mentions of absolute ones. Similarly, t"
2021.starsem-1.10,2020.emnlp-main.549,0,0.0621168,"Missing"
2021.starsem-1.10,N19-1423,0,0.0326668,"SOTA) systems on these datasets have reached Exact Match (EM) and F1 performances of 90.9 and 93.2, respectively 1 . Some of these models even outperform the human baseline (which lies at F1 = 89.4 for SQuAD 2.0). It is unclear to which extent the existing benchmarks actually require systems to comprehend texts and to what extent these systems rely on surface cues signalling a match between question and answer span. Most state-of-the-art models rely on transformer models such as BERT and ALBERT (Lan et al., 2020) that are pre-trained on supplementary tasks using large amounts of textual data (Devlin et al., 2019) and employ extensive selfattention mechanisms that have been shown to learn many of the features of a classic natural language processing (NLP) pipeline (Tenney et al., 2019). It has been shown for a number of tasks that such models rely on surface cues and on artifacts of the datasets. A recent example is the Argument Reading and Comprehension (ARC) task (Habernal et al., 2018). A deeper analysis of the data and the performance of transformer models has shown that they exploit only surface cues and artefacts of the data, failing to perform beyond chance when systematic (adversarial) modifica"
2021.starsem-1.10,2020.findings-emnlp.107,0,0.039248,"Missing"
2021.starsem-1.10,N19-1246,0,0.0387671,"Missing"
2021.starsem-1.10,S18-1121,0,0.0267827,"match between question and answer span. Most state-of-the-art models rely on transformer models such as BERT and ALBERT (Lan et al., 2020) that are pre-trained on supplementary tasks using large amounts of textual data (Devlin et al., 2019) and employ extensive selfattention mechanisms that have been shown to learn many of the features of a classic natural language processing (NLP) pipeline (Tenney et al., 2019). It has been shown for a number of tasks that such models rely on surface cues and on artifacts of the datasets. A recent example is the Argument Reading and Comprehension (ARC) task (Habernal et al., 2018). A deeper analysis of the data and the performance of transformer models has shown that they exploit only surface cues and artefacts of the data, failing to perform beyond chance when systematic (adversarial) modifications are applied on the dataset (Niven and Kao, 2019). Our motivation in this paper is to introduce a new question answering dataset that requires a deeper understanding of the text to answer questions beyond merely matching answer spans. In particular, in our dataset, the answers to questions can often 1 Current performance of ‘FPNet (ensemble)‘ at the time of writing according"
2021.starsem-1.10,P17-1147,0,0.020401,".0 (Rajpurkar et al., 2016) and 2.0 (Rajpurkar et al., 2018), which have enjoyed wide popularity in the research community. Together with NewsQA (Trischler et al., 2017), these datasets represent the largest, crowd-sourced, extractive QA datasets available. Questions here are answered by correctly identifying a span in a context paragraph, with version 2.0 introducing a subclass of unanswerable questions. More recently, SQuAD versions in languages other than English have been developed (Croce et al., 2018; Mozannar et al., 2019; d’Hoffschmidt et al., 2020; Carrino et al., 2020). The TriviaQA (Joshi et al., 2017) dataset integrates the notion of external evidence (Wikipedia articles) for trivia and open domain question answering and broadens the task to information retrieval (IR) settings. Related Work Datasets on machine reading and question answering tasks can be characterized by broad categories: 106 The RACE dataset (Lai et al., 2017) relies on a multiple choice setting and leverages data used for the assessment of reading comprehension by humans. It features simple reasoning challenges such as deducing relative values from mentions of absolute ones. Similarly, the Open Book QA (Mihaylov et al., 2"
2021.starsem-1.10,D17-1082,0,0.0314056,"agraph, with version 2.0 introducing a subclass of unanswerable questions. More recently, SQuAD versions in languages other than English have been developed (Croce et al., 2018; Mozannar et al., 2019; d’Hoffschmidt et al., 2020; Carrino et al., 2020). The TriviaQA (Joshi et al., 2017) dataset integrates the notion of external evidence (Wikipedia articles) for trivia and open domain question answering and broadens the task to information retrieval (IR) settings. Related Work Datasets on machine reading and question answering tasks can be characterized by broad categories: 106 The RACE dataset (Lai et al., 2017) relies on a multiple choice setting and leverages data used for the assessment of reading comprehension by humans. It features simple reasoning challenges such as deducing relative values from mentions of absolute ones. Similarly, the Open Book QA (Mihaylov et al., 2018) requires models to involve common sense knowledge to solve the task successfully. There are different ways to frame the task of answering questions by machine reading, including sentence retrieval (Momtazi and Klakow, 2015), multi-hop reasoning (Khot et al., 2020), and reasoning about multiple paragraphs or documents at the s"
2021.starsem-1.10,W19-4612,0,0.0439506,"Missing"
2021.starsem-1.10,P19-1459,0,0.0205622,"n mechanisms that have been shown to learn many of the features of a classic natural language processing (NLP) pipeline (Tenney et al., 2019). It has been shown for a number of tasks that such models rely on surface cues and on artifacts of the datasets. A recent example is the Argument Reading and Comprehension (ARC) task (Habernal et al., 2018). A deeper analysis of the data and the performance of transformer models has shown that they exploit only surface cues and artefacts of the data, failing to perform beyond chance when systematic (adversarial) modifications are applied on the dataset (Niven and Kao, 2019). Our motivation in this paper is to introduce a new question answering dataset that requires a deeper understanding of the text to answer questions beyond merely matching answer spans. In particular, in our dataset, the answers to questions can often 1 Current performance of ‘FPNet (ensemble)‘ at the time of writing according to the SQuAD 2.0 leaderboard https: //rajpurkar.github.io/SQuAD-explorer/. 105 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 105–115 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics not be fou"
2021.starsem-1.10,P18-2124,0,0.0497674,"Missing"
2021.starsem-1.10,P19-1452,0,0.067152,"Missing"
2021.starsem-1.10,D18-1260,0,0.0198892,"oshi et al., 2017) dataset integrates the notion of external evidence (Wikipedia articles) for trivia and open domain question answering and broadens the task to information retrieval (IR) settings. Related Work Datasets on machine reading and question answering tasks can be characterized by broad categories: 106 The RACE dataset (Lai et al., 2017) relies on a multiple choice setting and leverages data used for the assessment of reading comprehension by humans. It features simple reasoning challenges such as deducing relative values from mentions of absolute ones. Similarly, the Open Book QA (Mihaylov et al., 2018) requires models to involve common sense knowledge to solve the task successfully. There are different ways to frame the task of answering questions by machine reading, including sentence retrieval (Momtazi and Klakow, 2015), multi-hop reasoning (Khot et al., 2020), and reasoning about multiple paragraphs or documents at the same time (Dua et al., 2019; Cao et al., 2019). Recent work has considered the development of reasoning-based QA systems (Weber et al., 2019) as well as the integration of external (Banerjee and Baral, 2020) and commonsense knowledge (Clark et al., 2020) into the QA proces"
2021.starsem-1.10,W17-2623,0,0.0334795,"Missing"
2021.starsem-1.10,P19-1618,0,0.0225882,"atures simple reasoning challenges such as deducing relative values from mentions of absolute ones. Similarly, the Open Book QA (Mihaylov et al., 2018) requires models to involve common sense knowledge to solve the task successfully. There are different ways to frame the task of answering questions by machine reading, including sentence retrieval (Momtazi and Klakow, 2015), multi-hop reasoning (Khot et al., 2020), and reasoning about multiple paragraphs or documents at the same time (Dua et al., 2019; Cao et al., 2019). Recent work has considered the development of reasoning-based QA systems (Weber et al., 2019) as well as the integration of external (Banerjee and Baral, 2020) and commonsense knowledge (Clark et al., 2020) into the QA process. Other machine reading based QA datasets focus on answering questions on the basis of strucDataset SQuAD 1.0, 2.0 NewsQA TriviaQA RACE Open Book QA LC-QuAD 2.0 WikiHop MedHop QASC QALD-9 SciQA DROP Domain Open Open Open Open Open Open Open Closed Open Open Closed Both Task Type Extractive QA Extractive QA Extractive QA Multiple Choice Multiple Choice Graph Retrieval Extractive QA Extractive QA Multiple Choice Graph Retrieval Document Retrieval Extractive QA Samp"
2021.starsem-1.10,Q18-1021,0,0.0261439,"a (QALD) evaluation campaigns (Usbeck et al., 2018), now in its 9th edition and going back to 2011. Solving the QALD tasks requires mapping natural language questions in multiple languages into a corresponding SPARQL query (Cimiano et al., 2013). While QALD provided only hundreds of training samples, recent datasets such as LC-QuAD 2.0 (Trivedi et al., 2017; Dubey et al., 2019) rely on automatic generation and human postprocessing to generate sufficient sample counts required for modern deep learning architectures. A similar approach is taken by QASC (Khot et al., 2020) or WikiHop and MedHop (Welbl et al., 2018) that are aimed at multi hop inference across multiple documents, finding answers directly from the KG without the need to generate a query. Most closed domain datasets are smaller than their open domain counterparts since annotation usually requires experts that are inherently harder to source. Datasets such as the biomedical question answering corpus BiQA (Lamurias et al., 2020) make use of user generated content from other sources instead. The recent DROP dataset (Dua et al., 2019) focuses on complex reasoning tasks in form of both, open and closed domain, questions. The task combines the c"
buitelaar-etal-2006-ontology,callmeier-etal-2004-deepthought,1,\N,Missing
cimiano-etal-2004-clustering,C92-2082,0,\N,Missing
cimiano-etal-2004-clustering,P99-1016,0,\N,Missing
cimiano-etal-2004-clustering,P90-1034,0,\N,Missing
cimiano-etal-2004-clustering,P93-1024,0,\N,Missing
D13-1179,P07-2045,0,0.00343338,"Missing"
D13-1179,2005.mtsummit-papers.11,0,0.05364,"Missing"
D13-1179,P10-1116,0,0.0732658,"Missing"
D13-1179,D09-1092,0,0.153583,"Missing"
D13-1179,palmer-etal-1998-rapid,0,0.0865569,"citly defined topics, but then computes latent relations between these. Thus, the method combines the benefits of both explicit and latent topic modelling approaches. We show that on a crosslingual mate retrieval task, our model significantly outperforms LDA, LSI, and ESA, as well as a baseline that translates every word in a document into the target language. 1 Introduction Cross-lingual document matching is the task of, given a query document in some source language, estimating the similarity to a document in some target language. This task has important applications in machine translation (Palmer et al., 1998; Tam et al., 2007), word sense disambiguation (Li et al., 2010) and ontology alignment (Spiliopoulos et al., 2007). An approach that has become quite popular in recent years for cross-lingual document matching is Explicit Semantics Analysis (ESA, Gabrilovich and Markovitch (2007)) and its cross-lingual extension A key choice in Explicit Semantic Analysis is the document space that will act as the topic space. The standard choice is to regard all articles from a background document collection – Wikipedia articles are a typical choice – as the topic space. However, it is crucial to ensure that"
E06-2010,callmeier-etal-2004-deepthought,1,0.811114,"n extraction see e.g. Maedche et al., 2002; Lopez and Motta, 2004; Müller et al., 2004; Nirenburg and Raskin, 2004). The SOBA system consists of a web crawler, linguistic annotation components and a component for the transformation of linguistic annotations into an ontology-based representation. The web crawler acts as a monitor on relevant web domains (i.e. the FIFA2 and UEFA3 web sites), automatically downloads relevant documents from them and sends them to a linguistic annotation web service. Linguistic annotation and information extraction is based on the Heart-of-Gold (HoG) architecture (Callmeier et al. 2004), which provides a uniform and flexible infrastructure for building multilingual applications that use semantics- and XML-based natural language processing components. The linguistically annotated documents are further processed by the transformation component, which generates a knowledge base of soccer-related entities (players, teams, etc.) and events (matches, goals, etc.) by mapping annotated entities or events to ontology classes and their properties. Finally, an automatic hyperlinking component is used for the visualization of extracted entities and events. This component is based on the"
E17-1006,W15-0103,0,0.0217548,"(Baroni and Zamparelli, 2010; Guevara, 2010). Focussing on adjective-noun compositionality, the latter authors propose instead to model adjective meaning as matrices encoding linear mappings between noun vectors. These attempts to integrate formal semantic principles in the tradition of Frege (1892) into a distributional framework have been generalized to a “program for compositional distributional semantics” (Baroni et al., Related Work Attribute Learning from Adjectives and Nouns. Adjective-centric approaches to attribute learning from text date back to Almuhareb (2006) and Cimiano (2006). Bakhshandeh and Allen (2015) present a sequence tagging model in order to extract attribute nouns from adjective glosses in WordNet. Most recently, Petersen and Hellwig (2016) use a clustering approach based on adjective-noun co-occurrences in order to induce clusters of German adjectives that constitute the 55 ~ , respectively, in one and the tors ~a, ~n and attr same embedding space S ⊆ Rd . By designing a composition function f (~a, ~n) that produces phrase representations p~ ∈ S, we can use nearest neighbour search in S in order d that is most likely to predict the attribute attr expressed in the compositional semant"
E17-1006,D10-1115,0,0.0337159,"(iv) We show that the same model also scales to the task of predicting semantic similarity of adjectivenoun phrases, which indicates both the robustness of the model and the importance of attribute meaning as a major source of phrase similarity. 2 Compositionality. Modelling compositional processes at the intersection of word and phrase meaning in distributional semantic models has attracted considerable attention in the last years (Erk, 2012). Mitchell and Lapata (2010) have promoted a variety of vector mixture models for the task, which have been criticized for their syntactic agnosticism (Baroni and Zamparelli, 2010; Guevara, 2010). Focussing on adjective-noun compositionality, the latter authors propose instead to model adjective meaning as matrices encoding linear mappings between noun vectors. These attempts to integrate formal semantic principles in the tradition of Frege (1892) into a distributional framework have been generalized to a “program for compositional distributional semantics” (Baroni et al., Related Work Attribute Learning from Adjectives and Nouns. Adjective-centric approaches to attribute learning from text date back to Almuhareb (2006) and Cimiano (2006). Bakhshandeh and Allen (2015)"
E17-1006,P14-1023,0,0.195871,"apturing lexical meaning in NLP tasks (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014), our goal in this paper is to model attribute meaning based on word embeddings. In particular, we use CBOW embeddings of adjectives and nouns (Mikolov et al., 2013a) as underlying word representations and train a compositionality function in order to compute a phrase representation that is predictive of the implicitly conveyed attribute meaning. In fact, word embeddings (also referred to as predict models) have been shown to be highly effective in a variety of lexical semantic tasks (Baroni et al., 2014b), compared to “traditional” distributional semantic models (or count models) in the tradition of Harris (1954). However, this finding has been refuted to a certain extent by Levy et al. (2015), stating that much of the perceived superiority of word embeddings is due to hyperparameter optimizations rather than principled advantages. Moreover, the authors found that in many cases, tailoring count models to a particular task at hand is both feasible and beneficial in order to outperform the more generic embeddings. Introduction Attributes such as SIZE, WEIGHT or COLOR are part of the building b"
E17-1006,W10-2805,0,0.0547733,"odel also scales to the task of predicting semantic similarity of adjectivenoun phrases, which indicates both the robustness of the model and the importance of attribute meaning as a major source of phrase similarity. 2 Compositionality. Modelling compositional processes at the intersection of word and phrase meaning in distributional semantic models has attracted considerable attention in the last years (Erk, 2012). Mitchell and Lapata (2010) have promoted a variety of vector mixture models for the task, which have been criticized for their syntactic agnosticism (Baroni and Zamparelli, 2010; Guevara, 2010). Focussing on adjective-noun compositionality, the latter authors propose instead to model adjective meaning as matrices encoding linear mappings between noun vectors. These attempts to integrate formal semantic principles in the tradition of Frege (1892) into a distributional framework have been generalized to a “program for compositional distributional semantics” (Baroni et al., Related Work Attribute Learning from Adjectives and Nouns. Adjective-centric approaches to attribute learning from text date back to Almuhareb (2006) and Cimiano (2006). Bakhshandeh and Allen (2015) present a sequen"
E17-1006,P15-1028,0,0.213514,"Missing"
E17-1006,C10-1049,1,0.549131,"equent linguistic pattern. In these constructions, attribute 54 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 54–64, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics value space of an attribute. However, their approach falls short of making the respective attribute explicit. These approaches have in common that they do not consider the compositional semantics of an adjective in its phrasal context with a noun in order to derive attribute meaning. This is in contrast to Hartung and Frank (2010; 2011b) who frame attribute selection in a distributional count model which (i) encodes adjectives and nouns as distributional word vectors over attributes as shared dimensions of meaning and (ii) uses vector mixture operations in order to compose these word vectors into phrase reresentations that are predictive of compositional attribute meaning. Tandon et al. (2014) propose a semi-supervised method for populating a knowledge base with triples of nouns, attributes and adjectives that are acquired from adjective-noun phrases. Being based on label propagation over monosemous adjectives as seed"
E17-1006,W11-2506,1,0.888148,"of a composed representation already outperforms both count models by a wide margin. This indicates a clear advantage of CBOW embeddings over count-based representations for capturing attribute meaning at the word level. However, this holds only for adjectives; noun embeddings in isolation perform much worse. This is confirmed by the dilation results: Dilating the noun representation into the direction of the adjective performs considerably better than vice versa, while there is no improvement beyond the non-compositional adjective baseline. These findings are in line with Hartung (2015) and Hartung and Frank (2011a) who also observed that adjective representations capture more of the compositional attribute semantics in adjective-noun phrases than noun representations do. Considering the trained composition models, we find that weighting either the adjective or the noun in a full additive model substantially outperforms the respective non-compositional baseline. The overall best results are obtained by assigning trained weights to both the adjective and the noun embedding (P@1=0.56). This model also outperforms weighted vector addition6 using scalar weights by great margins. Experiment 1: Large-scale A"
E17-1006,D11-1050,1,0.951657,"of a composed representation already outperforms both count models by a wide margin. This indicates a clear advantage of CBOW embeddings over count-based representations for capturing attribute meaning at the word level. However, this holds only for adjectives; noun embeddings in isolation perform much worse. This is confirmed by the dilation results: Dilating the noun representation into the direction of the adjective performs considerably better than vice versa, while there is no improvement beyond the non-compositional adjective baseline. These findings are in line with Hartung (2015) and Hartung and Frank (2011a) who also observed that adjective representations capture more of the compositional attribute semantics in adjective-noun phrases than noun representations do. Considering the trained composition models, we find that weighting either the adjective or the noun in a full additive model substantially outperforms the respective non-compositional baseline. The overall best results are obtained by assigning trained weights to both the adjective and the noun embedding (P@1=0.56). This model also outperforms weighted vector addition6 using scalar weights by great margins. Experiment 1: Large-scale A"
E17-1006,P16-1187,0,0.180429,"re general compositional processes in syntactic configurations (i.e., a single lexical function for all adjective-noun phrases). In line with these authors, we aim at learning a lexical function which captures attribute meaning in the compositional semantics of adjective-noun phrases, while generalizing over individual attributes. Contrary to distributional count models, there is relatively few work on applying word embeddings to linguistic problems or NLP tasks related to compositionality. Notable exceptions are Socher et al. (2013) for sentiment analysis, as well as Salehi et al. (2015) and Cordeiro et al. (2016) who focus on predicting the degree of compositionality in nominal compounds rather than carving out a particular semantic relation that is expressed in their compositional semantics. 3 3.1 d := arg max cos(~ ~ ) attr p, attr attr ∈A where p~ = f (~a, ~n), cos denotes cosine vector similarity and A the set of all attributes considered. The compositional functions that we use in this work can be divided into baseline models, largely derived from Mitchell and Lapata (2010), and trainable models. 3.2.1 Baseline Models Adjective or Noun. The simplest model is to skip any composition and just use t"
E17-1006,Q15-1016,0,0.0610701,"articular, we use CBOW embeddings of adjectives and nouns (Mikolov et al., 2013a) as underlying word representations and train a compositionality function in order to compute a phrase representation that is predictive of the implicitly conveyed attribute meaning. In fact, word embeddings (also referred to as predict models) have been shown to be highly effective in a variety of lexical semantic tasks (Baroni et al., 2014b), compared to “traditional” distributional semantic models (or count models) in the tradition of Harris (1954). However, this finding has been refuted to a certain extent by Levy et al. (2015), stating that much of the perceived superiority of word embeddings is due to hyperparameter optimizations rather than principled advantages. Moreover, the authors found that in many cases, tailoring count models to a particular task at hand is both feasible and beneficial in order to outperform the more generic embeddings. Introduction Attributes such as SIZE, WEIGHT or COLOR are part of the building blocks of representing knowledge about real-world entities or events (Barsalou, 1992). In natural language, formal attributes find their counterpart in attribute nouns which can be used in order"
E17-1006,D11-1129,0,0.0387378,"Missing"
E17-1006,D14-1162,0,0.117622,"ase similarity prediction. Moreover, as the model captures a generalized layer of attribute meaning, it bears the potential to be used for predictions over various attribute inventories without re-training. 1 (1) a. hot summer → TEMPERATURE b. hot debate → EMOTIONALITY c. hot soup → TASTE/TEMPERATURE Previous work on this task has largely been carried out in distributional semantic models (cf. Hartung (2015) for an overview). In the face of the recent rise of distributed neural representations as a means of capturing lexical meaning in NLP tasks (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014), our goal in this paper is to model attribute meaning based on word embeddings. In particular, we use CBOW embeddings of adjectives and nouns (Mikolov et al., 2013a) as underlying word representations and train a compositionality function in order to compute a phrase representation that is predictive of the implicitly conveyed attribute meaning. In fact, word embeddings (also referred to as predict models) have been shown to be highly effective in a variety of lexical semantic tasks (Baroni et al., 2014b), compared to “traditional” distributional semantic models (or count models) in the tradi"
E17-1006,C16-1267,0,0.176165,"eaning as matrices encoding linear mappings between noun vectors. These attempts to integrate formal semantic principles in the tradition of Frege (1892) into a distributional framework have been generalized to a “program for compositional distributional semantics” (Baroni et al., Related Work Attribute Learning from Adjectives and Nouns. Adjective-centric approaches to attribute learning from text date back to Almuhareb (2006) and Cimiano (2006). Bakhshandeh and Allen (2015) present a sequence tagging model in order to extract attribute nouns from adjective glosses in WordNet. Most recently, Petersen and Hellwig (2016) use a clustering approach based on adjective-noun co-occurrences in order to induce clusters of German adjectives that constitute the 55 ~ , respectively, in one and the tors ~a, ~n and attr same embedding space S ⊆ Rd . By designing a composition function f (~a, ~n) that produces phrase representations p~ ∈ S, we can use nearest neighbour search in S in order d that is most likely to predict the attribute attr expressed in the compositional semantics of an adjective-noun phrase p: 2014a) that is centered around functional application as the general process to model compositionality in semant"
E17-1006,N15-1099,0,0.0700979,"Missing"
E17-1006,N16-1060,0,0.0795133,"Missing"
E17-1006,D13-1170,0,0.00462674,"ual composition functions for each adjective in the corpus), or designed to capture general compositional processes in syntactic configurations (i.e., a single lexical function for all adjective-noun phrases). In line with these authors, we aim at learning a lexical function which captures attribute meaning in the compositional semantics of adjective-noun phrases, while generalizing over individual attributes. Contrary to distributional count models, there is relatively few work on applying word embeddings to linguistic problems or NLP tasks related to compositionality. Notable exceptions are Socher et al. (2013) for sentiment analysis, as well as Salehi et al. (2015) and Cordeiro et al. (2016) who focus on predicting the degree of compositionality in nominal compounds rather than carving out a particular semantic relation that is expressed in their compositional semantics. 3 3.1 d := arg max cos(~ ~ ) attr p, attr attr ∈A where p~ = f (~a, ~n), cos denotes cosine vector similarity and A the set of all attributes considered. The compositional functions that we use in this work can be divided into baseline models, largely derived from Mitchell and Lapata (2010), and trainable models. 3.2.1 Baseline Mod"
E17-1006,W11-0114,0,\N,Missing
E17-1006,2014.lilt-9.5,0,\N,Missing
ehrmann-etal-2014-representing,C08-2017,0,\N,Missing
ehrmann-etal-2014-representing,van-assem-etal-2006-conversion,0,\N,Missing
ehrmann-etal-2014-representing,mccrae-etal-2012-collaborative,1,\N,Missing
ehrmann-etal-2014-representing,Q14-1019,1,\N,Missing
ehrmann-etal-2014-representing,E12-1059,0,\N,Missing
ehrmann-etal-2014-representing,P13-1133,0,\N,Missing
ehrmann-etal-2014-representing,P14-1044,1,\N,Missing
ehrmann-etal-2014-representing,S13-2040,1,\N,Missing
ehrmann-etal-2014-representing,P14-1089,1,\N,Missing
ehrmann-etal-2014-representing,P14-1122,1,\N,Missing
ehrmann-etal-2014-representing,francopoulo-etal-2006-lexical,0,\N,Missing
K15-1016,W14-4203,0,0.0309241,"Missing"
K15-1016,W11-2104,0,0.0698541,"Missing"
K15-1016,D11-1033,0,0.0380362,"arallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows"
K15-1016,P11-1022,0,0.0132309,"dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing et al. (2003). The impact of different features has been analyzed by Shah et al. (2013). A complete system and framework for quality estimation (including a list of possible features) is QuEst (Specia et al., 2013). For an overview of other cross-lingual applications and methods, we refer to Bikel and Zitouni (2012). 5 Conclusion and Future Work We have presented an approach that alleviates the need of training data for a target language when adapting a fine-grained sentiment analysis system to a n"
K15-1016,banea-etal-2008-bootstrapping,0,0.0326727,"models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multilingual sentiment analysis (Denecke, 2008). Building dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing"
K15-1016,P11-2018,0,0.031699,"maller degree. This can as well be observed in the number of predictions the models based on different thresholds generate: While the number of true positive aspects for the coffee machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora"
K15-1016,C10-1004,0,0.0219846,"nte, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text 159 Wh"
K15-1016,R09-1010,0,0.024722,"ctly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text 159 While cross-lingual annotation projection has been investigated in the context of polarity computation, we are only aware of two approaches exploiting cross-lingual annotation projection on the task of identifying aspects specifically with an evaluation on manually annotated data in more than one language. The CLOpinionMiner (Zhou et al."
K15-1016,P13-2147,1,0.935166,"n training with the projected training data, there is no beneficial effect of the filtering. In the following, we describe our methodology in detail, including the description of the machine translation, annotation projection, and quality estimation methods (Section 2), and present the evaluation on manually annotated data (Section 3). Related work is discussed in Section 4. We conclude with Section 5 and mention promising future steps. 2 2.1 Methods Supervised Model for Aspect and Subjective Phrase Detection phrases and their relations. The structure follows the proposed pipeline approach by Klinger and Cimiano (2013).1 However, in contrast to their work, we focus on the detection of phrases only, and exploit the detection of relations only during inference, such that the detection of relations has an effect on the detection of phrases, but is not evaluated directly. The phrase detection follows the idea of semiMarkov conditional random fields (Sarawagi and Cohen, 2004; Yang and Cardie, 2012) and models phrases as spans over tokens as variables. Factor templates for spans of type aspect and subjective take into account token strings, prefixes, suffixes, the inclusion of digits, and part-of-speech tags, bot"
K15-1016,klinger-cimiano-2014-usage,1,0.94017,"jected data to train a supervised model crucially depends on the quality of the translations and alignments. In order to reduce the impact of spurious translations, we filter out low-quality sentence pairs. To estimate this quality, we take three measures into consideration (following approaches described by Shah and Specia (2014), in addition to a manual assessment of the translation quality as an upper baseline): 2 Note that the learning is independent from the actual value for all 0 &lt; α &lt; (maxg∈Corpus |g|)−1 . 3 www.statmt.org/moses/ 4 These example domains are taken from the USAGE corpus (Klinger and Cimiano, 2014), which is used in Section 3. 155 1. The probability of the sentence in the source language given a language model build on unannotated text in the source language (measuring if the language to be translated is typical, referred to as Source LM). 2. The probability of the machine translated sentence given a language model built on unanno5 https://cloud.google.com/translate/ # reviews en de coffee machine cutlery microwave toaster trash can vacuum cleaner washing machine dish washer 75 49 100 100 100 51 49 98 108 72 100 4 99 140 88 0 Table 1: Frequencies of the corpus used in our experiments (K"
K15-1016,2005.mtsummit-papers.11,0,0.073372,"require a corpus annotated for some source language and a translation from the source to a target language. As the availability of a parallel training corpus cannot be assumed in general, we use statistical machine translation (SMT) methods, relying on phrase-based translation models that use large amounts of parallel data for training (Koehn, 2010). While using an open-source system such as Moses3 would have been an option, we note that the quality would be limited by whether the system can be trained on a representative corpus. A standard dataset that SMT systems are trained on is EuroParl (Koehn, 2005). EuroParl covers 21 languages and contains 1.920.209 sentences for the pair German/English. The corpus includes only 4 sentences with the term “toaster”, 12 with “knives” (mostly in the context of violence), 6 with “dishwasher” (in the context of regulations) and 0 with “trash can”. The terms “camera” and “display” are more frequent, with 208 and 1186 mentions, respectively, but they never occur together.4 The corpus is thus not representative for product reviews as we consider in this paper. 2.3 Quality Estimation-based Instance Filtering The performance of an approach relying on projection"
K15-1016,J10-4005,0,0.0170986,"in and to a new language, corresponding training data is needed. In order to circumvent the need for additional training data when addressing a new language, we project training data automatically from a source to a target language. As input to our approach we require a corpus annotated for some source language and a translation from the source to a target language. As the availability of a parallel training corpus cannot be assumed in general, we use statistical machine translation (SMT) methods, relying on phrase-based translation models that use large amounts of parallel data for training (Koehn, 2010). While using an open-source system such as Moses3 would have been an option, we note that the quality would be limited by whether the system can be trained on a representative corpus. A standard dataset that SMT systems are trained on is EuroParl (Koehn, 2005). EuroParl covers 21 languages and contains 1.920.209 sentences for the pair German/English. The corpus includes only 4 sentences with the term “toaster”, 12 with “knives” (mostly in the context of violence), 6 with “dishwasher” (in the context of regulations) and 0 with “trash can”. The terms “camera” and “display” are more frequent, wi"
K15-1016,P14-2095,0,0.0156694,"is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which"
K15-1016,N13-1073,0,0.592047,"Proceedings of the 19th Conference on Computational Language Learning, pages 153–163, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Es gibt mit Sicherheit bessere Maschinen , aber die bietet das beste Preis-Leistungs-Verh¨altnis . There are certainly better machines , but o↵ers the best price-performance ratio . Figure 1: Example for the projection of an annotation from the source language to the target language. The translation has been generated with the Google translate API (https://cloud.google.com/translate/). The alignment is induced with FastAlign (Dyer et al., 2013). • We propose to use a supervised approach to induce a fine-grained sentiment analysis model to predict aspect and subjective phrases on some target language, given training data in some source language. This approach relies on automatic translation of source training data and projection of annotations to the target language data. • We present an instance selection method that only selects sentences with a certain translation quality. For this, we incorporate different measures of translation and alignment confidence. We show that such an instance selection method leads to increased performan"
K15-1016,D11-1006,0,0.0280377,"ranslation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multili"
K15-1016,D10-1101,0,0.0315454,"nerate: While the number of true positive aspects for the coffee machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples"
K15-1016,P10-2049,0,0.0249686,"nerate: While the number of true positive aspects for the coffee machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples"
K15-1016,P07-1034,0,0.0528929,"et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machin"
K15-1016,P07-1123,0,0.103217,"tilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been too much work on approaches to transfer a model either directly or via annotation projection in the area of sentiment analysis. One example is based on sentence level annotations which are automatically translated to yield a resource in another language. This approach has been proven to work well across several languages (Banea et al., 2010; Mihalcea et al., 2007; Balahur and Turchi, 2014). Recent work approached multilingual opinion mining on the above-mentioned multi-lingual Youtube corpus with tree kernels predicting the polarity of a comment and whether it concerns the product or the video in which the product is featured. (Severyn et al., 2015). Brooke et al. (2009) compare dictionary and classification transfer from English to Spanish in a similar classification setting. In-target-language training approaches for finegrained sentiment analysis include those targeting the extraction of phrases or modelling it as text 159 While cross-lingual annot"
K15-1016,H05-1043,0,0.0965831,"on different thresholds generate: While the number of true positive aspects for the coffee machine subdomain is 1100, only 221 are predicted with a threshold of the manual quality assignment of 0. However, a treshold of 9 leads to 560 predictions and a threshold of 10 to 1291. This effect can be observed for subjective phrases as well. It increases from 465 to 827 while the gold number is 676. These observations hold for all filtering methods analogously. 4 Related Work classification (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012; Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English reso"
K15-1016,2014.eamt-1.22,0,0.022512,"occur together.4 The corpus is thus not representative for product reviews as we consider in this paper. 2.3 Quality Estimation-based Instance Filtering The performance of an approach relying on projection of training data from a source to a target language and using this automatically projected data to train a supervised model crucially depends on the quality of the translations and alignments. In order to reduce the impact of spurious translations, we filter out low-quality sentence pairs. To estimate this quality, we take three measures into consideration (following approaches described by Shah and Specia (2014), in addition to a manual assessment of the translation quality as an upper baseline): 2 Note that the learning is independent from the actual value for all 0 &lt; α &lt; (maxg∈Corpus |g|)−1 . 3 www.statmt.org/moses/ 4 These example domains are taken from the USAGE corpus (Klinger and Cimiano, 2014), which is used in Section 3. 155 1. The probability of the sentence in the source language given a language model build on unannotated text in the source language (measuring if the language to be translated is typical, referred to as Source LM). 2. The probability of the machine translated sentence given"
K15-1016,2013.mtsummit-papers.21,0,0.0151545,"a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing et al. (2003). The impact of different features has been analyzed by Shah et al. (2013). A complete system and framework for quality estimation (including a list of possible features) is QuEst (Specia et al., 2013). For an overview of other cross-lingual applications and methods, we refer to Bikel and Zitouni (2012). 5 Conclusion and Future Work We have presented an approach that alleviates the need of training data for a target language when adapting a fine-grained sentiment analysis system to a new language. Our approach relies on training data available for a source language and on automatic machine translation, in particular statistical methods, to project training data to t"
K15-1016,D08-1109,0,0.0198004,"gaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of aut"
K15-1016,D12-1122,0,0.143617,"ude with Section 5 and mention promising future steps. 2 2.1 Methods Supervised Model for Aspect and Subjective Phrase Detection phrases and their relations. The structure follows the proposed pipeline approach by Klinger and Cimiano (2013).1 However, in contrast to their work, we focus on the detection of phrases only, and exploit the detection of relations only during inference, such that the detection of relations has an effect on the detection of phrases, but is not evaluated directly. The phrase detection follows the idea of semiMarkov conditional random fields (Sarawagi and Cohen, 2004; Yang and Cardie, 2012) and models phrases as spans over tokens as variables. Factor templates for spans of type aspect and subjective take into account token strings, prefixes, suffixes, the inclusion of digits, and part-of-speech tags, both as full string and as bigrams, for the spans and their vicinity. In addition, the length of the span is modeled by cumulative binning. The relation template indicates how close an aspect is to a subjective phrase based on token distance and on the length of the shortest path in the dependency tree. The edge names of the shortest path are also included as features. It is further"
K15-1016,N01-1026,0,0.101386,"transfered to Chinese. Models are further improved by cotraining. Xu et al. (2013) perform self-training based on a projected corpus from English to Chinese to detect opinion holders. Due to the lack of existing manually annotated resources, to our knowledge no cross-language projection approach for fine-grained annotation at the level of aspect and subjective phrases has been proposed before. The projection of annotated data sets has been investigated in a variety of applications. Early work includes an approach to the projection of part-ofspeech tags and noun phrases (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and parsing information (Hwa et al., 2005) on a parallel corpus. Especially in syntactic and semantic parsing, heuristics to remove or correct spuriously projected annotations have been developed (Pad´o and Lapata, 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations,"
K15-1016,H01-1035,0,0.0640615,"lish data set which is transfered to Chinese. Models are further improved by cotraining. Xu et al. (2013) perform self-training based on a projected corpus from English to Chinese to detect opinion holders. Due to the lack of existing manually annotated resources, to our knowledge no cross-language projection approach for fine-grained annotation at the level of aspect and subjective phrases has been proposed before. The projection of annotated data sets has been investigated in a variety of applications. Early work includes an approach to the projection of part-ofspeech tags and noun phrases (Yarowsky et al., 2001; Yarowsky and Ngai, 2001) and parsing information (Hwa et al., 2005) on a parallel corpus. Especially in syntactic and semantic parsing, heuristics to remove or correct spuriously projected annotations have been developed (Pad´o and Lapata, 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available contain"
K15-1016,I08-3008,0,0.0335368,"emantic parsing, heuristics to remove or correct spuriously projected annotations have been developed (Pad´o and Lapata, 2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive imp"
K15-1016,P11-2120,0,0.0250719,"In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai, 2007). Other work has attempted to exploit information available in multiple languages to induce a model for a language for which sufficient training data is not available. For instance, universal tag sets take advantage of annotations that are aligned across languages (Snyder et al"
K15-1016,P13-4014,0,0.0293197,"Missing"
K15-1016,P14-1024,0,0.0222151,"2009; Agi´c et al., 2014). It is typical for these approaches to be applied on existing parallel corpora (one counter example is the work by Basili et al. (2009) who perform postprocessing of machine translated resources to improve the annotation for training semantic role labeling models). In cases in which no such parallel resources are available containing pertinent annotations, models can be transfered after training. Early work includes a cross-lingual parser adaption (Zeman and Resnik, 2008). A recent example is the projection of a metaphor detection model using a bilingual dictionary (Tsvetkov et al., 2014). A combination of model transfer and annotation projection for dependency parsing has been proposed by Kozhevnikov and Titov (2014). To improve quality of the overall corpus of projected annotations, the selection of data points for dependency parsing has been studied (Søgaard, 2011). Similarly, Axelrod et al. (2011) improve the average quality of machine translation systems by selection of promising training examples and show that such a selection approach has a positive impact. Related to the latter, a generic instance weighting scheme has been proposed for domain adaptation (Jiang and Zhai"
K15-1016,2003.mtsummit-papers.52,0,0.033574,"., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output probability, number of unknown words of a target language as well as parsing-based features have been used (Avramidis et al., 2011). The alignment context can also be taken into account (Bach et al., 2011). An overview on confidence measures for machine translation is for instance provided by Ueffing et al. (2003). The impact of different features has been analyzed by Shah et al. (2013). A complete system and framework for quality estimation (including a list of possible features) is QuEst (Specia et al., 2013). For an overview of other cross-lingual applications and methods, we refer to Bikel and Zitouni (2012). 5 Conclusion and Future Work We have presented an approach that alleviates the need of training data for a target language when adapting a fine-grained sentiment analysis system to a new language. Our approach relies on training data available for a source language and on automatic machine tra"
K15-1016,uryupina-etal-2014-sentube,0,0.0243616,"otated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger, 2009). A Chinese corpus annotated at the aspect and subjective phrase level is described by Zhao et al. (2014). There has not been"
K15-1016,P09-1027,0,0.0726727,"at are aligned across languages (Snyder et al., 2008). Delexicalization allows for applying a model to other languages (McDonald et al., 2011). Focusing on cross-lingual sentiment analysis, joint training of classification models on multiple languages shows an improvement over separated models. Balahur and Turchi (2014) analyzed the impact of using different machine translation approaches in such settings. Differences in sentiment expressions have been analyzed between English and Dutch (Bal et al., 2011). Co-training with nonannotated corpora has been shown to yield good results for Chinese (Wan, 2009). Ghorbel (2012) analyzed the impact of automatic translation on sentiment analysis. Finally, SentiWordNet has been used for multilingual sentiment analysis (Denecke, 2008). Building dictionaries for languages with scarce resources can be supported by bootstrapping approaches (Banea et al., 2008). Estimating the quality of machine translation can be understood as a ranking problem and thus be modeled as regression or classification. An important research focus is on investigating the impact of different features on predicting translation quality. For instance, sentence length, the output proba"
K15-1016,W05-0308,0,0.00961569,"i et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010b). Such models are typically trained or optimized on manually annotated data (Klinger and Cimiano, 2013; Yang and Cardie, 2012; Jakob and Gurevych, 2010a; Zhang et al., 2011). The necessary data, at least containing fine-grained annotations for aspects and subjective phrases instead of only an overall polarity score, are mainly available for the English language to a sufficient extent. Popular corpora used for training are for instance the J.D. Power and Associates Sentiment Corpora (Kessler et al., 2010) or the MPQA corpora (Wilson and Wiebe, 2005). Non-English resources are scarce. Examples are a YouTube corpus consisting of English and Italian comments (Uryupina et al., 2014), a not publicly available German Amazon review corpus of 270 sentences (Boland et al., 2013), in addition to the USAGE corpus (Klinger and Cimiano, 2014) we have used in this work, consisting of German and English reviews. The (non-fine-grained annotated) Spanish TASS corpus consists of Twitter messages (Saralegi and Vicente, 2012). The “Multilingual Subjectivity Analysis Gold Standard Data Set” focuses on subjectivity in the news domain (Balahur and Steinberger,"
K15-1016,H05-2017,0,\N,Missing
klinger-cimiano-2014-usage,N12-1085,0,\N,Missing
klinger-cimiano-2014-usage,N06-4006,0,\N,Missing
klinger-cimiano-2014-usage,ruppenhofer-etal-2008-finding,0,\N,Missing
klinger-cimiano-2014-usage,P11-2100,0,\N,Missing
klinger-cimiano-2014-usage,P04-1035,0,\N,Missing
klinger-cimiano-2014-usage,J09-3003,0,\N,Missing
klinger-cimiano-2014-usage,clematide-etal-2012-mlsa,0,\N,Missing
klinger-cimiano-2014-usage,P08-1036,0,\N,Missing
klinger-cimiano-2014-usage,P10-1059,0,\N,Missing
klinger-cimiano-2014-usage,P02-1053,0,\N,Missing
klinger-cimiano-2014-usage,P13-2147,1,\N,Missing
klinger-cimiano-2014-usage,S13-2052,0,\N,Missing
L16-1386,2016.gwc-1.9,1,0.821562,"gy, since they are used as domain specific ontologies. Additionally, other supporting ontologies have been added, such as GeoNames for the named entities; PROTON as an upper ontology; SKOS as a mapper between ontologies and terminological lexicons; Dublin Core as a metadata ontology. Also, for the purposes of search, Web Interface Querying EUCases Linking Platform was designed. For its Web Interface, the EUCases Linking Platform relies on a customized version of the GraphDB Workbench27 , developed by Ontotext AD. 5.2. Wordnet Interlingual Index (ILI) A recent development (Vossen et al., 2016; Bond et al., 2016) has been the adoption of LLOD technology by the wordnet community, with a new plan that uses LLOD as the basic mechanism for the creation of links between wordnets in different languages. This Collaborative InterLingual Index enables wordnets to share and link their resources for concepts lexicalized in any of the group’s languages. This was supported directly by a workshop at the 2016 Global WordNet Conference and will lead to the adoption of LLOD technology by a new community. In addition, the open multilingual wordnet (Bond et al., 2014) provides all open wordnets for download using the le"
L16-1386,calzolari-etal-2012-lre,0,0.0712075,"Missing"
L16-1386,ehrmann-etal-2014-representing,1,0.804428,"lable by attempting to download it and discarding all resources that are no longer available. We have attempted to notify the authors of resources that no longer meet the criteria for inclusion in the cloud. However, our experience has been that this did not motivate many authors to update their resources. 2.5. Vocabularies The Linguistic Linked Open Data Cloud has grown significantly in the last few years and most notably, unlike the non-linguistic LOD Cloud, is not centered around one nucleus but instead has used many different vocabularies and datasets to link to. Among these are BabelNet (Ehrmann et al., 2014), LexInfo (Cimiano et al., 2011), and Lexvo (de Melo, 2015). In addition, a number of new vocabularies have emerged including the OntoLex model,13 the NLP Interchange format NIF (Hellmann et al., 2013), the WordNet Interlingual Index (Sect. 5.2.), and the FrameBase schema (Rouces et al., 2015a) (Sect. 5.3.). These vocabularies have increased the power of linked data to represent the complete spectrum of language resources and show that new resources can be created that use the power of linked data to link across different types of languages resources, such as terminologies and dictionaries (Si"
L16-1386,federmann-etal-2012-meta,0,0.0601026,"Missing"
L16-1386,W15-4205,1,0.920405,"not necessarily created for this purpose, e.g., large collections of texts such as news articles, terminological or encyclopedic and general-purpose knowledge bases such as DBpedia (Bizer et al., 2009), or metadata collections. 2.2. Infrastructure and Metadata The OWLG provides guidelines to data publishers on how to include their resources in the LLOD cloud.6 The cloud diagram is currently generated from metadata maintained at DataHub7 and hence contains only resources described in DataHub. An alternative metadata repository specialized for linguistic resources is under development: Linghub (McCrae et al., 2015a).8 It aims to provide a search engine and index for linguistic resources and attempts to harmonize metadata from a number of different sources, including Metashare (Federmann et al., 2012), CLARIN VLO (Van Uytvanck et al., 2012), DataHub and LRE Map (Calzolari et al., 2012). It will soon replace DataHub in the generation of the cloud diagram. LingHub, 4 http://lod2.eu/ http://qtleap.eu/ 6 http://wiki.okfn.org/Working_Groups/ Linguistics/How_to_contribute 7 http://datahub.io 8 http://linghub.org 5 Datasets Links 28 53 103 126 128 41 78 167 203 209 February 2012 September 2013 November 2014 Ma"
L16-1386,W15-4201,0,0.02663,"Missing"
L16-1386,W15-4207,1,0.813122,"4), LexInfo (Cimiano et al., 2011), and Lexvo (de Melo, 2015). In addition, a number of new vocabularies have emerged including the OntoLex model,13 the NLP Interchange format NIF (Hellmann et al., 2013), the WordNet Interlingual Index (Sect. 5.2.), and the FrameBase schema (Rouces et al., 2015a) (Sect. 5.3.). These vocabularies have increased the power of linked data to represent the complete spectrum of language resources and show that new resources can be created that use the power of linked data to link across different types of languages resources, such as terminologies and dictionaries (Siemoneit et al., 2015) and corpora and dictionaries (McGovern et al., 2015). 3. OWLG members have been very active in promoting the development and adoption of linguistic linked data, which had an effect not only in the growth of the LLOD cloud but in the development of representation models, guidelines, and best practices. These activities have been developed in the context of a number of W3C groups and projects, as it is detailed in the rest of this section. 13 12 http://lodvader.aksw.org/ Other Community Group Efforts http://cimiano.github.io/ontolex/ specification.html 2437 3.1. OntoLex 8. LLOD aware services 1"
L16-1386,van-uytvanck-etal-2012-semantic,0,0.0695217,"Missing"
L16-1549,W12-1814,1,0.0640968,"Missing"
L16-1549,H93-1016,0,0.149821,"Missing"
L16-1549,W10-4341,1,0.0408093,"Missing"
L16-1549,wittenburg-etal-2006-elan,0,0.0345379,"Missing"
L16-1554,W10-0710,0,0.0187502,"lem. Alternatively, ontology lexicons could be induced automatically (Walter et al., 2014) or generated by means of translating an already existing lexicon (McCrae et al., 2011a; Arcan and Buitelaar, 2013); however, those methods have not yet 3477 reached an accuracy sufficient to produce high-quality lexicons off the shelf. In this paper we explore a further option, namely making use of crowdsourcing (Howe, 2006; Quinn and Bederson, 2011), which in recent years has already been used for a number of different tasks related both to natural language processing and ontologies (Snow et al., 2008; Ambati and Vogel, 2010; Acosta et al., 2013). To our knowledge, so far there exist no reports on using crowdsourcing specifically for the generation of ontology lexicons, and hence whether ontology lexicons of good quality can be generated this way at acceptable costs is an open question. In the following, we will present an approach to generating a Japanese ontology lexicon for DBpedia by means of crowdsourcing, which can also be applied both to other languages and other ontologies. 2. 2.1. Methodology Overall Workflow A particular challenge when trying to crowdsource ontology lexicons is finding a task design tha"
L16-1554,N13-2006,0,0.214651,"e ontology. Furthermore, in order to decide which verbalizations for a given ontology element are appropriate, often the language proficiency of native speakers will be necessary; hence, one either needs to have a very good command of the target language oneself, preferably at native speaker level, or one should at least be able to consult with native speakers, which in case of smaller target languages may pose a problem. Alternatively, ontology lexicons could be induced automatically (Walter et al., 2014) or generated by means of translating an already existing lexicon (McCrae et al., 2011a; Arcan and Buitelaar, 2013); however, those methods have not yet 3477 reached an accuracy sufficient to produce high-quality lexicons off the shelf. In this paper we explore a further option, namely making use of crowdsourcing (Howe, 2006; Quinn and Bederson, 2011), which in recent years has already been used for a number of different tasks related both to natural language processing and ontologies (Snow et al., 2008; Ambati and Vogel, 2010; Acosta et al., 2013). To our knowledge, so far there exist no reports on using crowdsourcing specifically for the generation of ontology lexicons, and hence whether ontology lexicon"
L16-1554,W10-0717,0,0.0229197,"the highest quality group to work on our tasks, which is what CrowdFlower advises for tasks one cannot formulate test questions for.11 Furthermore, for the translation task we experimented with different settings for the country of origin and language skills of allowed workers, as will be described in more detail in Section 3.2. 2.3. 3.2. Quality Control As for a task such as ours one cannot really formulate test questions, which form CrowdFlower’s main mechanism of quality control, we made use of a number of alternative control mechanisms that are commonly used with translationrelated tasks (Irvine and Klementiev, 2010; Zaidan and Callison-Burch, 2011): Generally, a good strategy for discouraging people from cheating is to design one’s task in a way that makes cheating as laborious and time-consuming as actually working on the job at hand. Therefore, we showed the English seed sentences — and, in case of the later evaluation stage, the Japanese candidate translations — to the workers in the form of images rather than text, so as to make it more difficult for people to make use of automatic translation services. This measure serves to prevent automatic translations, but it does not help against workers who e"
L16-1554,W11-1013,1,0.896672,"Missing"
L16-1554,D08-1027,0,0.0911274,"Missing"
L16-1554,P11-1122,0,0.314981,"the translation task easier for the crowdsourcing workers, but will probably allow us to eventually extract the Japanese verbalizations from the translated sentences automatically by means of the M-ATOLL framework (Walter et al., 2014). One obvious challenge with a crowdsourcing task such as this one, where more than one correct answer may exist for a given piece of input data and there is no straightforward way to automatically check the validity of the answers provided by the workers, is quality control. We adopt an approach that is commonly used in translation-related crowdsourcing tasks (Zaidan and Callison-Burch, 2011; Benjamin and Radetzky, 2014) and which involves soliciting multiple translations per English sentence from distinct workers, plus a second crowdsourcing stage in which Japanese workers will be asked to evaluate the translations received in the first stage. Based on these evaluations we can then decide which translations most probably include commonly accepted Japanese verbalizations of ontology elements that should be included in a Japanese ontology lexicon for DBpedia. As an example, assume we have shown the sentence Miguel de Cervantes wrote Don Quixote to three separate workers during the"
mccrae-etal-2012-collaborative,kemps-snijders-etal-2008-isocat,0,\N,Missing
mccrae-etal-2012-collaborative,W10-0719,0,\N,Missing
mccrae-etal-2012-collaborative,D08-1027,0,\N,Missing
mccrae-etal-2012-collaborative,W07-1501,0,\N,Missing
mccrae-etal-2012-collaborative,W09-1904,0,\N,Missing
mccrae-etal-2012-collaborative,francopoulo-etal-2006-lexical,0,\N,Missing
mccrae-etal-2012-collaborative,zesch-etal-2008-extracting,0,\N,Missing
mccrae-etal-2012-collaborative,van-assem-etal-2006-conversion,0,\N,Missing
N15-1088,D11-1131,0,0.0730455,"Missing"
N15-1088,P11-1149,0,0.0174224,"e utteranceor word level, which is costly and time-consuming to produce. In contrast, aiming to reduce the required manual effort, in this paper we explore the utility of weak supervision in the form of ambiguous context information for the induction of grammars applicable for both speech recognition and understanding. The utility of this kind of weak supervision has been explored previously in the field of semantic parsing (Chen et al., 2010; B¨orschinger et al., 2011; Chen and Mooney, 2008), and unsupervised approaches to semantic parsing have been proposed as well (Poon and Domingos, 2009; Goldwasser et al., 2011). While such approaches may be applied as parsing components for SLU systems – notice though that the SLU task differs from parsing of written text in that recognition errors and phenomena of spoken language must be handled, and that not all SLU models can be applied as an LM (Wang et al., 2011) – we are not aware of work aiming to transform these parsers into speech recognition grammars or investigating their performance with respect to different LMs applied with an ASR. Semantic parsers applied in pipeline-based SLU systems are in general usually learned in a supervised fashion. Other than s"
N15-1088,D09-1001,0,0.0206277,"which is annotated at the utteranceor word level, which is costly and time-consuming to produce. In contrast, aiming to reduce the required manual effort, in this paper we explore the utility of weak supervision in the form of ambiguous context information for the induction of grammars applicable for both speech recognition and understanding. The utility of this kind of weak supervision has been explored previously in the field of semantic parsing (Chen et al., 2010; B¨orschinger et al., 2011; Chen and Mooney, 2008), and unsupervised approaches to semantic parsing have been proposed as well (Poon and Domingos, 2009; Goldwasser et al., 2011). While such approaches may be applied as parsing components for SLU systems – notice though that the SLU task differs from parsing of written text in that recognition errors and phenomena of spoken language must be handled, and that not all SLU models can be applied as an LM (Wang et al., 2011) – we are not aware of work aiming to transform these parsers into speech recognition grammars or investigating their performance with respect to different LMs applied with an ASR. Semantic parsers applied in pipeline-based SLU systems are in general usually learned in a superv"
N15-1088,N06-1056,0,0.0362651,"ndard n-gram model with the ASR, since dependencies between acoustics and semantics can be captured. Their grammars are, however, learned in a supervised setting. In fact, while semantic grammars are often applied for speech recognition and/or understanding, they are often created manually or – as mentioned previously – learned from data containing semantic annotations, which are time-consuming to produce. In the field of Natural Language Processing (NLP), the development of semantic parsers has received considerable attention. While some researchers have considered fully supervised settings (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007), requiring accurate and complete semantic annota872 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 872–881, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tions, others have developed weakly supervised approaches exploiting ambiguous representations of the context in which an utterance is produced instead of accurate and complete annotations (Chen et al., 2010; B¨orschinger et al., 2011; Chen and Mooney, 2008). In this line, in this paper we explore how an approa"
N15-1088,D07-1071,0,0.041738,"the ASR, since dependencies between acoustics and semantics can be captured. Their grammars are, however, learned in a supervised setting. In fact, while semantic grammars are often applied for speech recognition and/or understanding, they are often created manually or – as mentioned previously – learned from data containing semantic annotations, which are time-consuming to produce. In the field of Natural Language Processing (NLP), the development of semantic parsers has received considerable attention. While some researchers have considered fully supervised settings (Wong and Mooney, 2006; Zettlemoyer and Collins, 2007), requiring accurate and complete semantic annota872 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 872–881, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tions, others have developed weakly supervised approaches exploiting ambiguous representations of the context in which an utterance is produced instead of accurate and complete annotations (Chen et al., 2010; B¨orschinger et al., 2011; Chen and Mooney, 2008). In this line, in this paper we explore how an approach that induces semantic parsers"
N19-1257,Q17-1010,0,0.015018,"the inputs to the convolutional network are only the cross-lingual embeddings, the network can be applied to any language for which the embeddings have been aligned. Since the word embeddings for source and target language share a common vector space, the shared parts of the target language model are able to process data samples from the completely unseen target language and perform accurate prediction i.e. enabling zero-shot cross-lingual extraction of opinion target expressions. We rely on two approaches to compute embeddings that are aligned across languages. Both methods rely on fastText (Bojanowski et al., 2017) to compute monolingual embeddings trained on Wikipedia articles. The first method is the one proposed by Smith et al. (2017), which computes a singular value decomposition (SVD) on a dictionary of translated word pairs to obtain an optimal, orthogonal projection matrix from one space into the other. We refer to this method as SVD-aligned. We use these embeddings3 in our experiments in Sections 3.3, 3.4 and 3.6. The second method proposed by Lample et al. (2018) performs the alignment of embeddings 3 Obtained from: https://github.com/ Babylonpartners/fastText_multilingual 3 Evaluation In this"
N19-1257,D10-1101,0,0.0266432,"system that addresses opinion target extraction as a sequence labeling problem based on a perceptron algorithm with token, word shape and clustering-based features. Toh and Wang (2014) propose a Conditional Random Field (CRF) as a sequence labeling model that includes a variety of features such as Part-ofSpeech (POS) tags and dependency tree features, word clusters and features derived from the WordNet taxonomy. The model is later improved us2492 ing neural network output probabilities (Toh and Su, 2016) and achieved the best results on the SemEval 2016 dataset for English restaurant reviews. Jakob and Gurevych (2010) follow a very similar approach that addresses opinion target extraction as a sequence labeling problem using CRFs. Their approach includes features derived from words, Part-of-Speech tags and dependency paths, and performs well in a single and cross-domain setting. Kumar et al. (2016) present a CRF-based model that makes use of a variety of morphological and linguistic features and is one of the few systems that submitted results for more than one language for the SemEval 2016 ABSA challenge. The strong reliance on high-level NLP features, such as dependency trees, named-entity information an"
N19-1257,S16-1174,0,0.592623,"t I is O also O really O nice O . O By rephrasing the task in this way, we can address it using established sequence tagging models. In this work, we use a multi-layer convolutional neural network (CNN) as our sequence tagging model. The model receives a sequence of words as input features and predicts an output sequence of IOB tags. In order to keep our model simple and our results clear, we restrict our input representation to a sequence of word embeddings. While additional features such as Part-ofSpeech (POS) tags are known to perform well in the domain of OTE extraction (Toh and Su, 2016; Kumar et al., 2016; Jebbara and Cimiano, 2016), they would require a separately trained model for POS-tag prediction which can not be assumed to be available for every language. We refrain from using more complex architectures such as memory networks as our goal is mainly to investigate the possibility of performing zero-shot cross-lingual transfer learning for OTE prediction. Being the 1 Note that the B token is only used to indicate the boundary of two consecutive phrases. first approach proposing this, we leave the question of how to increase performance of the approach by using more complex architectures to"
N19-1257,D17-1310,0,0.286585,"in Table 3. We can see that the competition is strongest for English where we fall behind recent monolingual systems. This corresponds to rank 7 of 19 of the original SemEval competition. Regarding the other languages, we see that we are close to the best Spanish and Dutch systems and even clearly outperform systems for Russian and Turkish by at least 7 points in F1 -score. With that, we present the first approach on this task to achieve such competitive performances for a variety of languages 2491 System Toh and Su (2016) en 0.723 0.666 Kumar et al. (2016) 0.685 Pontiki et al. (2016)* 0.441 Li and Lam (2017) 0.734 all→target (Ours) 0.660 ` Alvarez-L´ opez et al. (2016) es nl ru tr – 0.685 0.697 0.520 – 0.687 – – 0.644 0.506 – 0.624 – – – 0.493 – 0.567 – – – 0.419 – 0.490 Table 3: Overview of the current state-of-the-art for opinion target extraction for 5 languages. Our model is trained on the combined training data of all languages and evaluated on the respective test datasets. The row marked with * is the baseline provided by the workshop organizers. To our knowledge, no better model is published for Russian and Turkish. with a single, multilingual model. 3.7 Discussion and Future Work The pres"
N19-1257,S15-2127,0,0.135062,"gical and syntactic features seem to explain some of the relatively low results. However, with the small sample of languages and the many potential influencing factors at play, we are aware that it is not possible to draw any strong conclusions. Further research has to be conducted in this direction to answer open questions. 4 Related Work Our work brings together the domains of opinion target extraction on the one side and cross lingual learning on the other side. In this section, we give a brief overview of both domains and point out parallels to previous work. Opinion Target Extraction San Vicente et al. (2015) present a system that addresses opinion target extraction as a sequence labeling problem based on a perceptron algorithm with token, word shape and clustering-based features. Toh and Wang (2014) propose a Conditional Random Field (CRF) as a sequence labeling model that includes a variety of features such as Part-ofSpeech (POS) tags and dependency tree features, word clusters and features derived from the WordNet taxonomy. The model is later improved us2492 ing neural network output probabilities (Toh and Su, 2016) and achieved the best results on the SemEval 2016 dataset for English restauran"
N19-1257,E99-1023,0,0.0763513,"ngual learning and show that we can improve by 6 to 8 points in F1 -Score compared to a model trained on a single source language. • We investigate the benefit of augmenting the zero-shot approach with additional data points from the target language. We observe that we can save hundreds of annotated data points by employing a cross-lingual approach. • We compare two methods for obtaining cross-lingual word embeddings on the task. 2 Approach A common approach for extracting opinion target expressions is to phrase the task as a sequence tagging problem using the well-known IOB scheme (Tjong Kim Sang and Veenstra, 1999) to represent OTEs as a sequence of tags. According to this scheme, each word in our text is marked with one of three tags, namely I, O or B that indicate if the word is at the Beginning1 , Inside or Outside of a target expression. An example of such an encoding can be seen below: The O wine I list I is O also O really O nice O . O By rephrasing the task in this way, we can address it using established sequence tagging models. In this work, we use a multi-layer convolutional neural network (CNN) as our sequence tagging model. The model receives a sequence of words as input features and predict"
N19-1257,S16-1045,0,0.0523806,"Missing"
N19-1257,S14-2038,0,0.061003,"is not possible to draw any strong conclusions. Further research has to be conducted in this direction to answer open questions. 4 Related Work Our work brings together the domains of opinion target extraction on the one side and cross lingual learning on the other side. In this section, we give a brief overview of both domains and point out parallels to previous work. Opinion Target Extraction San Vicente et al. (2015) present a system that addresses opinion target extraction as a sequence labeling problem based on a perceptron algorithm with token, word shape and clustering-based features. Toh and Wang (2014) propose a Conditional Random Field (CRF) as a sequence labeling model that includes a variety of features such as Part-ofSpeech (POS) tags and dependency tree features, word clusters and features derived from the WordNet taxonomy. The model is later improved us2492 ing neural network output probabilities (Toh and Su, 2016) and achieved the best results on the SemEval 2016 dataset for English restaurant reviews. Jakob and Gurevych (2010) follow a very similar approach that addresses opinion target extraction as a sequence labeling problem using CRFs. Their approach includes features derived fr"
N19-1257,S16-1002,0,\N,Missing
P07-1112,W05-1004,1,0.719353,"rk described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nouns from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (Markert et al., 2003) and (Etzioni et al., 2005). These approaches rely on a combination of the usage of lexico-syntactic pattens conveying a certain relation of interest as described in (Hearst, 1992) with the idea of using the web as a big corpus (cf. (Kilgariff and Grefenstette, 2003)). Our approach directly builds on our previous work (Cimiano and Wenderoth, 2005) an adheres to the principled idea of learning ranked qualia structures. In fact, a ranking of qualia elements is useful as it helps to determine a cut-off point and as a reliability indicator for lexicographers inspecting the qualia structures. In contrast to our previous work, the focus of this paper lies in analyzing different measures for ranking the qualia elements in the automatically acquired qualia structures. We also introduce additional patterns for the agentive role which make use of wildcard operators. Further, we present a gold standard for qualia structures created for the 30 wor"
P07-1112,C92-2082,0,0.524373,"ems relying on publicly available resources such as WordNet (Fellbaum, 1998) or FrameNet (Baker et al., 1998) as source of lexical/world knowledge. The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nouns from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (Markert et al., 2003) and (Etzioni et al., 2005). These approaches rely on a combination of the usage of lexico-syntactic pattens conveying a certain relation of interest as described in (Hearst, 1992) with the idea of using the web as a big corpus (cf. (Kilgariff and Grefenstette, 2003)). Our approach directly builds on our previous work (Cimiano and Wenderoth, 2005) an adheres to the principled idea of learning ranked qualia structures. In fact, a ranking of qualia elements is useful as it helps to determine a cut-off point and as a reliability indicator for lexicographers inspecting the qualia structures. In contrast to our previous work, the focus of this paper lies in analyzing different measures for ranking the qualia elements in the automatically acquired qualia structures. We also i"
P07-1112,W96-0309,0,0.355824,"elements are actually ranked for each qualia role with respect to some measure. The specific contribution of the paper lies in the extensive analysis and quantitative comparison of different measures for ranking the qualia elements. Further, for the first time, we present a quantitative evaluation of such an approach for learning qualia structures with respect to a handcrafted gold standard. 1 Introduction Qualia structures have been originally introduced by (Pustejovsky, 1991) and are used for a variety of purposes in natural language processing (NLP), such as for the analysis of compounds (Johnston and Busa, 1996) as well as co-composition and coercion (Pustejovsky, 1991), but also for bridging reference resolution (Bos et al., 1995). Further, it has also 1 The work reported in this paper has been supported by the X-Media project, funded by the European Commission under EC grant number IST-FP6-026978 as well by the SmartWeb project, funded by the German Ministry of Research. Thanks to all our colleagues for helping to evaluate the approach. been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (Voorhees, 1994; Pustejovsky et al., 1993)."
P07-1112,W03-2606,0,0.166904,"r is that currently qualia structures need to be created by hand, which is probably also the reason why there are almost no practical NLP systems using qualia structures, but a lot of systems relying on publicly available resources such as WordNet (Fellbaum, 1998) or FrameNet (Baker et al., 1998) as source of lexical/world knowledge. The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nouns from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (Markert et al., 2003) and (Etzioni et al., 2005). These approaches rely on a combination of the usage of lexico-syntactic pattens conveying a certain relation of interest as described in (Hearst, 1992) with the idea of using the web as a big corpus (cf. (Kilgariff and Grefenstette, 2003)). Our approach directly builds on our previous work (Cimiano and Wenderoth, 2005) an adheres to the principled idea of learning ranked qualia structures. In fact, a ranking of qualia elements is useful as it helps to determine a cut-off point and as a reliability indicator for lexicographers inspecting the qualia structures. In co"
P07-1112,W05-1003,0,0.0143064,"earning Telic and Agentive relations from corpora analyzing two different approaches: one relying on matching certain lexicosyntactic patterns as in the work presented here, but also a second approach consisting in training a maximum entropy model classifier. The patterns used by (Yamada and Baldwin, 2004) differ substantially from the ones used in this paper, which is mainly due to the fact that search engines do not provide support for regular expressions and thus instantiating a pattern as ’V[+ing] Noun’ is impossible in our approach as the verbs are unknown a priori. Poesio and Almuhareb (Poesio and Almuhareb, 2005) present a machine learning based approach to classifying attributes into the six categories: quality, part, related-object, activity, related-agent and non-attribute. 7 Conclusion We have presented an approach to automatically learning qualia structures from the Web. Such an approach is especially interesting either for lexicographers aiming at constructing lexicons, but even more for natural language processing systems relying on deep lexical knowledge as represented by qualia structures. In particular, we have focused on learning ranked qualia structures which allow to find an ideal cut-off"
P07-1112,J93-2005,0,0.0272582,"s (Johnston and Busa, 1996) as well as co-composition and coercion (Pustejovsky, 1991), but also for bridging reference resolution (Bos et al., 1995). Further, it has also 1 The work reported in this paper has been supported by the X-Media project, funded by the European Commission under EC grant number IST-FP6-026978 as well by the SmartWeb project, funded by the German Ministry of Research. Thanks to all our colleagues for helping to evaluate the approach. been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (Voorhees, 1994; Pustejovsky et al., 1993). One major bottleneck however is that currently qualia structures need to be created by hand, which is probably also the reason why there are almost no practical NLP systems using qualia structures, but a lot of systems relying on publicly available resources such as WordNet (Fellbaum, 1998) or FrameNet (Baker et al., 1998) as source of lexical/world knowledge. The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nouns from the Web. The approach is inspired in recent work on using the Web to identify instances of a rela"
P07-1112,J91-4003,0,0.659991,"ntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines. In our approach, the qualia elements are actually ranked for each qualia role with respect to some measure. The specific contribution of the paper lies in the extensive analysis and quantitative comparison of different measures for ranking the qualia elements. Further, for the first time, we present a quantitative evaluation of such an approach for learning qualia structures with respect to a handcrafted gold standard. 1 Introduction Qualia structures have been originally introduced by (Pustejovsky, 1991) and are used for a variety of purposes in natural language processing (NLP), such as for the analysis of compounds (Johnston and Busa, 1996) as well as co-composition and coercion (Pustejovsky, 1991), but also for bridging reference resolution (Bos et al., 1995). Further, it has also 1 The work reported in this paper has been supported by the X-Media project, funded by the European Commission under EC grant number IST-FP6-026978 as well by the SmartWeb project, funded by the German Ministry of Research. Thanks to all our colleagues for helping to evaluate the approach. been argued that qualia"
P07-1112,Y04-1012,0,0.560409,"nked qualia structures. In fact, a ranking of qualia elements is useful as it helps to determine a cut-off point and as a reliability indicator for lexicographers inspecting the qualia structures. In contrast to our previous work, the focus of this paper lies in analyzing different measures for ranking the qualia elements in the automatically acquired qualia structures. We also introduce additional patterns for the agentive role which make use of wildcard operators. Further, we present a gold standard for qualia structures created for the 30 words used in the evaluation of Yamada and Baldwin (Yamada and Baldwin, 2004). The evaluation 888 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 888–895, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics presented here is thus much more extensive than our previous one (Cimiano and Wenderoth, 2005), in which only 7 words were used. We present a quantitative evaluation of our approach and a comparison of the different ranking measures with respect to this gold standard. Finally, we also provide an evaluation in which test persons were asked to inspect and rate the learned qualia structures a"
P07-1112,P98-1013,0,\N,Missing
P07-1112,C98-1013,0,\N,Missing
P07-1112,P02-1054,0,\N,Missing
P13-2147,D10-1101,0,0.618677,"cameras and cars), showing that our model outperforms state-ofthe-art models, as well as on a new dataset consisting of Twitter posts. 1 While the three key variables (subjective phrase, polarity and target) intuitively influence each other bidirectionally, most work in the area of opinion mining has concentrated on either predicting one of these variables in isolation (e. g. subjective expressions by Yang and Cardie (2012)) or modeling the dependencies uni-directionally in a pipeline architecture, e. g. predicting targets on the basis of perfect and complete knowledge about subjective terms (Jakob and Gurevych, 2010). However, such pipeline models do not allow for inclusion of bidirectional interactions between the key variables. In this paper, we propose a model that can include bidirectional dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis for a joint model: Introduction Sentiment analysis or opinion mining is the task of identifying subjective statements about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target"
P13-2147,P11-2018,0,0.0536127,"al dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis for a joint model: Introduction Sentiment analysis or opinion mining is the task of identifying subjective statements about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target. Opinion analysis is thus typically approached as a classification (T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004) or segmentation (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012) task by which fragments of the input are classified or labelled as representing a subjective phrase (Yang and Cardie, 2012), a polarity or a target (Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010). As an example, the sentence “I like the low weight of the camera.” • What is the impact of the performance loss of a non-perfect subjective term extraction in comparison to perfect knowledge? • Further, how does perfect knowledge about targets influence the prediction of subjective terms? • How is the latter affected if the knowledge a"
P13-2147,N13-1039,0,0.0195395,"Missing"
P13-2147,P04-1035,0,0.00714512,"s paper, we propose a model that can include bidirectional dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis for a joint model: Introduction Sentiment analysis or opinion mining is the task of identifying subjective statements about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target. Opinion analysis is thus typically approached as a classification (T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004) or segmentation (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012) task by which fragments of the input are classified or labelled as representing a subjective phrase (Yang and Cardie, 2012), a polarity or a target (Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010). As an example, the sentence “I like the low weight of the camera.” • What is the impact of the performance loss of a non-perfect subjective term extraction in comparison to perfect knowledge? • Further, how does perfect knowledge about targets influence the prediction of"
P13-2147,H05-1043,0,0.267282,"about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target. Opinion analysis is thus typically approached as a classification (T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004) or segmentation (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012) task by which fragments of the input are classified or labelled as representing a subjective phrase (Yang and Cardie, 2012), a polarity or a target (Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010). As an example, the sentence “I like the low weight of the camera.” • What is the impact of the performance loss of a non-perfect subjective term extraction in comparison to perfect knowledge? • Further, how does perfect knowledge about targets influence the prediction of subjective terms? • How is the latter affected if the knowledge about targets is imperfect, i. e. predicted by a learned model? We study these questions using imperatively defined factor graphs (IDFs, McCallum et al. (2008), McCallum et al. (2009)) to show how these bidirectional dependencies can b"
P13-2147,N12-1085,0,0.072235,"key variables. In this paper, we propose a model that can include bidirectional dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis for a joint model: Introduction Sentiment analysis or opinion mining is the task of identifying subjective statements about products, their polarity (e. g. positive, negative or neutral) in addition to the particular aspect or feature of the entity that is under discussion, i. e., the socalled target. Opinion analysis is thus typically approached as a classification (T¨ackstr¨om and McDonald, 2011; Sayeed et al., 2012; Pang and Lee, 2004) or segmentation (Choi et al., 2010; Johansson and Moschitti, 2011; Yang and Cardie, 2012) task by which fragments of the input are classified or labelled as representing a subjective phrase (Yang and Cardie, 2012), a polarity or a target (Hu and Liu, 2004; Li et al., 2010; Popescu and Etzioni, 2005; Jakob and Gurevych, 2010). As an example, the sentence “I like the low weight of the camera.” • What is the impact of the performance loss of a non-perfect subjective term extraction in comparison to perfect knowledge? • Further, how does perfect knowledge about targets influe"
P13-2147,P11-2100,0,0.296896,"Missing"
P13-2147,D12-1122,0,0.0832053,"on of target and subjective phrases in both directions, thus providing an upper bound for the impact of a joint model in comparison to a pipeline model. We report results on two public datasets (cameras and cars), showing that our model outperforms state-ofthe-art models, as well as on a new dataset consisting of Twitter posts. 1 While the three key variables (subjective phrase, polarity and target) intuitively influence each other bidirectionally, most work in the area of opinion mining has concentrated on either predicting one of these variables in isolation (e. g. subjective expressions by Yang and Cardie (2012)) or modeling the dependencies uni-directionally in a pipeline architecture, e. g. predicting targets on the basis of perfect and complete knowledge about subjective terms (Jakob and Gurevych, 2010). However, such pipeline models do not allow for inclusion of bidirectional interactions between the key variables. In this paper, we propose a model that can include bidirectional dependencies, attempting to answer the following questions which so far have not been addressed but provide the basis for a joint model: Introduction Sentiment analysis or opinion mining is the task of identifying subject"
P13-2147,H05-2017,0,\N,Missing
P18-4012,N16-1097,0,0.0391421,"Missing"
P18-4012,burchardt-etal-2006-salto,0,0.0243847,"f annotations (e. g., from multiple imports). It is implemented as a web service in order to support remote annotation workflows for multiple users in parallel. Figure 1: Example template following a schema derived from the Spinal Cord Injury Ontology. Availability and License. A demo installation is available at http://psink.techfak. uni-bielefeld.de/santo/. The source code of the application is publicly available under the Apache 2.0 License at https://github. com/ag-sc/SANTO. 2 Related Work Most annotation frameworks for text focus on the sentence level. Examples include syntactic parsing (Burchardt et al., 2006) or semantic role labeling (Kakkonen, 2006; Yimam et al., 2013). Other tools focus on segmentation annotation tasks, for instance Callisto (Day et al., 2004), WordFreak (Morton and LaCivita, 2003), MMax2 (M¨uller and Strube, 2006), or GATE Teamware (Bontcheva et al., 2013) (though the latter also supports more complex schemata). Brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), eHost (South et al., 2012) and CAT (Lenzi et al., 2012) support approaches for relational annotations. These tools are easy to use and highly flexible regarding the specification of annotation schemes. Projec"
P18-4012,day-etal-2004-callisto,0,0.0452109,"gure 1: Example template following a schema derived from the Spinal Cord Injury Ontology. Availability and License. A demo installation is available at http://psink.techfak. uni-bielefeld.de/santo/. The source code of the application is publicly available under the Apache 2.0 License at https://github. com/ag-sc/SANTO. 2 Related Work Most annotation frameworks for text focus on the sentence level. Examples include syntactic parsing (Burchardt et al., 2006) or semantic role labeling (Kakkonen, 2006; Yimam et al., 2013). Other tools focus on segmentation annotation tasks, for instance Callisto (Day et al., 2004), WordFreak (Morton and LaCivita, 2003), MMax2 (M¨uller and Strube, 2006), or GATE Teamware (Bontcheva et al., 2013) (though the latter also supports more complex schemata). Brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), eHost (South et al., 2012) and CAT (Lenzi et al., 2012) support approaches for relational annotations. These tools are easy to use and highly flexible regarding the specification of annotation schemes. Projects are easy to manage due to administration interfaces and remote annotation is supported. However, these approaches share the limitation that all relational"
P18-4012,W12-2416,0,0.0218138,"er the Apache 2.0 License at https://github. com/ag-sc/SANTO. 2 Related Work Most annotation frameworks for text focus on the sentence level. Examples include syntactic parsing (Burchardt et al., 2006) or semantic role labeling (Kakkonen, 2006; Yimam et al., 2013). Other tools focus on segmentation annotation tasks, for instance Callisto (Day et al., 2004), WordFreak (Morton and LaCivita, 2003), MMax2 (M¨uller and Strube, 2006), or GATE Teamware (Bontcheva et al., 2013) (though the latter also supports more complex schemata). Brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), eHost (South et al., 2012) and CAT (Lenzi et al., 2012) support approaches for relational annotations. These tools are easy to use and highly flexible regarding the specification of annotation schemes. Projects are easy to manage due to administration interfaces and remote annotation is supported. However, these approaches share the limitation that all relational structures need to be anchored at the textual surface. Thus, annotating complex templates as in Figure 1 becomes tedious and visually cumbersome, especially in cases of complex nestings within or across templates, or when fillers are widely dispersed across mu"
P18-4012,E12-2021,0,0.209166,"Missing"
P18-4012,bartalesi-lenzi-etal-2012-cat,0,0.0254089,"https://github. com/ag-sc/SANTO. 2 Related Work Most annotation frameworks for text focus on the sentence level. Examples include syntactic parsing (Burchardt et al., 2006) or semantic role labeling (Kakkonen, 2006; Yimam et al., 2013). Other tools focus on segmentation annotation tasks, for instance Callisto (Day et al., 2004), WordFreak (Morton and LaCivita, 2003), MMax2 (M¨uller and Strube, 2006), or GATE Teamware (Bontcheva et al., 2013) (though the latter also supports more complex schemata). Brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), eHost (South et al., 2012) and CAT (Lenzi et al., 2012) support approaches for relational annotations. These tools are easy to use and highly flexible regarding the specification of annotation schemes. Projects are easy to manage due to administration interfaces and remote annotation is supported. However, these approaches share the limitation that all relational structures need to be anchored at the textual surface. Thus, annotating complex templates as in Figure 1 becomes tedious and visually cumbersome, especially in cases of complex nestings within or across templates, or when fillers are widely dispersed across multiple sentences in a text. W"
P18-4012,P13-4001,0,0.101117,"Missing"
P18-4012,N03-4009,0,0.100058,"wing a schema derived from the Spinal Cord Injury Ontology. Availability and License. A demo installation is available at http://psink.techfak. uni-bielefeld.de/santo/. The source code of the application is publicly available under the Apache 2.0 License at https://github. com/ag-sc/SANTO. 2 Related Work Most annotation frameworks for text focus on the sentence level. Examples include syntactic parsing (Burchardt et al., 2006) or semantic role labeling (Kakkonen, 2006; Yimam et al., 2013). Other tools focus on segmentation annotation tasks, for instance Callisto (Day et al., 2004), WordFreak (Morton and LaCivita, 2003), MMax2 (M¨uller and Strube, 2006), or GATE Teamware (Bontcheva et al., 2013) (though the latter also supports more complex schemata). Brat (Stenetorp et al., 2012), WebAnno (Yimam et al., 2013), eHost (South et al., 2012) and CAT (Lenzi et al., 2012) support approaches for relational annotations. These tools are easy to use and highly flexible regarding the specification of annotation schemes. Projects are easy to manage due to administration interfaces and remote annotation is supported. However, these approaches share the limitation that all relational structures need to be anchored at the"
P18-4012,D17-1004,0,0.0491584,"Missing"
P18-4012,N06-4006,0,0.0395608,"administration interfaces and remote annotation is supported. However, these approaches share the limitation that all relational structures need to be anchored at the textual surface. Thus, annotating complex templates as in Figure 1 becomes tedious and visually cumbersome, especially in cases of complex nestings within or across templates, or when fillers are widely dispersed across multiple sentences in a text. We propose a tool to frame complex relational annotation problems as slot filling tasks. To our knowledge, the only existing tool for this purpose is the Prot´eg´e plugin Knowtator (Ogren, 2006), which is, however, not web-based, comparably difficult to use with multiple annotators, and no longer actively supported since 2009. Thus, our main contribution is an annotation tool which combines the advantages of (i) enabling complex relational slot filling with distant fillers, and (ii) ease of use in web-based environments in order to facilitate remote collaboration. SANTO is ontology-based, i. e., entity and relation types are derived from an underlying ontology. The same idea is prominent in several annotation tools within the Semantic Web community (cf. Oliveira and Rocha, 2013). Con"
S14-2016,D13-1179,1,0.850781,") = cos(Φ(di ), Φ(dj )) = ||Φ(di ) |Φ(dj )|| sim(di , dj ) = cos(XT di , XT dj ) = T dT i XX dj T ||X di |XT dj || The key challenge with topic modelling is choosing a good background document collection B = {b1 , ..., bN }. A simple minimal criterion for a good background document collection is that each document in this collection should be maximally similar to itself and less similar to any other document:  ∀i 6= j 1 = sim(bj , bj ) &gt; sim(bi , bj ) ≥ 0 −(AT A)−1 AT BC0 C0  A 0 B C  =I (1) The inverse C0 is approximated by the Jacobi Preconditioner, J, of CT C: As shown in McCrae et al. (2013), this property is satisfied by the following projection: ΦONETA (d) = (XT X)−1 XT d C0 &apos; JCT  ||c1 ||−2  =  0 And hence the similarity between two documents can be calculated as: sim(di , dj ) = cos(ΦONETA (di ), ΦONETA (dj )) 3 (AT A)−1 AT 0 4 Approximations .. 0 . ||cN2 ||−2  (2)  T C Normalization A key factor in the effectiveness of topic-based methods is the appropriate normalization of the elements of the document matrix X. This is even more relevant for orthonormal topics as the matrix inversion procedure can be very sensitive to small changes in the matrix. In this context, we c"
W03-1903,J96-2004,0,0.0200093,"citly annotated as discourse new. The MUC coreference scheme (Hirschman and Chinchor, 1997) is restricted to the annotation of coreference relations, where coreference is also defined as an equivalence relation. Though this annotation scheme may seem quite simple, we agree with (Hirschman and Chinchor, 1997) that it is complex enough when taking into account the agreement of the annotators on a task. In fact, it has been shown that the agreement of subjects annotating bridging (Poesio and Vieira, 1998) or discourse (Cimiano, 2003) relations can be too low for tentative conclusion to be drawn (Carletta, 1996). The motivation of the MUC coreference scheme was thus to develop an annotation scheme leading to a good agreement. On the other hand, our motivation is to show how our ontology-based framework can be applied to the annotation of anaphoric relations in written texts and from this perspective the MUC coreference annotation scheme would have been in fact too restricted to actually show all the advantages of our approach. The UCREL (Fligelstone, 1992) and DRAMA (Passoneau, 1996) annotation schemes are more related to ours than the schemes above in the sense that they also provide a rich set of p"
W03-1903,P91-1008,0,0.138342,"Missing"
W03-1903,J93-2004,0,0.0236766,"arning based approaches to part-of-speech tagging, word sense disambiguation, information extraction or anaphora resolution - just to name a few - rely on corpora annotated with the corresponding phenomenon to be trained and tested on. In this paper, we argue that linguistic annotation can to some extent be considered a special case of semantic annotation with regard to an ontology. Part-of-Speech (POS) annotation for example can be seen as the task of choosing the appropriate tag for a word from an ontology of word categories (compare for example the Penn Treebank POS tagset as described in (Marcus et al., 1993)). The annotation of word senses such as used by machine-learning based word sense disambiguation (WSD) tools corresponds to the task of selecting the correct semantic class or concept for a word from an underlying ontology such as WordNet (Resnik, 1997). Annotation by template filling such as used to train machine-learning based information extraction (IE) systems as (Ciravegna, 2001) can be seen as the task of finding and marking all the attributes of a given ontological concept in a text. An ontological concept in this sense can be a launching event, a management succession event or a perso"
W03-1903,W01-1612,0,0.061053,"Missing"
W03-1903,J98-2001,0,0.205152,"Missing"
W03-1903,W97-0209,0,0.044125,"hat linguistic annotation can to some extent be considered a special case of semantic annotation with regard to an ontology. Part-of-Speech (POS) annotation for example can be seen as the task of choosing the appropriate tag for a word from an ontology of word categories (compare for example the Penn Treebank POS tagset as described in (Marcus et al., 1993)). The annotation of word senses such as used by machine-learning based word sense disambiguation (WSD) tools corresponds to the task of selecting the correct semantic class or concept for a word from an underlying ontology such as WordNet (Resnik, 1997). Annotation by template filling such as used to train machine-learning based information extraction (IE) systems as (Ciravegna, 2001) can be seen as the task of finding and marking all the attributes of a given ontological concept in a text. An ontological concept in this sense can be a launching event, a management succession event or a person together with attributes such as name, affiliation, position, etc. The annotation of anaphoric or bridging relations is actually the task of identifying the semantic relation between two linguistic expressions representing a certain ontological concept"
W03-1903,J00-4005,0,0.0715996,"Missing"
W03-1903,W99-0309,0,\N,Missing
W05-1004,W96-0309,0,0.934687,"ting a lexicon of qualia structures. The approach is based on the idea of matching certain lexicosyntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines. We evaluate our approach qualitatively by comparing our automatically learned qualia structures with the ones from the literature, but also quantitatively by presenting results of a human evaluation. 1 Introduction Qualia Structures have been originally introduced by (Pustejovsky, 1991) and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (Johnston and Busa, 1996), co-composition and coercion (Pustejovsky, 1991) as well as for bridging reference resolution (Bos et al., 1995). Further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (Voorhees, 1994; Pustejovsky et al., 1993). One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as WordNet (Fellbaum, 1998) or FrameNet1 1 as source"
W05-1004,W02-1030,0,0.0114578,"ssue and presents an approach to automatically learning qualia structures for nominals from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (Markert et al., 2003) and (Cimiano and Staab, 2004). These approaches are in essence a combination of the usage of lexico-syntactic pattens conveying a certain relation of interest such as in (Hearst, 1992), (Charniak and Berland, 1999), (Iwanska et al., 2000) or (Poesio et al., 2002) with the idea of using the web as a big corpus (Resnik and Smith, 2003), (Grefenstette, 1999), (Keller et al., 2002). The idea of learning Qualia Structures from the Web is not only a very practical, it is in fact a principled one. While single lexicographers creating qualia structures or lexicon entries in general - might take very subjective decisions, the structures learned from the Web do not mirror the view of a single person, but of the whole world as represented on the World Wide Web. Thus, an approach learning qualia structures from the Web is in principle more reliable than letting lexicographers craft lexical entries on their own. Obviously, on the other hand, using an automatic web based approach"
W05-1004,W03-2606,0,0.212257,"93). One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as WordNet (Fellbaum, 1998) or FrameNet1 1 as source of lexical/world knowledge. The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (Markert et al., 2003) and (Cimiano and Staab, 2004). These approaches are in essence a combination of the usage of lexico-syntactic pattens conveying a certain relation of interest such as in (Hearst, 1992), (Charniak and Berland, 1999), (Iwanska et al., 2000) or (Poesio et al., 2002) with the idea of using the web as a big corpus (Resnik and Smith, 2003), (Grefenstette, 1999), (Keller et al., 2002). The idea of learning Qualia Structures from the Web is not only a very practical, it is in fact a principled one. While single lexicographers creating qualia structures or lexicon entries in general - might take very"
W05-1004,poesio-etal-2002-acquiring,0,0.0155521,"ellbaum, 1998) or FrameNet1 1 as source of lexical/world knowledge. The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (Markert et al., 2003) and (Cimiano and Staab, 2004). These approaches are in essence a combination of the usage of lexico-syntactic pattens conveying a certain relation of interest such as in (Hearst, 1992), (Charniak and Berland, 1999), (Iwanska et al., 2000) or (Poesio et al., 2002) with the idea of using the web as a big corpus (Resnik and Smith, 2003), (Grefenstette, 1999), (Keller et al., 2002). The idea of learning Qualia Structures from the Web is not only a very practical, it is in fact a principled one. While single lexicographers creating qualia structures or lexicon entries in general - might take very subjective decisions, the structures learned from the Web do not mirror the view of a single person, but of the whole world as represented on the World Wide Web. Thus, an approach learning qualia structures from the Web is in principle more reliable than letting l"
W05-1004,J93-2005,0,0.165984,"tures with the ones from the literature, but also quantitatively by presenting results of a human evaluation. 1 Introduction Qualia Structures have been originally introduced by (Pustejovsky, 1991) and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (Johnston and Busa, 1996), co-composition and coercion (Pustejovsky, 1991) as well as for bridging reference resolution (Bos et al., 1995). Further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (Voorhees, 1994; Pustejovsky et al., 1993). One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qualia structures, but a lot of systems using globally available resources such as WordNet (Fellbaum, 1998) or FrameNet1 1 as source of lexical/world knowledge. The work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (Mar"
W05-1004,J91-4003,0,0.326832,"rocessing at a larger scale. Furthermore, our approach can be also used support a lexicographer in the task of manually creating a lexicon of qualia structures. The approach is based on the idea of matching certain lexicosyntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines. We evaluate our approach qualitatively by comparing our automatically learned qualia structures with the ones from the literature, but also quantitatively by presenting results of a human evaluation. 1 Introduction Qualia Structures have been originally introduced by (Pustejovsky, 1991) and are used for a variety of purposes in Natural Language processing such as the analysis of compounds (Johnston and Busa, 1996), co-composition and coercion (Pustejovsky, 1991) as well as for bridging reference resolution (Bos et al., 1995). Further, it has also been argued that qualia structures and lexical semantic relations in general have applications in information retrieval (Voorhees, 1994; Pustejovsky et al., 1993). One major bottleneck however is that currently Qualia Structures need to be created by hand, which is probably also the reason why there are no practical system using qua"
W05-1004,J03-3002,0,0.0354673,"he work described in this paper addresses this issue and presents an approach to automatically learning qualia structures for nominals from the Web. The approach is inspired in recent work on using the Web to identify instances of a relation of interest such as in (Markert et al., 2003) and (Cimiano and Staab, 2004). These approaches are in essence a combination of the usage of lexico-syntactic pattens conveying a certain relation of interest such as in (Hearst, 1992), (Charniak and Berland, 1999), (Iwanska et al., 2000) or (Poesio et al., 2002) with the idea of using the web as a big corpus (Resnik and Smith, 2003), (Grefenstette, 1999), (Keller et al., 2002). The idea of learning Qualia Structures from the Web is not only a very practical, it is in fact a principled one. While single lexicographers creating qualia structures or lexicon entries in general - might take very subjective decisions, the structures learned from the Web do not mirror the view of a single person, but of the whole world as represented on the World Wide Web. Thus, an approach learning qualia structures from the Web is in principle more reliable than letting lexicographers craft lexical entries on their own. Obviously, on the othe"
W05-1004,Y04-1012,0,0.313502,"it is not always as straightforward to find lexico-syntactic patterns reliably conveying a certain relation. In fact, we did not find any patterns reliably identifying qualia elements for the Agentive role. Certainly, it would have been possible to find the source of the creation by using patterns such as X is made by Y or X is produced by Y. However, we found that these patterns do not reliably convey a verb describing how an object is brought into existence. The fact that it is far from straightforward to find patterns indicating an Agentive role is further corroborated by the research in (Yamada and Baldwin, 2004), in which only one pattern indicating a qualia relation is used, namely ’NN BE V[+en]’ in order to match passive constructions such as the book was written. On the other hand it is clear that constructing a reliable clue for this pattern is not straightforward given the current state-of-the-art concerning search engine queries. Nevertheless, in order to also get results for the Agentive role, we apply a different method here. Instead of issuing a query which is used to search for possible candidates for the role, we take advantage of the fact that the verbs which describe how something comes"
W05-1004,C92-2082,0,\N,Missing
W05-1004,P99-1008,0,\N,Missing
W09-3726,C96-1024,0,0.039832,"paper we present a novel formalism for semantic construction called DUDES (Dependency-based Underspecified Discourse REpresentation Structures). The DUDES formalism has been designed to overcome the rigidity of semantic composition based on the lambda calculus (where the order of application is typically fixed) and provides some flexibility with respect to the direction of the dependence and with respect to the order of application of arguments. In this short paper we present the DUDES formalism and work through a simple example. DUDES bears some resemblance to the work on λ-DRT [2] and LUDs [1] as well as with the work of Copestake et al. [4] and represents a generalization of the formalism introduced in [3]. A detailed discussion of the relation to these formalisms is clearly out of the scope of this paper. DUDES are characterized by three main facts. First, they represent semantic dependencies and are thus inherently suitable for a dependency-based grammar formalism assuming that syntactic dependencies correspond to semantic dependencies (though the correspondence might be “inverted”). Second, they explicitly encode scope relations and are thus able to yield underspecified represe"
W09-3726,P01-1019,0,0.0988811,"construction called DUDES (Dependency-based Underspecified Discourse REpresentation Structures). The DUDES formalism has been designed to overcome the rigidity of semantic composition based on the lambda calculus (where the order of application is typically fixed) and provides some flexibility with respect to the direction of the dependence and with respect to the order of application of arguments. In this short paper we present the DUDES formalism and work through a simple example. DUDES bears some resemblance to the work on λ-DRT [2] and LUDs [1] as well as with the work of Copestake et al. [4] and represents a generalization of the formalism introduced in [3]. A detailed discussion of the relation to these formalisms is clearly out of the scope of this paper. DUDES are characterized by three main facts. First, they represent semantic dependencies and are thus inherently suitable for a dependency-based grammar formalism assuming that syntactic dependencies correspond to semantic dependencies (though the correspondence might be “inverted”). Second, they explicitly encode scope relations and are thus able to yield underspecified representations as output (in contrast to the linear log"
W10-4408,W08-2222,0,0.0127557,"i has 1 300 000 inhabitants. 3. 1 300 000 people live in Hawaii. This paper shows how domain-specific grammars can be automatically generated from a declarative model of the lexicon-ontology interface and how those grammars can be used for question answering. We show a specific implementation of the approach using Lexicalized Tree Adjoining Grammars. The main characteristic of the generated elementary trees is that they constitute domains of locality that span lexicalizations of ontological concepts rather than being based on requirements of single lexical heads. 1 For these sentences, Boxer (Bos, 2008) generates generic Discourse Representation Structures (DRSs) that can roughly be represented as in 4 (for the sentence in 1) and 5 (for the sentences in 2 and 3). 4. 5. Introduction Many approaches to the interpretation of natural language represent meanings as generic logical forms. However, some domain-specific applications require semantic representations that are aligned to a specific ontology and thus cannot be provided by a generic, ontology-independent semantic construction. To illustrate this, consider the example of a geographical ontology that contains a data property1 population re"
W10-4408,P88-1032,0,0.209187,"49 grammar entries for domain-independent elements such as determiners, wh-words, auxiliary verbs, and so on. The complete set of grammar entries finally constitutes a domain-specific grammar that can be used for parsing and interpretation. We demonstrate this by feeding it into our question answering system Pythia. (For the Geobase dataset, the LexInfo model, the grammar files and a demo check http://sc.cit-ec.uni-bielefeld/pythia). Its architecture is depicted in Figure 4. First, the input is handed to a parser that works along the lines of the Earley-type parser devised by Schabes & Joshi (Schabes & Joshi, 1988). It constructs an LTAG derivation tree, considering only the syntactic components of the lexical entries involved. Next, syntactic and semantic composition rules apply in tandem in order to construct a derived tree together with an according DUDE. The syntactic composition rules are TAG’s familiar substitution and adjoin operation, and the semantic composition rules are parallel operations on DUDEs: an argument saturating operation (much like function application) that interprets substitution, and a union S DP↓ VP V border DP DET which DP↓ NP↓ DP Hawaii NP states Applying all substitutions yi"
W11-1013,E09-1010,0,0.0122886,"uch as bilingual lexica. Instead, in this paper we look at how we may gain an adequate translation using statistical machine translation approaches that also utilise the semantic information beyond the label or term describing the concept, that is relations among the concepts in the ontology, as well as the attributes or properties that describe concepts, as will be explained in more detail in section 2. Current work in machine translation has shown that word sense disambiguation can play an important role by using the surrounding words as context to disambiguate terms (Carpuat and Wu, 2007) (Apidianaki, 2009). Such techniques have 116 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 116–125, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics been extrapolated to the translation of taxonomies and ontologies, in which the “context” of a taxonomy or ontology label corresponds to the ontology structure that surrounds the label in question. This structure, which is made up of the lexical information provided by labels and the semantic information provided by the ontology structure, defines the sense"
W11-1013,D07-1007,0,0.0334307,"her similar resources such as bilingual lexica. Instead, in this paper we look at how we may gain an adequate translation using statistical machine translation approaches that also utilise the semantic information beyond the label or term describing the concept, that is relations among the concepts in the ontology, as well as the attributes or properties that describe concepts, as will be explained in more detail in section 2. Current work in machine translation has shown that word sense disambiguation can play an important role by using the surrounding words as context to disambiguate terms (Carpuat and Wu, 2007) (Apidianaki, 2009). Such techniques have 116 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 116–125, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics been extrapolated to the translation of taxonomies and ontologies, in which the “context” of a taxonomy or ontology label corresponds to the ontology structure that surrounds the label in question. This structure, which is made up of the lexical information provided by labels and the semantic information provided by the ontology structure,"
W11-1013,P07-2045,0,0.00517699,"than WER and, as expected, performs better. 5 Approaches for taxonomy and ontology translation 5.1 Domain adaptation It is generally the case that many ontologies and taxonomies focus on only a very specific domain, thus it seems likely that adaptation of translation systems by use of an in-domain corpus may improve translation quality. This is particularly valid in the case of ontologies which frequently contain “subject” annotations6 for not only the whole data structure but often individual elements. To demonstrate this we tried to translate the IFRS 2009 taxonomy using the Moses Decoder (Koehn et al., 2007), which we trained on the EuroParl corpus (Koehn, 2005), translating from Spanish to English. As the IFRS taxonomy is on the topic of finance and accounting, we 6 For example from the Dublin Core vocabulary: see http: //dublincore.org/ WER∗ METEOR NIST BLEU Baseline 0.135 0.324 1.229 0.090 With domain adaptation 0.138 0.335 1.278 0.116 English Dutch German Spanish P (syn|s) 0.147 0.137 0.125 0.126 P (syn|p) 0.012 0.011 0.007 0.012 P (syn|n) 0.001 0.001 0.001 0.001 Table 4: Results of domain-adapted translation. ∗ Lower WER scores are better Table 5: Probability of syntactic relationship given"
W11-1013,2005.mtsummit-papers.11,0,0.0231548,"taxonomy and ontology translation 5.1 Domain adaptation It is generally the case that many ontologies and taxonomies focus on only a very specific domain, thus it seems likely that adaptation of translation systems by use of an in-domain corpus may improve translation quality. This is particularly valid in the case of ontologies which frequently contain “subject” annotations6 for not only the whole data structure but often individual elements. To demonstrate this we tried to translate the IFRS 2009 taxonomy using the Moses Decoder (Koehn et al., 2007), which we trained on the EuroParl corpus (Koehn, 2005), translating from Spanish to English. As the IFRS taxonomy is on the topic of finance and accounting, we 6 For example from the Dublin Core vocabulary: see http: //dublincore.org/ WER∗ METEOR NIST BLEU Baseline 0.135 0.324 1.229 0.090 With domain adaptation 0.138 0.335 1.278 0.116 English Dutch German Spanish P (syn|s) 0.147 0.137 0.125 0.126 P (syn|p) 0.012 0.011 0.007 0.012 P (syn|n) 0.001 0.001 0.001 0.001 Table 4: Results of domain-adapted translation. ∗ Lower WER scores are better Table 5: Probability of syntactic relationship given a semantic relationship in IFRS labels chose all terms"
W11-1013,J10-4005,0,0.0200211,".108 0.036 0.134 0.122 0.209 0.214 0.303 0.169 0.183 0.062 0.266 0.164 0.177 0.111 0.251 0.194 0.151 0.067 0.210 0.204 0.143 0.129 0.221 0.120 Table 3: Correlation between manual evaluation results and automatic evaluation scores label length of 2.45 tokens and the translations generated had an average length of 2.16 tokens. We then created a data set by mixing the translations from the web translation services with a number of translations from the source ontologies, to act as a control. We then gave these translations to 3 evaluators, who scored them for adequacy and fluency as described in Koehn (2010). Finally, we calculated the Pearson correlation coefficient between the automatic scores and the manual scores obtained. These are presented in table 3 and figure 1. As we can see from these results, one metric, namely METEOR, seems to perform best in evaluating the quality of the translations. In fact this is not surprising as there is a clear mathematical deficiency that both NIST and BLEU have for evaluating translations for very short labels like the ones we have here. To illustrate this, we recall the formulation of BLEU as given in (Papineni et al., 2002): B LEU = BP · exp( N ∑ wn log p"
W11-1013,P10-1023,0,0.049565,"Missing"
W11-1013,P02-1040,0,0.0801228,"eriving a corpus from Wikipedia, for example it is possible to provide some hierarchical links by the use of the category that a page belongs to, such as has been performed by the DBpedia project (Auer et al., 2007). 4 Evaluation metrics for taxonomy and ontology translation Given the linguistic differences in taxonomy and ontology labels, it seems necessary to investigate the effectiveness of various metrics for the evaluation of translation quality. There are a number of metrics that are widely used for evaluating translation. Here we will focus on some of the most widely used, namely BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and WER (McCowan et al., 2004). However, it is not clear which of these methods correlate best with human evaluation, particularly for the ontologies with short labels. To evaluate this we collected a mixture of ontologies with short labels on the topics of human diseases, agriculture, geometry and project management, producing 437 labels. These were translated with web translation services from English to Spanish, in particular Google Translate3 , Yahoo! BabelFish4 and SDL FreeTranslation5 . Having obtained translations for each lab"
W11-1013,J99-4008,0,0.230196,"ilar) ontology structures to be compared. Figure 2: Two approaches to translate ontology labels. From a technical point of view, we consider the translation task as a word sense disambiguation task. We identify two methods for comparing ontology structures, which are illustrated in Figure 2. The first method relies on a multilingual resource, i.e., a multilingual ontology or taxonomy. The ontology represented on the left-hand side of the figure consists of several monolingual conceptualizations related to each other by means of an interlingual index, as is the case in the EuroWordNet lexicon (Vossen, 1999). For example, if the original label is chair for seat in English, several translations for it are obtained in Spanish such as: silla (for seat), c´atedra (for university position), presidente (for person leading a meeting). Each of these correspond to a sense in the English WordNet, and hence each translation selects a hierachical structure with English labels. The next step is to compare the input structure of the original ontology containing chair against the three different structures in English representing the several senses of chair and obtain the corresponding label in Spanish. The sec"
W11-1013,C08-1125,0,0.204106,"Missing"
W11-1013,W05-0909,0,\N,Missing
W11-2406,W10-4408,1,\N,Missing
W11-2406,P09-1110,0,\N,Missing
W12-1801,W10-4408,1,0.783837,"r Computational Linguistics formation is commonly captured in grammars, that are either hand-crafted or created by means of machine learning techniques. In order to be able to generate high-quality grammars with as little manual effort as possible, we aim at (semi) automating the knowledge-based generation of lexica and grammars. To achieve this, it is crucial to leverage Web resources for enriching ontologies with lexical and linguistic information, i.e. information about how ontological concepts are lexicalized in different languages, capturing in particular lexical and syntactic variation (Unger et al., 2010). This knowledge-centered grammar generation process may be merged with methods for automatically inferring structure from lightly annotated corpus, including data harvested from the Web, in a bottomup fashion (Tur and De Mori, 2011). For a dialog system to be able to exploit ontologies, lexica and grammars, these three resources need to be tightly aligned, i.e. they need to share domain-relevant vocabulary. For this alignment, we propose to build on Semantic Web standards, mainly in order to support the incorporation of already existing data, to share resources for SDS engineering, and facili"
W13-2102,C90-3059,0,0.205155,"Missing"
W13-2102,W07-2322,0,0.0192443,"ata in the microplanning and realisation steps, thus being comparable to systems like HALogen (Langkilde and Knight, 1998) and pCRU (Belz, 2008). The main difference is that it uses Semantic Web data as base. Since the emergence of the Semantic Web there has been a strong interest in NLG from Semantic Web data, especially for providing users with natural language access to structured data. Work in this area comprises verbalization of ontologies as well as RDF knowledge bases; for an overview see (Bouayad-Agha et al., to appear). Of particular interest in the context of our work is NaturalOWL (Galanis and Androutsopoulos, 2007), a system that produces descriptions of entities and classes relying on linguistic annotations of domain data in RDF format, similar 6 Conclusion and future work We have presented a principled natural language generation architecture that follows a classical NLG architecture but exploits an ontology lexicon as well as statistical information derived from a domain corpus in the lexicalisation and surface realisation steps. The system has been implemented and adapted to the task of generating cooking recipe texts on the basis of RDF representations of recipes. In an evaluation with 93 participa"
W13-2102,W06-1405,0,0.0525164,"Missing"
W13-2102,P98-1116,0,0.0892556,"ation has been a concern in both strands of research. PEBA-II (Milosavljevic et al., 1996), for example, generates targetgroup-specific texts for novice and experts users from taxonomical information, relying on a phrasal lexicon that is similar in spirit to our ontology lexicon. Statistical approaches such as (Isard et al., 2006), on the other hand, use text corpora to generate personalized texts. Our approach is hybrid in the sense that it enriches a classical rule-based approach with statistical data in the microplanning and realisation steps, thus being comparable to systems like HALogen (Langkilde and Knight, 1998) and pCRU (Belz, 2008). The main difference is that it uses Semantic Web data as base. Since the emergence of the Semantic Web there has been a strong interest in NLG from Semantic Web data, especially for providing users with natural language access to structured data. Work in this area comprises verbalization of ontologies as well as RDF knowledge bases; for an overview see (Bouayad-Agha et al., to appear). Of particular interest in the context of our work is NaturalOWL (Galanis and Androutsopoulos, 2007), a system that produces descriptions of entities and classes relying on linguistic anno"
W13-2102,W12-1526,0,0.0139241,"niversity, Germany Abstract Open Data cloud, which contains a wide range of factual knowledge that is very interesting to many applications and for many purposes. However, due to the fact that it is available as RDF, it is not directly accessible to humans. Thus, natural language generation from RDF data has recently become an important topic for research, leading to the development of various systems generating natural language text from knowledge bases (Bouayad-Agha et al., 2012a; Mellish and Sun, 2006; Sun and Mellish, 2007; Wilcock and Jokinen, 2003) as well as corresponding shared tasks (Banik et al., 2012; Bouayad-Agha et al., 2012b). Natural language generation (NLG) from knowledge bases requires knowledge about how the concepts in the underlying ontology— individuals, classes and relations—are realised linguistically. For this purpose, lemon, a lexicon model for ontologies, has been developed (McCrae et al., 2011). One of the use cases of lemon is to support natural language generation systems that take as input a knowledge base structured with respect to a given ontology. In this paper, we present a system that relies on lemon lexica for selecting suitable lexicalisations of a given concept"
W13-2102,W05-1601,0,0.0164832,"professionals, and that such texts are preferred by advanced cooks. The rejection of H3 might be caused by the fact that recipes for advanced cooks include some but actually not many technical terms and are therefore also comprehensible for novices. 5 Related work There have been different approaches to natural language generation, ranging from template-based to statistical architectures. While early NLG systems were mainly based on manually created rules (Bourbeau et al., 1990; Reiter et al., 1992), later approaches started applying statistical methods to the subtasks involved in generation (Belz, 2005), focusing on scalability and easy portability and often relying on overgeneration and subsequent ranking of generation possibilities. Personalization has been a concern in both strands of research. PEBA-II (Milosavljevic et al., 1996), for example, generates targetgroup-specific texts for novice and experts users from taxonomical information, relying on a phrasal lexicon that is similar in spirit to our ontology lexicon. Statistical approaches such as (Isard et al., 2006), on the other hand, use text corpora to generate personalized texts. Our approach is hybrid in the sense that it enriches"
W13-2102,mccrae-etal-2012-collaborative,1,0.822549,"ery to the knowledge base. This verb would therefore only be used in the context of technical registers, i.e. with advanced cooks as target group. After having manually created lexical entries with their base forms, we automatically enrich them with inflectional forms extracted from Wiktionary, as already indicated in Figure 2. The ontology, the RDF recipes as well as the ontology lexicon can be accessed at http://www.sc.cit-ec.uni-bielefeld. de/natural-language-generation. Although the manual creation of lemon lexica is feasible for small domains (and supported by tools such as lemon source (McCrae et al., 2012)), it does not scale to larger domains without a significant amount of effort. Therefore corpus-based methods for the semiautomatic creation of ontology lexica are currently developed, see (Walter et al., 2013). 16 17 lemon : sense [ lemon : reference action : schneiden ]. Figure 2: Lexical entry for the verb schneiden, denoting a cutting action. 1 2 : tranchieren a lemon : LexicalEntry ; lexinfo : partOfSpeech lexinfo : verb ; 3 4 5 lemon : canonicalForm [ lemon : writtenRep "" tranchieren "" @de ]; 6 7 8 9 10 11 12 13 14 15 lemon : sense [ lemon : reference action : schneiden ; lemon : conditi"
W13-2102,C92-1038,0,0.0328595,"d be generated. • A zero anaphora, i.e. an empty referring expression, as in Bake for 60 minutes or Simmer until done. The use of those variants is regulated by a system parameter λpronoun , where a high value forces the use of abstract expressions and zero anaphora, while a low value prefers the use of exact ingredient names. In future work the decision of which referring expression to use should be decided on the basis of general principles, such as uniqueness of the referent, avoidance of unnecessary and inappropriate modifiers, brevity, and preference for simple lexical items, see, e.g., (Reiter and Dale, 1992). An exception to the above rules are interim ingredients, whose realisation is determined as follows. If there is a lexical entry for the interim, it is used for verbalization. If there is no lexical entry, then the name of the main ingredient used in the creation of the interim is used. Furthermore, we define and exploit manually specified meaning postulates to create names for specific, common interims. For example dough is used if the interim is generated from flour and at least one of the ingredients butter, sugar, egg or backing powder. 3.2 Surface realisation The input to the surface re"
W13-2102,W12-1527,0,0.0214253,"L¨ uker, David Nagel, Christina Unger Semantic Computing Group Cognitive Interaction Technology – Center of Excellence (CITEC), Bielefeld University, Germany Abstract Open Data cloud, which contains a wide range of factual knowledge that is very interesting to many applications and for many purposes. However, due to the fact that it is available as RDF, it is not directly accessible to humans. Thus, natural language generation from RDF data has recently become an important topic for research, leading to the development of various systems generating natural language text from knowledge bases (Bouayad-Agha et al., 2012a; Mellish and Sun, 2006; Sun and Mellish, 2007; Wilcock and Jokinen, 2003) as well as corresponding shared tasks (Banik et al., 2012; Bouayad-Agha et al., 2012b). Natural language generation (NLG) from knowledge bases requires knowledge about how the concepts in the underlying ontology— individuals, classes and relations—are realised linguistically. For this purpose, lemon, a lexicon model for ontologies, has been developed (McCrae et al., 2011). One of the use cases of lemon is to support natural language generation systems that take as input a knowledge base structured with respect to a giv"
W13-2102,A92-1009,0,0.107829,"et groups or stylistic variants. that texts generated for professionals are indeed perceived as being generated for professionals, and that such texts are preferred by advanced cooks. The rejection of H3 might be caused by the fact that recipes for advanced cooks include some but actually not many technical terms and are therefore also comprehensible for novices. 5 Related work There have been different approaches to natural language generation, ranging from template-based to statistical architectures. While early NLG systems were mainly based on manually created rules (Bourbeau et al., 1990; Reiter et al., 1992), later approaches started applying statistical methods to the subtasks involved in generation (Belz, 2005), focusing on scalability and easy portability and often relying on overgeneration and subsequent ranking of generation possibilities. Personalization has been a concern in both strands of research. PEBA-II (Milosavljevic et al., 1996), for example, generates targetgroup-specific texts for novice and experts users from taxonomical information, relying on a phrasal lexicon that is similar in spirit to our ontology lexicon. Statistical approaches such as (Isard et al., 2006), on the other h"
W13-2102,W07-2316,0,0.0385535,"puting Group Cognitive Interaction Technology – Center of Excellence (CITEC), Bielefeld University, Germany Abstract Open Data cloud, which contains a wide range of factual knowledge that is very interesting to many applications and for many purposes. However, due to the fact that it is available as RDF, it is not directly accessible to humans. Thus, natural language generation from RDF data has recently become an important topic for research, leading to the development of various systems generating natural language text from knowledge bases (Bouayad-Agha et al., 2012a; Mellish and Sun, 2006; Sun and Mellish, 2007; Wilcock and Jokinen, 2003) as well as corresponding shared tasks (Banik et al., 2012; Bouayad-Agha et al., 2012b). Natural language generation (NLG) from knowledge bases requires knowledge about how the concepts in the underlying ontology— individuals, classes and relations—are realised linguistically. For this purpose, lemon, a lexicon model for ontologies, has been developed (McCrae et al., 2011). One of the use cases of lemon is to support natural language generation systems that take as input a knowledge base structured with respect to a given ontology. In this paper, we present a system"
W13-2102,C90-1021,0,\N,Missing
W13-2102,C98-1112,0,\N,Missing
W13-5203,E12-1059,0,0.0126561,"rase table of the Moses system (Koehn et al., 2007) trained on Europarl data (Koehn, 2005). We used the system primarily in an ‘off-the-shelf’ manner in order to focus on the effect of adding the linked data translations. Moses uses a log-linear model as the baseline for its translations, where translations are generated by a decoder and evaluated according to the following model: Translations mined from linguistic linked data For finding translations from linguistic linked data, we focus on the lemonUby (Eckle-Kohler et al., 2013) resource, which is a linked data version of the UBY resource (Gurevych et al., 2012). This resource contains lemon versions of a number of resources in particular: p(t|f ) = exp( X φi (t, f )) i Where t is the candidate translation sentence, f is the input foreign text and φi are scoring functions. In the phrase-based model the translation is derived compositionally by considering phrases and their translations stored in the so called phrase table of the Moses system. The main challenge in integrating these translations derived from linked data lies in the fact that they lack a probability score. For each translation pair (a, b) derived from the linked data, we distinguish tw"
W13-5203,J10-4005,0,0.0249879,"s been a massive explosion in the amount and quality of data available as linked data on the web. This data frequently describes entities in multiple languages and as such can be used as a source of translations. In particular, the web contains much data about named entities, such as locations, films, people and so forth, and often these named entities have translations in many languages. In this paper, we address the question of what can be achieved by using the large amounts of data available as multilingual Linked Open Data (LOD). As state-of-theart statistical machine translation systems (Koehn, 2010) are typically trained on outdated or out-ofdomain parallel corpora such as on the transcripts of the European Parliament (Koehn, 2005), we expect to increase the coverage of domain-specific terminology. In addition, there has recently been a move towards the publication of of language resources using linked data principles (Chiarcos et al., 2011), which can be expected to lead to a significant increase the availability of information relevant to NLP on the Web. In particular, the representation of legacy resources such as Wiktionary and 2 Mining translations from the linked open data cloud Ob"
W13-5203,P98-1013,0,0.0313697,"e, f is the input foreign text and φi are scoring functions. In the phrase-based model the translation is derived compositionally by considering phrases and their translations stored in the so called phrase table of the Moses system. The main challenge in integrating these translations derived from linked data lies in the fact that they lack a probability score. For each translation pair (a, b) derived from the linked data, we distinguish two cases: If the translation is already in the phrase table, we add a new feature that is set to 1.0 to indicate that the translation was found • FrameNet (Baker et al., 1998) • OmegaWiki 3 • VerbNet (Schuler, 2005) • Wiktionary 4 • WordNet (Fellbaum, 2010) 1 We use the dump of the 3.5 version The dump was downloaded on June 21st 2013 3 http://omegawiki.org 4 http://www.wiktionary.org 2 9 Resource BTC DBpedia FreeBase All English Labels 398,902,866 7,332,616 41,261,806 447,497,288 German Labels 144,226 590,381 1,654,254 2,338,861 Translations 51,756 540,134 259,923 665,910 Table 1: The number of labels and translations found in the linked data cloud by resource &lt;OW_eng_LexicalEntry_0#CanonicalForm&gt; lemon:writtenRep &quot;rain&quot;@eng. &lt;OW_eng_LexicalEntry_0&gt; lemon:canonica"
W13-5203,P03-1021,0,0.00589599,"fluent in English, and had a Cohen’s Kappa Agreement of 0.56. In addition, we calculated BLEU (Papineni et al., 2002) scores. The results are presented in Table 3. Table 3: The comparative evaluation of the translations with and without linked data from the Linked Data Cloud. If the translation was not in the phrase table, we add a new entry with probability 1.0 for all scores and the feature for linked data set to 1.0. For all other translations, the feature indicating provenance from the Linked Data Cloud is set to 0.0. The weights for the loglinear model are learned using the MERT system (Och, 2003). As such we do not use the linked data itself to choose between different translation candidates but rely on the methods built into the machine translations system, in particular the language model. 5 Results We extracted the baseline phrase table, reordering and language model from version 7 of the EuroParl corpus translating from English to German. In order to evaluate the impact of Linked Data translations on translation quality, we rely on the News Commentary 2011 corpus provided as part of the WMT-12 translation task (Callison-Burch et al., 2012). We found that 25,688 translations from t"
W13-5203,P02-1040,0,0.0880865,"translation. For each translation, we performed a manual evaluation with two evaluators. They were both presented with 50 translations, one with linked data and one without linked data and asked to choose the best one (“no opinion” was also allowed). The translations were presented in a random order and there was no indication which system they came from so this experiment was performed blind. The evaluators were a native English speaker, who is fluent in German, and a native German speaker, who is fluent in English, and had a Cohen’s Kappa Agreement of 0.56. In addition, we calculated BLEU (Papineni et al., 2002) scores. The results are presented in Table 3. Table 3: The comparative evaluation of the translations with and without linked data from the Linked Data Cloud. If the translation was not in the phrase table, we add a new entry with probability 1.0 for all scores and the feature for linked data set to 1.0. For all other translations, the feature indicating provenance from the Linked Data Cloud is set to 0.0. The weights for the loglinear model are learned using the MERT system (Och, 2003). As such we do not use the linked data itself to choose between different translation candidates but rely o"
W13-5203,W12-3102,0,0.0675323,"Missing"
W13-5203,C98-1013,0,\N,Missing
W13-5203,P07-2045,0,\N,Missing
W13-5203,2005.mtsummit-papers.11,0,\N,Missing
W13-5501,I08-1051,0,0.0753902,"esent typologically relevant phenomena, along with examples for their illustration and annotations (glosses) and translations applied to these examples (structurally comparable to corpus data), or word lists (structurally comparable to lexical-semantic resources). RDF as a generic representation formalism is thus particularly appealing for this class of resources. Finally, for linguistic corpora (Fig. 1, corpora), the potential of the Linked Data paradigm for modeling, processing and querying of corpora is immense, and RDF conversions of semantically annotated corpora have been proposed early [3]. RDF provides a graph-based data model as required for the interoperable representation of arbitrary kinds of annotation [2, 15], and this flexibility makes it a promising candidate for a general means of representation for corpora with complex and heterogeneous annotations. RDF does not only establish interoperability between annotations within a corpus, but also between corpora and other linguistic resources [4]. In comparison to other types of linguistic resources, corpora are currently underrepresented in the LLOD cloud, but the development of schemes for corpora and/or NLP annotations re"
W13-5501,chiarcos-2012-ontologies,1,0.897695,"to establish conceptual interoperability between language resources. If resourcespecific annotations or abbreviations are expanded into references to repositories of linguistic terminology and/or metadata categories, linguistic annotations, grammatical features and metadata specifications become more easily comparable. Important repositories developed by different communities include GOLD [9] and ISOcat [20, 19], yet, only recently these terminology repositories were put in relation with each other using Linked Data principles and with linguistic resources, e.g., within the OLiA architecture [5]. Linguistic databases are a particularly heterogeneous group of linguistic resources; they contain complex and manifold types of information, e.g., feature structures that represent typologically relevant phenomena, along with examples for their illustration and annotations (glosses) and translations applied to these examples (structurally comparable to corpus data), or word lists (structurally comparable to lexical-semantic resources). RDF as a generic representation formalism is thus particularly appealing for this class of resources. Finally, for linguistic corpora (Fig. 1, corpora), the p"
W13-5501,W07-1501,0,0.0927928,"s refer to ISOcat URIs. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for common problems, e.g., the development of a database that is capable of supporting flexible, graph-based data structures as necessary for multi-layer corpora [15]. Beyond this, another advantage warrants a mention: The distributed approach of the Linked Data paradigm facilitates the distributed development of a web of resources and collaboration between researchers that provide and use this data and that employ a shared set of technologies. One consequence is the emergence of interdisciplinary efforts to create large and interconnected sets of resources in linguistics and beyond. LDL-2013 aims to provide a forum to discuss and to facilitate such on-going developments. LLOD: Building the Cloud Recent years have seen not only a number of approaches to pr"
W13-5501,wright-2004-global,0,0.0894745,"able URIs, it is possible to combine information from physically separated repositories in a single query at runtime. Information from different resources in the cloud can then be integrated freely. Dynamic Import If cross-references between linguistic resources are represented by resolvable URIs instead of system-defined ID references or static copies of parts from another resource, it is not only possible to resolve them at runtime, but also to have access to the most recent version of a resource. For community-maintained terminology repositories like the ISO TC37/SC4 Data Category Registry [20, 19, ISOcat], for example, new categories, definitions or examples can be introduced occasionally, and this information is available immediately to anyone whose resources refer to ISOcat URIs. Ecosystem RDF as a data exchange framework is maintained by an interdisciplinary, large and active community, and it comes with a developed infrastructure that provides APIs, database implementations, technical support and validators for various RDF-based languages, e.g., reasoners for OWL. For developers of linguistic resources, this ecosystem can provide technological support or off-the-shelf implementations for c"
W13-5507,ide-etal-2000-xces,0,0.0754649,"11,812 constituents (including terminal nodes) from the orthographically corrected chat messages (resulting in a total average of 17.75 constituent nodes per chat message). 3 From internal representations to RDF 3.1 Internal representation We developed FiESTA (an acronym for “format for extensive spatiotemporal annotations”), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000). There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003). One of the most pressing problems is the restriction to a single, flat stream or sequence of primary data (called “text” in some approaches), or a single, flat timeline. In several data collections we need to support multiple timelines, especially in cases where multiple novel recording and tracking"
W13-5507,W03-0804,0,0.0333824,"e spatiotemporal annotations”), which takes into account various approaches, among them, the annotation graph approach (Bird and Liberman, 2001), the NITE object model (Evert et al., 2003), the speech transcription facilities of the TEI P5 specification (TEI Consortium, 2008), and the (X)CES standard (Ide et al., 2000). There were shortcomings in all these approaches that made it very difficult to express complex multimodal data structures. These shortcomings can also be found in theories and models that are more established in the Linked Data community, such as POWLA (Chiarcos, 2012) or LAF (Ide et al., 2003). One of the most pressing problems is the restriction to a single, flat stream or sequence of primary data (called “text” in some approaches), or a single, flat timeline. In several data collections we need to support multiple timelines, especially in cases where multiple novel recording and tracking devices are used whose temporal synchronisation is nontrivial (because of irregular tracking intervals, computational delay, etc.). However, when working in a project with a limited duration, researchers are under time pressure, as a consequence, it can become necessary to perform analyses of dat"
W13-5507,P03-1054,0,0.00355911,"mat internal RDF data structure Turtle custom RDF RDF Views RDF Views Views RDF/XML FiESTA / MExiCo library POSEIdON library Rails view generation Figure 3: Architecture of the corpus management web application, grouped into scopes of responsibility of the respective libraries (FiESTA and POSEIdON). 1. A transformation of the written messages into orthographically and syntactically correct utterances. This was necessary for the parser (see below) to perform with an adequate accuracy. 2. Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008). 3. Syntactic and semantic properties of sentences were annotated, among them elaborateness (e.g., fragments and full sentences), speech acts (e.g., greetings, instructions, corrections, feedback) and localisation strategies – for instance, whether positions were described in relation to present objects (“to the right of the circle”), by describing absolute locations of the board itself (“into the bottomleft corner”), or by using metaphors (such as points of the compass, floors of buildings for rows: “south of"
W13-5507,W08-1006,0,0.0158476,"XML FiESTA / MExiCo library POSEIdON library Rails view generation Figure 3: Architecture of the corpus management web application, grouped into scopes of responsibility of the respective libraries (FiESTA and POSEIdON). 1. A transformation of the written messages into orthographically and syntactically correct utterances. This was necessary for the parser (see below) to perform with an adequate accuracy. 2. Utterances were segmented into sentences and then parsed with the Stanford Parser (Klein and Manning, 2002; Klein and Manning, 2003), using the German version trained on the Negra corpus (Rafferty and Manning, 2008). 3. Syntactic and semantic properties of sentences were annotated, among them elaborateness (e.g., fragments and full sentences), speech acts (e.g., greetings, instructions, corrections, feedback) and localisation strategies – for instance, whether positions were described in relation to present objects (“to the right of the circle”), by describing absolute locations of the board itself (“into the bottomleft corner”), or by using metaphors (such as points of the compass, floors of buildings for rows: “south of the circle”). 4. The parse trees were further annotated with basic tree measures (d"
W14-0507,altosaar-etal-2010-speech,0,0.0739945,"Missing"
W14-0507,sloetjes-wittenburg-2008-annotation,0,0.0643646,"Missing"
W14-0507,E12-1024,0,0.0553988,"Missing"
W14-2608,P06-1038,0,0.0113195,"ected excerpts containing the phrase “said sarcastically”, removed that phrase and performed a regression analysis on the remaining text, exploiting the number of words as well as the occurrence of adjectives, adverbs, interjections, exclamation and question marks as features. Tsur et al. (2010) present a system to identify sarcasm in Amazon product reviews exploiting features such as sentence length, punctuation marks, the total number of completely capitalized words and automatically generated patterns which are based on the occurrence frequency of different terms (following the approach by Davidov and Rappoport (2006)). Unfortunately, their corpus is not publicly available. Carvalho et al. (2009) use eight patterns to identify ironic utterances in comments on articles from a Portuguese online newspaper. These patterns contain positive predicates and utilize punctuation, interjections, positive words, emoticons, or onomatopoeia and acronyms for laughing as well as some Portuguese-specific patterns considering the verb-morphology. Gonz´alezIb´an˜ ez et al. (2011) differentiate between sarcastic and positive or negative Twitter messages. They Background Irony is an important and frequent device in human commu"
W14-2608,W10-2914,0,0.395054,"zed as being ironic or non-ironic. We investigate different classifiers and focus on the impact analysis of different features by investigating what effect their elimination has on the performance of the approach. In the following, we describe the features used and the set of classifiers compared. 2.1 Features 2.2 To estimate if a review is ironic or not, we measure a set of features. Following the idea that irony is expressing the opposite of its literal content, we take into account the imbalance between the overall (prior) polarity of words in the review and the star-rating (as proposed by Davidov et al. (2010)). We assume the imbalance to hold if the star-rating Classifiers In order to perform the classification based on the features mentioned above, we explore a set of standard classifiers typically used in text classification research. We employ the open source machine learning library scikit-learn (Pedregosa et al., 2011) for Python. 7 Note that examples can show that this is not always the case. Funny or odd products ironically receive a positive starrating. However, this feature may be a strong indicator for irony. 6 The system as implemented to perform the described experiments is made availa"
W14-2608,filatova-2012-irony,0,0.106996,"fiers of sentence complexity, structural, morphosyntactic and semantic ambiguity, polarity, unexpectedness, and emotional activation, imagery, and pleasantness of words. Tepperman et al. (2006) performed experiments to recognize sarcasm in spoken language, specifically in the expression “yeah right”, using spectral, contextual and prosodic cues. On the one hand, their results show that it is possible to identify sarcasm based on spectral and contextual features and, on the other hand, they confirm that prosody is insufficient to reliably detect sarcasm (Rockwell, 2005, p. 118). Very recently, Filatova (2012) published a product review corpus from Amazon, being annotated with Amazon Mechanical Turk. It contains 437 ironic and 817 non-ironic reviews. A more detailed description of this resource can be found in Section 3.1. To our knowledge, no automatic classification approach has been evaluated on this corpus. We therefore contribute a text classification system including the previously mentioned features. Our results serve as a strong baseline on this corpus as well as an “executable review” of previous work.6 2 is positive (i. e., 4 or 5 stars) but the majority of words is negative, and, vice ve"
W14-2608,P11-2102,0,0.388971,"ed words and automatically generated patterns which are based on the occurrence frequency of different terms (following the approach by Davidov and Rappoport (2006)). Unfortunately, their corpus is not publicly available. Carvalho et al. (2009) use eight patterns to identify ironic utterances in comments on articles from a Portuguese online newspaper. These patterns contain positive predicates and utilize punctuation, interjections, positive words, emoticons, or onomatopoeia and acronyms for laughing as well as some Portuguese-specific patterns considering the verb-morphology. Gonz´alezIb´an˜ ez et al. (2011) differentiate between sarcastic and positive or negative Twitter messages. They Background Irony is an important and frequent device in human communication that is used to convey an attitude or evaluation towards the propositional content of a message, typically in a humorous fashion (Abrams, 1957, p. 165–168). Between the age of six (Nakassis and Snedeker, 2002) and eight years (Creusere, 2007), children are able to recognize ironic utterances or at least notice that something in the situation is not common (Glenwright and Pexman, 2007). The principle of inferability (Kreuz, 1996) states tha"
W14-2608,W07-0101,0,0.759812,"erent theories such as the echoic account (Wilson and Sperber, 1992), the Pretense Theory (Clark and Gerrig, 1984) or the Allusional Pretense Theory (KumonNakamura et al., 1995) have challenged the understanding that an ironic utterance typically conveys the opposite of its literal propositional content. However, in spite of the fact that the attributive nature of irony is widely accepted (see Wilson and Sperber (2012)), no formal or operational definition of irony is available as of today. 1.2 Previous Work Corpora providing annotations as to whether expressions are ironic or not are scarce. Kreuz and Caucci (2007) have automatically generated such a corpus exploiting Google Book search5 . They collected excerpts containing the phrase “said sarcastically”, removed that phrase and performed a regression analysis on the remaining text, exploiting the number of words as well as the occurrence of adjectives, adverbs, interjections, exclamation and question marks as features. Tsur et al. (2010) present a system to identify sarcasm in Amazon product reviews exploiting features such as sentence length, punctuation marks, the total number of completely capitalized words and automatically generated patterns whic"
W14-2608,C96-2162,0,0.153989,"Missing"
W14-2608,W11-1715,0,\N,Missing
W14-2608,barbieri-saggion-2014-modelling-irony,0,\N,Missing
W14-2608,maynard-greenwood-2014-cares,0,\N,Missing
W14-2608,P13-2147,1,\N,Missing
W14-3418,H92-1045,0,0.0379033,"as an entity by the CRF. In addition, the cardinality of these entities in A is taken into account by cumulative binning. The feature long form holds if one of the long forms previously defined to correspond with the abbreviation occurs in the text (in arbitrary position). Besides using all features, we perform a greedy search for the best feature set by wrapping the best model configuration. A detailed discussion of the feature selection process follows in Section 5.3. 4.2.3 Feature Propagation Inspired by the “one sense per discourse” heuristic commonly adopted in word sense disambiguation (Gale et al., 1992), we apply two feature combination strategies. In the following, n denotes the number of occurrences of the abbreviation in an abstract. In the setting propagationall , n − 1 identical linked instances are added for each occurrence. Each new instance consists of the disjunction of the feature vectors of all occurrences. Based on the intuition that the first mention of an abbreviation might carry particularly valuable information, propagationfirst introduces one additional linked instance for each occurrence, in which the feature vector is joined with the first occurrence. 8 Using the stopword"
W14-3418,W03-1306,0,0.118085,"Missing"
W14-4724,W06-1805,0,0.510785,"Missing"
W14-4724,W13-2102,1,0.879649,"Missing"
W14-4724,P88-1004,0,0.702557,"Missing"
W14-4724,peters-peters-2000-treatment,0,0.0389614,"round, heavy, wooden, inlaid magnifying glass • ‘round’ represents the Formal role (giving indications of shape and dimensionality) • ‘heavy’ and ‘wooden’ related to the Constitutive role and indicate the relation between the object and its parts (e. g. by specifying weight, material, parts and components) • ‘inlaid’ is the Agentive role of the lexical item, denoting the factors that have been involved in the generation of the objects, such as creator, artifact, natural kind, and causal chain • ‘magnifying’ describes the Telic role of ‘glass’, since it shows its purpose and function Finally, Peters and Peters (2000) provide one of the few other practical reports on modelling adjectives with ontologies, in the context of the SIMPLE lexica. This work is primarily focussed on the categorization of by means of intensional and extensional properties, rather than due to their logical modelling. 6 Conclusion In this paper we have proposed an approach to model the semantics of adjectives in the context of the lexicon-ontology interface with a focus on the ontology-lexicon model lemon. We have argued that the semantics of adjectives, in particular gradable and privative adjectives, is beyond what can be expressed"
W14-4724,J91-4003,0,0.422441,"e gradable, i.e. whether a comparative or superlative statement with these adjectives makes sense. For example, adjectives such as ‘big’ or ‘tall’ can express relationships such as ‘X is bigger than Y ’. However it is not possible to say that one individual is ‘more former’. Most gradable adjectives are subsective (e. g.‘a big mouse’ is not ‘a big animal’ (Morzycki, 2013a)). Finally, we consider operator or property-modifying adjectives. They can be understood along the lines of privative adjectives but differ in that they represent operators that modify some property in the qualia structure (Pustejovsky, 1991) of the class. For instance, we may express the adjective ‘former’ in lambda calculus as a function that takes a class C as input and returns the class of entities that were a member of C to some prior time point t (Partee, 2003): λC[λx∃tC(x, t) ∩ t < now] Such adjectives have not only a difference in semantic meaning but can also frequently have syntactic impact, for example in adjective ordering restrictions, as they may be reordered with only semantic impact (Teodorescu, 2006), e.g., 4. (a) A big red car. (b) ? A red big car. 5. (a) A famous former actor. (b) A former famous actor. Finally,"
W14-6204,doddington-etal-2004-automatic,0,0.0240142,"rmance and for specific entity classes such as organisms (Pafilis et al., 2013) or symptoms (Savova et al., 2010; Jimeno et al., 2008). A detailed overview on named entity recognition, covering other domains as well, can be found in Nadeau and Sekine (2007). The use case described in this paper, however, involves a highly relational problem structure in the sense that individual facts or relations have to be aggregated in order to yield accurate, holistic domain knowledge, which corresponds most closely to the problem structure encountered in event extraction, as triggered by the ACE program (Doddington et al., 2004; Ji and Grishman, 2008; Strassel et al., 2008), and the BioNLP shared task series (Nedellec et al., 2013; Tsujii et al., 2011; Tsujii, 2009). General semantic search engines in the biomedical domain mainly focus on isolated entities. Relations are typically only taken into account by co-occurrence on abstract or sentence level. Examples for such search engines include GoPubMed (Doms and Schroeder, 2005), SCAIView (Hofmann-Apitius et al., 2008), and GeneView (Thomas et al., 2012). With respect to the extraction methodology, our work is similar to Saggion et al. (2007) and Buitelaar et al. (200"
W14-6204,P08-1030,0,0.0139659,"ntity classes such as organisms (Pafilis et al., 2013) or symptoms (Savova et al., 2010; Jimeno et al., 2008). A detailed overview on named entity recognition, covering other domains as well, can be found in Nadeau and Sekine (2007). The use case described in this paper, however, involves a highly relational problem structure in the sense that individual facts or relations have to be aggregated in order to yield accurate, holistic domain knowledge, which corresponds most closely to the problem structure encountered in event extraction, as triggered by the ACE program (Doddington et al., 2004; Ji and Grishman, 2008; Strassel et al., 2008), and the BioNLP shared task series (Nedellec et al., 2013; Tsujii et al., 2011; Tsujii, 2009). General semantic search engines in the biomedical domain mainly focus on isolated entities. Relations are typically only taken into account by co-occurrence on abstract or sentence level. Examples for such search engines include GoPubMed (Doms and Schroeder, 2005), SCAIView (Hofmann-Apitius et al., 2008), and GeneView (Thomas et al., 2012). With respect to the extraction methodology, our work is similar to Saggion et al. (2007) and Buitelaar et al. (2008), in that a combinati"
W14-6204,N06-4006,0,0.0388148,"Missing"
W14-6204,strassel-etal-2008-linguistic,0,0.0190519,"rganisms (Pafilis et al., 2013) or symptoms (Savova et al., 2010; Jimeno et al., 2008). A detailed overview on named entity recognition, covering other domains as well, can be found in Nadeau and Sekine (2007). The use case described in this paper, however, involves a highly relational problem structure in the sense that individual facts or relations have to be aggregated in order to yield accurate, holistic domain knowledge, which corresponds most closely to the problem structure encountered in event extraction, as triggered by the ACE program (Doddington et al., 2004; Ji and Grishman, 2008; Strassel et al., 2008), and the BioNLP shared task series (Nedellec et al., 2013; Tsujii et al., 2011; Tsujii, 2009). General semantic search engines in the biomedical domain mainly focus on isolated entities. Relations are typically only taken into account by co-occurrence on abstract or sentence level. Examples for such search engines include GoPubMed (Doms and Schroeder, 2005), SCAIView (Hofmann-Apitius et al., 2008), and GeneView (Thomas et al., 2012). With respect to the extraction methodology, our work is similar to Saggion et al. (2007) and Buitelaar et al. (2008), in that a combination of gazetteers and ext"
W14-6204,W09-1400,0,\N,Missing
W15-4205,calzolari-etal-2012-lre,0,0.098711,"ar}@insight-centre.org Abstract statistical taggers, statistical parsers, and statistical machine translation systems) or they require lexico-semantic resources as background knowledge to perform some task (e.g. word sense disambiguation). As the number of language resources available keeps growing, the task of discovering and finding resources that are pertinent to a particular task becomes increasingly difficult. While there are a number of repositories that collect and index metadata of language resources, such as META-SHARE (Federmann et al., 2012), CLARIN (Broeder et al., 2010), LRE-Map (Calzolari et al., 2012), Datahub.io1 and OLAC (Simons and Bird, 2003), they do not provide a complete solution to the discovery problem for two reasons. First, integrated search over all these different repositories is not possible, as they use different data models, different vocabularies and expose different interfaces and APIs. Second, these repositories must strike a balance between quality and coverage, either opting for coverage at the expense of quality of metadata, or vice versa. When collecting metadata from multiple resources, we understand that there are two principal challenges: property harmonization an"
W15-4205,choukri-etal-2012-using,0,0.0195543,"x XML schema is provided to describe metadata of resources (Gavrilidou et al., 2012). At the same time, considerable effort has been devoted to ensuring data quality (Piperidis, 2012). In contrast, CLARIN does not provide a single schema, but a set of ‘profiles’ that are described in a schema language called the CMDI Component Specification Language (Broeder et al., 2012). Each institute describing resources using CMDI can instantiate the vocabulary to suit their particular needs. Similarly, an attempt has been made to catalogue language resources by assigning them a single unique identifier (Choukri et al., 2012). Other more decentralized approaches are found in initiatives such as the LRE-Map (Calzolari et al., 2012) which provides a repository for researchers who want to submit the resources accompanying papers submitted to conferences. Most fields in LRE-Map consist of a text field with some prespecified options to select and a thorough analysis of the results has been conducted (Mariani et al., 2014). Similarly, the Open Linguistics Working Group (Chiarcos et al., 2012) has been collecting language resources published as linked data in a Related Work Interoperability of metadata is an important pr"
W15-4205,de-marneffe-etal-2006-generating,0,0.0158775,"Missing"
W15-4205,federmann-etal-2012-meta,0,0.277944,"Missing"
W15-4205,broeder-etal-2010-data,0,0.0785251,"Missing"
W15-4205,mariani-etal-2014-facing,0,0.0288608,"Missing"
W15-4205,piperidis-2012-meta,0,0.0544122,"oaches have been pursued to collect metadata of resources. Large consortium-led projects and initiatives such as the CLARIN projects and METANET have attempted to create metadata standards for representing linguistic data. Interoperability of the data stemming from these two repositories is however severely limited due to incompatibilities in their data models. META-SHARE favors a qualitative approach in which a relatively complex XML schema is provided to describe metadata of resources (Gavrilidou et al., 2012). At the same time, considerable effort has been devoted to ensuring data quality (Piperidis, 2012). In contrast, CLARIN does not provide a single schema, but a set of ‘profiles’ that are described in a schema language called the CMDI Component Specification Language (Broeder et al., 2012). Each institute describing resources using CMDI can instantiate the vocabulary to suit their particular needs. Similarly, an attempt has been made to catalogue language resources by assigning them a single unique identifier (Choukri et al., 2012). Other more decentralized approaches are found in initiatives such as the LRE-Map (Calzolari et al., 2012) which provides a repository for researchers who want t"
W15-4205,Q14-1019,1,\N,Missing
W15-4205,gavrilidou-etal-2012-meta,0,\N,Missing
W15-4207,ehrmann-etal-2014-representing,1,0.796049,"degree of linking of the Linguistic Linked Data cloud. In this paper we describe the results of a small project attempting to link four datasets of different types (two terminologies, one lexico-conceptual resource and one corpus). As terminological resources, we have considered the Glossary of the European Migration Network (EMN)1 as well as the Interactive Terminology for Europe (IATE) 2 . They are both represented using the lemon model (McCrae et al., 2012). As lexico-conceptual resource we rely on BabelNet (Navigli and Ponzetto, 2012), which has been previously migrated into Linked Data (Ehrmann et al., 2014). As corpus we use the Manually Annotated Subcorpus (MASC) of the American National Corpus (Ide et al., 2008), which contains disambiguated links to BabelNet. We describe how the datasets have been migrated to RDF and describe our methodology for linking the datasets at the lexical entry level and present a sampled evaluation of the quality of the induced links. We first use a simple technique based on strict matching of the canonical form of lexical entries in different resources. By this we then link the EMN to both IATE and BabelNet. MASC has been previously linked to BabelNet and we includ"
W15-4207,W07-1501,0,0.0905595,"Missing"
W15-4207,ide-etal-2008-masc,0,0.0185752,"attempting to link four datasets of different types (two terminologies, one lexico-conceptual resource and one corpus). As terminological resources, we have considered the Glossary of the European Migration Network (EMN)1 as well as the Interactive Terminology for Europe (IATE) 2 . They are both represented using the lemon model (McCrae et al., 2012). As lexico-conceptual resource we rely on BabelNet (Navigli and Ponzetto, 2012), which has been previously migrated into Linked Data (Ehrmann et al., 2014). As corpus we use the Manually Annotated Subcorpus (MASC) of the American National Corpus (Ide et al., 2008), which contains disambiguated links to BabelNet. We describe how the datasets have been migrated to RDF and describe our methodology for linking the datasets at the lexical entry level and present a sampled evaluation of the quality of the induced links. We first use a simple technique based on strict matching of the canonical form of lexical entries in different resources. By this we then link the EMN to both IATE and BabelNet. MASC has been previously linked to BabelNet and we included these links into our version of MASC. The paper is structured as follows: in the next Section 2 we briefly"
W15-4207,Q14-1019,0,0.0727669,"Missing"
W17-4124,W15-3904,0,0.071631,"Missing"
W17-4124,D10-1101,0,0.0430935,"nalysis on the one side and character-level neural text processing on the other side. In this section we give a brief overview of both domains and point out parallels to previous work. Fine-Grained Sentiment Analysis San Vicente et al. (2015) present a system that addresses opinion target extraction as a sequence labeling problem based on a perceptron algorithm with local features. Toh and Wang (2014) propose a Conditional Random Field (CRF) as a sequence labeling model that includes a variety of features such as Part-ofSpeech (POS) tags and dependencies, word clusters and WordNet taxonomies. Jakob and Gurevych (2010) follow a very similar approach that addresses opinion target extraction as a sequence labeling problem using CRFs. Their approach includes features derived from words, POS tags and dependency paths, and performs well in a single and cross-domain setting. Klinger and Cimiano (2013a,b) model the task of joint aspect and opinion term extraction using probabilistic graphical models and rely on Markov Chain Monte Carlo methods for inference. They demonstrate the impact of a joint architecture on the task with a strong impact on the extraction of aspect terms, but less so for the extraction of opin"
W17-4124,P13-2147,1,0.839553,"n target extraction as a sequence labeling problem based on a perceptron algorithm with local features. Toh and Wang (2014) propose a Conditional Random Field (CRF) as a sequence labeling model that includes a variety of features such as Part-ofSpeech (POS) tags and dependencies, word clusters and WordNet taxonomies. Jakob and Gurevych (2010) follow a very similar approach that addresses opinion target extraction as a sequence labeling problem using CRFs. Their approach includes features derived from words, POS tags and dependency paths, and performs well in a single and cross-domain setting. Klinger and Cimiano (2013a,b) model the task of joint aspect and opinion term extraction using probabilistic graphical models and rely on Markov Chain Monte Carlo methods for inference. They demonstrate the impact of a joint architecture on the task with a strong impact on the extraction of aspect terms, but less so for the extraction of opinion terms. 3 Model In this work, we approach the task of extracting opinion target expressions by phrasing it as a sequence labeling problem. Doing so allows us to extract an arbitrary number of multi-word expressions in a given text. We use the IOB scheme (Tjong Kim Sang and Veen"
W17-4124,E99-1023,0,0.0611932,"Cimiano (2013a,b) model the task of joint aspect and opinion term extraction using probabilistic graphical models and rely on Markov Chain Monte Carlo methods for inference. They demonstrate the impact of a joint architecture on the task with a strong impact on the extraction of aspect terms, but less so for the extraction of opinion terms. 3 Model In this work, we approach the task of extracting opinion target expressions by phrasing it as a sequence labeling problem. Doing so allows us to extract an arbitrary number of multi-word expressions in a given text. We use the IOB scheme (Tjong Kim Sang and Veenstra, 1999) to represent OTEs as a sequence of tags. According to this scheme, each word in our text receives one of 3 tags, namely I, O or B that indicate if the word is at the Beginning1 , Inside or Outside of an expression: The O Character-Level Neural Network Models Character-level neural network models are gaining interest in many research areas such as language modeling (Kim et al., 2016), spelling correction (Sakaguchi et al., 2017), text classification (Zhang et al., 2015) and more. Most similar works from the area of character-level word representations can be found in (dos Santos and Zadrozny,"
W17-4124,S14-2038,0,0.0999651,"rned character-level word embeddings in more detail. Finally, Section 5 summarizes our findings and presents directions for future work. 2 Related Work Our work brings together the domains of finegrained sentiment analysis on the one side and character-level neural text processing on the other side. In this section we give a brief overview of both domains and point out parallels to previous work. Fine-Grained Sentiment Analysis San Vicente et al. (2015) present a system that addresses opinion target extraction as a sequence labeling problem based on a perceptron algorithm with local features. Toh and Wang (2014) propose a Conditional Random Field (CRF) as a sequence labeling model that includes a variety of features such as Part-ofSpeech (POS) tags and dependencies, word clusters and WordNet taxonomies. Jakob and Gurevych (2010) follow a very similar approach that addresses opinion target extraction as a sequence labeling problem using CRFs. Their approach includes features derived from words, POS tags and dependency paths, and performs well in a single and cross-domain setting. Klinger and Cimiano (2013a,b) model the task of joint aspect and opinion term extraction using probabilistic graphical mode"
W17-4124,P16-1101,0,0.0194609,"interest in developing sentiment analysis models that predict sentiment at a more fine-grained level than at the level of a complete document. A key task within fine-grained sentiment analysis consists in identifying so called opinion target expressions (OTE). These are the objects of a sentiment expression. Consider the following example: “ Moules were excellent , but the lobster ravioli was VERY salty ! ” 159 Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 159–167, c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics. Ma and Hovy, 2016). In these works, word and character level representations are successfully learned and combined to improve Part-of-Speech (POS) tagging and Named Entity Recognition (NER). dos Santos and Zadrozny (2014) and dos Santos et al. (2015) apply a convolutional neural network (CNN) to the raw character sequence that detects character patterns and represents them as a fixed-sized embedding vector. The concatenated sequence of word and character-level embeddings is then used to predict POS or NER tags for each word. Ma and Hovy (2016) use a similar CNN-based word structure model. However, the subsequen"
W17-4124,S16-1002,0,0.0236393,"7, 2017. 2017 Association for Computational Linguistics. Ma and Hovy, 2016). In these works, word and character level representations are successfully learned and combined to improve Part-of-Speech (POS) tagging and Named Entity Recognition (NER). dos Santos and Zadrozny (2014) and dos Santos et al. (2015) apply a convolutional neural network (CNN) to the raw character sequence that detects character patterns and represents them as a fixed-sized embedding vector. The concatenated sequence of word and character-level embeddings is then used to predict POS or NER tags for each word. Ma and Hovy (2016) use a similar CNN-based word structure model. However, the subsequent processing of the embedded word sequence is carried out using a bidirectional Long Short-Term Memory network (LSTM). An example of character-level text classification not requiring any tokenization is given by Zhang et al. (2015). In their work, the authors perform text classification using character-level CNNs on very large datasets and obtain comparable results to traditional models based on words. Their findings suggest that the standard tokenization of text is indeed something to be reconsidered. character-level neural"
W17-4124,S15-2127,0,0.0609151,"Missing"
W18-4501,D17-1118,0,0.130601,"e, New Mexico, USA, August 25, 2018. 2 Related work: Distributional approaches to semantic change Word-level semantic change. There is a great body of work which uses word vector representations to study changes in word semantics. Examples include Gulordava and Baroni (2011) or Radinsky et al. (2011), while Kim et al. (2014) are among the first to use neural word embeddings to analyze changes in word meanings. Kulkarni et al. (2015) use a similar approach, but automatically identify points of meaning changes based on shifts in the mean of the respective time series. Hamilton et al. (2016) and Dubossarsky et al. (2017) present methods to more precisely quantify trends in changes of word meaning and address a central problem in using word embeddings for diachronic analyses: They align the axes of the vector spaces from neighboring time periods using a mapping derived with orthogonal Procrustes (Hamilton et al., 2016). This method will be presented in more detail in Section 4.3. Concept-level semantic change. There are a number of approaches to studying lexical semantic change above the level of individual words. These include simple co-occurrence statistics or topic modeling (Blei and Lafferty, 2006) which a"
W18-4501,W11-2508,0,0.131067,"y1 . This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. 1 https://gitlab.com/morlikowski/diachronic-analogies-code License details: 1 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 1–11 Santa Fe, New Mexico, USA, August 25, 2018. 2 Related work: Distributional approaches to semantic change Word-level semantic change. There is a great body of work which uses word vector representations to study changes in word semantics. Examples include Gulordava and Baroni (2011) or Radinsky et al. (2011), while Kim et al. (2014) are among the first to use neural word embeddings to analyze changes in word meanings. Kulkarni et al. (2015) use a similar approach, but automatically identify points of meaning changes based on shifts in the mean of the respective time series. Hamilton et al. (2016) and Dubossarsky et al. (2017) present methods to more precisely quantify trends in changes of word meaning and address a central problem in using word embeddings for diachronic analyses: They align the axes of the vector spaces from neighboring time periods using a mapping deriv"
W18-4501,P16-1141,0,0.49295,"erature, pages 1–11 Santa Fe, New Mexico, USA, August 25, 2018. 2 Related work: Distributional approaches to semantic change Word-level semantic change. There is a great body of work which uses word vector representations to study changes in word semantics. Examples include Gulordava and Baroni (2011) or Radinsky et al. (2011), while Kim et al. (2014) are among the first to use neural word embeddings to analyze changes in word meanings. Kulkarni et al. (2015) use a similar approach, but automatically identify points of meaning changes based on shifts in the mean of the respective time series. Hamilton et al. (2016) and Dubossarsky et al. (2017) present methods to more precisely quantify trends in changes of word meaning and address a central problem in using word embeddings for diachronic analyses: They align the axes of the vector spaces from neighboring time periods using a mapping derived with orthogonal Procrustes (Hamilton et al., 2016). This method will be presented in more detail in Section 4.3. Concept-level semantic change. There are a number of approaches to studying lexical semantic change above the level of individual words. These include simple co-occurrence statistics or topic modeling (Bl"
W18-4501,E17-1006,1,0.841214,"cases. In sum, it is beneficial for prediction of diachronic changes in concept vocabularies to treat the concept terms as analogous when weights are learnt to compensate for diachronic drift. However, while all models tend to be coherent in relation to the represented concept, they are only to some degree discriminative in regard to the vocabulary of other concepts. Future work should carry out more in-depth evaluations, annotating task-specific ground truth data and exploring evaluation settings like zero-shot learning which has been show to obtain promising results in related problems (cf. Hartung et al. (2017)). We also expect benefits from training with an objective function which includes negative examples and relates more closely to MRR. Beyond this, we are interested in designing more complex and task-specific models. Not last, we plan to explore use cases based on cooperation with scholars from the humanities. For example, we see potential in analysing how an author’s use of specific concepts changes across works using a combination of both interpretative and automatic methods of diachronic analogy recovery. Acknowledgments We thank Steven Claeyssens, Curator of Digital Collections at the Koni"
W18-4501,W14-2517,0,0.221524,"on 4.0 International License. http://creativecommons.org/licenses/by/4.0/. 1 https://gitlab.com/morlikowski/diachronic-analogies-code License details: 1 Proceedings of Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pages 1–11 Santa Fe, New Mexico, USA, August 25, 2018. 2 Related work: Distributional approaches to semantic change Word-level semantic change. There is a great body of work which uses word vector representations to study changes in word semantics. Examples include Gulordava and Baroni (2011) or Radinsky et al. (2011), while Kim et al. (2014) are among the first to use neural word embeddings to analyze changes in word meanings. Kulkarni et al. (2015) use a similar approach, but automatically identify points of meaning changes based on shifts in the mean of the respective time series. Hamilton et al. (2016) and Dubossarsky et al. (2017) present methods to more precisely quantify trends in changes of word meaning and address a central problem in using word embeddings for diachronic analyses: They align the axes of the vector spaces from neighboring time periods using a mapping derived with orthogonal Procrustes (Hamilton et al., 201"
W18-4501,W14-1618,0,0.0229054,"odel concept change in terms of analogies between concept vocabularies at different points in time. This extends well-established synchronic models of analogy based on word embeddings (Mikolov et al., 2013b) to the diachronic case. We build on the underlying parallelogram model of analogy (Rumelhart and Abrahamson (1973), cf. Chen et al. (2017)), assuming that analogies of the type of “a is to b as c is to d“ can be described by linear relationships between distributional representations of the four words. While parallelogram relationships can be found in other vector representations as well (Levy and Goldberg, 2014), embeddings derived with skip-gram can be considered a robust baseline for analogy tasks (Levy et al., 2015). We detail our approach (Section 3) and propose a number of simple models to learn diachronic analogies (Section 4) which are evaluated quantitatively (Section 6) on a corpus of historical Dutch newspapers (Section 5). We report on two related experiments which are motivated by the intuition that diachronic analogies should be coherent in regard to the represented concept and discriminative in regard to the vocabulary of other concepts. All related code is released publicly1 . This wor"
W18-4501,Q15-1016,0,0.0410095,"ng-standing topic within philosophy, history and linguistics. However, recent work on the computational analysis of semantic change based on word embeddings has surprisingly little to offer in this regard. Most work focuses on the meaning of individual words. In comparison, there are only few contributions which analyze concepts and the changing vocabularies which are used to express them (Kenter et al., 2015; Recchia et al., 2016). The goal of this paper is to provide insights into how distributional semantics (Harris, 1954; Firth, 1957), in particular word embeddings (Mikolov et al., 2013a; Levy et al., 2015), can be used to analyze concept change. We propose to model concept change in terms of analogies between concept vocabularies at different points in time. This extends well-established synchronic models of analogy based on word embeddings (Mikolov et al., 2013b) to the diachronic case. We build on the underlying parallelogram model of analogy (Rumelhart and Abrahamson (1973), cf. Chen et al. (2017)), assuming that analogies of the type of “a is to b as c is to d“ can be described by linear relationships between distributional representations of the four words. While parallelogram relationship"
W18-4501,N13-1090,0,0.674886,"ion of concepts is a long-standing topic within philosophy, history and linguistics. However, recent work on the computational analysis of semantic change based on word embeddings has surprisingly little to offer in this regard. Most work focuses on the meaning of individual words. In comparison, there are only few contributions which analyze concepts and the changing vocabularies which are used to express them (Kenter et al., 2015; Recchia et al., 2016). The goal of this paper is to provide insights into how distributional semantics (Harris, 1954; Firth, 1957), in particular word embeddings (Mikolov et al., 2013a; Levy et al., 2015), can be used to analyze concept change. We propose to model concept change in terms of analogies between concept vocabularies at different points in time. This extends well-established synchronic models of analogy based on word embeddings (Mikolov et al., 2013b) to the diachronic case. We build on the underlying parallelogram model of analogy (Rumelhart and Abrahamson (1973), cf. Chen et al. (2017)), assuming that analogies of the type of “a is to b as c is to d“ can be described by linear relationships between distributional representations of the four words. While paral"
W18-4501,P17-1072,0,0.0187812,"ng and address a central problem in using word embeddings for diachronic analyses: They align the axes of the vector spaces from neighboring time periods using a mapping derived with orthogonal Procrustes (Hamilton et al., 2016). This method will be presented in more detail in Section 4.3. Concept-level semantic change. There are a number of approaches to studying lexical semantic change above the level of individual words. These include simple co-occurrence statistics or topic modeling (Blei and Lafferty, 2006) which are only loosely related to our work based on word embeddings. For example, Tan et al. (2017) present a topic modeling approach to study relations between ideas that helps to detect gradual substitution and prevalence, or mutual fostering and coexistence. Kenter et al. (2015) is a central reference point for our work, because they explicitly attempt to model changes in concept meaning, use word embeddings and also adress their method at use cases from the (digital) humanities. Most importantly, Kenter et al. (2015) also published a ground truth dataset for quantitative evaluation. The authors present a method to trace concept vocabularies through a timestamped corpus based on a set of"
W18-4501,J06-3003,0,0.0596003,"iations of f , which are motivated by an inductive learning perspective on analogy (Cornu´ejols and Ales-Bianchetti, 1998) and methods of analogy recovery used in connection with word embeddings (Mikolov et al., 2013b). Consequently, we view a 4-tuple (~at0 ,~bt0 ,~at1 ,~bt1 ) as constituting a loose diachronic analogy between concept terms, for ECONOMIC EFFICIENCY e.g. “efficiency is to robotization at one point in time as efficiency was to mechanization at an earlier point in time“. Analogies between two word pairs are based on highly similar semantic relations among the words in each pair (Turney, 2006). In our adaptation of analogies between concept terms, relational similarity is implied by the assumption that both term pairs relate to the same concept. Note that the type of semantic relation underlying our notion of diachronic concept analogies is rather generic, as it only describes membership of a characterizing term in a concept. 4 Learning diachronic analogies 4.1 Baselines and models This section describes two baselines and two preliminary models to learn to complete diachronic analogies. Each model is based on different intuitions and assumptions about the characteristics of diachro"
W19-5806,W11-1012,0,0.0133416,"t least in higher-level tasks like QA, we observe a recent trend in the literature to focus on improving model architecture over improving input features, possibly due to the ability of neural networks to learn hierarchical features. Following this paradigm, the state-of-the-art in many tasks has recently been improved, for example in semantic role labeling through a deep highway BiLSTM (He et al., 2017). However, the last papers that make use of semantic roles are mostly from the last decade (Shen and Lapata, 2007; Kaisser and Webber, 2007; Sammons et al., 2009; Wu and Fung, 2009; Liu, 2009; Gao and Vogel, 2011). Most current QA architectures use word and character embeddings only (Seo et al., 2016; Wang et al., 2017; Xiong et al., 2016; Shen et al., 2017; Hu et al., 2018; Yu et al., 2018). Our results suggest that the QANet model benefits more from better inputs than from optimizing its structure: When exploring the parameters, we observed that the embedding dimensionality of words and characters had the biggest impact. In the experiments regarding linguistic input features, we found that injecting linguistic information into the input had a stronger effect than any of the previously explored parame"
W19-5806,P17-1044,0,0.0181013,"eature engineering to construct more specific information from the dependency tree or by using a more suitable network architecture for embedding such structures. Feature Engineering for Neural Architectures At least in higher-level tasks like QA, we observe a recent trend in the literature to focus on improving model architecture over improving input features, possibly due to the ability of neural networks to learn hierarchical features. Following this paradigm, the state-of-the-art in many tasks has recently been improved, for example in semantic role labeling through a deep highway BiLSTM (He et al., 2017). However, the last papers that make use of semantic roles are mostly from the last decade (Shen and Lapata, 2007; Kaisser and Webber, 2007; Sammons et al., 2009; Wu and Fung, 2009; Liu, 2009; Gao and Vogel, 2011). Most current QA architectures use word and character embeddings only (Seo et al., 2016; Wang et al., 2017; Xiong et al., 2016; Shen et al., 2017; Hu et al., 2018; Yu et al., 2018). Our results suggest that the QANet model benefits more from better inputs than from optimizing its structure: When exploring the parameters, we observed that the embedding dimensionality of words and char"
W19-5806,W07-1206,0,0.0211579,"for embedding such structures. Feature Engineering for Neural Architectures At least in higher-level tasks like QA, we observe a recent trend in the literature to focus on improving model architecture over improving input features, possibly due to the ability of neural networks to learn hierarchical features. Following this paradigm, the state-of-the-art in many tasks has recently been improved, for example in semantic role labeling through a deep highway BiLSTM (He et al., 2017). However, the last papers that make use of semantic roles are mostly from the last decade (Shen and Lapata, 2007; Kaisser and Webber, 2007; Sammons et al., 2009; Wu and Fung, 2009; Liu, 2009; Gao and Vogel, 2011). Most current QA architectures use word and character embeddings only (Seo et al., 2016; Wang et al., 2017; Xiong et al., 2016; Shen et al., 2017; Hu et al., 2018; Yu et al., 2018). Our results suggest that the QANet model benefits more from better inputs than from optimizing its structure: When exploring the parameters, we observed that the embedding dimensionality of words and characters had the biggest impact. In the experiments regarding linguistic input features, we found that injecting linguistic information into"
W19-5806,N06-2001,0,0.0150381,"ed Work In recent years, research about feature engineering for NLP models has subsided to some extent. This might be attributed to the ability of neural networks to perform hierarchical feature learning (Bengio, 2009). Using neural approaches, many of the core NLP tasks like part-of-speech (PoS) tagging (Koo et al., 2008), dependency parsing (Chen and Manning, 2014), named entity recognition (Lample et al., 2016) and semantic role labelling (Roth and Woodsend, 2014; Zhou and Xu, 2015) have been improved. However, recent papers that make use of the improved performance in these areas are few (Alexandrescu and Kirchhoff, 2006; Sennrich and Haddow, 2016). Thus, we want to evaluate whether adding linguistic information to the inputs of a QA model improves the performance. Our approach to integrating linguistic input features by embedding each individually and concatenating the embeddings is inspired by Sennrich and Haddow (2016), who apply this approach in the context of machine translation. This paper builds upon a host of recent developments in neural architectures for question answering or reading comprehension. While most approaches rely heavily on recurrent layers (Huang et al., 2017; Hu et al., 2018; Seo et al"
W19-5806,P08-1068,0,0.175231,"Missing"
W19-5806,J81-4005,0,0.721867,"Missing"
W19-5806,D14-1082,0,0.0381204,"t our re-implementation of the deep neural QANet architecture (Yu et al., 2018) benefits considerably from these linguistically enriched representations, which we consider a promising first step towards generalizable, rapidly adaptable QA models. 2 Related Work In recent years, research about feature engineering for NLP models has subsided to some extent. This might be attributed to the ability of neural networks to perform hierarchical feature learning (Bengio, 2009). Using neural approaches, many of the core NLP tasks like part-of-speech (PoS) tagging (Koo et al., 2008), dependency parsing (Chen and Manning, 2014), named entity recognition (Lample et al., 2016) and semantic role labelling (Roth and Woodsend, 2014; Zhou and Xu, 2015) have been improved. However, recent papers that make use of the improved performance in these areas are few (Alexandrescu and Kirchhoff, 2006; Sennrich and Haddow, 2016). Thus, we want to evaluate whether adding linguistic information to the inputs of a QA model improves the performance. Our approach to integrating linguistic input features by embedding each individually and concatenating the embeddings is inspired by Sennrich and Haddow (2016), who apply this approach in t"
W19-5806,J08-2001,0,0.060507,"Missing"
W19-5806,J05-1004,0,0.0468567,"to answer “who” did “what” to “whom”, “where”, “when” and “how”? To do that, each constituent in a sentence is assigned a semantic role from a predifined set of roles like agent, patient or location (M´arquez et al., 2008). Since semantic role labeling aims at identifying relevant aspects of events that are directly related to the above-mentioned WH questions, question answering models should directly benefit from this kinds of information. We used the mate-plus tools (Roth and Woodsend, 2014) for parsing the complete SQuAD dataset and to obtain PropBank-labeled semantic roles per input word (Palmer et al., 2005). We added the role <PREDICATE&gt; to the set of semantic roles to provide the model with pointers to the basic events. Words that did not correspond to any semantic role were assigned a <NOROLE&gt; label. Integration of Linguistic Features in QANet. In the standard QANet architecture, words and corresponding characters are embedded individually and then concatenated to obtain one representation vector per input word. Following Sennrich and Haddow (2016), we enrich this process by mapping each of the linguistic input features described above to its own embedding space and then including them into th"
W19-5806,D14-1162,0,0.0845442,"related. Thus, improvements on the development set of SQuAD should also lead to improvements on the test set. Based on this claim, we only report results on the development set, since the test set of SQuAD is not publicly available. The training set consists of 87599 samples and the test set consists of 10570 samples. All texts are in English language. Preprocessing. To preprocess SQuAD, we used the spaCy library for tokenization. We truncated or padded each paragraph to length 400 and each question to length 30. Each token was transformed into lower case and embedded using pre-trained GloVe (Pennington et al., 2014) embeddings. All words that were either out-of-vocabulary or not present at training time were mapped to a randomly initialized unknown token (<UNK&gt;). For each token, we extracted all characters and then truncated or padded them to 16 characters per word. Each character embedding was initialized randomly. ∆F1 ∆EM word embeddings character embeddings # convolutional layers shared wheights in encoding # encoder blocks # attention heads # highway layers model dimensionalty pointwise feed-forward layers 2.4 1.6 1.5 1.3 0.9 0.6 0.4 0.5 0.2 1.8 1.7 2.2 1.3 0.9 1.2 1.0 0.8 0.0 combination of best set"
W19-5806,D14-1045,0,0.150811,"from these linguistically enriched representations, which we consider a promising first step towards generalizable, rapidly adaptable QA models. 2 Related Work In recent years, research about feature engineering for NLP models has subsided to some extent. This might be attributed to the ability of neural networks to perform hierarchical feature learning (Bengio, 2009). Using neural approaches, many of the core NLP tasks like part-of-speech (PoS) tagging (Koo et al., 2008), dependency parsing (Chen and Manning, 2014), named entity recognition (Lample et al., 2016) and semantic role labelling (Roth and Woodsend, 2014; Zhou and Xu, 2015) have been improved. However, recent papers that make use of the improved performance in these areas are few (Alexandrescu and Kirchhoff, 2006; Sennrich and Haddow, 2016). Thus, we want to evaluate whether adding linguistic information to the inputs of a QA model improves the performance. Our approach to integrating linguistic input features by embedding each individually and concatenating the embeddings is inspired by Sennrich and Haddow (2016), who apply this approach in the context of machine translation. This paper builds upon a host of recent developments in neural arc"
W19-5806,W16-2209,0,0.129239,"about feature engineering for NLP models has subsided to some extent. This might be attributed to the ability of neural networks to perform hierarchical feature learning (Bengio, 2009). Using neural approaches, many of the core NLP tasks like part-of-speech (PoS) tagging (Koo et al., 2008), dependency parsing (Chen and Manning, 2014), named entity recognition (Lample et al., 2016) and semantic role labelling (Roth and Woodsend, 2014; Zhou and Xu, 2015) have been improved. However, recent papers that make use of the improved performance in these areas are few (Alexandrescu and Kirchhoff, 2006; Sennrich and Haddow, 2016). Thus, we want to evaluate whether adding linguistic information to the inputs of a QA model improves the performance. Our approach to integrating linguistic input features by embedding each individually and concatenating the embeddings is inspired by Sennrich and Haddow (2016), who apply this approach in the context of machine translation. This paper builds upon a host of recent developments in neural architectures for question answering or reading comprehension. While most approaches rely heavily on recurrent layers (Huang et al., 2017; Hu et al., 2018; Seo et al., 2016; Shen et al., 2017;"
W19-5806,D07-1002,0,0.059562,"le network architecture for embedding such structures. Feature Engineering for Neural Architectures At least in higher-level tasks like QA, we observe a recent trend in the literature to focus on improving model architecture over improving input features, possibly due to the ability of neural networks to learn hierarchical features. Following this paradigm, the state-of-the-art in many tasks has recently been improved, for example in semantic role labeling through a deep highway BiLSTM (He et al., 2017). However, the last papers that make use of semantic roles are mostly from the last decade (Shen and Lapata, 2007; Kaisser and Webber, 2007; Sammons et al., 2009; Wu and Fung, 2009; Liu, 2009; Gao and Vogel, 2011). Most current QA architectures use word and character embeddings only (Seo et al., 2016; Wang et al., 2017; Xiong et al., 2016; Shen et al., 2017; Hu et al., 2018; Yu et al., 2018). Our results suggest that the QANet model benefits more from better inputs than from optimizing its structure: When exploring the parameters, we observed that the embedding dimensionality of words and characters had the biggest impact. In the experiments regarding linguistic input features, we found that injecting li"
W19-5806,P17-1018,0,0.0480954,". Thus, we want to evaluate whether adding linguistic information to the inputs of a QA model improves the performance. Our approach to integrating linguistic input features by embedding each individually and concatenating the embeddings is inspired by Sennrich and Haddow (2016), who apply this approach in the context of machine translation. This paper builds upon a host of recent developments in neural architectures for question answering or reading comprehension. While most approaches rely heavily on recurrent layers (Huang et al., 2017; Hu et al., 2018; Seo et al., 2016; Shen et al., 2017; Wang et al., 2017; Xiong et al., 2016), we chose to reimplement QANet, a self-attention based architecture (Yu et al., 2018). Apart from that, we use the tools from Roth and Woodsend (2014) for extracting semantic roles over the whole Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). 3 Extending QANet with Linguistic Input Features As a testbed in order to assess the impact of linguistic input features in neural QA models, we make use of (a re-implementation of) QANet (Yu et al., 2018). By default, QANet solely uses word and character inputs. However, numerous off-theshelf NLP tools are ava"
W19-5806,N09-2004,0,0.0335439,"ing for Neural Architectures At least in higher-level tasks like QA, we observe a recent trend in the literature to focus on improving model architecture over improving input features, possibly due to the ability of neural networks to learn hierarchical features. Following this paradigm, the state-of-the-art in many tasks has recently been improved, for example in semantic role labeling through a deep highway BiLSTM (He et al., 2017). However, the last papers that make use of semantic roles are mostly from the last decade (Shen and Lapata, 2007; Kaisser and Webber, 2007; Sammons et al., 2009; Wu and Fung, 2009; Liu, 2009; Gao and Vogel, 2011). Most current QA architectures use word and character embeddings only (Seo et al., 2016; Wang et al., 2017; Xiong et al., 2016; Shen et al., 2017; Hu et al., 2018; Yu et al., 2018). Our results suggest that the QANet model benefits more from better inputs than from optimizing its structure: When exploring the parameters, we observed that the embedding dimensionality of words and characters had the biggest impact. In the experiments regarding linguistic input features, we found that injecting linguistic information into the input had a stronger effect than any"
W19-5806,P15-1109,0,0.0259261,"y enriched representations, which we consider a promising first step towards generalizable, rapidly adaptable QA models. 2 Related Work In recent years, research about feature engineering for NLP models has subsided to some extent. This might be attributed to the ability of neural networks to perform hierarchical feature learning (Bengio, 2009). Using neural approaches, many of the core NLP tasks like part-of-speech (PoS) tagging (Koo et al., 2008), dependency parsing (Chen and Manning, 2014), named entity recognition (Lample et al., 2016) and semantic role labelling (Roth and Woodsend, 2014; Zhou and Xu, 2015) have been improved. However, recent papers that make use of the improved performance in these areas are few (Alexandrescu and Kirchhoff, 2006; Sennrich and Haddow, 2016). Thus, we want to evaluate whether adding linguistic information to the inputs of a QA model improves the performance. Our approach to integrating linguistic input features by embedding each individually and concatenating the embeddings is inspired by Sennrich and Haddow (2016), who apply this approach in the context of machine translation. This paper builds upon a host of recent developments in neural architectures for quest"
