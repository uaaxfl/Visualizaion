2020.ijclclp-2.2,N16-1030,0,0.101541,"Missing"
2020.ijclclp-2.2,P16-1101,0,0.0848367,"Missing"
2020.rocling-1.32,W11-2838,0,0.0615567,"Missing"
2020.rocling-1.32,W12-2006,0,0.0640657,"Missing"
2020.rocling-1.32,W13-3601,0,0.0476703,"Missing"
2020.rocling-1.32,W16-0506,0,0.0434903,"Missing"
2020.rocling-1.32,D14-1162,0,0.089668,"Missing"
2020.rocling-1.32,Q17-1010,0,0.0114955,"Missing"
2020.smm4h-1.23,W18-5914,0,0.0329105,"Missing"
2020.smm4h-1.23,W18-5904,0,0.188942,"Missing"
2020.smm4h-1.23,W18-5910,0,0.0280559,"Missing"
2021.bionlp-1.30,P19-1215,0,0.0588167,"iction. We only use the entailment relation that was annotated from the training, validation and test datasets. Comparing the lengths of two the sentences in each pair, the longer sentences will be regarded as a question, while the other is used as the corresponding summary. A total of 4,683 pairs were collected from this dataset. 269 • MEDIQA 2019 – RQE Dataset (Ben Abacha et al., 2019) training, validation and test sets were composed of data from an independent set of consumer health questions. The MeQSum Dataset of consumer health questions and their summaries can be used for training (Ben Abacha and Demner-Fushman, 2019b). The validation and test sets consist of consumer health questions received by the U.S. National Library of Medicine (NLM) in December 2020. Their associated summaries were manually created by medical experts for evaluation. In summary, during the system development phase, the training and validation sets respectively consisted of 1,000 and 50 consumer health questions and their associated summaries for system designing and implementation. In total, only 100 consumer health questions in the test dataset were used for final performance evaluation. The Recognizing Question Entailment (RQE) ta"
2021.bionlp-1.30,W19-5039,0,0.0972966,"evaluated on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experimental results showed that good abstractive summarization performance can Fine-tuning Many summarization datasets contain original texts and their referenced summarizes written in declarative sentences. Question summaries written in interrogative sentences are relatively rare. Hence, in addition to the training set provided by task organizers, we also collected and processed the following datasets to fine-tune the QS task. • MEDIQA 2019 – NLI Dataset (Ben Abacha et al., 2019) The Natural Language Inference (NLI) task of the MEDIQA 2019 challenge identifies three relations between two sentences including entailment, neutral, and contradiction. We only use the entailment relation that was annotated from the training, validation and test datasets. Comparing the lengths of two the sentences in each pair, the longer sentences will be regarded as a question, while the other is used as the corresponding summary. A total of 4,683 pairs were collected from this dataset. 269 • MEDIQA 2019 – RQE Dataset (Ben Abacha et al., 2019) training, validation and test sets were compos"
2021.bionlp-1.30,2021.bionlp-1.8,0,0.0308065,"COVID-19 pandemic, the Epidemic Question Answering (EPIC-QA) track in TREC 2020 conference focuses on developing systems capable of automatically answering questions about COVID-19. In the question part of EPIC-QA data, two prepared sets of approximately 45 questions were provided: one for expert-level questions and one for consumer-level questions. Without considering the question levels, we regard the longer questions as original questions and the corresponding shorter question are their summaries. 3 3.1 Evaluation Data The experimental datasets were mainly provided by task organizers (Ben Abacha et al., 2021). The 3.2 Settings The pre-trained PEGASUS models were downloaded from the HuggingFace (Wolf et al., 2019). A PEGASUS model was trained with sampled gap sentence ratios on both C4 (Raffel et al., 2020) and HugeNews datasets, and important sentences were sampled stochastically. We selected the PEGASUS-Large model and its mixed and stochastic model (denoting PEGASUS-LargeXSum) on the XSum (Narayan et al., 2018) datasets, containing 227k BBC news articles from 2010 to 2017 covering a wide variety of subjects along with professionally written single-sentence summarizes. To confirm model performanc"
2021.bionlp-1.30,2020.coling-main.498,0,0.253511,"ded as the official score to rank the participating teams’ performance in the leaderboard. outperformed the others than the others in almost all evaluation metrics. A total of 22 teams participated in the QS task, each submitting at least one entry. Our best submission achieved an R2-F1 score of 0.1597, significantly outperforming the baseline model with a score of 0.1373 and ranking third place among all 128 submissions. In addition to ROUGE metrics, task organizers also use several evaluation metrics that may be better adapted to the QS task. Our best submission also achieved a HOLMS score (Mrabet and Demner-Fushman, 2020) of 0.5783, ranking first among all 128 submissions. Our best submission had a BERTScore-F1 (Zhang et al., 2020b) of 0.6960, ranked ninth among all submissions. 3.4 4 Results Table 1 shows the results on the QS validation set of MEDIQA 2021 challenge. Both PEGASUS models outperformed the BART models in a half of the metrics. The mixed and stochastic models on the XSum datasets usually outperformed than those without the XSum optimization using both BART and PEGASUS transformers. The PEGASUSLarge-XSum model obtained the best overall score of 0.1469 in R2-F1, considered as the ranking metric. Du"
2021.smm4h-1.18,2020.smm4h-1.1,0,0.0120301,"Recall 0.8197 0.8361 Test Set F1-score 0.7782 0.8128 Precision 0.6750 0.7452 Recall 0.7890 0.7597 F1-score 0.73 0.75 Table 2: Submission results on the SMM4H 2021 Task 5 validation and test datasets. were also proposed to address this multi-class classification task (Reddy, 2020). Our participated SMM4H 2021 Task 5 is new binary classification task, which aims at distinguishing tweets that self-report potential cases of COVID-19 from those that do not. COVID-19 Twitter Monitor was presented to show interactive visualizations of the analysis results on tweets related to the COVID-19 pandemic (Cornelius et al., 2020). An iterative graph-based approach was proposed to detect COVID-19 emerging symptoms using context-based twitter embeddings (Santosh et al., 2020). A large twitter dataset of COVID-19 chatter was used to identify discourse around drug mentions (Tekumalla and Banda, 2020). 3 approach) (Liu et al., 2019) is a replication study of BERT pretraining (Devlin et al., 2018) that carefully measures the impact of key parameters and training data size. We observe that RoBERTa transformers have usually performed well for many SMM4H 2020 tweet classification tasks (Klein et al., 2020c). Hence, we explore"
2021.smm4h-1.18,2020.nlpcovid19-2.25,0,0.0319483,"the SMM4H 2021 Task 5 validation and test datasets. were also proposed to address this multi-class classification task (Reddy, 2020). Our participated SMM4H 2021 Task 5 is new binary classification task, which aims at distinguishing tweets that self-report potential cases of COVID-19 from those that do not. COVID-19 Twitter Monitor was presented to show interactive visualizations of the analysis results on tweets related to the COVID-19 pandemic (Cornelius et al., 2020). An iterative graph-based approach was proposed to detect COVID-19 emerging symptoms using context-based twitter embeddings (Santosh et al., 2020). A large twitter dataset of COVID-19 chatter was used to identify discourse around drug mentions (Tekumalla and Banda, 2020). 3 approach) (Liu et al., 2019) is a replication study of BERT pretraining (Devlin et al., 2018) that carefully measures the impact of key parameters and training data size. We observe that RoBERTa transformers have usually performed well for many SMM4H 2020 tweet classification tasks (Klein et al., 2020c). Hence, we explore the usage of RoBERTa transformers and fine-tune the downstream tasks. For Task 4, we use training, validation, and test datasets provided by task o"
C14-2015,C12-1184,1,0.41187,"ign Language (EFL), support for CFL learners is relatively sparse, especially in terms of tools designed to automatically detect and correct Chinese grammatical errors. For example, while Microsoft Word has integrated robust English spelling and grammar checking functions for years, such tools for Chinese are still quite primitive. In contrast to the plethora of research related to EFL learning, relatively few studies have focused on grammar checking for CFL learners. Wu et al. (2010) proposed relative position and parse template language models to detect Chinese errors written by US learner. Yu and Chen (2012) proposed a classifier to detect word-ordering errors in Chinese sentences from the HSK dynamic composition corpus. Chang et al. (2012) proposed a penalized probabilistic First-Order Inductive Learning (pFOIL) algorithm for error diagnosis. In summary, although there are many approaches and tools to help EFL learners, the research problem described above for CFL learning is still under-explored. In addition, no common platform is available to compare different approaches and to promote the study of this important issue. This study develops a sentence judgment system using both rule-based and n"
C14-2015,C02-1049,0,0.0362478,"le-baased method, and n-gram statistical method. m 2.1 Prre-processin ng Chinese is written without w word boundaries.. As a result,, prior to thee implementaation of mosst Natural Languagge Processingg (NLP) task ks, texts musst undergo au utomatic wo ord segmentaation. Autom matic Chinese word segmenteers are generrally trained by an input lexicon and d probability models. Ho owever, it usually ssuffers from the unknown word (i.e., the out-of-v vocabulary, or o OOV) prooblem. In thiss study, a corpus-bbased learninng method is used to merg rge unknown n words to tacckle the OOV V problem (C Chen and Ma, 2002). This is foollowed by a reliable andd cost-effectiive POS-tagg ging method to label the segmented words with parts--of-speech (T Tsai and Cheen, 2004). For example, take the Chhinese senten nce “歐巴 馬是美國 國總統” (Obbama is the president p off the USA). It was segm mented and taagged in thee form of “POS:W Word” sequennce shown as follows: Nb:歐巴馬 馬 SHI:是 Nc:美國 N Naa:總統. Amo ong these words, tthe translatioon of a foreign proper naame “歐巴馬 馬” (Obama) is not likelyy to be inclu uded in a lexicon aand thereforee is extracted d by the unkknown word detection meechanism. Inn this case, th he special PO"
C14-2015,O04-2005,0,0.0729102,"Missing"
E14-4003,P08-1004,1,0.944569,"lves human intervention of handcrafted rules or tagged examples as the input for machine learning to recognize the assertion of a particular relationship between two entities in texts (Riloff, 1996; Soderland, 1999). Although machine learning helps enumerate potential relation patterns for extraction, this approach is often limited to extracting the relation sets that are predefined. In addition, traditional IE has focused on satisfying pre-specified requests from small homogeneous corpora, leaving the question open whether it can scale up to massive and heterogeneous corpora such as the Web (Banko and Etzioni, 2008; Etzioni et al., 2008, 2011). The relatively rich morpho-syntactic marking system of English (e.g., verbal inflection, nominal case, clausal markers) makes the syntactic roles of many words detectable from their surface forms. A tensed verb in English, for example, generally indicates its main verb status of a clause. The pinning down of the main verb in a Chinese clause, on the other hand, must rely on other linguistic cues such as word context due to the lack of tense markers. In contrast to the syntax-oriented English language, Chinese is discourse-oriented and rich in ellipsis – meaning i"
E14-4003,D11-1142,1,0.917387,"n2, Oren Etzioni3, Anthony Fader4 1 2 Information Technology Center, National Taiwan Normal University Dept. of Computer Science and Information Engineering, National Taiwan University 3 Allen Institute for Artificial Intelligence, Seattle, WA 4 Dept. of Computer Science and Engineering, University of Washington {samtseng, lhlee, sylin, skylock, meijun}@ntnu.edu.tw, hhchen@ntu.edu.tw, OrenE@allenai.org, afader@cs.washington.edu massive text corpora, where target relations are unknown in advance. Several Open IE systems, such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), ReVerb (Fader et al., 2011), and OLLIE (Mausam et al., 2012) achieve promising performance in open relation extraction on English sentences. However, application of these systems poses challenges to those languages that are very different from English, such as Chinese, as grammatical functions in English and Chinese are realized in markedly different ways. It is not sure whether those techniques for English still work for Chinese. This issue motivates us to extend the state-of-the-art Open IE systems to extract relations from Chinese texts. Abstract This study presents the Chinese Open Relation Extraction (CORE) system"
E14-4003,W00-1205,0,0.017299,"Head”-labeled words only, can be applied to strain out from each component of this triple the most prominent word: “民主黨 / 發佈 / 報告” (‘Democrats / released / report’). ‘ 燈 泡 /Na’ were annotated as two nominal phrases (i.e., ‘NP’), and ‘發明/VC 了/Di’ was annotated as a verbal phrase (i.e., ‘VP’). CKIP parser also adopts dependency decisionmaking and example-based approaches to label the semantic role “Head”, showing the status of a word or a phrase as the pivotal constituent of a sentence (You and Chen, 2004). CORE adopts the head-driven principle to identify the main relation in a given sentence (Huang et al., 2000). Firstly, a relation is defined by both the “Head”labeled verb and the other words in the syntactic chunk headed by the verb. Secondly, the noun phrases preceding/preceded by the relational chunk are regarded as the candidates of the head’s arguments. Finally, the entity-relation triple is identified in the form of (entity1, relation, entity2). Regarding the example sentence described above, the triple (愛迪生/Edison, 發明 Figure 1: The parsed tree of a Chinese sentence. 4 Experiments and Evaluation We adopted the same test set released by ReVerb for performance evaluation. The test set consists o"
E14-4003,W12-0702,0,0.0486469,"(‘Apples nutritious’), “ 蘋 果 是 營 養 豐 富 的 ” (‘Apples are nutritious’), and “蘋果富含營養” 12 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–16, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics (‘Apples are rich in nutrition’) are semantically synonymous sentences, but the first one, which lacks an overt verb, is used far more often than the other two. Presumably, an adequate multilingual IE system must take into account those intrinsic differences between languages. For multilingual open IE, Gamallo et al. (2012) adopts a rule-based dependency parser to extract relations represented in English, Spanish, Portuguese, and Galician. For each parsed sentence, they separate each verbal clause and then identify each one’s verb participants, including their functions: subject, direct object, attribute, and prepositional complements. A set of rules is then applied on the clause constituents to extract the target triples. For Chinese open IE, we adopt a similar general approach. The main differences are the processing steps specific to Chinese language. This paper introduces the Chinese Open Relation Extraction"
E14-4003,P10-1013,0,0.482828,", Mei-Jun Liu1, Hsin-Hsi Chen2, Oren Etzioni3, Anthony Fader4 1 2 Information Technology Center, National Taiwan Normal University Dept. of Computer Science and Information Engineering, National Taiwan University 3 Allen Institute for Artificial Intelligence, Seattle, WA 4 Dept. of Computer Science and Engineering, University of Washington {samtseng, lhlee, sylin, skylock, meijun}@ntnu.edu.tw, hhchen@ntu.edu.tw, OrenE@allenai.org, afader@cs.washington.edu massive text corpora, where target relations are unknown in advance. Several Open IE systems, such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), ReVerb (Fader et al., 2011), and OLLIE (Mausam et al., 2012) achieve promising performance in open relation extraction on English sentences. However, application of these systems poses challenges to those languages that are very different from English, such as Chinese, as grammatical functions in English and Chinese are realized in markedly different ways. It is not sure whether those techniques for English still work for Chinese. This issue motivates us to extend the state-of-the-art Open IE systems to extract relations from Chinese texts. Abstract This study presents the Chinese Open Relat"
E14-4003,W04-1116,0,0.0178753,"riple is chunked from its original sentence fully automatically. Finally, a filtering process, which retains “Head”-labeled words only, can be applied to strain out from each component of this triple the most prominent word: “民主黨 / 發佈 / 報告” (‘Democrats / released / report’). ‘ 燈 泡 /Na’ were annotated as two nominal phrases (i.e., ‘NP’), and ‘發明/VC 了/Di’ was annotated as a verbal phrase (i.e., ‘VP’). CKIP parser also adopts dependency decisionmaking and example-based approaches to label the semantic role “Head”, showing the status of a word or a phrase as the pivotal constituent of a sentence (You and Chen, 2004). CORE adopts the head-driven principle to identify the main relation in a given sentence (Huang et al., 2000). Firstly, a relation is defined by both the “Head”labeled verb and the other words in the syntactic chunk headed by the verb. Secondly, the noun phrases preceding/preceded by the relational chunk are regarded as the candidates of the head’s arguments. Finally, the entity-relation triple is identified in the form of (entity1, relation, entity2). Regarding the example sentence described above, the triple (愛迪生/Edison, 發明 Figure 1: The parsed tree of a Chinese sentence. 4 Experiments and"
E14-4003,W12-6338,0,0.0116822,"correctly once “ 了 ” is associated with its following identified as the verbal phrase that heads the sentence. This verbal phrase is regarded as the center of a potential relation. The two noun phrases before and after the verbal phrase, i.e., the NP “白宮 預算 委員會 的 民主黨” and NP character, instead of its precedent word. We adopt CKIP, the best-performing parser in the bakeoff of SIGHAN 2012 (Tseng et al., 2012), to do syntactic structure analysis. The CKIP solution re-estimates the contextdependent probability for Chinese parsing and improves the performance of probabilistic context-free grammar (Hsieh et al., 2012). For the example sentence above, ‘愛迪生/Nb’ and “報告” are regarded as the entities that complete the relation. A potential entity-relation-entity triple (i.e., 白宮預算委員會的民主黨 / 星期一 發佈 / 報告, ‘Democrats on the House Budget Committee / on Monday released / a report’) is extracted accordingly. This triple is chunked from its original sentence fully automatically. Finally, a filtering process, which retains “Head”-labeled words only, can be applied to strain out from each component of this triple the most prominent word: “民主黨 / 發佈 / 報告” (‘Democrats / released / report’). ‘ 燈 泡 /Na’ were annotated as two"
E14-4003,C02-1049,0,0.0327284,"source for their extractor. Experimental results indicated that parsed dependency features lead to further improvements over TextRunner. Chinese is generally written without word boundaries. As a result, prior to the implementation of most NLP tasks, texts must undergo automatic word segmentation. Automatic Chinese word segmenters are generally trained by an input lexicon and probability models. However, it usually suffers from the unknown word (i.e., the out-ofvocabulary, or OOV) problem. In CORE, a corpus-based learning method to merge the unknown words is adopted to tackle the OOV problem (Chen and Ma, 2002). This is followed by a reliable and cost-effective POS-tagging method to label the segmented words with partof-speeches (Tsai and Chen, 2004). Take the Chinese sentence “愛迪生發明了燈泡” (‘Edison ReVerb (Fader et al., 2011) introduced another approach by identifying first a verb-centered relational phrase that satisfies their pre-defined syntactic and lexical constraints, and then split the input sentence into an Argument-VerbArgument triple. This approach involves only POS tagging for English and “regular expression”-like matching. As such, it is suitable for large corpora, and likely to be applica"
E14-4003,O04-2005,0,0.0104221,"nese is generally written without word boundaries. As a result, prior to the implementation of most NLP tasks, texts must undergo automatic word segmentation. Automatic Chinese word segmenters are generally trained by an input lexicon and probability models. However, it usually suffers from the unknown word (i.e., the out-ofvocabulary, or OOV) problem. In CORE, a corpus-based learning method to merge the unknown words is adopted to tackle the OOV problem (Chen and Ma, 2002). This is followed by a reliable and cost-effective POS-tagging method to label the segmented words with partof-speeches (Tsai and Chen, 2004). Take the Chinese sentence “愛迪生發明了燈泡” (‘Edison ReVerb (Fader et al., 2011) introduced another approach by identifying first a verb-centered relational phrase that satisfies their pre-defined syntactic and lexical constraints, and then split the input sentence into an Argument-VerbArgument triple. This approach involves only POS tagging for English and “regular expression”-like matching. As such, it is suitable for large corpora, and likely to be applicable to Chinese. invented the light bulb’) for instance. It was segmented and tagged as follows: 愛迪生/Nb 發 明/VC 了/Di 燈泡/Na. Among these words, t"
E14-4003,W12-6335,1,0.892043,"ith other character, such as “了解” meaning “understand”. 會/Nc, 的/DE, 民主黨/Nb, 星期一/Nd, 發怖/VE, Therefore, “ 愛 迪 生 發 明 了 解 藥 ” (‘Edison 報 告 /Na. Next, “ 星 期 一 /Nd 發 佈 /VE” is invented a cure’) would be segmented incorrectly once “ 了 ” is associated with its following identified as the verbal phrase that heads the sentence. This verbal phrase is regarded as the center of a potential relation. The two noun phrases before and after the verbal phrase, i.e., the NP “白宮 預算 委員會 的 民主黨” and NP character, instead of its precedent word. We adopt CKIP, the best-performing parser in the bakeoff of SIGHAN 2012 (Tseng et al., 2012), to do syntactic structure analysis. The CKIP solution re-estimates the contextdependent probability for Chinese parsing and improves the performance of probabilistic context-free grammar (Hsieh et al., 2012). For the example sentence above, ‘愛迪生/Nb’ and “報告” are regarded as the entities that complete the relation. A potential entity-relation-entity triple (i.e., 白宮預算委員會的民主黨 / 星期一 發佈 / 報告, ‘Democrats on the House Budget Committee / on Monday released / a report’) is extracted accordingly. This triple is chunked from its original sentence fully automatically. Finally, a filtering process, whic"
huang-etal-2008-quality,W02-1811,0,\N,Missing
huang-etal-2008-quality,W03-1709,0,\N,Missing
huang-etal-2008-quality,ma-huang-2006-uniform,1,\N,Missing
huang-etal-2008-quality,W03-1730,0,\N,Missing
huang-etal-2008-quality,J03-3001,0,\N,Missing
I17-4001,W14-1701,0,0.206261,"Missing"
I17-4001,W15-4401,1,0.64702,"ications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al., 2013) and hybrid methods (Lee et al., 2014). In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL (Yu et al., 2014). A second version of this shared task in NLP-TEA was collocated with the ACLIJCNLP-2015 (Lee et al., 2015) and COLING2016 (Lee et al., 2016). In conjunction with the IJCNLP 2017, the shared task for Chinese grammatical error diagnosis is organized again. The main purpose of these shared tasks is to provide a common setting so that researchers who approach the tasks using different linguistic factors and computational techniques can compare their results. Such technical evaluations allow researchers to exchange their experiences to advance the field and eventually develop optimal solutions to this shared task. The rest of this paper is organized as follows. Section 2 describes the task in detail. S"
I17-4001,W16-4906,1,0.565027,"Missing"
I17-4001,C12-1184,0,0.0985207,"Missing"
I17-4002,D14-1162,0,0.0889845,"Missing"
I17-4002,W10-0208,0,0.0330003,", lhlee@ntnu.edu.tw, wangjin@ynu.edu.cn, kfwong@se.cuhk.edu.hk 1 numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Wang et al., 2016a) or texts (Kim et al., 2010; Paltoglou et al, 2013; Wang et al., 2016b). Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014) The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing. Sentiment lexicons with valence-arousal ratings are useful resources for the developm"
I17-4002,N16-1066,1,0.779642,"e output format is “term_id, valence_rating, arousal_rating”. Below are the input/output formats of the example words “好” (good), “非常好” (very good), “滿意” (satisfy) and “不滿意” (not satisfy). Example 1: Input: 1, 好 Output: 1, 6.8, 5.2 Example 2: Input: 2, 非常好 Output: 2, 8.500, 6.625 Example 3: Input: 3, 滿意 Output: 3, 7.2, 5.6 Example 4: Input: 4, 不滿意 Output: 4, 2.813, 5.688 Test set: For single words, we selected 750 words that were not included in the CVAW 2.0 from NTUSD (Ku and Chen, 2007) using the same method presented in our previous task on Dimensional Sentiment Analysis for Chinese Words (Yu et al, 2016b). Each single word in both training and test sets was annotated with valence-arousal ratings by five annotators and the average ratings were taken as ground truth. Each multi-word phrase was rated by at least 10 different annotators. Once the rating process was finished, a corpus clean up procedure was performed to remove outlier ratings that did not fall within the mean plus/minus 1.5 standard deviations. They were then excluded from the calculation of the average ratings for each phrase. The policy of this shared task was implemented as is an open test. That is, in addition to the above of"
I17-4002,P16-2037,1,0.861068,"ontact: lcyu@saturn.yzu.edu.tw, lhlee@ntnu.edu.tw, wangjin@ynu.edu.cn, kfwong@se.cuhk.edu.hk 1 numerical values on multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this twodimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Wang et al., 2016a) or texts (Kim et al., 2010; Paltoglou et al, 2013; Wang et al., 2016b). Dimensional sentiment analysis has emerged as a compelling topic for research with applications including antisocial behavior detection (Munezero et al., 2011), mood analysis (De Choudhury et al., 2012) and product review ranking (Ren and Nickerson, 2014) The IJCNLP 2017 features a shared task for dimensional sentiment analysis for Chinese words, providing an evaluation platform for the development and implementation of advanced techniques for affective computing. Sentiment lexicons with valence-arousal ratings are usef"
I17-4002,W11-3704,0,0.0240642,"Missing"
L18-1363,granfeldt-etal-2006-cefle,0,0.0557797,"sh learners from different native language backgrounds (Granger, 2003). The Cambridge Learner Corpus (CLC) is established to assist English Language Teaching/Training (ELT) publishers to create various learning aids including dictionaries and ELT course books (Nicholls, 2003). The NUS Corpus of Learner English (NUCLE) is annotated for the development and evaluation 2298 of grammatical error correction systems (Dahlmeier et al., 2013). Additional learner corpora exist for European languages. The Lund CEFLE is a leaner corpus of texts in French produced by adolescent Swedish learners of French (Granfeldt et al., 2006). The Error-Annotated German Learner Corpus (EAGLE) is a corpus of beginning learners with grammatical error annotation (Boyd, 2010). The ASK corpus is a learner corpus of Norwegian as a second language that contains essays collected from language tests (Tenfjord et al., 2006). The CzeSL corpus is a learner corpus of Czech that has been annotated using multi-layer error types (Hana et al., 2010). The Hungarian Learner Corpus is composed of student journals annotated for learner errors using tagging sets from different linguistic categories, including phonology, morphology and syntax (Dickinson"
L18-1363,W10-1802,0,0.00952611,"n systems (Dahlmeier et al., 2013). Additional learner corpora exist for European languages. The Lund CEFLE is a leaner corpus of texts in French produced by adolescent Swedish learners of French (Granfeldt et al., 2006). The Error-Annotated German Learner Corpus (EAGLE) is a corpus of beginning learners with grammatical error annotation (Boyd, 2010). The ASK corpus is a learner corpus of Norwegian as a second language that contains essays collected from language tests (Tenfjord et al., 2006). The CzeSL corpus is a learner corpus of Czech that has been annotated using multi-layer error types (Hana et al., 2010). The Hungarian Learner Corpus is composed of student journals annotated for learner errors using tagging sets from different linguistic categories, including phonology, morphology and syntax (Dickinson and Ledbetter, 2012). Recently, learner corpora have been established for Asian languages. The HSK Dynamic Composition Corpus contains simplified Chinese essays written by learners of Chinese, annotated with different error types (Cui and Zhang, 2011; Zhang and Cui, 2013). The Jinan Chinese Learner Corpus is a collection of texts produced for educational applications (Wang et al., 2015). Lang-8"
L18-1363,W16-4919,0,0.0513604,"Missing"
L18-1363,W12-3617,0,0.0260242,"orpora have been established for Asian languages. The HSK Dynamic Composition Corpus contains simplified Chinese essays written by learners of Chinese, annotated with different error types (Cui and Zhang, 2011; Zhang and Cui, 2013). The Jinan Chinese Learner Corpus is a collection of texts produced for educational applications (Wang et al., 2015). Lang-8 contains a Japanese learner corpus extracted from a language learning and exchange social network service (Mizumoto et al., 2011). Linguistic properties of Korean particle errors have been outlined and annotated in collected learner writings (Lee et al., 2012). The present study follows the research trend of worldwide learner corpora construction to build the TOCFL learner corpus. To the best of our knowledge, this is the first traditional Chinese learner corpus to be publicly available for research. 3. Annotation The Steering Committee for the Test Of Proficiency-Huaya (SC-TOP) aims to develop and promote the Test Of Chinese as a Foreign Language (TOCFL) to assess the proficiency of CFL learners. The TOCFL writing test references the proficiency levels of the Common European Framework of Reference (CEFR) (Little, 2006). The testing principle is ta"
L18-1363,W15-4403,0,0.0567095,"Missing"
L18-1363,W16-4908,0,0.0155741,"elds (CRF) (Wu et al., 2015a). The NTOU system proposed two sentence likelihood functions based on frequencies of Google n-grams to diagnose grammatical errors (Lin, & Chen, 2015). The NCYU system also used statistical word and part-of-speech patterns based CRFs to detect grammatical errors (Yeh et al., 2015). The TMU examined corpus augmentation and explored syntax-based and hierarchical phrase-based translation models for use in this task (Zhao et al. 2015). For the NLPTEA 2016 shared task, the ANO system and CYUT-III system diagnosed grammatical errors based on the CRF (Chen et al., 2016a; Liu et al., 2016) along with word order sensitive embedding approaches (Chou et al., 2016). The NTOU system generated and scored correction candidates for grammatical error diagnosis (Chen et al., 2016b). The HIT system adopted long short-term memory (LSTM) neural networks to identify grammatical errors (Zheng et al., 2016). The PKU system presented a model based on bidirectional LSTM (Huang, & Wang, 2016). The NCYU system proposed the structure of the recurrent neural network using LSTM to detect grammatical errors (Yeh et al., 2016). The YUN-HPCC system built single word embeddings based convolutional neural"
L18-1363,N15-1160,0,0.0256534,"-Negrillo and Fernández-Domínguez, 2006; Tono, 2003). From a linguistic perspective, annotated learner corpora are valuable resources for research in second language acquisition (Swanson and Charniak, 2013), foreign language teaching (Wang and Seneff, 2007), and contrastive interlanguage analysis (Granger, 2015). In engineering, such language resources can be used to develop natural language processing techniques for educational applications, such as automatic essay scoring (Yannakoudakis et al., 2011), assessment report generation (Sakaguchi et al., 2013), and native language identification (Malmasi and Dras, 2015). Automated grammatical error detection and correction are important research directions and a number of competitions have been organized to encourage innovation (Leacock et al., 2014). For example, Helping Our Own (HOO) is a series of shared tasks for correcting errors in non-native English texts (Dale and Kilgarriff, 2011, Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors for learners of English as a foreign language (Ng et al., 2013; 2014). The NLPTEA workshops had hosted a series of shared tasks for Chinese grammatical error diagnosis (Yu et al., 2014"
L18-1363,I11-1017,0,0.131902,"ts from different linguistic categories, including phonology, morphology and syntax (Dickinson and Ledbetter, 2012). Recently, learner corpora have been established for Asian languages. The HSK Dynamic Composition Corpus contains simplified Chinese essays written by learners of Chinese, annotated with different error types (Cui and Zhang, 2011; Zhang and Cui, 2013). The Jinan Chinese Learner Corpus is a collection of texts produced for educational applications (Wang et al., 2015). Lang-8 contains a Japanese learner corpus extracted from a language learning and exchange social network service (Mizumoto et al., 2011). Linguistic properties of Korean particle errors have been outlined and annotated in collected learner writings (Lee et al., 2012). The present study follows the research trend of worldwide learner corpora construction to build the TOCFL learner corpus. To the best of our knowledge, this is the first traditional Chinese learner corpus to be publicly available for research. 3. Annotation The Steering Committee for the Test Of Proficiency-Huaya (SC-TOP) aims to develop and promote the Test Of Chinese as a Foreign Language (TOCFL) to assess the proficiency of CFL learners. The TOCFL writing test"
L18-1363,W14-1701,0,0.0639364,"Missing"
L18-1363,W13-3601,0,0.0121562,"oudakis et al., 2011), assessment report generation (Sakaguchi et al., 2013), and native language identification (Malmasi and Dras, 2015). Automated grammatical error detection and correction are important research directions and a number of competitions have been organized to encourage innovation (Leacock et al., 2014). For example, Helping Our Own (HOO) is a series of shared tasks for correcting errors in non-native English texts (Dale and Kilgarriff, 2011, Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors for learners of English as a foreign language (Ng et al., 2013; 2014). The NLPTEA workshops had hosted a series of shared tasks for Chinese grammatical error diagnosis (Yu et al., 2014; Lee et al., 2015b; Lee et al., 2016b). Recently, the IJCNLP 2017 shared task 1 has focused on Chinese grammatical error diagnosis (Rao et al., 2017). All these competitions require annotated learner corpora for system development and evaluation. To our best knowledge, only two previous studies have manually annotated learner corpora for Chinese as a foreign language (CFL). One is the HSK Dynamic Composition Corpus constructed by the Beijing Language and Culture University"
L18-1363,I17-4001,1,0.679203,"ized to encourage innovation (Leacock et al., 2014). For example, Helping Our Own (HOO) is a series of shared tasks for correcting errors in non-native English texts (Dale and Kilgarriff, 2011, Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors for learners of English as a foreign language (Ng et al., 2013; 2014). The NLPTEA workshops had hosted a series of shared tasks for Chinese grammatical error diagnosis (Yu et al., 2014; Lee et al., 2015b; Lee et al., 2016b). Recently, the IJCNLP 2017 shared task 1 has focused on Chinese grammatical error diagnosis (Rao et al., 2017). All these competitions require annotated learner corpora for system development and evaluation. To our best knowledge, only two previous studies have manually annotated learner corpora for Chinese as a foreign language (CFL). One is the HSK Dynamic Composition Corpus constructed by the Beijing Language and Culture University (Cui and Zhang, 2011; Zhang and Cui, 2013); the other is the Jinan Chinese Learner Corpus (Wang et al., 2015). The target language of these two studies is simplified Chinese, and no traditional Chinese learner corpus is available for public research. This pilot study thu"
L18-1363,P13-2043,0,0.0183768,"nguage is an important task for learner corpus research (Díaz-Negrillo and Fernández-Domínguez, 2006; Tono, 2003). From a linguistic perspective, annotated learner corpora are valuable resources for research in second language acquisition (Swanson and Charniak, 2013), foreign language teaching (Wang and Seneff, 2007), and contrastive interlanguage analysis (Granger, 2015). In engineering, such language resources can be used to develop natural language processing techniques for educational applications, such as automatic essay scoring (Yannakoudakis et al., 2011), assessment report generation (Sakaguchi et al., 2013), and native language identification (Malmasi and Dras, 2015). Automated grammatical error detection and correction are important research directions and a number of competitions have been organized to encourage innovation (Leacock et al., 2014). For example, Helping Our Own (HOO) is a series of shared tasks for correcting errors in non-native English texts (Dale and Kilgarriff, 2011, Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors for learners of English as a foreign language (Ng et al., 2013; 2014). The NLPTEA workshops had hosted a series of shared t"
L18-1363,N13-1009,0,0.0224523,"ther research. To our best knowledge, this is the first annotated learner corpus of traditional Chinese, and the entire learner corpus will be publicly released. Keywords: computer-assisted language learning, second language acquisition, grammatical error diagnosis, interlanguage analysis 1. Introduction Annotating learners’ inappropriate usage of written language is an important task for learner corpus research (Díaz-Negrillo and Fernández-Domínguez, 2006; Tono, 2003). From a linguistic perspective, annotated learner corpora are valuable resources for research in second language acquisition (Swanson and Charniak, 2013), foreign language teaching (Wang and Seneff, 2007), and contrastive interlanguage analysis (Granger, 2015). In engineering, such language resources can be used to develop natural language processing techniques for educational applications, such as automatic essay scoring (Yannakoudakis et al., 2011), assessment report generation (Sakaguchi et al., 2013), and native language identification (Malmasi and Dras, 2015). Automated grammatical error detection and correction are important research directions and a number of competitions have been organized to encourage innovation (Leacock et al., 2014"
L18-1363,tenfjord-etal-2006-ask,0,0.0565424,"The NUS Corpus of Learner English (NUCLE) is annotated for the development and evaluation 2298 of grammatical error correction systems (Dahlmeier et al., 2013). Additional learner corpora exist for European languages. The Lund CEFLE is a leaner corpus of texts in French produced by adolescent Swedish learners of French (Granfeldt et al., 2006). The Error-Annotated German Learner Corpus (EAGLE) is a corpus of beginning learners with grammatical error annotation (Boyd, 2010). The ASK corpus is a learner corpus of Norwegian as a second language that contains essays collected from language tests (Tenfjord et al., 2006). The CzeSL corpus is a learner corpus of Czech that has been annotated using multi-layer error types (Hana et al., 2010). The Hungarian Learner Corpus is composed of student journals annotated for learner errors using tagging sets from different linguistic categories, including phonology, morphology and syntax (Dickinson and Ledbetter, 2012). Recently, learner corpora have been established for Asian languages. The HSK Dynamic Composition Corpus contains simplified Chinese essays written by learners of Chinese, annotated with different error types (Cui and Zhang, 2011; Zhang and Cui, 2013). Th"
L18-1363,N07-1059,0,0.0410418,"annotated learner corpus of traditional Chinese, and the entire learner corpus will be publicly released. Keywords: computer-assisted language learning, second language acquisition, grammatical error diagnosis, interlanguage analysis 1. Introduction Annotating learners’ inappropriate usage of written language is an important task for learner corpus research (Díaz-Negrillo and Fernández-Domínguez, 2006; Tono, 2003). From a linguistic perspective, annotated learner corpora are valuable resources for research in second language acquisition (Swanson and Charniak, 2013), foreign language teaching (Wang and Seneff, 2007), and contrastive interlanguage analysis (Granger, 2015). In engineering, such language resources can be used to develop natural language processing techniques for educational applications, such as automatic essay scoring (Yannakoudakis et al., 2011), assessment report generation (Sakaguchi et al., 2013), and native language identification (Malmasi and Dras, 2015). Automated grammatical error detection and correction are important research directions and a number of competitions have been organized to encourage innovation (Leacock et al., 2014). For example, Helping Our Own (HOO) is a series o"
L18-1363,W15-0614,0,0.105041,"ror diagnosis (Yu et al., 2014; Lee et al., 2015b; Lee et al., 2016b). Recently, the IJCNLP 2017 shared task 1 has focused on Chinese grammatical error diagnosis (Rao et al., 2017). All these competitions require annotated learner corpora for system development and evaluation. To our best knowledge, only two previous studies have manually annotated learner corpora for Chinese as a foreign language (CFL). One is the HSK Dynamic Composition Corpus constructed by the Beijing Language and Culture University (Cui and Zhang, 2011; Zhang and Cui, 2013); the other is the Jinan Chinese Learner Corpus (Wang et al., 2015). The target language of these two studies is simplified Chinese, and no traditional Chinese learner corpus is available for public research. This pilot study thus aims to build such a learner corpus of traditional Chinese to expand research resources, especially for the study of linguistic differences or similarities among Chinese learners around the world. This study annotated grammatical errors in texts collected from learner essays written as part of the Test Of Chinese as Foreign Language (TOCFL). The TOCFL learner corpus contained 2,837 essays written by learners originating from a total"
L18-1363,W15-4402,0,0.0178847,"rror types (Yeh et al., 2014). To compensate for data insufficiency for supervised machine learning, the TMU system extracted a Chinese learner corpus from the Lang-8 website, and used it as a parallel corpus for phrasebased statistical machine translation for grammatical error identification (Zhao et al. 2014). For the NLPTEA 2015 shared task, The HITSZ system presented an ensemble learning based method to detect and identify grammatical errors (Xiang et al. 2015). The SCAU system adopted a hybrid model by integrating rule-based and n-gram statistical methods for grammatical error diagnosis (Wu et al., 2015b). The CYUT team built an error diagnosis system based on the Conditional Random Fields (CRF) (Wu et al., 2015a). The NTOU system proposed two sentence likelihood functions based on frequencies of Google n-grams to diagnose grammatical errors (Lin, & Chen, 2015). The NCYU system also used statistical word and part-of-speech patterns based CRFs to detect grammatical errors (Yeh et al., 2015). The TMU examined corpus augmentation and explored syntax-based and hierarchical phrase-based translation models for use in this task (Zhao et al. 2015). For the NLPTEA 2016 shared task, the ANO system and"
L18-1363,W15-4418,0,0.0183597,"rror types (Yeh et al., 2014). To compensate for data insufficiency for supervised machine learning, the TMU system extracted a Chinese learner corpus from the Lang-8 website, and used it as a parallel corpus for phrasebased statistical machine translation for grammatical error identification (Zhao et al. 2014). For the NLPTEA 2015 shared task, The HITSZ system presented an ensemble learning based method to detect and identify grammatical errors (Xiang et al. 2015). The SCAU system adopted a hybrid model by integrating rule-based and n-gram statistical methods for grammatical error diagnosis (Wu et al., 2015b). The CYUT team built an error diagnosis system based on the Conditional Random Fields (CRF) (Wu et al., 2015a). The NTOU system proposed two sentence likelihood functions based on frequencies of Google n-grams to diagnose grammatical errors (Lin, & Chen, 2015). The NCYU system also used statistical word and part-of-speech patterns based CRFs to detect grammatical errors (Yeh et al., 2015). The TMU examined corpus augmentation and explored syntax-based and hierarchical phrase-based translation models for use in this task (Zhao et al. 2015). For the NLPTEA 2016 shared task, the ANO system and"
L18-1363,W15-4415,0,0.0165488,"r detection (Lin and Chan, 2014). The NCYU system adopted word segmentation and part-of-speech tagging techniques to identify missing and redundant error types (Yeh et al., 2014). To compensate for data insufficiency for supervised machine learning, the TMU system extracted a Chinese learner corpus from the Lang-8 website, and used it as a parallel corpus for phrasebased statistical machine translation for grammatical error identification (Zhao et al. 2014). For the NLPTEA 2015 shared task, The HITSZ system presented an ensemble learning based method to detect and identify grammatical errors (Xiang et al. 2015). The SCAU system adopted a hybrid model by integrating rule-based and n-gram statistical methods for grammatical error diagnosis (Wu et al., 2015b). The CYUT team built an error diagnosis system based on the Conditional Random Fields (CRF) (Wu et al., 2015a). The NTOU system proposed two sentence likelihood functions based on frequencies of Google n-grams to diagnose grammatical errors (Lin, & Chen, 2015). The NCYU system also used statistical word and part-of-speech patterns based CRFs to detect grammatical errors (Yeh et al., 2015). The TMU examined corpus augmentation and explored syntax-b"
L18-1363,P11-1019,0,0.0250825,"tion Annotating learners’ inappropriate usage of written language is an important task for learner corpus research (Díaz-Negrillo and Fernández-Domínguez, 2006; Tono, 2003). From a linguistic perspective, annotated learner corpora are valuable resources for research in second language acquisition (Swanson and Charniak, 2013), foreign language teaching (Wang and Seneff, 2007), and contrastive interlanguage analysis (Granger, 2015). In engineering, such language resources can be used to develop natural language processing techniques for educational applications, such as automatic essay scoring (Yannakoudakis et al., 2011), assessment report generation (Sakaguchi et al., 2013), and native language identification (Malmasi and Dras, 2015). Automated grammatical error detection and correction are important research directions and a number of competitions have been organized to encourage innovation (Leacock et al., 2014). For example, Helping Our Own (HOO) is a series of shared tasks for correcting errors in non-native English texts (Dale and Kilgarriff, 2011, Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors for learners of English as a foreign language (Ng et al., 2013; 2014"
L18-1363,W16-4920,0,0.0243398,"hes (Chou et al., 2016). The NTOU system generated and scored correction candidates for grammatical error diagnosis (Chen et al., 2016b). The HIT system adopted long short-term memory (LSTM) neural networks to identify grammatical errors (Zheng et al., 2016). The PKU system presented a model based on bidirectional LSTM (Huang, & Wang, 2016). The NCYU system proposed the structure of the recurrent neural network using LSTM to detect grammatical errors (Yeh et al., 2016). The YUN-HPCC system built single word embeddings based convolutional neural networks and LSTM neural networks for this task (Yang et al., 2016). In terms of performance, a good system should have a high F1 score and a low false positive rate. Overall, none of the participating systems provided satisfactory results when measuring different metrics (i.e. False Positive Rate, Accuracy, Precision, Recall and F1), indicating the difficulty of developing systems for effective grammatical error diagnosis, especially in the context of CFL. Recently, neural network-based deep learning techniques have shown promising results in identifying Chinese grammatical errors. However, a large amount of training data is needed to train and fine-tune the"
L18-1363,W16-4918,0,0.0271347,"II system diagnosed grammatical errors based on the CRF (Chen et al., 2016a; Liu et al., 2016) along with word order sensitive embedding approaches (Chou et al., 2016). The NTOU system generated and scored correction candidates for grammatical error diagnosis (Chen et al., 2016b). The HIT system adopted long short-term memory (LSTM) neural networks to identify grammatical errors (Zheng et al., 2016). The PKU system presented a model based on bidirectional LSTM (Huang, & Wang, 2016). The NCYU system proposed the structure of the recurrent neural network using LSTM to detect grammatical errors (Yeh et al., 2016). The YUN-HPCC system built single word embeddings based convolutional neural networks and LSTM neural networks for this task (Yang et al., 2016). In terms of performance, a good system should have a high F1 score and a low false positive rate. Overall, none of the participating systems provided satisfactory results when measuring different metrics (i.e. False Positive Rate, Accuracy, Precision, Recall and F1), indicating the difficulty of developing systems for effective grammatical error diagnosis, especially in the context of CFL. Recently, neural network-based deep learning techniques have"
L18-1363,W15-4417,0,0.0206394,"m statistical methods for grammatical error diagnosis (Wu et al., 2015b). The CYUT team built an error diagnosis system based on the Conditional Random Fields (CRF) (Wu et al., 2015a). The NTOU system proposed two sentence likelihood functions based on frequencies of Google n-grams to diagnose grammatical errors (Lin, & Chen, 2015). The NCYU system also used statistical word and part-of-speech patterns based CRFs to detect grammatical errors (Yeh et al., 2015). The TMU examined corpus augmentation and explored syntax-based and hierarchical phrase-based translation models for use in this task (Zhao et al. 2015). For the NLPTEA 2016 shared task, the ANO system and CYUT-III system diagnosed grammatical errors based on the CRF (Chen et al., 2016a; Liu et al., 2016) along with word order sensitive embedding approaches (Chou et al., 2016). The NTOU system generated and scored correction candidates for grammatical error diagnosis (Chen et al., 2016b). The HIT system adopted long short-term memory (LSTM) neural networks to identify grammatical errors (Zheng et al., 2016). The PKU system presented a model based on bidirectional LSTM (Huang, & Wang, 2016). The NCYU system proposed the structure of the recurr"
L18-1363,W16-4907,0,0.0257358,"015). The TMU examined corpus augmentation and explored syntax-based and hierarchical phrase-based translation models for use in this task (Zhao et al. 2015). For the NLPTEA 2016 shared task, the ANO system and CYUT-III system diagnosed grammatical errors based on the CRF (Chen et al., 2016a; Liu et al., 2016) along with word order sensitive embedding approaches (Chou et al., 2016). The NTOU system generated and scored correction candidates for grammatical error diagnosis (Chen et al., 2016b). The HIT system adopted long short-term memory (LSTM) neural networks to identify grammatical errors (Zheng et al., 2016). The PKU system presented a model based on bidirectional LSTM (Huang, & Wang, 2016). The NCYU system proposed the structure of the recurrent neural network using LSTM to detect grammatical errors (Yeh et al., 2016). The YUN-HPCC system built single word embeddings based convolutional neural networks and LSTM neural networks for this task (Yang et al., 2016). In terms of performance, a good system should have a high F1 score and a low false positive rate. Overall, none of the participating systems provided satisfactory results when measuring different metrics (i.e. False Positive Rate, Accurac"
N16-1066,esuli-sebastiani-2006-sentiwordnet,0,0.00308504,"h, users proactively provide their feelings and opinions after browsing the web content. For example, users may read a news article and then offer comments. A user can also review the products available for sale in online stores. In the manual annotation method, trained annotators are asked to create affective annotations for specific language resources for research purposes. Several well-known affective resources are introduced as follows. SentiWordNet is a lexical resource for opinion mining, which assigns to each synset of WordNet three sentiment ratings: positive, negative, and objective (Esuli and Sebastiani, 2006). Linguistic Inquiry and Word Count (LIWC) calculates the degree to which people use different categories of words across a broad range of texts (Pennebaker et al., 2007). In the LIWC 2007 version, the annotators were asked to note their emotions and thoughts about personally relevant topics. The Affective Norms for English Words (ANEW) provides 1,034 English words with ratings in the dimensions of pleasure, arousal and dominance (Bradley and Lang, 1999). In addition to these English-language sentiment lexicons, a few Chinese lexicons have been constructed. The Chinese LIWC (C-LIWC) dictionary"
N16-1066,P14-1147,0,0.00719352,"fective states are generally represented using either categorical or dimensional approaches. The categorical approach represents affective states as several discrete classes such as positive, neutral, negative, and Ekman’s six basic emotions (e.g., anger, happiness, fear, sadness, disgust and surprise) (Ekman, 1992). Based on this representation, various practical applications have been developed such as aspect-based sentiment analysis (Schouten and Frasincar, 2016; Pontiki et al., 2015), Twitter sentiment analysis (Saif et al., 2013; Rosenthal et al., 2015), deceptive opinion spam detection (Li et al., 2014), and cross-lingual portability (Banea et al., 2013; Xu et al., 2015). The dimensional approach represents affective states as continuous numerical values in multiple dimensions, such as valence-arousal (VA) space (Russell, 1980), as shown in Fig. 1. The valence represents the degree of pleasant and unpleasant (i.e., positive and negative) feelings, while the arousal represents the degree of excitement and calm. Based on this representation, any affective state can be represented as a point in the VA coordinate plane. For many application domains (e.g., product reviews, political stance detect"
N16-1066,W02-1011,0,0.044171,"WC with manual revisions to fit the practical characteristics of Chinese usages (Huang et al., 2012). The NTU Sentiment dictionary (NTUSD) has adopted a combination of manual and automatic methods to include positive and negative emotional words (Ku and Chen, 2007). Among the above affective lexicons, only ANEW is dimensional, providing realvalued scores for three dimensions, and the others are categorical, providing information related to sentiment polarity or intensity. In addition to lexicon resources, several Englishlanguage affective corpora have been proposed, such as Movie Review Data (Pang et al. 2002), the MPQA Opinion Corpus (Wiebe et al., 2005), and Affective Norms for English Text (ANET) (Bradley and Lang, 2007). In addition, only ANET provides VA ratings. The above dimensional affective resources ANEW and ANET have been used for both word- and sentence-level VA prediction in previous studies (Wei et al., 2011; Gökçay et al., 2012; Malandrakis et al., 2013; Paltoglou et al., 2013; Yu et al., 2015). In this study, we follow the manual annotation approach to build a Chinese affective lexicon and corpus in the VA dimensions. 3 Affective Resource Construction This section describes the proc"
N16-1066,S15-2082,0,0.0179115,"Missing"
N16-1066,J11-2001,0,0.00435175,"Missing"
N16-1066,P15-2129,1,0.693208,"l, providing information related to sentiment polarity or intensity. In addition to lexicon resources, several Englishlanguage affective corpora have been proposed, such as Movie Review Data (Pang et al. 2002), the MPQA Opinion Corpus (Wiebe et al., 2005), and Affective Norms for English Text (ANET) (Bradley and Lang, 2007). In addition, only ANET provides VA ratings. The above dimensional affective resources ANEW and ANET have been used for both word- and sentence-level VA prediction in previous studies (Wei et al., 2011; Gökçay et al., 2012; Malandrakis et al., 2013; Paltoglou et al., 2013; Yu et al., 2015). In this study, we follow the manual annotation approach to build a Chinese affective lexicon and corpus in the VA dimensions. 3 Affective Resource Construction This section describes the process of building Chinese affective resources with valence-arousal ratings, including the CVAW and CAVT. The CVAW is built on the Chinese affective lexicon C-LIWC, and then annotated with VA ratings for each word. Five annotators were trained to rate each word in the valence and arousal dimensions using the Self Assessment Manikin (SAM) model (Lang, 1980). The SAM model provides affective pictures, which c"
O15-2001,W15-4401,1,0.880156,"Missing"
O15-2001,C14-2015,1,0.887239,"Missing"
O15-2001,W15-3106,1,0.873443,"Missing"
O15-2001,W13-4406,1,0.884207,"Missing"
O15-2001,C12-1184,0,0.230946,"Missing"
O15-2001,P00-1032,0,0.188091,"Missing"
O15-2001,W14-6820,1,\N,Missing
S17-2165,C12-3003,0,0.0268013,"s phrases that capture the main topics mentioned in a given text. Automatically extracting keyphrases and determining their relations from scientific articles has various applications, such as recommending articles to readers, matching reviewers to submissions, facilitating the exploration of huge document collections, and so on. An adapted nominal group chunker and a supervised ranking method based on support vector machines have previously been used to extract keyphrase candidates (Eichler and Neumann, 2010). The conditional random field based keyphrase extraction method has been presented (Bhaskar et al., 2012). A naïve approach has been proposed to investigate characteristics of keyphrases with section information from well-structured scientific articles (Park et al., 2010). Features broadly used for the supervised approaches in scientific articles have been assessed in the compilation of a comprehensive feature list (Kim and Kan, 2009). Maximal sequences and page ranking have been combined to discover latent keyphrases within scientific articles (Ortiz et al., 2010). Noun phrases containing multiple modifiers have been extracted from earth science publications and generalized by matching tree patt"
S17-2165,P14-5010,0,0.00259608,"and identify their relations. The linear chain CRF is empirically effective for predicting the sequence of labels given a sequence input. A word in a sentence is regarded as a state in our CRF. Given an observation and its adjacent states in terms of the distinguished features, the probability of reaching a state is determined based on the Stochastic Gradient Descent. In the testing phase, the proposed CRF reports the sequence of categories with the largest probability as the identified result. The following four features are used for training the CRF model with the Stanford CoreNLP toolkit (Manning et al., 2014). • Word: the original words in the sentence of a scientific article are directly used without any revision. Word Task O O O O O O Task Task Pro. O O O O Mat. O O O O Syn. O O O O Hyp. O O O O O O O O O O O Syn. Syn. O O O Task O O Syn. O O Task O O O O O O O O O O O Syn. O O O O O O Table 2: An example sentence with encoding. • Syntactic Phrase: a phrasal category which is a type of syntactic unit in the grammar structure. Noun phrases are usually regarded as keyphrases in scientific texts. Hence, we only adopt noun phrases and their upper phrasal category as features. Table 1 shows an exampl"
S17-2165,P17-2054,0,0.0513647,"the supervised approaches in scientific articles have been assessed in the compilation of a comprehensive feature list (Kim and Kan, 2009). Maximal sequences and page ranking have been combined to discover latent keyphrases within scientific articles (Ortiz et al., 2010). Noun phrases containing multiple modifiers have been extracted from earth science publications and generalized by matching tree patterns to the syntax trees of the sources texts (Marsi and Öztürk, 2015). Keyphrase boundary classification has been regarded as a multi-task learning problem using deep recurrent neural network (Augenstein and Søgaard, 2017). The ScienceIE task seeks solutions to automatically identify keyphrases within scientific publications, label them, and determine their relationships. Specifically, the ScienceIE task contains three subtasks: (A) Identification of keyphrases: to identify all the keyphrases within a given scientific publication; (B) Classification of identified keyphrases: to label each keyphrase as Process, Task, or Material; (C) Extraction of relationships between two identified keyphrases: to label keyphrases as Hyponym-of or Synonymof. The ScienceIE task presents three evaluation scenarios. In Scenario 1,"
S17-2165,S17-2091,0,0.0639809,"nonym-of, and Hyponym-of). During the testing phase, all trained CRF models are parallel to label one of types. The tags predicting by both Synonym-of and Hyponym-of CRF models are reliable dependently on the other three models, because pairs of keyphrase should be identified first for relations. Hence, we check the pairs of keyphrases to keep those are identified by Task, Process and Material CRF models. Finally, we integrate all identified results as our system outputs without handling any conflicts. 3 3.1 Evaluation Data The datasets for the ScienceIE task were provided by task organizers (Augenstein et al., 2017). The collected corpus consisted of journal articles from ScienceDirect open access publications evenly distributed among Computer Science, Material Science and Physics. The training, development, and test datasets were comprised of sampled paragraphs, of which 350 were used for training data, 50 for development, and 100 for testing. These datasets were made available to participants without copyright restrictions. No external resources were used to supplement the datasets. To pre/post-process the datasets, we transformed alphabet-based start/end counts into word-based positions. 3.2 Implement"
S17-2165,D15-1057,0,0.0325511,"gate characteristics of keyphrases with section information from well-structured scientific articles (Park et al., 2010). Features broadly used for the supervised approaches in scientific articles have been assessed in the compilation of a comprehensive feature list (Kim and Kan, 2009). Maximal sequences and page ranking have been combined to discover latent keyphrases within scientific articles (Ortiz et al., 2010). Noun phrases containing multiple modifiers have been extracted from earth science publications and generalized by matching tree patterns to the syntax trees of the sources texts (Marsi and Öztürk, 2015). Keyphrase boundary classification has been regarded as a multi-task learning problem using deep recurrent neural network (Augenstein and Søgaard, 2017). The ScienceIE task seeks solutions to automatically identify keyphrases within scientific publications, label them, and determine their relationships. Specifically, the ScienceIE task contains three subtasks: (A) Identification of keyphrases: to identify all the keyphrases within a given scientific publication; (B) Classification of identified keyphrases: to label each keyphrase as Process, Task, or Material; (C) Extraction of relationships"
S17-2165,S10-1037,0,0.0323205,"eyphrase candidates (Eichler and Neumann, 2010). The conditional random field based keyphrase extraction method has been presented (Bhaskar et al., 2012). A naïve approach has been proposed to investigate characteristics of keyphrases with section information from well-structured scientific articles (Park et al., 2010). Features broadly used for the supervised approaches in scientific articles have been assessed in the compilation of a comprehensive feature list (Kim and Kan, 2009). Maximal sequences and page ranking have been combined to discover latent keyphrases within scientific articles (Ortiz et al., 2010). Noun phrases containing multiple modifiers have been extracted from earth science publications and generalized by matching tree patterns to the syntax trees of the sources texts (Marsi and Öztürk, 2015). Keyphrase boundary classification has been regarded as a multi-task learning problem using deep recurrent neural network (Augenstein and Søgaard, 2017). The ScienceIE task seeks solutions to automatically identify keyphrases within scientific publications, label them, and determine their relationships. Specifically, the ScienceIE task contains three subtasks: (A) Identification of keyphrases"
S17-2165,S10-1038,0,0.0328369,"applications, such as recommending articles to readers, matching reviewers to submissions, facilitating the exploration of huge document collections, and so on. An adapted nominal group chunker and a supervised ranking method based on support vector machines have previously been used to extract keyphrase candidates (Eichler and Neumann, 2010). The conditional random field based keyphrase extraction method has been presented (Bhaskar et al., 2012). A naïve approach has been proposed to investigate characteristics of keyphrases with section information from well-structured scientific articles (Park et al., 2010). Features broadly used for the supervised approaches in scientific articles have been assessed in the compilation of a comprehensive feature list (Kim and Kan, 2009). Maximal sequences and page ranking have been combined to discover latent keyphrases within scientific articles (Ortiz et al., 2010). Noun phrases containing multiple modifiers have been extracted from earth science publications and generalized by matching tree patterns to the syntax trees of the sources texts (Marsi and Öztürk, 2015). Keyphrase boundary classification has been regarded as a multi-task learning problem using deep"
W09-3418,W06-1001,0,0.116172,"ies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the most reliable and essential resources for linguistic studies for all languages (Magnini and Cavaglia, 2000; Soria et al. 2009; Strapparava and Valitutti, 2004). Lexical Markup Framework (LMF, ISO24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons (Francopoulo et al., 2009). One important purpose of LMF is to define a standard for lexicons which covers multilingual lexical information (Francopoulo et al., 2006b). In this study, we describe the design and implementation of the Wordnet-LMF (Soria et al. 2009) to represent lexical semantics in Chinese WordNet. The rest of this paper is organized as follows: Section 2 introduces Chinese WordNet and Lexical Markup Framework. Section 3 describes how we represent Chinese WordNet in the Lexical Markup Framework (CWN-LMF). Section 4 presents an example on Chinese word sense distinction using CWN-LMF format. Quantitative analysis of compiled CWN-LMF is presented in Section 5. We also describe the application scenario using CWN-LMF for information interoperab"
W09-3418,magnini-cavaglia-2000-integrating,0,0.458758,"integrates WordNet, English-Chinese Translation Equiva1 Wordnet, available online at http://wordnetweb.princeton.edu/perl/webwn 2 Global WordNet Association (GWA), available online at http://www.globalwordnet.org/ lents Database (ECTED) and SUMO for crosslanguage linguistic studies. As a follow-up, Chinese WordNet (CWN) has been built as a robust lexical knowledge system which also embodies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the most reliable and essential resources for linguistic studies for all languages (Magnini and Cavaglia, 2000; Soria et al. 2009; Strapparava and Valitutti, 2004). Lexical Markup Framework (LMF, ISO24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons (Francopoulo et al., 2009). One important purpose of LMF is to define a standard for lexicons which covers multilingual lexical information (Francopoulo et al., 2006b). In this study, we describe the design and implementation of the Wordnet-LMF (Soria et al. 2009) to represent lexical semantics in Chinese WordNet. The rest of this paper is organized as follows: Section 2 in"
W09-3418,huang-etal-2004-sinica,1,0.944371,"MF used to represent lexical semantics in Chinese WordNet. The compiled CWN-LMF will be released to the community for linguistic researches. 1 Introduction Princeton WordNet 1 is an English lexical database that groups nouns, verbs, adjectives and adverbs into sets of cognitive synonyms, which are named as synsets (Fellbaum, 1998; Miller, 1995). The Global WordNet Association (GWA) 2 built on the results of Princeton WordNet and Euro WordNet (Vossen, 2004) is a free and public association that provides a platform to share and connect all languages in the world. For Mandarin Chinese in Taiwan, Huang et al. (2004) constructed the Academia Sinica Bilingual Ontological Wordnet (Sinica BOW) which integrates WordNet, English-Chinese Translation Equiva1 Wordnet, available online at http://wordnetweb.princeton.edu/perl/webwn 2 Global WordNet Association (GWA), available online at http://www.globalwordnet.org/ lents Database (ECTED) and SUMO for crosslanguage linguistic studies. As a follow-up, Chinese WordNet (CWN) has been built as a robust lexical knowledge system which also embodies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the"
W09-3418,I05-3014,1,0.746685,"analyzed according to the guidelines of Chinese word sense distinctions (CKIP, 2003; Huang et al. 2003) which contain information including Partof-Speech, sense definition, example sentences, corresponding English synset(s) from Princeton WordNet, lexical semantic relations and so on. Unlike Princeton WordNet, CWN has not been constructed mainly on the synsets and semantic relations. Rather it focuses to provide precise expression for the Chinese sense division and the semantic relations needs to be based on the linguistic theories, especially lexical semantics (Huang et al., 2008). Moreover, Huang et al. (2005) designed and implemented the Sinica Sense Management System (SSMS) to store and manage word sense data generated in the analysis stage. SSMS is meaning-driven. Each sense of a lemma is identified specifically using a unique identifier and given a separate entry. There are 8,646 lemmas / 25,961 senses until December 2008 have been analyzed and stored in SSMS. Figure 1 shows the result of sense distinction for 足跡 zu-ji ‘footprint’ as an example in Chinese WordNet. Huang et al. (2004) proposed Domain LexicoTaxonomy (DLT) as a domain taxonomy populated with lexical entries. By using DLT with Chin"
W09-3418,strapparava-valitutti-2004-wordnet,0,0.058237,"n Equiva1 Wordnet, available online at http://wordnetweb.princeton.edu/perl/webwn 2 Global WordNet Association (GWA), available online at http://www.globalwordnet.org/ lents Database (ECTED) and SUMO for crosslanguage linguistic studies. As a follow-up, Chinese WordNet (CWN) has been built as a robust lexical knowledge system which also embodies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the most reliable and essential resources for linguistic studies for all languages (Magnini and Cavaglia, 2000; Soria et al. 2009; Strapparava and Valitutti, 2004). Lexical Markup Framework (LMF, ISO24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons (Francopoulo et al., 2009). One important purpose of LMF is to define a standard for lexicons which covers multilingual lexical information (Francopoulo et al., 2006b). In this study, we describe the design and implementation of the Wordnet-LMF (Soria et al. 2009) to represent lexical semantics in Chinese WordNet. The rest of this paper is organized as follows: Section 2 introduces Chinese WordNet and Lexical Markup Framework"
W09-3418,P06-2106,1,0.884198,"Missing"
W09-3418,vossen-etal-2008-kyoto,1,0.832458,"guages. A unified framework is needed for information exchange. LMF is hence adopted as the framework at lexical semantic level in this project. The WordNet in these languages are compiled with designed WordNet-LMF format. CWN-LMF will also be involved and benefit for cross-language interpretabilities in semantic search field. 7 6 Application Scenarios The EU-7 project, KYOTO (Knowledge Yielding Ontologies for Transition-based Organization), wants to make knowledge sharable between communities of people, culture, language and computers, by assigning meaning to text and giving text to meaning (Vossen et al., 2008a; 2008b). The goal of KYOTO is a system that allows people in communities to define the meaning of their words and terms in a shared Wiki platform so that it becomes anchored across languages and cultures but also so that a computer can use this knowledge to detect knowledge and facts in text. KYOTO is a generic system offering knowledge transition and information across different target groups, transgressing linguistic, cultural and geographic boundaries. Initially developed for the environmental domain, KYOTO will be usable in any knowledge domain for mining, organizing, and distributing in"
W09-3418,O05-5001,1,\N,Missing
W12-6335,W00-1205,0,0.221253,"language issues. Chinese parsing has been a resurged research area in recent years thanks to the commercial needs in mobile applications, and there is a pressing need for a common evaluation platform where different approaches can be fairly compared. Relevant events include the CoNLL-X (the 10th Conference on Computational Natural Language Learning, 2006) shared task, which evaluates multilingual dependency parsing techniques. This shared task provides the community with a benchmark for evaluating their parsers across different languages. The Chinese data is derived from the Sinica Treebank (Huang et al, 2000; Chen et al., 1999; Chen et al. 2003), which is regarded as the first data set designing for traditional Chinese parsing evaluation. The CoNLL 2007 shared task was the second year event devoted to dependency parsing. The task consists of two separate tasks: a multi-lingual track and a domain adaption track. The designed ideas of the shared task are motivated by the expectation that a parser should be trainable for any language, possibly by adjusting some parameters. The traditional Chinese data set can be used in this multilingual parsing evaluation. At SIGHAN Bake-offs 2012, we organize the"
W12-6335,P03-1054,0,0.0142258,"itted runs adopt a single parsing model, i.e. Single System, to accomplish the evaluated task. In Task 4-1, we received 8 submitted results, including 7 from closed track systems and 1 from an open track system. In Task 4-2, we received 4 submissions, including 3 from closed track systems and 1 from an open track system. 5.1 Analysis of sentence parsing We evaluated the sentence parsing performance of both tracks separately. Table 3 and Table 4 show the evaluated results in closed track and open track, respectively. For closed track, we implement the baseline system using the Stanford parser (Klein and Manning, 2003; Levy and Manning, 2003) with default parameters for performance comparison. We only adopt the training set to learn the Chinese parsing model. In formal testing phase, there were 75 sentences that cannot be parsed using the re-train Stanford parser. Experimental results indicate that the baseline system achieves micro-averaging and macro-averaging F1 at 0.5822 and 0.5757, respectively. Parts of the submitted runs perform better than the baseline results. Systems come from NEURun1 and NEU-Run2 achieve the best performance, i.e. 0.7078 for micro-averaging F1 and 0.7211 for macro-averaging F1."
W12-6335,I05-1016,0,0.74365,"els and their boundaries were evaluated in 202 sub-task 1, so the performance is the same. Note that the NCTU&NTUT-Run1 was submitted a few days after the formal test deadline. However, we also evaluated their results for more information. Only one team took part in the open track. The performance measures of this submission are micro-averaging F1 score: 0.4355 and macroaveraging F1 score: 0.4287. For performance ID comparison, we invited the Chinese Knowledge Information Processing Group (CKIP) in the Institute of information Science, Academia Sinica, to modify their designed Chinese parser (Yang et al. 2005; 2008; Hsieh et al. 2007) for this evaluation. The CKIP parser achieves the best microaveraging and macro-averaging F1 scores at 0.7287 and 0.7448, respectively. Task 4-1 Participants Open 1 Chaoyang University of Technology (CYUT) 2 National Central University (NCU) 3 National Chiayi University (NCYU) 4 National Chiao Tung University & National Taipei University of Technology (NCTU&NTUT) 5 6 7 8 University of Macau (UM) Northeastern University (NEU) Peking University (PKU) Japan Patent Information Organization (JAPIO) Total Task 4-2 Closed Open Closed 1 1 1 2 1 1 2 1 1 8 2 1 1 3 Table 2: Res"
W12-6335,I08-2098,0,0.0305006,"Missing"
W12-6335,P03-1056,0,0.0418737,"parsing model, i.e. Single System, to accomplish the evaluated task. In Task 4-1, we received 8 submitted results, including 7 from closed track systems and 1 from an open track system. In Task 4-2, we received 4 submissions, including 3 from closed track systems and 1 from an open track system. 5.1 Analysis of sentence parsing We evaluated the sentence parsing performance of both tracks separately. Table 3 and Table 4 show the evaluated results in closed track and open track, respectively. For closed track, we implement the baseline system using the Stanford parser (Klein and Manning, 2003; Levy and Manning, 2003) with default parameters for performance comparison. We only adopt the training set to learn the Chinese parsing model. In formal testing phase, there were 75 sentences that cannot be parsed using the re-train Stanford parser. Experimental results indicate that the baseline system achieves micro-averaging and macro-averaging F1 at 0.5822 and 0.5757, respectively. Parts of the submitted runs perform better than the baseline results. Systems come from NEURun1 and NEU-Run2 achieve the best performance, i.e. 0.7078 for micro-averaging F1 and 0.7211 for macro-averaging F1. These two runs have the s"
W12-6335,O07-4005,0,0.660252,"were evaluated in 202 sub-task 1, so the performance is the same. Note that the NCTU&NTUT-Run1 was submitted a few days after the formal test deadline. However, we also evaluated their results for more information. Only one team took part in the open track. The performance measures of this submission are micro-averaging F1 score: 0.4355 and macroaveraging F1 score: 0.4287. For performance ID comparison, we invited the Chinese Knowledge Information Processing Group (CKIP) in the Institute of information Science, Academia Sinica, to modify their designed Chinese parser (Yang et al. 2005; 2008; Hsieh et al. 2007) for this evaluation. The CKIP parser achieves the best microaveraging and macro-averaging F1 scores at 0.7287 and 0.7448, respectively. Task 4-1 Participants Open 1 Chaoyang University of Technology (CYUT) 2 National Central University (NCU) 3 National Chiayi University (NCYU) 4 National Chiao Tung University & National Taipei University of Technology (NCTU&NTUT) 5 6 7 8 University of Macau (UM) Northeastern University (NEU) Peking University (PKU) Japan Patent Information Organization (JAPIO) Total Task 4-2 Closed Open Closed 1 1 1 2 1 1 2 1 1 8 2 1 1 3 Table 2: Result submission statistics"
W12-6335,W04-1116,0,\N,Missing
W12-6335,O99-4004,0,\N,Missing
W13-4406,P00-1032,0,0.81353,"the correct characters of detected errors. Spelling check must be done within a context, say a sentence or a long phrase with a certain meaning, and cannot be done within one word (Mays et al., 1991). However, spelling check in Chinese is very different from that in English or other alphabetic languages. There are no word delimiters between words and the length of each word is very short. There are several previous studies addressing the Chinese spelling check problem. Chang (1995) has proposed a bi-gram language model to substitute the confusing character for error detection and correction. Zhang et al. (2000) have presented an approximate word-matching algorithm to detect and correct Chinese spelling errors us35 Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing (SIGHAN-7), pages 35–42, Nagoya, Japan, 14 October 2013. uation. Section 4 proposes the evaluation metrics for both subtasks. Section 5 presents the results of participants’ approaches for performance comparison. Section 6 elaborates on the semantic and pragmatic aspects of automatic correction of Chinese text. Finally, we conclude this paper with the findings and future research direction in the Section 7. 2 <DOC Ni"
W14-6820,W13-4406,1,0.29649,"data preparation, performance metrics, and evaluation results based on essays written by Chinese as a foreign language learners. The hope is that such evaluations can produce more advanced Chinese spelling check techniques. 1 Introduction Chinese spelling errors frequently arise from confusion between multiple Chinese characters which are phonologically and visually similar, but semantically distinct (Liu et al., 2011). The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to"
W14-6820,W13-4418,1,0.817147,"licly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage composed of several sentences, the checker should identify all possible spelling errors, highlight their locations and suggest possible correction"
W14-6820,W13-4414,1,0.830631,"lly similar, but semantically distinct (Liu et al., 2011). The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check"
W14-6820,W13-4409,0,0.0469075,"ta sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage compo"
W14-6820,W13-4420,1,\N,Missing
W15-3106,W10-4107,0,0.0316855,"ith spelling errors for which no errors are detected. The criteria for judging correctness are determined at two levels as follows. (1) Detection level: all locations of incorrect characters in a given passage should be completely identical with the gold standard. (2) Correction level: all locations and corresponding corrections of incorrect characters should be completely identical with the gold standard. 33 In addition to achieve satisfactory detection/correction performance, reducing the false positive rate, that is the mistaken identification of errors where none exist, is also important (Wu et al., 2010). The following metrics are measured at both levels with the help of the confusion matrix. • F1=0.57 (=2*0.5*0.67/(0.5+0.67)) • Correction-level • Accuracy =0.4 (=2/5) • False Positive Rate (FPR) = FP / (FP+TN) • Accuracy = (TP+TN) / (TP+FP+TN+FN) • Precision = TP / (TP+FP) Notes: {“B2-1923-2, 8, 誤, 41, 情”, “B22731-1, 0”} / {“A2-0092-2, 5, 玩”, “A20243-1, 3, 件, 4, 康”, “B2-1923-2, 8, 誤, 41, 情”, “B2-2731-1, 0”, “B2-3754-3, 11, 觀”} • Recall = TP / (TP+FN) • Precision = 0.25 (=1/4) • F1= 2 *Precision*Recall/(Precision+Recall) Notes: {“B2-1923-2, 8, 誤 , 41, 情 ”} / {“A2-0092-2, 5, 玩”, “A2-0243-1, 3,"
W15-3106,W13-4406,1,0.421764,"these make Chinese spell checking a challengeable task. An empirical analysis indicated that Chinese spelling errors frequently arise from confusion among multiple-character words, which are phonologically and visually similar, but semantically distinct (Liu et al., 2011). The automatic spelling checker should have both capabilities of identifying the spelling errors and suggesting the correct characters of erroneous usages. The SIGHAN 2013 Bake-off for Chinese Spelling Check was the first campaign to provide data sets as benchmarks for the performance evaluation of Chinese spelling checkers (Wu et al., 2013). The data in SIGHAN 2013 originated from the essays written by native Chinese speakers. Following the experience of the first evaluation, the second bake-off was held in CIPS-SIGHAN Joint CLP2 Task Description The goal of this task is to evaluate the capability of a Chinese spelling checker. A passage consisting of several sentences with/without spelling errors is given as the input. The checker should return the locations of incorrect characters and suggest the correct characters. Each character or punctuation mark occupies 1 spot for counting location. The input instance is given a unique p"
W15-3106,W14-6820,1,\N,Missing
W15-4401,C14-1028,0,0.0732007,"an language technologies for English grammatical error correction have attracted more attention in recent years (Ng et al., 2013; 2014). In contrast to the plethora of research related to develop NLP tools for learners of English as a foreign language, relatively few studies have focused on detecting and correcting grammatical errors for use by learners of Chinese as a foreign language (CFL). A classifier has been designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). A ranking SVMbased model has been further explored to suggest corrections for word-ordering errors (Cheng et al., 2014). Relative positioning and parse template language models have been proposed to detect Chinese grammatical errors written by US learners (Wu et al., 2010). A penalized probabilistic first-order inductive learning algorithm has been presented for Chinese grammatical error diagnosis (Chang et al. 2012). A set of linguistic rules with syntactic information was manually crafted to detect CFL grammatical errors (Lee et al., 2013). A sentence judgment system has been 2 Task Description The goal of this shared task is to develop NLP tools for identifying the grammatical errors in sentences written by"
W15-4401,C14-2015,1,0.875633,"Missing"
W15-4401,W14-1701,0,0.204729,"Missing"
W15-4401,W13-3601,0,0.033284,"ummarizes the findings and offers futures research directions. Abstract This paper introduces the NLP-TEA 2015 shared task for Chinese grammatical error diagnosis. We describe the task, data preparation, performance metrics, and evaluation results. The hope is that such an evaluation campaign may produce more advanced Chinese grammatical error diagnosis techniques. All data sets with gold standards and evaluation tools are publicly available for research purposes. 1 Introduction Human language technologies for English grammatical error correction have attracted more attention in recent years (Ng et al., 2013; 2014). In contrast to the plethora of research related to develop NLP tools for learners of English as a foreign language, relatively few studies have focused on detecting and correcting grammatical errors for use by learners of Chinese as a foreign language (CFL). A classifier has been designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). A ranking SVMbased model has been further explored to suggest corrections for word-ordering errors (Cheng et al., 2014). Relative positioning and parse template language models have been proposed to detect Chinese grammatical er"
W15-4401,C12-1184,0,0.156036,"All data sets with gold standards and evaluation tools are publicly available for research purposes. 1 Introduction Human language technologies for English grammatical error correction have attracted more attention in recent years (Ng et al., 2013; 2014). In contrast to the plethora of research related to develop NLP tools for learners of English as a foreign language, relatively few studies have focused on detecting and correcting grammatical errors for use by learners of Chinese as a foreign language (CFL). A classifier has been designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). A ranking SVMbased model has been further explored to suggest corrections for word-ordering errors (Cheng et al., 2014). Relative positioning and parse template language models have been proposed to detect Chinese grammatical errors written by US learners (Wu et al., 2010). A penalized probabilistic first-order inductive learning algorithm has been presented for Chinese grammatical error diagnosis (Chang et al. 2012). A set of linguistic rules with syntactic information was manually crafted to detect CFL grammatical errors (Lee et al., 2013). A sentence judgment system has been 2 Task Descri"
W16-0513,K15-1010,0,0.0493788,"Missing"
W16-0513,P15-1068,0,0.0230547,"Missing"
W16-0513,C12-1038,0,0.0476375,"Missing"
W16-0513,P11-1092,0,0.0312142,"usions are finally drawn in Section 6. 2 Related Work Automated grammatical error detection and correction for second/foreign language learners has attracted considerable research attention. Although commercial products such as Microsoft Word have long provided grammatical checking for English, researchers in NLP have found that there is still much room for improvement in this area. A number of techniques have recently been proposed to deal with various types of writing errors. A novel approach based on alternating structure optimization was proposed to correct article and preposition errors (Dahlmeier and Ng, 2011). A linguistically motivated approach was also proposed to correct verb errors (Rozovskaya et al., 2014). A classifier was designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). Linguistic structures with interacting grammatical properties were identified to address such dependencies via joint inference and learning (Rozovskaya and Roth, 2013). A set of linguistic rules with syntactic information was handcrafted for detecting errors in Chinese sentences (Lee et al., 2013). A sentence judgment system was developed using both rulebased linguistic analysis and an n-gram"
W16-0513,W11-2838,0,0.0349601,"best F-score of 0.6108 ranked second among the ten submissions. Our system also achieved an F-score of 0.7419 for the probabilistic estimation track, ranking fourth among the nine submissions. 1 Introduction Automated grammatical error detection and correction are important tasks and research topics in computational linguistics. A number of competitive tasks have been organized to encourage innovation in this direction (Leacock et al., 2014). For examples, Helping Our Own (HOO) was a series of shared tasks used for correcting grammatical errors of English texts written by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors among learners of English as a foreign language in the educational application (Ng et al., 2013; 2014). The first NLP-TEA workshop featured a shared task on grammatical error diagnosis for learners of Chinese as a foreign language (Yu et al., 2014). The following year, a similar Chinese grammatical error diagnosis shared task was held in the second NLP-TEA workshop in conjunction with ACL-IJCNLP 2015 (Lee et al., 2015). These competitions reflect the need for automated writing assistance for various appl"
W16-0513,W12-2006,0,0.0264763,"ed second among the ten submissions. Our system also achieved an F-score of 0.7419 for the probabilistic estimation track, ranking fourth among the nine submissions. 1 Introduction Automated grammatical error detection and correction are important tasks and research topics in computational linguistics. A number of competitive tasks have been organized to encourage innovation in this direction (Leacock et al., 2014). For examples, Helping Our Own (HOO) was a series of shared tasks used for correcting grammatical errors of English texts written by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors among learners of English as a foreign language in the educational application (Ng et al., 2013; 2014). The first NLP-TEA workshop featured a shared task on grammatical error diagnosis for learners of Chinese as a foreign language (Yu et al., 2014). The following year, a similar Chinese grammatical error diagnosis shared task was held in the second NLP-TEA workshop in conjunction with ACL-IJCNLP 2015 (Lee et al., 2015). These competitions reflect the need for automated writing assistance for various applications. The Automa"
W16-0513,W16-0506,0,0.0400992,"Missing"
W16-0513,D15-1052,0,0.0234013,"Missing"
W16-0513,W15-4401,1,0.83985,"correcting grammatical errors of English texts written by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors among learners of English as a foreign language in the educational application (Ng et al., 2013; 2014). The first NLP-TEA workshop featured a shared task on grammatical error diagnosis for learners of Chinese as a foreign language (Yu et al., 2014). The following year, a similar Chinese grammatical error diagnosis shared task was held in the second NLP-TEA workshop in conjunction with ACL-IJCNLP 2015 (Lee et al., 2015). These competitions reflect the need for automated writing assistance for various applications. The Automated Evaluation of Scientific Writing (AESW) shared task seeks to promote the use of NLP tools to help improve the quality of scientific writing in English by predicting whether a given sentence needs language editing or not. The AESW shared task contains two tracks: (1) a Boolean prediction track in which a sentence in need of editing will result in a binary classifier outputting true; otherwise the system should return false; and (2) a probabilistic estimation track in which the system e"
W16-0513,C14-2015,1,0.856882,"d to correct verb errors (Rozovskaya et al., 2014). A classifier was designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). Linguistic structures with interacting grammatical properties were identified to address such dependencies via joint inference and learning (Rozovskaya and Roth, 2013). A set of linguistic rules with syntactic information was handcrafted for detecting errors in Chinese sentences (Lee et al., 2013). A sentence judgment system was developed using both rulebased linguistic analysis and an n-gram statistical method for detecting grammatical errors (Lee et al., 2014). A penalized probabilistic first-order inductive learning algorithm was presented for Chinese grammatical error diagnosis (Chang et al., 2012). Relative position and parse template language models were proposed to correct grammatical errors (Wu et al., 2010). Dependency trees were used to train a language model for correcting grammati123 cal errors at the tree level (Zhang and Wang, 2014). A classification-based system and a statistical machine translation-based system were combined to improve correction quality (Susanto et al., 2014). Different from correcting grammatical errors independentl"
W16-0513,W14-1701,0,0.0295116,"Missing"
W16-0513,W13-3601,0,0.020586,"ntroduction Automated grammatical error detection and correction are important tasks and research topics in computational linguistics. A number of competitive tasks have been organized to encourage innovation in this direction (Leacock et al., 2014). For examples, Helping Our Own (HOO) was a series of shared tasks used for correcting grammatical errors of English texts written by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012). The CoNLL 2013/2014 shared tasks aimed to correct grammatical errors among learners of English as a foreign language in the educational application (Ng et al., 2013; 2014). The first NLP-TEA workshop featured a shared task on grammatical error diagnosis for learners of Chinese as a foreign language (Yu et al., 2014). The following year, a similar Chinese grammatical error diagnosis shared task was held in the second NLP-TEA workshop in conjunction with ACL-IJCNLP 2015 (Lee et al., 2015). These competitions reflect the need for automated writing assistance for various applications. The Automated Evaluation of Scientific Writing (AESW) shared task seeks to promote the use of NLP tools to help improve the quality of scientific writing in English by predicti"
W16-0513,D14-1162,0,0.080731,"of the F-score between human annotators. More recently, deep learning techniques have been widely applied to problems in natural language processing with promising results. This trend motivates us to explore convolutional neural networks to automatically evaluate scientific writing at the sentence level. 3 The NTNU-YZU System Figure 1 shows our Convolutional Neural Network (CNN) architecture for the AESW shared task. An input sentence is represented as a sequence of words. Each word refers to a row looked up in a word embedding matrix generating from Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). We use convolutions over the sentence matrix to extract the features. A single convolution layer is adopted. The sliding window is called a filter in the CNN. We obtain the full convolutions by sliding the filters over the whole Figure 1: The illustration of our convolutional neural network architecture for the AESW shared task. matrix. Each filter performs the convolution operation on the sentence matrix and generates a feature map. A pooling layer is then used to subsample features over each map. The most common approach to pooling is to apply a max operation to reduce the dimensionality f"
W16-0513,E14-1038,0,0.0143098,"ion for second/foreign language learners has attracted considerable research attention. Although commercial products such as Microsoft Word have long provided grammatical checking for English, researchers in NLP have found that there is still much room for improvement in this area. A number of techniques have recently been proposed to deal with various types of writing errors. A novel approach based on alternating structure optimization was proposed to correct article and preposition errors (Dahlmeier and Ng, 2011). A linguistically motivated approach was also proposed to correct verb errors (Rozovskaya et al., 2014). A classifier was designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). Linguistic structures with interacting grammatical properties were identified to address such dependencies via joint inference and learning (Rozovskaya and Roth, 2013). A set of linguistic rules with syntactic information was handcrafted for detecting errors in Chinese sentences (Lee et al., 2013). A sentence judgment system was developed using both rulebased linguistic analysis and an n-gram statistical method for detecting grammatical errors (Lee et al., 2014). A penalized probabilistic first"
W16-0513,D13-1074,0,0.0129857,"nt in this area. A number of techniques have recently been proposed to deal with various types of writing errors. A novel approach based on alternating structure optimization was proposed to correct article and preposition errors (Dahlmeier and Ng, 2011). A linguistically motivated approach was also proposed to correct verb errors (Rozovskaya et al., 2014). A classifier was designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). Linguistic structures with interacting grammatical properties were identified to address such dependencies via joint inference and learning (Rozovskaya and Roth, 2013). A set of linguistic rules with syntactic information was handcrafted for detecting errors in Chinese sentences (Lee et al., 2013). A sentence judgment system was developed using both rulebased linguistic analysis and an n-gram statistical method for detecting grammatical errors (Lee et al., 2014). A penalized probabilistic first-order inductive learning algorithm was presented for Chinese grammatical error diagnosis (Chang et al., 2012). Relative position and parse template language models were proposed to correct grammatical errors (Wu et al., 2010). Dependency trees were used to train a la"
W16-0513,D14-1102,0,0.0340242,"Missing"
W16-0513,P13-1143,0,0.0356618,"Missing"
W16-0513,C12-1184,0,0.030377,"al products such as Microsoft Word have long provided grammatical checking for English, researchers in NLP have found that there is still much room for improvement in this area. A number of techniques have recently been proposed to deal with various types of writing errors. A novel approach based on alternating structure optimization was proposed to correct article and preposition errors (Dahlmeier and Ng, 2011). A linguistically motivated approach was also proposed to correct verb errors (Rozovskaya et al., 2014). A classifier was designed to detect word-ordering errors in Chinese sentences (Yu and Chen, 2012). Linguistic structures with interacting grammatical properties were identified to address such dependencies via joint inference and learning (Rozovskaya and Roth, 2013). A set of linguistic rules with syntactic information was handcrafted for detecting errors in Chinese sentences (Lee et al., 2013). A sentence judgment system was developed using both rulebased linguistic analysis and an n-gram statistical method for detecting grammatical errors (Lee et al., 2014). A penalized probabilistic first-order inductive learning algorithm was presented for Chinese grammatical error diagnosis (Chang et"
W16-0513,D14-1033,0,0.0248309,"Missing"
W16-0513,N15-1060,0,\N,Missing
W16-4906,W11-2838,0,0.0260814,"Of the 15 teams registered for this shared task, 9 teams developed the system and submitted a total of 36 runs. We expected this evaluation campaign could lead to the development of more advanced NLP techniques for educational applications, especially for Chinese error detection. All data sets with gold standards and scoring scripts are made publicly available to researchers. 1 Introduction Recently, automated grammar checking for learners of English as a foreign language has attracted more attention. For example, Helping Our Own (HOO) is a series of shared tasks in correcting textual errors (Dale and Kilgarriff, 2011; Dale et al., 2012). The shared tasks at CoNLL 2013 and CoNLL 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013; 2014). Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language(CFL) learners. Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based a"
W16-4906,W12-2006,0,0.0459908,"for this shared task, 9 teams developed the system and submitted a total of 36 runs. We expected this evaluation campaign could lead to the development of more advanced NLP techniques for educational applications, especially for Chinese error detection. All data sets with gold standards and scoring scripts are made publicly available to researchers. 1 Introduction Recently, automated grammar checking for learners of English as a foreign language has attracted more attention. For example, Helping Our Own (HOO) is a series of shared tasks in correcting textual errors (Dale and Kilgarriff, 2011; Dale et al., 2012). The shared tasks at CoNLL 2013 and CoNLL 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013; 2014). Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language(CFL) learners. Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al.,"
W16-4906,W14-1701,0,0.174327,"Missing"
W16-4906,W13-3601,0,0.0358318,"cational applications, especially for Chinese error detection. All data sets with gold standards and scoring scripts are made publicly available to researchers. 1 Introduction Recently, automated grammar checking for learners of English as a foreign language has attracted more attention. For example, Helping Our Own (HOO) is a series of shared tasks in correcting textual errors (Dale and Kilgarriff, 2011; Dale et al., 2012). The shared tasks at CoNLL 2013 and CoNLL 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013; 2014). Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language(CFL) learners. Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al., 2013) and hybrid methods (Lee et al., 2014). In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural La"
W16-4906,W15-4401,1,0.297001,"cations which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al., 2013) and hybrid methods (Lee et al., 2014). In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL (Yu et al., 2014). A second version of this shared task in NLP-TEA was collocated with the ACL-IJCNLP-2015 (Lee et al., 2015). In conjunction with the COLLING 2016, the third NLP-TEA features a shared task for Chinese grammatical error diagnosis again. The main purpose of these shared tasks is to provide a common setting so that researchers who approach the tasks using different linguistic factors and computational techniques can compare their results. Such technical evaluations allow researchers to exchange their experiences to advance the field and eventually develop optimal solutions to this shared task. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://"
W16-4906,C14-2015,1,0.85477,"2013 and CoNLL 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013; 2014). Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language(CFL) learners. Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al., 2013) and hybrid methods (Lee et al., 2014). In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL (Yu et al., 2014). A second version of this shared task in NLP-TEA was collocated with the ACL-IJCNLP-2015 (Lee et al., 2015). In conjunction with the COLLING 2016, the third NLP-TEA features a shared task for Chinese grammatical error diagnosis again. The main purpose of these shared tasks is to provide a common setting"
W16-4906,C12-1184,0,0.124723,"rrors (Dale and Kilgarriff, 2011; Dale et al., 2012). The shared tasks at CoNLL 2013 and CoNLL 2014 focused on grammatical error correction, increasing the visibility of educational application research in the NLP community (Ng et al., 2013; 2014). Many of these learning technologies focus on learners of English as a Foreign Language (EFL), while relatively few grammar checking applications have been developed to support Chinese as a Foreign Language(CFL) learners. Those applications which do exist rely on a range of techniques, such as statistical learning (Chang et al, 2012; Wu et al, 2010; Yu and Chen, 2012), rule-based analysis (Lee et al., 2013) and hybrid methods (Lee et al., 2014). In response to the limited availability of CFL learner data for machine learning and linguistic analysis, the ICCE-2014 workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA) organized a shared task on diagnosing grammatical errors for CFL (Yu et al., 2014). A second version of this shared task in NLP-TEA was collocated with the ACL-IJCNLP-2015 (Lee et al., 2015). In conjunction with the COLLING 2016, the third NLP-TEA features a shared task for Chinese grammatical error diagnosis"
W19-5058,D18-1187,0,0.0874533,"Missing"
W19-5058,D17-1070,0,0.0431338,"l Language Inference (SNLI) corpus is a well-known dataset and serves as a benchmark for NLI system evaluations (Bowman et al., 2015). However, it is restricted to a single text genre. Therefore, the MedNLI dataset, which is annotated by doctors and grounded in patients’ medical histories, was built to perform NLI tasks in the clinical domain (Romanov and Shivade, 2018). In addition to feature-based methods and bag-of-words (BOW) models, other experiments have tested several modern neural networks-based models for the specialized and knowledge intensive field of medicine, including InferSent (Conneau et al., 2017) and ESIM (Chen et al., 2017) The MEDIQA challenge focuses on attracting research efforts in Natural Language Inference (NLI), Recognizing Question Entailment (RQE) and their applications in medical Question Answering (QA). The MEDIQA challenge includes three tasks: 1) NLI: identifying three inference relations between two medical sentences, that is, entailment, neutral and contradiction. 2) RQE: identifying entailment between two questions in the context of QA. 3) QA: filtering and improving the input ranks of retrieved answers, generated by the medical QA system CHiQA. The reuse of NLI and/o"
W19-5058,P17-1152,0,0.0763698,"Missing"
W19-5058,D15-1075,0,0.0364024,"ng Short-Term Memory) network with an attention mechanism for medical text inferences. A total of 42 teams participated in natural language inference task at MEDIQA 2019. Our best accuracy score of 0.84 ranked the top-third among all submissions in the leaderboard. 1 Introduction Natural Language Inference (NLI) is the task of determining whether a given hypothesis is true (entailment), false (contradiction) or undetermined (neutral) by inferring a given premise. The Stanford Natural Language Inference (SNLI) corpus is a well-known dataset and serves as a benchmark for NLI system evaluations (Bowman et al., 2015). However, it is restricted to a single text genre. Therefore, the MedNLI dataset, which is annotated by doctors and grounded in patients’ medical histories, was built to perform NLI tasks in the clinical domain (Romanov and Shivade, 2018). In addition to feature-based methods and bag-of-words (BOW) models, other experiments have tested several modern neural networks-based models for the specialized and knowledge intensive field of medicine, including InferSent (Conneau et al., 2017) and ESIM (Chen et al., 2017) The MEDIQA challenge focuses on attracting research efforts in Natural Language In"
W19-5058,I17-1102,0,0.0275303,"of words. Each word refers to a row looked up in a word embedding matrix from the last layer of the original BERT (Devlin et al., 2018). In this NLI task, we model the sentence-pairs using Bidirectional LSTM (Graves et al., 2013), an extension of the traditional LSTM, to train two LSTMs on the input pairs. The second LSTM is a reversed copy of the first one, so that we can take full advantage of both past and future input features for a specific time step. Consequently, we leverage the word attention mechanism to capture the distinguishing influence of the words and then form a dense vector (Yang et al., 2017). We then use final softmax activation function to classify the sentence-pairs to obtain the probability that belongs to each of the three classes. During the training phase, if a sentence-pair (premise vs. hypothesis) is true (entailment), the class is assigned as 1, and 0 otherwise (contradiction). If both sentences are neutral 3 3.1 Evaluation Data The datasets were mainly provided by task organizers (Ben Abacha et al., 2019). The sentence-pairs for the NLI task were collected from the MedNLI dataset (Romanov and Shivade, 2018). The training, validation and test datasets were comprised of d"
Y08-1042,W03-1004,0,0.0868378,"ywords: Top-bag-of-word similarity, Text Source Classification, Contrastive Approach, Comparable Corpus, Chinese Gigaword. 1. Introduction Comparable corpora are corpora which select similar texts in more than one language or language variety 1 . These texts are typically gathered during the same time period. Comparable corpora are different from parallel corpora are widely used as resources for statistical machine translation, bilingual lexicons. Comparable corpora overcome the scarcity and limitations of parallel corpora, since sources for original, monolingual texts are much more abundant (Barzilay and Elhadad, 2003; Munteanu et al., 2004; Shao and Ng, 2004; Talvensaari et al., 2007). The degree and nature of lexical similarity and contrast among Mandarin Chinese used in different Chinese speaking societies were widely observed but not thoroughly studied due to the lack of comparable corpora. Recently, LDC’s Chinese Gigaword (2003)contains three sets of monolingual corpora selected according to the same set of criteria but in different language varieties from China, Singapore and Taiwan. We will explore it as a comparable corpus for variations of Chinese in this paper. In particular, we propose a measure"
Y08-1042,O02-2002,0,0.0946677,"measure. Shao and Ng (2004) proposed a method by combining both context and transliteration information for the task of mining new word translations. They translated Chinese Words into English and tested it on Chinese and English Gigaword. Munteanu et al. (2004) improved machine translation performance via parallel sentence extraction from comparable corpora that consist of two large monolingual news texts in English and Arabic. Talvensaari et al. (2007) used Relative Average Term Frequency (RATF) valve to create a comparable corpus from articles by a Swedish news agency and a U.S. newspaper. Chen and You (2002) proposed using only syntactic related co-occurrences as context vectors and adopted information theoretic methods for measuring word similarity to solve the problem of data sparseness and characteristic precision. Gao et al. (2002) extended the basic cooccurrence model by adding a decaying factor that decreases the mutual information when the distance between the terms increases. The experimental results also indicated that their proposed triple translation model brings further improvements than word-by-word translation. Weeds and Weir (2005) proposed a flexible framework called as co-occurre"
Y08-1042,P98-1069,0,0.0201269,"concludes this study. 1 Definition of comparable corpus according http://www.ilc.cnr.it/EAGLES/corpustyp/node21.html to EAGLES report, 22nd Pacific Asia Conference on Language, Information and Computation, pages 404–410 404 accessed at 2. Literature Review 2.1 Word Similarity Measures in Comparable Corpus A comparable corpus is one which gathered the similar texts in more than one language or language variety from the same time periods. Comparable corpora were widely used in researches issues consist of machine translation, natural language processing and cross language information retrieval. Fung and Yee (1998) used a comparable-corpus-based approach to estimate the similarity between a word and its translation candidates. Fung and Cheung (2004) used multi-level bootstrapping to iteratively improve alignment for extracting parallel sentences from a quasi-comparable corpus. Cheng et al. (2004) mined bilingual search results obtained from search engines to translate unknown query terms. Their approach was with Web corpora that can alleviate the problem of the lack of large bilingual corpora and benefit cross-language Web search. Barzilay and Elhadad (2003) focused on monolingual comparable corpus, i.e"
Y08-1042,huang-etal-2008-quality,1,0.821955,"Missing"
Y08-1042,ma-huang-2006-uniform,1,0.875784,"Missing"
Y08-1042,N04-1034,0,0.0469673,"larity, Text Source Classification, Contrastive Approach, Comparable Corpus, Chinese Gigaword. 1. Introduction Comparable corpora are corpora which select similar texts in more than one language or language variety 1 . These texts are typically gathered during the same time period. Comparable corpora are different from parallel corpora are widely used as resources for statistical machine translation, bilingual lexicons. Comparable corpora overcome the scarcity and limitations of parallel corpora, since sources for original, monolingual texts are much more abundant (Barzilay and Elhadad, 2003; Munteanu et al., 2004; Shao and Ng, 2004; Talvensaari et al., 2007). The degree and nature of lexical similarity and contrast among Mandarin Chinese used in different Chinese speaking societies were widely observed but not thoroughly studied due to the lack of comparable corpora. Recently, LDC’s Chinese Gigaword (2003)contains three sets of monolingual corpora selected according to the same set of criteria but in different language varieties from China, Singapore and Taiwan. We will explore it as a comparable corpus for variations of Chinese in this paper. In particular, we propose a measure of top-bag-of-word sim"
Y08-1042,C04-1089,0,0.0572338,"ssification, Contrastive Approach, Comparable Corpus, Chinese Gigaword. 1. Introduction Comparable corpora are corpora which select similar texts in more than one language or language variety 1 . These texts are typically gathered during the same time period. Comparable corpora are different from parallel corpora are widely used as resources for statistical machine translation, bilingual lexicons. Comparable corpora overcome the scarcity and limitations of parallel corpora, since sources for original, monolingual texts are much more abundant (Barzilay and Elhadad, 2003; Munteanu et al., 2004; Shao and Ng, 2004; Talvensaari et al., 2007). The degree and nature of lexical similarity and contrast among Mandarin Chinese used in different Chinese speaking societies were widely observed but not thoroughly studied due to the lack of comparable corpora. Recently, LDC’s Chinese Gigaword (2003)contains three sets of monolingual corpora selected according to the same set of criteria but in different language varieties from China, Singapore and Taiwan. We will explore it as a comparable corpus for variations of Chinese in this paper. In particular, we propose a measure of top-bag-of-word similarity for compari"
Y08-1042,W02-1811,0,0.0180443,"Missing"
Y08-1042,J05-4002,0,0.0629182,"icles by a Swedish news agency and a U.S. newspaper. Chen and You (2002) proposed using only syntactic related co-occurrences as context vectors and adopted information theoretic methods for measuring word similarity to solve the problem of data sparseness and characteristic precision. Gao et al. (2002) extended the basic cooccurrence model by adding a decaying factor that decreases the mutual information when the distance between the terms increases. The experimental results also indicated that their proposed triple translation model brings further improvements than word-by-word translation. Weeds and Weir (2005) proposed a flexible framework called as co-occurrence retrieval for lexical distributional similarity. Zheng et al. (2007) presented a novel word co-occurrence model based on an ontology representation of word sense and its related applications. 2.2 Introduction to Chinese Gigaword Automatic annotation is remains a challenging task in Chinese language processing. For instance, ACL SigHan has hosted four bakeoff competition for segmentation, but none for POS tagging. There is only a handful of POS tagging systems and automatic taggers which are widely accepted and accessible. In Taiwan, Academ"
Y08-1042,C04-1151,0,\N,Missing
Y08-1042,C98-1066,0,\N,Missing
Y08-1042,N03-1031,0,\N,Missing
Y09-1031,W04-2214,0,0.154547,"and Cavaglia, 2000). Synsets have been semi-automatically annotated with at least one domain labels. These domain labels, such as Music, Transport, and Law, are selected from a set of about 200 labels that are hierarchally organized referred to the Dewey Decimal Classification (DDC). Magnini and Cavaglia (2000) manually annotated a small number of high level synsets with their domain labels. Then, an automatic procedure exploited some of the WordNet relations to extend the manual assignment to the reachable synsets. In addition, an exception procedure was used to prevent a wrong propagation. Bentivogli et al. (2004) further revised the WordNet Domains Hierarchy (WDH) with a clear semantics and evaluated the coverage and balancing of Basic Domains of WDH. The latest version, WordNet Domains 3.2, contains the mapping between Princeton WordNet 2.0 synsets and their corresponding domains. 45 Basic Domains of total 168 domains are used to annotate WordNet synsets. Take “00197005-n history law” for example, “00197005-n” is the synset off set and Part-of-Speech and “history law” is the list of domains associated to the synset. Notice that an additional label named as “Factotum” was assign to Generic synset, whi"
Y09-1031,J06-1003,0,0.0141984,"andarin Chinese in Taiwan, Huang et al. (2004a) constructed the Academia Sinica Bilingual Ontological Wordnet (Sinica BOW), which integrates WordNet, English-Chinese Translation Equivalents Database (ECTED) and SUMO for cross-language linguistic studies. As a follow-up, Chinese WordNet has been built as a robust lexical knowledge system which embodies a precise expression of sense and sense relations as well (Huang et al., 2008b). In recent years, WordNet-like resources have become one of the most reliable and essential resource for linguistic studies for all languages (Niles and Pease, 2003; Budanitsky and Hirst, 2006; Soria et al., 2009a ). Semantic domain labels, characterized by domain-specific lexica, are profitably used to describe texts and word senses according to general subjects, such as sport, finance, and politics. WordNet Domains (Magnini and Cavaglia, 2000) was created by extending the Princeton WordNet with domains labels. Synsets have been semi-automatically annotated with at least one domain label. A domain can include synsets of different part-of-speech and from different WordNet sub-hierarchies. So far the existing WordNets such as Italian WordNet, ∗ This work was funded by National Scien"
Y09-1031,W02-1106,1,0.800153,"t and WordNet Domains. Chinese WordNet focuses to provide precise expression for the Chinese sense division and lexical semantic relations. In addition, partial senses are annotated with domain nodes from Domain Lexico-Taxonomy. WordNet Domains contains the mapping between Princeton WordNet 2.0 synsets and their corresponding domains. Synsets have been semi-automatically annotated with at least one domain label, which is selected from Dewey Decimal Classification (DDC). Since cross-lingual lexical semantic relation inferences were examined by bootstrapping a Sinica BOW with Princeton WordNet (Huang et al. 2002; 2003b), we decide to use bootstrapping methods for constructing a language resources named as Chinese WordNet Domains. By using an existing WordNet Domains as a medium, we automatically annotate word senses of Chinese WordNet with semantic domain labels from three aspects: 1) Princeton WordNet alignment, 2) lexical semantic relations and 3) domain taxonomy mapping. Details will be described as the following subsections. 3.1 Alignment-mediated Domain Prediction Word senses of Chinese WordNet are strictly aligned with the corresponding synsets of Princeton WordNet; therefore, we can use alignm"
Y09-1031,huang-etal-2004-sinica,1,0.935732,"ng Chinese WordNet Domains to the community for research purposes. Keywords: Bootstrapping, Chinese WordNet, WordNet Domains, Multi-label. 1 Introduction Princeton WordNet is an English lexical database that groups nouns, verbs, adjectives and adverbs into sets of cognitive synonyms, which are named as synsets (Fellbaum, 1998; Miller, 1995). The Global WordNet Association (GWA), built on the results of Princeton WordNet and Euro WordNet (Vossen, 2004), is a free and public association that provides a platform that shares and connects all languages in the world. For Mandarin Chinese in Taiwan, Huang et al. (2004a) constructed the Academia Sinica Bilingual Ontological Wordnet (Sinica BOW), which integrates WordNet, English-Chinese Translation Equivalents Database (ECTED) and SUMO for cross-language linguistic studies. As a follow-up, Chinese WordNet has been built as a robust lexical knowledge system which embodies a precise expression of sense and sense relations as well (Huang et al., 2008b). In recent years, WordNet-like resources have become one of the most reliable and essential resource for linguistic studies for all languages (Niles and Pease, 2003; Budanitsky and Hirst, 2006; Soria et al., 200"
Y09-1031,magnini-cavaglia-2000-integrating,0,0.630896,"s a follow-up, Chinese WordNet has been built as a robust lexical knowledge system which embodies a precise expression of sense and sense relations as well (Huang et al., 2008b). In recent years, WordNet-like resources have become one of the most reliable and essential resource for linguistic studies for all languages (Niles and Pease, 2003; Budanitsky and Hirst, 2006; Soria et al., 2009a ). Semantic domain labels, characterized by domain-specific lexica, are profitably used to describe texts and word senses according to general subjects, such as sport, finance, and politics. WordNet Domains (Magnini and Cavaglia, 2000) was created by extending the Princeton WordNet with domains labels. Synsets have been semi-automatically annotated with at least one domain label. A domain can include synsets of different part-of-speech and from different WordNet sub-hierarchies. So far the existing WordNets such as Italian WordNet, ∗ This work was funded by National Science Council, Taiwan under Grants NSC 97-2923-I-001-001-MY3, and also cooperated with EU-FP7 KYOTO project. Copyright 2009 by Lung-Hao Lee, Yu-Ting Yu, and Chu-Ren Huang 23rd Pacific Asia Conference on Language, Information and Computation, pages 288–296 288"
Y09-1031,W09-3418,1,\N,Missing
Y09-1031,I05-3014,1,\N,Missing
Y09-1031,O05-5001,1,\N,Missing
