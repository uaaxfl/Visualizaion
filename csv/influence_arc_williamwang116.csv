2020.acl-main.18,P17-1162,0,0.0516566,"Missing"
2020.acl-main.18,N12-1093,0,0.041873,"ted Work NLG from Structured Data As it is a core objective in many NLP applications, natural language generation from structured data/knowledge (NLG) has been studied for many years. Early traditional NLG systems follow the pipeline paradigm that explicitly divides generation into content selection, macro/micro planning and surface realization (Reiter and Dale, 1997). Such a pipeline paradigm largely relies on templates and hand-engineered features. Many works have been proposed to tackle the individual modules, such as (Liang et al., 2009; Walker et al., 2001; Lu et al., 2009). Later works (Konstas and Lapata, 2012, 2013) investigated modeling context selection and surface realization in an unified framework. Most recently, with the success of deep neural networks, data-driven, neural based approaches have been used, including the end-to-end methods that jointly model context selection and surface realization (Liu et al., 2018; Wiseman et al., 2018; Puduppully et al., 2018). Such data-driven approaches achieve good performance on several benchmarks like E2E challenge (Novikova et al., 2017), WebNLG challenge (Gardent et al., 2017) and W IKI B IO (Lebret et al., 2016). However, they rely on massive amoun"
2020.acl-main.18,W17-5525,0,0.428765,"ahmer, 2018) is an important research problem for various NLP applications. Some examples are taskoriented dialog, question answering (He et al., 2017; Ghazvininejad et al., 2018; Su et al., 2016; Saha et al., 2018; Yin et al., 2016) and interdisciplinary applications such as medicine (Hasan and Farri, 2019; Cawsey et al., 1997) and healthcare (Hasan and Farri, 2019; DiMarco et al., 2007). There is great potential to use automatic NLG systems in a wide range of real-life applications. Recently, deep neural network based NLG systems have been developed, such as those seen in the E2E challenge (Novikova et al., 2017), W EATHER G OV (Liang et al., 2009), as well as more complex Motivated by this, we propose the new task of fewshot natural language generation: given only a handful of labeled instances (e.g., 50 - 200 training instances), the system is required to produce satisfactory text outputs (e.g., BLEU ≥ 20). To the best of our knowledge, such a problem in NLG community still remains under-explored. Herein, we propose a simple yet very effective approach that can generalize across different domains. In general, to describe information in a table, we need two skills to compose coherent and faithful sen"
2020.acl-main.18,D14-1162,0,0.0820109,"Missing"
2020.acl-main.18,N18-1202,0,0.0401584,"s of paired training examples are required, meanwhile without the need for any target examples. This is especially important for real-world use cases where such large target-side gold references are mostly hard to obtain. Therefore, our task is more challenging and closer to real-world settings. 2.2 Large Scale Pre-Trained Models Many of the current best-performing methods for various NLP tasks adopt a combination of pretraining followed by supervised fine-tuning, using task-specific data. Different levels of pre-training include word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018), sentence embeddings (Le and Mikolov, 2014; Kiros et al., 2015), and most recently, language modeling based pre-training like BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019). Such models are pre-trained on large-scale open-domain corpora, and provide down-streaming tasks with rich prior knowledge while boosting their performance. In this paper, we adopt the idea of employing a pre-trained language model to endow in-domain NLG models with language modeling ability, which cannot be well learned from few shot training instances. 3 3.1 pcopy = sigmoid(Wc ct + Ws st + Wx xt + b) P i Wh"
2020.acl-main.18,P17-1099,0,0.197413,"Missing"
2020.acl-main.18,P16-1162,0,0.0252728,"s by crawling Wikipedia pages. After filtering and cleanup, we end up with 23,651 instances for Books domain and 39,450 instances for Songs domain2 . Together with the Humans domain of the original W IKI B IO dataset, for all three domains we conduct experiments by varying the training dataset size to 50, 100, 200 and 500. The rest of data is used for validation (1,000) and testing. The weight λ of the copy loss term is set to 0.7. Other parameter settings can be found in Appendix A. To deal with vocabulary limitation of few-shot training, for all models we adopt the Byte Pair Encoding (BPE) (Sennrich et al., 2016) and subword vocabulary in (Radford et al., 2019). We compare the proposed method with other approaches investigated in Section 3, serving as the baselines - Base-original: the original model 2 Note that the target text sometimes contains information not in the infobox. This is out of the scope of the fewshot generation in this work. Therefore we further filter the datasets and remove the ones with rare words out of infobox. Check (Dhingra et al., 2019) for a related study of this issue on the WikiBio dataset Results and Analysis Following previous work (Liu et al., 2018), we first conduct aut"
2020.acl-main.18,D16-1054,0,0.0192812,"BLEU points improvement. Our code and data can be found at https: 1. Can we significantly reduce human annotation effort to achieve reasonable performance using neural NLG models? 2. Can we make the best of generative pre-training, as prior knowledge, to generate text from structured data? //github.com/czyssrs/Few-Shot-NLG 1 Introduction Natural language generation (NLG) from structured data or knowledge (Gatt and Krahmer, 2018) is an important research problem for various NLP applications. Some examples are taskoriented dialog, question answering (He et al., 2017; Ghazvininejad et al., 2018; Su et al., 2016; Saha et al., 2018; Yin et al., 2016) and interdisciplinary applications such as medicine (Hasan and Farri, 2019; Cawsey et al., 1997) and healthcare (Hasan and Farri, 2019; DiMarco et al., 2007). There is great potential to use automatic NLG systems in a wide range of real-life applications. Recently, deep neural network based NLG systems have been developed, such as those seen in the E2E challenge (Novikova et al., 2017), W EATHER G OV (Liang et al., 2009), as well as more complex Motivated by this, we propose the new task of fewshot natural language generation: given only a handful of labe"
2020.acl-main.18,N01-1003,0,0.0945709,"rage of over 8.0 BLEU on various domains. 2 2.1 Related Work NLG from Structured Data As it is a core objective in many NLP applications, natural language generation from structured data/knowledge (NLG) has been studied for many years. Early traditional NLG systems follow the pipeline paradigm that explicitly divides generation into content selection, macro/micro planning and surface realization (Reiter and Dale, 1997). Such a pipeline paradigm largely relies on templates and hand-engineered features. Many works have been proposed to tackle the individual modules, such as (Liang et al., 2009; Walker et al., 2001; Lu et al., 2009). Later works (Konstas and Lapata, 2012, 2013) investigated modeling context selection and surface realization in an unified framework. Most recently, with the success of deep neural networks, data-driven, neural based approaches have been used, including the end-to-end methods that jointly model context selection and surface realization (Liu et al., 2018; Wiseman et al., 2018; Puduppully et al., 2018). Such data-driven approaches achieve good performance on several benchmarks like E2E challenge (Novikova et al., 2017), WebNLG challenge (Gardent et al., 2017) and W IKI B IO ("
2020.acl-main.18,D17-1239,0,0.0788567,"Missing"
2020.acl-main.18,D18-1356,0,0.0912113,"Missing"
2020.acl-main.18,W16-0106,0,0.0301025,"d data can be found at https: 1. Can we significantly reduce human annotation effort to achieve reasonable performance using neural NLG models? 2. Can we make the best of generative pre-training, as prior knowledge, to generate text from structured data? //github.com/czyssrs/Few-Shot-NLG 1 Introduction Natural language generation (NLG) from structured data or knowledge (Gatt and Krahmer, 2018) is an important research problem for various NLP applications. Some examples are taskoriented dialog, question answering (He et al., 2017; Ghazvininejad et al., 2018; Su et al., 2016; Saha et al., 2018; Yin et al., 2016) and interdisciplinary applications such as medicine (Hasan and Farri, 2019; Cawsey et al., 1997) and healthcare (Hasan and Farri, 2019; DiMarco et al., 2007). There is great potential to use automatic NLG systems in a wide range of real-life applications. Recently, deep neural network based NLG systems have been developed, such as those seen in the E2E challenge (Novikova et al., 2017), W EATHER G OV (Liang et al., 2009), as well as more complex Motivated by this, we propose the new task of fewshot natural language generation: given only a handful of labeled instances (e.g., 50 - 200 training"
2020.acl-main.265,W19-3821,0,0.0448601,"Missing"
2020.acl-main.265,D14-1067,0,0.0156878,"1 Introduction With the wealth of information being posted online daily, relation extraction has become increasingly important. Relation extraction aims specifically to extract relations from raw sentences and represent them as succinct relation tuples of the form (head, relation, tail) e.g., (Barack Obama, spouse, Michelle Obama). The concise representations provided by relation extraction models have been used to extend Knowledge Bases (KBs) (Riedel et al., 2013; Subasic et al., 2019; Trisedya et al., 2019). These KBs are then used heavily in NLP systems, such as question answering systems (Bordes et al., 2014; Yin et al., 2016; Cui et al., 2019). In recent years, much focus in the Neural Relation Extraction (NRE) community has been centered on improvements in model precision and the reduction of noise (Lin et al., 2016; Liu et al., 2017; Wu et al., 2017; Feng et al., 2018; Vashishth et al., 2018; Qin et al., 2018). Yet, little attention has been devoted towards the fairness of such systems. We take the first step at understanding and evaluating gender bias in NRE systems by measuring the differences in model performance when extracting relations from sentences written about females versus sentence"
2020.acl-main.265,N19-3002,0,0.0177559,"ed different metrics to evaluate gender bias, for example, by using the performance difference of the model on male and female datapoints for bias evaluation (Lu et al., 2018; Kiritchenko and Mohammad, 2018). Other metrics have been proposed to evaluate fairness of predictors and allocative bias (Dwork et al., 2012; Hardt et al., 2016), such as Equality of Opportunity. In this work, we use both of these metrics to evaluate NRE models. Mitigation Methods. After discovering gender bias existing, prior work has developed various methods to mitigate that bias (Escud´e Font and Costa-juss`a, 2019; Bordia and Bowman, 2019). Those mitigation methods can be applied in different levels of a model, including in the training phase, in the embedding layer, or in the inference procedure. In this paper, we test three existing debiasing approaches, namely data augmentation (Zhao et al., 2018; Lu et al., 2018), and word embedding debiasing technique (Hard Debiasing (Bolukbasi et al., 2016)) for mitigating bias in NRE models. 2944 Train Development Test Total Original Dataset Entity Pairs Instances M F M F 12,139 4,571 27,048 9,391 1,587 553 3,416 1,144 1,030 1,101 2,320 2,284 14,756 6,225 32,784 12,819 Equalized Dataset"
2020.acl-main.265,D19-3029,0,0.0704855,"ther discrepancy: amongst articles we sampled, proportionally, the spouse relation is mentioned more often relative to hypernym, birthPlace, and birthDate in female articles than in male articles. Additionally, we show that amongst female and male articles we sampled, hypernyms are mentioned more often in male than female articles relative to spouse, birthPlace, and birthDate (see Section 2). This observation aligns with the literature, arguing that authors do not write about the two genders equally (Wagner et al., 2015; Graells-Garrido et al., 2015). 4 Gender Bias in NRE We evaluate OpenNRE (Han et al., 2019), a popular open-source NRE system. OpenNRE implements the approach from (Lin et al., 2016). To convert sentences into vectors, researchers propose convolutional neural networks as well as the pieceweise convoultional neural networks (PCNN) which retain more structural information between entities (Zeng et al., 2015). In this work, we use a PCNN with Selective Attention for the experiments. We train every encoder-selector combination on the training set of WikiGenderBias and its genderequalized version. We input Word2Vec (Mikolov et al., 2013) word embeddings trained on WikiGenPerformance Pari"
2020.acl-main.265,P11-1055,0,0.0602945,", and many are unable to extract intra-sentence relations (Bach and Badaskar, 2007). When data annotation is insufficient or hard to obtain and semi-supervised approaches are insufficient, the distant supervision assumption is used to collect data to train supervised models (Mintz et al., 2009). Given a relation (e1 , r, e2 ) in a knowledge base (KB), distant supervision assumes any sentence that contains both e1 and e2 expresses r (Mintz et al., 2009). Great efforts have been made to improve NRE models by mitigating the effects of noise in the training data introduced by Distant Supervision (Hoffmann et al., 2011; Surdeanu et al., 2012; Lin et al., 2016; Liu et al., 2017; Feng et al., 2018; Qin et al., 2018). However, to our knowledge, there are no studies on bias or ethics in NRE, which is filled by this work. 3 WikiGenderBias We define gender bias in NRE as a difference in model performance when predicting on sentences from male versus female articles. Thus, we need articles written about entities for which we can identify the gender information. However, to obtain gender information for existing annotated datasets could be costly or impossible. Thus, we elected to create WikiGenderBias with this ge"
2020.acl-main.265,S18-2005,0,0.0560903,"Missing"
2020.acl-main.265,P16-1200,0,0.411465,"nt them as succinct relation tuples of the form (head, relation, tail) e.g., (Barack Obama, spouse, Michelle Obama). The concise representations provided by relation extraction models have been used to extend Knowledge Bases (KBs) (Riedel et al., 2013; Subasic et al., 2019; Trisedya et al., 2019). These KBs are then used heavily in NLP systems, such as question answering systems (Bordes et al., 2014; Yin et al., 2016; Cui et al., 2019). In recent years, much focus in the Neural Relation Extraction (NRE) community has been centered on improvements in model precision and the reduction of noise (Lin et al., 2016; Liu et al., 2017; Wu et al., 2017; Feng et al., 2018; Vashishth et al., 2018; Qin et al., 2018). Yet, little attention has been devoted towards the fairness of such systems. We take the first step at understanding and evaluating gender bias in NRE systems by measuring the differences in model performance when extracting relations from sentences written about females versus sentences written about males. If a NRE model predicts a relation such occupation with higher recall on male entities, this could lead to the resulted knowledge bases having more occupation information for males than for f"
2020.acl-main.265,D17-1189,0,0.0623649,"t relation tuples of the form (head, relation, tail) e.g., (Barack Obama, spouse, Michelle Obama). The concise representations provided by relation extraction models have been used to extend Knowledge Bases (KBs) (Riedel et al., 2013; Subasic et al., 2019; Trisedya et al., 2019). These KBs are then used heavily in NLP systems, such as question answering systems (Bordes et al., 2014; Yin et al., 2016; Cui et al., 2019). In recent years, much focus in the Neural Relation Extraction (NRE) community has been centered on improvements in model precision and the reduction of noise (Lin et al., 2016; Liu et al., 2017; Wu et al., 2017; Feng et al., 2018; Vashishth et al., 2018; Qin et al., 2018). Yet, little attention has been devoted towards the fairness of such systems. We take the first step at understanding and evaluating gender bias in NRE systems by measuring the differences in model performance when extracting relations from sentences written about females versus sentences written about males. If a NRE model predicts a relation such occupation with higher recall on male entities, this could lead to the resulted knowledge bases having more occupation information for males than for females (see the il"
2020.acl-main.265,D19-1530,0,0.0199034,"imately equal, then train on this modified, equalized distribution. Data Augmentation. The contexts in which males and females are written about can differ; for instance, on Wikipedia women are more often written about with words related to sexuality than men (Graells-Garrido et al., 2015). Data augmentation mitigates these contextual biases by replacing masculine words in a sentence with their corresponding feminine words and vice versa for all sentences in a corpus, and then training on the union of the original and augmented corpora3 (Zhao et al., 2018; Lu et al., 2018; Dixon et al., 2018; Maudslay et al., 2019; Zhao et al., 2019). Word Embedding Debiasing Word embeddings can encode gender biases (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018) and this can affect bias in downstream predictions for models using the embeddings (Zhao et al., 2018; Font and Costa-Jussa, 2019). In this work, we apply the Hard-Debiasing technique (Bolukbasi et al., 2016). We applied Hard-Debiasing to Word2Vec embeddings (Mikolov et al., 2013), which we trained on the sentences in WikiGenderBias. When used in conjunction with data augmentation, the embeddings are re-trained on the union of the two corpor"
2020.acl-main.265,mendes-etal-2012-dbpedia,0,0.0144587,"e when predicting on sentences from male versus female articles. Thus, we need articles written about entities for which we can identify the gender information. However, to obtain gender information for existing annotated datasets could be costly or impossible. Thus, we elected to create WikiGenderBias with this gender information to be able to detect scenarios like that in Figure 1. The data statistics of WikiGenderBias are given in Table 1. 3.1 Dataset Creation Wikipedia is associated with a knowledge base, DBPedia, that contains relation information for entities with articles on Wikipedia (Mendes et al., 2012). Many of these entities have gender information and their corresponding articles are readily available. Therefore, we create our dataset based on sentences extracted from Wikipedia. To generate WikiGenderBias, we use a variant of the distant supervision assumption: for a given relation between two entities, if one sentence from an article written about one entity also mentions the other entity, then we assume that such sentence expresses the relation. For instance, if we know (Barack, spouse, Michelle) is a relation tuple and we find the sentence He and Michelle were married in Barack’s Wikip"
2020.acl-main.265,P09-1113,0,0.680096,"as supervised, including feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 2005) and kernelbased methods (Lodhi et al., 2002; Zelenko et al., 2003), or semi-supervised (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006), or purely unsupervised (Etzioni et al., 2008). Supervised approaches suffer from the need for large amounts of labelled data, which is sometimes not feasible, and generalizes poorly to open domain relation extraction, since labeled data is required for every entity-relation type (Bach and Badaskar, 2007; Mintz et al., 2009). Many semi-supervised approaches rely on patternmatching, which is not robust, and many are unable to extract intra-sentence relations (Bach and Badaskar, 2007). When data annotation is insufficient or hard to obtain and semi-supervised approaches are insufficient, the distant supervision assumption is used to collect data to train supervised models (Mintz et al., 2009). Given a relation (e1 , r, e2 ) in a knowledge base (KB), distant supervision assumes any sentence that contains both e1 and e2 expresses r (Mintz et al., 2009). Great efforts have been made to improve NRE models by mitigating"
2020.acl-main.265,P06-1015,0,0.138817,"ender-equalized dataset created by down-sampling male instances. Neural Relation Extraction. Relation extraction is a task in NLP with a long history that typically seeks to extract structured tuples (e1 , r, e2 ) from texts (Bach and Badaskar, 2007). Early on, learning algorithms for relation extraction models were typically categorized as supervised, including feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 2005) and kernelbased methods (Lodhi et al., 2002; Zelenko et al., 2003), or semi-supervised (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006), or purely unsupervised (Etzioni et al., 2008). Supervised approaches suffer from the need for large amounts of labelled data, which is sometimes not feasible, and generalizes poorly to open domain relation extraction, since labeled data is required for every entity-relation type (Bach and Badaskar, 2007; Mintz et al., 2009). Many semi-supervised approaches rely on patternmatching, which is not robust, and many are unable to extract intra-sentence relations (Bach and Badaskar, 2007). When data annotation is insufficient or hard to obtain and semi-supervised approaches are insufficient, the di"
2020.acl-main.265,P18-1199,1,0.903039,"se, Michelle Obama). The concise representations provided by relation extraction models have been used to extend Knowledge Bases (KBs) (Riedel et al., 2013; Subasic et al., 2019; Trisedya et al., 2019). These KBs are then used heavily in NLP systems, such as question answering systems (Bordes et al., 2014; Yin et al., 2016; Cui et al., 2019). In recent years, much focus in the Neural Relation Extraction (NRE) community has been centered on improvements in model precision and the reduction of noise (Lin et al., 2016; Liu et al., 2017; Wu et al., 2017; Feng et al., 2018; Vashishth et al., 2018; Qin et al., 2018). Yet, little attention has been devoted towards the fairness of such systems. We take the first step at understanding and evaluating gender bias in NRE systems by measuring the differences in model performance when extracting relations from sentences written about females versus sentences written about males. If a NRE model predicts a relation such occupation with higher recall on male entities, this could lead to the resulted knowledge bases having more occupation information for males than for females (see the illustration in Figure 1). Eventually, the gender bias in knowledge bases may aff"
2020.acl-main.265,N13-1008,0,0.0243098,"s mitigation approaches have a negative effect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in relation extraction. 1 Introduction With the wealth of information being posted online daily, relation extraction has become increasingly important. Relation extraction aims specifically to extract relations from raw sentences and represent them as succinct relation tuples of the form (head, relation, tail) e.g., (Barack Obama, spouse, Michelle Obama). The concise representations provided by relation extraction models have been used to extend Knowledge Bases (KBs) (Riedel et al., 2013; Subasic et al., 2019; Trisedya et al., 2019). These KBs are then used heavily in NLP systems, such as question answering systems (Bordes et al., 2014; Yin et al., 2016; Cui et al., 2019). In recent years, much focus in the Neural Relation Extraction (NRE) community has been centered on improvements in model precision and the reduction of noise (Lin et al., 2016; Liu et al., 2017; Wu et al., 2017; Feng et al., 2018; Vashishth et al., 2018; Qin et al., 2018). Yet, little attention has been devoted towards the fairness of such systems. We take the first step at understanding and evaluating gend"
2020.acl-main.265,N18-2002,0,0.0396004,"Missing"
2020.acl-main.265,D19-1339,1,0.836473,"ate WikiGenderBias, a new dataset for evaluating gender bias in NRE systems. • We present an evaluation framework to demonstrate that gender bias is exhibited in NRE model outputs. • We test several existing bias mitigation approaches to reducing gender bias in NRE system. Our analysis sheds light for designing future mitigating techniques. 2 Related Work Gender Bias Measurement. Existing studies have revealed gender bias in various NLP tasks (Zhao et al., 2017; Rudinger et al., 2018; Zhao et al., 2018; Dixon et al., 2018; Lu et al., 2018; Kiritchenko and Mohammad, 2018; Romanov et al., 2019; Sheng et al., 2019; Sun et al., 2019). People have proposed different metrics to evaluate gender bias, for example, by using the performance difference of the model on male and female datapoints for bias evaluation (Lu et al., 2018; Kiritchenko and Mohammad, 2018). Other metrics have been proposed to evaluate fairness of predictors and allocative bias (Dwork et al., 2012; Hardt et al., 2016), such as Equality of Opportunity. In this work, we use both of these metrics to evaluate NRE models. Mitigation Methods. After discovering gender bias existing, prior work has developed various methods to mitigate that bias"
2020.acl-main.265,D12-1042,0,0.0357208,"o extract intra-sentence relations (Bach and Badaskar, 2007). When data annotation is insufficient or hard to obtain and semi-supervised approaches are insufficient, the distant supervision assumption is used to collect data to train supervised models (Mintz et al., 2009). Given a relation (e1 , r, e2 ) in a knowledge base (KB), distant supervision assumes any sentence that contains both e1 and e2 expresses r (Mintz et al., 2009). Great efforts have been made to improve NRE models by mitigating the effects of noise in the training data introduced by Distant Supervision (Hoffmann et al., 2011; Surdeanu et al., 2012; Lin et al., 2016; Liu et al., 2017; Feng et al., 2018; Qin et al., 2018). However, to our knowledge, there are no studies on bias or ethics in NRE, which is filled by this work. 3 WikiGenderBias We define gender bias in NRE as a difference in model performance when predicting on sentences from male versus female articles. Thus, we need articles written about entities for which we can identify the gender information. However, to obtain gender information for existing annotated datasets could be costly or impossible. Thus, we elected to create WikiGenderBias with this gender information to be"
2020.acl-main.265,P19-1023,0,0.0142747,"ect on NRE. Our analysis lays groundwork for future quantifying and mitigating bias in relation extraction. 1 Introduction With the wealth of information being posted online daily, relation extraction has become increasingly important. Relation extraction aims specifically to extract relations from raw sentences and represent them as succinct relation tuples of the form (head, relation, tail) e.g., (Barack Obama, spouse, Michelle Obama). The concise representations provided by relation extraction models have been used to extend Knowledge Bases (KBs) (Riedel et al., 2013; Subasic et al., 2019; Trisedya et al., 2019). These KBs are then used heavily in NLP systems, such as question answering systems (Bordes et al., 2014; Yin et al., 2016; Cui et al., 2019). In recent years, much focus in the Neural Relation Extraction (NRE) community has been centered on improvements in model precision and the reduction of noise (Lin et al., 2016; Liu et al., 2017; Wu et al., 2017; Feng et al., 2018; Vashishth et al., 2018; Qin et al., 2018). Yet, little attention has been devoted towards the fairness of such systems. We take the first step at understanding and evaluating gender bias in NRE systems by measuring the differ"
2020.acl-main.265,W16-0106,0,0.023369,"he wealth of information being posted online daily, relation extraction has become increasingly important. Relation extraction aims specifically to extract relations from raw sentences and represent them as succinct relation tuples of the form (head, relation, tail) e.g., (Barack Obama, spouse, Michelle Obama). The concise representations provided by relation extraction models have been used to extend Knowledge Bases (KBs) (Riedel et al., 2013; Subasic et al., 2019; Trisedya et al., 2019). These KBs are then used heavily in NLP systems, such as question answering systems (Bordes et al., 2014; Yin et al., 2016; Cui et al., 2019). In recent years, much focus in the Neural Relation Extraction (NRE) community has been centered on improvements in model precision and the reduction of noise (Lin et al., 2016; Liu et al., 2017; Wu et al., 2017; Feng et al., 2018; Vashishth et al., 2018; Qin et al., 2018). Yet, little attention has been devoted towards the fairness of such systems. We take the first step at understanding and evaluating gender bias in NRE systems by measuring the differences in model performance when extracting relations from sentences written about females versus sentences written about ma"
2020.acl-main.265,D15-1203,0,0.0186653,"female articles relative to spouse, birthPlace, and birthDate (see Section 2). This observation aligns with the literature, arguing that authors do not write about the two genders equally (Wagner et al., 2015; Graells-Garrido et al., 2015). 4 Gender Bias in NRE We evaluate OpenNRE (Han et al., 2019), a popular open-source NRE system. OpenNRE implements the approach from (Lin et al., 2016). To convert sentences into vectors, researchers propose convolutional neural networks as well as the pieceweise convoultional neural networks (PCNN) which retain more structural information between entities (Zeng et al., 2015). In this work, we use a PCNN with Selective Attention for the experiments. We train every encoder-selector combination on the training set of WikiGenderBias and its genderequalized version. We input Word2Vec (Mikolov et al., 2013) word embeddings trained on WikiGenPerformance Parity Score The goal of a successful relation extraction model is to maximize F1 score while minimizing the model performance gender gap (or disparity score). However, when comparing different systems, it is hard to decide what is the right balance between these two objectives. On one end, a model which has zero gender"
2020.acl-main.265,N19-1064,1,0.822506,"in on this modified, equalized distribution. Data Augmentation. The contexts in which males and females are written about can differ; for instance, on Wikipedia women are more often written about with words related to sexuality than men (Graells-Garrido et al., 2015). Data augmentation mitigates these contextual biases by replacing masculine words in a sentence with their corresponding feminine words and vice versa for all sentences in a corpus, and then training on the union of the original and augmented corpora3 (Zhao et al., 2018; Lu et al., 2018; Dixon et al., 2018; Maudslay et al., 2019; Zhao et al., 2019). Word Embedding Debiasing Word embeddings can encode gender biases (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018) and this can affect bias in downstream predictions for models using the embeddings (Zhao et al., 2018; Font and Costa-Jussa, 2019). In this work, we apply the Hard-Debiasing technique (Bolukbasi et al., 2016). We applied Hard-Debiasing to Word2Vec embeddings (Mikolov et al., 2013), which we trained on the sentences in WikiGenderBias. When used in conjunction with data augmentation, the embeddings are re-trained on the union of the two corpora. Below, we give me"
2020.acl-main.265,D17-1323,1,0.82348,"thDate and birthPlace, as they are not intuitively related to gender. Experiment results confirm our conjecture. Our contributions are as such: • We create WikiGenderBias, a new dataset for evaluating gender bias in NRE systems. • We present an evaluation framework to demonstrate that gender bias is exhibited in NRE model outputs. • We test several existing bias mitigation approaches to reducing gender bias in NRE system. Our analysis sheds light for designing future mitigating techniques. 2 Related Work Gender Bias Measurement. Existing studies have revealed gender bias in various NLP tasks (Zhao et al., 2017; Rudinger et al., 2018; Zhao et al., 2018; Dixon et al., 2018; Lu et al., 2018; Kiritchenko and Mohammad, 2018; Romanov et al., 2019; Sheng et al., 2019; Sun et al., 2019). People have proposed different metrics to evaluate gender bias, for example, by using the performance difference of the model on male and female datapoints for bias evaluation (Lu et al., 2018; Kiritchenko and Mohammad, 2018). Other metrics have been proposed to evaluate fairness of predictors and allocative bias (Dwork et al., 2012; Hardt et al., 2016), such as Equality of Opportunity. In this work, we use both of these m"
2020.acl-main.265,N18-2003,1,0.87482,"pation; engineer) relation. However, the model only predicts that the sentence from the male article expresses the occupation relation. If on a large scale, models extract the (entity; occupation; engineer) relation more often for males, knowledge bases will contain information for male engineers more often than female. Question answering models that query these knowledge bases may give biased answers and propagate gender bias downstream. exhibited in a model that is trained on a relation extraction dataset; and (2) examining if the existing bias mitigation techniques (Bolukbasi et al., 2016; Zhao et al., 2018; Lu et al., 2018) can be applied to reduce the bias in an NRE system while maintaining its performance. Carrying out such an evaluation is difficult with existing NRE datasets, such as the NYT dataset (Sandhaus, 2018), because there is no reliable way to obtain gender information about the entities mentioned in input sentences. Therefore, we create a new dataset, WikiGenderBias, specifically aimed at evaluating gender bias for NRE. WikiGenderBias is a distantly supervised dataset extracted using Wikipedia and DBPedia. It contains 45,000 sentences, each of which describe either a male or femal"
2020.acl-main.265,P05-1052,0,0.145408,"apoints as male (female) if that is the gender of the subject of the article. The left two entries are for the dataset taken from the true distribution; the right two are the gender-equalized dataset created by down-sampling male instances. Neural Relation Extraction. Relation extraction is a task in NLP with a long history that typically seeks to extract structured tuples (e1 , r, e2 ) from texts (Bach and Badaskar, 2007). Early on, learning algorithms for relation extraction models were typically categorized as supervised, including feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 2005) and kernelbased methods (Lodhi et al., 2002; Zelenko et al., 2003), or semi-supervised (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006), or purely unsupervised (Etzioni et al., 2008). Supervised approaches suffer from the need for large amounts of labelled data, which is sometimes not feasible, and generalizes poorly to open domain relation extraction, since labeled data is required for every entity-relation type (Bach and Badaskar, 2007; Mintz et al., 2009). Many semi-supervised approaches rely on patternmatching, which is not robust, and many a"
2020.acl-main.265,P05-1053,0,0.18774,"e and we define datapoints as male (female) if that is the gender of the subject of the article. The left two entries are for the dataset taken from the true distribution; the right two are the gender-equalized dataset created by down-sampling male instances. Neural Relation Extraction. Relation extraction is a task in NLP with a long history that typically seeks to extract structured tuples (e1 , r, e2 ) from texts (Bach and Badaskar, 2007). Early on, learning algorithms for relation extraction models were typically categorized as supervised, including feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 2005) and kernelbased methods (Lodhi et al., 2002; Zelenko et al., 2003), or semi-supervised (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006), or purely unsupervised (Etzioni et al., 2008). Supervised approaches suffer from the need for large amounts of labelled data, which is sometimes not feasible, and generalizes poorly to open domain relation extraction, since labeled data is required for every entity-relation type (Bach and Badaskar, 2007; Mintz et al., 2009). Many semi-supervised approaches rely on patternmatching, which"
2020.acl-main.265,D17-1187,0,0.0193625,"of the form (head, relation, tail) e.g., (Barack Obama, spouse, Michelle Obama). The concise representations provided by relation extraction models have been used to extend Knowledge Bases (KBs) (Riedel et al., 2013; Subasic et al., 2019; Trisedya et al., 2019). These KBs are then used heavily in NLP systems, such as question answering systems (Bordes et al., 2014; Yin et al., 2016; Cui et al., 2019). In recent years, much focus in the Neural Relation Extraction (NRE) community has been centered on improvements in model precision and the reduction of noise (Lin et al., 2016; Liu et al., 2017; Wu et al., 2017; Feng et al., 2018; Vashishth et al., 2018; Qin et al., 2018). Yet, little attention has been devoted towards the fairness of such systems. We take the first step at understanding and evaluating gender bias in NRE systems by measuring the differences in model performance when extracting relations from sentences written about females versus sentences written about males. If a NRE model predicts a relation such occupation with higher recall on male entities, this could lead to the resulted knowledge bases having more occupation information for males than for females (see the illustration in Fig"
2020.acl-main.316,D18-1354,0,0.0179272,". Later work on this problem focuses on less powerful decoders (Yang et al., 2017; Semeniuta et al., 2017), modified regularization objective (Higgins et al., 2017; Bahuleyan et al., 2019; Wang and Wang, 2019), alternative posterior families (Rezende and Mohamed, 2015; Xu and Durrett, 2018; Davidson et al., 2018; Xiao et al., 2018), richer prior distributions (Tomczak and Welling, 2018), improved optimization (He et al., 2019) or KL annealing strategy (Fu et al., 2019), the use of skip connections (Dieng et al., 2019), hierarchical or autoregressive posterior distributions (Park et al., 2018; Du et al., 2018), and narrowing the amortization gap (Hjelm et al., 2016; Kim et al., 2018; Marino et al., 2018). We provide the encoderdecoder incompatibility as a new perspective on the posterior collapse problem. Empirically, our approach can be combined with the above ones to alleviate the problem further. 3456 Coupled-VAE (λm = 10.0) VAE Text A (sampled from PTB): now those routes are n’t expected to begin until jan they are n’t expected to be completed the new york stock exchange is scheduled to resume today the new york stock exchange is scheduled to resume it is n’t clear that it will be sold through"
2020.acl-main.316,D17-1066,0,0.0706381,"epresentations from massive text data, there has been much interest in using VAE for text modeling (Zhao et al., 2017; Xu and Durrett, 2018; He et al., 2019). Prior work has observed that the optimization of VAE suffers from the posterior collapse problem, i.e., the posterior becomes nearly identical to the prior and the decoder degenerate into a standard language model (Bowman et al., 2016; Zhao et al., 2017). A widely mentioned explanation is that a strong decoder makes the collapsed posterior a good local optimum of ELBO, and existing solutions include weakened decoders (Yang et al., 2017; Semeniuta et al., 2017), modified regularization terms (Higgins et al., 2017; Wang and Wang, 2019), alternative posterior families (Rezende and Mohamed, 2015; Davidson et al., 2018), richer prior distributions (Tomczak and Welling, 2018), improved optimization strategies (He et al., 2019), and narrowed amortization gaps (Kim et al., 2018). In this paper, we provide a novel perspective for the posterior collapse problem. By comparing the optimization dynamics of VAE with deterministic autoencoders (DAE), we observe the incompatibility between a poorly optimized encoder and a decoder with too strong expressiveness. Fr"
2020.acl-main.316,J93-2004,0,\N,Missing
2020.acl-main.316,D16-1172,0,\N,Missing
2020.acl-main.316,N19-1021,0,\N,Missing
2020.acl-main.316,N19-1411,0,\N,Missing
2020.acl-main.316,N16-1014,0,\N,Missing
2020.acl-main.316,K16-1002,0,\N,Missing
2020.acl-main.316,N18-1162,0,\N,Missing
2020.acl-main.316,N19-1025,1,\N,Missing
2020.acl-main.316,D19-1370,0,\N,Missing
2020.acl-main.708,N19-1423,0,0.507173,"fidelity issue of natural language generation (NLG) by encouraging the model to learn to reuse the verbatim of certain inputs through copy mechanism (See et al., 2017; Gu et al., 2016; Wiseman et al., 2017; Liu et al., 2018), structured attention (Liu et al., 2018), or planning and selection/entity modeling (Puduppully et al., 2019a,b). While shown to be effective, most such methods so far are primarily focused on surfacelevel realization and simply restate the facts in the underlying data (Figure 1). Introduction Neural network models, especially the recent wave of massive models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), have shown the ability to generate natural language text at an astonishing level of fluency and coherence. For the generated text to fulfill its purpose, however, a critHowever, humans have the ability to generalize beyond superficial facts (e.g., “Canada has got 3 gold medals.”) by inferring and communicating with new statements that can be entailed from these facts (e.g., “Canada obtained the most gold medals.”). We believe it is important for NLG models to be able to generalize beyond the superficla facts given to them as well. Therefore, we propose a new"
2020.acl-main.708,P19-1483,0,0.414126,"e detailed description of logical inference is listed in the Appendix. 2) while existing datasets are often restricted to a specific domain such as weather (Liang et al., 2009), restaurant (Duˇsek et al., 2019), NBA (Wiseman et al., 2017), etc, L OGIC NLG uses open-domain tables without prior knowledge about their schema. As such, existing methods based on surface-level copying (See et al., 2017; Gu et al., 2016; Puduppully et al., 2019a) becomes insufficient, so are the existing fidelity evaluation based on the surfacelevel information extraction (Wiseman et al., 2017; Rohrbach et al., 2018; Dhingra et al., 2019), which extracts surface triples in a certain pre-defined form (i.e. subj-pred-obj, n-gram) and compare them with the surface content given in the knowledge. Most neural generation models follow a monotonic generation schema from left to right with the current prediction only depending on the preceding words. Logical NLG poses unique challenges to the traditional generation scheme due to the mismatch between sequence order and logical order. As illustrated in Figure 2, the word “2” is derived from the logical inference of ‘diff(Silver medal of Colombia, Silver medal of Canada)) → 2.’ In other"
2020.acl-main.708,P18-1068,0,0.0223697,"[ E [r(y1:n , T)]] log p(yi |y1:i−1 ; β) yi ∼p(yi |y1:i−1 ) yi+1:n where we only use one trajectory to approximate the inner roll-out expectation for efficiency. 5 Coarse-to-Fine Generation As discussed before, the baseline models follow the monotonic generation scheme and suffer from the mismatch between sequence order and logical order (Figure 2). In this section, we propose an imperfect remedy for such a situation based on the coarse-to-fine generation paradigm. Before plunging into technical details, it is helpful to first realize the resemblance between logical NLG and semantic parsing (Dong and Lapata, 2018). Compared to traditional NLG tasks like machine translation and summarization, logical NLG is closer to semantic parsing in the sense that a model may make catastrophic errors that are impossible to be corrected at later steps (Figure 2). Therefore, we take inspiration from semantic parsing models (Dong and Lapata, 2018) that have proven effective in mitigating such errors and propose a coarse-to-fine generation scheme. We break down generation into two phases. In the first phase, 7934 ?! GPT-2 [ENT] obtained [ENT] more [ENT] than [ENT]. Canada obtained 1 more gold medal than Mexico. Figure 7"
2020.acl-main.708,D18-1324,0,0.165167,"er of word “2” should be after “more”, “silver”, and “Canada”, while the sequence order of “2” is before those words. Since the monotonic generation scheme is purely based on sequence order while agnostic to logical order, existing NLG models struggle to maintain the fidelity as they cannot model the logical dependency on future tokens. To alleviate such an order mismatch, an NLG model must have the capability to plan ahead for the next few steps before generation. In this context, we believe L OGIC NLG to be an important testbed to study such a planing/inference ability in generation models (Ford et al., 2018; Welleck et al., 2019). In this paper, we further propose a non-monotonic coarse-to-fine generation model and show that it is able to alleviate the order mismatch problem and achieve better performance. The contribution of this work is three-fold: i) We propose a new research problem of logical natural language generation, and provide novel metrics to approximately evaluate the logical fidelity of generation models. ii) We justify the mismatch problem between sequence order and logical order of the traditional monotonic generation scheme in logical NLG. iii) We conduct comprehensive experimen"
2020.acl-main.708,Q19-1042,0,0.0223052,"ve performance on the existing datasets (Chen and Mooney, 2008; Liang et al., 2009; Lebret et al., 2016; Duˇsek et al., 2019; Wiseman et al., 2017) since the annotated text are mostly surface-level annotation without logical inference. Unlike them, L OGIC NLG has rich inference, which poses great challenges to existing models and evaluations. Non-monotonic Generation There have been attempts recently to study the problem of nonmonotonic text generation, which aims to teach the generation model to learn the generation order without external supervision (Ford et al., 2018; Welleck et al., 2019; Gu et al., 2019; Mansimov et al., 2019). These models have shown to learn rational generation order to approach similar performance as the left-to-right case. These approaches are useful at capturing more sophisticated dependency within the sentence, which provides a plausible direction to pursue in L OGIC NLG. Factualness Evaluation Fidelity is an important research topic in generation, In ROTOWIRE (Wiseman et al., 2017) and MSCOCO (Lin et al., 2014), IE-based extractive evaluation (Rohrbach et al., 2018; Dhingra et al., 2019) are adopted for surfacelevel matching to replace costly human evaluation. In abst"
2020.acl-main.708,P16-1154,0,0.287135,"quires a generation model to generate natural language statements that can be logically entailed by the facts in the table instead of simply restating certain superficial facts in natural language. ical property that is necessary but often overlooked is fidelity, i.e., what is generated should be faithful to the underlying data, knowledge, or meaning representation. A line of recent work has started to address the surface-level fidelity issue of natural language generation (NLG) by encouraging the model to learn to reuse the verbatim of certain inputs through copy mechanism (See et al., 2017; Gu et al., 2016; Wiseman et al., 2017; Liu et al., 2018), structured attention (Liu et al., 2018), or planning and selection/entity modeling (Puduppully et al., 2019a,b). While shown to be effective, most such methods so far are primarily focused on surfacelevel realization and simply restate the facts in the underlying data (Figure 1). Introduction Neural network models, especially the recent wave of massive models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), have shown the ability to generate natural language text at an astonishing level of fluency and coherence. For the generated text"
2020.acl-main.708,P83-1022,0,0.31488,"Missing"
2020.acl-main.708,D16-1128,0,0.393294,"Missing"
2020.acl-main.708,P09-1011,0,0.262247,"ations over the given table (Pasupat and Liang, 2015). To empower research in this direction, we collect a new corpus L OGIC NLG based on the existing TabFact (Chen et al., 2019), which brings two major renovations to the existing NLG paradigm: 1) the text involves diversified types of logical inferences including math operations like max/min/sum/add, comparison operations like same/different, and counting operations like total/only. A more detailed description of logical inference is listed in the Appendix. 2) while existing datasets are often restricted to a specific domain such as weather (Liang et al., 2009), restaurant (Duˇsek et al., 2019), NBA (Wiseman et al., 2017), etc, L OGIC NLG uses open-domain tables without prior knowledge about their schema. As such, existing methods based on surface-level copying (See et al., 2017; Gu et al., 2016; Puduppully et al., 2019a) becomes insufficient, so are the existing fidelity evaluation based on the surfacelevel information extraction (Wiseman et al., 2017; Rohrbach et al., 2018; Dhingra et al., 2019), which extracts surface triples in a certain pre-defined form (i.e. subj-pred-obj, n-gram) and compare them with the surface content given in the knowledg"
2020.acl-main.708,J13-2005,0,0.050877,"Missing"
2020.acl-main.708,N19-1117,0,0.0160624,"s used to control whether the current cell is already encoded. Such a mechanism can help LSTM to identify the boundary between different cells to grasp local information. 4.2 Pre-trained Models To further enhance the fluency and resolve the out-of-vocabulary problem, we use pre-trained language models and finetune them on L OGIC NLG. Specifically, we consider two models based on GPT-2 (Radford et al., 2019) and BERT (Devlin et al., 2019), respectively, and name them as GPTTableGen and BERT-TableGen. Table Linearization We follow previous work on linearizing knowledge base as natural language (Liu et al., 2019; Zhang et al., 2019) to propose “table linearization”, which uses template to flatten the table T as a document PT = w1 , · · · , w|T |fed into pre-trained language models to generate statement Y , where we use wi to denote the i-th word in the generated paragraph PT and |T |to denote the length of the paragraph (the word wi is either a table entry or a functional word in the template). As depicted in the left bottom part of Figure 6, the original table T is transformed into a paragraph by horizontally scanning each cell T11 → T1,CT → TRT ,CT in the table. GPT-TabGen we directly feed the para"
2020.acl-main.708,P02-1040,0,0.108571,"4: The domain distribution of L OGIC NLG. the Tij being the content in the (i, j)-th cell. Tij could be a word, a number, a phrase or even a natural language sentence. The annotated statement is a sentence Y = y1 , y2 , · · · , yn , we aim to train a neural generation model p(Y |T) to generate statement Yˆ which are both fluent and logically (numerically) supported by the given table T. 3 Automatic Evaluation In this section, we discuss the evaluation of our proposed NLG task. The fluency evaluation is simply based on the standard metrics like Perplexity (Bengio et al., 2003) and BLEU-1,2,3 (Papineni et al., 2002) based on NLTK (Bird, 2006). The most challenging problem is to evaluate the logical fidelity of the generated sentences, which is also the core problem of our paper. The existing IE-based extractive evaluation (Wiseman et al., 2017) leads to two issues as shown in Figure 3: 1) Empty Extraction: the sentence can not be formulated as (subject, predicate, object) structure, thus the IE system fail to extract triples for verification. 2) False Negative: the sentence is a logical composition (instead of surface form) of the fact from the table, the IE system cannot match it against the table. For"
2020.acl-main.708,P15-1142,0,0.0481374,"y. There is no back-tracking once the model makes a wrong decision like “5”. with generating natural language statements that can be logically entailed by the given data (i.e., the premises). The new task requires a model to jointly reason and generate sentences that are consistent both linguistically and logically. Since there are a variety of reasoning/inference tasks such as natural language inference (Bowman et al., 2015) and commonsense reasoning (Talmor et al., 2019), to avoid confusion, this paper is specifically focused on inferences involving symbolic operations over the given table (Pasupat and Liang, 2015). To empower research in this direction, we collect a new corpus L OGIC NLG based on the existing TabFact (Chen et al., 2019), which brings two major renovations to the existing NLG paradigm: 1) the text involves diversified types of logical inferences including math operations like max/min/sum/add, comparison operations like same/different, and counting operations like total/only. A more detailed description of logical inference is listed in the Appendix. 2) while existing datasets are often restricted to a specific domain such as weather (Liang et al., 2009), restaurant (Duˇsek et al., 2019)"
2020.acl-main.708,P19-1195,0,0.485723,"estating certain superficial facts in natural language. ical property that is necessary but often overlooked is fidelity, i.e., what is generated should be faithful to the underlying data, knowledge, or meaning representation. A line of recent work has started to address the surface-level fidelity issue of natural language generation (NLG) by encouraging the model to learn to reuse the verbatim of certain inputs through copy mechanism (See et al., 2017; Gu et al., 2016; Wiseman et al., 2017; Liu et al., 2018), structured attention (Liu et al., 2018), or planning and selection/entity modeling (Puduppully et al., 2019a,b). While shown to be effective, most such methods so far are primarily focused on surfacelevel realization and simply restate the facts in the underlying data (Figure 1). Introduction Neural network models, especially the recent wave of massive models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), have shown the ability to generate natural language text at an astonishing level of fluency and coherence. For the generated text to fulfill its purpose, however, a critHowever, humans have the ability to generalize beyond superficial facts (e.g., “Canada has got 3 gold medals.”"
2020.acl-main.708,P17-1099,0,0.484149,"ce. Logical NLG requires a generation model to generate natural language statements that can be logically entailed by the facts in the table instead of simply restating certain superficial facts in natural language. ical property that is necessary but often overlooked is fidelity, i.e., what is generated should be faithful to the underlying data, knowledge, or meaning representation. A line of recent work has started to address the surface-level fidelity issue of natural language generation (NLG) by encouraging the model to learn to reuse the verbatim of certain inputs through copy mechanism (See et al., 2017; Gu et al., 2016; Wiseman et al., 2017; Liu et al., 2018), structured attention (Liu et al., 2018), or planning and selection/entity modeling (Puduppully et al., 2019a,b). While shown to be effective, most such methods so far are primarily focused on surfacelevel realization and simply restate the facts in the underlying data (Figure 1). Introduction Neural network models, especially the recent wave of massive models like BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019), have shown the ability to generate natural language text at an astonishing level of fluency and coherence. For t"
2020.acl-main.708,N19-1421,0,0.0203242,"Total] [Wrong ] Figure 2: When making the decision at the third step, the model needs to foresee the future tokens to ensure logical consistency. There is no back-tracking once the model makes a wrong decision like “5”. with generating natural language statements that can be logically entailed by the given data (i.e., the premises). The new task requires a model to jointly reason and generate sentences that are consistent both linguistically and logically. Since there are a variety of reasoning/inference tasks such as natural language inference (Bowman et al., 2015) and commonsense reasoning (Talmor et al., 2019), to avoid confusion, this paper is specifically focused on inferences involving symbolic operations over the given table (Pasupat and Liang, 2015). To empower research in this direction, we collect a new corpus L OGIC NLG based on the existing TabFact (Chen et al., 2019), which brings two major renovations to the existing NLG paradigm: 1) the text involves diversified types of logical inferences including math operations like max/min/sum/add, comparison operations like same/different, and counting operations like total/only. A more detailed description of logical inference is listed in the Ap"
2020.acl-main.708,W19-3620,0,0.110713,"ld be after “more”, “silver”, and “Canada”, while the sequence order of “2” is before those words. Since the monotonic generation scheme is purely based on sequence order while agnostic to logical order, existing NLG models struggle to maintain the fidelity as they cannot model the logical dependency on future tokens. To alleviate such an order mismatch, an NLG model must have the capability to plan ahead for the next few steps before generation. In this context, we believe L OGIC NLG to be an important testbed to study such a planing/inference ability in generation models (Ford et al., 2018; Welleck et al., 2019). In this paper, we further propose a non-monotonic coarse-to-fine generation model and show that it is able to alleviate the order mismatch problem and achieve better performance. The contribution of this work is three-fold: i) We propose a new research problem of logical natural language generation, and provide novel metrics to approximately evaluate the logical fidelity of generation models. ii) We justify the mismatch problem between sequence order and logical order of the traditional monotonic generation scheme in logical NLG. iii) We conduct comprehensive experiments with state-of-the-ar"
2020.acl-main.708,D15-1199,0,0.147515,"Missing"
2020.acl-main.708,D18-1356,0,0.058826,"rations like “only, unique”. 4) the model is not able to perform mathematical aggregation like average, sum, etc. Overall, the string-based operations are easier than numericbased operations, how to infuse the numeric knowledge is an open research question to move forward. 7 Related Work Natural Language Generation Natural language generation is a long-standing problem (Kukich, 1983; Holmes-Higgin, 1994; Reiter and Dale, 1997), which involves generating text from records or data. Recently, many neural-based generation models have been proposed (Puduppully et al., 2019a,b; Lebret et al., 2016; Wiseman et al., 2018) to achieve impressive performance on the existing datasets (Chen and Mooney, 2008; Liang et al., 2009; Lebret et al., 2016; Duˇsek et al., 2019; Wiseman et al., 2017) since the annotated text are mostly surface-level annotation without logical inference. Unlike them, L OGIC NLG has rich inference, which poses great challenges to existing models and evaluations. Non-monotonic Generation There have been attempts recently to study the problem of nonmonotonic text generation, which aims to teach the generation model to learn the generation order without external supervision (Ford et al., 2018; We"
2020.acl-main.708,D18-1437,0,0.276255,"like total/only. A more detailed description of logical inference is listed in the Appendix. 2) while existing datasets are often restricted to a specific domain such as weather (Liang et al., 2009), restaurant (Duˇsek et al., 2019), NBA (Wiseman et al., 2017), etc, L OGIC NLG uses open-domain tables without prior knowledge about their schema. As such, existing methods based on surface-level copying (See et al., 2017; Gu et al., 2016; Puduppully et al., 2019a) becomes insufficient, so are the existing fidelity evaluation based on the surfacelevel information extraction (Wiseman et al., 2017; Rohrbach et al., 2018; Dhingra et al., 2019), which extracts surface triples in a certain pre-defined form (i.e. subj-pred-obj, n-gram) and compare them with the surface content given in the knowledge. Most neural generation models follow a monotonic generation schema from left to right with the current prediction only depending on the preceding words. Logical NLG poses unique challenges to the traditional generation scheme due to the mismatch between sequence order and logical order. As illustrated in Figure 2, the word “2” is derived from the logical inference of ‘diff(Silver medal of Colombia, Silver medal of C"
2020.acl-main.708,D15-1075,0,\N,Missing
2020.acl-main.708,P06-4018,0,\N,Missing
2020.acl-main.708,P11-1060,0,\N,Missing
2020.acl-main.708,D17-1239,0,\N,Missing
2020.emnlp-main.276,K16-1002,0,0.0181711,"can create new responses, but the vanilla sequence to sequence model tends to produce generic responses (Li et al., 2016). One way to address the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et al., 2019; Song et al., 2019), and retrieved candidate responses (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a). Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder (Bowman et al., 2016; Zhao et al., 2017) and the generative adversarial network (Goodfellow et al., 2014; Li et al., 2017a; Zhang et al., 2018; Xu et al., 2018; Tuan and Lee, 2019). Our approach falls into the latter category. The differences between our approach and other adversarial learning-based approaches are as follows. First, we cast the dialogue generation model as an SCM to explore potential responses in the environment where observed responses occur. Second, we learn on counterfactual responses that inferred from the SCM. Third, a pre-trained behavior policy is involved during the generation process, ma"
2020.emnlp-main.276,W19-5944,0,0.021886,"Missing"
2020.emnlp-main.276,P18-1139,0,0.0237976,"Missing"
2020.emnlp-main.276,P17-4012,0,0.0285167,"lready been divided into training, validation, and test sets, as shown in Table 1. Given a dialogue that consists of K utterances, we divide it into K-1 instances. Each instance has at most three continuous utterances. The last utterance is the response, and the previous utterances are concatenated as the dialogue history. 4.2 Time (s/epoch) on counterfactual responses. It is model-agnostic and can be applied to any adversarial learningbased dialogue generation model, such as REGS, DPGAN, and StepGAN. 4.3 Training Details We implement REGS, StepGAN, and their variants with COPT using OpenNMT (Klein et al., 2017), an open-source framework for building sequence to sequence models. We manually tune the parameters according to the perplexity on the validation set. The vocabulary consists of the most frequent 10,000 words. Including more words (up to 17,438, the total number of DailyDialog vocabulary) observes no improvement but takes more time for training. We use 300 dimensional GloVe (Pennington et al., 2014) vectors to initialize word embeddings. Both the encoder and the decoder are a two-layer LSTM in G and a single layer LSTM in D. The number of hidden units is 500. During the adversarial learning p"
2020.emnlp-main.276,W06-1303,0,0.0144713,"butions of this paper are summarized as follows: • We connect the concept of counterfactual reasoning with the dialogue generation by casting the dialogue generation model as a structural causal model. • Our counterfactual response is of higher quality than the response synthesized from scratch in standard adversarial learning-based dialogue generation model. • Our approach is model-agnostic and can be applied to any adversarial learning-based dialogue generation model. 2 Related Work Dialogue Generation Data-driven dialogue systems can be roughly divided into two categories: retrieval-based (Leuski et al., 2006; Ji et al., 2014; Yan et al., 2016) and generation-based (Shang et al., 2015b; Sordoni et al., 2015b; Vinyals and Le, 2015). Responses of retrieval-based methods come from a fixed candidate response set and thus are incapable of being customized. The generation-based methods can create new responses, but the vanilla sequence to sequence model tends to produce generic responses (Li et al., 2016). One way to address the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et"
2020.emnlp-main.276,N16-1014,0,0.153822,"s the conventional adversarial learning approaches. 1 Dialogue History: What are you up to this Friday? Counterfactual Response: I am going to gym with a friend. Figure 1: An example of a counterfactual response, which is a potential response inferred in hindsight from given observed response. Introduction Open-domain dialogue generation (Shang et al., 2015a; Vinyals and Le, 2015; Sordoni et al., 2015a) intends to produce coherent responses given dialogue history. Nevertheless, it suffers from data insufficiency problem as there may exist many potential responses for a given dialogue history (Li et al., 2016). An ideal way of exploring the potential responses is to train the model by chatting with real users, which is usually time-consuming and labor-intensive in practice. Although replacing a real user with a user simulator could address the issue, the simulator only roughly approximates real user statistics, and its development process is costly (Su et al., 2016). In contrast, humans could independently reason potential responses based on past experiences from the true environment. Having observed a response, one might naturally ask himself or herself: “What ∗ Corresponding author. what if I res"
2020.emnlp-main.276,D17-1230,0,0.299845,"m. After generating the counterfactual response, the generator will receive a reward from a discriminator and optimize itself accordingly. Intuitively, a counterfactual response is synthesized by grounding the model in the scenario where an observed response occurs, rather than the scenario sampled from scratch as standard adversarial learning-based approaches. This improves the quality of the synthesized responses and subsequently benefits the model that learns from the synthesis. To verify the effectiveness of our approach, we conduct experiments on the public available DailyDialog dataset (Li et al., 2017b). Experimental results show that our approach significantly outperforms previous adversarial learning-based approaches in both automatic and human evaluations. The contributions of this paper are summarized as follows: • We connect the concept of counterfactual reasoning with the dialogue generation by casting the dialogue generation model as a structural causal model. • Our counterfactual response is of higher quality than the response synthesized from scratch in standard adversarial learning-based dialogue generation model. • Our approach is model-agnostic and can be applied to any adversa"
2020.emnlp-main.276,I17-1099,0,0.0726707,"m. After generating the counterfactual response, the generator will receive a reward from a discriminator and optimize itself accordingly. Intuitively, a counterfactual response is synthesized by grounding the model in the scenario where an observed response occurs, rather than the scenario sampled from scratch as standard adversarial learning-based approaches. This improves the quality of the synthesized responses and subsequently benefits the model that learns from the synthesis. To verify the effectiveness of our approach, we conduct experiments on the public available DailyDialog dataset (Li et al., 2017b). Experimental results show that our approach significantly outperforms previous adversarial learning-based approaches in both automatic and human evaluations. The contributions of this paper are summarized as follows: • We connect the concept of counterfactual reasoning with the dialogue generation by casting the dialogue generation model as a structural causal model. • Our counterfactual response is of higher quality than the response synthesized from scratch in standard adversarial learning-based dialogue generation model. • Our approach is model-agnostic and can be applied to any adversa"
2020.emnlp-main.276,D16-1230,0,0.0757596,"Missing"
2020.emnlp-main.276,D15-1166,0,0.036369,"ining set as Y and a modelgenerated response as Yˆ . 3.3 Counterfactual Off-Policy Training Our COPT approach is model-agnostic and can be applied to any adversarial learning-based dialogue generation model. Without loss of generality, we take the combination of COPT and the reward for every generation step (REGS) model (Li et al., 2017a) as an example in this section. It consists of two main components: a generator G and a discriminator D. Generator The generator G is a sequence to sequence (Seq2Seq) model (Sutskever et al., 2014) equipped with the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). During the encoding process, G reads the dialogue history into hidden states using an encoder LSTM (Hochreiter and Schmidhuber, 1997): Hi = LSTM(Xi , Hi−1 ), (1) where Xi is the i-th word of the dialogue history, and Hi denotes the corresponding hidden state. 3440 Observed Response: Well, I have the day off from work. Counterfactual Response: I’m going to the gym with a friend. Inferred Scenario Y U SCM X Y=fµ(X, U) u fµ fπ U Y X SCM Y= fπ(X, U) Intervention Discriminator Reward Policy Gradient Message: Hey. What are you up to this Friday? Figure 3: The architecture of our COPT approach. π i"
2020.emnlp-main.276,C16-1316,0,0.0175302,"ialogue systems can be roughly divided into two categories: retrieval-based (Leuski et al., 2006; Ji et al., 2014; Yan et al., 2016) and generation-based (Shang et al., 2015b; Sordoni et al., 2015b; Vinyals and Le, 2015). Responses of retrieval-based methods come from a fixed candidate response set and thus are incapable of being customized. The generation-based methods can create new responses, but the vanilla sequence to sequence model tends to produce generic responses (Li et al., 2016). One way to address the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et al., 2019; Song et al., 2019), and retrieved candidate responses (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a). Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder (Bowman et al., 2016; Zhao et al., 2017) and the generative adversarial network (Goodfellow et al., 2014; Li et al., 2017a; Zhang et al., 2018; Xu et al., 2018; Tuan and Lee, 2019). Our approach falls into the latter category. The differences be"
2020.emnlp-main.276,P02-1040,0,0.106054,"Missing"
2020.emnlp-main.276,N18-1162,0,0.023827,"Missing"
2020.emnlp-main.276,D14-1162,0,0.0835341,"can be applied to any adversarial learningbased dialogue generation model, such as REGS, DPGAN, and StepGAN. 4.3 Training Details We implement REGS, StepGAN, and their variants with COPT using OpenNMT (Klein et al., 2017), an open-source framework for building sequence to sequence models. We manually tune the parameters according to the perplexity on the validation set. The vocabulary consists of the most frequent 10,000 words. Including more words (up to 17,438, the total number of DailyDialog vocabulary) observes no improvement but takes more time for training. We use 300 dimensional GloVe (Pennington et al., 2014) vectors to initialize word embeddings. Both the encoder and the decoder are a two-layer LSTM in G and a single layer LSTM in D. The number of hidden units is 500. During the adversarial learning process, we use the ADAM algorithm to alternately optimize G and D for one batch and five batches. The batch size is 64. We have tested the learning rate from 1e-6 to 1e-3. REGS+COPT and StepGAN+COPT achieve the best performance on 1e-5. The number of parameters for all the baselines is in a range of 21M to 26M. Equipping an adversarial learning baseline with COPT will introduce extra parameters with"
2020.emnlp-main.276,D19-1509,0,0.0376887,"Missing"
2020.emnlp-main.276,P15-1152,0,0.0444287,"Missing"
2020.emnlp-main.276,N15-1020,0,0.0640256,"Missing"
2020.emnlp-main.276,P16-1230,0,0.0489445,"Missing"
2020.emnlp-main.276,D18-1428,0,0.190331,"the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et al., 2019; Song et al., 2019), and retrieved candidate responses (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a). Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder (Bowman et al., 2016; Zhao et al., 2017) and the generative adversarial network (Goodfellow et al., 2014; Li et al., 2017a; Zhang et al., 2018; Xu et al., 2018; Tuan and Lee, 2019). Our approach falls into the latter category. The differences between our approach and other adversarial learning-based approaches are as follows. First, we cast the dialogue generation model as an SCM to explore potential responses in the environment where observed responses occur. Second, we learn on counterfactual responses that inferred from the SCM. Third, a pre-trained behavior policy is involved during the generation process, making our approach an off-policy algorithm and benefits the exploration of potential responses. Counterfactual Reasoning The counterfactual"
2020.emnlp-main.276,P17-1061,0,0.0173267,"nses, but the vanilla sequence to sequence model tends to produce generic responses (Li et al., 2016). One way to address the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et al., 2019; Song et al., 2019), and retrieved candidate responses (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a). Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder (Bowman et al., 2016; Zhao et al., 2017) and the generative adversarial network (Goodfellow et al., 2014; Li et al., 2017a; Zhang et al., 2018; Xu et al., 2018; Tuan and Lee, 2019). Our approach falls into the latter category. The differences between our approach and other adversarial learning-based approaches are as follows. First, we cast the dialogue generation model as an SCM to explore potential responses in the environment where observed responses occur. Second, we learn on counterfactual responses that inferred from the SCM. Third, a pre-trained behavior policy is involved during the generation process, making our approach an"
2020.emnlp-main.276,P19-1366,1,0.837886,"n be roughly divided into two categories: retrieval-based (Leuski et al., 2006; Ji et al., 2014; Yan et al., 2016) and generation-based (Shang et al., 2015b; Sordoni et al., 2015b; Vinyals and Le, 2015). Responses of retrieval-based methods come from a fixed candidate response set and thus are incapable of being customized. The generation-based methods can create new responses, but the vanilla sequence to sequence model tends to produce generic responses (Li et al., 2016). One way to address the generic response problem is by introducing external knowledge, such as keywords (Mou et al., 2016; Zhu et al., 2019b), topics (Xing et al., 2017), persona information (Zhang et al., 2019; Song et al., 2019), and retrieved candidate responses (Song et al., 2018; Wu et al., 2019; Zhu et al., 2019a). Another way is to optimize the architecture of networks. There are two architectures widely employed in this research line: the variational auto-encoder (Bowman et al., 2016; Zhao et al., 2017) and the generative adversarial network (Goodfellow et al., 2014; Li et al., 2017a; Zhang et al., 2018; Xu et al., 2018; Tuan and Lee, 2019). Our approach falls into the latter category. The differences between our approach"
2020.emnlp-main.357,P04-3031,0,0.160682,"upervisedly. Similar to the iterative editor, we also apply teacher-forcing training (feeding in Ot−1 and ht−1 ) to further update G. In this way, G can improve the generalizability by considering the counterfactual U 0 , which is more diverse than U . 4 where wˆi0 and are the ith word token. By minimizing L0E , the model has an opportunity to access U 0 , which is different from the original wi0 training data. With the help of our iterative explainer, SSCR improves the generalizability by reasoning diverse counterfactual instructions I 0 even if under data scarcity. 3.6 First, we apply NLTK (Bird and Loper, 2004) to parse out tokens in the original I. The types of token on i-CLEVR and CoDraw are shown in Table 1. We then replace these tokens with randomly sampled tokens of the same type to get the counterfactual I 0 . Finally, I 0 combines with the original image G as the intervention data U 0 . Our experiments show that this simple yet effective intervention makes the training data more diverse and deals with data scarcity during our counterfactual reasoning. For each turn t, with It0 from U 0 , we predict the counterfactual resulting image Vt0 : Vt0 = G([ft−1 , h0t ]), 1: while COUNTERFACTUAL REASON"
2020.emnlp-main.357,P19-1651,0,0.126983,"deal with the data scarcity issue. SSCR allows the model to think about expected, resulting images under unseen instructions. Since there are no ground-truth resulting images, we propose cross-task consistency (CTC), which adopts an iterative explainer to reconstruct the instruction of each step. With CTC, we can supply detailed token-level training loss (e.g., wrong objects or wrong positions), which is better than only using the discriminator, and consider these counterfactual instructions in a self-supervised scenario. The experimental results on i-CLEVR (ElNouby et al., 2019) and CoDraw (Kim et al., 2019) show that our SSCR can improve the correctness of the ILBIE task in both aspects of object identity and position. In summary, our contributions are three-fold: • We introduce SSCR that incorporates counterfactual thinking into the ILBIE task to deal with the data scarcity issue. • The proposed cross-task consistency (CTC) and counterfactual reasoning methods help train the generator better, improve the generalizability, and achieve the SOTA results on i-CLEVR and CoDraw. • Extensive ablation studies show that SSCR is effective even with only partial training data. the visual differences betwe"
2020.emnlp-main.357,P19-1161,0,0.0129537,"the data scarcity issue. Counterfactual Thinking (Roese, 1997) is a concept that refers to the human propensity to consider possible alternatives to events that have happened already. People can consider different outcomes from a wide range of conditions and engage in causal reasoning by asking questions like “What if ...?” or “If I had only....” Previous works (Kusner et al., 2017; Garg et al., 2019) have shown how counterfactual fairness improves the robustness of the model and makes it more explainable. Furthermore, counterfactual thinking has also been applied to augment training targets (Zmigrod et al., 2019; Fu et al., 2020). In this paper, we incorporate counterfactual thinking into that ILBIE task that considers counterfactual instructions to deal with the data scarcity issue and improve the generalizability. 2 3 Related Work Text-to-Image (T2I) generates an image that matches the given instruction. T2I is challenging yet important that has a vast potential in practical applications like art generation or automatic design (Nguyen et al., 2017; Reed et al., 2017; Tan et al., 2019). With the success of a generative adversarial network (Goodfellow et al., 2015) on the image generation task, sever"
2020.emnlp-main.357,P02-1040,0,0.114734,"Missing"
2020.emnlp-main.473,P19-3019,0,0.0282255,"are biased with regard to the subject of a sentence when that subject belongs to an underprivileged group, and Shen et al. (2018) tests sentiment analysis tools with intent-controlled pairs with varying stylistic inclinations. Studies regarding AAVE have analyzed tasks such as POS tagging (Jørgensen et al., 2016), detecting AAVE syntax (Stewart, 2014), voice recognition and transcription (Dorn, 2019), dependency parsing (Blodgett et al., 2016), and hate speech detection (Sap et al., 2019), but not language generation. Coupled with concerns that NLG tools can be used for generating fake news (Gehrmann et al., 2019) or impersonating internet users (Zellers et al., 2019), it is important that current work investigates the contexts in which NLG models display bias against certain demographics. In this paper, we examine the bias of GPT-2 text generation against AAVE features. We create a new dataset of AAVE/SAE content-controlled pairs by retrieving AAVE tweets and employing human translators to obtain their SAE counterparts. By doing so, we isolate AAVE syntactic structures and lexical items. We then prompt GPT-2 with the first segments of each AAVE/SAE pair. The generated text is compared to its correspon"
2020.emnlp-main.473,N16-1130,0,0.0480576,"Missing"
2020.emnlp-main.473,E14-3004,0,0.0219045,"ansformer-based language model that generates high-quality, coherent text when prompted by arbitrary input (Radford et al., 2019). However, GPT-2 displays bias ∗ towards particular social groups (Solaiman et al., 2019). Sheng et al. (2019) shows that NLG tools are biased with regard to the subject of a sentence when that subject belongs to an underprivileged group, and Shen et al. (2018) tests sentiment analysis tools with intent-controlled pairs with varying stylistic inclinations. Studies regarding AAVE have analyzed tasks such as POS tagging (Jørgensen et al., 2016), detecting AAVE syntax (Stewart, 2014), voice recognition and transcription (Dorn, 2019), dependency parsing (Blodgett et al., 2016), and hate speech detection (Sap et al., 2019), but not language generation. Coupled with concerns that NLG tools can be used for generating fake news (Gehrmann et al., 2019) or impersonating internet users (Zellers et al., 2019), it is important that current work investigates the contexts in which NLG models display bias against certain demographics. In this paper, we examine the bias of GPT-2 text generation against AAVE features. We create a new dataset of AAVE/SAE content-controlled pairs by retri"
2020.emnlp-main.473,P19-1163,0,0.0501059,"r, GPT-2 displays bias ∗ towards particular social groups (Solaiman et al., 2019). Sheng et al. (2019) shows that NLG tools are biased with regard to the subject of a sentence when that subject belongs to an underprivileged group, and Shen et al. (2018) tests sentiment analysis tools with intent-controlled pairs with varying stylistic inclinations. Studies regarding AAVE have analyzed tasks such as POS tagging (Jørgensen et al., 2016), detecting AAVE syntax (Stewart, 2014), voice recognition and transcription (Dorn, 2019), dependency parsing (Blodgett et al., 2016), and hate speech detection (Sap et al., 2019), but not language generation. Coupled with concerns that NLG tools can be used for generating fake news (Gehrmann et al., 2019) or impersonating internet users (Zellers et al., 2019), it is important that current work investigates the contexts in which NLG models display bias against certain demographics. In this paper, we examine the bias of GPT-2 text generation against AAVE features. We create a new dataset of AAVE/SAE content-controlled pairs by retrieving AAVE tweets and employing human translators to obtain their SAE counterparts. By doing so, we isolate AAVE syntactic structures and le"
2020.emnlp-main.473,D19-1339,0,0.194942,"llions of people from predominately Black communities in the United States and Canada use variants of AAVE on a daily basis. Although AAVE has historically been used in spoken contexts, the growing use of social media has encouraged AAVE in written media for which NLP models are increasingly being used. Recent work in Natural Language Generation (NLG) has introduced GPT-2, a Transformer-based language model that generates high-quality, coherent text when prompted by arbitrary input (Radford et al., 2019). However, GPT-2 displays bias ∗ towards particular social groups (Solaiman et al., 2019). Sheng et al. (2019) shows that NLG tools are biased with regard to the subject of a sentence when that subject belongs to an underprivileged group, and Shen et al. (2018) tests sentiment analysis tools with intent-controlled pairs with varying stylistic inclinations. Studies regarding AAVE have analyzed tasks such as POS tagging (Jørgensen et al., 2016), detecting AAVE syntax (Stewart, 2014), voice recognition and transcription (Dorn, 2019), dependency parsing (Blodgett et al., 2016), and hate speech detection (Sap et al., 2019), but not language generation. Coupled with concerns that NLG tools can be used for g"
2020.emnlp-main.473,D13-1170,0,0.0186095,"Missing"
2020.emnlp-main.697,W05-0909,0,0.124759,"WikiBio Train 34,338 42,061 582,657 Val 4,313 4,672 72,831 Test 4,222 4,693 72,831 Input RDF Triple Dialog Act Table Table 2: Statistics of different data-to-text datasets 8640 2019) as the generator to decode description from a table. For the ablation purposes, we list the performance of all non-pre-trained KGPT to see the performance gain brought by pre-training alone. All the best models are selected based on the validation set score, and the numbers are reported in the following tables are for test split. For evaluation, we report the performance with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004) using e2e-metric6 . It’s worth noting that we perform comprehensive data contamination studies in the following experiments to make sure the pre-training data contains very little overlap with the test split in downstream tasks. We filter out potentially information-leaking pages during the data crawling process. 5.3 Model Seq2Seq† Seq2Seq+Delex† Seq2Seq+Copy† GCN KGPT-Graph w/o Pre KGPT-Seq w/o Pre KGPT-Graph w/ Pre KGPT-Seq w/ Pre Preliminary Study on KGT EXT In the preliminary study, we evaluate our pretrained model’s performance on the held-out set of KGT EXT to co"
2020.emnlp-main.697,D19-1299,0,0.017622,"20b,a) to see the benefit of KGPT over existing pre-trained LM. Model Seq2Seq Seq2Seq+Delex KGPT-Graph w/o Pre KGPT-Seq w/o Pre Template-GPT-2 KGPT-Graph w/ Pre KGPT-Seq w/ Pre Table 5: Experimental results on E2E’s test set. NTemp is from Wiseman et al. (2018), TGen is from Duˇsek and Jurcicek (2016), SLUG2SLUG is from Juraska et al. (2018) and Adapt is from Elder et al. (2018). WikiBio Dataset We list WikiBio’s experimental results in Table 6 and compare with models like Table2Seq(Bao et al., 2018), Order Planning (Sha et al., 2018), Field Gating (Liu et al., 2018), Background-KB Attention (Chen et al., 2019), Hybrid Hierarchical Model (Liu et al., 2019a) trained with multiple auxiliary loss functions. We also train Template-GPT-2 on this dataset to observe pre-trained model’s performance. As can be seen from the table, KGPT can achieve better results than the mentioned baseline models. Pre-training can yield an improvement of roughly 0.5 BLEU4. As this dataset trainin/testing have similar table schema and the large number of training instances already teach the model to memorize the generation patterns, exploiting an external corpus of on par size (1.8M) does not bring a significant boost. So is"
2020.emnlp-main.697,2020.acl-main.708,1,0.860267,"ta-to-text is a longstanding problem (Kukich, 1983; Reiter and Dale, 1997), which involves generating natural language surface form from structured data. The traditional system is primarily built on a template-based algorithm. Recently, with the development of deep learning, attention has been gradually shifted to end-to-end neural generation models, which achieve significant performances on existing largescale datasets like WebNLG (Shimorina and Gardent, 2018), E2ENLG (Duˇsek et al., 2019), WikiBio (Lebret et al., 2016), ROTOWIRE (Wiseman et al., 2017), TOTTO (Parikh et al., 2020), LogicNLG (Chen et al., 2020a), etc. However, these neural generation models are mainly focused on fully supervised learning requiring a huge amount of human annotation for the specific task. Our paper focuses on building a more generalized model architecture, which can adapt to specific tasks well with only a handful of training instances. Knowledge-Grounded Language Modeling It is of primary importance to ground language models on existing knowledge of various forms. The neural language models (Bengio et al., 2003) have been shown to well capture the co-occurrences of n-grams in the sentences, but falls short to mainta"
2020.emnlp-main.697,2020.acl-main.18,1,0.92412,"ta-to-text is a longstanding problem (Kukich, 1983; Reiter and Dale, 1997), which involves generating natural language surface form from structured data. The traditional system is primarily built on a template-based algorithm. Recently, with the development of deep learning, attention has been gradually shifted to end-to-end neural generation models, which achieve significant performances on existing largescale datasets like WebNLG (Shimorina and Gardent, 2018), E2ENLG (Duˇsek et al., 2019), WikiBio (Lebret et al., 2016), ROTOWIRE (Wiseman et al., 2017), TOTTO (Parikh et al., 2020), LogicNLG (Chen et al., 2020a), etc. However, these neural generation models are mainly focused on fully supervised learning requiring a huge amount of human annotation for the specific task. Our paper focuses on building a more generalized model architecture, which can adapt to specific tasks well with only a handful of training instances. Knowledge-Grounded Language Modeling It is of primary importance to ground language models on existing knowledge of various forms. The neural language models (Bengio et al., 2003) have been shown to well capture the co-occurrences of n-grams in the sentences, but falls short to mainta"
2020.emnlp-main.697,W19-8614,0,0.0236769,"a total of four MVP awards. Language Model e inc of plays fo r awarded Moses ition pos Power Forward me Harden n ar aw Ground instance NBA d de awarded Hakeem r male gende instance of human mb er of Houston Rockets Figure 1: An example from the constructed KGT EXT, which pairs a hyperlinked sentence from Wikipedia with a knowledge subgraph from WikiData. ation models based on different strategies like softtemplate (Wiseman et al., 2018; Ye et al., 2020), copy-mechanism (See et al., 2017), content planning (Reed et al., 2018; Moryossef et al., 2019), and structure awareness (Liu et al., 2018; Colin and Gardent, 2019) have achieved impressive results. However, existing studies are primarily focused on fully supervised setting requiring substantial labeled annotated data for each subtask, which restricts their adoption in real-world applications. Data-to-text generation, i.e., generating textual description from structured data, is an important task with many real-world applications such as generating weather reports (Liang et al., 2009), sports news (Wiseman et al., 2017), dialog response (Wen et al., 2016; Duˇsek et al., 2019), etc. Neural generhttps://github.com/wenhuchen/KGPT award io pt MVP Introductio"
2020.emnlp-main.697,P19-1285,0,0.068746,"Missing"
2020.emnlp-main.697,N19-1423,0,0.570118,"er 30 ROUGE-L on WebNLG while all other baselines fail. Under the few-shot setting, our model only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models. These experiments consistently prove the strong generalization ability of our proposed framework1 . In this paper, we are interested in developing a general-purpose model that can easily adapt to different domains/tasks and achieve strong performance with only a small amount or even zero annotated examples. Our model draws inspiration from the recent wave of pre-trained language model (Devlin et al., 2019; Radford et al., 2019; Dai et al., 2019) to exploit large-scale unlabeled data from the web for pre-training. The data pairs are constructed through the following procedure. We first crawl sentences with hyperlinks from Wikipedia, and then link the hyperlinked entities to Wiki8635 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8635–8648, c November 16–20, 2020. 2020 Association for Computational Linguistics Data (Vrandeˇci´c and Kr¨otzsch, 2014) to find their 1-hop knowledge triples. Finally, we build a subgraph based on the linked triples. Such"
2020.emnlp-main.697,P16-2008,0,0.0627961,"Missing"
2020.emnlp-main.697,W18-6556,0,0.037239,"Missing"
2020.emnlp-main.697,2020.acl-main.398,0,0.0678005,"Missing"
2020.emnlp-main.697,N18-1014,0,0.0149831,"Pre BLEU 55.17 65.93 66.19 67.37 66.47 67.67 67.87 68.05 METEOR 38.75 44.83 44.54 45.23 44.20 45.33 44.50 45.80 ROUGE 65.01 68.50 67.72 70.89 67.78 70.39 70.00 70.92 can benefit the model’s few-shot learning capability but also compare with other pre-trained LM (Chen et al., 2020b,a) to see the benefit of KGPT over existing pre-trained LM. Model Seq2Seq Seq2Seq+Delex KGPT-Graph w/o Pre KGPT-Seq w/o Pre Template-GPT-2 KGPT-Graph w/ Pre KGPT-Seq w/ Pre Table 5: Experimental results on E2E’s test set. NTemp is from Wiseman et al. (2018), TGen is from Duˇsek and Jurcicek (2016), SLUG2SLUG is from Juraska et al. (2018) and Adapt is from Elder et al. (2018). WikiBio Dataset We list WikiBio’s experimental results in Table 6 and compare with models like Table2Seq(Bao et al., 2018), Order Planning (Sha et al., 2018), Field Gating (Liu et al., 2018), Background-KB Attention (Chen et al., 2019), Hybrid Hierarchical Model (Liu et al., 2019a) trained with multiple auxiliary loss functions. We also train Template-GPT-2 on this dataset to observe pre-trained model’s performance. As can be seen from the table, KGPT can achieve better results than the mentioned baseline models. Pre-training can yield an improvement of"
2020.emnlp-main.697,P83-1022,0,0.648641,"le the non-pre-trained baselines only generate gibberish text. A quantitative study shows that our pre-training scheme can reduce annotation costs by roughly 15x to achieve a decent BLEU score of 30. Our contribution is summarized as follows: i). We design a distantly supervised learning algorithm to exploit large-scale unlabeled web text to pre-train data-to-text models. ii). The proposed pre-training algorithm can bring significant performance under different settings, especially zero-shot and few-shot scenarios. 2 Related Work Data-to-Text Generation Data-to-text is a longstanding problem (Kukich, 1983; Reiter and Dale, 1997), which involves generating natural language surface form from structured data. The traditional system is primarily built on a template-based algorithm. Recently, with the development of deep learning, attention has been gradually shifted to end-to-end neural generation models, which achieve significant performances on existing largescale datasets like WebNLG (Shimorina and Gardent, 2018), E2ENLG (Duˇsek et al., 2019), WikiBio (Lebret et al., 2016), ROTOWIRE (Wiseman et al., 2017), TOTTO (Parikh et al., 2020), LogicNLG (Chen et al., 2020a), etc. However, these neural ge"
2020.emnlp-main.697,D16-1128,0,0.0813833,"Missing"
2020.emnlp-main.697,2020.acl-main.703,0,0.0955665,"Missing"
2020.emnlp-main.697,P09-1011,0,0.176479,"l., 2018; Ye et al., 2020), copy-mechanism (See et al., 2017), content planning (Reed et al., 2018; Moryossef et al., 2019), and structure awareness (Liu et al., 2018; Colin and Gardent, 2019) have achieved impressive results. However, existing studies are primarily focused on fully supervised setting requiring substantial labeled annotated data for each subtask, which restricts their adoption in real-world applications. Data-to-text generation, i.e., generating textual description from structured data, is an important task with many real-world applications such as generating weather reports (Liang et al., 2009), sports news (Wiseman et al., 2017), dialog response (Wen et al., 2016; Duˇsek et al., 2019), etc. Neural generhttps://github.com/wenhuchen/KGPT award io pt MVP Introduction 1 1956 Basketball rt 1 Moses;Harden;Hakeem Description: American Basketball Player o sp s Data-to-text generation has recently attracted substantial interests due to its wide applications. Existing methods have shown impressive performance on an array of tasks. However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their application to new tasks and domains. In"
2020.emnlp-main.697,W04-1013,0,0.0155763,"4,313 4,672 72,831 Test 4,222 4,693 72,831 Input RDF Triple Dialog Act Table Table 2: Statistics of different data-to-text datasets 8640 2019) as the generator to decode description from a table. For the ablation purposes, we list the performance of all non-pre-trained KGPT to see the performance gain brought by pre-training alone. All the best models are selected based on the validation set score, and the numbers are reported in the following tables are for test split. For evaluation, we report the performance with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004) using e2e-metric6 . It’s worth noting that we perform comprehensive data contamination studies in the following experiments to make sure the pre-training data contains very little overlap with the test split in downstream tasks. We filter out potentially information-leaking pages during the data crawling process. 5.3 Model Seq2Seq† Seq2Seq+Delex† Seq2Seq+Copy† GCN KGPT-Graph w/o Pre KGPT-Seq w/o Pre KGPT-Graph w/ Pre KGPT-Seq w/ Pre Preliminary Study on KGT EXT In the preliminary study, we evaluate our pretrained model’s performance on the held-out set of KGT EXT to conduct ablation study ove"
2020.emnlp-main.697,2021.ccl-1.108,0,0.131242,"Missing"
2020.emnlp-main.697,P19-1598,0,0.0230308,"k. Our paper focuses on building a more generalized model architecture, which can adapt to specific tasks well with only a handful of training instances. Knowledge-Grounded Language Modeling It is of primary importance to ground language models on existing knowledge of various forms. The neural language models (Bengio et al., 2003) have been shown to well capture the co-occurrences of n-grams in the sentences, but falls short to maintain the faithfulness or consistency to world facts. To combat such an issue, different knowledgegrounded language models (Ahn et al., 2016; Hayashi et al., 2020; Logan et al., 2019) have been proposed to infuse structured knowledge into the neural language model. These models are mainly focused on enhancing the factualness of unconditional generative models. Inspired by these pioneering studies, we explore the possibility to connect the unconditional generative model with downstream conditional generation tasks. The most straightforward knowledge-intensive conditional generative task is the data-to-text generation, which aims to verbatim given knowledge into lexical format. We demonstrate great potential of the knowledge-grounded pretraining in enhancing the model’s fact"
2020.emnlp-main.697,W02-0109,0,0.424538,"rained GPT-2 model as the decoder part 8636 to perform table-to-text generation. However, their knowledge encoder is still trained from scratch, which compromises the performance. In this paper, we follow the existing paradigm to construct an unlabeled web data for LM pre-training. 3 Dataset Construction The construction process has two stages, namely the crawling stage and the selection stage: 3.1 Hyperlinked Sentence Crawling We use English Wikidump2 as our data source. For each Wikipedia page, we split the whole paragraphs into an array of sentences and then tokenize with the nltk toolkit (Loper and Bird, 2002). We loop through each sentence to keep the sentences with more than 2 Wikipedia anchor links and within the length of 10 and 50. For each candidate sentence, we use its Wikipedia hyperlink to query WikiData (Vrandeˇci´c and Kr¨otzsch, 2014) and obtain its corresponding entity page3 . We retrieve the neighboring knowledge triples from these entity pages to construct a local 1-hop graph for each entity. The knowledge triples are divided into two types: 1) the object of the triple is also an entity like ‘(Roma F.C., country, Italy)’, 2) the object of the triple is in plain text like ‘(Roma F.C.,"
2020.emnlp-main.697,D15-1166,0,0.0148812,"ervising the copy attention does not have much influence on the performance. Therefore, in the following experiments, we will run experiments for both encoding schemes with a copy mechanism without copy loss. Model KGPT-Graph KGPT-Graph + Copy Loss KGPT-Graph w/o Copy KGPT-Seq KGPT-Seq + Copy Loss KGPT-Seq w/o Copy BLEU-4 24.71 24.77 22.69 24.49 24.31 22.92 Perplexity 4.86 4.91 7.23 4.95 4.93 7.11 Table 3: Ablation Study on held-out set of KGT EXT. 5.4 the known models under the unconstrained setting. The baseline models (Shimorina and Gardent, 2018) uses sequence-to-sequence attention model (Luong et al., 2015) as the backbone and propose delexicalization and copy mechanism to enhance model’s capability to handle rare items from the input. The GCN model (Marcheggiani and Perez-Beltrachini, 2018) uses graph convolutional neural encoder to encode the structured data input. Its implementation is from Github7 . As can be seen, KGPT without pre-training already achieves better performance than the GCN baseline. With pre-training, the performance is further boosted by 1-2 BLEU-4, which reflects the effectiveness of our method. Fully-Supervised Results We experiment with KGPT under the standard fully-super"
2020.emnlp-main.697,P19-1197,0,0.209259,"Missing"
2020.emnlp-main.697,W18-6501,0,0.0185408,"s with a copy mechanism without copy loss. Model KGPT-Graph KGPT-Graph + Copy Loss KGPT-Graph w/o Copy KGPT-Seq KGPT-Seq + Copy Loss KGPT-Seq w/o Copy BLEU-4 24.71 24.77 22.69 24.49 24.31 22.92 Perplexity 4.86 4.91 7.23 4.95 4.93 7.11 Table 3: Ablation Study on held-out set of KGT EXT. 5.4 the known models under the unconstrained setting. The baseline models (Shimorina and Gardent, 2018) uses sequence-to-sequence attention model (Luong et al., 2015) as the backbone and propose delexicalization and copy mechanism to enhance model’s capability to handle rare items from the input. The GCN model (Marcheggiani and Perez-Beltrachini, 2018) uses graph convolutional neural encoder to encode the structured data input. Its implementation is from Github7 . As can be seen, KGPT without pre-training already achieves better performance than the GCN baseline. With pre-training, the performance is further boosted by 1-2 BLEU-4, which reflects the effectiveness of our method. Fully-Supervised Results We experiment with KGPT under the standard fully-supervised setting to compare its performance with other state-of-the-art algorithms. WebNLG Challenge We list WebNLG’s experimental results in Table 4, here we compare with 6 https://github.co"
2020.emnlp-main.697,P09-1113,0,0.118306,"the web for pre-training. The data pairs are constructed through the following procedure. We first crawl sentences with hyperlinks from Wikipedia, and then link the hyperlinked entities to Wiki8635 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8635–8648, c November 16–20, 2020. 2020 Association for Computational Linguistics Data (Vrandeˇci´c and Kr¨otzsch, 2014) to find their 1-hop knowledge triples. Finally, we build a subgraph based on the linked triples. Such automatic alignment between knowledge graph and texts provides distant supervision (Mintz et al., 2009) for pre-training but it is bound to be noisy. Therefore, we design a selection strategy and only retain plausible alignments with high semantic overlap. The harvested knowledge-grounded corpus KGT EXT consists of over 1.8M (knowledge subgraph, text) pairs, as depicted in Figure 1. We unify the input of KGT EXT and downstream data-to-text tasks into a generalized format and design a novel architecture KGPT to encode it. We use KGT EXT to first pre-train KGPT and then fine-tune it on downstream data-to-text tasks like WebNLG (Shimorina and Gardent, 2018), E2ENLG (Duˇsek et al., 2019) and WikiBi"
2020.emnlp-main.697,N19-1236,0,0.0437022,"Missing"
2020.emnlp-main.697,P02-1040,0,0.106864,"nsformers Dataset WebNLG E2ENLG WikiBio Train 34,338 42,061 582,657 Val 4,313 4,672 72,831 Test 4,222 4,693 72,831 Input RDF Triple Dialog Act Table Table 2: Statistics of different data-to-text datasets 8640 2019) as the generator to decode description from a table. For the ablation purposes, we list the performance of all non-pre-trained KGPT to see the performance gain brought by pre-training alone. All the best models are selected based on the validation set score, and the numbers are reported in the following tables are for test split. For evaluation, we report the performance with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) and ROUGE-L (Lin, 2004) using e2e-metric6 . It’s worth noting that we perform comprehensive data contamination studies in the following experiments to make sure the pre-training data contains very little overlap with the test split in downstream tasks. We filter out potentially information-leaking pages during the data crawling process. 5.3 Model Seq2Seq† Seq2Seq+Delex† Seq2Seq+Copy† GCN KGPT-Graph w/o Pre KGPT-Seq w/o Pre KGPT-Graph w/ Pre KGPT-Seq w/ Pre Preliminary Study on KGT EXT In the preliminary study, we evaluate our pretrained model’s performance o"
2020.emnlp-main.697,W18-6535,0,0.0228263,"arden have been named the NBA's Most Valuable Player while playing for the Rockets, for a total of four MVP awards. Language Model e inc of plays fo r awarded Moses ition pos Power Forward me Harden n ar aw Ground instance NBA d de awarded Hakeem r male gende instance of human mb er of Houston Rockets Figure 1: An example from the constructed KGT EXT, which pairs a hyperlinked sentence from Wikipedia with a knowledge subgraph from WikiData. ation models based on different strategies like softtemplate (Wiseman et al., 2018; Ye et al., 2020), copy-mechanism (See et al., 2017), content planning (Reed et al., 2018; Moryossef et al., 2019), and structure awareness (Liu et al., 2018; Colin and Gardent, 2019) have achieved impressive results. However, existing studies are primarily focused on fully supervised setting requiring substantial labeled annotated data for each subtask, which restricts their adoption in real-world applications. Data-to-text generation, i.e., generating textual description from structured data, is an important task with many real-world applications such as generating weather reports (Liang et al., 2009), sports news (Wiseman et al., 2017), dialog response (Wen et al., 2016; Duˇsek"
2020.emnlp-main.697,P17-1099,0,0.312802,"Malone, Hakeem Olajuwon, and James Harden have been named the NBA's Most Valuable Player while playing for the Rockets, for a total of four MVP awards. Language Model e inc of plays fo r awarded Moses ition pos Power Forward me Harden n ar aw Ground instance NBA d de awarded Hakeem r male gende instance of human mb er of Houston Rockets Figure 1: An example from the constructed KGT EXT, which pairs a hyperlinked sentence from Wikipedia with a knowledge subgraph from WikiData. ation models based on different strategies like softtemplate (Wiseman et al., 2018; Ye et al., 2020), copy-mechanism (See et al., 2017), content planning (Reed et al., 2018; Moryossef et al., 2019), and structure awareness (Liu et al., 2018; Colin and Gardent, 2019) have achieved impressive results. However, existing studies are primarily focused on fully supervised setting requiring substantial labeled annotated data for each subtask, which restricts their adoption in real-world applications. Data-to-text generation, i.e., generating textual description from structured data, is an important task with many real-world applications such as generating weather reports (Liang et al., 2009), sports news (Wiseman et al., 2017), dial"
2020.emnlp-main.697,W18-6543,0,0.166266,"dge graph and texts provides distant supervision (Mintz et al., 2009) for pre-training but it is bound to be noisy. Therefore, we design a selection strategy and only retain plausible alignments with high semantic overlap. The harvested knowledge-grounded corpus KGT EXT consists of over 1.8M (knowledge subgraph, text) pairs, as depicted in Figure 1. We unify the input of KGT EXT and downstream data-to-text tasks into a generalized format and design a novel architecture KGPT to encode it. We use KGT EXT to first pre-train KGPT and then fine-tune it on downstream data-to-text tasks like WebNLG (Shimorina and Gardent, 2018), E2ENLG (Duˇsek et al., 2019) and WikiBio (Liu et al., 2018). Experimental results demonstrate KGPT’s several advantages: 1) with full downstream dataset, KGPT can achieve remarkably better performance than known competitive baselines, 2) with zero training, KGPT can still achieve a reasonable score on WebNLG. 3) with a few training instances, KGPT can maintain a high BLEU score while the non-pre-trained baselines only generate gibberish text. A quantitative study shows that our pre-training scheme can reduce annotation costs by roughly 15x to achieve a decent BLEU score of 30. Our contributi"
2020.emnlp-main.697,N16-1015,0,0.0576472,"Missing"
2020.emnlp-main.697,D18-1356,0,0.469173,".ucsb.edu, su.809@osu.edu Abstract Houston Rockets Moses Malone, Hakeem Olajuwon, and James Harden have been named the NBA's Most Valuable Player while playing for the Rockets, for a total of four MVP awards. Language Model e inc of plays fo r awarded Moses ition pos Power Forward me Harden n ar aw Ground instance NBA d de awarded Hakeem r male gende instance of human mb er of Houston Rockets Figure 1: An example from the constructed KGT EXT, which pairs a hyperlinked sentence from Wikipedia with a knowledge subgraph from WikiData. ation models based on different strategies like softtemplate (Wiseman et al., 2018; Ye et al., 2020), copy-mechanism (See et al., 2017), content planning (Reed et al., 2018; Moryossef et al., 2019), and structure awareness (Liu et al., 2018; Colin and Gardent, 2019) have achieved impressive results. However, existing studies are primarily focused on fully supervised setting requiring substantial labeled annotated data for each subtask, which restricts their adoption in real-world applications. Data-to-text generation, i.e., generating textual description from structured data, is an important task with many real-world applications such as generating weather reports (Liang et"
2020.emnlp-main.708,P11-1020,0,0.036625,"the output space. In contrast to text classification or regression problems with finite output space, generation could be seen as a combinatorial optimization problem, where we often have exponentially many options |V |` (here |V |is the size of the vocabulary and ` is the sentence length). With the advances of both Computer Vision and NLP techniques in deep learning, there have been growing interests in visually grounded NLG tasks, such as image captioning (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014; Vedantam et al., 2015), video captioning (Xu et al., 2016; Wang et al., 2019; Chen and Dolan, 2011) and visual storytelling (Huang et al., 2016). For example, Figure 1 shows an example of image captioning from the popular Flickr30k dataset. In this paper, instead of crunching numbers and modifying model architectural designs to achieve new “state-of-the-art” results on leaderboards, we focus on re-assessing the current practices in visually grounded language generation research, including problems, datasets, evaluations, and tasks, from the sample variance angle. Given the differences in annotators’ utility function and human visual attention models, how could the sample variance in caption"
2020.emnlp-main.708,D13-1128,0,0.0374636,"ality of the generated text, including BLEU (Papineni Task Dataset 11.8 6k 1k 1k 5 12.3 29k 1k 1k MS COCO’14 5 10.5 83k 5k 5k PASCAL-50S 50 8.8 — — 1k VATEX en 10 15.2 26k 3k 6k VATEX cn 10 14.0 26k 3k 6k 5 56.8 8k 1k 1k Image Captioning Flickr30k Video Captioning #ref #len #train #val #test 5 Flickr8k Visual Storytelling VIST Table 1: Dataset statistics. #ref is the number of parallel references per visual instance; #len is the average reference length; #train, #val, and #test are the number of visual instances of training, validation, and test sets. et al., 2002), ROUGE (Lin, 2004), METEOR (Elliott and Keller, 2013), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016) and the most recent BERTScore (Zhang* et al., 2020) that is based on the pretrained BERT model. We use nlg-eval3 (Sharma et al., 2017) for the calculation of BLEU, METEOR, ROUGE L and CIDEr. Note that we applied a patch4 and choose to use IDF from the MSCOCO Vaildation Dataset when calculating consensus CIDEr score for each dataset. We use the authors’ releases for SPICE5 and BERTScore6 . BERTScore has been rescaled with baseline scores. 3 Reference Variance within Datasets In this section, we examine the sample variance among text"
2020.emnlp-main.708,N16-1147,0,0.0934603,"ation or regression problems with finite output space, generation could be seen as a combinatorial optimization problem, where we often have exponentially many options |V |` (here |V |is the size of the vocabulary and ` is the sentence length). With the advances of both Computer Vision and NLP techniques in deep learning, there have been growing interests in visually grounded NLG tasks, such as image captioning (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014; Vedantam et al., 2015), video captioning (Xu et al., 2016; Wang et al., 2019; Chen and Dolan, 2011) and visual storytelling (Huang et al., 2016). For example, Figure 1 shows an example of image captioning from the popular Flickr30k dataset. In this paper, instead of crunching numbers and modifying model architectural designs to achieve new “state-of-the-art” results on leaderboards, we focus on re-assessing the current practices in visually grounded language generation research, including problems, datasets, evaluations, and tasks, from the sample variance angle. Given the differences in annotators’ utility function and human visual attention models, how could the sample variance in captions teach us building robust and reliable visua"
2020.emnlp-main.708,W04-1013,0,0.0609678,"n to evaluate the quality of the generated text, including BLEU (Papineni Task Dataset 11.8 6k 1k 1k 5 12.3 29k 1k 1k MS COCO’14 5 10.5 83k 5k 5k PASCAL-50S 50 8.8 — — 1k VATEX en 10 15.2 26k 3k 6k VATEX cn 10 14.0 26k 3k 6k 5 56.8 8k 1k 1k Image Captioning Flickr30k Video Captioning #ref #len #train #val #test 5 Flickr8k Visual Storytelling VIST Table 1: Dataset statistics. #ref is the number of parallel references per visual instance; #len is the average reference length; #train, #val, and #test are the number of visual instances of training, validation, and test sets. et al., 2002), ROUGE (Lin, 2004), METEOR (Elliott and Keller, 2013), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016) and the most recent BERTScore (Zhang* et al., 2020) that is based on the pretrained BERT model. We use nlg-eval3 (Sharma et al., 2017) for the calculation of BLEU, METEOR, ROUGE L and CIDEr. Note that we applied a patch4 and choose to use IDF from the MSCOCO Vaildation Dataset when calculating consensus CIDEr score for each dataset. We use the authors’ releases for SPICE5 and BERTScore6 . BERTScore has been rescaled with baseline scores. 3 Reference Variance within Datasets In this section, we exa"
2020.emnlp-main.708,Q14-1006,0,0.386596,"is a challenging problem in Natural Language Processing (NLP)—the complex nature of NLG tasks arise particularly in the output space. In contrast to text classification or regression problems with finite output space, generation could be seen as a combinatorial optimization problem, where we often have exponentially many options |V |` (here |V |is the size of the vocabulary and ` is the sentence length). With the advances of both Computer Vision and NLP techniques in deep learning, there have been growing interests in visually grounded NLG tasks, such as image captioning (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014; Vedantam et al., 2015), video captioning (Xu et al., 2016; Wang et al., 2019; Chen and Dolan, 2011) and visual storytelling (Huang et al., 2016). For example, Figure 1 shows an example of image captioning from the popular Flickr30k dataset. In this paper, instead of crunching numbers and modifying model architectural designs to achieve new “state-of-the-art” results on leaderboards, we focus on re-assessing the current practices in visually grounded language generation research, including problems, datasets, evaluations, and tasks, from the sample variance angle. Given the"
2020.emnlp-main.708,P02-1040,0,0.108466,"Missing"
2020.emnlp-main.708,P18-1083,1,0.921625,"n. Datasets Seven commonly used datasets in Table 1 are considered: Flickr8k (Hodosh et al., 2013), Flickr30k (Young et al., 2014), MS COCO (Lin et al., 2014), PASCAL-50S (Vedantam et al., 2015), VATEX en (English), VATEX cn (Chinese) (Wang et al., 2019), and VIST (Huang et al., 2016), covering the tasks of image captioning, video captioning, and visual storytelling. Models We apply an implementation1 of Xu et al. (2015) for image captioning. We implement the Enc-Dec baseline model proposed by Wang et al. (2019) for video captioning. For visual storytelling, we use the AREL model2 proposed by Wang et al. (2018). Metrics We utilize six automatic metrics for natural language generation to evaluate the quality of the generated text, including BLEU (Papineni Task Dataset 11.8 6k 1k 1k 5 12.3 29k 1k 1k MS COCO’14 5 10.5 83k 5k 5k PASCAL-50S 50 8.8 — — 1k VATEX en 10 15.2 26k 3k 6k VATEX cn 10 14.0 26k 3k 6k 5 56.8 8k 1k 1k Image Captioning Flickr30k Video Captioning #ref #len #train #val #test 5 Flickr8k Visual Storytelling VIST Table 1: Dataset statistics. #ref is the number of parallel references per visual instance; #len is the average reference length; #train, #val, and #test are the number of visual"
2020.findings-emnlp.190,W13-3802,0,0.142481,"Missing"
2020.findings-emnlp.190,D18-1547,0,0.026323,"re are two examples with logic type count and superlative. The function nodes are in blue, and the text nodes in grey. the ground truth logical forms. This can be one direct application of our dataset. In this work, we focus on NLG. 2 Related Work NLG from structured data or knowledge has been studied for many years. There are various applications, such as the automatic generations of weather reports (Liang et al., 2009), sport reports (Wiseman et al., 2017), clinical and health reports (DiMarco et al., 2007; Lee, 2018), response generation in task-oriented dialogue systems (Wen et al., 2015; Budzianowski et al., 2018; Duˇsek et al., 2019), etc. Traditional methods typically employ a pipelinebased approach including content selection, planning and surface realization (Reiter and Dale, 1997; Gatt and Krahmer, 2018). Recent data-driven 2097 methods tend to conflate the pipeline modules into one end-to-end neural networks, such as (Liu et al., 2018; Wiseman et al., 2017, 2018; Gong et al., 2019). Most recently, large-scale pre-trained models (Radford et al., 2019; Song et al., 2019; Raffel et al., 2019) have achieved new state-ofthe-arts on various generation tasks. Chen et al. (2019b) demonstrate that a simp"
2020.findings-emnlp.190,2020.acl-main.167,0,0.0468855,"Missing"
2020.findings-emnlp.190,W18-6501,0,0.0225776,"e employ the seq2seq with an attention model from (Bahdanau et al., 2015). The input sequence is formulated as the concatenation of the table caption, table headers, the linearized table content, and the linearized logical form. Pointer generator (See et al., 2017) adds the copy mechanism upon the seq2seq with an attention model, allowing the decoder to copy tokens from the input directly. Such a mechanism is known to be critical for fidelity-preserving generation with abundant entities, numbers, etc. Graph2seq+copy There is a line of research for graph neural network based encoders, such as (Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018), etc. We employ one representative model, Graph2seq (Xu et al., 2018), to encode the logical forms. The table caption and headers are first fed into a seq2seq, followed by the graph encoder for the logical form. We also add the copy mechanism to allow copying from the input. Transformer+copy The popular Transformer model (Vaswani et al., 2017) has shown remarkable progress in many tasks including NLG. In addition to the original Transformer structure, we add the copy mechanism where the last hidden layer is used to calculate the attention score and the copy switch. We also a"
2020.findings-emnlp.190,S16-1166,0,0.0326973,"Missing"
2020.findings-emnlp.190,W17-5525,0,0.210254,"ds building an advanced NLG system capable of natural, faithful, and human-like generation. The dataset and code is available at https://github. com/czyssrs/Logic2Text. 1 Introduction Natural language generation (NLG) from structured data has been an important research problem in many applications. Recent data-driven methods have achieved good performances on various NLG tasks (Liu et al., 2018; Freitag and Roy, 2018; Chen et al., 2019b). However most studies focus on surface descriptions of simple record sequences, for example, attribute-value pairs of fixed or very limited schema, like E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016). In real-world cases for multi-row tables, it is often more desirable and plausible to provide descriptions involving higherlevel logical inference across data records. For example, in Figure 1, instead of plain restatements, human readers would be more favorable to abstract descriptions that can summarize or conclude information over the table records. To produce such logical-level generations of high fidelity, it is not yet appropriate to provide only the table as the input in a real-world NLG system, based on the following reasons: 1) Low Fidelity. Given o"
2020.findings-emnlp.190,2020.emnlp-main.89,0,0.112028,"Missing"
2020.findings-emnlp.190,D17-1239,0,0.0664743,"Missing"
2020.findings-emnlp.190,P16-1162,0,0.00704377,"er is used to calculate the attention score and the copy switch. We also add segment embeddings for different input components, similar as (Devlin et al., 2019). GPT-2 Generally, with Transformer based structures, recent large-scale pre-trained models have achieved new SOTA results in a wide range of NLP tasks. A typical workflow is to use the pre-trained model as initialization, then fine-tune the model on task-specific data. In this work, we employ the generative pre-training model, GPT-2 (Radford et al., 2019), as one of our baselines. For all neural models we use Byte-Pair Encoding (BPE) (Sennrich et al., 2016) and the subword vocabulary used in (Radford et al., 2019). Refer to Appendix C for more implementation details. 5.2 Fully-Supervised Setting For automatic evaluations, we employ BLEU-45 (B-4), ROUGE-1, 2, 4, and L (F measure)6 , noted as R-1, R-2, R-4, and R-L. The results for all baselines are presented in Table 2. For models without pre-training, the copy mechanism brings a significant improvement, comparing pointer-generator and seq2seq. This is because the descriptions in our dataset involve much factual information from the table and the logical form, e.g., entity names, and numbers. How"
2020.findings-emnlp.190,D18-1356,0,0.0972689,"Missing"
2020.findings-emnlp.190,J90-1004,0,0.770276,"Missing"
2020.findings-emnlp.190,D18-1112,0,0.0635715,"el from (Bahdanau et al., 2015). The input sequence is formulated as the concatenation of the table caption, table headers, the linearized table content, and the linearized logical form. Pointer generator (See et al., 2017) adds the copy mechanism upon the seq2seq with an attention model, allowing the decoder to copy tokens from the input directly. Such a mechanism is known to be critical for fidelity-preserving generation with abundant entities, numbers, etc. Graph2seq+copy There is a line of research for graph neural network based encoders, such as (Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018), etc. We employ one representative model, Graph2seq (Xu et al., 2018), to encode the logical forms. The table caption and headers are first fed into a seq2seq, followed by the graph encoder for the logical form. We also add the copy mechanism to allow copying from the input. Transformer+copy The popular Transformer model (Vaswani et al., 2017) has shown remarkable progress in many tasks including NLG. In addition to the original Transformer structure, we add the copy mechanism where the last hidden layer is used to calculate the attention score and the copy switch. We also add segment embeddi"
2020.findings-emnlp.190,P18-1150,0,0.0679919,"Missing"
2020.findings-emnlp.190,D15-1199,0,\N,Missing
2020.findings-emnlp.190,N01-1003,0,\N,Missing
2020.findings-emnlp.190,P09-1011,0,\N,Missing
2020.findings-emnlp.190,D09-1042,0,\N,Missing
2020.findings-emnlp.190,P17-1099,0,\N,Missing
2020.findings-emnlp.190,P19-1600,0,\N,Missing
2020.findings-emnlp.190,D19-1299,0,\N,Missing
2020.findings-emnlp.190,2020.tacl-1.54,0,\N,Missing
2020.findings-emnlp.190,2020.acl-main.708,1,\N,Missing
2020.findings-emnlp.190,D19-1310,0,\N,Missing
2020.findings-emnlp.62,N19-1268,0,0.0336298,"8), navigation in urban environments (Chen et al., 2019) is particularly challenging, since urban environments are often more diverse and complex. Several research studies (Mirowski et al., 2018; Li et al., 2019; Bruce et al., 2018) have been conducted to solve the problem. In this paper, we also focus on the urban VLN task. As shown in Fig. 1, given a natural language instruction, the agent perceives local visual scene and chooses actions at every time step, learning to match the instruction with the produced trajectory and navigate to the destination. Existing VLN models (Wang et al., 2019; Tan et al., 2019; Ke et al., 2019; Ma et al., 2019b,a; Fried et al., 2018; Wang et al., 2018) seem to neglect the importance of the STOP action and treat all actions equally. However, this can lead to undesirable behaviors, also noticed in Cirik et al. (2018); Blukis et al. (2018), that the agent fails to stop at the target although it might be on the right path, because the STOP action is severely underestimated. We argue that the STOP action in the urban VLN tasks is crucially important and deserves special treatment. First, in contrast to errors on other actions that are likely to be fixed later in the jou"
2020.findings-emnlp.91,D13-1160,0,0.560987,"estions WebQSP WebQComplex MetaQA WikiTableQA 5.8K 4.7K 34K 400k 22K no no no no no yes yes yes yes yes yes yes yes yes yes SQuAD-v1 DROP TriviaQA HotpotQA Natural-QA 107K 99K 95K 112K 300K 1 1 &gt;1 &gt;1 &gt;1 no no no no no no yes no yes yes H YBRID QA 70K &gt;&gt;1 yes yes #Table 13,000 #Passage 293,269 #Row/#Column 15.7/4.4 #Words/Passage 103 #Cell 70 #Ques 69,611 #Links/Table 44 #Words/Ques 18.9 Table 2: Statistics of Table and Passage in our dataset. Table 1: Comparison of existing datasets, where #docs means the number of documents provided for a specific question. 1) KB-only datasets: WebQuestions (Berant et al., 2013), WebQSP (Yih et al., 2016), WebComplex (Talmor and Berant, 2018), MetaQA (Zhang et al., 2018), WikiTableQuestion (Pasupat and Liang, 2015). 2) Text-only datasets with single passage: like SQuAD (Rajpurkar et al., 2016), DROP (Dua et al., 2019). 3) open-domain Text-Only dataset: TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), Natural Questions (Kwiatkowski et al., 2019). tion. The dataset consists of roughly 70K questionanswering pairs aligned with 13,000 Wikipedia tables. As Wikitables (Bhagavatula et al., 2013) are curated from high-standard professionals to organize a set of in"
2020.findings-emnlp.91,P17-1171,0,0.238979,"have the following values of “argmax, argmin, argmaxdate, argmin-date”, the target and condition slots have their potential values based on the table field and its corresponding entries. Though we do not have the ground-truth annotation for these simple SQL queries, we can use heuristics to infer them from the denotation. We use the synthesized question-SQL pairs to train the parser model. 1030 4.2 Passage-Only Model 4.3 …Men… Retriever …100… Linking In this setting, we design a model that only uses the hyperlinked passages from the given table to find the answer. Our model is based on DrQA (Chen et al., 2017), which first uses an ensemble of several retrievers to retrieve related documents and then concatenate several documents together to do reading comprehension with the state-of-the-art BERT model (Devlin et al., 2019). The basic architecture is depicted in Figure 4, where we use the retriever to retrieve the top-5 passages from the pool and then concatenate them as a document for the MRC model, and the maximum length of the concatenated document is set to 512. TF-IDF Longest-Substring Min/Max Greater/Less Equal Yan male male Cell Match Win Ans Reasoning Yan male male Win Yan male … 2012 … Copy"
2020.findings-emnlp.91,N19-1405,0,0.0516762,"rop at most the first 12 sentences from its introduction session as the associated passage. 3) we apply some additional rules to avoid improper tables and finally collect 13,000 high-quality tables. • The answer should be a minimum text span from either a table cell or a specific passage. Based on the above criteria, we hire five CSmajored graduate students as our “human expert” to decide the acceptance of a HIT. The average completion time for one HIT is 12 minutes, and payment is $2.3 U.S. dollars/HIT. Annotation De-biasing As has been suggested in previous papers (Kaushik and Lipton, 2018; Chen and Durrett, 2019; Clark et al., 2019), the existing benchmarks on multi-hop reasoning question answering have annotation biases, which makes designing multi-hop models unnecessary. We discuss different biases and our prevention as follows: • Table Bias: our preliminary study observes that the annotators prefer to ask questions regarding the top part of the table. In order to deal with this issue, we explicitly highlight certain regions in the table to encourage crowd-workers to raise questions regarding the given uniformly-distributed regions. Question/Answer Collection We release 13K HITs (human intelligence"
2020.findings-emnlp.91,D19-1418,0,0.0217195,"sentences from its introduction session as the associated passage. 3) we apply some additional rules to avoid improper tables and finally collect 13,000 high-quality tables. • The answer should be a minimum text span from either a table cell or a specific passage. Based on the above criteria, we hire five CSmajored graduate students as our “human expert” to decide the acceptance of a HIT. The average completion time for one HIT is 12 minutes, and payment is $2.3 U.S. dollars/HIT. Annotation De-biasing As has been suggested in previous papers (Kaushik and Lipton, 2018; Chen and Durrett, 2019; Clark et al., 2019), the existing benchmarks on multi-hop reasoning question answering have annotation biases, which makes designing multi-hop models unnecessary. We discuss different biases and our prevention as follows: • Table Bias: our preliminary study observes that the annotators prefer to ask questions regarding the top part of the table. In order to deal with this issue, we explicitly highlight certain regions in the table to encourage crowd-workers to raise questions regarding the given uniformly-distributed regions. Question/Answer Collection We release 13K HITs (human intelligence task) on the Amazon"
2020.findings-emnlp.91,N19-1423,0,0.257718,"er … commonly known as Rio 2016 , was an international multi-sport event … … Beijing Olympic … event BERT MRC Passage-Only Q: Where was the XXI Olympic held? Figure 4: Illustration of the table-only and passageonly baselines, both are based on BERT Encoder. et al., 2017; Xu et al., 2017), which uses a neural network to parse the given questions into a symbolic form and execute against the table. We follow the SQLNet (Xu et al., 2017) to flatten the prediction of the whole SQL query into a slot filling procedure. More specifically, our parser model first encode the input question q using BERT (Devlin et al., 2019) and then decode the aggregation, target, condition separately as described in Figure 4. The aggregation slot can have the following values of “argmax, argmin, argmaxdate, argmin-date”, the target and condition slots have their potential values based on the table field and its corresponding entries. Though we do not have the ground-truth annotation for these simple SQL queries, we can use heuristics to infer them from the denotation. We use the synthesized question-SQL pairs to train the parser model. 1030 4.2 Passage-Only Model 4.3 …Men… Retriever …100… Linking In this setting, we design a mo"
2020.findings-emnlp.91,N19-1246,0,0.0665024,"Missing"
2020.findings-emnlp.91,P17-1147,0,0.143614,"7/4.4 #Words/Passage 103 #Cell 70 #Ques 69,611 #Links/Table 44 #Words/Ques 18.9 Table 2: Statistics of Table and Passage in our dataset. Table 1: Comparison of existing datasets, where #docs means the number of documents provided for a specific question. 1) KB-only datasets: WebQuestions (Berant et al., 2013), WebQSP (Yih et al., 2016), WebComplex (Talmor and Berant, 2018), MetaQA (Zhang et al., 2018), WikiTableQuestion (Pasupat and Liang, 2015). 2) Text-only datasets with single passage: like SQuAD (Rajpurkar et al., 2016), DROP (Dua et al., 2019). 3) open-domain Text-Only dataset: TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), Natural Questions (Kwiatkowski et al., 2019). tion. The dataset consists of roughly 70K questionanswering pairs aligned with 13,000 Wikipedia tables. As Wikitables (Bhagavatula et al., 2013) are curated from high-standard professionals to organize a set of information regarding a given theme, its information is mostly absent in the text. Such a complementary nature makes WikiTables an ideal environment for hybrid question answering. To ensure that the answers cannot be hacked by singlehop or homogeneous models, we carefully employ different strategies to calibra"
2020.findings-emnlp.91,D18-1546,0,0.0637662,"Missing"
2020.findings-emnlp.91,D13-1161,0,0.0977104,"Missing"
2020.findings-emnlp.91,Q19-1026,0,0.129613,"Missing"
2020.findings-emnlp.91,P15-1142,0,0.638002,"heterogeneous information in HybridQA. However, the hybrid model’s score is still far behind human performance. Hence, HybridQA can serve as a challenging benchmark to study question answering with heterogeneous information. 1 Introduction Question answering systems aim to answer any form of question of our interests, with evidence provided by either free-form text like Wikipedia passages (Rajpurkar et al., 2016; Chen et al., 2017; Yang et al., 2018) or structured data like Freebase/WikiData (Berant et al., 2013; Kwiatkowski et al., 2013; Yih et al., 2015; Weston et al., 2015) and WikiTables (Pasupat and Liang, 2015). Both 1 https://github.com/wenhuchen/HybridQA forms have their advantages, the free-form corpus has in general better coverage while structured data has better compositionality to handle complex multi-hop questions. Due to the advantages of different representation forms, people like to combine them in real world applications. Therefore, it is sometime not ideal to assume the question has answer in a passage. This paper aims to simulate a more realistic setting where the evidences are distributed into heterogeneous data, and the model requires to aggregate information from different forms for"
2020.findings-emnlp.91,D16-1264,0,0.466245,"o no yes no yes yes H YBRID QA 70K &gt;&gt;1 yes yes #Table 13,000 #Passage 293,269 #Row/#Column 15.7/4.4 #Words/Passage 103 #Cell 70 #Ques 69,611 #Links/Table 44 #Words/Ques 18.9 Table 2: Statistics of Table and Passage in our dataset. Table 1: Comparison of existing datasets, where #docs means the number of documents provided for a specific question. 1) KB-only datasets: WebQuestions (Berant et al., 2013), WebQSP (Yih et al., 2016), WebComplex (Talmor and Berant, 2018), MetaQA (Zhang et al., 2018), WikiTableQuestion (Pasupat and Liang, 2015). 2) Text-only datasets with single passage: like SQuAD (Rajpurkar et al., 2016), DROP (Dua et al., 2019). 3) open-domain Text-Only dataset: TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), Natural Questions (Kwiatkowski et al., 2019). tion. The dataset consists of roughly 70K questionanswering pairs aligned with 13,000 Wikipedia tables. As Wikitables (Bhagavatula et al., 2013) are curated from high-standard professionals to organize a set of information regarding a given theme, its information is mostly absent in the text. Such a complementary nature makes WikiTables an ideal environment for hybrid question answering. To ensure that the answers cannot be hack"
2020.findings-emnlp.91,D19-1242,0,0.0937105,"corpus has in general better coverage while structured data has better compositionality to handle complex multi-hop questions. Due to the advantages of different representation forms, people like to combine them in real world applications. Therefore, it is sometime not ideal to assume the question has answer in a passage. This paper aims to simulate a more realistic setting where the evidences are distributed into heterogeneous data, and the model requires to aggregate information from different forms for answering a question. There has been some pioneering work on building hybrid QA systems (Sun et al., 2019, 2018; Xiong et al., 2019). These methods adopts KB-only datasets (Berant et al., 2013; Yih et al., 2015; Talmor and Berant, 2018) to simulate a hybrid setting by randomly masking KB triples and replace them with text corpus. Experimental results have proved decent improvement, which shed lights on the potential of hybrid question answering systems to integrate heterogeneous information. Though there already exist numerous valuable questions answering datasets as listed in Table 1, these datasets were initially designed to use either structured or unstructured information during annotation. T"
2020.findings-emnlp.91,D18-1455,0,0.0256627,"red tables are also an interesting form. Different datasets like WikiTableQuestions (Pasupat and Liang, 2015), WikiSQL (Zhong et al., 2017), SPIDER (Yu et al., 2018), TabFact (Chen et al., 2020) have been proposed to build models which can interact with such structured information. However, both KB and tables are known to suffer from low coverage issues. Therefore, H YBRID QA combine tables with text as complementary information to answer natural questions. Hybrid QA There are some pioneering studies on designing hybrid question answering systems to aggregate heterogeneous information. GRAFT (Sun et al., 2018) proposes to use the early fusion system and use heuristics to build a questionspecific subgraph that contains sentences from corpus and entities, facts from KB. PullNet (Sun et al., 2019) improves over GRAFT to use an integrated framework that dynamically learns to retrieve and reason over heterogeneous information to find the best answers. More recently, KAReader (Xiong et al., 2019) proposes to reformulate the questions in latent space by reading retrieved text snippets under KB-incomplete cases. These models simulate a ‘fake’ KB-incomplete scenario by masking out triples from KB. In contra"
2020.findings-emnlp.91,N18-1059,0,0.350666,"ns. Due to the advantages of different representation forms, people like to combine them in real world applications. Therefore, it is sometime not ideal to assume the question has answer in a passage. This paper aims to simulate a more realistic setting where the evidences are distributed into heterogeneous data, and the model requires to aggregate information from different forms for answering a question. There has been some pioneering work on building hybrid QA systems (Sun et al., 2019, 2018; Xiong et al., 2019). These methods adopts KB-only datasets (Berant et al., 2013; Yih et al., 2015; Talmor and Berant, 2018) to simulate a hybrid setting by randomly masking KB triples and replace them with text corpus. Experimental results have proved decent improvement, which shed lights on the potential of hybrid question answering systems to integrate heterogeneous information. Though there already exist numerous valuable questions answering datasets as listed in Table 1, these datasets were initially designed to use either structured or unstructured information during annotation. There is no guarantee that these questions need to aggregate heterogeneous information to find the answer. Therefore, designing hybr"
2020.findings-emnlp.91,P19-1417,1,0.902073,"tter coverage while structured data has better compositionality to handle complex multi-hop questions. Due to the advantages of different representation forms, people like to combine them in real world applications. Therefore, it is sometime not ideal to assume the question has answer in a passage. This paper aims to simulate a more realistic setting where the evidences are distributed into heterogeneous data, and the model requires to aggregate information from different forms for answering a question. There has been some pioneering work on building hybrid QA systems (Sun et al., 2019, 2018; Xiong et al., 2019). These methods adopts KB-only datasets (Berant et al., 2013; Yih et al., 2015; Talmor and Berant, 2018) to simulate a hybrid setting by randomly masking KB triples and replace them with text corpus. Experimental results have proved decent improvement, which shed lights on the potential of hybrid question answering systems to integrate heterogeneous information. Though there already exist numerous valuable questions answering datasets as listed in Table 1, these datasets were initially designed to use either structured or unstructured information during annotation. There is no guarantee that t"
2020.findings-emnlp.91,D18-1259,0,0.0513007,"70 #Ques 69,611 #Links/Table 44 #Words/Ques 18.9 Table 2: Statistics of Table and Passage in our dataset. Table 1: Comparison of existing datasets, where #docs means the number of documents provided for a specific question. 1) KB-only datasets: WebQuestions (Berant et al., 2013), WebQSP (Yih et al., 2016), WebComplex (Talmor and Berant, 2018), MetaQA (Zhang et al., 2018), WikiTableQuestion (Pasupat and Liang, 2015). 2) Text-only datasets with single passage: like SQuAD (Rajpurkar et al., 2016), DROP (Dua et al., 2019). 3) open-domain Text-Only dataset: TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), Natural Questions (Kwiatkowski et al., 2019). tion. The dataset consists of roughly 70K questionanswering pairs aligned with 13,000 Wikipedia tables. As Wikitables (Bhagavatula et al., 2013) are curated from high-standard professionals to organize a set of information regarding a given theme, its information is mostly absent in the text. Such a complementary nature makes WikiTables an ideal environment for hybrid question answering. To ensure that the answers cannot be hacked by singlehop or homogeneous models, we carefully employ different strategies to calibrate the annotation process. An"
2020.findings-emnlp.91,P15-1128,0,0.0587257,"huge success in proving that the deep learning model can show strong competence in terms of understanding textonly evidence. Unlike these datasets, H YBRID QA leverages structured information in the evidence form, where the existing models are not able to handle, which distinguishes it from the other datasets. KB/Table-Based QA Structured knowledge is known as unambiguous and compositional, which absorbs lots of attention to the QA system built on KB/Tables. There have been multiple datasets like WebQuestion (Berant et al., 2013), ComplexWebQuestions (Talmor and Berant, 2018), WebQuestionSP (Yih et al., 2015) on using FreeBase to answer natural questions. Besides KB, structured or semi-structured tables are also an interesting form. Different datasets like WikiTableQuestions (Pasupat and Liang, 2015), WikiSQL (Zhong et al., 2017), SPIDER (Yu et al., 2018), TabFact (Chen et al., 2020) have been proposed to build models which can interact with such structured information. However, both KB and tables are known to suffer from low coverage issues. Therefore, H YBRID QA combine tables with text as complementary information to answer natural questions. Hybrid QA There are some pioneering studies on desig"
2020.findings-emnlp.91,P16-2033,0,0.0384552,"Missing"
2020.findings-emnlp.91,D18-1425,0,0.0724362,"e to handle, which distinguishes it from the other datasets. KB/Table-Based QA Structured knowledge is known as unambiguous and compositional, which absorbs lots of attention to the QA system built on KB/Tables. There have been multiple datasets like WebQuestion (Berant et al., 2013), ComplexWebQuestions (Talmor and Berant, 2018), WebQuestionSP (Yih et al., 2015) on using FreeBase to answer natural questions. Besides KB, structured or semi-structured tables are also an interesting form. Different datasets like WikiTableQuestions (Pasupat and Liang, 2015), WikiSQL (Zhong et al., 2017), SPIDER (Yu et al., 2018), TabFact (Chen et al., 2020) have been proposed to build models which can interact with such structured information. However, both KB and tables are known to suffer from low coverage issues. Therefore, H YBRID QA combine tables with text as complementary information to answer natural questions. Hybrid QA There are some pioneering studies on designing hybrid question answering systems to aggregate heterogeneous information. GRAFT (Sun et al., 2018) proposes to use the early fusion system and use heuristics to build a questionspecific subgraph that contains sentences from corpus and entities, f"
2020.lrec-1.747,P17-1171,0,0.0116115,"essing Preprocessing usually includes tokenization, stemming, and generalization or weighting words. To convert tokenized texts into features, Term Frequency-Inverse Document Frequency (TF-IDF) and Linguistic Inquiry and Word Count (LIWC) are frequently used. For word sequences, pre-learned word embedding vectors such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) are commonly used. When using entire articles as inputs, an additional preprocessing step is to identify the central claims from raw texts. Thorne et al. (2018) rank the sentences using TFIDF and DrQA system (Chen et al., 2017). These operations are closely related to subtasks, such as word embeddings, named entity recognition, disambiguation or coreference resolution. 5.2. Machine Learning Models As mentioned in Section 3., the majority of existing research uses supervised methods while semi-supervised or unsupervised methods are less commonly used. In this section, we mainly describe classification models with several actual examples. 5.2.1. Non-Neural Network Models Support Vector Machine (SVM) and Naive Bayes Classifier (NBC) are frequently used classification models (Conroy et al., 2015; Khurana and Intelligent"
2020.lrec-1.747,N16-1138,0,0.0606915,", mostly true, pants on fire}, note that the history counts only include the history for inaccurate statements. unverified pieces of information at the time of posting. In other words, rumor must contain information that can be verified rather than subjective opinions or feelings. 2.3. Stance Detection Stance detection is the task of assessing what side of debate an author is on from text. It is different from fake news detection in that it is not for veracity but consistency. Stance detection can be a subtask of fake news detection since it can be applied to searching documents for evidence (Ferreira and Vlachos, 2016). PHEME, one of the fakenews datasets has tweets related to news, capturing the behavior of users who trust or untrust. 2.4. Sentiment Analysis Sentiment analysis is the task of extracting emotions, such as customers’ favorable or unfavorable impression of a restaurant. Different from rumor detection and fake news detection, sentiment analysis is not to do an objective verification of claim but to analyze personal emotions. 3. Task Formulations In Section 2., we compared related problems with fake news detection to define the scope of this survey. In this survey, The general goal of fake news"
2020.lrec-1.747,W18-5516,0,0.0215588,"Missing"
2020.lrec-1.747,C18-1131,0,0.0453471,"Missing"
2020.lrec-1.747,D14-1181,0,0.0106799,"Missing"
2020.lrec-1.747,I17-2043,0,0.327385,"Missing"
2020.lrec-1.747,P09-2078,0,0.349549,"Missing"
2020.lrec-1.747,P14-1095,0,0.0185534,"class labels, and those labels are learned as independent labels with i.i.d assumptions (Rashkin et al., 2017; Wang, 2017). One of the conditions for fake news classifiers to achieve good performances is to have sufficient labeled data. However, to obtain reliable labels requires a lot of time and labor. Therefore, semi/weakly-supervised and unsupervised methods are proposed (Rubin and Vashchilko, 2012; Bhattacharjee et al., 2017). 3.2. Regression Fake news detection can also be formulated as a regression task, where the output is a numeric score of truthfulness. 6087 This approach is used by Nakashole and Mitchell (2014). Usually, evaluation is done by calculating the difference between the predicted scores and the ground truth scores or using Pearson/Spearman Correlations. However, since the available datasets have discrete ground truth scores, the challenge here is how to convert the discrete labels to numeric scores. 4. Datasets A significant challenge for automated fake news detection is the availability and quality of the datasets. We categorize public fake-news datasets into three categories: claims, entire articles, and Social Networking Services (SNS) data. Claims are one or a few sentences including"
2020.lrec-1.747,D14-1162,0,0.0850977,"Missing"
2020.lrec-1.747,D17-1317,0,0.482196,"ulate the fake news detection as a binary classification problem. However, categorizing all the news into two classes (fake or real) is difficult because there are cases where the news is partially real and partially fake. To address this problem, adding additional classes is common practice. Mainly, a category for the news which is neither completely real nor completely fake, or, more than two degrees of truthfulness are set as additional classes. When using these datasets, the expected outputs are multi-class labels, and those labels are learned as independent labels with i.i.d assumptions (Rashkin et al., 2017; Wang, 2017). One of the conditions for fake news classifiers to achieve good performances is to have sufficient labeled data. However, to obtain reliable labels requires a lot of time and labor. Therefore, semi/weakly-supervised and unsupervised methods are proposed (Rubin and Vashchilko, 2012; Bhattacharjee et al., 2017). 3.2. Regression Fake news detection can also be formulated as a regression task, where the output is a numeric score of truthfulness. 6087 This approach is used by Nakashole and Mitchell (2014). Usually, evaluation is done by calculating the difference between the predicte"
2020.lrec-1.747,W12-0415,0,0.0239888,"ce. Mainly, a category for the news which is neither completely real nor completely fake, or, more than two degrees of truthfulness are set as additional classes. When using these datasets, the expected outputs are multi-class labels, and those labels are learned as independent labels with i.i.d assumptions (Rashkin et al., 2017; Wang, 2017). One of the conditions for fake news classifiers to achieve good performances is to have sufficient labeled data. However, to obtain reliable labels requires a lot of time and labor. Therefore, semi/weakly-supervised and unsupervised methods are proposed (Rubin and Vashchilko, 2012; Bhattacharjee et al., 2017). 3.2. Regression Fake news detection can also be formulated as a regression task, where the output is a numeric score of truthfulness. 6087 This approach is used by Nakashole and Mitchell (2014). Usually, evaluation is done by calculating the difference between the predicted scores and the ground truth scores or using Pearson/Spearman Correlations. However, since the available datasets have discrete ground truth scores, the challenge here is how to convert the discrete labels to numeric scores. 4. Datasets A significant challenge for automated fake news detection"
2020.lrec-1.747,C18-1283,0,0.0141541,"egorize and summarize available datasets, NLP approaches, and results, providing first-hand experiences and accessible introductions for new researchers interested in this problem. 2. Related Problems 2.1. Fact-Checking Fact-checking is the task of assessing the truthfulness of claims made by public figures such as politicians, pundits, etc (Vlachos and Riedel, 2014). Many researchers do not distinguish fake news detection and fact-checking since both of them are to assess the truthfulness of claims. Generally, fake news detection usually focuses on news events while fact-checking is broader. Thorne and Vlachos (2018) provides a comprehensive review of this topic. 2.2. Rumor Detection There is not a consistent definition of rumor detection. A recent survey (Zubiaga et al., 2018) defines rumor detection as separating personal statements into rumor or nonrumor, where rumor is defined as a statement consisting of 6086 Name LIAR FEVER BUZZFEEDNEWS BUZZFACE SOME - LIKE - IT- HOAX PHEME CREDBANK FAKENEWSNET BS DETECTOR Main Input short claim short claim FB post FB post FB post Tweet Tweet article article Data Size 12,836 185,445 2,282 2,263 15,500 330 60 million 23,921 - Label six-grade three-grade four-grade fo"
2020.lrec-1.747,N18-1074,0,0.13996,"re published, which can use as an improved version of the first two. A recent benchmark dataset for fake news detection is LIAR (Wang, 2017). This dataset collected data from Politifact as Vlachos and Riedel (2014), but includes 12,836 real-world short statements, and each statement is labeled with six-grade truthfulness. The information about the subjects, party, context, and speakers are also included in this dataset. For the datasets from Politifact articles, Rashkin et al. (2017) also published large datasets. They collect articles from PunditFact (Politifact’s spin-off site) too. F ever (Thorne et al., 2018) is a dataset providing related evidence for fact-checking. In this point, it is similar to E MERGENT. Fever contains 185,445 claims generated from Wikipedia data. Each statement is labeled as Supported, Refuted, or Not Enough Info. They also marked which sentences from Wikipedia they use as evidence. Fever makes it possible to develop a system that can predict the truthfulness of a claim together with the evidence, even though the type of facts and evidence from Wikipedia may still exhibit some major stylistic differences from those in real-world political campaigns. 4.2. Entire-Article Datas"
2020.lrec-1.747,W14-2508,0,0.151457,"ural Language Processing solutions for automatic fake news detection; • We systematically analyze how fake news detection is aligned with existing NLP tasks, and discuss the assumptions and notable issues for different formulations of the problem; • We categorize and summarize available datasets, NLP approaches, and results, providing first-hand experiences and accessible introductions for new researchers interested in this problem. 2. Related Problems 2.1. Fact-Checking Fact-checking is the task of assessing the truthfulness of claims made by public figures such as politicians, pundits, etc (Vlachos and Riedel, 2014). Many researchers do not distinguish fake news detection and fact-checking since both of them are to assess the truthfulness of claims. Generally, fake news detection usually focuses on news events while fact-checking is broader. Thorne and Vlachos (2018) provides a comprehensive review of this topic. 2.2. Rumor Detection There is not a consistent definition of rumor detection. A recent survey (Zubiaga et al., 2018) defines rumor detection as separating personal statements into rumor or nonrumor, where rumor is defined as a statement consisting of 6086 Name LIAR FEVER BUZZFEEDNEWS BUZZFACE SO"
2020.lrec-1.747,P17-2067,1,0.959738,"tection as a binary classification problem. However, categorizing all the news into two classes (fake or real) is difficult because there are cases where the news is partially real and partially fake. To address this problem, adding additional classes is common practice. Mainly, a category for the news which is neither completely real nor completely fake, or, more than two degrees of truthfulness are set as additional classes. When using these datasets, the expected outputs are multi-class labels, and those labels are learned as independent labels with i.i.d assumptions (Rashkin et al., 2017; Wang, 2017). One of the conditions for fake news classifiers to achieve good performances is to have sufficient labeled data. However, to obtain reliable labels requires a lot of time and labor. Therefore, semi/weakly-supervised and unsupervised methods are proposed (Rubin and Vashchilko, 2012; Bhattacharjee et al., 2017). 3.2. Regression Fake news detection can also be formulated as a regression task, where the output is a numeric score of truthfulness. 6087 This approach is used by Nakashole and Mitchell (2014). Usually, evaluation is done by calculating the difference between the predicted scores and"
2020.lrec-1.747,D18-1010,0,0.0254716,"Missing"
2020.lrec-1.755,D17-1070,0,0.0191086,"lengths of our text data. Figure 2 shows the proportion of samples per text length for both Fakeddit and FEVER. It can be seen that our dataset contains a higher proportion of longer text starting from word lengths of around 17, while FEVER’s captions peak at around 10 words. In addition, while FEVER’s peak is very sharp, Fakeddit has a much smaller and more gradual slope. Fakeddit also provides a broader diversity of text lengths, with samples containing almost 100 words. Experiments Fake News Detection Multiple methods were employed for text and image feature extraction. We used InferSent (Conneau et al., 2017) and BERT (Devlin et al., 2019) to generate text embeddings for the title of the Reddit submissions. VGG16 (Simonyan and Zisserman, 2015), EfficientNet (Tan and Le, 2019), and ResNet50 (He et al., 2016) were utilized to extract the features of the Reddit submission thumbnails. We used the InferSent model because it performs very well as a universal sentence embeddings generator. For this model, we loaded a vocabulary of 1 million of the most common words in English and used fastText embeddings (Joulin et al., 2017). We obtained encoded sentence features of length 4096 for each submission title"
2020.lrec-1.755,N19-1423,0,0.0309524,"e 2 shows the proportion of samples per text length for both Fakeddit and FEVER. It can be seen that our dataset contains a higher proportion of longer text starting from word lengths of around 17, while FEVER’s captions peak at around 10 words. In addition, while FEVER’s peak is very sharp, Fakeddit has a much smaller and more gradual slope. Fakeddit also provides a broader diversity of text lengths, with samples containing almost 100 words. Experiments Fake News Detection Multiple methods were employed for text and image feature extraction. We used InferSent (Conneau et al., 2017) and BERT (Devlin et al., 2019) to generate text embeddings for the title of the Reddit submissions. VGG16 (Simonyan and Zisserman, 2015), EfficientNet (Tan and Le, 2019), and ResNet50 (He et al., 2016) were utilized to extract the features of the Reddit submission thumbnails. We used the InferSent model because it performs very well as a universal sentence embeddings generator. For this model, we loaded a vocabulary of 1 million of the most common words in English and used fastText embeddings (Joulin et al., 2017). We obtained encoded sentence features of length 4096 for each submission title using InferSent. In addition,"
2020.lrec-1.755,E17-2068,0,0.0330586,"ethods were employed for text and image feature extraction. We used InferSent (Conneau et al., 2017) and BERT (Devlin et al., 2019) to generate text embeddings for the title of the Reddit submissions. VGG16 (Simonyan and Zisserman, 2015), EfficientNet (Tan and Le, 2019), and ResNet50 (He et al., 2016) were utilized to extract the features of the Reddit submission thumbnails. We used the InferSent model because it performs very well as a universal sentence embeddings generator. For this model, we loaded a vocabulary of 1 million of the most common words in English and used fastText embeddings (Joulin et al., 2017). We obtained encoded sentence features of length 4096 for each submission title using InferSent. In addition, we used the BERT model. BERT achieves state-of-the-art results on many classification tasks, including Q&A and named entity recognition. To obtain fixed-length BERT embedding vectors, we used the bert-as-service(Xiao, 2018) tool, to map variable-length text/sentences into a 768 element array for each Reddit submission title. For our experiments, we utilized the pretrained BERT-Large, Uncased model. We employed VGG16, ResNet50, and EfficientNet models for encoding images. VGG16 and Res"
2020.lrec-1.755,P19-2050,0,0.0124463,"ta. Only four datasets contain over half a million samples, with CREDBANK (Mitra and Gilbert, 2015) and FakeNewsCorpus5 being the largest, both containing millions of samples. In addition, many of the datasets separate their data into a small number of classes, such as fake vs. true. Datasets such as NELA-GT2018 (Nørregaard et al., 2019), LIAR (Wang, 2017), and FakeNewsCorpus provide more fine-grained labels. While some datasets include data from a variety of categories (Zubiaga et al., 2016), many contain data from specific areas, such as politics and celebrity gossip (Tacchini et al., 2017; Pathak and Srihari, 2019; Shu et al., 2018; Abu Salem et al., 2019; Santia and Williams, 2018)6 . These data samples may contain limited scopes of context and styles of writing due to their limited number of categories. 5 https://github.com/several27/FakeNewsCorpus https://github.com/BuzzFeedNews/2016-10-facebook-factcheck 6 2.2. Image Datasets Most of the existing fake news datasets collect only text data. However, fake news can also come in the form of images. Existing fake image datasets are limited in size and diversity, making dataset research in this area important. Image features supply models with more data t"
2020.lrec-1.755,N18-1074,0,0.0299208,"aset by expanding the size and time range as well as including text data and other metadata. This expanded data makes up only two of 22 sources of data present in our research. The image-verification-corpus (Boididou et al., 2018), like ours, contains both text and image data. While it does contain a larger number of samples than other conventional datasets, it still pales in comparison to Fakeddit. 2.3. Fact-Checking Due to the unique aspect of multimodality, Fakeddit can also be applied to the realm of implicit fact-checking. Other existing datasets utilized for fact-checking include FEVER (Thorne et al., 2018) and Fauxtography (Zlatkova et al., 2019). The former consists of altered claims utilized for textual verification. The latter utilizes both text and image data in order to fact-check claims about images. Using both text and image data, researchers can use Fakeddit for verifying truth and proof: utilizing image data as evidence for text truthfulness or using the text data as evidence for image truthfulness. Compared to other existing datasets, Fakeddit provides a larger breadth of novel features that can be applied in a number of applications: fake news text, image, text+image classification a"
2020.lrec-1.755,P17-2067,1,0.848946,"r specific characteristics. 2.1. Text Datasets When comparing fake news datasets, a few trends can be seen. Most of the datasets are small in size, which can be ineffective for current machine learning models that require large quantities of training data. Only four datasets contain over half a million samples, with CREDBANK (Mitra and Gilbert, 2015) and FakeNewsCorpus5 being the largest, both containing millions of samples. In addition, many of the datasets separate their data into a small number of classes, such as fake vs. true. Datasets such as NELA-GT2018 (Nørregaard et al., 2019), LIAR (Wang, 2017), and FakeNewsCorpus provide more fine-grained labels. While some datasets include data from a variety of categories (Zubiaga et al., 2016), many contain data from specific areas, such as politics and celebrity gossip (Tacchini et al., 2017; Pathak and Srihari, 2019; Shu et al., 2018; Abu Salem et al., 2019; Santia and Williams, 2018)6 . These data samples may contain limited scopes of context and styles of writing due to their limited number of categories. 5 https://github.com/several27/FakeNewsCorpus https://github.com/BuzzFeedNews/2016-10-facebook-factcheck 6 2.2. Image Datasets Most of the"
2020.lrec-1.755,D19-1216,0,0.0263224,"ge as well as including text data and other metadata. This expanded data makes up only two of 22 sources of data present in our research. The image-verification-corpus (Boididou et al., 2018), like ours, contains both text and image data. While it does contain a larger number of samples than other conventional datasets, it still pales in comparison to Fakeddit. 2.3. Fact-Checking Due to the unique aspect of multimodality, Fakeddit can also be applied to the realm of implicit fact-checking. Other existing datasets utilized for fact-checking include FEVER (Thorne et al., 2018) and Fauxtography (Zlatkova et al., 2019). The former consists of altered claims utilized for textual verification. The latter utilizes both text and image data in order to fact-check claims about images. Using both text and image data, researchers can use Fakeddit for verifying truth and proof: utilizing image data as evidence for text truthfulness or using the text data as evidence for image truthfulness. Compared to other existing datasets, Fakeddit provides a larger breadth of novel features that can be applied in a number of applications: fake news text, image, text+image classification as well as implicit fact-checking. Other d"
2020.nlpcovid19-acl.15,D11-1145,0,0.0558012,"sed for modeling disease propagation comes from social media. This can provide researchers with access to unfiltered data with clues as to how the pandemic evolves. Current research on the COVID-19 outbreak concerning social media includes word frequency and sentiment analysis of tweets (Rajput et al., 2020) and studies on the spread of misinformation (Kouzy et al., 2020; Singh et al., 2020). Social media has also been utilized for other disease predictions. Several papers propose models to identify tweets in which the author or nearby person has the attributed disease (Kanouchi et al., 2015; Aramaki et al., 2011; Lamb et al., 2013; Kitagawa et al., 2015). Iso et al. (2016) and Huang et al. (2016) utilize word frequencies to align tweets to disease rates. A shortcoming of the above models is they do not consider how one region’s outbreak may relate to another. Many of the proposed models also rely on lengthy keyword lists or syntactic features that may not generalize across languages. Text embeddings from models such as multilingual BERT (mBERT) (Devlin et al., 2019) and LASER (Artetxe and Schwenk, 2019) can allow us to combine features and make connections across languages for semantic alignment. We"
2020.nlpcovid19-acl.15,Q19-1038,0,0.0134814,"identify tweets in which the author or nearby person has the attributed disease (Kanouchi et al., 2015; Aramaki et al., 2011; Lamb et al., 2013; Kitagawa et al., 2015). Iso et al. (2016) and Huang et al. (2016) utilize word frequencies to align tweets to disease rates. A shortcoming of the above models is they do not consider how one region’s outbreak may relate to another. Many of the proposed models also rely on lengthy keyword lists or syntactic features that may not generalize across languages. Text embeddings from models such as multilingual BERT (mBERT) (Devlin et al., 2019) and LASER (Artetxe and Schwenk, 2019) can allow us to combine features and make connections across languages for semantic alignment. We present an analysis of Twitter usage for crosslingual COVID-19 outbreak alignment. We study the ability to correlate social media tweets across languages and countries in a pandemic scenario. Based on this demonstration, researchers can study various cross-cultural reactions to the pandemic on social media. We aim to analyze how one country’s tweets align with its own outbreak and if those same tweets can be used to predict the state of another country. This can allow us to determine how actions"
2020.nlpcovid19-acl.15,N19-1423,0,0.0142657,"Several papers propose models to identify tweets in which the author or nearby person has the attributed disease (Kanouchi et al., 2015; Aramaki et al., 2011; Lamb et al., 2013; Kitagawa et al., 2015). Iso et al. (2016) and Huang et al. (2016) utilize word frequencies to align tweets to disease rates. A shortcoming of the above models is they do not consider how one region’s outbreak may relate to another. Many of the proposed models also rely on lengthy keyword lists or syntactic features that may not generalize across languages. Text embeddings from models such as multilingual BERT (mBERT) (Devlin et al., 2019) and LASER (Artetxe and Schwenk, 2019) can allow us to combine features and make connections across languages for semantic alignment. We present an analysis of Twitter usage for crosslingual COVID-19 outbreak alignment. We study the ability to correlate social media tweets across languages and countries in a pandemic scenario. Based on this demonstration, researchers can study various cross-cultural reactions to the pandemic on social media. We aim to analyze how one country’s tweets align with its own outbreak and if those same tweets can be used to predict the state of another country. This"
2020.nlpcovid19-acl.15,U16-1004,0,0.0244024,"ers with access to unfiltered data with clues as to how the pandemic evolves. Current research on the COVID-19 outbreak concerning social media includes word frequency and sentiment analysis of tweets (Rajput et al., 2020) and studies on the spread of misinformation (Kouzy et al., 2020; Singh et al., 2020). Social media has also been utilized for other disease predictions. Several papers propose models to identify tweets in which the author or nearby person has the attributed disease (Kanouchi et al., 2015; Aramaki et al., 2011; Lamb et al., 2013; Kitagawa et al., 2015). Iso et al. (2016) and Huang et al. (2016) utilize word frequencies to align tweets to disease rates. A shortcoming of the above models is they do not consider how one region’s outbreak may relate to another. Many of the proposed models also rely on lengthy keyword lists or syntactic features that may not generalize across languages. Text embeddings from models such as multilingual BERT (mBERT) (Devlin et al., 2019) and LASER (Artetxe and Schwenk, 2019) can allow us to combine features and make connections across languages for semantic alignment. We present an analysis of Twitter usage for crosslingual COVID-19 outbreak alignment. We"
2020.nlpcovid19-acl.15,C16-1008,0,0.0886017,"Missing"
2020.nlpcovid19-acl.15,P15-1160,0,0.0182087,"formation that can be used for modeling disease propagation comes from social media. This can provide researchers with access to unfiltered data with clues as to how the pandemic evolves. Current research on the COVID-19 outbreak concerning social media includes word frequency and sentiment analysis of tweets (Rajput et al., 2020) and studies on the spread of misinformation (Kouzy et al., 2020; Singh et al., 2020). Social media has also been utilized for other disease predictions. Several papers propose models to identify tweets in which the author or nearby person has the attributed disease (Kanouchi et al., 2015; Aramaki et al., 2011; Lamb et al., 2013; Kitagawa et al., 2015). Iso et al. (2016) and Huang et al. (2016) utilize word frequencies to align tweets to disease rates. A shortcoming of the above models is they do not consider how one region’s outbreak may relate to another. Many of the proposed models also rely on lengthy keyword lists or syntactic features that may not generalize across languages. Text embeddings from models such as multilingual BERT (mBERT) (Devlin et al., 2019) and LASER (Artetxe and Schwenk, 2019) can allow us to combine features and make connections across languages for s"
2020.nlpcovid19-acl.15,P15-3005,0,0.0144968,"s from social media. This can provide researchers with access to unfiltered data with clues as to how the pandemic evolves. Current research on the COVID-19 outbreak concerning social media includes word frequency and sentiment analysis of tweets (Rajput et al., 2020) and studies on the spread of misinformation (Kouzy et al., 2020; Singh et al., 2020). Social media has also been utilized for other disease predictions. Several papers propose models to identify tweets in which the author or nearby person has the attributed disease (Kanouchi et al., 2015; Aramaki et al., 2011; Lamb et al., 2013; Kitagawa et al., 2015). Iso et al. (2016) and Huang et al. (2016) utilize word frequencies to align tweets to disease rates. A shortcoming of the above models is they do not consider how one region’s outbreak may relate to another. Many of the proposed models also rely on lengthy keyword lists or syntactic features that may not generalize across languages. Text embeddings from models such as multilingual BERT (mBERT) (Devlin et al., 2019) and LASER (Artetxe and Schwenk, 2019) can allow us to combine features and make connections across languages for semantic alignment. We present an analysis of Twitter usage for cr"
2020.nlpcovid19-acl.15,N13-1097,0,0.0523434,"Missing"
2020.paclic-1.39,T75-2012,0,0.526851,"et al., 2012; Ullman and Pullman, 2015; Walenski et al., 2014), but scarcely investigated in the aging population (Rieckmann and Bäckman, 2009). In the current research, we aimed to investigate whether and how declarative memory associates with the rule-based syntactic process, especially the syntactic complexity in Chinese older adults via delving into its two component subcategories, namely episodic memory and semantic memory as described below. 1.2 Episodic Memory and Semantic Memory Declarative memory can be divided into episodic memory and semantic memory (Greenberg and Verfaellie, 2010; Tulving, 1972). Episodic memory refers to the collection of specific events or experiences which entails the personal realization of what was happening and can be reconstructed vividly with many details from the individuals’ perspective (Garrard et al., 1997). That is to say, episodic memory includes the exact contextual information, such as times, locations, persons, and related feelings, and most individuals regard themselves as the actors or contemporary witnesses in these autobiographic events. Therefore, episodic memory involves the emotional charge and the whole context surrounding an occurrence, rath"
2021.acl-long.339,P19-1601,0,0.0144492,"with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to create new (response) content. And the key is content relevance to the dialogue history, rather than content preservation of the input. Second, the data for text style transfer is isomorphic. Data in different styles are in the same free-text format. However, our conversational data are context-response pairs while th"
2021.acl-long.339,N19-1423,0,0.0329375,"Missing"
2021.acl-long.339,N19-1125,0,0.0494538,"Missing"
2021.acl-long.339,D19-1190,0,0.278819,"onversational systems (Gan et al., 2017). Currently, most of the existing parallel datasets are not stylistically consistent. Samples in these datasets are usually contributed by a variety of users, resulting in an averaging effect across style characteristics (Zhang et al., 2018a). Meanwhile, constructing a parallel stylistic dataset for training the open-domain conversational agents is both labor-intensive and time-consuming. Recent studies show the effect of stylizing responses using a monolingual dataset in the desired style and a conventional conversational dataset (Niu and Bansal, 2018; Gao et al., 2019b). However, increasing style intensity often leads to Corresponding author. Style Intensity S2S: I just want to rent a room. Style Fusion: I hope I can share. S2S+LM: My friend had a considerable share in clearing the matter up. Ours: I should prefer having a partner to being alone. Figure 1: An example of responses generated by S2S, S2S+LM (Niu and Bansal, 2018), Style Fusion (Gao et al., 2019b), and our approach, targeting the Holmes style, which is quite formal and polite. Introduction ∗ Content Relevance the expense of decreasing content relevance between dialogue history and response. As"
2021.acl-long.339,N19-1320,0,0.015434,"4.5 and subsequently combine the content with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to create new (response) content. And the key is content relevance to the dialogue history, rather than content preservation of the input. Second, the data for text style transfer is isomorphic. Data in different styles are in the same free-text format. However, our conversational"
2021.acl-long.339,W19-5944,0,0.0241973,"Missing"
2021.acl-long.339,P19-1041,0,0.0187464,"An example of responses generated by baselines and our approach. in our approach. This is because most of the style information is disentangled into Z s in our approach, making its distribution different from sub-sequences of Z c . 4.5 and subsequently combine the content with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to create new (response) content. And the key is"
2021.acl-long.339,W16-6010,0,0.0273768,"and style in latent space by diluting sentence-level information in style representations. Combining the desired style representation and a response content representation will then obtain a stylistic response. Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores, compared with baselines. Human evaluation results show that our approach significantly improves style intensity and maintains content relevance. 1 Linguistic style is an essential aspect of natural language interaction and provides particular ways of using language to engage with the audiences (Kabbara and Cheung, 2016). In human-bot conversations, it is crucial to generate stylistic responses for increasing user engagement to conversational systems (Gan et al., 2017). Currently, most of the existing parallel datasets are not stylistically consistent. Samples in these datasets are usually contributed by a variety of users, resulting in an averaging effect across style characteristics (Zhang et al., 2018a). Meanwhile, constructing a parallel stylistic dataset for training the open-domain conversational agents is both labor-intensive and time-consuming. Recent studies show the effect of stylizing responses usi"
2021.acl-long.339,P18-1139,0,0.0493674,"Missing"
2021.acl-long.339,N16-1014,0,0.0876725,"Missing"
2021.acl-long.339,N18-1169,0,0.0593197,"Missing"
2021.acl-long.339,I17-1099,0,0.0634669,"orpus-level feature since sentences within a dataset have the same style. In contrast, the content representation is a sentence-level feature decided by a sentence itself. We thus disentangle the content and style by diluting sentence-level information in the style representation. This encourages the encoding of content information into the content representation. Otherwise, the content information will be corrupted in the style representation, making it hard to reconstruct the original content in the subsequent decoding process. We conduct experiments on DailyDialogue conversational dataset (Li et al., 2017) and Holmes monolingual stylistic dataset (Gao et al., 2019b). Experimental results show that our proposed approach improves style intensity and maintains content relevance. Our contributions are listed below: • We propose a unified framework to simultaneously improve style intensity and maintain content relevance for neural stylistic response generation. • We introduce a scheme of learning latent variables by a diluting strategy to disentangle the style and content. • Experimental results show that our approach achieves higher performance in style intensity without decreasing content relevanc"
2021.acl-long.339,D16-1230,0,0.0685257,"Missing"
2021.acl-long.339,Q18-1027,0,0.0959872,"g user engagement to conversational systems (Gan et al., 2017). Currently, most of the existing parallel datasets are not stylistically consistent. Samples in these datasets are usually contributed by a variety of users, resulting in an averaging effect across style characteristics (Zhang et al., 2018a). Meanwhile, constructing a parallel stylistic dataset for training the open-domain conversational agents is both labor-intensive and time-consuming. Recent studies show the effect of stylizing responses using a monolingual dataset in the desired style and a conventional conversational dataset (Niu and Bansal, 2018; Gao et al., 2019b). However, increasing style intensity often leads to Corresponding author. Style Intensity S2S: I just want to rent a room. Style Fusion: I hope I can share. S2S+LM: My friend had a considerable share in clearing the matter up. Ours: I should prefer having a partner to being alone. Figure 1: An example of responses generated by S2S, S2S+LM (Niu and Bansal, 2018), Style Fusion (Gao et al., 2019b), and our approach, targeting the Holmes style, which is quite formal and polite. Introduction ∗ Content Relevance the expense of decreasing content relevance between dialogue histor"
2021.acl-long.339,P18-1080,0,0.0185419,"forward to getting the information about that. S2S+LM Style Fusion Ours Table 7: An example of responses generated by baselines and our approach. in our approach. This is because most of the style information is disentangled into Z s in our approach, making its distribution different from sub-sequences of Z c . 4.5 and subsequently combine the content with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style tran"
2021.acl-long.339,P18-2031,0,0.028426,"out that. S2S+LM Style Fusion Ours Table 7: An example of responses generated by baselines and our approach. in our approach. This is because most of the style information is disentangled into Z s in our approach, making its distribution different from sub-sequences of Z c . 4.5 and subsequently combine the content with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to creat"
2021.acl-long.339,P15-1152,0,0.0290416,"y. The maximum length is set to 90 for the dialogue history and 30 for the response. During the training process, we use the ADAM optimizer, whose learning rate is 0.0003. σ 2 for sampling  in Equation 8 is 0.12 . Table 2 shows the average running time on a single TITAN X (Pascal) GPU. During the inference process, the weights γ and η for re-ranking are set to 0.5. The weight (accuracy) of n-gram classifier is 0.93, 0.87, 0.77, and 0.65 for n from 1 to 4. The number of candidate responses, Ny , is set to 10. The radius r is set to 3. • S2S, the sequence-to-sequence response generation model (Shang et al., 2015). 4 • S2S+LM, a S2S trained on C and a stylistic language model trained on S (Niu and Bansal, 2018). During the inference process, it generates a stylistic response by interpolating outputs of the two models. Automatic Evaluation Considering that it is unfair to evaluate a response by the classifiers that are used for selecting the response (Song et al., 2020), we fine-tune a BERT (Devlin et al., 2019) to measure style intensity. Concretely, positive samples are the stylistic sentences. Negative samples are 2 3 Results 4.1 http://yanran.li/dailydialog https://github.com/golsun/StyleFusion 4 43"
2021.acl-long.339,2020.coling-main.203,0,0.0411487,"ement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to create new (response) content. And the key is content relevance to the dialogue history, rather than content preservation of the input. Second, the data for text style transfer is isomorphic. Data in different styles are in the same free-text format. However, our conversational data are context-response pairs while the stylistic data are free-texts, which"
2021.acl-long.339,P18-1090,0,0.0284223,"our approach. This is because most of the style information is disentangled into Z s in our approach, making its distribution different from sub-sequences of Z c . 4.5 and subsequently combine the content with the desired style. The disentanglement can be achieved by adversarial learning (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018; Yang et al., 2018; Logeswaran et al., 2018), reinforcement learning (Jain et al., 2019), back-translation (Prabhumoye et al., 2018; Nogueira dos Santos et al., 2018), multi-task learning (John et al., 2019), and removing stylistic phrases (Li et al., 2018; Xu et al., 2018; Zhang et al., 2018b). The other way transfers the style without disentangled representations, for example using generator-evaluator architecture (Gong et al., 2019), cycle reconstruction (Dai et al., 2019), parameter sharing (Wang et al., 2020), and data augmentation (Zhang et al., 2020a). The main difference between our task and text style transfer lies in two aspects. First, all the content to be generated is available in the input in text style transfer, while our task needs to create new (response) content. And the key is content relevance to the dialogue history, rather than content pre"
2021.acl-long.339,2020.findings-emnlp.140,0,0.0647547,"Missing"
2021.acl-long.339,N18-1138,0,0.0591403,"Missing"
2021.acl-long.339,2020.acl-main.294,0,0.253563,"(8) ¯ s remains The batch average style representation Z ¯ s (Sj ) consistent with the target, i.e., being Z AE when the target is Sj . The updated smoothness objective is as follows: c s ¯AE Lsmooth,conv = − log P (Yi |[Zconv :Z (Yi )]), c s ¯ Lsmooth,style = −U log P (Yi |[Zstyle : ZAE (Yi )]) s s s ¯AE Zsoft = ZS2S (Xi ) + α ∗ Z (Sj ), c s ¯AE − (1 − U ) log P (Sj |[Zstyle :Z (Sj )]). (9) The final training loss is the sum of the response generation loss, fusion objective, and smoothness objective: L =LS2S + Lfuse + Lsmooth . (10) Here, we do not employ pre-training models, i.e., DialoGPT (Zhang et al., 2020b) and OpenAI GPT2 (Radford et al., 2019). This is because the disentanglement is usually conducted on a sentence representation. While most of the pre-training models depend on the attention mechanism, and there is no static global sentence representation during the decoding process. 2.4 c (X ) by S2S encoder and subsequently obtain ZS2S i c (X ) sample Z c (Yˆi ) from the hypersphere of ZS2S i with a mannually tuned radius r. After that, we gen¯ s (Sj ), erate Yˆi by concatenating Z c (Yˆi ) and Z AE which is the batch average style representation of randomly sampled stylistic sentences. Con"
2021.acl-long.339,D18-1138,0,0.135572,"s style intensity and maintains content relevance. 1 Linguistic style is an essential aspect of natural language interaction and provides particular ways of using language to engage with the audiences (Kabbara and Cheung, 2016). In human-bot conversations, it is crucial to generate stylistic responses for increasing user engagement to conversational systems (Gan et al., 2017). Currently, most of the existing parallel datasets are not stylistically consistent. Samples in these datasets are usually contributed by a variety of users, resulting in an averaging effect across style characteristics (Zhang et al., 2018a). Meanwhile, constructing a parallel stylistic dataset for training the open-domain conversational agents is both labor-intensive and time-consuming. Recent studies show the effect of stylizing responses using a monolingual dataset in the desired style and a conventional conversational dataset (Niu and Bansal, 2018; Gao et al., 2019b). However, increasing style intensity often leads to Corresponding author. Style Intensity S2S: I just want to rent a room. Style Fusion: I hope I can share. S2S+LM: My friend had a considerable share in clearing the matter up. Ours: I should prefer having a par"
2021.acl-long.339,2020.acl-demos.30,0,0.392173,"(8) ¯ s remains The batch average style representation Z ¯ s (Sj ) consistent with the target, i.e., being Z AE when the target is Sj . The updated smoothness objective is as follows: c s ¯AE Lsmooth,conv = − log P (Yi |[Zconv :Z (Yi )]), c s ¯ Lsmooth,style = −U log P (Yi |[Zstyle : ZAE (Yi )]) s s s ¯AE Zsoft = ZS2S (Xi ) + α ∗ Z (Sj ), c s ¯AE − (1 − U ) log P (Sj |[Zstyle :Z (Sj )]). (9) The final training loss is the sum of the response generation loss, fusion objective, and smoothness objective: L =LS2S + Lfuse + Lsmooth . (10) Here, we do not employ pre-training models, i.e., DialoGPT (Zhang et al., 2020b) and OpenAI GPT2 (Radford et al., 2019). This is because the disentanglement is usually conducted on a sentence representation. While most of the pre-training models depend on the attention mechanism, and there is no static global sentence representation during the decoding process. 2.4 c (X ) by S2S encoder and subsequently obtain ZS2S i c (X ) sample Z c (Yˆi ) from the hypersphere of ZS2S i with a mannually tuned radius r. After that, we gen¯ s (Sj ), erate Yˆi by concatenating Z c (Yˆi ) and Z AE which is the batch average style representation of randomly sampled stylistic sentences. Con"
2021.acl-short.61,P19-1620,0,0.0222415,"fact verification model. We propose a simple yet general framework Question Answering for Claim Generation (QACG) to generate three types of claims from any given evidence: 1) claims that are supported by the evidence, 2) claims that are refuted by the evidence, and 3) claims that the evidence does Not have Enough Information (NEI) to verify. To generate claims, we utilize Question Generation (QG) (Zhao et al., 2018; Liu et al., 2020a; Pan et al., 2020), which aims to automatically ask questions from textual inputs. QG has been shown to benefit various NLP tasks, such as enriching QA corpora (Alberti et al., 2019), checking factual consistency for summarization (Wang et al., 2020), and data augmentation for semantic parsing (Guo et al., 2018). To the best of our knowledge, we are the first to employ QG for fact verification. As illustrated in Figure 1, given a passage P as the evidence, we first employ a Question Generator to generate a question–answer pair (Q, A) for the evidence. We then convert (Q, A) into a claim C (QA-to-Claim) based on the following logical assumptions: a) if P can answer Q and A is the correct answer, then C is a supported claim; b) if P can answer Q but A is an incorrect answer"
2021.acl-short.61,2020.emnlp-main.256,0,0.0354021,"atasets. We evaluate fact verification on three different test sets based on FEVER: 1) FEVER-S/R: Since only the supported and refuted claims are labeled with gold evidence in FEVER, we take the claim–evidence pairs of these two classes from the FEVER test set for evaluation. 2) FEVER-Symmetric: this is a carefullydesigned unbiased test set designed by Schuster et al. (2019) to detect the robustness of the fact verification model. Note that only supported and refuted claims are present in this test set. 3) FEVER-S/R/N: The full FEVER test set are used for a three-class verification. We follow Atanasova et al. (2020) to use the system of Malon (2019) to retrieve evidence sentences for NEI claims. NEI claim generation. We need to generate a question q 0 which is relevant but cannot be answered by P. To this end, we link P back to its original Wikipedia article W and expand the evidence with additional contexts Pext , which are five randomly-retrieved sentences from W that are not present in P. In our example in Figure 1, one additional context retrieved is “By the time the riots ended, 63 people had been killed”. We then concatenate P and Pext as the expanded evidence, based on which we generate a supporte"
2021.acl-short.61,2021.ccl-1.108,0,0.0945055,"Missing"
2021.acl-short.61,N18-1074,0,0.0516085,"Missing"
2021.acl-short.61,2020.acl-main.450,0,0.0268532,"tion Answering for Claim Generation (QACG) to generate three types of claims from any given evidence: 1) claims that are supported by the evidence, 2) claims that are refuted by the evidence, and 3) claims that the evidence does Not have Enough Information (NEI) to verify. To generate claims, we utilize Question Generation (QG) (Zhao et al., 2018; Liu et al., 2020a; Pan et al., 2020), which aims to automatically ask questions from textual inputs. QG has been shown to benefit various NLP tasks, such as enriching QA corpora (Alberti et al., 2019), checking factual consistency for summarization (Wang et al., 2020), and data augmentation for semantic parsing (Guo et al., 2018). To the best of our knowledge, we are the first to employ QG for fact verification. As illustrated in Figure 1, given a passage P as the evidence, we first employ a Question Generator to generate a question–answer pair (Q, A) for the evidence. We then convert (Q, A) into a claim C (QA-to-Claim) based on the following logical assumptions: a) if P can answer Q and A is the correct answer, then C is a supported claim; b) if P can answer Q but A is an incorrect answer, then C is a refuted claim; c) if P cannot answer Q, then C is a NE"
2021.acl-short.61,2020.acl-main.549,0,0.0486278,"Missing"
2021.eacl-demos.39,D19-1224,0,0.0114204,"h models can be costly to train. For example, XLNet-Large (Yang et al., 2019) was trained on 512 TPU v3 chips for 500K steps, which costs around 61,440 dollars2 , let alone staggeringly large carbon emission. Moreover, despite impressive performance gain, the fine-tuning and inference efficiency of NLP models remain under-explored. As recently mentioned in a tweet3 , the popular AI text adventure game AI Dungeon has reached 100 million inferences. The energy efficiency of inference cost could be critical to both business planning and environmental impact. Previous work (Schwartz et al., 2019; Dodge et al., 2019) on this topic proposed new metrics like FPO (floating-point operations) and other practices to report experimental results based on computing budget. Other benchmarks like (Coleman et al., 2017) and (Mattson et al., 2019) compare the efficiency of models on the classic reading comprehension task SQuAD and machine translation tasks. However, there has not been any concrete or practical reference for accurate estimation on NLP model pretraining, fine-tunning, and inference considering multi-task energy efficiency. Energy efficiency can be reflected in many metrics, including carbon emission, el"
2021.eacl-demos.39,2021.ccl-1.108,0,0.0658677,"Missing"
2021.eacl-demos.39,D16-1264,0,0.012233,"ulkbenchmark.github.io/. 1 Introduction Environmental concerns of machine learning research have been rising as the carbon emission of specific tasks like neural architecture search reached an exceptional “ocean boiling” level (Strubell et al., 2019). Increased carbon emission has been one of the key factors to aggravate global warming 1 . Research and development processes like parameter search further increase the environmental impact. When using cloud-based machines, the environmental impact is strongly correlated with the financial cost. The recent emergence of leaderboards such as SQuAD (Rajpurkar et al., 2016), GLUE (Wang et al., 2018), and SuperGLUE (Wang et al., 2019) has greatly boosted the development of advanced 1 models in the NLP community. Pretrained models have proven to be the key ingredient for achieving state-of-the-art in conventional metrics. However, such models can be costly to train. For example, XLNet-Large (Yang et al., 2019) was trained on 512 TPU v3 chips for 500K steps, which costs around 61,440 dollars2 , let alone staggeringly large carbon emission. Moreover, despite impressive performance gain, the fine-tuning and inference efficiency of NLP models remain under-explored. As"
2021.eacl-demos.39,W03-0419,0,0.194982,"Missing"
2021.eacl-demos.39,D13-1170,0,0.00695001,"Missing"
2021.eacl-demos.39,P19-1355,0,0.0680586,"Missing"
2021.eacl-demos.39,W18-5446,0,0.0610967,"Missing"
2021.eacl-main.103,D18-1549,0,0.0229604,"d Text Style Transfer is an approach to mitigate the lack of parallel data for supervised training. One line of work encodes the text into a latent vector and manipulate the text representation in the latent space to transfer the style. Shen et al. (2017); Hu et al. (2017); Yang et al. (2018) use variational auto-encoder to encode the text, and use a discriminator to modify text style. John et al. (2019); Fu et al. (2018) rely on models with encoder-decoder structure to transfer the style. Another line of work enriches the training data by generating pseudo-parallel data via back-translation (Artetxe et al., 2018; Lample et al., 2018b,a; Zhang et al., 2018). 3 3.1 Methods Task Definition In the vision-and-language navigation task, the reasoning navigator is asked to find the correct path to reach the target location following the instructions (a set of sentences) X = {s1 , s2 , . . . , sm }. The navigation procedure can be viewed as a series of decision making processes. At each time step t, the navigation environment presents an image view vt . With reference to the instruction X and the visual view vt , the navigator is expected to choose an action at ∈ A. The action set A for urban environment navi"
2021.eacl-main.103,N19-1423,0,0.0340014,"here x1 , x2 , . . . , xn denotes the tokens in the original instruction X , n is the total token number in X , and N denotes the maximum view number in the trajectory. 3.4 VLN Transformer The VLN Transformer is the navigation agent that generates actions in the outdoor VLN task. As illustrated in Fig. 4, our VLN Transformer is composed of an instruction encoder, a trajectory encoder, a cross-modal encoder that fuses the modality of the instruction encodings and trajectory encodings, and an action predictor. Instruction Encoder The instruction encoder is a pre-trained uncased BERT-base model (Devlin et al., 2019). Each piece of navigation instruction is split into multiple sentences by the fullstop punctuations. For the ith sentence si = h2s h3s h4s . t a . on ha n ee ht cti ot so u s ur rig se rd. lf s g i yo ter wa r n til n yo rse wnin i o n u he of t u ge o yo li a t h G t t e aig ra ta ien d lef Or red ht. str ga rn Go rking the ur rig Tu pa yo (4) (5) o1v h1v o2v o3v h2v h3v View Encoder Instruction Encoder j=1 ht = LST M ([vˆt ; sˆt ; vt0 ]) o4s o3s Cross-Modal Encoder (3) where Ws refers to the learnable parameters, attnst,j is the attention weight over the jth sentence encoding s0j at step t,"
2021.eacl-main.103,2020.acl-main.225,0,0.0176445,"In contrast, humanannotated instructions for the outdoor VLN task emphasize the visual environment’s attributes as navigation targets. It frequently refers to objects in the panorama, such as traffic lights, cars, awnings, etc. The goal of conducting multimodal text style transfer is to inject more object-related information in the surrounding navigation environment to the machine-generated instruction while keeping the correct guiding signals. Masking-and-Recovering Scheme The multimodal text style transfer model is trained with a “masking-and-recovering"" (Zhu et al., 2019; Liu et al., 2019; Donahue et al., 2020; Huang et al., 2020a) scheme to inject objects that appeared in the panorama into the instructions. We mask out certain portions in the instructions and try to recover the missing contents with the help of the remaining instruction skeleton and the paired trajectory. To be specific, we use NLTK (Bird et al., 2009) to mask out the object-related tokens in the human-annotated instructions, and the street names 1209 Outdoor Navigation Task Finetune VLN Transformer Human-annotated Instructions Pre-train Train External Resources Inference Input Machine-generated Instructions Multimodal Text Style"
2021.eacl-main.103,D13-1128,0,0.0307525,"ightly improve the navigation results, but still hinders the agent from navigating correctly. The stylemodified instructions improve the agent’s performance on all the navigation metrics, suggesting that our Multimodal Text Style Transfer learning approach can assist the outdoor VLN task. Quality of the Generated Instruction We evaluate the quality of instructions generated by the Speaker and the MTST model. We utilize five automatic metrics for natural language generation to evaluate the quality of the generated instructions, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Elliott and Keller, 2013), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). In addition, we calculate the guiding signal match rate (MR) by comparing the appearance of “turn left” and “turn right”. If the generated instruction contains the 1213 Dev Set Model Test Set TC ↑ SPD ↓ SED ↑ CLS ↑ nDTW ↑ SDTW ↑ TC ↑ SPD ↓ SED ↑ CLS ↑ nDTW ↑ SDTW ↑ RCONCAT +M-50 +M-50 +style 10.6 11.8 11.9 20.4 19.1 19.9 10.3 11.4 11.5 48.1 48.7 48.9 22.5 23.1 23.8 9.8 10.9 11.1 11.8 12.1 12.6 20.4 19.4 20.4 11.5 11.8 12.3 47.9 49.4 48.0 22.9 24.0 23.9 11.1 11.3 11.8 GA +M-50 +M-50 +style 12.0 12.3 12.9 18.7 18.5 18.5 11.6 11.8"
2021.eacl-main.103,2020.acl-main.226,0,0.18691,"onment during testing, Majumdar et al. (2020) proposes to test in previously explored environments and convert the VLN task to a classification task over the possible paths. This approach performs well in the indoor setting, but is not suitable for outdoor VLN where the environment graph is different. Multimodal Pre-training has attracted much attention to improving multimodal tasks performances. The models usually adopt the Transformer structure to encode the visual features and the textual features (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Sun et al., 2019; Li et al., 2019; Huang et al., 2020b; Luo et al., 2020; Li et al., 2020; Zheng et al., 2020; Wei et al., 2020; Tsai et al., 2019). During pre-training, these models use tasks such as masked language modeling, masked region modeling, image-text matching to learn the cross-modal encoding ability, which later benefits the multimodal downstream tasks. Majumdar et al. (2020) proposes to use image-text pairs from the web to pre-train VLN-BERT, a visiolinguistic transformer-based model similar to the model proposed by Lu et al. (2019). A concurrent work by Hao et al. (2020) proposes to use Transformer for indoor VLN. Our VLN Transform"
2021.eacl-main.103,P19-1181,0,0.406857,"and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Sun et al., 2019; Li et al., 2019; Huang et al., 2020b; Luo et al., 2020; Li et al., 2020; Zheng et al., 2020; Wei et al., 2020; Tsai et al., 2019). During pre-training, these models use tasks such as masked language modeling, masked region modeling, image-text matching to learn the cross-modal encoding ability, which later benefits the multimodal downstream tasks. Majumdar et al. (2020) proposes to use image-text pairs from the web to pre-train VLN-BERT, a visiolinguistic transformer-based model similar to the model proposed by Lu et al. (2019). A concurrent work by Hao et al. (2020) proposes to use Transformer for indoor VLN. Our VLN Transformer is different from their model in several key aspects: (1) The pre-training objectives are different: Hao et al. (2020) pre-trains the model 1208 on the same dataset for training, while we create an augmented, stylized dataset for outdoor VLN using the proposed MTST method. (2) Benefiting from the effective external resource, a simple navigation loss is employed in our VLN Transformer, while they adopt the masked language modeling to better train their model. (3) Model-wise, instead of encod"
2021.eacl-main.103,P19-1041,0,0.0284811,", we use sentence-level encoding since Touchdown instructions are much longer than R2R instructions. (4) We encode the trajectory history, while their model encodes the panorama for the current step. Unsupervised Text Style Transfer is an approach to mitigate the lack of parallel data for supervised training. One line of work encodes the text into a latent vector and manipulate the text representation in the latent space to transfer the style. Shen et al. (2017); Hu et al. (2017); Yang et al. (2018) use variational auto-encoder to encode the text, and use a discriminator to modify text style. John et al. (2019); Fu et al. (2018) rely on models with encoder-decoder structure to transfer the style. Another line of work enriches the training data by generating pseudo-parallel data via back-translation (Artetxe et al., 2018; Lample et al., 2018b,a; Zhang et al., 2018). 3 3.1 Methods Task Definition In the vision-and-language navigation task, the reasoning navigator is asked to find the correct path to reach the target location following the instructions (a set of sentences) X = {s1 , s2 , . . . , sm }. The navigation procedure can be viewed as a series of decision making processes. At each time step t,"
2021.eacl-main.103,J82-2005,0,0.676966,"Missing"
2021.eacl-main.103,W04-1013,0,0.0211754,"Speaker model can slightly improve the navigation results, but still hinders the agent from navigating correctly. The stylemodified instructions improve the agent’s performance on all the navigation metrics, suggesting that our Multimodal Text Style Transfer learning approach can assist the outdoor VLN task. Quality of the Generated Instruction We evaluate the quality of instructions generated by the Speaker and the MTST model. We utilize five automatic metrics for natural language generation to evaluate the quality of the generated instructions, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Elliott and Keller, 2013), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). In addition, we calculate the guiding signal match rate (MR) by comparing the appearance of “turn left” and “turn right”. If the generated instruction contains the 1213 Dev Set Model Test Set TC ↑ SPD ↓ SED ↑ CLS ↑ nDTW ↑ SDTW ↑ TC ↑ SPD ↓ SED ↑ CLS ↑ nDTW ↑ SDTW ↑ RCONCAT +M-50 +M-50 +style 10.6 11.8 11.9 20.4 19.1 19.9 10.3 11.4 11.5 48.1 48.7 48.9 22.5 23.1 23.8 9.8 10.9 11.1 11.8 12.1 12.6 20.4 19.4 20.4 11.5 11.8 12.3 47.9 49.4 48.0 22.9 24.0 23.9 11.1 11.3 11.8 GA +M-50 +M-50 +style 12.0"
2021.eacl-main.103,P19-1406,0,0.0179697,"s and directions. In contrast, humanannotated instructions for the outdoor VLN task emphasize the visual environment’s attributes as navigation targets. It frequently refers to objects in the panorama, such as traffic lights, cars, awnings, etc. The goal of conducting multimodal text style transfer is to inject more object-related information in the surrounding navigation environment to the machine-generated instruction while keeping the correct guiding signals. Masking-and-Recovering Scheme The multimodal text style transfer model is trained with a “masking-and-recovering"" (Zhu et al., 2019; Liu et al., 2019; Donahue et al., 2020; Huang et al., 2020a) scheme to inject objects that appeared in the panorama into the instructions. We mask out certain portions in the instructions and try to recover the missing contents with the help of the remaining instruction skeleton and the paired trajectory. To be specific, we use NLTK (Bird et al., 2009) to mask out the object-related tokens in the human-annotated instructions, and the street names 1209 Outdoor Navigation Task Finetune VLN Transformer Human-annotated Instructions Pre-train Train External Resources Inference Input Machine-generated Instructions"
2021.eacl-main.103,P02-1040,0,0.109115,"dding textual attention to the Speaker model can slightly improve the navigation results, but still hinders the agent from navigating correctly. The stylemodified instructions improve the agent’s performance on all the navigation metrics, suggesting that our Multimodal Text Style Transfer learning approach can assist the outdoor VLN task. Quality of the Generated Instruction We evaluate the quality of instructions generated by the Speaker and the MTST model. We utilize five automatic metrics for natural language generation to evaluate the quality of the generated instructions, including BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Elliott and Keller, 2013), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016). In addition, we calculate the guiding signal match rate (MR) by comparing the appearance of “turn left” and “turn right”. If the generated instruction contains the 1213 Dev Set Model Test Set TC ↑ SPD ↓ SED ↑ CLS ↑ nDTW ↑ SDTW ↑ TC ↑ SPD ↓ SED ↑ CLS ↑ nDTW ↑ SDTW ↑ RCONCAT +M-50 +M-50 +style 10.6 11.8 11.9 20.4 19.1 19.9 10.3 11.4 11.5 48.1 48.7 48.9 22.5 23.1 23.8 9.8 10.9 11.1 11.8 12.1 12.6 20.4 19.4 20.4 11.5 11.8 12.3 47.9 49.4 48.0 22.9 24.0 23.9 11.1 11.3 11.8 GA +M-5"
2021.eacl-main.103,D19-1514,0,0.0212049,"e most existing works select navigation actions dynamically along the way in the unseen environment during testing, Majumdar et al. (2020) proposes to test in previously explored environments and convert the VLN task to a classification task over the possible paths. This approach performs well in the indoor setting, but is not suitable for outdoor VLN where the environment graph is different. Multimodal Pre-training has attracted much attention to improving multimodal tasks performances. The models usually adopt the Transformer structure to encode the visual features and the textual features (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Sun et al., 2019; Li et al., 2019; Huang et al., 2020b; Luo et al., 2020; Li et al., 2020; Zheng et al., 2020; Wei et al., 2020; Tsai et al., 2019). During pre-training, these models use tasks such as masked language modeling, masked region modeling, image-text matching to learn the cross-modal encoding ability, which later benefits the multimodal downstream tasks. Majumdar et al. (2020) proposes to use image-text pairs from the web to pre-train VLN-BERT, a visiolinguistic transformer-based model similar to the model proposed by Lu et al. (2019). A concurr"
2021.eacl-main.103,N19-1268,0,0.0268868,"tal decision-making processes (Fenton et al., 2020). Outdoor visionand-language navigation (VLN) is such a task, where an agent navigates in an urban environment by grounding natural language instructions in visual scenes, as illustrated in Fig. 1. To generate a series of correct actions, the navigation agent must comprehend the instructions and reason through the visual environment. 1 Our code and dataset is released at https://github. com/VegB/VLN-Transformer. Different from indoor navigation (Anderson et al., 2018; Wang et al., 2018; Fried et al., 2018; Wang et al., 2019; Ma et al., 2019a; Tan et al., 2019; Ma et al., 2019b; Ke et al., 2019), the outdoor navigation task takes place in urban environments that contain diverse street views (Mirowski et al., 2018; Chen et al., 2019; Mehta et al., 2020). The vast urban area leads to a much larger space for an agent to explore and usually contains longer trajectories and a wider range of objects for visual grounding. This requires more informative instructions to address the complex navigation environment. However, it is expensive to collect human-annotated instructions that depict the complicated visual scenes to train a navigation agent. The issue"
2021.eacl-main.103,P19-1656,0,0.0284123,"ents and convert the VLN task to a classification task over the possible paths. This approach performs well in the indoor setting, but is not suitable for outdoor VLN where the environment graph is different. Multimodal Pre-training has attracted much attention to improving multimodal tasks performances. The models usually adopt the Transformer structure to encode the visual features and the textual features (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Sun et al., 2019; Li et al., 2019; Huang et al., 2020b; Luo et al., 2020; Li et al., 2020; Zheng et al., 2020; Wei et al., 2020; Tsai et al., 2019). During pre-training, these models use tasks such as masked language modeling, masked region modeling, image-text matching to learn the cross-modal encoding ability, which later benefits the multimodal downstream tasks. Majumdar et al. (2020) proposes to use image-text pairs from the web to pre-train VLN-BERT, a visiolinguistic transformer-based model similar to the model proposed by Lu et al. (2019). A concurrent work by Hao et al. (2020) proposes to use Transformer for indoor VLN. Our VLN Transformer is different from their model in several key aspects: (1) The pre-training objectives are d"
2021.eacl-main.103,2020.findings-emnlp.62,1,0.849996,"Missing"
2021.eacl-main.103,2020.acl-main.683,0,0.0137944,"o test in previously explored environments and convert the VLN task to a classification task over the possible paths. This approach performs well in the indoor setting, but is not suitable for outdoor VLN where the environment graph is different. Multimodal Pre-training has attracted much attention to improving multimodal tasks performances. The models usually adopt the Transformer structure to encode the visual features and the textual features (Tan and Bansal, 2019; Lu et al., 2019; Chen et al., 2020; Sun et al., 2019; Li et al., 2019; Huang et al., 2020b; Luo et al., 2020; Li et al., 2020; Zheng et al., 2020; Wei et al., 2020; Tsai et al., 2019). During pre-training, these models use tasks such as masked language modeling, masked region modeling, image-text matching to learn the cross-modal encoding ability, which later benefits the multimodal downstream tasks. Majumdar et al. (2020) proposes to use image-text pairs from the web to pre-train VLN-BERT, a visiolinguistic transformer-based model similar to the model proposed by Lu et al. (2019). A concurrent work by Hao et al. (2020) proposes to use Transformer for indoor VLN. Our VLN Transformer is different from their model in several key aspects:"
2021.eacl-main.196,D19-1065,0,0.0728642,"ntroduction The task of generating textual descriptions of images tests a machine’s ability to understand visual data and interpret it in natural language. It is a fundamental research problem lying at the intersection of natural language processing, computer vision, and cognitive science. For example, single-image captioning (Farhadi et al., 2010; Kulkarni et al., 2013; Vinyals et al., 2015; Xu et al., 2015) has been extensively studied. Recently, a new intriguing task, visual comparison, along with several benchmarks (Jhamtani and Berg-Kirkpatrick, 2018; Tan et al., 2019; Park et al., 2019; Forbes et al., 2019) has drawn increasing attention in the community. To complete the task and generate comparative descriptions, a machine should understand the visual differences between a pair of images (see Figure 1). Previous methods (Jhamtani and Berg-Kirkpatrick, 2018) often consider the pair of pre-trained visual features such as the ResNet features (He et al., 2016) as a whole, and build end-to-end neural networks to predict the description of visual comparison directly. In contrast, humans can easily reason about the visual components of a single image and describe the visual differences between two ima"
2021.eacl-main.196,W04-1013,0,0.0642238,"tasets The Birds-to-Words (B2W) has 3347 image pairs, and each has around 5 descriptions of visual difference. This leads to 12890/1556/1604 captions for train/val/test splits. Since B2W contains only visual comparisons, We use the CUB200-2011 dataset (CUB) (Wah et al., 2011), which consists of single-image captions as an auxiliary to facilitate the training of semantic understanding. CUB has 8855/2933 images of birds for train/val splits, and each image has 10 captions. Evaluation Metrics Performances are first eval1 uated on three automatic metrics : BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr-D (Vedantam et al., 2015). Each generated description is compared to all five reference paragraphs. Note for this particular task, researchers observe that CIDEr-D is susceptible to common patterns in the data (See Table 1 for proof), and ROUGE-L is anecdotally correlated with higherquality descriptions (which is noted in previous work (Forbes et al., 2019)). Hence we consider ROUGE-L as the major metric for evaluating performances. We then perform a human evaluation to further verify the performance. Implementation Details We use Adam as the optimizer with an initial learning rate"
2021.eacl-main.196,P02-1040,0,0.113818,"riments 3.1 Experimental Setup Datasets The Birds-to-Words (B2W) has 3347 image pairs, and each has around 5 descriptions of visual difference. This leads to 12890/1556/1604 captions for train/val/test splits. Since B2W contains only visual comparisons, We use the CUB200-2011 dataset (CUB) (Wah et al., 2011), which consists of single-image captions as an auxiliary to facilitate the training of semantic understanding. CUB has 8855/2933 images of birds for train/val splits, and each image has 10 captions. Evaluation Metrics Performances are first eval1 uated on three automatic metrics : BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and CIDEr-D (Vedantam et al., 2015). Each generated description is compared to all five reference paragraphs. Note for this particular task, researchers observe that CIDEr-D is susceptible to common patterns in the data (See Table 1 for proof), and ROUGE-L is anecdotally correlated with higherquality descriptions (which is noted in previous work (Forbes et al., 2019)). Hence we consider ROUGE-L as the major metric for evaluating performances. We then perform a human evaluation to further verify the performance. Implementation Details We use Adam as the optimizer with an"
2021.eacl-main.196,P19-1182,0,0.018964,"ions aligned to the image saliency. Introduction The task of generating textual descriptions of images tests a machine’s ability to understand visual data and interpret it in natural language. It is a fundamental research problem lying at the intersection of natural language processing, computer vision, and cognitive science. For example, single-image captioning (Farhadi et al., 2010; Kulkarni et al., 2013; Vinyals et al., 2015; Xu et al., 2015) has been extensively studied. Recently, a new intriguing task, visual comparison, along with several benchmarks (Jhamtani and Berg-Kirkpatrick, 2018; Tan et al., 2019; Park et al., 2019; Forbes et al., 2019) has drawn increasing attention in the community. To complete the task and generate comparative descriptions, a machine should understand the visual differences between a pair of images (see Figure 1). Previous methods (Jhamtani and Berg-Kirkpatrick, 2018) often consider the pair of pre-trained visual features such as the ResNet features (He et al., 2016) as a whole, and build end-to-end neural networks to predict the description of visual comparison directly. In contrast, humans can easily reason about the visual components of a single image and descri"
2021.eacl-main.196,P18-1083,1,0.851744,"features. Second, joint learning with a single-image caption L2C[B2W+CUB] can help improve the ability of semantic understanding, thus, the overall performance of the model. Finally, our method also has a smaller gap between validation and test set compared to neural naturalist, indicating its potential capability to generalize for unseen samples. 3.3 Human Evaluation To fully evaluate our model, we conduct a pairwise human evaluation on Amazon Mechanical Turk with 100 image pairs randomly sampled from the test set, each sample was assigned to 5 workers to eliminate human variance. Following Wang et al. (2018), for each image pair, workers are presented with two paragraphs from different models and asked to choose the better one based on text quality . As shown in Table 2, L2C outperforms CNN+LSTM, which is consistent with automatic metrics. 3.4 Ablation Studies Effect of Individual Components We perform ablation studies to show the effectiveness of semantic pooling, total variance loss, and graph reasoning, as shown in Table 3. First, without semantic pooling, the model degrades to average pooling, and results show that semantic pooling can better preserve the spatial relations for the visual repr"
2021.eacl-main.236,P83-1022,0,0.36146,"Missing"
2021.eacl-main.236,2020.acl-main.454,0,0.06463,"However, along with these improvements, researchers find that neural models are more prone to a phenomenon called hallucination, where models generate description tokens that are not supported by the source inputs. This phenomenon seriously damages the applicability of neural language generation models in practice where information accuracy is vital. Hallucination has been observed in various conditional NLG tasks such as image captioning (Rohrbach et al., 2018), data-to-text generation (Wiseman et al., 2017; Nie et al., 2019; Parikh et al., 2020), abstractive summarization (Cao et al., 2018; Durmus et al., 2020), and neural machine We believe that there is a common theme across all the hallucination explanations in conditional NLG tasks: predictive uncertainty. In language generation, predictive uncertainty quantifies the entropy of the token probability distributions a model predicts. There are multiple sources of uncertainty. Two major ones frequently studied are aleatoric and epistemic uncertainties, where the former comes from the data or measurements, and the latter is concerned with the model. With recent progress in Bayesian neural networks (BNNs) (Hinton and Van Camp, 1993; Neal, 1995) and un"
2021.eacl-main.236,P19-1256,0,0.257579,"y measured by standard metrics on different natural language generation (NLG) tasks. However, along with these improvements, researchers find that neural models are more prone to a phenomenon called hallucination, where models generate description tokens that are not supported by the source inputs. This phenomenon seriously damages the applicability of neural language generation models in practice where information accuracy is vital. Hallucination has been observed in various conditional NLG tasks such as image captioning (Rohrbach et al., 2018), data-to-text generation (Wiseman et al., 2017; Nie et al., 2019; Parikh et al., 2020), abstractive summarization (Cao et al., 2018; Durmus et al., 2020), and neural machine We believe that there is a common theme across all the hallucination explanations in conditional NLG tasks: predictive uncertainty. In language generation, predictive uncertainty quantifies the entropy of the token probability distributions a model predicts. There are multiple sources of uncertainty. Two major ones frequently studied are aleatoric and epistemic uncertainties, where the former comes from the data or measurements, and the latter is concerned with the model. With recent p"
2021.eacl-main.236,N19-4009,0,0.0811609,"Missing"
2021.eacl-main.236,P02-1040,0,0.109908,"Missing"
2021.eacl-main.236,D18-1437,0,0.365143,"l network models have brought drastic improvements of generation quality measured by standard metrics on different natural language generation (NLG) tasks. However, along with these improvements, researchers find that neural models are more prone to a phenomenon called hallucination, where models generate description tokens that are not supported by the source inputs. This phenomenon seriously damages the applicability of neural language generation models in practice where information accuracy is vital. Hallucination has been observed in various conditional NLG tasks such as image captioning (Rohrbach et al., 2018), data-to-text generation (Wiseman et al., 2017; Nie et al., 2019; Parikh et al., 2020), abstractive summarization (Cao et al., 2018; Durmus et al., 2020), and neural machine We believe that there is a common theme across all the hallucination explanations in conditional NLG tasks: predictive uncertainty. In language generation, predictive uncertainty quantifies the entropy of the token probability distributions a model predicts. There are multiple sources of uncertainty. Two major ones frequently studied are aleatoric and epistemic uncertainties, where the former comes from the data or measur"
2021.eacl-main.236,D17-1239,0,0.13146,"s of generation quality measured by standard metrics on different natural language generation (NLG) tasks. However, along with these improvements, researchers find that neural models are more prone to a phenomenon called hallucination, where models generate description tokens that are not supported by the source inputs. This phenomenon seriously damages the applicability of neural language generation models in practice where information accuracy is vital. Hallucination has been observed in various conditional NLG tasks such as image captioning (Rohrbach et al., 2018), data-to-text generation (Wiseman et al., 2017; Nie et al., 2019; Parikh et al., 2020), abstractive summarization (Cao et al., 2018; Durmus et al., 2020), and neural machine We believe that there is a common theme across all the hallucination explanations in conditional NLG tasks: predictive uncertainty. In language generation, predictive uncertainty quantifies the entropy of the token probability distributions a model predicts. There are multiple sources of uncertainty. Two major ones frequently studied are aleatoric and epistemic uncertainties, where the former comes from the data or measurements, and the latter is concerned with the mo"
2021.eacl-main.236,2020.emnlp-main.508,0,0.360505,"emic and aleatoric uncertainty in vision tasks such as semantic segmentation and depth regression. They show that it is important to model aleatoric uncertainty with large datasets and real-time applications and epistemic uncertainty with small datasets and safety-critical applications. Other applications of uncertainty quantification have been explored in the context of time series predictions (Zhu and Laptev, 2017), natural language processing tasks (Xiao and Wang, 2019), etc. More broadly, prediction entropy has been analyzed in different neural language generation tasks (Ott et al., 2018; Xu et al., 2020). Depeweg et al. (2018) shows how to extract and decompose uncertainty in Bayesian neural networks with latent variables for decisionmaking purposes. They show that active learning and risk-sensitive reinforcement learning both benefit from uncertainty decomposition. 7 Discussion and Conclusions We investigate the relationship between hallucination and predictive uncertainty in image captioning and data-to-text generation tasks and show that predictions with higher uncertainty are more prone to hallucination. In particular, epistemic uncertainty is more indicative of hallucination than aleator"
2021.eacl-main.244,W02-1033,0,0.0882412,"questions result from the informationseeking nature of the open-domain QA task where the users usually use the minimal number of words for searching and they are not aware of the potential ambiguous factors. To solve this kind of questions, an interactive QA system might be necessary. In the appendix, we show more ambiguous questions in which other kinds of constraints are missing. Related Work The task of answering questions without specifying specific domains has been intensively studied since the earlier TREC QA competitions (Voorhees, 1999). Studies in the early stage (Kwok et al., 2001; Brill et al., 2002; Ferrucci et al., 2010; Baudiˇs, 2015) mostly rely on highly sophisticated pipelines and heterogeneous resources. Built on the recent advances in machine reading comprehension, Chen et al. (2017) show that open-domain QA can be simply formulated as a reading comprehension problem with the help of a standard IR component that provides candidate paragraphs for answer extraction. This two-stage formulation is simple yet effective to achieve competitive performance while using Wikipedia as the only knowledge resource. Following this formulation, a couple of recent studies have proposed to improve"
2021.eacl-main.244,P17-1171,0,0.451755,"e samples in each batch. Across three opendomain QA datasets, our method consistently outperforms a strong dense retrieval baseline that uses 6 times more computation for training. On two of the datasets, our method achieves more than 4-point absolute improvement in terms of answer exact match. 1 Introduction With the promise of making the vast amount of information buried in text easily accessible via userfriendly natural language queries, the area of opendomain QA has attracted lots of attention in recent years. Existing open-domain QA systems are typically made of two essential components (Chen et al., 2017). A retrieval module first retrieves a compact set of paragraphs from the whole corpus ? Equal Contribution. Our code is available at https://github.com/ xwhan/ProQA.git. (such as Wikipedia) that includes millions of documents. Then a reading module is deployed to extract an answer span from the retrieved paragraphs. Over the past few years, much of the progress in open-domain QA has been focusing on improving the reading module of the system, which only needs to process a small number of retrieved paragraphs. Specifically, improvements include stronger reading comprehension models (Wang et al"
2021.eacl-main.244,C18-1154,0,0.0370513,"Missing"
2021.eacl-main.244,D13-1160,0,0.0627463,"ed in §3.5. At inference time, we use a linear combination of the retrieval score and the answer span score to rank the answer candidates from the top-5 retrieved paragraphs. The linear combination weight is selected based on the validation performance on each tested dataset. 3 3.1 Experiments Datasets We center our studies on QA datasets that reflect real-world information-seeking scenarios. We consider 1) NaturalQuestions-Open (Kwiatkowski et al., 2019; Lee et al., 2019), which includes around 10K real-user queries (79,168/8,757/3,610 for train/dev/test) from Google Search; 2) WebQuestions (Berant et al., 2013), which is originally designed for knowledge base QA and includes 5,810 questions (3,417/361/2,032 for train/dev/test) generated by Google Suggest API; 3) CuratedTREC (Baudis and Sediv´y, 2015), which includes 2,180 real-user queries (1,353/133/694 for train/dev/test) from MSNSearch and AskJeeves logs. Compared to other datasets such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017), questions in these datasets are created without the presence of ground-truth answers and the answer paragraphs, thus are less likely to have lexical overlap with the paragraph. 3.2 Essential Impl"
2021.eacl-main.244,D15-1075,0,0.0448843,"ing models (Wang et al., 2018a; Lin et al., 2018) that assign more accurate relevance scores to the retrieved paragraphs. However, the performance is still bounded by the retrieval modules, which simply rely on traditional IR methods such as TF-IDF or BM25 (Robertson and Zaragoza, 2009). These methods retrieve text solely based on n-gram lexical overlap and can fail on cases when deep semantic matching is required and when there are no common lexicons between the question and the target paragraph. While neural models have proven effective at learning deep semantic matching between text pairs (Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2018; Devlin et al., 2019), they usually require computing question-dependent paragraph encodings (i.e., the same paragraph will have different representations when considering different questions), which is formidable considering space constraints and retrieval efficiency in practice. More recent studies (Lee et al., 2019; Chang et al., 2020; Guu et al., 2020) show that such a dilemma can be resolved with large-scale matching-oriented pretraining. These approaches use separate encoders for questions and paragraphs and simply model the matching between the q"
2021.eacl-main.244,N19-1423,0,0.191835,"more accurate relevance scores to the retrieved paragraphs. However, the performance is still bounded by the retrieval modules, which simply rely on traditional IR methods such as TF-IDF or BM25 (Robertson and Zaragoza, 2009). These methods retrieve text solely based on n-gram lexical overlap and can fail on cases when deep semantic matching is required and when there are no common lexicons between the question and the target paragraph. While neural models have proven effective at learning deep semantic matching between text pairs (Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2018; Devlin et al., 2019), they usually require computing question-dependent paragraph encodings (i.e., the same paragraph will have different representations when considering different questions), which is formidable considering space constraints and retrieval efficiency in practice. More recent studies (Lee et al., 2019; Chang et al., 2020; Guu et al., 2020) show that such a dilemma can be resolved with large-scale matching-oriented pretraining. These approaches use separate encoders for questions and paragraphs and simply model the matching between the question and paragraph using inner products of the output vecto"
2021.eacl-main.244,P17-1147,0,0.0679741,"der 1) NaturalQuestions-Open (Kwiatkowski et al., 2019; Lee et al., 2019), which includes around 10K real-user queries (79,168/8,757/3,610 for train/dev/test) from Google Search; 2) WebQuestions (Berant et al., 2013), which is originally designed for knowledge base QA and includes 5,810 questions (3,417/361/2,032 for train/dev/test) generated by Google Suggest API; 3) CuratedTREC (Baudis and Sediv´y, 2015), which includes 2,180 real-user queries (1,353/133/694 for train/dev/test) from MSNSearch and AskJeeves logs. Compared to other datasets such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017), questions in these datasets are created without the presence of ground-truth answers and the answer paragraphs, thus are less likely to have lexical overlap with the paragraph. 3.2 Essential Implementation Details For pretraining, we use a batch size of 80 and accumulate the gradients every 8 batches. We use the Adam optimizer (Kingma and Ba, 2015) with learning rate 1e-5 and conduct 90K parameter updates. Following previous work (Lee et al., 2019), we use the 12-20-2018 snapshot of English Wikipedia as our open-domain QA corpus. When splitting the documents into chunks, we try to reuse the"
2021.eacl-main.244,2020.emnlp-main.550,0,0.10531,"plementation. retrieving documents, our system achieves better performance and is also much faster at inference time, due to the much smaller model size. The stateof-the-art REALM model (Guu et al., 2020) uses a more complicated pretraining approach that requires asynchronously refreshing the corpus index at train time. As it relies on ORQA initialization and further pretraining updates, it is even more computational expensive at training time. Also, as our method directly improves the ORQA pretraining, our method could easily stack with the REALM pretraining approach. Concurrent to our work, Karpukhin et al. (2020) show that it is possible to use the groundtruth answer paragraphs in the original NaturalQuestions dataset to train a stronger dense retriever. However, they use a larger index dimension (768) while encoding paragraphs and also retrieve more paragraphs (20∼100) for answer extraction. As a larger index dimension naturally leads to better retrieval results (Luan et al., 2020) (despite sacrificing search efficiency) and using more paragraphs increases the recall of matched answer spans3 , this concurrent result is not directly comparable to ours, and we leave the combination effect of efficient"
2021.eacl-main.244,P18-1161,0,0.552402,"available at https://github.com/ xwhan/ProQA.git. (such as Wikipedia) that includes millions of documents. Then a reading module is deployed to extract an answer span from the retrieved paragraphs. Over the past few years, much of the progress in open-domain QA has been focusing on improving the reading module of the system, which only needs to process a small number of retrieved paragraphs. Specifically, improvements include stronger reading comprehension models (Wang et al., 2018b; Yang et al., 2019; Xiong et al., 2020; Min et al., 2019a) and paragraph reranking models (Wang et al., 2018a; Lin et al., 2018) that assign more accurate relevance scores to the retrieved paragraphs. However, the performance is still bounded by the retrieval modules, which simply rely on traditional IR methods such as TF-IDF or BM25 (Robertson and Zaragoza, 2009). These methods retrieve text solely based on n-gram lexical overlap and can fail on cases when deep semantic matching is required and when there are no common lexicons between the question and the target paragraph. While neural models have proven effective at learning deep semantic matching between text pairs (Bowman et al., 2015; Parikh et al., 2016; Chen et"
2021.eacl-main.244,Q19-1026,0,0.125871,"aining paradigm. Figure 1 depicts the whole pretraining process. Pretraining Data Generation Previous dense retrieval approaches usually rely on simple heuristics to generate synthetic matching pairs for pretraining, which do not necessarily reflect the underlying matching pattern between questions and paragraphs. To minimize the gap between pretraining and the end task, we learn to generate high-quality questions from the paragraphs using a state-of-the-art pretrained seq2seq model, i.e., BART (Lewis et al., 2019). More specifically, we finetune BART on the original NaturalQuestions dataset (Kwiatkowski et al., 2019) such that it learns to generate questions given the groundtruth answer string and the groundtruth paragraph (labeled as long answer in NaturalQuestions). We concatenate the paragraph and the answer string with a separating token as the input to the BART model. We find this simple input scheme is effective enough to generate high-quality questions, achieving a 55.6 ROGUE-L score on the dev set. Samples of the generated questions could be found in the appendix. Afterward, we use spaCy1 to recognize potential answer spans (named entities or dates) in all paragraphs in the corpus and use the fine"
2021.eacl-main.244,D19-1284,0,0.438178,"t of paragraphs from the whole corpus ? Equal Contribution. Our code is available at https://github.com/ xwhan/ProQA.git. (such as Wikipedia) that includes millions of documents. Then a reading module is deployed to extract an answer span from the retrieved paragraphs. Over the past few years, much of the progress in open-domain QA has been focusing on improving the reading module of the system, which only needs to process a small number of retrieved paragraphs. Specifically, improvements include stronger reading comprehension models (Wang et al., 2018b; Yang et al., 2019; Xiong et al., 2020; Min et al., 2019a) and paragraph reranking models (Wang et al., 2018a; Lin et al., 2018) that assign more accurate relevance scores to the retrieved paragraphs. However, the performance is still bounded by the retrieval modules, which simply rely on traditional IR methods such as TF-IDF or BM25 (Robertson and Zaragoza, 2009). These methods retrieve text solely based on n-gram lexical overlap and can fail on cases when deep semantic matching is required and when there are no common lexicons between the question and the target paragraph. While neural models have proven effective at learning deep semantic matchi"
2021.eacl-main.244,D18-1053,0,0.091969,"ehension problem with the help of a standard IR component that provides candidate paragraphs for answer extraction. This two-stage formulation is simple yet effective to achieve competitive performance while using Wikipedia as the only knowledge resource. Following this formulation, a couple of recent studies have proposed to improve the system using stronger reading comprehension models (Yang et al., 2019; Wang et al., 2018b), more effective learning objectives (Clark and Gardner, 2018; Min et al., 2019a; Wang et al., 2019) or paragraph reranking models (Wang et al., 2018a; Lin et al., 2018; Lee et al., 2018). However, the retrieval components in these systems are still based on traditional inverted index methods, which are efficient but might fail when the target paragraph does not have enough lexicon overlap with the question. In contrast to the sparse term-based features used in TF-IDF or BM25, dense paragraph vectors learned by deep neural networks (Zhang et al., 2017; Conneau et al., 2017) can capture much richer semantics beyond the n-gram term features. To build effective paragraph encoders tailed for the paragraph retrieval in open-domain QA, more recent studies (Lee et al., 2019; Chang et"
2021.eacl-main.244,P19-1612,0,0.281198,"fail on cases when deep semantic matching is required and when there are no common lexicons between the question and the target paragraph. While neural models have proven effective at learning deep semantic matching between text pairs (Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2018; Devlin et al., 2019), they usually require computing question-dependent paragraph encodings (i.e., the same paragraph will have different representations when considering different questions), which is formidable considering space constraints and retrieval efficiency in practice. More recent studies (Lee et al., 2019; Chang et al., 2020; Guu et al., 2020) show that such a dilemma can be resolved with large-scale matching-oriented pretraining. These approaches use separate encoders for questions and paragraphs and simply model the matching between the question and paragraph using inner products of the output vectors. Thus, these systems only need to encode all paragraphs in a question-agnostic fashion, and the resulted dense corpus index could be fixed and reused 2803 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2803–2815 April 19 - 23,"
2021.eacl-main.244,D16-1244,0,0.102628,"Missing"
2021.eacl-main.244,P19-1436,0,0.0374392,"Missing"
2021.eacl-main.244,N19-4013,0,0.142831,"Missing"
2021.eacl-main.244,D19-1599,0,0.136136,"Missing"
2021.emnlp-demo.30,2020.nlpcovid19-acl.17,0,0.017861,"y filtering (Allcott and Gentzkow, 2017). As such, it is important in today’s society to create systems that can provide credible and reliable information to users. This is especially true in the context of emergent domains which, unlike more established sectors, may contain rapidly changing information. COVID-19 follows this pattern, with over 100,000 related articles published in 2020 and new research findings still frequently reported (Else, 2020). However, the vast interest and exposure surrounding this topic have consequently generated a rise in misinformation (Kouzy et al., 2020; Medina Serrano et al., 2020). This can lead to lower 259 1 https://trec.nist.gov/data.html Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 259–266 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: An overview of the COVID-19 open-domain question-answering system. The retrieval component is shown on the left and the reading comprehension/answer extraction component is shown on the right. 19-related question-answering systems have been created in recent months (Bhatia et al., 2020; Yan et al., 2021; Reddy et al., 2020). However,"
2021.emnlp-demo.30,2020.nlpcovid19-acl.18,0,0.0428963,"-19-related questions from credible scientific sources. 2 Retrieval The retrieval model consists of a dense retriever and contains further layers of re-ranking. In the following sections, we describe the data used to train our model, along with the model details and re-ranking strategies. 2.1 Data As mentioned in Section 1, several COVID-19related datasets have been published throughout the pandemic. However, there are a limited number of sizable datasets focused on the general areas of information retrieval and question-answering. In order to train on in-domain data, we utilize the COVID-QA (Möller et al., 2020) dataset to finetune our model for the document retrieval task. COVID-QA is a COVID-19 question-answering dataset and contains multiple question-answer pairs for each context document (2,019 QA pairs in total), where the documents are COVID-19-related PubMed3 articles. In order to transform the question-answering dataset for our retrieval task, we choose to utilize the questions and their related context articles during training. We split each context article into size 3 260 https://pubmed.ncbi.nlm.nih.gov/ Model Dense Retrieval BM25 Hybrid Model FM@5 FM@20 FM@50 0.300 0.346 0.362 0.471 0.486"
2021.emnlp-demo.30,P17-1171,0,0.0304549,"of similar systems that can be adapted and modified for other developing emergent domains. 1 Introduction compliance with various preventative measures such as social distancing, which in turn can continue the spread of the virus (Bridgman et al., 2020; Tasnim et al., 2020). A question-answering system that allows users to ask free-text questions with answers deriving from published articles and reliable scientific sources can help mitigate this spread of misinformation and inform the public at the same time. The task of open-domain question-answering has risen in prominence in recent years (Chen et al., 2017; Yang et al., 2019; Xiong et al., 2021a). Systems have evolved from keyword-based approaches (Salton and McGill, 1986) to the utilization of neural networks with dense passage retrieval (Xiong et al., 2021b). Furthermore, largescale datasets have been used to train and test these systems, such as general knowledge datasets (Joshi et al., 2017; Nguyen et al., 2016) and domainspecific datasets1 (Tsatsaronis et al., 2012). However, many of these systems are evaluated on these established datasets with abundant questions and clearly defined answers. In the case of an emergent domain system, this"
2021.emnlp-demo.30,P17-1147,0,0.0301134,"tions with answers deriving from published articles and reliable scientific sources can help mitigate this spread of misinformation and inform the public at the same time. The task of open-domain question-answering has risen in prominence in recent years (Chen et al., 2017; Yang et al., 2019; Xiong et al., 2021a). Systems have evolved from keyword-based approaches (Salton and McGill, 1986) to the utilization of neural networks with dense passage retrieval (Xiong et al., 2021b). Furthermore, largescale datasets have been used to train and test these systems, such as general knowledge datasets (Joshi et al., 2017; Nguyen et al., 2016) and domainspecific datasets1 (Tsatsaronis et al., 2012). However, many of these systems are evaluated on these established datasets with abundant questions and clearly defined answers. In the case of an emergent domain system, this likely will not be available and the reduced data size can result in lower answer precision. In this paper, we build an open-domain questionanswering system in the emergent domain of COVID-19. We aim to overcome a staple issue with emergent domain question-answering systems: lack of data. While several COVID-19-related datasets have been publi"
2021.emnlp-demo.30,P18-2124,0,0.0291705,"hat are symptoms of covid?” dataset into training, development, and test sets and utilize this to evaluate the model. 3.2 Methodology The reading comprehension model performs extractive question-answering. Given a question and paragraph pair, the model learns to find start and end tokens to represent the answer span (or spans) in the paragraph text. This is done by choosing the highest-ranked start and end tokens produced by the model where the start token is earlier than the end token in the text sequence. We utilize a variant of BioBERT (Lee et al., 2019) that is fine-tuned on the SQuAD2.0 (Rajpurkar et al., 2018) dataset4 . We find that fine-tuning this model on COVIDQA allows the model to train on both in-domain (COVID-QA) and out-domain (SQuAD2.0) data and increases results for this task when evaluated on the test set of COVID-QA. 3.3 Multiple Answers Some retrieved documents may contain answer spans that are not contiguous. In order to accommodate this, we rank the top-m start and end tokens according to confidence scores and select the pairs of tokens that do not overlap with higher-ranked answer spans. This allows each document to highlight up to m answers rather than just one answer and increase"
2021.emnlp-demo.30,D19-1410,0,0.040361,"Missing"
2021.emnlp-demo.30,2020.nlpcovid19-acl.1,0,0.0457615,"Missing"
2021.emnlp-demo.30,2021.eacl-main.244,1,0.780508,"d and modified for other developing emergent domains. 1 Introduction compliance with various preventative measures such as social distancing, which in turn can continue the spread of the virus (Bridgman et al., 2020; Tasnim et al., 2020). A question-answering system that allows users to ask free-text questions with answers deriving from published articles and reliable scientific sources can help mitigate this spread of misinformation and inform the public at the same time. The task of open-domain question-answering has risen in prominence in recent years (Chen et al., 2017; Yang et al., 2019; Xiong et al., 2021a). Systems have evolved from keyword-based approaches (Salton and McGill, 1986) to the utilization of neural networks with dense passage retrieval (Xiong et al., 2021b). Furthermore, largescale datasets have been used to train and test these systems, such as general knowledge datasets (Joshi et al., 2017; Nguyen et al., 2016) and domainspecific datasets1 (Tsatsaronis et al., 2012). However, many of these systems are evaluated on these established datasets with abundant questions and clearly defined answers. In the case of an emergent domain system, this likely will not be available and the re"
2021.emnlp-demo.30,N19-4013,0,0.0481257,"Missing"
2021.emnlp-main.300,D17-1057,0,0.0566295,"Missing"
2021.emnlp-main.300,2020.findings-emnlp.91,1,0.870336,"Missing"
2021.emnlp-main.300,N19-1423,0,0.0217368,"generator, we remove the retriever and directly use the pre-trained Longformer as the input encoder in the program generator, and encode the whole report. The table rows are linearized similar as in §5.1. 6 Experimental Results Experiment Setups. For the retriever, we use BERT-base as the classifier (other pre-trained models perform similarly). Since most of the examples in our dataset have 1 or 2 facts, and we find that longer inputs lower the performance of the program generator, we take the top 3 ranked facts as the retriever results. For the program generator, we experiment on using BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and FinBert (Araci, 2019) as the encoder, to test the performances of popular large pre-trained models. For all models, we use the Adam optimizer (Kingma and Ba, 2015). Check Appendix B for more details of training and parameter settings. 6.1 QA Model Performance Baselines Exe Acc Prog Acc TF-IDF + Single Op 1.01 0.90 Retriever + Direct Generation 0.30 - Pre-Trained Longformer (base) 21.90 20.48 Retriever + Seq2seq 20.40 18.29 Retriever + NeRd (BERT-base) 52.48 49.90 FinQANet (FinBert) 53.71 51.71 FinQANet (BERT-base) 54.95 53.52 FinQANet (BERT-large) 57.43 55.52"
2021.emnlp-main.300,N19-1246,0,0.0383846,"Missing"
2021.emnlp-main.300,P18-4007,0,0.0613014,"Missing"
2021.emnlp-main.300,N16-1136,0,0.0586174,"Missing"
2021.emnlp-main.300,2021.ccl-1.108,0,0.0327705,"Missing"
2021.emnlp-main.300,P15-1142,0,0.0237626,"ity to study and analyze how the current pre-trained models perform on complex and specialized domains. • We construct a new large-scale dataset, F IN QA, with 8,281 examples written by financial experts, with fully annotated numerical reasoning programs. 2 Related Work Questions Answering. There have been several QA datasets involving numerical understandings and calculations. The major source is from structured tables or knowledge bases, owning the nature to succinctly organize numerical information. Popular datasets include ComplexWebQuestions (Talmor and Berant, 2018), WikiTableQuestions (Pasupat and Liang, 2015), Spider (Yu et al., 2018), TabFact (Chen et al., 2020b), etc. For reading comprehension, the dataset most related to ours is the DROP dataset (Dua et al., 2019), which applies simple calculations over texts. The top methods on DROP typically use specific prediction heads for each kind of calculation. HybridQA (Chen et al., 2020c) targets QA over both the table and the text, but not with the focus of numerical reasoning. All these existing datasets are built upon the general domain (mostly based on Wikipedia). In contrast, our dataset focus on the finance domain, which demonstrates much more c"
2021.emnlp-main.300,2020.emnlp-main.308,0,0.011133,"the text, but not with the focus of numerical reasoning. All these existing datasets are built upon the general domain (mostly based on Wikipedia). In contrast, our dataset focus on the finance domain, which demonstrates much more complex nature in numerical reasoning questions, combining both the structured tables and unstructured texts. Another kind of QA datasets related to ours is the math word problem datasets, like MaWPS (Koncel-Kedziorski et al., 2016), MathQA (Amini et al., 2019). The task is to generate the solution programs given a short input math problem. Existing models include (Kim et al., 2020; Chen et al., 2020a,d), etc. Financial NLP. Financial NLP has become one of the major application domains attracting growing attentions. Previous works in finance domain include risk management to detect fraud (Han et al., • We experiment on various baselines and find 2018; Wang et al., 2019; Nourbakhsh and Bang, that the models are still far behind expert per- 2019), sentiment analysis to assist market predicformance, strongly motivating future research. tion (Day and Lee, 2016; Wang et al., 2013; Akhtar 3698 et al., 2017), opinionated Question Answering (Liu et al., 2020), such as the FiQA2"
2021.emnlp-main.300,P18-2124,0,0.0855194,"Missing"
2021.emnlp-main.300,I13-1097,0,0.0128253,"k is to generate the solution programs given a short input math problem. Existing models include (Kim et al., 2020; Chen et al., 2020a,d), etc. Financial NLP. Financial NLP has become one of the major application domains attracting growing attentions. Previous works in finance domain include risk management to detect fraud (Han et al., • We experiment on various baselines and find 2018; Wang et al., 2019; Nourbakhsh and Bang, that the models are still far behind expert per- 2019), sentiment analysis to assist market predicformance, strongly motivating future research. tion (Day and Lee, 2016; Wang et al., 2013; Akhtar 3698 et al., 2017), opinionated Question Answering (Liu et al., 2020), such as the FiQA2 dataset built from forums and social media. Recent works attempt to develop pre-trained models specialized for finance domain (Yang et al., 2020; Araci, 2019), and the downstream tasks are mostly sentiment classifications. To the best of our knowledge, there is no previous work and dataset on building QA systems of numerical reasoning on financial reports. The table operations take arguments of table row names. We use the special token #n to denote the result from the nth step. For example, in Fig"
2021.emnlp-main.300,D19-1185,0,0.0153161,"oth the structured tables and unstructured texts. Another kind of QA datasets related to ours is the math word problem datasets, like MaWPS (Koncel-Kedziorski et al., 2016), MathQA (Amini et al., 2019). The task is to generate the solution programs given a short input math problem. Existing models include (Kim et al., 2020; Chen et al., 2020a,d), etc. Financial NLP. Financial NLP has become one of the major application domains attracting growing attentions. Previous works in finance domain include risk management to detect fraud (Han et al., • We experiment on various baselines and find 2018; Wang et al., 2019; Nourbakhsh and Bang, that the models are still far behind expert per- 2019), sentiment analysis to assist market predicformance, strongly motivating future research. tion (Day and Lee, 2016; Wang et al., 2013; Akhtar 3698 et al., 2017), opinionated Question Answering (Liu et al., 2020), such as the FiQA2 dataset built from forums and social media. Recent works attempt to develop pre-trained models specialized for finance domain (Yang et al., 2020; Araci, 2019), and the downstream tasks are mostly sentiment classifications. To the best of our knowledge, there is no previous work and dataset o"
2021.emnlp-main.300,D18-1259,0,0.123965,"n reports, written by financial experts. We also between the times of two events. However, most annotate the gold reasoning programs to enprior work only targeted the general domain, where sure full explainability. We further introduce the questions involve much less calculation (mostly baselines and conduct comprehensive experiments in our dataset. The results demonstrate one-step calculation) than that of the financial dothat popular, large, pre-trained models fall far main. Financial QA is more challenging than classhort of expert humans in acquiring finance sic QA (Rajpurkar et al., 2018; Yang et al., 2018) knowledge and in complex multi-step numerbecause it requires the system to spot relevant inical reasoning on that knowledge. Our dataset formation across heterogeneous sources, such as — the first of its kind — should therefore entables and unstructured texts, and then create a able significant, new community research into numerical reasoning path to connect all the inforcomplex application domains. The dataset and mation. It also takes substantial knowledge to ask code are publicly available1 . meaningful financial questions. It is not clear how 1 Introduction well the large language models,"
2021.emnlp-main.300,D18-1425,0,0.0606888,"Missing"
2021.emnlp-main.471,P19-1310,0,0.0589096,"Missing"
2021.emnlp-main.471,P19-1309,0,0.144936,"rphic without being well-aligned, so we quantify multilinguality in diverse ways. 4.1 Bitext Retrieval Task The bitext retrieval task consists of finding all sentences in a paired set of documents that are translations of each other. This process can be carried out between two comparable corpora, such as Wikipedia (“bitext mining”), but we use the 5,050 bitexts collected from the Bible corpus. We mine in two directions: for each sentence in document X , we find a match in document Y, and viceversa. We then take the intersection of those two searches, which has proven to be a useful heuristic (Artetxe and Schwenk, 2019a; Jones and Wijaya, 2021). Note that this task can be thought of as the sentence-level analog to the bilingual lexicon induction (BLI) task used in Vuli´c et al. (2020) and Dubossarsky et al. (2020). x and y, namely scoremargin (x, y) = 2k cos (x, y) P P cos (x, z) + z∈N Nk (y) cos (y, z) z∈N Nk (x) After retrieving sentence pairs in both directions and keeping the intersection, we compute standard F1-score against ground-truth alignments. Average margin score We also introduce a novel alignment metric in the form of the average margin score across ground-truth sentence alignments. Namely, gi"
2021.emnlp-main.471,Q19-1038,0,0.103417,"rphic without being well-aligned, so we quantify multilinguality in diverse ways. 4.1 Bitext Retrieval Task The bitext retrieval task consists of finding all sentences in a paired set of documents that are translations of each other. This process can be carried out between two comparable corpora, such as Wikipedia (“bitext mining”), but we use the 5,050 bitexts collected from the Bible corpus. We mine in two directions: for each sentence in document X , we find a match in document Y, and viceversa. We then take the intersection of those two searches, which has proven to be a useful heuristic (Artetxe and Schwenk, 2019a; Jones and Wijaya, 2021). Note that this task can be thought of as the sentence-level analog to the bilingual lexicon induction (BLI) task used in Vuli´c et al. (2020) and Dubossarsky et al. (2020). x and y, namely scoremargin (x, y) = 2k cos (x, y) P P cos (x, z) + z∈N Nk (y) cos (y, z) z∈N Nk (x) After retrieving sentence pairs in both directions and keeping the intersection, we compute standard F1-score against ground-truth alignments. Average margin score We also introduce a novel alignment metric in the form of the average margin score across ground-truth sentence alignments. Namely, gi"
2021.emnlp-main.471,2020.acl-main.536,0,0.16551,"nt variables (see Section 6.2). aligned than languages from two unrelated families (e.g., Japanese and Swahili)? Are languages which are geographically closer or share an alphabet better aligned? How do factors from linguistic typology (like word order and morphological marking) affect alignment? Recent work has looked at the typological and training-related factors affecting cross-lingual alignment in monolingual embedding space (Vuli´c et al., 2020; Dubossarsky et al., 2020), assessed the cross-linguality of pretrained language models using probing tasks and downstream performance measures (Conneau et al., 2020; Wu and Dredze, 2019, 2020; Pires et al., 2019; Groenwold et al., 2020), and probed Transformer models (Wolf et al., 2020) for linguistic structure (see Rogers et al. 2020 1 Introduction for an overview of over 150 studies). However, a Cross-lingual language models are polyglots inso- gap in the research exists regarding the following far as they house representations for many different question: What are the linguistic, quasi-linguistic, languages in the same space. But to what extent and training-related factors determining the crossare they good polyglots? The answer depends, linguality of"
2021.emnlp-main.471,Q19-1021,0,0.0275041,"y and subfamily as categorical features. For language pairs, we define two binary variables: same family and same subfamily, corresponding to whether two languages are in the same family or subfamily, respectively. 4 As validated by performance on downstream tasks. Morphological typology Though it is possible to make fine-grained distinctions in morphological typology in theory, we simply draw a binary distinction between languages that are widely considered polysynthetic (mostly Amerindian languages) and all other languages. Even more so than word order, morphological complexity is gradient (Cotterell et al., 2019). But we argue that polysynthetic languages pose a unique challenge for NLP systems and so perform one-vs-all binary coding such that individual languages are associated with a polysynthesis status and language pairs are associated with the feature same polysynthesis status. We classify 17 languages in the corpus as polysynthetic. Typological distance We also use typological word vectors from lang2vec7 (Malaviya et al., 2017), based on the URIEL8 typological database (Littell et al., 2017) to compute the distance between languages on the basis of aggregated linguistic features. Specifically, w"
2021.emnlp-main.471,P18-1128,0,0.0129559,"the previous experiments center around prediction of the dependent variables, we bolster our analysis with classical statistical methods that aim to explicitly control for covariates.15 Since we’re dealing with categorical features, we use ANCOVA (ANalysis of COVAriance). ANCOVA We run ANCOVAs separately for LaBSE and LASER and for each of the five dependent variables. We examine the languagepair-specific features, and look at same word order and same polysynthesis status separately as our “between” variables, and combined sentences, combined in-family sentences, and combined in15 We consult Dror et al. (2018); Achen (2005) for guidelines on statistical tests (in NLP). subfamily sentences as our three covariates. Overall, same word order had a statistically significant (p &lt;&lt; 0.05) effect for 8/10 ANCOVAs, though effect sizes (ηp2 ) were generally small16 (Cohen, 1988). Same polysynthesis status had a statistically significant effect for 10/10 ANCOVAs, with effect sizes being definitively small except for F1-score and ECOND-HM/average margin score (ηp2 ≈ 0.1-0.16 for LaBSE, ηp2 ≈ 0.05 for LASER). These results suggest that although same word order and same polysynthesis status are some of the more i"
2021.emnlp-main.471,2020.acl-main.560,0,0.0122462,"or implied, of the U.S. Government. The U.S. Government is authorized to reproduce 19 and distribute reprints for Government purposes All languages have no training data for LaBSE or LASER, and most have no in-family data either. notwithstanding any copyright notation herein. 5841 12 Ethical Considerations When drawing inferences about multilingual language models, it is crucial to take into account languages that are low-resource, Indigenous, and endangered. Previous works have looked at the challenges facing these sorts of under-resourced and under-studied languages (e.g. Mager et al. 2018; Joshi et al. 2020) and proposed broad solutions and guidelines (e.g. Kann et al. 2019; Bender 2019). The Bible corpus (Christodouloupoulos and Steedman, 2014) that we use in our analysis includes 35 languages that are zero-shot for LaBSE and 45 that are zero-shot for LASER, all of which could be classified as low-resource or extremely low-resource. This means that, for our case studies, we can test our conclusions on extremely low-resource languages (including Indigenous languages) that are typically underrepresented in NLP. While the Bible corpus enables us to extend our work to low-resource languages, we also"
2021.emnlp-main.471,D19-1329,0,0.0197296,"to reproduce 19 and distribute reprints for Government purposes All languages have no training data for LaBSE or LASER, and most have no in-family data either. notwithstanding any copyright notation herein. 5841 12 Ethical Considerations When drawing inferences about multilingual language models, it is crucial to take into account languages that are low-resource, Indigenous, and endangered. Previous works have looked at the challenges facing these sorts of under-resourced and under-studied languages (e.g. Mager et al. 2018; Joshi et al. 2020) and proposed broad solutions and guidelines (e.g. Kann et al. 2019; Bender 2019). The Bible corpus (Christodouloupoulos and Steedman, 2014) that we use in our analysis includes 35 languages that are zero-shot for LaBSE and 45 that are zero-shot for LASER, all of which could be classified as low-resource or extremely low-resource. This means that, for our case studies, we can test our conclusions on extremely low-resource languages (including Indigenous languages) that are typically underrepresented in NLP. While the Bible corpus enables us to extend our work to low-resource languages, we also acknowledge that the corpus owes its existence largely to a settle"
2021.emnlp-main.471,2020.lrec-1.352,0,0.0701103,"Missing"
2021.emnlp-main.471,P19-1493,0,0.356158,"uages from two unrelated families (e.g., Japanese and Swahili)? Are languages which are geographically closer or share an alphabet better aligned? How do factors from linguistic typology (like word order and morphological marking) affect alignment? Recent work has looked at the typological and training-related factors affecting cross-lingual alignment in monolingual embedding space (Vuli´c et al., 2020; Dubossarsky et al., 2020), assessed the cross-linguality of pretrained language models using probing tasks and downstream performance measures (Conneau et al., 2020; Wu and Dredze, 2019, 2020; Pires et al., 2019; Groenwold et al., 2020), and probed Transformer models (Wolf et al., 2020) for linguistic structure (see Rogers et al. 2020 1 Introduction for an overview of over 150 studies). However, a Cross-lingual language models are polyglots inso- gap in the research exists regarding the following far as they house representations for many different question: What are the linguistic, quasi-linguistic, languages in the same space. But to what extent and training-related factors determining the crossare they good polyglots? The answer depends, linguality of sentence representations in shared in part, on"
2021.emnlp-main.471,2020.emnlp-main.365,0,0.0535319,"Missing"
2021.emnlp-main.471,2020.tacl-1.54,0,0.0119758,"habet better aligned? How do factors from linguistic typology (like word order and morphological marking) affect alignment? Recent work has looked at the typological and training-related factors affecting cross-lingual alignment in monolingual embedding space (Vuli´c et al., 2020; Dubossarsky et al., 2020), assessed the cross-linguality of pretrained language models using probing tasks and downstream performance measures (Conneau et al., 2020; Wu and Dredze, 2019, 2020; Pires et al., 2019; Groenwold et al., 2020), and probed Transformer models (Wolf et al., 2020) for linguistic structure (see Rogers et al. 2020 1 Introduction for an overview of over 150 studies). However, a Cross-lingual language models are polyglots inso- gap in the research exists regarding the following far as they house representations for many different question: What are the linguistic, quasi-linguistic, languages in the same space. But to what extent and training-related factors determining the crossare they good polyglots? The answer depends, linguality of sentence representations in shared in part, on how well-aligned and isomorphic the embedding space, and what are the relative weights representations are, and not all lang"
2021.emnlp-main.471,2021.eacl-main.115,0,0.0381582,"ism between two vector spaces, of which we use three. Note that unlike Vuli´c et al. (2020) and Dubossarsky et al. (2020), who investigate isomorphism between monolingual spaces, we examine cross-lingual isomorphism within shared embedding space. These metrics thus technically quantify vector subspace isomorphism, where each subspace comprises embeddings in a particular language. Gromov-Hausdorff distance The Hausdorff distance between two metric spaces X and Y, given by Task performance Margin scoring, introduced by Artetxe and Schwenk (2019a), has shown success on the bitext retrieval task (Schwenk et al., 2021; Schwenk et al., 2019; Keung et al., 2021; H(X , Y) = max[sup inf d(x, y), sup inf d(x, y)] Tran et al., 2020; Fan et al., 2020; Jones and Wix∈X y∈Y y∈Y x∈X jaya, 2021). Margin score may be thought of as intuitively measures the worst-case distance be“relativized” cosine similarity, in that it selects vectween the nearest neighbors of X and Y (Vuli´c tors that “stand out” most among their neighbors in et al., 2020). The Gromov-Hausdorff distance then terms of proximity, rather than ones that are simply minimizes this distance over all isometric transclosest together. The method requires initi"
2021.emnlp-main.471,P19-1355,0,0.0476036,"Missing"
2021.emnlp-main.471,2020.wmt-1.139,0,0.0985957,"Missing"
2021.emnlp-main.471,vatanen-etal-2010-language,0,0.0434933,"the bitext retrieval task for English-Inuktitut, using raw and morphologically segmented Inuktitut, for LaBSE (top) and LASER (bottom). 9 Case Study 2: Word Order & Isomorphism To test the validity of our findings on the same word order feature, we examine whether embeddings in languages with similar word orders are more isomorphic to each other than those with substantially different word orders, sampling from a different corpus than the one we use in our main analysis. To this end, we select twelve zero-shot19 languages from the Universal Declaration of Human Rights (UDHR) parallel corpus (Vatanen et al., 2010). Six of these are canonically verb-initial: K’iche’, Mam, Chinanteco, Tzotzil, Mixteco, and Garifuna. The other six are subject-initial: Chickasaw, Quechua, Achuar-Shiwiar, Bambara, Dagaare, and Guarani. We hypothesize that similar-word-order language pairs will be more isomorphic, on average, than pairs of languages with disparate word orders. We  compute SVG and ECOND-HM across all 12 2 = 66 language pairs for LaBSE and LASER separately and group the results based on whether the language pairs have similar word order or different word order. The averages of these groups are given in Table"
2021.emnlp-main.471,2020.emnlp-main.257,0,0.0604361,"Missing"
2021.emnlp-main.471,D19-1077,0,0.343605,"ion 6.2). aligned than languages from two unrelated families (e.g., Japanese and Swahili)? Are languages which are geographically closer or share an alphabet better aligned? How do factors from linguistic typology (like word order and morphological marking) affect alignment? Recent work has looked at the typological and training-related factors affecting cross-lingual alignment in monolingual embedding space (Vuli´c et al., 2020; Dubossarsky et al., 2020), assessed the cross-linguality of pretrained language models using probing tasks and downstream performance measures (Conneau et al., 2020; Wu and Dredze, 2019, 2020; Pires et al., 2019; Groenwold et al., 2020), and probed Transformer models (Wolf et al., 2020) for linguistic structure (see Rogers et al. 2020 1 Introduction for an overview of over 150 studies). However, a Cross-lingual language models are polyglots inso- gap in the research exists regarding the following far as they house representations for many different question: What are the linguistic, quasi-linguistic, languages in the same space. But to what extent and training-related factors determining the crossare they good polyglots? The answer depends, linguality of sentence representat"
2021.emnlp-main.471,2020.repl4nlp-1.16,0,0.0172626,"ir relationship with the above metrics using diverse statistical analyses. • We uncover novel and pronounced effects of morphology agreement and word order agreement on cross-linguality, demonstrate the importance of in-family training data in ensuring multilinguality, and validate our linguistic findings with two empirical case studies on low-resource languages. 2 Related Work approach cross-linguality by focusing on zero-shot cross-lingual transfer in mBERT, and show that each mBERT layer retains language-specific information and that token overlap correlates with cross-lingual performance. Wu and Dredze (2020) home in on low-resource languages, finding that they often fail to reap the benefits of massively multilingual joint training but that their performance can be boosted by providing similar-language training data. Somewhat contrary to others’ results (including ours), Karthikeyan et al. (2020) find that lexical overlap factors in negligibly to cross-lingual transfer, while the depth of the network is integrally important. Vuli´c et al. (2020) and Dubossarsky et al. (2020) look at how typological features, trainingrelated factors, and measures of vector space isomorphism predict cross-lingual p"
2021.emnlp-main.471,P19-1307,0,0.0282374,"and (4) The analytic methods (a blend of prediction-based and classical statistical techniques, supplemented by performance-based case studies on extremely low-resource languages). Various studies have assessed the cross-linguality of pretrained language models. Recent efforts have approached this question via performance on an array of downstream NLP tasks (Conneau et al., 2020; Wu and Dredze, 2019, 2020; Karthikeyan et al., 2020; Pires et al., 2019; Groenwold et al., 2020), and others have proposed methods for better cross-lingual alignment in light of systematic crosslingual deficiencies (Zhang et al., 2019; Xia et al., 3 Bible Corpus 2021). Our study hews closest methodologically to Vuli´c et al. (2020) and Dubossarsky et al. (2020), The source of the bitexts we evaluate on is the superparallel Bible corpus2 from Christodouloupouwho investigate the determinants of cross-lingual isomorphism using monolingual fastText em- los and Steedman (2014), whence we gather texts beddings (Bojanowski et al., 2016; Joulin et al., for 101 languages and bitexts for 5,050 language pairs.3 We evaluate on the Books of Matthew and 2016; Mikolov et al., 2013). Findings from these studies have been mixed, John in th"
2021.findings-acl.416,W18-5521,0,0.0279917,"conspiracy theories, we now investigate the effects of varying the two together. In our previous experiments, we utilize Mechanical Turk to identify conspiracy theories among the generated text. However, we understand that human evaluation is not feasible for detecting conspiracy theories on a large scale. Instead, we desire to advance towards a more automated evaluation of memorization. As such, we investigate whether we can define a relationship between the memorization of conspiracy theories and perplexity across the different model parameters. Following previous studies on fact-checking (Chakrabarty et al., 2018; Wang and McKeown, 2010) and model memorization (Carlini et al., 2020), we evaluate model generations against Google search results. This time, we utilize our General dataset, made up of conspiracy theories generated with the generic prompt “The conspiracy theory is that”. We query Google with a generated conspiracy theory at each temperature setting and compare this theory to the first page of results. We did not manually use Google search for our generated text and instead created a script to automate this and scrape the text from the first page of results. We provided the minimum amount of"
2021.findings-acl.416,P18-1082,0,0.0302024,"DNA alteration and claims of the pandemic acting as a cover plan to implant trackable microchips3 . The belief in these theories can prevent herd immunity through the lack of vaccinations4 5 . 2.2 NLG spreading conspiracy theories As NLG models are being utilized for various tasks such as chatbots and recommendations systems (Gatt and Krahmer, 2018), cases arise in which these conspiracy theories and other biases can propagate unintentionally (Bender et al., 2021). We present one such scenario in which an NLG model has memorized some conspiracy theories and is being used for story generation (Fan et al., 2018). An unaware individual may utilize this application and, given a prompt about the Holocaust, may receive a generated story discussing Holocaust denial. The user, now having been exposed to a new conspiracy theory, may choose to ignore this generated text at this stage. However, a potential negative outcome is that the user may become interested in this story and search the statements online out of curiosity. This can lead the user down the “rabbit hole” of conspiracy theories online (O’Callaghan et al., 2015) and alter their original assumptions towards believing this conspiracy theory. 2.3 W"
2021.findings-acl.416,2020.emnlp-main.473,1,0.772422,"emorization in generative language models. 1 Introduction Recent advances in natural language processing technologies have opened a new space for individuals to digest information. One of these rapidly developing technologies is neural natural language generation. These models, made up of millions, or even billions (Brown et al., 2020), of parameters, train on large-scale datasets. While attempts are made to ensure that only “safe” data is utilized for training these models, several studies have shown the prevalence of biases produced by these pretrained generation models (Sheng et al., 2019; Groenwold et al., 2020; Solaiman et al., 2019). Of equally alarming concern are the memorization and subsequent generation of factually incorrect data. Conspiracy theories are one particular type of this data that can be especially damaging. While it is not new for researchers to learn that a model may memorize data (Radhakrishnan et al., 2019), we argue that the growing usage of machine learning models in society warrants targeted investigation to deter potential harms from problematic data. In this paper, we address the upsides and pitfalls of memorization in generative language models and its relationship with c"
2021.findings-acl.416,P16-1154,0,0.038659,"the model as a black box when evaluating its outputs. Due to the difficulty of distinguishing among the three categories of memorization, generalization, and hallucination, we follow previous work and refer to both memorized and generalized generations as memorized samples for the rest of the paper. 4 When is memorization a good thing? While we focus most of this paper on the downsides of memorization in natural language generation models, it is still important to address the benefits. There are several situations in which memorized information may be utilized, such as in dialogue generation (Gu et al., 2016). When used in the chatbot setting, a model may be asked questions on real-world knowledge. Assuming the model has learned correct factual information, this memorization can prove useful. Furthermore, conspiracy theories are a part of language and culture. It is not inherently bad that a model is aware of the existence or concept of conspiracy theories, particularly in cases where models may be deployed as an intervention in response to human-written conspiratorial text. This only becomes harmful when the model cannot recognize text as a conspiracy theory and generates text from the viewpoint"
2021.findings-acl.416,P19-1256,0,0.0185268,"en further. In the context of conspiracy theories, we establish three types of generations: • Memorized: generated conspiracy theories with exact matches existing within the training data. • Generalized: generations that do not have exact matches in the data but produce text that follows the same ideas as those in the training data. • Hallucinated: generations about topics that are neither factually correct nor follow any of the existing conspiracy theories surrounding the topic. Studies on memorization tend to focus on either memorization vs. generalization or memorization vs. hallucination (Nie et al., 2019). In the latter case, it is easy to see how the term “memorization” can apply to the first two categories. Ideally, in an NLG model, we would hope for generations to be generalized since direct memorization can have the downsides of generating sensitive information (Carlini et al., 2019). There are also cases when hallucinations are ideal, such as in the realm of creative story-telling. Should we be able to distinguish among these categories, we could gain deeper insight into what and how these models learn during training. However, we acknowledge that classifying generations based on these ca"
2021.findings-acl.416,P02-1040,0,0.112553,"3: Spearman correlation of model perplexity vs. Google search BLEU score for GPT-2 generated conspiracy theories across varying temperature settings. Each generated theory is evaluated against the first page of Google search results with the BLEU metric. more specific location information or cookies. The temperature values of 0.4, 0.7, and 1 are used as lower temperature values start to produce many duplicate generations and lead to small sample sizes for this evaluation. We obtain the text snippet under each search result and evaluate this against the conspiracy theory with the BLEU metric (Papineni et al., 2002). The BLEU metric is utilized since many search results do not contain complete sentences and are instead highlighted phrases from the text related to the query and concatenated by ellipses. The perplexity score for a conspiracy theory is then calculated for each model size. The resulting BLEU and perplexity scores are ranked with the highest BLEU and lowest perplexity scores first. We use Spearman’s ranking correlation (Hogg et al., 2005) to determine the resulting alignment between the two. These results are shown in Figure 3. We find a strong relationship between a generated conspiracy theo"
2021.findings-acl.416,D19-1339,0,0.0220973,"f the drawbacks of memorization in generative language models. 1 Introduction Recent advances in natural language processing technologies have opened a new space for individuals to digest information. One of these rapidly developing technologies is neural natural language generation. These models, made up of millions, or even billions (Brown et al., 2020), of parameters, train on large-scale datasets. While attempts are made to ensure that only “safe” data is utilized for training these models, several studies have shown the prevalence of biases produced by these pretrained generation models (Sheng et al., 2019; Groenwold et al., 2020; Solaiman et al., 2019). Of equally alarming concern are the memorization and subsequent generation of factually incorrect data. Conspiracy theories are one particular type of this data that can be especially damaging. While it is not new for researchers to learn that a model may memorize data (Radhakrishnan et al., 2019), we argue that the growing usage of machine learning models in society warrants targeted investigation to deter potential harms from problematic data. In this paper, we address the upsides and pitfalls of memorization in generative language models and"
2021.naacl-main.249,D15-1001,0,0.0252555,"and learning those latent hints still relies on the sparse environmental reward. To deal with this issue, we propose semi-supervised initialization (SSI) that enables the agent to experience various possible hints in advance. We adopt a hint module to generate possible hints for random states and allow the agent to learn from them. With SSI, agents have a better-initialized policy during training for each task. From another point of view, in this paper, we propose a semi-supervised initialization method and investigate the abilities of NLP on controlling complex actions in game environments (Narasimhan et al., 2015; Ammanabrolu and Riedl, 2019). We perform SSI first and train-evaluate on those tasks built from ExtLang. Experimental results show that with SSI, better-initialized policy not only learns faster but also has a higher success rate. Most Reinforcement Learning (RL) methods (Mnih et al., 2013, 2016) rely on an agent to explore and maximize the feedback reward. Since designing a reward for each step is impractical, a common setting is only to give out the achieved signal. In Atari Grand Challenge (Kurin et al., 2017), only if achieving the goal, the environmental reward is 1. However, this spars"
2021.naacl-main.249,N19-1358,0,0.0152324,"t hints still relies on the sparse environmental reward. To deal with this issue, we propose semi-supervised initialization (SSI) that enables the agent to experience various possible hints in advance. We adopt a hint module to generate possible hints for random states and allow the agent to learn from them. With SSI, agents have a better-initialized policy during training for each task. From another point of view, in this paper, we propose a semi-supervised initialization method and investigate the abilities of NLP on controlling complex actions in game environments (Narasimhan et al., 2015; Ammanabrolu and Riedl, 2019). We perform SSI first and train-evaluate on those tasks built from ExtLang. Experimental results show that with SSI, better-initialized policy not only learns faster but also has a higher success rate. Most Reinforcement Learning (RL) methods (Mnih et al., 2013, 2016) rely on an agent to explore and maximize the feedback reward. Since designing a reward for each step is impractical, a common setting is only to give out the achieved signal. In Atari Grand Challenge (Kurin et al., 2017), only if achieving the goal, the environmental reward is 1. However, this sparse-reward setting makes the age"
2021.naacl-main.469,2020.acl-main.135,1,0.841429,"dge Generate a comparison-type multi-hop question QC by fusing two single-hop questions Selection Generation Fusion Table 1: The 8 basic operators for MQA-QG, categorized into 3 groups. Selection: retrieve relevant information from contexts. Generation: generate information from a single context. Fusion: fuse retrieved/generated information to construct multi-hop questions. Each operator is defined as a function mapping f (X) → Y . have largely been addressed. QG research has started to generate more complex questions that require deep comprehension and multi-hop reasoning (Tuan et al., 2020; Pan et al., 2020; Xie et al., 2020; Yu et al., 2020). For example, Tuan et al. (2020) proposed a multi-state attention mechanism to mimic the multi-hop reasoning process. Pan et al. (2020) parsed the input passage as a semantic graph to facilitate the reasoning over different entities. However, these supervised methods require large amounts of human-written multi-hop questions as training data. Instead, we propose the first unsupervised QG system to generate multi-hop questions without the need to access those annotated data. 3 Methodology nents: operators, reasoning graphs, and question filtration. Operators"
2021.naacl-main.469,P02-1040,0,0.116995,"Missing"
2021.naacl-main.469,2020.emnlp-main.89,0,0.0222683,"able T into a document PT and then respectively. The remaining reasoning graphs defeed PT to the pre-trained GPT-2 model (Radford fine four types of multi-hop questions. 1) Table-toet al., 2019) to generate the output sentence Y . To avoid irrelevant information in PT , we apply a tem- Text: bridge-type question between table and text, where the answer comes from the text. 2) Text-toplate that only describes the row where the target Table: bridge-type question between table and text, entity locates. We then finetune the model on the where the answer comes from the table. 3) TextToTTo dataset (Parikh et al., 2020), a large-scale to-Text: bridge-type question between two texts. dataset of controlled table-to-text generation, by 4) Comparison: comparison-type question based maximizing the likelihood of p(Y |PT ; β), with β denoting the model parameters. The implementa- on two passages. These four reasoning chains can tion details and the model evaluation are in Ap- cover a large portion of questions in existing multihop QA datasets, such as HotpotQA and HybridQA. pendix A.1. We generate QA pairs by executing each reasoning • QuesToSent: This operator convert a question graph. Our framework can easily ext"
2021.naacl-main.469,2020.emnlp-main.713,0,0.0193627,"ltiple data sources in order to propose a reasonable question. Extractive Question Answering (EQA) is the task of answering questions by selecting a span from To address the above problem, we pursue a more the given context document. Works on EQA can realistic setting, i.e., unsupervised multi-hop QA, be divided into the single-hop (Rajpurkar et al., in which we assume no human-labeled multi-hop 2016, 2018; Kwiatkowski et al., 2019) and multi- question is available for training, and we explore hop cases (Yang et al., 2018; Welbl et al., 2018; the possibility of generating human-like multi-hop Perez et al., 2020). Unlike single-hop QA, which question–answer pairs to train the QA model. We assumes the question can be answered with a sin- study multi-hop QA for both the homogeneous gle sentence or document, multi-hop QA requires case where relevant evidence is in the textual combining disjoint pieces of evidence to answer forms (Yang et al., 2018) and the heterogeneous a question. Though different well-designed neu- case where evidence is manifest in both tabular ral models (Qiu et al., 2019; Fang et al., 2020) and textual forms (Chen et al., 2020b). Though have achieved near-human performance on the su"
2021.naacl-main.469,2020.emnlp-main.468,0,0.0268573,"Unsupervised / Zero-Shot QA has been proposed to train question answering models without any humanlabeled training data. Lewis et al. (2019) proposed the first unsupervised QA model which generates synthetic (context, question, answer) triples to train the QA model using unsupervised machine translation. However, the generated questions are unlike human-written questions and tend to have a lot of lexical overlaps with the context. To address this, followup works utilized the Wikipedia cited documents (Li et al., 2020), predefined templates (Fabbri et al., 2020), or pretrained language model (Puri et al., 2020) to produce more natural questions resembling the human-annotated ones. However, all the existing studies are focused on the SQuAD (Rajpurkar et al., 2016) dataset to answer single-hop and text-only questions. These methods do not generalize to multi-hop QA because they lack integrating and reasoning over disjoint pieces of evidence. Furthermore, they are restricted to text-based QA without considering structured or semi-structured data sources such as KB and Table. In contrast, we propose the first framework for unsupervised multi-hop QA, which can reason over disjoint structured or unstructu"
C10-1129,P08-1092,0,0.0141358,"eature set that consists of interesting task-specific features. For example, they monitor the number of previously submitted edits from the same author or IP, which is a good feature to model author contribution. Their other contributions are the use of a logistic regression classifier, as well as the use of lexical features. They successfully demonstrate the use of lexical features like vulgarism frequency. Using all features, they reach an average precision of 0.83 and recall of 0.77. In addition to previous work on vandalism detection, there is also earlier work using the web for modeling. Biadsy et al. (2008) extract patterns in Wikipedia to generate biographies automatically. In their experiment, they show that when using Wikipedia as the only resource for extracting named entities and corresponding collocational patterns, although the precision is typically high, recall can be very low. For that reason, they choose to use Google to retrieve training data from the Web. In our approach, instead of using Wikipedia edits and historical revisions, we also select the Web as a resource to train our shallow syntactic and semantic models. 3 Analysis of Types of Vandalism In order to better understand the"
C10-1129,P05-1063,0,0.0900022,"kle all the above types of vandalism, and our focus is on the syntactically ill-formed and semantically illintentioned types that could not be detected by rule-based systems and straightforward lexical features. 1148 4 Our System We propose a shallow syntactic-semantic focused classification approach for vandalism detection (Table 2). In contrast to previous work, our approach concentrates on the aspect of using natural language techniques to model vandalism. Our shallow syntactic and semantic modeling approaches extend the traditional n-gram language modeling method with topic-specific ntag (Collins et al., 2005) syntax models and topic-specific syntactic n-gram semantic models. Moreover, in the Wikipedia vandalism detection task, since we do not have a sufficient amount of training data to model the topic of each edit, we propose the idea of using the Web as corpus by retrieving search engine results to learn our topic-specific n-tag syntax and syntactic n-gram semantic models. The difference between our syntactic and semantic modeling is that n-tag syntax models only model the order of sentence constituents, disregarding the corresponding words. Conversely, for our syntactic n-gram models, we do kee"
C10-1129,W00-1308,0,0.122941,"ams for Shallow Syntactic and Semantic Modeling In Figure 1, we present the overview of our approach, which uses Web-trained topic-specific training for both: (1) n-tag syntax models for shallow syntactic modeling and (2) syntactic ngram models for shallow semantic modeling. For each Wikipedia edit, we consider its title as an approximate semantic representation, using it as a query to build topic-specific models. In addition, we also use the title information to model the syntax of this topic. Given Rdiff, we produce the syntactic version of the diff-ed text using a probabilistic POS tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). The edit title is extracted from the corpus (either Rnew or Rold) and is used to query multiple Web search engines in order to collect the n-tag and n-gram training data from the top-k results. Before we start training language models, we tag the top-k results using the POS tagger. Note that when modeling n-tag syntax models, it is necessary to remove all the words. With the POS-only sequences, we train topic-specific ntag models to describe the syntax of normal text on the same topic associated with this edit. With the original tagged sequences, we train syntactic n"
C10-1129,N03-1033,0,0.250537,"Semantic Modeling In Figure 1, we present the overview of our approach, which uses Web-trained topic-specific training for both: (1) n-tag syntax models for shallow syntactic modeling and (2) syntactic ngram models for shallow semantic modeling. For each Wikipedia edit, we consider its title as an approximate semantic representation, using it as a query to build topic-specific models. In addition, we also use the title information to model the syntax of this topic. Given Rdiff, we produce the syntactic version of the diff-ed text using a probabilistic POS tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). The edit title is extracted from the corpus (either Rnew or Rold) and is used to query multiple Web search engines in order to collect the n-tag and n-gram training data from the top-k results. Before we start training language models, we tag the top-k results using the POS tagger. Note that when modeling n-tag syntax models, it is necessary to remove all the words. With the POS-only sequences, we train topic-specific ntag models to describe the syntax of normal text on the same topic associated with this edit. With the original tagged sequences, we train syntactic n-gram models to represent"
C18-1002,D13-1135,0,0.575604,"is the first rule-based study that integrate Hobbs-algorithm into the resolution of zero pronoun in the Chinese Treebank (Xue et al., 2005). After that, a variety of learning-based methods have been investigated. Zhao and Ng (2007) use the learning-based model to locate and resolve zero anaphoras. They investigate a serious of features and apply the decision-tree algorithm to train the classifier. To better capture the syntactic-level information, Kong and Zhou (2010) introduce the context sensitive tree-kernel unified framework for zero anaphor resolution. On the base of Zhao and Ng (2007), Chen and Ng (2013) further investigate their model, introducing two extensions to the resolver, namely, novel features and zero pronoun links. However, these work deeply rely on annotation dataset. To alleviate this issue, Chen and Ng (2014) present the first unsupervised model that first convert zero anaphoras into ten pre-defined pronouns and then apply a ranking-based pronoun resolution model to select antecedent mentions. Chen and Ng (2015) build a discourse-aware model that can jointly locate and resolve zero anaphoras. More recently, with the advance of neural network techniques, deep-learning-based metho"
C18-1002,P15-2053,0,0.802613,"tic-level information, Kong and Zhou (2010) introduce the context sensitive tree-kernel unified framework for zero anaphor resolution. On the base of Zhao and Ng (2007), Chen and Ng (2013) further investigate their model, introducing two extensions to the resolver, namely, novel features and zero pronoun links. However, these work deeply rely on annotation dataset. To alleviate this issue, Chen and Ng (2014) present the first unsupervised model that first convert zero anaphoras into ten pre-defined pronouns and then apply a ranking-based pronoun resolution model to select antecedent mentions. Chen and Ng (2015) build a discourse-aware model that can jointly locate and resolve zero anaphoras. More recently, with the advance of neural network techniques, deep-learning-based methods are introduced and have been demonstrated to be effective for this task. Chen and Ng (2016) first introduce a feed-forward neural network framework, where zero anaphoras are encoded by its previous word and headword. However, their model overlooks context of a zero anaphora, which inevitably misses some valuable information. Naturally, some works try to alleviate this issue by investigating information from associate texts."
C18-1002,P16-1074,0,0.227082,"in Chinese 基于注意力神经网络模型的零指代消解研究 传统的关于零指代的方法提出了多种关于先行语和零代词的表示模型。这些模型中，研 究者们用零代词的上下文信息来帮助表示缺省的信息。为了更好的帮助建模零代词，我 们提出了一种基于注意力机制的神经网络模型，通过注意力模型来获取更有表示性信 息的上下文信息。实验结果表明：我们的方法能够有效提升效果，并在中文OntoNotes 5.0数据集上取得了最好的结果，超越了现有的基准系统。 1 Introduction In natural languages, expressions that can be deduced contextually by people are frequently omitted in texts. This is special the case in pro-dropped languages, such as Chinese, where a kind of anaphoric expression is frequently eliminated. A zero pronoun is a gap in the sentence that is found when a phonetically null form is used to refer to a real-world entity (Chen and Ng, 2016). We here show a case of zero pronouns from the OntoNotes-5.0 dataset. 这 次 地震 φ1 有 一些 房屋 塌 的 , 这 里面 如果 有 建房 的 质量 问题 ， φ2 是 要 追究 责任 的 。 In this earthquck φ1 some rooms collapsed, if there exsit some room quality issues, φ2 will need to call to account. We use φ to represent the zero pronouns in this example. Among these zero anaphoras, we can assign the mention “政府/the government” that appears in leading text, to be the antecedent of φ2 while there are no such mentions for φ1 . Hence, φ2 is an anaphoric zero pronoun, and φ1 is the un-anaphoric case. ∗ Corresponding author. This work is licensed"
C18-1002,P00-1022,0,0.865194,"Missing"
C18-1002,P11-1081,0,0.442913,"hnique for modeling candidates, our model learns to encode more informative parts of the mentions. All these bring advantages to the resolution of zero pronouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphor"
C18-1002,P06-1079,0,0.736449,"to encode more informative parts of the mentions. All these bring advantages to the resolution of zero pronouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphoric zero pronoun resolution. Compared to the"
C18-1002,D15-1260,0,0.44975,"lution of zero pronouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphoric zero pronoun resolution. Compared to the prior studies that have the underutilized context of zero pronouns, we investigate an att"
C18-1002,D16-1132,0,0.184762,"ouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphoric zero pronoun resolution. Compared to the prior studies that have the underutilized context of zero pronouns, we investigate an attention 15 mechanism"
C18-1002,W03-1024,0,0.737876,"didates, our model learns to encode more informative parts of the mentions. All these bring advantages to the resolution of zero pronouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphoric zero pronoun resolutio"
C18-1002,D10-1086,0,0.651867,"summary of early efforts for zero pronoun resolution both for Chinese and other languages. 2.1 Zero Pronoun Resolution for Chinese Converse (2006) is the first rule-based study that integrate Hobbs-algorithm into the resolution of zero pronoun in the Chinese Treebank (Xue et al., 2005). After that, a variety of learning-based methods have been investigated. Zhao and Ng (2007) use the learning-based model to locate and resolve zero anaphoras. They investigate a serious of features and apply the decision-tree algorithm to train the classifier. To better capture the syntactic-level information, Kong and Zhou (2010) introduce the context sensitive tree-kernel unified framework for zero anaphor resolution. On the base of Zhao and Ng (2007), Chen and Ng (2013) further investigate their model, introducing two extensions to the resolver, namely, novel features and zero pronoun links. However, these work deeply rely on annotation dataset. To alleviate this issue, Chen and Ng (2014) present the first unsupervised model that first convert zero anaphoras into ten pre-defined pronouns and then apply a ranking-based pronoun resolution model to select antecedent mentions. Chen and Ng (2015) build a discourse-aware"
C18-1002,P17-1010,1,0.759771,"vitably misses some valuable information. Naturally, some works try to alleviate this issue by investigating information from associate texts. Yin et al. (2017a) introduce a novel memory-based network neural network model that learns to encode zero anaphoras by its texts and antecedent mentions. They take advantage of multihops architecture, producing abstract information from external-memories as hints for explaining zero anaphoras. Yin et al. (2017b) focus on encoding global-information for candidates, where a hierarchical candidate encoder is introduced that learns to model the candidates. Liu et al. (2017) investigate the issue of generating pseudo training-data for the task of zero anaphora resolution. They use a novelty two-step training strategy that helps to overcome the diversity between the generated pseudo training-data and the real one. Even though these above-mentioned methods can reveal the semantic of zero anaphoras by its context, they regard all the words equally, overlooking the diversity of different words. In this paper, we focus on exploring an effective way of modeling zero pronoun by using the associated texts. More specifically, we integrate a novel self-attentive mechanism,"
C18-1002,I11-1085,0,0.630928,"he mentions. All these bring advantages to the resolution of zero pronouns. 2.2 Zero Pronoun Resolution for other Languages There has been a variety of work on zero pronoun resolution for other languages besides Chinese, such as Korean and Japanese. These methods could be categorized as rule-based and learning-based. Ferr´andez and Peral (2000) investigate a rule-based method that can encode preferences for candidates for resolving zero anaphoras in Spanish. In recent time, learning-based methods (Han, 2006; Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006; Iida et al., 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015; Iida et al., 2016) have been well studied. Iida et al. (2016) present a novel CNNbased deep neural network model for intrasentential subjective zero anaphora resolution in Japanese. As clues, they use both the surface-word and the dependency tree-structure of a sentence. Their model gains higher precision, which is needed for real-world natural language processing (NLP) applications. 3 Methodology We introduce an attention-based neural network model for anaphoric zero pronoun resolution. Compared to the prior studies that have the underutilized conte"
C18-1002,D16-1021,1,0.813139,"argue that some of the words in noun phrases contains more important information than others, a need that leads to the usage of attention mechanism. To alleviate the above-mentioned issues, in this paper, we propose a novel attention-based neural network model to deal with the task. Following existing neural network work for Chinese zero pronoun resolution (Chen and Ng, 2016; Yin et al., 2017a; Yin et al., 2017b), we focus on anaphoric zero pronoun resolution task, introducing a pair-wise model to resolve anaphoric zero pronouns. For some natural language processing tasks (Mnih et al., 2014; Tang et al., 2016), people investigate to apply attention mechanism on top of the convolutional neural network or recurrent neural network to introduce an extra source of information to guide the modeling of useful information. However, since zero pronouns are simple gaps that have no such kind of extra information, the above-mentioned attention mechanism can rarely be directly practiced for modeling these gaps. Inspired by (Lin et al., 2017), we here investigate the usage of a self-attentive mechanism for encoding the zero anaphoras. With the help of self-attentive mechanism, our model is able to effectively f"
C18-1002,D17-1135,1,0.274906,". ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 13 Proceedings of the 27th International Conference on Computational Linguistics, pages 13–23 Santa Fe, New Mexico, USA, August 20-26, 2018. With the fact that zero pronouns are gaps that have no text, it is almost impracticable to representant the zero pronouns by themselves. This issue has received increasingly attention. In recent time, deep neural network methods for Chinese zero pronoun resolution (Chen and Ng, 2016; Yin et al., 2017a; Yin et al., 2017b) have been proposed and are intended to encode zero pronouns into the vector-space semantic by additional elements. Chen and Ng (2016) propose a neural network model that learns to encode the anaphoric zero pronoun by using the leading word and governing verb, which leads to the insufficient text issue. To better use associated text information for expressing zero pronouns, Yin et al. (2017a) present the ZP-centered-LSTM architecture that learns to encode zero pronouns by their text words. However, it could bring with some defects: their model regards all the words in the"
C18-1002,D07-1057,0,0.802724,"noun resolution. Next, we will intorduce our attention-based neural network model in Section 3. In Section 4, empirical evaluation results are shown. And finally, we conclude in Section 5. 14 2 Related Work In this section, we give a brief summary of early efforts for zero pronoun resolution both for Chinese and other languages. 2.1 Zero Pronoun Resolution for Chinese Converse (2006) is the first rule-based study that integrate Hobbs-algorithm into the resolution of zero pronoun in the Chinese Treebank (Xue et al., 2005). After that, a variety of learning-based methods have been investigated. Zhao and Ng (2007) use the learning-based model to locate and resolve zero anaphoras. They investigate a serious of features and apply the decision-tree algorithm to train the classifier. To better capture the syntactic-level information, Kong and Zhou (2010) introduce the context sensitive tree-kernel unified framework for zero anaphor resolution. On the base of Zhao and Ng (2007), Chen and Ng (2013) further investigate their model, introducing two extensions to the resolver, namely, novel features and zero pronoun links. However, these work deeply rely on annotation dataset. To alleviate this issue, Chen and"
D13-1086,D08-1072,0,0.0479371,"Missing"
D13-1086,N09-1068,0,0.152453,"Missing"
D13-1086,P07-1033,0,0.406157,"Missing"
D13-1086,D12-1119,0,0.0225342,"Missing"
D13-1086,N13-1080,0,0.352305,"Missing"
D13-1131,D12-1124,0,0.0955751,"processing, researchers have studied the problem of brand identification from image using histogram comparison (Pelisson et al., 2003). However, to the best of our knowledge, even though textual data is vastly available, the problems of automatic brand identification from raw text and computational branding analytics, are new. Although the domain of our data is on branding, our work also aligns with previous work in text and language classification. Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the L1,∞ sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for structured predicti"
D13-1131,P11-1137,0,0.110024,"nding analytics, are new. Although the domain of our data is on branding, our work also aligns with previous work in text and language classification. Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the L1,∞ sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for structured prediction problems. Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Paul a"
D13-1131,D11-1139,0,0.02433,"n to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the L1,∞ sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for structured prediction problems. Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Paul and Dredze, 2012). Besides lacking sparsity, since the traditional discriminative methods in NLP often use interdependent features such as n-grams tokens, and part-ofspeech tags, they also suffer from the problem of n"
D13-1131,W11-2018,1,0.822974,"transpose ~ j , and assume that each feature trix, I = j X (e.g. unigram in our task) is a random variable that has a multinomial distribution over the instances in the training set. Then, we compare each pair of features, and calculate the interfeature distance matrix Dist with Euclidean distance as a measure, and use the k-nearest neighbors (kNN) method (Beyer et al., 1999) to select the top neighbors of each feature. 2. Derive the affinity matrix A. To assign the weight on the edge E(p,q) for each connected nodes (the kNN of V in Dist), we use the cosine similarity cosine(Vp , Vq ) metric (Wang and Hirschberg, 2011). 3. Generate the degree matrix D and Laplacian matrix L. As discussed earlier, we sum up the symmetric affinity matrix by row, and obtain a diagonal degree matrix D, and we further define a Laplacian matrix L = D − A. To calculate the above matrices in an efficient manner, we partition the covariate into blocks, and process each block in parallel (Chen et al., 2011). An intuitive example of the graph G, its associated affinity matrix A, and Laplacian matrix L, is shown in Figure 1. Parameter Estimation: Regarding the optimization of objective function in (12-13), a notable problem is that the"
D13-1131,W12-1603,1,0.779628,"ns with previous work in text and language classification. Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the L1,∞ sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for structured prediction problems. Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Paul and Dredze, 2012). Besides lacking sparsity, since the traditional discriminative metho"
D13-1131,P12-1078,1,0.82474,"ns with previous work in text and language classification. Over the years, logistic regression and linear kernel SVM have shown to be very successful in various regression and classification tasks in NLP (Chahuneau et al., 2012; Biadsy et al., 2011). Recently, sparse discriminative methods that model the sparse nature of text become attractive, because unlike dense models, they are less likely to overfit to the training data, easier to interpret, and often lead to state-of-the-art results. For example, Eisenstein et al. (2011b) use the L1,∞ sparsity model to discover sociolinguistic patterns. Wang et al. (2012a) compare lasso, ridge, and elastic net models to predict impoliteness behaviors in teenager conversations. Martins et al. (2011) investigate the tree-structured overlapping group lasso for structured prediction problems. Chen et al. (2013) study the use of element-wise, group-wise, and hierarchical sparsity models for dialogue act classifcation. Sparse inducing priors are also investigated and shown to be effective in generative models for topic modeling (Eisenstein et al., 2011a; Wang et al., 2012b; Paul and Dredze, 2012). Besides lacking sparsity, since the traditional discriminative metho"
D14-1122,W09-2307,0,0.0610002,"Missing"
D14-1122,C10-3004,0,0.0239477,"Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have al"
D14-1122,C02-1126,0,0.0732128,"Missing"
D14-1122,D07-1098,0,0.0226316,"first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chine"
D14-1122,I11-1136,0,0.0214741,"endency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the"
D14-1122,D14-1108,1,0.831036,"(Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset (Ling et al., 2013) for dependency annotation. An interesting aspect of this Weibo dataset is that, besides the Chinese posts, it also includes a copy of the English translations. This allows us to observe some interesting phenomena that mark the differences of the two languages. For example: • Function words are more frequently used in English than in Chinese. When examinFigure 1: An example of pro-drop phenomenon from the Weibo d"
D14-1122,P03-1056,0,0.306413,"d L is the length; (2) a database D of token relations from the corpus; (3) first-order logic inference rule set R. 4 4.1 A Programmable Parser with Personalized PageRank Inference A key problem in multilingual dependency parsing is that generic feature templates may not work well for every language. For example, Martins (2012) shows that for Chinese dependency parsing, when adding the generic grandparents and siblings features, the performance was worse than using the standard bilexical, unilexical, and part-of-speech features. Unfortunately, for many parsers such as Stanford Chinese Parser (Levy and Manning, 2003) and MaltParser (Nivre et al., 2007), it is very difficult for programmers to specify the feature templates and inference rules for dependency arc prediction. In this work, we present a Chinese dependency parsing method for Weibo, based on efficient probabilistic first-order logic programming (Wang et al., 2013). The advantage of probabilistic programming for parsing is that, software engineers can simply conduct theory engineering, and optimize the performance of the parser for a specific genre of the target language. Recently, probabilistic programming approaches (Goodman et al., 2012; Wang"
D14-1122,D12-1132,0,0.0227334,"hared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Li"
D14-1122,D11-1109,0,0.0154232,"rized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 201"
D14-1122,P13-1018,0,0.0336836,"and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset (Ling et al., 2013) for dependency annotation. An interesting aspect of this Weibo dataset is that, besides the Chinese posts, it also includes a copy of the English translations. This allows us to observe some interesting phenomena that mark the differences of the two languages. For example: • Function words are more frequently used in English than in Chinese. When examinFigure 1: An example of pro-drop phenomenon from the Weibo data. ing this English version of the Weibo corpus for the total counts of the word “the”, there are 2,084 occurrences in 2,003 sentences. Whereas in Chinese, there are only 52 occurren"
D14-1122,C12-1106,0,0.0370394,"Missing"
D14-1122,J93-2004,0,0.046559,"Internet users (Yang et al., 2012), making it one of the most popular social media services in the world. While Weibo posts are abundantly available, NLP techniques for analyzing Weibo posts have not been well-studied in the past. Syntactic analysis of Weibo is made difficult for three reasons: first, in the last few decades, Computational Linguistics researchers have primarily focused on building resources and tools using standard English newswire corpora2 , and thus, 1 http://en.wikipedia.org/wiki/Sina Weibo For example, Wall Street Journal articles are used for building the Penn Treebank (Marcus et al., 1993). 2 • We show that the proposed approach outperforms an off-the-shelf dependency parser, as well as a strong baseline trained on the same in-domain data. In the next section, we describe existing work on dependency parsing for Chinese. In Section 3, we present the new Chinese Weibo Treebank to the research community. In Section 4, we introduce the proposed efficient probabilistic programming approach for parsing Weibo. We show the experimental results in Section 5, and conclude in Section 6. 3 http://www.cs.cmu.edu/˜yww/data/WeiboTreebank.zip 1152 Proceedings of the 2014 Conference on Empirica"
D14-1122,P05-1012,0,0.0784823,", William W. Cohen Language Technologies Institute & Machine Learning Department Carnegie Mellon University Pittsburgh, PA 15213, USA. {yww,lingpenk,krivard,wcohen}@cs.cmu.edu Abstract there are fewer resources in other languages in general. Second, microblog posts are typically short, noisy (Gimpel et al., 2011), and can be considered as a “dialect”, which is very different from news data. Due to the differences in genre, part-of-speech taggers and parsers trained on newswire corpora typically fail on social media texts. Third, most existing parsers use languageindependent standard features (McDonald et al., 2005), and these features may not be optimal for Chinese (Martins, 2012). To most of the application developers, the parser is more like a blackbox, which is not directly programmable. Therefore, it is non-trivial to adapt these generic parsers to language-specific social media text. In this paper, we present a new probabilistic dependency parsing approach for Weibo, with the following contributions: Dependency parsing is a core task in NLP, and it is widely used by many applications such as information extraction, question answering, and machine translation. In the era of social media, a big chall"
D14-1122,P14-5021,0,0.0195765,"ly prominent in the short text, which clearly creates a problem for parsing. For example, in the Chinese Weibo data, we have observed the sentence in Figure 1. To facilitate the annotation process, we first preprocess the Weibo posts using the Stanford NLP pipeline, including a Chinese Word Segmenter (Tseng et al., 2005) and a Chinese Partof-Speech tagger (Toutanova and Manning, 2000). Two native speakers of Chinese with strong linguistic backgrounds have annotated the dependency relations from 1,000 posts of the µtopia dataset, using the FUDG (Schneider et al., 2013) and GFL annotation tool (Mordowanec et al., 2014). The annotators communicate regularly during the annotation process, and a coding manual that relies majorly on the Stanford Dependencies (Chang et al., 2009) is designed. The annotation process has two stages: in the first stage, we rely on the word segmentation produced by the segmenter, and produce a draft version of the treebank; in the second stage, the annotators actively discuss the difficult cases to reach agreements, manually correct the mis-segmented word tokens, and revise the annotations of the tricky cases. The final inter-annotator agreement rate on a randomly-selected subset of"
D14-1122,I05-3027,0,0.0331806,"head of the tree occurs more frequent on the left-to-middle of the sentence, while the distribution of the head is more complicated in Chinese. This is also verified from the parallel Weibo data. • Another well-known issue in Chinese is that Chinese is a pro-drop topical language. This is extremely prominent in the short text, which clearly creates a problem for parsing. For example, in the Chinese Weibo data, we have observed the sentence in Figure 1. To facilitate the annotation process, we first preprocess the Weibo posts using the Stanford NLP pipeline, including a Chinese Word Segmenter (Tseng et al., 2005) and a Chinese Partof-Speech tagger (Toutanova and Manning, 2000). Two native speakers of Chinese with strong linguistic backgrounds have annotated the dependency relations from 1,000 posts of the µtopia dataset, using the FUDG (Schneider et al., 2013) and GFL annotation tool (Mordowanec et al., 2014). The annotators communicate regularly during the annotation process, and a coding manual that relies majorly on the Stanford Dependencies (Chang et al., 2009) is designed. The annotation process has two stages: in the first stage, we rely on the word segmentation produced by the segmenter, and pr"
D14-1122,D07-1096,0,0.138857,"Missing"
D14-1122,C08-1132,0,0.015777,"that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese"
D14-1122,D07-1111,0,0.0513418,"fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and r"
D14-1122,W13-2307,0,0.0473676,"Missing"
D14-1122,D08-1059,0,0.015826,"g has attracted many interests in the last fifteen years. Bikel and Chiang (2000; 2002) are among the first to use Penn Chinese Tree Bank for dependency parsing, where they adapted Xia’s head rules (Xia, 1999). An important milestone for Chinese dependency parsing is that, a few years later, the CoNLL shared task launched a track for multilingual dependency parsing, which also included Chinese (Buchholz and Marsi, 2006; Nilsson et al., 2007). These shared tasks soon popularized Chinese dependency parsing by making datasets available, and there has been growing amount of literature since then (Zhang and Clark, 2008; Nivre et al., 2007; Sagae and Tsujii, 2007; Che et al., 2010; Carreras, 2007; Duan et al., 2007). Besides the CoNLL shared tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc pre"
D14-1122,P14-1125,0,0.0166563,"red tasks, there are also many interesting studies on Chinese dependency parsing. For example, researchers have studied case (Yu et al., 2008) and morphological (Li and Zhou, 2012) structures for learning a Chinese dependency parser. Another direction is to perform joint learning and inference for POS tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011; Li et al., 2011; Ma et al., 2012). In recent years, there has been growing interests in dependency arc prediction in Chinese (Che et al., 2014), and researchers have also investigated characterlevel Chinese dependency parsing (Zhang et al., 2014). However, even though the above methods all have merits, the results are reported only on standard newswire based Chinese Treebank (e.g. from People’s Daily (Liu et al., 2006)), and it is unclear how they would perform on Weibo data. To the best of our knowledge, together with the recent study on parsing tweets (Kong et al., 2014), we are among the first to study the problem of dependency parsing for social media text. 3 The Chinese Weibo Treebank We use the publicly available µtopia dataset (Ling et al., 2013) for dependency annotation. An interesting aspect of this Weibo dataset is that, be"
D14-1122,W00-1308,0,0.0168013,"dle of the sentence, while the distribution of the head is more complicated in Chinese. This is also verified from the parallel Weibo data. • Another well-known issue in Chinese is that Chinese is a pro-drop topical language. This is extremely prominent in the short text, which clearly creates a problem for parsing. For example, in the Chinese Weibo data, we have observed the sentence in Figure 1. To facilitate the annotation process, we first preprocess the Weibo posts using the Stanford NLP pipeline, including a Chinese Word Segmenter (Tseng et al., 2005) and a Chinese Partof-Speech tagger (Toutanova and Manning, 2000). Two native speakers of Chinese with strong linguistic backgrounds have annotated the dependency relations from 1,000 posts of the µtopia dataset, using the FUDG (Schneider et al., 2013) and GFL annotation tool (Mordowanec et al., 2014). The annotators communicate regularly during the annotation process, and a coding manual that relies majorly on the Stanford Dependencies (Chang et al., 2009) is designed. The annotation process has two stages: in the first stage, we rely on the word segmentation produced by the segmenter, and produce a draft version of the treebank; in the second stage, the a"
D14-1122,W00-1201,0,\N,Missing
D14-1122,W06-2920,0,\N,Missing
D14-1122,P11-2008,0,\N,Missing
D14-1122,D07-1101,0,\N,Missing
D15-1306,W11-0705,0,0.031656,"Missing"
D15-1306,P98-1013,0,0.0536978,"w∈W (1) For each word in a tweet, we query the external embeddings, and replace them with their knn words to create a new training instance. For example, consider the tweet “Being late is terrible” with the punctuality label, after searching for knn words for each token, we create a new training instance: “Be behind are bad” with the same label. Frame-Semantic Embeddings Although lexical (Mikolov et al., 2013a) and dependency based embeddings (Levy and Goldberg, 2014) have been studied, semantic-based embedding is still less understood. We consider the continuous embedding of semantic frames (Baker et al., 1998). To do this, we semantically parsed 3.8 million tweets using SEMAFOR (Das et al., 2010), and built a continuous bag-of-frame model to represent each semantic frame using Word2Vec3 . We then use the same data augmentation approach to create additional instances with these semantic frame embeddings. 3 https://code.google.com/p/word2vec/ Features Lexical +POS +Dependency* +Semantic Frames* Precision .341 .345 .349 .365 Recall .342 .346 .350 .367 F1 .341 .346 .350 .366 Table 4: Comparing linguistic features for categorizing annoying behaviors. The best results are highlighted in bold.* indicates"
D15-1306,N10-1138,0,0.0660896,"s in tweets, we have also extracted typed dependency triples (e.g., nsubj(hate,I)) using the MaltParser (Nivre et al., 2007). In this section, we describe our methods for the qualitative and quantitative analyses. In particular, we briefly review a supervised approach of using sparse mixed-effects topic model to visualize the topical words to analyze this behavior data. For the quantitative task of automatic categorization of tweets, we propose a novel approach to create additional training data, using continuous lexical and semantic representations. 4.1 2 • Frame-Semantics Features: SEMAFOR (Das et al., 2010) is a state-ofthe-art frame-semantics parser that produces FrameNet-style semantic annotation. We use SEMAFOR to extract frame-level semantic features. Supervised Topic Modeling To analyze the salient words for each category of annoying behaviors, we utilize SAGE (Eisenstein et al., 2011), a state-of-the-art mixed-effect topic model, which has been used in several NLP applications (Sim et al., 2012; Wang et al., 2012). SAGE is ideal for our text analytic purposes, because it is supervised, and it builds relatively clean topic models by considering the additive effects and the background distri"
D15-1306,P11-2008,0,0.074754,"Missing"
D15-1306,P11-2102,0,0.0214285,"Missing"
D15-1306,D14-1108,0,0.0915834,"Missing"
D15-1306,P14-2050,0,0.0151375,"or the k-nearest-neighbor (knn) word w for a query term using cosine sim~ and target word vectors ilarity between query Q ~ W: ~ W ~) arg max cosine(Q, w∈W (1) For each word in a tweet, we query the external embeddings, and replace them with their knn words to create a new training instance. For example, consider the tweet “Being late is terrible” with the punctuality label, after searching for knn words for each token, we create a new training instance: “Be behind are bad” with the same label. Frame-Semantic Embeddings Although lexical (Mikolov et al., 2013a) and dependency based embeddings (Levy and Goldberg, 2014) have been studied, semantic-based embedding is still less understood. We consider the continuous embedding of semantic frames (Baker et al., 1998). To do this, we semantically parsed 3.8 million tweets using SEMAFOR (Das et al., 2010), and built a continuous bag-of-frame model to represent each semantic frame using Word2Vec3 . We then use the same data augmentation approach to create additional instances with these semantic frame embeddings. 3 https://code.google.com/p/word2vec/ Features Lexical +POS +Dependency* +Semantic Frames* Precision .341 .345 .349 .365 Recall .342 .346 .350 .367 F1 .3"
D15-1306,D14-1214,0,0.0347901,"s. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz´alezIb´anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behaviors are not well studied in general. 3 The Dataset We use the Twitter corpus with 9 million sampled messages collected in prior work (Cheng et al., 2010), which includes a total of 121K users. The dataset includes latitude and longitude information. We extract 3,375 tweets1 with #petpeeve hashtags. We follow past work to annotate the tweets (Ritter et al., 2012; Li et al., 2014a): we apply the LDA clustering + human-identification approach to label the categories of the described annoying behaviors in these tweets. The human annotation process includes two stages: first, the annotators identify the 50 categories from the clustering process, and use these topics as a candi2558 1 http://www.cs.cmu.edu/˜yww/data/petpeeves.zip date label set to annotate the data; in the second stage, the categories are refined (to 60 classes) from the first pass, and the data is re-annotated with the refined human-specified category labels. Due to the complexity of this fine-grained ann"
D15-1306,P13-1018,0,0.0764148,"Missing"
D15-1306,N10-1020,0,0.0201899,"Missing"
D15-1306,D11-1141,0,0.0534446,"Missing"
D15-1306,W12-3203,0,0.0164561,"automatic categorization of tweets, we propose a novel approach to create additional training data, using continuous lexical and semantic representations. 4.1 2 • Frame-Semantics Features: SEMAFOR (Das et al., 2010) is a state-ofthe-art frame-semantics parser that produces FrameNet-style semantic annotation. We use SEMAFOR to extract frame-level semantic features. Supervised Topic Modeling To analyze the salient words for each category of annoying behaviors, we utilize SAGE (Eisenstein et al., 2011), a state-of-the-art mixed-effect topic model, which has been used in several NLP applications (Sim et al., 2012; Wang et al., 2012). SAGE is ideal for our text analytic purposes, because it is supervised, and it builds relatively clean topic models by considering the additive effects and the background distribution of words. Therefore, we can use SAGE to visualize the salient words for each category of annoying behaviors using the 3,375 #petpeeve tweets. Each tweet is treated as a document, and we use Markov Chain Monte Carlo for inference. To facilitate the geographical analysis, we use Google’s reverse geocoding service to extract the state information from coordinates, and apply SAGE for visualizati"
D15-1306,N03-1033,0,0.0559279,"Missing"
D15-1306,P10-1040,0,0.0415135,"we consider the feasibility of leveraging external resources, in particular, continuous word embeddings (Mikolov et al., 2013a) to enhance the multiclass text categorization model. Two major challenges for leveraging word embeddings for tweet classification are: 1) because word embeddings are continuous, it is difficult to fuse them with other discrete syntactic and semantic features; 2) it is not straightforward how one should transform the word-level representation to the tweet-level representation. In our preliminary experiments, we have evaluated the continuous word representation method (Turian et al., 2010), as well as incorporating neighboring words in the embeddings as additional features, but both methods fail to outperform the lexical baseline that uses only bag-of-word unigrams. To solve this problem, we propose the use of neighboring words in continuous representations to create new instances to augment the training 2559 weather rains STORM Blizzarad snowed SNOW smoking JAYECANE reggie smoking smoke smokers ungratefulness helped ungrateful clearly r them silence guilty R response conversation sending traffic cop lane pulled speed Slow showoff louis rims seein makin bag timewasting wastingm"
D15-1306,P12-1078,1,0.815117,"zation of tweets, we propose a novel approach to create additional training data, using continuous lexical and semantic representations. 4.1 2 • Frame-Semantics Features: SEMAFOR (Das et al., 2010) is a state-ofthe-art frame-semantics parser that produces FrameNet-style semantic annotation. We use SEMAFOR to extract frame-level semantic features. Supervised Topic Modeling To analyze the salient words for each category of annoying behaviors, we utilize SAGE (Eisenstein et al., 2011), a state-of-the-art mixed-effect topic model, which has been used in several NLP applications (Sim et al., 2012; Wang et al., 2012). SAGE is ideal for our text analytic purposes, because it is supervised, and it builds relatively clean topic models by considering the additive effects and the background distribution of words. Therefore, we can use SAGE to visualize the salient words for each category of annoying behaviors using the 3,375 #petpeeve tweets. Each tweet is treated as a document, and we use Markov Chain Monte Carlo for inference. To facilitate the geographical analysis, we use Google’s reverse geocoding service to extract the state information from coordinates, and apply SAGE for visualization. The categories t"
D15-1306,Q14-1034,0,0.0478993,"Missing"
D15-1306,C98-1013,0,\N,Missing
D17-1060,E17-1013,0,0.104797,"bvious answers, intelligent machines must be able to reason with existing resources, and learn to infer an unknown answer. More specifically, we situate our study in the context of multi-hop reasoning, which is the task of learning explicit inference formulas, given a large KG. For example, if the KG includes the 1 Code and the NELL dataset are available at https:// github.com/xwhan/DeepPath. 564 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 564–573 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2015; Das et al., 2017) still rely on first learning the PRA paths, which only operates in a discrete space. Comparing to PRA, our method reasons in a continuous space, and by incorporating various criteria in the reward function, our reinforcement learning (RL) framework has better control and more flexibility over the path-finding process. Neural symbolic machine (Liang et al., 2016) is a more recent work on KG reasoning, which also applies reinforcement learning but has a different flavor from our work. NSM learns to compose programs that can find answers to natural language questions, while our RL model tries to"
D17-1060,D14-1044,0,0.138073,"Missing"
D17-1060,D15-1038,0,0.022809,"alk approach for integrating the background KG and text—the method performs structure learning of logic programs and information extraction from text at the same time. A potential bottleneck for random walk inference is that supernodes connecting to large amount of formulas will create huge fan-out areas that significantly slow down the inference and affect the accuracy. Toutanova et al. (2015) provide a convolutional neural network solution to multi-hop reasoning. They build a CNN model based on lexicalized dependency paths, which suffers from the error propagation issue due to parse errors. Guu et al. (2015) uses KG embeddings to answer path queries. Zeng et al. (2014) described a CNN model for relational extraction, but it does not explicitly model the relational paths. Neelakantan et al. (2015) propose a recurrent neural networks model for modeling relational paths in knowledge base completion (KBC), but it trains too many separate models, and therefore it does not scale. Note that many of the recent KG reasoning methods (Neelakantan et al., 3 Methodology In this section, we describe in detail our RL-based framework for multi-hop relation reasoning. The specific task of relation reasoning is to"
D17-1060,P15-1016,0,0.533403,"leneck for random walk inference is that supernodes connecting to large amount of formulas will create huge fan-out areas that significantly slow down the inference and affect the accuracy. Toutanova et al. (2015) provide a convolutional neural network solution to multi-hop reasoning. They build a CNN model based on lexicalized dependency paths, which suffers from the error propagation issue due to parse errors. Guu et al. (2015) uses KG embeddings to answer path queries. Zeng et al. (2014) described a CNN model for relational extraction, but it does not explicitly model the relational paths. Neelakantan et al. (2015) propose a recurrent neural networks model for modeling relational paths in knowledge base completion (KBC), but it trains too many separate models, and therefore it does not scale. Note that many of the recent KG reasoning methods (Neelakantan et al., 3 Methodology In this section, we describe in detail our RL-based framework for multi-hop relation reasoning. The specific task of relation reasoning is to find reliable predictive paths between entity pairs. We formulate the path finding problem as a sequential decision making problem which can be solved with a RL agent. We first describe the e"
D17-1060,P15-1067,0,0.0622824,"or KG embeddings. we explore methods from both of these two classes in our experiments. For path based methods, we compare our RL model with the PRA (Lao et al., 2011a) algorithm, which has been used in a couple of reasoning methods (Gardner et al., 2013; Neelakantan et al., 2015). PRA is a data-driven algorithm using random walks (RW) to find paths and obtain path features. For embedding based methods, we evaluate several state-of-the-art embeddings designed for knowledge base completion, such as TransE (Bordes et al., 2013), TransH (Wang et al., 2014), TransR (Lin et al., 2015) and TransD (Ji et al., 2015) . The implementation of PRA is based on the Dataset and Settings Table 1 shows the statistics of the two datasets we conduct our experiments on. Both of them 569 FB15K-237 NELL-995 Tasks PRA RL TransE TransR Tasks PRA RL TransE TransR teamSports birthPlace personNationality filmDirector filmWrittenBy filmLanguage tvLanguage capitalOf organizationFounded musicianOrigin ... 0.987 0.441 0.846 0.349 0.601 0.663 0.960 0.829 0.281 0.426 0.955 0.531 0.823 0.441 0.457 0.670 0.969 0.783 0.309 0.514 0.896 0.403 0.641 0.386 0.563 0.642 0.804 0.554 0.390 0.361 0.784 0.417 0.720 0.399 0.605 0.641 0.906 0."
D17-1060,D15-1174,0,0.837272,"random walk with restart strategies for multi-hop reasoning. Gardner et al. (2013; 2014) propose a modification to PRA that computes feature similarity in the vector space. Wang and Cohen (2015) introduce a recursive random walk approach for integrating the background KG and text—the method performs structure learning of logic programs and information extraction from text at the same time. A potential bottleneck for random walk inference is that supernodes connecting to large amount of formulas will create huge fan-out areas that significantly slow down the inference and affect the accuracy. Toutanova et al. (2015) provide a convolutional neural network solution to multi-hop reasoning. They build a CNN model based on lexicalized dependency paths, which suffers from the error propagation issue due to parse errors. Guu et al. (2015) uses KG embeddings to answer path queries. Zeng et al. (2014) described a CNN model for relational extraction, but it does not explicitly model the relational paths. Neelakantan et al. (2015) propose a recurrent neural networks model for modeling relational paths in knowledge base completion (KBC), but it trains too many separate models, and therefore it does not scale. Note t"
D17-1060,D14-1181,0,0.00233687,"Missing"
D17-1060,P15-1035,1,0.798093,"p to large scale knowledge graphs, outperforming PRA and KG embedding methods in two tasks. In the next section, we outline related work in path-finding and embedding methods in KGs. We describe the proposed method in Section 3. We show experimental results in Section 4. Finally, we conclude in Section 5. 2 Related Work The Path-Ranking Algorithm (PRA) method (Lao et al., 2011b) is a primary path-finding approach that uses random walk with restart strategies for multi-hop reasoning. Gardner et al. (2013; 2014) propose a modification to PRA that computes feature similarity in the vector space. Wang and Cohen (2015) introduce a recursive random walk approach for integrating the background KG and text—the method performs structure learning of logic programs and information extraction from text at the same time. A potential bottleneck for random walk inference is that supernodes connecting to large amount of formulas will create huge fan-out areas that significantly slow down the inference and affect the accuracy. Toutanova et al. (2015) provide a convolutional neural network solution to multi-hop reasoning. They build a CNN model based on lexicalized dependency paths, which suffers from the error propagat"
D17-1060,D11-1049,0,0.707481,"; • Our learning method uses a complex reward function that considers accuracy, efficiency, and path diversity simultaneously, offering better control and more flexibility in the pathfinding process; • We show that our method can scale up to large scale knowledge graphs, outperforming PRA and KG embedding methods in two tasks. In the next section, we outline related work in path-finding and embedding methods in KGs. We describe the proposed method in Section 3. We show experimental results in Section 4. Finally, we conclude in Section 5. 2 Related Work The Path-Ranking Algorithm (PRA) method (Lao et al., 2011b) is a primary path-finding approach that uses random walk with restart strategies for multi-hop reasoning. Gardner et al. (2013; 2014) propose a modification to PRA that computes feature similarity in the vector space. Wang and Cohen (2015) introduce a recursive random walk approach for integrating the background KG and text—the method performs structure learning of logic programs and information extraction from text at the same time. A potential bottleneck for random walk inference is that supernodes connecting to large amount of formulas will create huge fan-out areas that significantly sl"
D17-1060,C14-1220,0,0.00862883,"ethod performs structure learning of logic programs and information extraction from text at the same time. A potential bottleneck for random walk inference is that supernodes connecting to large amount of formulas will create huge fan-out areas that significantly slow down the inference and affect the accuracy. Toutanova et al. (2015) provide a convolutional neural network solution to multi-hop reasoning. They build a CNN model based on lexicalized dependency paths, which suffers from the error propagation issue due to parse errors. Guu et al. (2015) uses KG embeddings to answer path queries. Zeng et al. (2014) described a CNN model for relational extraction, but it does not explicitly model the relational paths. Neelakantan et al. (2015) propose a recurrent neural networks model for modeling relational paths in knowledge base completion (KBC), but it trains too many separate models, and therefore it does not scale. Note that many of the recent KG reasoning methods (Neelakantan et al., 3 Methodology In this section, we describe in detail our RL-based framework for multi-hop relation reasoning. The specific task of relation reasoning is to find reliable predictive paths between entity pairs. We formu"
D17-1060,D13-1080,0,\N,Missing
D17-1191,P05-1053,0,0.0256069,"understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction. 1 Introduction Relation extraction is the task of predicting attributes and relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005). For example, given a sentence “Barack Obama was born in Honolulu, Hawaii.”, a relation classifier aims at predicting the relation of “bornInCity”. Relation extraction is the key component for building relation knowledge graphs, and it is of crucial significance to natural language processing applications such as structured search, sentiment analysis, question answering, and summarization. A major issue for relation extraction is the lack of labeled training data. In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al., William Yang Wang Department of Computer Science Univer"
D17-1191,P09-1113,0,0.210908,"ntities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005). For example, given a sentence “Barack Obama was born in Honolulu, Hawaii.”, a relation classifier aims at predicting the relation of “bornInCity”. Relation extraction is the key component for building relation knowledge graphs, and it is of crucial significance to natural language processing applications such as structured search, sentiment analysis, question answering, and summarization. A major issue for relation extraction is the lack of labeled training data. In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al., William Yang Wang Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106 USA william@cs.ucsb.edu 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction— it uses knowledge base facts to select a set of noisy instances from unlabeled data. Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance. Following their success, Zeng et al. (2015) proposed a piece-wise max-pooli"
D17-1191,C16-1238,0,0.0170655,"California, Santa Barbara Santa Barbara, CA 93106 USA william@cs.ucsb.edu 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction— it uses knowledge base facts to select a set of noisy instances from unlabeled data. Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance. Following their success, Zeng et al. (2015) proposed a piece-wise max-pooling strategy to improve the CNNs. Various attention strategies (Lin et al., 2016; Shen and Huang, 2016) for CNNs are also proposed, obtaining impressive results. However, most of these neural relation extraction models are relatively shallow CNNs—typically only one convolutional layer and one fully connected layer were involved, and it was not clear whether deeper models could have benefits on distilling signals from noisy inputs in this task. In this paper, we investigate the effects of training deeper CNNs for distantly-supervised relation extraction. More specifically, we designed a convolutional neural network based on residual learning (He et al., 2016)—we show how one can incorporate word"
D17-1191,D12-1042,0,0.134398,"classifier aims at predicting the relation of “bornInCity”. Relation extraction is the key component for building relation knowledge graphs, and it is of crucial significance to natural language processing applications such as structured search, sentiment analysis, question answering, and summarization. A major issue for relation extraction is the lack of labeled training data. In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al., William Yang Wang Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106 USA william@cs.ucsb.edu 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction— it uses knowledge base facts to select a set of noisy instances from unlabeled data. Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance. Following their success, Zeng et al. (2015) proposed a piece-wise max-pooling strategy to improve the CNNs. Various attention strategies (Lin et al., 2016; Shen and Huang, 2016) for CNNs are also proposed, obtaining impressive results. However, most of these"
D17-1191,D15-1203,0,0.311146,"ent years, distant supervision (Mintz et al., 2009; Hoffmann et al., William Yang Wang Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106 USA william@cs.ucsb.edu 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction— it uses knowledge base facts to select a set of noisy instances from unlabeled data. Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance. Following their success, Zeng et al. (2015) proposed a piece-wise max-pooling strategy to improve the CNNs. Various attention strategies (Lin et al., 2016; Shen and Huang, 2016) for CNNs are also proposed, obtaining impressive results. However, most of these neural relation extraction models are relatively shallow CNNs—typically only one convolutional layer and one fully connected layer were involved, and it was not clear whether deeper models could have benefits on distilling signals from noisy inputs in this task. In this paper, we investigate the effects of training deeper CNNs for distantly-supervised relation extraction. More spec"
D17-1191,C14-1220,0,0.861694,"ion. A major issue for relation extraction is the lack of labeled training data. In recent years, distant supervision (Mintz et al., 2009; Hoffmann et al., William Yang Wang Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106 USA william@cs.ucsb.edu 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction— it uses knowledge base facts to select a set of noisy instances from unlabeled data. Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance. Following their success, Zeng et al. (2015) proposed a piece-wise max-pooling strategy to improve the CNNs. Various attention strategies (Lin et al., 2016; Shen and Huang, 2016) for CNNs are also proposed, obtaining impressive results. However, most of these neural relation extraction models are relatively shallow CNNs—typically only one convolutional layer and one fully connected layer were involved, and it was not clear whether deeper models could have benefits on distilling signals from noisy inputs in this task. In this paper, we investigate the"
D17-1191,P11-1055,0,0.212228,"Missing"
D17-1191,P16-1200,0,0.556347,"ence University of California, Santa Barbara Santa Barbara, CA 93106 USA william@cs.ucsb.edu 2011; Surdeanu et al., 2012) emerges as the most popular method for relation extraction— it uses knowledge base facts to select a set of noisy instances from unlabeled data. Among all the machine learning approaches for distant supervision, the recently proposed Convolutional Neural Networks (CNNs) model (Zeng et al., 2014) achieved the state-of-the-art performance. Following their success, Zeng et al. (2015) proposed a piece-wise max-pooling strategy to improve the CNNs. Various attention strategies (Lin et al., 2016; Shen and Huang, 2016) for CNNs are also proposed, obtaining impressive results. However, most of these neural relation extraction models are relatively shallow CNNs—typically only one convolutional layer and one fully connected layer were involved, and it was not clear whether deeper models could have benefits on distilling signals from noisy inputs in this task. In this paper, we investigate the effects of training deeper CNNs for distantly-supervised relation extraction. More specifically, we designed a convolutional neural network based on residual learning (He et al., 2016)—we show how o"
D18-1038,P16-1168,0,0.0671343,"Missing"
D18-1038,P15-2130,0,0.0572375,"Missing"
D18-1038,P17-1163,0,0.0507404,"Missing"
D18-1038,W14-4340,0,0.0784981,"Missing"
D18-1038,Q17-1022,0,0.282844,"Missing"
D18-1038,kamholz-etal-2014-panlex,0,0.0230065,"for testing. We use the German and Italian as the target language to transfer our knowledge from English DST system. In the experiments, we do not have access to any training or validation dataset for German and Italian, and we only have access to their testing dataset which is composed of 400 dialogs. For external resource, we use the IWSLT2014 Ted Talk parallel corpus (Mauro et al., 2012) from the official website4 for bilingual corpus scenario. In the IWSLT2014 parallel corpus, we only keep the sentences between 4 and 40 words and decrease the sentence pairs to around 150K. We use Panlex (Kamholz et al., 2014) as our data source and crawl translations for all the words appearing in the dialog datasets to build our bilingual dictionary. We specifically investigate two kinds of pretrained embedding, and we use Glove (Pennington et al., 2014) as the monolingual embedding and MUSE (Conneau et al., 2017) as the bilingual embedding to see their impacts on the DST performance. We split the raw DST corpus into turn-level examples. During training, we use the ground truth previous state Vt−1 as inputs. At test time, we use the model searched states as the previous state to continue tracking intention until"
D18-1038,D17-1302,0,0.0540255,"s, showing that our methods can accurately track dialog states for 2.2 Cross-Lingual Transfer Learning Cross-lingual transfer learning has been a very popular topic during the years, which can be seen as a transductive process. In such process, the input domains of the source and target are different (Pan and Yang, 2010) since each language has its own distinct lexicon. By discovering the underlying connections between the source and target domain, we could design transfer algorithms for different tasks. Recently, algorithms have been successfully designed for POS tagging (Zhang et al., 2016; Kim et al., 2017), NER (Pan et al., 2017; Ni et al., 2017) as well as image captioning (Miyazaki and Shimizu, 2016). These methods first aim at discovering the relatedness between two languages and separate languagecommon modules from language-specific modules, then resort to external resources to transfer the knowledge across the language boundary. Our method addresses the transfer learning using a teacher-student framework and proposes to use the teacher to gradually guide the student to make more proper decisions. 1 https://github.com/wenhuchen/ Cross-Lingual-NBT 415 German test dataset English training dat"
D18-1038,P17-1135,0,0.09568,"Missing"
D18-1038,P17-4012,0,0.183119,"get language word by word using the bilingual dictionary, which is used to train the NBT in target side. We demonstrate the results for our proposed algorithms and other competing algorithms in Table 2, from which we can easily conclude that that (i) our Decoupled NBT does not affect the performance, and (ii) our cross-lingual NBT framework is able to achieve significantly better accuracy for both languages in both parallel-resource scenarios. Compare with Translator/WBW. With bilingual corpus, XL-NBT-C with pre-trained bilingual embedding can significantly outperform our Translator baseline (Klein et al., 2017). This is intuitive because the translation model requires 3 https://github.com/nmrksic/ neural-belief-tracker/tree/master/data 4 https://wit3.fbk.eu/mt.php?release= 2014-01 5 https://github.com/nmrksic/ neural-belief-tracker 420 Error Type Examples Modify Failure Machine: I have two options that fit that description, golden wok Chinese restaurant and the Nirala which serves Indian food, do you have a preference? User: How about Nirala, whats the address and phone of that? Previous State: food=Chinese; Prediction: food=none; Groundtruth: food=Indian Maintain Failure History Failure Machine: th"
D18-1038,I17-1074,0,0.0897675,"Missing"
D18-1038,P17-1178,0,0.0378339,"ods can accurately track dialog states for 2.2 Cross-Lingual Transfer Learning Cross-lingual transfer learning has been a very popular topic during the years, which can be seen as a transductive process. In such process, the input domains of the source and target are different (Pan and Yang, 2010) since each language has its own distinct lexicon. By discovering the underlying connections between the source and target domain, we could design transfer algorithms for different tasks. Recently, algorithms have been successfully designed for POS tagging (Zhang et al., 2016; Kim et al., 2017), NER (Pan et al., 2017; Ni et al., 2017) as well as image captioning (Miyazaki and Shimizu, 2016). These methods first aim at discovering the relatedness between two languages and separate languagecommon modules from language-specific modules, then resort to external resources to transfer the knowledge across the language boundary. Our method addresses the transfer learning using a teacher-student framework and proposes to use the teacher to gradually guide the student to make more proper decisions. 1 https://github.com/wenhuchen/ Cross-Lingual-NBT 415 German test dataset English training dataset User: I’m looking"
D18-1038,2012.eamt-1.60,0,0.0331517,"es. The train, valid and test datasets for three different languages (English, German, Italian) are available online3 . We use the English as source language where 600 dialogs are used for training, 200 for validation and 400 for testing. We use the German and Italian as the target language to transfer our knowledge from English DST system. In the experiments, we do not have access to any training or validation dataset for German and Italian, and we only have access to their testing dataset which is composed of 400 dialogs. For external resource, we use the IWSLT2014 Ted Talk parallel corpus (Mauro et al., 2012) from the official website4 for bilingual corpus scenario. In the IWSLT2014 parallel corpus, we only keep the sentences between 4 and 40 words and decrease the sentence pairs to around 150K. We use Panlex (Kamholz et al., 2014) as our data source and crawl translations for all the words appearing in the dialog datasets to build our bilingual dictionary. We specifically investigate two kinds of pretrained embedding, and we use Glove (Pennington et al., 2014) as the monolingual embedding and MUSE (Conneau et al., 2017) as the bilingual embedding to see their impacts on the DST performance. We sp"
D18-1038,D14-1162,0,0.0888557,"ly have access to their testing dataset which is composed of 400 dialogs. For external resource, we use the IWSLT2014 Ted Talk parallel corpus (Mauro et al., 2012) from the official website4 for bilingual corpus scenario. In the IWSLT2014 parallel corpus, we only keep the sentences between 4 and 40 words and decrease the sentence pairs to around 150K. We use Panlex (Kamholz et al., 2014) as our data source and crawl translations for all the words appearing in the dialog datasets to build our bilingual dictionary. We specifically investigate two kinds of pretrained embedding, and we use Glove (Pennington et al., 2014) as the monolingual embedding and MUSE (Conneau et al., 2017) as the bilingual embedding to see their impacts on the DST performance. We split the raw DST corpus into turn-level examples. During training, we use the ground truth previous state Vt−1 as inputs. At test time, we use the model searched states as the previous state to continue tracking intention until the end of the 6.2 Results Here we highlight the baselines we use to compare with our cross-lingual algorithm as follows: (1) Supervised: this baseline algorithm assumes the existence of annotated dialog belief tracking datasets, and"
D18-1038,P13-1046,0,0.0281024,"ing no target-side dialog data, our method relies on other easy-to-access parallel resources to understand the connection between languages. Depending on the popularity and availability of target language resources, we study two kinds of parallel data: bilingual corpus and bilingual dictionary, and we respectively design two transfer learning strategies. languages with zero annotated data. 2 2.1 Related Work Dialog State Tracking Broadly speaking, the dialog belief tracking algorithms can be divided into three families: 1) hand-crafted rules 2) generative models, and 3) maximum-entropy model (Metallinou et al., 2013). Later on, many deep learning based discriminative models have surged to replace the traditional strategies (Henderson et al., 2014a; Mrksic et al., 2017; Williams et al., 2016) and achieved state-of-the-art results on various datasets. Though the discriminative models are reported to achieve fairly high accuracy, their applications are heavily restricted by the domain, ontology, and language. Recently, a pointer network based algorithm (Xu and Hu, 2018) and another multi-domain algorithm (Rastogi et al., 2017) have been proposed to break the ontology and domain boundary. Besides, (Mrkˇsi´c e"
D18-1038,E17-1042,0,0.0553359,"Missing"
D18-1038,D15-1199,0,0.0815689,"Missing"
D18-1038,P17-1062,0,0.036233,"Missing"
D18-1038,P18-1134,0,0.0115166,"ef tracking algorithms can be divided into three families: 1) hand-crafted rules 2) generative models, and 3) maximum-entropy model (Metallinou et al., 2013). Later on, many deep learning based discriminative models have surged to replace the traditional strategies (Henderson et al., 2014a; Mrksic et al., 2017; Williams et al., 2016) and achieved state-of-the-art results on various datasets. Though the discriminative models are reported to achieve fairly high accuracy, their applications are heavily restricted by the domain, ontology, and language. Recently, a pointer network based algorithm (Xu and Hu, 2018) and another multi-domain algorithm (Rastogi et al., 2017) have been proposed to break the ontology and domain boundary. Besides, (Mrkˇsi´c et al., 2017) has proposed an algorithm to train a unified framework to deal with multiple languages with annotated datasets. In contrast, our paper focuses on breaking the language boundary and transfer DST knowledge from one language into other zeroannotation languages. We use the popular Wizard-of-Oz (RojasBarahona et al., 2017) dataset as our DST benchmark to evaluate the effectiveness of our crosslingual transfer learning. We specify English as the so"
D18-1038,N16-1156,0,0.0393479,"the proposed methods, showing that our methods can accurately track dialog states for 2.2 Cross-Lingual Transfer Learning Cross-lingual transfer learning has been a very popular topic during the years, which can be seen as a transductive process. In such process, the input domains of the source and target are different (Pan and Yang, 2010) since each language has its own distinct lexicon. By discovering the underlying connections between the source and target domain, we could design transfer algorithms for different tasks. Recently, algorithms have been successfully designed for POS tagging (Zhang et al., 2016; Kim et al., 2017), NER (Pan et al., 2017; Ni et al., 2017) as well as image captioning (Miyazaki and Shimizu, 2016). These methods first aim at discovering the relatedness between two languages and separate languagecommon modules from language-specific modules, then resort to external resources to transfer the knowledge across the language boundary. Our method addresses the transfer learning using a teacher-student framework and proposes to use the teacher to gradually guide the student to make more proper decisions. 1 https://github.com/wenhuchen/ Cross-Lingual-NBT 415 German test dataset E"
D18-1038,D16-1137,0,\N,Missing
D18-1223,N18-1165,1,0.856653,"redicate, object). This kind of structured knowledge is essential for many downstream applications such as Question Answering and Semantic Web. Despite KGs’ large scale, they are known to be highly incomplete (Min et al., 2013). To automatically complete KGs, extensive research efforts (Nickel et al., 2011; Bordes et al., 2013; Yang 1 Histogram of relation frequency on Wikidata 500 Code and datasets could be found at https://github.com/xwhan/ One-shot-Relational-Learning. et al., 2014; Trouillon et al., 2016; Lao and Cohen, 2010; Neelakantan et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) have been made to build relational learning models that could infer missing triples by learning from existing ones. These methods explore the statistical information of triples or path patterns to infer new facts of existing relations; and have achieved considerable performance on various public datasets. However, those datasets (e.g. FB15k, WN18) used by previous models mostly only cover common relations in KGs. For more practical scenarios, we believe the desired KG completion models should handle two key properties of KGs. First, as shown in Figure 1, a large portion of KG relations are ac"
D18-1223,D15-1082,0,0.0368976,"Missing"
D18-1223,N13-1095,0,0.0328599,"ation frequencies in Wikidata. There are a large portion of relations that only have a few triples. Introduction Large-scale knowledge graphs (Suchanek et al., 2007; Vrandeˇci´c and Kr¨otzsch, 2014; Bollacker et al., 2008; Auer et al., 2007; Carlson et al., 2010) represent every piece of information as binary relationships between entities, usually in the form of triples i.e. (subject, predicate, object). This kind of structured knowledge is essential for many downstream applications such as Question Answering and Semantic Web. Despite KGs’ large scale, they are known to be highly incomplete (Min et al., 2013). To automatically complete KGs, extensive research efforts (Nickel et al., 2011; Bordes et al., 2013; Yang 1 Histogram of relation frequency on Wikidata 500 Code and datasets could be found at https://github.com/xwhan/ One-shot-Relational-Learning. et al., 2014; Trouillon et al., 2016; Lao and Cohen, 2010; Neelakantan et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) have been made to build relational learning models that could infer missing triples by learning from existing ones. These methods explore the statistical information of triples or path patterns to infer new f"
D18-1223,P15-1016,0,0.0904123,"etween entities, usually in the form of triples i.e. (subject, predicate, object). This kind of structured knowledge is essential for many downstream applications such as Question Answering and Semantic Web. Despite KGs’ large scale, they are known to be highly incomplete (Min et al., 2013). To automatically complete KGs, extensive research efforts (Nickel et al., 2011; Bordes et al., 2013; Yang 1 Histogram of relation frequency on Wikidata 500 Code and datasets could be found at https://github.com/xwhan/ One-shot-Relational-Learning. et al., 2014; Trouillon et al., 2016; Lao and Cohen, 2010; Neelakantan et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) have been made to build relational learning models that could infer missing triples by learning from existing ones. These methods explore the statistical information of triples or path patterns to infer new facts of existing relations; and have achieved considerable performance on various public datasets. However, those datasets (e.g. FB15k, WN18) used by previous models mostly only cover common relations in KGs. For more practical scenarios, we believe the desired KG completion models should handle two key properties of KGs. First, as"
D18-1223,P18-1150,0,0.0606118,"Missing"
D18-1223,D17-1060,1,0.894664,"n the form of triples i.e. (subject, predicate, object). This kind of structured knowledge is essential for many downstream applications such as Question Answering and Semantic Web. Despite KGs’ large scale, they are known to be highly incomplete (Min et al., 2013). To automatically complete KGs, extensive research efforts (Nickel et al., 2011; Bordes et al., 2013; Yang 1 Histogram of relation frequency on Wikidata 500 Code and datasets could be found at https://github.com/xwhan/ One-shot-Relational-Learning. et al., 2014; Trouillon et al., 2016; Lao and Cohen, 2010; Neelakantan et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) have been made to build relational learning models that could infer missing triples by learning from existing ones. These methods explore the statistical information of triples or path patterns to infer new facts of existing relations; and have achieved considerable performance on various public datasets. However, those datasets (e.g. FB15k, WN18) used by previous models mostly only cover common relations in KGs. For more practical scenarios, we believe the desired KG completion models should handle two key properties of KGs. First, as shown in Figure 1,"
D18-1223,N18-1109,1,0.901118,"ot pay attention to those sparse symbols. More recently, several models (Shi and Weninger, 2017; Xie et al., 2016) have been proposed to handle unseen entities by leveraging text descriptions. In contrast to these approaches, our model deals with long-tail or newly added relations and focuses on one-shot relational learning without any external information, such as text descriptions of entities or relations. Few-Shot Learning Recent deep learning based few-shot learning approaches fall into two main categories: (1) metric based approaches (Koch, 2015; Vinyals et al., 2016; Snell et al., 2017; Yu et al., 2018), which try to learn generalizable metrics and the corresponding matching functions from a set of training tasks. Most methods in this class adopt the general matching framework proposed in deep siamese network (Koch, 2015). One example is the Matching Networks (Vinyals et al., 2016), which make predictions by comparing the input example with a small labeled support set; (2) meta-learner based approaches (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Finn et al., 2017; Li et al., 2017), which aim to learn the optimization of model parameters (by either outputting the parameter updates or"
D18-1223,D15-1174,0,0.345211,"entity t given the head entity and the query relation: (h, r, ?). As our purpose is to infer unseen facts for newly added or existing long-tail relations, we focus on the latter case. In contrast to previous work that usually assumes enough triples for the query relation are available for training, this work studies the case where only one training triple is available. To be more specific, the goal is to rank the true tail entity ttrue higher than other candidate entities t ∈ Ch,r , given only an example triple (h0 , r, t0 ). The candidates set is constructed using the entity type constraint (Toutanova et al., 2015). It is also worth noting that when we predict new facts of the relation r, we only consider a closed set of entities, i.e. no unseen entities during testing. For open-world settings where new entities might appear during testing, external information such as text descriptions about these entities are usually required and we leave this to future work. 3.2 One-Shot Learning Settings This section describes the settings for the training and evaluation of our one-shot learning model. The goal of our work is to learn a metric that could be used to predict new facts with oneshot examples. Following"
D18-1388,D10-1111,0,0.0299796,"31 - November 4, 2018. 2018 Association for Computational Linguistics by the degree to which they lean towards a particular ideology. 3. Finally, differing from most works, which typically focus on congressional speeches, we conduct ideology detection of news articles by assembling a large-scale diverse dataset spanning more than 50 sources. 2 Related Work Several works study the detection of political ideology through the lens of computational linguistics and natural language processing (Laver et al., 2003; Monroe and Maeda, 2004; Thomas et al., 2006; Lin et al., 2008; Carroll et al., 2009; Ahmed and Xing, 2010; Gentzkow and Shapiro, 2010; Gerrish and Blei, 2011; Sim et al., 2013). Gentzkow and Shapiro (2010) first attempt to rate the ideological leaning of news sources by proposing a measure called “slant index” which captures the degree to which a particular newspaper uses partisan terms or co-allocations. Gerrish and Blei (2011) predict the voting patterns of Congress members based on supervised topic models. Other works use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010; Lin et al., 2008). Sim et al. (2013) propose a novel HMM-based model to in"
D18-1388,P15-2072,0,0.0126934,"a and politicians today are so subtle that even word-choice may require one to adopt a particular ideological position (Iyyer et al., 2014). For example, conservatives tend to use the term tax reform, while liberals use tax simplification. Though objectivity and unbiased reporting remains a cornerstone of professional journalism, several scholars argue that the media displays ideological bias (Gentzkow and Shapiro, 2010; Groseclose and Milyo, 2005; Iyyer et al., 2014). Even if one were to argue that such bias may not be reflective of a lack of objectivity, prior research Dardis et al. (2008); Card et al. (2015) note that framing of topics can significantly influence policy. Since manual detection of political ideology is challenging at a large scale, there has been extensive work on developing computational models for automatically inferring the political ideology of articles, blogs, statements, and congressional speeches (Gentzkow and Shapiro, 2010; Iyyer et al., 2014; Preot¸iuc-Pietro et al., 2017; Sim et al., 2013). In this paper, we consider the detection of ideological bias at the news article level, in contrast to recent work by Iyyer et al. (2014) who focus on the sentence level or the work o"
D18-1388,P14-1105,0,0.71907,"m all of the above views to identify the ideology evinced by a news article. Our model draws on advances in representation learning in natural language processing and network science to capture cues from both textual content and the network structure of news articles. We empirically evaluate our model against a battery of baselines and show that our model outperforms state of the art by 10 percentage points F1 score. 1 Introduction Many issues covered or discussed by the media and politicians today are so subtle that even word-choice may require one to adopt a particular ideological position (Iyyer et al., 2014). For example, conservatives tend to use the term tax reform, while liberals use tax simplification. Though objectivity and unbiased reporting remains a cornerstone of professional journalism, several scholars argue that the media displays ideological bias (Gentzkow and Shapiro, 2010; Groseclose and Milyo, 2005; Iyyer et al., 2014). Even if one were to argue that such bias may not be reflective of a lack of objectivity, prior research Dardis et al. (2008); Card et al. (2015) note that framing of topics can significantly influence policy. Since manual detection of political ideology is challeng"
D18-1388,D14-1181,0,0.00910241,"Missing"
D18-1388,P17-1068,0,0.27253,"Missing"
D18-1388,D13-1010,0,0.619253,"o, 2010; Groseclose and Milyo, 2005; Iyyer et al., 2014). Even if one were to argue that such bias may not be reflective of a lack of objectivity, prior research Dardis et al. (2008); Card et al. (2015) note that framing of topics can significantly influence policy. Since manual detection of political ideology is challenging at a large scale, there has been extensive work on developing computational models for automatically inferring the political ideology of articles, blogs, statements, and congressional speeches (Gentzkow and Shapiro, 2010; Iyyer et al., 2014; Preot¸iuc-Pietro et al., 2017; Sim et al., 2013). In this paper, we consider the detection of ideological bias at the news article level, in contrast to recent work by Iyyer et al. (2014) who focus on the sentence level or the work of (Preot¸iuc-Pietro et al., 2017) who focus on inferring ideological bias of social media users. Prior research exists on detecting ideological biases of news articles or documents (Gentzkow and Shapiro, 2010; Gerrish and Blei, 2011; Iyyer et al., 2014). However, all of these works generally only model the text of the news article. However, in the online world, news articles do not just contain text but have a r"
D18-1388,W06-1639,0,0.13179,"uage Processing, pages 3518–3527 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics by the degree to which they lean towards a particular ideology. 3. Finally, differing from most works, which typically focus on congressional speeches, we conduct ideology detection of news articles by assembling a large-scale diverse dataset spanning more than 50 sources. 2 Related Work Several works study the detection of political ideology through the lens of computational linguistics and natural language processing (Laver et al., 2003; Monroe and Maeda, 2004; Thomas et al., 2006; Lin et al., 2008; Carroll et al., 2009; Ahmed and Xing, 2010; Gentzkow and Shapiro, 2010; Gerrish and Blei, 2011; Sim et al., 2013). Gentzkow and Shapiro (2010) first attempt to rate the ideological leaning of news sources by proposing a measure called “slant index” which captures the degree to which a particular newspaper uses partisan terms or co-allocations. Gerrish and Blei (2011) predict the voting patterns of Congress members based on supervised topic models. Other works use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010; Lin et al.,"
D18-1388,N16-1174,0,0.362776,"log likelihood function, we can now optimize it using stochastic gradient ascent. Having learned the embedding matrix F for each source node, we now model the link structure of any given article A simply by the average of the network embedding representations for each link l referenced in A. In particular, we compute znetwork 1 P as: znetwork = |A| F (l). l∈A 4.2.3 Modeling the Content of articles To model the content of an article, we use a hierarchical approach with attention. In particular, we compute attention at both levels: (a) words and (b) sentences. We closely follow the approach by (Yang et al., 2016) which learns a latent representation of a document d using both word and sentence attention models. We model the article A hierarchically, by first representing each sentence i with a hidden representation si . We model the fact that not all words contribute equally in the sentence through a word level attention mechanism. We then learn the representation of the article A by composing these individual sentence level representations with a sentence level attention mechanism. Learning sentence representations We first map each word to its embedding matrix through a lookup embedding matrix W . W"
D18-1391,I17-1078,0,0.0685395,"elines. In the next section, we outline the related work on hate speech detection, fine-grained text classification, and Variational Autoencoder. In Section 3, we explore the CVAE as a discriminative model, and our proposed method is described in Section 4. Experimental results are presented and discussed in Section 5. Finally, we conclude in Section 6. 2 2.1 Related Work Hate Speech Detection An extensive body of work has been dedicated to automatic hate speech detection. Most of the work focuses on binary classification. Warner and Hirschberg (2012) differentiate between antisemitic or not. Gao et al. (2017), Zhong et al. (2016), and Nobata et al. (2016) differentiate between abusive or not. Waseem and Hovy (2016), Burnap and Williams (2016), and Davidson et al. (2017) focus on three-way classification. Waseem and Hovy (2016) classify each input tweet as racist hate speech, sexist hate speech, or neither. Burnap and Williams (2016) build classifiers for hate speech based on race, sexual orientation or disability, while Davidson et al. (2017) train a model to differentiate among three classes: containing hate speech, only offensive language, or neither. Badjatiya et al. (2017) use the dataset prov"
D18-1391,N15-1011,0,0.0246695,"n. Our work is most closely related to (Van Hee et al., 2015), which focuses on fine-grained cyberbullying classification. However, this study only focuses on seven categories of cyberbullying while our dataset consists of 40 classes. Therefore, our classification task is much more fine-grained and challenging. 2.2 Fine-grained Text Classification Our work is also related to text classification. Convolutional Neural Networks (CNN) have been successfully applied to the text classification task. Kim (2014) applies CNN at the word level while Zhang et al. (2015) apply CNN at the character level. Johnson and Zhang (2015) exploit the word order of text data with CNN for accurate text categorization. Socher et al. (2013) introduces the Recursive Neural Tensor Network for text classification. Recurrent Neural Networks (RNN) are also commonly used for text classification (Tai et al., 2015; Yogatama et al., 2017). Lai et al. (2015) and Zhou et al. (2015) further combine RNN with CNN. Tang et al. (2015) and Yang et al. (2016) exploit the hierarchical structure for document classification. Tang et al. (2015) generate from the sentence representation to the document representation while Yang et al. (2016) generate fr"
D18-1391,D14-1181,0,0.00408172,"djatiya et al. (2017) use the dataset provided by Waseem and Hovy (2016) to do threeway classification. Our work is most closely related to (Van Hee et al., 2015), which focuses on fine-grained cyberbullying classification. However, this study only focuses on seven categories of cyberbullying while our dataset consists of 40 classes. Therefore, our classification task is much more fine-grained and challenging. 2.2 Fine-grained Text Classification Our work is also related to text classification. Convolutional Neural Networks (CNN) have been successfully applied to the text classification task. Kim (2014) applies CNN at the word level while Zhang et al. (2015) apply CNN at the character level. Johnson and Zhang (2015) exploit the word order of text data with CNN for accurate text categorization. Socher et al. (2013) introduces the Recursive Neural Tensor Network for text classification. Recurrent Neural Networks (RNN) are also commonly used for text classification (Tai et al., 2015; Yogatama et al., 2017). Lai et al. (2015) and Zhou et al. (2015) further combine RNN with CNN. Tang et al. (2015) and Yang et al. (2016) exploit the hierarchical structure for document classification. Tang et al. ("
D18-1391,D16-1076,0,0.0285438,"Missing"
D18-1391,D13-1170,0,0.00268374,"ng classification. However, this study only focuses on seven categories of cyberbullying while our dataset consists of 40 classes. Therefore, our classification task is much more fine-grained and challenging. 2.2 Fine-grained Text Classification Our work is also related to text classification. Convolutional Neural Networks (CNN) have been successfully applied to the text classification task. Kim (2014) applies CNN at the word level while Zhang et al. (2015) apply CNN at the character level. Johnson and Zhang (2015) exploit the word order of text data with CNN for accurate text categorization. Socher et al. (2013) introduces the Recursive Neural Tensor Network for text classification. Recurrent Neural Networks (RNN) are also commonly used for text classification (Tai et al., 2015; Yogatama et al., 2017). Lai et al. (2015) and Zhou et al. (2015) further combine RNN with CNN. Tang et al. (2015) and Yang et al. (2016) exploit the hierarchical structure for document classification. Tang et al. (2015) generate from the sentence representation to the document representation while Yang et al. (2016) generate from the word-level attention to the sentence-level attention. However, the division of the hierarchie"
D18-1391,P15-1150,0,0.0307265,"more fine-grained and challenging. 2.2 Fine-grained Text Classification Our work is also related to text classification. Convolutional Neural Networks (CNN) have been successfully applied to the text classification task. Kim (2014) applies CNN at the word level while Zhang et al. (2015) apply CNN at the character level. Johnson and Zhang (2015) exploit the word order of text data with CNN for accurate text categorization. Socher et al. (2013) introduces the Recursive Neural Tensor Network for text classification. Recurrent Neural Networks (RNN) are also commonly used for text classification (Tai et al., 2015; Yogatama et al., 2017). Lai et al. (2015) and Zhou et al. (2015) further combine RNN with CNN. Tang et al. (2015) and Yang et al. (2016) exploit the hierarchical structure for document classification. Tang et al. (2015) generate from the sentence representation to the document representation while Yang et al. (2016) generate from the word-level attention to the sentence-level attention. However, the division of the hierarchies in our HCVAE is according to semantic levels, rather than according to document compositions. We generate from the category-level representations to the group-level re"
D18-1391,D15-1167,0,0.0151401,"ication. Convolutional Neural Networks (CNN) have been successfully applied to the text classification task. Kim (2014) applies CNN at the word level while Zhang et al. (2015) apply CNN at the character level. Johnson and Zhang (2015) exploit the word order of text data with CNN for accurate text categorization. Socher et al. (2013) introduces the Recursive Neural Tensor Network for text classification. Recurrent Neural Networks (RNN) are also commonly used for text classification (Tai et al., 2015; Yogatama et al., 2017). Lai et al. (2015) and Zhou et al. (2015) further combine RNN with CNN. Tang et al. (2015) and Yang et al. (2016) exploit the hierarchical structure for document classification. Tang et al. (2015) generate from the sentence representation to the document representation while Yang et al. (2016) generate from the word-level attention to the sentence-level attention. However, the division of the hierarchies in our HCVAE is according to semantic levels, rather than according to document compositions. We generate from the category-level representations to the group-level representations. Moreover, the most commonly used datasets by these works (Yelp reviews, Yahoo answers, AGNews, IMDB"
D18-1391,R15-1086,0,0.273339,"Missing"
D18-1391,W12-2103,0,0.370823,"Missing"
D18-1391,N16-2013,0,0.0615803,"assification, and Variational Autoencoder. In Section 3, we explore the CVAE as a discriminative model, and our proposed method is described in Section 4. Experimental results are presented and discussed in Section 5. Finally, we conclude in Section 6. 2 2.1 Related Work Hate Speech Detection An extensive body of work has been dedicated to automatic hate speech detection. Most of the work focuses on binary classification. Warner and Hirschberg (2012) differentiate between antisemitic or not. Gao et al. (2017), Zhong et al. (2016), and Nobata et al. (2016) differentiate between abusive or not. Waseem and Hovy (2016), Burnap and Williams (2016), and Davidson et al. (2017) focus on three-way classification. Waseem and Hovy (2016) classify each input tweet as racist hate speech, sexist hate speech, or neither. Burnap and Williams (2016) build classifiers for hate speech based on race, sexual orientation or disability, while Davidson et al. (2017) train a model to differentiate among three classes: containing hate speech, only offensive language, or neither. Badjatiya et al. (2017) use the dataset provided by Waseem and Hovy (2016) to do threeway classification. Our work is most closely related to (Van Hee e"
D18-1391,N16-1174,0,0.0222816,"Neural Networks (CNN) have been successfully applied to the text classification task. Kim (2014) applies CNN at the word level while Zhang et al. (2015) apply CNN at the character level. Johnson and Zhang (2015) exploit the word order of text data with CNN for accurate text categorization. Socher et al. (2013) introduces the Recursive Neural Tensor Network for text classification. Recurrent Neural Networks (RNN) are also commonly used for text classification (Tai et al., 2015; Yogatama et al., 2017). Lai et al. (2015) and Zhou et al. (2015) further combine RNN with CNN. Tang et al. (2015) and Yang et al. (2016) exploit the hierarchical structure for document classification. Tang et al. (2015) generate from the sentence representation to the document representation while Yang et al. (2016) generate from the word-level attention to the sentence-level attention. However, the division of the hierarchies in our HCVAE is according to semantic levels, rather than according to document compositions. We generate from the category-level representations to the group-level representations. Moreover, the most commonly used datasets by these works (Yelp reviews, Yahoo answers, AGNews, IMDB reviews (Diao et al., 2"
D18-1391,D16-1050,0,0.0187747,"s the ground truth hate group label. yˆg is the output prediction of the hate group. p is the posterior distribution of the latent variable z while p0 is the prior distribution of z. Note that this structure is used for training. During testing, the posterior network is replaced by the prior network to compute yˆg and thus yg is not available during testing. Refer to Section 3 for detailed explanation. many complicated generation tasks, such as handwritten digits (Kingma and Welling, 2013; Salimans et al., 2015), faces (Kingma and Welling, 2013; Rezende et al., 2014), and machine translation (Zhang et al., 2016). CVAE (Larsen et al., 2016; Sohn et al., 2015) is an extension of the original VAE framework that incorporates conditions during generation. In addition to image generation, CVAE has also been successfully applied to several NLP tasks, such as dialog generation (Zhao et al., 2017). Although so far CVAE has always been used as a generative model, we explore the performance of the CVAE as a discriminative model and further propose a hierarchical CVAE model, which exploits the hate group category (ideology) information for training. 3 CVAE Baseline We formulate our classification task as the fol"
D18-1391,P17-1061,0,\N,Missing
D18-1391,P16-5005,0,\N,Missing
D19-1442,J93-2004,0,0.0640985,"ons of any forms since it is now not restricted to be factorized, and therefore it is more likely for q in Figure 3 to be closer to the true posterior p. In this case, the Eq(z|x) [p(x|z)] can be higher since now we have latent codes sampled from a more accurate posterior, and then this expectation will be able to leverage the decrease of KL even in the final training stage. 5 Experimental Results 5.1 Datasets Data Yelp13 PTB Yahoo Train 62522 42068 100K Valid 7773 3370 10K Test 8671 3761 10K Vocab 15K 10K 20K Table 2: Size and vocabulary size for each dataset. In the paper, we use Penn Tree (Marcus et al., 1993), Yahoo Answers (Xu and Durrett, 2018; Yang et al., 2017), and Yelp 13 reviews (Xu et al., 2016) to test our model performance over variational language modeling tasks. We use these three large datasets as they are widely used in all other variational language modeling approaches (Bowman et al., 2015; Yang et al., 2017; Xu and Durrett, 2018; Xiao et al., 2018; He et al., 2018; Kim et al., 2018). Table 2 shows the statistics, vocabulary size, and number of samples in Train/Validation/Test for each dataset. Model LSTM-LM (Yang et al., 2017) VAE (Bowman et al., 2015) vmf-VAE (Xu and Durrett, 2018"
D19-1442,N19-1025,1,0.742399,"Missing"
D19-1442,P14-1109,1,0.926081,"ns should be factorized or dimensional-wise independent for tractability. We argue that it leads to the posterior collapse problem since any variational posterior learned in this way does not maintain the correlation among latent codes and will never match the true posterior which is unlikely factorized. We avert this problem by proposing a Neural Gaussian Copula (Copula-VAE) model to train VAE on text data. Copula (Nelsen, 2007) can model dependencies of high-dimensional random variables and is very successful in risk management (Kole et al., 2007; McNeil et al., 2005), financial management (Wang and Hua, 2014), and other tasks that require the modeling of dependencies. We provide a reparameterization trick (Kingma and Welling, 2014) to incorporate it with VAE for language modeling. We argue that by maintaining the dependency relationships over latent codes, we can dramatically improve the performance of variational language modeling and avoid posterior collapse. Our major contributions can be summarized as the following: • We propose Neural parameterized Gaussian Copula to get a better estimation of the posterior for latent codes. • We provide a reparameterization technique for Gaussian Copula VAE."
D19-1442,N15-1039,1,0.860561,"drawbacks of VAE and explain how introducing a copula model could help avert the posterior collapse problem. 2 Related Work Copula: Before the rise of Deep Learning Copula (Nelsen, 2007) is a multivariate distribution whose marginals are all uniformly distributed. Over the years, it is widely used to extract correlation within high-dimensional random variables, and achieves great success in many subjects such as risk management (Kole et al., 2007; McNeil et al., 2005), finance (Wang and Hua, 2014), civil engineering(Chen et al., 2012; Zhang and Singh, 2006), and visual description generation (Wang and Wen, 2015). In the past, copula is often estimated by Maximum Likelihood method (Choro´s et al., 2010; Jaworski et al., 2010) via parametric or semi-parametric approaches (Tsukahara, 2005; Choro´s et al., 2010). One major difficulty when estimating the copula and extracting dependencies is the dimensionality of random variables. To overcome the curse of dimensionality, a graphical model called vine copula (Joe and Kurowicka, 2011; Czado, 2010; Bedford et al., 2002) is proposed to estimate a high-dimensional copula density by breaking it into a set of bivariate conditional copula densities. However, this"
D19-1442,D16-1172,0,0.0309739,"r q in Figure 3 to be closer to the true posterior p. In this case, the Eq(z|x) [p(x|z)] can be higher since now we have latent codes sampled from a more accurate posterior, and then this expectation will be able to leverage the decrease of KL even in the final training stage. 5 Experimental Results 5.1 Datasets Data Yelp13 PTB Yahoo Train 62522 42068 100K Valid 7773 3370 10K Test 8671 3761 10K Vocab 15K 10K 20K Table 2: Size and vocabulary size for each dataset. In the paper, we use Penn Tree (Marcus et al., 1993), Yahoo Answers (Xu and Durrett, 2018; Yang et al., 2017), and Yelp 13 reviews (Xu et al., 2016) to test our model performance over variational language modeling tasks. We use these three large datasets as they are widely used in all other variational language modeling approaches (Bowman et al., 2015; Yang et al., 2017; Xu and Durrett, 2018; Xiao et al., 2018; He et al., 2018; Kim et al., 2018). Table 2 shows the statistics, vocabulary size, and number of samples in Train/Validation/Test for each dataset. Model LSTM-LM (Yang et al., 2017) VAE (Bowman et al., 2015) vmf-VAE (Xu and Durrett, 2018) VAE-NF non-diag-VAE copula-VAE(cho) λ = 0.4 NLL 116.2 105.2 96.0 96.8 105.1 92.2 KL 1.74 5.7 0"
D19-1442,D18-1480,0,0.158127,"ed at https://github.com/ kingofspace0wzz/copula-vae-lm et al., 2013) is proposed based on the theory of VI and achieves great success over a huge number of tasks, such as transfer learning (Shen et al., 2017), unsupervised learning (Jang et al., 2017), image generation (Gregor et al., 2015), semi-supervised classification (Jang et al., 2017), and dialogue generation (Zhao et al., 2017). VAE is able to learn a continuous space of latent random variables which are useful for a lot of classification and generation tasks. Recent studies (Bowman et al., 2015; Yang et al., 2017; Xiao et al., 2018; Xu and Durrett, 2018) show that when it comes to text generation and language modeling, VAE does not perform well and often generates random texts without making good use of the learned latent codes. This phenomenon is called Posterior Collapse, where the Kullback-Leibler (KL) divergence between the posterior and the prior (often assumed to be a standard Gaussian) vanishes. It makes the latent codes completely useless because any text input will be mapped to a standard Gaussian variable. Many recent studies (Yang et al., 2017; Xu and Durrett, 2018; Xiao et al., 2018; Miao et al., 4333 Proceedings of the 2019 Confe"
D19-1442,P17-1061,0,0.0307389,"tion to a target distribution using calculus of variation. After the rise of deep learning (Krizhevsky et al., 2012), a deep generative model called Variational Autoencoder (Kingma and Welling, 2014; Hoffman 1 Code will be released at https://github.com/ kingofspace0wzz/copula-vae-lm et al., 2013) is proposed based on the theory of VI and achieves great success over a huge number of tasks, such as transfer learning (Shen et al., 2017), unsupervised learning (Jang et al., 2017), image generation (Gregor et al., 2015), semi-supervised classification (Jang et al., 2017), and dialogue generation (Zhao et al., 2017). VAE is able to learn a continuous space of latent random variables which are useful for a lot of classification and generation tasks. Recent studies (Bowman et al., 2015; Yang et al., 2017; Xiao et al., 2018; Xu and Durrett, 2018) show that when it comes to text generation and language modeling, VAE does not perform well and often generates random texts without making good use of the learned latent codes. This phenomenon is called Posterior Collapse, where the Kullback-Leibler (KL) divergence between the posterior and the prior (often assumed to be a standard Gaussian) vanishes. It makes the"
D19-1444,E17-1075,0,0.130974,"l dependency structure. (2) Even if labels do not demonstrate explicit correlation, they can still have implicit semantic dependencies. Introduction Multi-label classification aims at learning to make predictions on instances that are associated with multiple labels simultaneously, whereas in a classic multi-class classification setting, typically one instance has only one label. Multi-label classification is a common learning paradigm in a large amount of real-world natural language processing (NLP) applications, such as fine-grained entity typing (Ling and Weld, 2012; Shimaoka et al., 2017; Abhishek et al., 2017; Xin et al., 2018) and text classification (Nam et al., 2014; Liu et al., 2017; Chen et al., 2017; Wu et al., 2018). Significant amounts of research studies have been dedicated to tackle the multi-label classification problem (Zhang and Zhou, 2014), from traditional statistical models (Zhang and Zhou, 2007; Zhou et al., 2012; Surdeanu et al., 2012) to neural network-based models (Nam et al., 2014; Shimaoka et al., 2017). These models have variable structures, but usually, share the standard crossentropy loss function for training. After training, these models tend to use a single prediction p"
D19-1444,W17-2614,0,0.0887425,"e implicit semantic dependencies. Introduction Multi-label classification aims at learning to make predictions on instances that are associated with multiple labels simultaneously, whereas in a classic multi-class classification setting, typically one instance has only one label. Multi-label classification is a common learning paradigm in a large amount of real-world natural language processing (NLP) applications, such as fine-grained entity typing (Ling and Weld, 2012; Shimaoka et al., 2017; Abhishek et al., 2017; Xin et al., 2018) and text classification (Nam et al., 2014; Liu et al., 2017; Chen et al., 2017; Wu et al., 2018). Significant amounts of research studies have been dedicated to tackle the multi-label classification problem (Zhang and Zhou, 2014), from traditional statistical models (Zhang and Zhou, 2007; Zhou et al., 2012; Surdeanu et al., 2012) to neural network-based models (Nam et al., 2014; Shimaoka et al., 2017). These models have variable structures, but usually, share the standard crossentropy loss function for training. After training, these models tend to use a single prediction policy for all the labels to generate the final predictions. Actually, the above process is based o"
D19-1444,D18-1398,0,0.0677667,"and Jonsson, 2000; Thrun and Pratt, 2012). There are two types of meta-learning: • learning a meta-policy to update model parameters (Andrychowicz et al., 2016; Mishra et al., 2018). • Our learning method can learn a weight and a decision policy for each label, which can then be incorporated into training and prediction. • We show that our method is model-agnostic and can apply to different models in multilabel classification and outperform baselines. • learning a good parameter initialization for fast adaptation (Duan et al., 2016; Vinyals et al., 2016; Finn et al., 2017; Snell et al., 2017; Gu et al., 2018). In Section 2, we outline related work in multilabel classification and meta-learning. We then describe our proposed method in Section 3. We show In this paper, we propose to extend meta-learning algorithm for multi-label classification based on the first category. Instead of only training the 4355 GRU St-1 St St+1 St+2 pt-1 pt pt+1 wt-1 wt wt+1 rt-1 rt C Meta-Learner rt+1 C C Classiﬁer batch Bt-1 batch Bt batch Bt+1 Figure 2: The meta-learning framework for multi-label classification. model with meta-policy, we also consider prediction with meta-policy. 3 Method In this section, we describe"
D19-1444,D14-1181,0,0.00270615,"sted in Table 4. Reuters-21578: The instances are collected Reuters news articles during the period 1987 to 1991. We use the same training/test split as previous work (Yang, 2001; Nam et al., 2014). RCV1-V2: RCV1-V2 collects newswire stories from Reuters (Lewis et al., 2004). The training and test dataset originally consist of 23, 149 train and 781, 265 test instances, but we switch them to better training and evaluation (Nam et al., 2014). Setup Many researches have proved convolutional neural networks (CNN) are effective in extracting information for text classification (LeCun et al., 1998; Kim, 2014; Zhang et al., 2015). Following the (Kim, 2014), we set a CNN model as the classifier C to evaluate our method. Concretely, we use CNN-non-static mentioned in Kim (2014), which means we initialize the word embeddings with pre-trained Word2Vec and update the word embeddings during the training. The standard cross-entropy loss function is implemented, and the thresholds of all the classes are set as 0.5. We still use strict accuracy, loose macro, and loose micro scores to evaluate the model perResults The results of text classification are shown in Table 5. From the results, we can observe that"
D19-1444,D15-1099,0,0.0170244,"n Policy: The rankbased policy method RCut and proportionbased assignments PCut are jointly considered to set prediction policies for different labels after obtaining the trained classifier (Yang, 2001). • ODR Prediction Policy: After training, an optimal decision rule is to implement to choose prediction policies based on maximizing micro F1 scores (Lipton et al., 2014). • Predictions-as-Features: The model trains a classifier for each label, organize the classifiers in a partially ordered structure, and take predictions produced by the former classifiers as the latter classifiers’ features (Li et al., 2015). • Subset Maximization: The model views the multi-label prediction problem as classifier chains, and then replace classifier chains with recurrent neural networks (Nam et al., 2017). Datasets #Types Max Hierarchy Depth #Training #Testing FIGER OntoNotes BBN 128 2 2,690,286 563 89 3 220,398 9,603 47 2 86,078 13,187 /location, /location/region Output Context & Mention Representation Attention Table 1: The statistics of entity typing datasets. Fine-grained Entity Typing Word Embeddings Datasets We evaluate our method on three widely-used fine-grained entity typing datasets, FIGER, OntoNotes, and"
D19-1444,D16-1144,0,0.0898671,"ies, although one object may have multiple labels simultaneously in the multi-label classification setting, the level of difficulty of prediction for different labels can vary a lot. Firstly, for those labels that are aligned with knowledge graphs, they usually indicate a hierarchical dependency struc4354 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4354–4364, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ture (Ling and Weld, 2012; Ren et al., 2016; Ruder et al., 2016). It is intuitive that one trained classifier is easier to distinguish high-level parent labels, such as /organization and /person, but harder to distinguish low-level child labels, such as /news and /broadcast. Secondly, for those cases where labels do not demonstrate explicit correlation, the labels still contain implicit semantic dependencies, which is extremely common in the NLP field. For instance, the label /urban and /economics have obvious semantic correlation even if they are not organized in a hierarchical structure. Meanwhile, the labels with more implicit depen"
D19-1444,D16-1103,0,0.0184517,"object may have multiple labels simultaneously in the multi-label classification setting, the level of difficulty of prediction for different labels can vary a lot. Firstly, for those labels that are aligned with knowledge graphs, they usually indicate a hierarchical dependency struc4354 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4354–4364, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ture (Ling and Weld, 2012; Ren et al., 2016; Ruder et al., 2016). It is intuitive that one trained classifier is easier to distinguish high-level parent labels, such as /organization and /person, but harder to distinguish low-level child labels, such as /news and /broadcast. Secondly, for those cases where labels do not demonstrate explicit correlation, the labels still contain implicit semantic dependencies, which is extremely common in the NLP field. For instance, the label /urban and /economics have obvious semantic correlation even if they are not organized in a hierarchical structure. Meanwhile, the labels with more implicit dependencies are easier to"
D19-1444,D12-1042,0,0.0515641,"one instance has only one label. Multi-label classification is a common learning paradigm in a large amount of real-world natural language processing (NLP) applications, such as fine-grained entity typing (Ling and Weld, 2012; Shimaoka et al., 2017; Abhishek et al., 2017; Xin et al., 2018) and text classification (Nam et al., 2014; Liu et al., 2017; Chen et al., 2017; Wu et al., 2018). Significant amounts of research studies have been dedicated to tackle the multi-label classification problem (Zhang and Zhou, 2014), from traditional statistical models (Zhang and Zhou, 2007; Zhou et al., 2012; Surdeanu et al., 2012) to neural network-based models (Nam et al., 2014; Shimaoka et al., 2017). These models have variable structures, but usually, share the standard crossentropy loss function for training. After training, these models tend to use a single prediction policy for all the labels to generate the final predictions. Actually, the above process is based on the assumption that there is no dependency among the labels. However, as shown in Figure 1, this assumption is hard to be satisfied in real-world datasets, and the complex label dependencies receive little attention in multi-label classification (Demb"
D19-1444,N18-1113,1,0.833493,"dependencies. Introduction Multi-label classification aims at learning to make predictions on instances that are associated with multiple labels simultaneously, whereas in a classic multi-class classification setting, typically one instance has only one label. Multi-label classification is a common learning paradigm in a large amount of real-world natural language processing (NLP) applications, such as fine-grained entity typing (Ling and Weld, 2012; Shimaoka et al., 2017; Abhishek et al., 2017; Xin et al., 2018) and text classification (Nam et al., 2014; Liu et al., 2017; Chen et al., 2017; Wu et al., 2018). Significant amounts of research studies have been dedicated to tackle the multi-label classification problem (Zhang and Zhou, 2014), from traditional statistical models (Zhang and Zhou, 2007; Zhou et al., 2012; Surdeanu et al., 2012) to neural network-based models (Nam et al., 2014; Shimaoka et al., 2017). These models have variable structures, but usually, share the standard crossentropy loss function for training. After training, these models tend to use a single prediction policy for all the labels to generate the final predictions. Actually, the above process is based on the assumption t"
D19-1444,E17-1119,0,0.167567,"anized in a hierarchical dependency structure. (2) Even if labels do not demonstrate explicit correlation, they can still have implicit semantic dependencies. Introduction Multi-label classification aims at learning to make predictions on instances that are associated with multiple labels simultaneously, whereas in a classic multi-class classification setting, typically one instance has only one label. Multi-label classification is a common learning paradigm in a large amount of real-world natural language processing (NLP) applications, such as fine-grained entity typing (Ling and Weld, 2012; Shimaoka et al., 2017; Abhishek et al., 2017; Xin et al., 2018) and text classification (Nam et al., 2014; Liu et al., 2017; Chen et al., 2017; Wu et al., 2018). Significant amounts of research studies have been dedicated to tackle the multi-label classification problem (Zhang and Zhou, 2014), from traditional statistical models (Zhang and Zhou, 2007; Zhou et al., 2012; Surdeanu et al., 2012) to neural network-based models (Nam et al., 2014; Shimaoka et al., 2017). These models have variable structures, but usually, share the standard crossentropy loss function for training. After training, these models tend to us"
D19-1482,D14-1179,0,0.0074336,"Missing"
D19-1482,W05-0909,0,0.0578595,"Missing"
D19-1482,K16-1002,0,0.0449271,"Missing"
D19-1482,W17-3011,0,0.0898014,"Missing"
D19-1482,D18-1391,1,0.88411,"Missing"
D19-1482,N18-2019,1,0.805662,"Missing"
D19-1482,D14-1181,0,0.00419651,"Missing"
D19-1482,W17-1101,0,0.113089,"Missing"
D19-1482,D16-1127,0,0.460371,"be formulated as the following equation: X Obj = max log p(r|c) (1) (c,r)∈D where c is the conversation, r is the corresponding intervention response, and D is the dataset. This task is closely related to the response generation and dialog generation, though several differences exist including dialog length, language cadence, and word imbalances. As a baseline, we chose the most common methods of these two tasks, 4759 such as Seq2Seq and VAE, to determine the initial feasibility of automatically generate intervention responses. More recent Reinforcement Learning method for dialog generation (Li et al., 2016) can also be applied to this task with slight modification. Future work will explore more complex, and unique models. Similar to (Li et al., 2016), a generative model is considered as an agent. However, different from dialog generation, generative intervention does not have multiple turns of utterance, so the action of the agent is to select a token in the response. The state of the agent is given by the input posts and the previously generated tokens. Another result due to this difference is that the rewards with regard to ease of answering or information flow do not apply to this case, but t"
D19-1482,P02-1040,0,0.109296,"Missing"
D19-1482,R15-1086,0,0.0679708,"Missing"
D19-1482,W12-2103,0,0.171531,"Missing"
D19-1482,N16-2013,0,0.417395,"Missing"
D19-1623,W05-0909,0,0.367039,"ur main contributions are three-fold: • We are the first to introduce DSR to abstractive summarization and achieve better results than conventional rewards. • Unlike ROUGE, our DSR does not rely on crossentropy loss (XENT) to produce readable phrases. Thus, no exposure bias is introduced. • DSR improves generated tokens’ diversity and fluency while avoiding unnecessary repetitions. 2 Methodology Background While sequence models are usually trained using XENT, they are typically evaluated at test time using discrete NLP metrics such as B LEU (Papineni et al., 2002), ROUGE (Lin, 2004), M ETEOR (Banerjee and Lavie, 2005). Therefore, they suffer from both the exposure bias 6038 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6038–6044, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and non-differentiable task metric issues. To solve these problems, many resorts to deep RL with sequence to sequence model (Paulus et al., 2018; Ranzato et al., 2016; Ryang and Abekawa, 2012), where the learning agent interacts with a given environment. However, RL models ha"
D19-1623,N16-1012,0,0.0234643,"the reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language. 1 Introduction Abstractive summarization is a task of paraphrasing a long article with fewer words. Unlike extractive summarization, abstractive summaries can include tokens out of the article’s vocabulary. There exists several encoder-decoder approaches such as attention-based architecture (Bahdanau et al., 2015; Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016), incorporating graph techniques (Moawad and Aref, 2012; Ganesan et al., 2010), and adding pointer generator (Vinyals et al., 2015; Nallapati et al., 2016; See et al., 2017; Bello et al., 2017). However, long sentence generation suffers from exposure bias (Bahdanau et al., 2017) as the error accumulates during the decoding process. ∗ Equal contributions. Many innovative deep RL methods (Ranzato et al., 2016; Wu et al., 2016; Paulus et al., 2018; Lamb et al., 2016) are developed to alleviate this issue by providing sentence-level feedback after generating a complete sentence, in addition to opt"
D19-1623,N19-1423,0,0.0757774,"y innovative deep RL methods (Ranzato et al., 2016; Wu et al., 2016; Paulus et al., 2018; Lamb et al., 2016) are developed to alleviate this issue by providing sentence-level feedback after generating a complete sentence, in addition to optimal transport usage (Napoles et al., 2012). However, commonly used automatic evaluation metrics for generating sentence-level rewards count exact n-grams matches and are not robust to different words that share similar meanings since the semantic level reward is deficient. Currently, many studies on contextualized word representations (Peters et al., 2018; Devlin et al., 2019) prove that they have a powerful capacity of reflecting distributional semantic. In this paper, we propose to use the distributional semantic reward to boost the RL-based abstractive summarization system. Moreover, we design several novel objective functions. Experiment results show that they outperform the conventional objectives while increasing the sentence fluency. Our main contributions are three-fold: • We are the first to introduce DSR to abstractive summarization and achieve better results than conventional rewards. • Unlike ROUGE, our DSR does not rely on crossentropy loss (XENT) to p"
D19-1623,C10-1039,0,0.0446165,"ets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language. 1 Introduction Abstractive summarization is a task of paraphrasing a long article with fewer words. Unlike extractive summarization, abstractive summaries can include tokens out of the article’s vocabulary. There exists several encoder-decoder approaches such as attention-based architecture (Bahdanau et al., 2015; Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016), incorporating graph techniques (Moawad and Aref, 2012; Ganesan et al., 2010), and adding pointer generator (Vinyals et al., 2015; Nallapati et al., 2016; See et al., 2017; Bello et al., 2017). However, long sentence generation suffers from exposure bias (Bahdanau et al., 2017) as the error accumulates during the decoding process. ∗ Equal contributions. Many innovative deep RL methods (Ranzato et al., 2016; Wu et al., 2016; Paulus et al., 2018; Lamb et al., 2016) are developed to alleviate this issue by providing sentence-level feedback after generating a complete sentence, in addition to optimal transport usage (Napoles et al., 2012). However, commonly used automatic"
D19-1623,K16-1028,0,0.0520222,"Missing"
D19-1623,W12-3018,0,0.0234138,"Missing"
D19-1623,P17-1098,0,0.0718389,"Missing"
D19-1623,P02-1040,0,0.105154,"objectives while increasing the sentence fluency. Our main contributions are three-fold: • We are the first to introduce DSR to abstractive summarization and achieve better results than conventional rewards. • Unlike ROUGE, our DSR does not rely on crossentropy loss (XENT) to produce readable phrases. Thus, no exposure bias is introduced. • DSR improves generated tokens’ diversity and fluency while avoiding unnecessary repetitions. 2 Methodology Background While sequence models are usually trained using XENT, they are typically evaluated at test time using discrete NLP metrics such as B LEU (Papineni et al., 2002), ROUGE (Lin, 2004), M ETEOR (Banerjee and Lavie, 2005). Therefore, they suffer from both the exposure bias 6038 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6038–6044, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and non-differentiable task metric issues. To solve these problems, many resorts to deep RL with sequence to sequence model (Paulus et al., 2018; Ranzato et al., 2016; Ryang and Abekawa, 2012), where the learning agent in"
D19-1623,N18-1202,0,0.0444595,"al contributions. Many innovative deep RL methods (Ranzato et al., 2016; Wu et al., 2016; Paulus et al., 2018; Lamb et al., 2016) are developed to alleviate this issue by providing sentence-level feedback after generating a complete sentence, in addition to optimal transport usage (Napoles et al., 2012). However, commonly used automatic evaluation metrics for generating sentence-level rewards count exact n-grams matches and are not robust to different words that share similar meanings since the semantic level reward is deficient. Currently, many studies on contextualized word representations (Peters et al., 2018; Devlin et al., 2019) prove that they have a powerful capacity of reflecting distributional semantic. In this paper, we propose to use the distributional semantic reward to boost the RL-based abstractive summarization system. Moreover, we design several novel objective functions. Experiment results show that they outperform the conventional objectives while increasing the sentence fluency. Our main contributions are three-fold: • We are the first to introduce DSR to abstractive summarization and achieve better results than conventional rewards. • Unlike ROUGE, our DSR does not rely on crossen"
D19-1623,D15-1044,0,0.042761,"thout being limited to the surface form of the reference sentences. Human judgments on Gigaword and CNN/Daily Mail datasets show that our proposed distributional semantics reward (DSR) has distinct superiority in capturing the lexical and compositional diversity of natural language. 1 Introduction Abstractive summarization is a task of paraphrasing a long article with fewer words. Unlike extractive summarization, abstractive summaries can include tokens out of the article’s vocabulary. There exists several encoder-decoder approaches such as attention-based architecture (Bahdanau et al., 2015; Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016), incorporating graph techniques (Moawad and Aref, 2012; Ganesan et al., 2010), and adding pointer generator (Vinyals et al., 2015; Nallapati et al., 2016; See et al., 2017; Bello et al., 2017). However, long sentence generation suffers from exposure bias (Bahdanau et al., 2017) as the error accumulates during the decoding process. ∗ Equal contributions. Many innovative deep RL methods (Ranzato et al., 2016; Wu et al., 2016; Paulus et al., 2018; Lamb et al., 2016) are developed to alleviate this issue by providing sentence-level feedback after gene"
D19-1623,D12-1024,0,0.0258572,"ete NLP metrics such as B LEU (Papineni et al., 2002), ROUGE (Lin, 2004), M ETEOR (Banerjee and Lavie, 2005). Therefore, they suffer from both the exposure bias 6038 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6038–6044, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics and non-differentiable task metric issues. To solve these problems, many resorts to deep RL with sequence to sequence model (Paulus et al., 2018; Ranzato et al., 2016; Ryang and Abekawa, 2012), where the learning agent interacts with a given environment. However, RL models have poor sample efficiency and lead to very slow convergence rate. Therefore, RL methods usually start from a pretrained policy, which is established by optimizing XENT at each word generation step. 0 LXENT = − n X log P (yt |y1 , . . . , yt−1 , x). (1) t=1 Then, during RL stage, the conventional way is to adopt self-critical strategy to fine-tune based on the target evaluation metric, 0 LRL = n X log P (yˆt |yˆ1 , . . . , yt−1 ˆ , x) (2) t=1 × (rmetric (y b ) − rmetric (ˆ y )) Distributional Semantic Reward Dur"
D19-1623,P17-1099,0,0.0968343,"ing the lexical and compositional diversity of natural language. 1 Introduction Abstractive summarization is a task of paraphrasing a long article with fewer words. Unlike extractive summarization, abstractive summaries can include tokens out of the article’s vocabulary. There exists several encoder-decoder approaches such as attention-based architecture (Bahdanau et al., 2015; Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016), incorporating graph techniques (Moawad and Aref, 2012; Ganesan et al., 2010), and adding pointer generator (Vinyals et al., 2015; Nallapati et al., 2016; See et al., 2017; Bello et al., 2017). However, long sentence generation suffers from exposure bias (Bahdanau et al., 2017) as the error accumulates during the decoding process. ∗ Equal contributions. Many innovative deep RL methods (Ranzato et al., 2016; Wu et al., 2016; Paulus et al., 2018; Lamb et al., 2016) are developed to alleviate this issue by providing sentence-level feedback after generating a complete sentence, in addition to optimal transport usage (Napoles et al., 2012). However, commonly used automatic evaluation metrics for generating sentence-level rewards count exact n-grams matches and are n"
D19-1623,P19-1427,0,0.122942,"Missing"
D19-1623,1983.tc-1.13,0,0.544662,"Missing"
D19-5806,P17-1171,0,0.0353296,"ctive Bridge Reasoning for Open-Domain Multi-Hop Question Answering Wenhan Xiong† , Mo Yu‡ , Xiaoxiao Guo‡ , Hong Wang† , Shiyu Chang‡ , Murray Campbell‡ , William Yang Wang† † University of California, Santa Barbara ‡ IBM Research {xwhan, william}@cs.ucsb.edu, {yum,mcam}@us.ibm.com, {shiyu.chang, xiaoxiao.guo}@ibm.com Abstract search. In this paper, we focus on the practical open-domain HotpotQA benchmark where the questions are asked upon natural language passages instead of knowledge bases and the supporting passages are not known beforehand. The typical pipeline of open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Htut et al., 2018) is to first use an IR system to retrieve a compact set of paragraphs and then run a machine reading model over the concatenated or reranked paragraphs. While IR works reasonably well for simple questions1 , it often fails to retrieve the correct answer paragraph for multi-hop questions. This is due to the fact that the question often cannot fully cover the information for the second or further hops. Consider the question “What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?” from the HotpotQA (Yang et al."
D19-5806,N18-1059,0,0.0348029,"Missing"
D19-5806,D14-1179,0,0.0387723,"Missing"
D19-5806,P18-1078,0,0.0151082,"ssages into a standard passage reader to predict the final answer to the multi-hop question. 3.1 The Base Span Prediction Model Both the bridge reasoner and the passage reader use a model that predicts a relevant span given a 2 66.07 F1 and 49.43 EM with full support access versus 64.77 F1 and 50.96 EM with only answer passage access. 49 extract the correct answer span. We run the target passage reader on the top 10 answer passage candidates predicted by the bridge reasoner. question. We use the same model architecture for both tasks and the architecture is base on the document QA model from (Clark and Gardner, 2018), which is used by Yang et al. (2018) as the baseline for HotpotQA. The model uses a shared bidirectional GRU (Cho et al., 2014) to encode the question and the passages. The encoded questions and passages are then passed to a bidirectional attention layer (Seo et al., 2017) to get the questionaware passage states. The state vectors are enhanced by a self-attention layer (Wang et al., 2017) and are finally fed into linear layers to predict the start and end span scores at every word position. 3.2 Training Passages from Cross-validation As we are using the same set of training questions for trai"
D19-5806,P17-1018,0,0.0174812,"reader on the top 10 answer passage candidates predicted by the bridge reasoner. question. We use the same model architecture for both tasks and the architecture is base on the document QA model from (Clark and Gardner, 2018), which is used by Yang et al. (2018) as the baseline for HotpotQA. The model uses a shared bidirectional GRU (Cho et al., 2014) to encode the question and the passages. The encoded questions and passages are then passed to a bidirectional attention layer (Seo et al., 2017) to get the questionaware passage states. The state vectors are enhanced by a self-attention layer (Wang et al., 2017) and are finally fed into linear layers to predict the start and end span scores at every word position. 3.2 Training Passages from Cross-validation As we are using the same set of training questions for training the bridge reasoner and the target passage reader, there will be a discrepancy between the training and evaluation of QA: at evaluation time, the reader sees the passages predicted by the bridge reasoner, while at training time, the groundtruth answer passage is known. On the other hand, we also cannot use the predicted passages for training the reader, as the bridge reasoner itself i"
D19-5806,P19-1259,0,0.0808276,"Missing"
D19-5806,Q18-1021,0,0.100685,"Missing"
D19-5806,N18-4017,0,0.0116842,"Multi-Hop Question Answering Wenhan Xiong† , Mo Yu‡ , Xiaoxiao Guo‡ , Hong Wang† , Shiyu Chang‡ , Murray Campbell‡ , William Yang Wang† † University of California, Santa Barbara ‡ IBM Research {xwhan, william}@cs.ucsb.edu, {yum,mcam}@us.ibm.com, {shiyu.chang, xiaoxiao.guo}@ibm.com Abstract search. In this paper, we focus on the practical open-domain HotpotQA benchmark where the questions are asked upon natural language passages instead of knowledge bases and the supporting passages are not known beforehand. The typical pipeline of open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Htut et al., 2018) is to first use an IR system to retrieve a compact set of paragraphs and then run a machine reading model over the concatenated or reranked paragraphs. While IR works reasonably well for simple questions1 , it often fails to retrieve the correct answer paragraph for multi-hop questions. This is due to the fact that the question often cannot fully cover the information for the second or further hops. Consider the question “What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?” from the HotpotQA (Yang et al., 2018) dataset. Since the name of the"
D19-5806,D18-1259,0,0.291902,"t al., 2017; Wang et al., 2018; Htut et al., 2018) is to first use an IR system to retrieve a compact set of paragraphs and then run a machine reading model over the concatenated or reranked paragraphs. While IR works reasonably well for simple questions1 , it often fails to retrieve the correct answer paragraph for multi-hop questions. This is due to the fact that the question often cannot fully cover the information for the second or further hops. Consider the question “What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?” from the HotpotQA (Yang et al., 2018) dataset. Since the name of the person (Shirley Temple) is not directly mentioned in the question and the answer is about another aspect of the person other than film acting, traditional IR heuristics based on n-gram matching might fail to retrieve the answer passage. In fact, the correct answer passage of Shirley Temple never appears in the top passages ranked by the default IR method of HotpotQA. Instead of predicting the answer passage with text matching between passages and questions, we claim that the answer passage can be better inferred based on the context-level information. Noticing t"
D19-5806,D14-1162,0,0.0869645,"reasoning to future work. Question: What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell ? Figure 1: The overview of our QA system. The bridge reasoner reads the start passages retrieved by an IR system and predicts a set of candidate bridges (anchor links) that lead to the answer passages, which is further processed by the passage reader to return the answer. swer coverage of the top-ranked passages and thus increase the final QA performance by 14 point F1. Despite that our bridge reasoner and passage reader only learn above GloVe embeddings (Pennington et al., 2014), we achieve competitive performance with methods that use BERT (Devlin et al., 2018) in multiple modules. 2 Problem Definition and Motivation An open-domain multi-hop QA system aims to answer complex questions by retrieving evidence from a large open-domain passage corpus, such as Wikipedia. Usually the evidence scatters in a distributed set of supporting passages p1 , p2 , ..., pn that forms an ordered chain. Each pi provides evidence that partially fulfills the information required to answer the question, as well as provides clues (usually concepts or entities) that lead to the next support"
D19-5806,D16-1264,0,0.189657,"Missing"
I11-1032,P11-1135,0,0.0208302,"labeled pages but to a potentially unlimited number of unlabeled pages. Co-training involves building two independent views (classifiers), letting them automatically label the unlabeled data and incorporating this self-labeled data into the seed training set to improve system performance. Mihalcea (2004) shows that co-training is useful in word sense disamgibuation, Wan (2009) uses SVM for cotraining and shows improvement for cross-lingual sentiment analysis, and recent research has established the effectiveness of co-training for a variety of NLP tasks (Yu and K¨ubler, 2011; Li et al., 2011; Bergsma et al., 2011). In this section, we present our co-training classifiers, as well as an algorithm that is specifically tailored to automatic event identification. The Co-training Algorithm The motivation behind our co-training algorithm is to make use of the online summarization data for event identification, and improve recall as well as precision.The pseudo code of this algorithm is provided in Algorithm 1. Algorithm 1 The co-training algorithm Given: (1) a set S of labeled seed training examples; (2) a set U of unlabeled news summary examples; Initialize the iteration parameter k, pool size u, and leap si"
I11-1032,P09-1068,0,0.0250238,"ing sections detail the data, features and techniques used. 3.1 Related Work Corpora Seed Training Data and Test Data Annotation The problem of identifying and understanding events in natural language text has been explored in many ways that have produced a variety of perspectives on the challenges involved. Tasks explored have included the mapping of verblevel events into aspectual classes (Siegel and McKeown, 2000), detection of specific atomic events (Filatova and Hatzivassiloglou, 2003), supervised event classification (Bethard and Martin, 2006) and unsupervised learning of event schemas (Chambers and Jurafsky, 2009). The notion of what constitutes an event has similarly 1 Our Approach Our primary corpus of news documents and queries is derived from the DARPA Global Autonomous Language Exploitation (GALE) distillation training and evaluation sets. Fifty event queries provided for the GALE task were used for our experiments (randomly divided into 30 training queries and 20 test queries), and the set of documents was restricted to those containing at least one keyword bigram from any query in the set2 . For each query, we consider all sentences from the 2 As determined by the information-retrieval pipeline"
I11-1032,N03-2019,0,0.0913712,"Missing"
I11-1032,W06-1618,0,0.379417,"sentence “The discussions between Israelis and Palestinians follow last month’s Annapolis meeting where Israeli Prime Minister and Palestinian President Mahmoud Abbas met and agreed to try to negotiate a deal before the end of 2008.” is eventrelevant, whereas the sentence “Turkey has close ties to both Israel and the Palestinians.” is not. Yet both sentences feature the named entities from the query. The term “peace talks” does not appear in the event-relevant sentence, but is implied by “negotiate”. Previous approaches have addressed this problem using supervised learning (Mani et al., 2003; Bethard and Martin, 2006; Manshadi et al., 2008), where the relation between the query and eventrelevant sentences is learned. Still, gathering a large amount of training data is difficult and people often do not agree on what counts as relevant to an event description (Filatova and Hatzivassiloglou, 2003), making the process of manually labeling sentences as events both time consuming and expensive. This is supported by our own experiments with Amazon Mechanical Turk (AMT); we were only able to obtain 685 labeled event-relevant sentences on which 3 or more AMT users agreed after 16 days of posting. This is a strikin"
I11-1032,J00-4004,1,0.756329,"Missing"
I11-1032,W04-1000,0,0.330511,", our results show that co-training yields improvement even though are classifiers are not independent. Semi-Supervised Approach The original co-training framework (Blum and Mitchell, 1998) was introduced in the context of Web page classification where one typically has access to a limited number of labeled pages but to a potentially unlimited number of unlabeled pages. Co-training involves building two independent views (classifiers), letting them automatically label the unlabeled data and incorporating this self-labeled data into the seed training set to improve system performance. Mihalcea (2004) shows that co-training is useful in word sense disamgibuation, Wan (2009) uses SVM for cotraining and shows improvement for cross-lingual sentiment analysis, and recent research has established the effectiveness of co-training for a variety of NLP tasks (Yu and K¨ubler, 2011; Li et al., 2011; Bergsma et al., 2011). In this section, we present our co-training classifiers, as well as an algorithm that is specifically tailored to automatic event identification. The Co-training Algorithm The motivation behind our co-training algorithm is to make use of the online summarization data for event iden"
I11-1032,W04-2405,0,0.492392,". However, our results show that co-training yields improvement even though are classifiers are not independent. Semi-Supervised Approach The original co-training framework (Blum and Mitchell, 1998) was introduced in the context of Web page classification where one typically has access to a limited number of labeled pages but to a potentially unlimited number of unlabeled pages. Co-training involves building two independent views (classifiers), letting them automatically label the unlabeled data and incorporating this self-labeled data into the seed training set to improve system performance. Mihalcea (2004) shows that co-training is useful in word sense disamgibuation, Wan (2009) uses SVM for cotraining and shows improvement for cross-lingual sentiment analysis, and recent research has established the effectiveness of co-training for a variety of NLP tasks (Yu and K¨ubler, 2011; Li et al., 2011; Bergsma et al., 2011). In this section, we present our co-training classifiers, as well as an algorithm that is specifically tailored to automatic event identification. The Co-training Algorithm The motivation behind our co-training algorithm is to make use of the online summarization data for event iden"
I11-1032,H01-1056,0,0.0796951,"Missing"
I11-1032,P05-1045,0,0.00359298,"tators and robots. 5 Fleiss’ κ = 0.417, generally assumed to indicate moderate agreement between annotators 4 284 classifiers over a balanced corpus created by directly adding some summary data to the seed corpus. We observed the performance of these classifiers was poorer than that of classifiers trained only on the unaugmented seed corpus, suggesting that the assumption was too strong and that the summaries consist of both query-related and query-unrelated sentences. This supports the use of such a corpus in a semi-supervised setting. 3.2 the query. We used the Stanford Named Entity tagger (Finkel et al., 2005) to obtain named entities features from the unlabeled data. Note that the second view of our approach is clearly not independent of the first view, as the named entity matching is based on lexical matching. However, named entities are known as an informative source for selecting relevant sentences for a query since they serve as unique identifiers (Parton et al., 2008). Previous studies (Krogel and Scheffer, 2004) show that if the independence assumption of co-training approach is violated, the co-training approach can yield negative results. However, our results show that co-training yields i"
I11-1032,D08-1027,0,0.038708,"Missing"
I11-1032,rosenthal-etal-2010-towards,1,0.811558,"Missing"
I11-1032,P09-1027,0,0.0252743,"classifiers are not independent. Semi-Supervised Approach The original co-training framework (Blum and Mitchell, 1998) was introduced in the context of Web page classification where one typically has access to a limited number of labeled pages but to a potentially unlimited number of unlabeled pages. Co-training involves building two independent views (classifiers), letting them automatically label the unlabeled data and incorporating this self-labeled data into the seed training set to improve system performance. Mihalcea (2004) shows that co-training is useful in word sense disamgibuation, Wan (2009) uses SVM for cotraining and shows improvement for cross-lingual sentiment analysis, and recent research has established the effectiveness of co-training for a variety of NLP tasks (Yu and K¨ubler, 2011; Li et al., 2011; Bergsma et al., 2011). In this section, we present our co-training classifiers, as well as an algorithm that is specifically tailored to automatic event identification. The Co-training Algorithm The motivation behind our co-training algorithm is to make use of the online summarization data for event identification, and improve recall as well as precision.The pseudo code of thi"
I11-1032,W11-0323,0,0.0656265,"Missing"
I11-1032,W04-1017,0,\N,Missing
I17-2070,P02-1040,0,0.1001,"ry (a non-standard expression). We randomly select 371,028 entries for training, resulting in 907,624 sequence pairs of instance and reference explanation. The test set includes 50,000 entries, and 61,330 sentences. Note that all testing target non-standard expressions, instances and examples do not overlap with those in the training dataset. Experimental Settings 5 Our implementation is based on Tensorflow4 . For input embeddings, we randomly initialize the word vectors. We use stochastic gradient descent with adaptive learning rate to optimize the cross entropy function. We use BLEU scores (Papineni et al., 2002) for the evaluation. Conclusion In this paper, we introduce a new task of learning to explain newly emerged, non-standard English words and phrases. To do this, we collected 15year of UrbanDictionary data, and designed a dual encoder attentive sequence-to-sequence model to learn the hidden context representation and the hidden non-standard expression embedding. We showed that combining word-level and characterlevel models improved the performance for this task Quantitative and Qualitative Results Quantitative experimental results are showed in Table 1. Here we compare the performance of our pr"
I17-2070,P09-2041,0,0.010539,"nglish on newswire data. However, the nonstandard part of the language is not well-studied in the community, even though it becomes more and more important in the era of social media. While we agree that one must take a cautious approach to automatic generation of non-standard language (Hickman, 2013), but for many practical purposes, it is also of crucial importance for machines to be able to understand and explain this important subversion of the language. In the NLP community, using dictionaries of non-standard language as an external knowledge source is useful for many tasks. For example, Burfoot and Baldwin (2009) consult the slang defini1 For example, more than 2K entries are submitted daily to Urban Dictionary (Kolt and Lazier, 2009), the largest online slang resource. 413 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 413–417, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP and McKeown, 2011). However, we argue that using a small, fixed-size dictionary approach may be suboptimal: it suffers from the low coverage problem, and to keep the dictionary up to date, maintaining such dictionary is also expensive and time-consuming. To the best of"
I17-2070,P11-1077,0,0.0315187,"Missing"
I17-2070,W14-4012,0,0.0449583,"Missing"
I17-2070,D10-1124,0,0.0376748,"Missing"
I17-2070,C10-1129,1,0.846565,"Missing"
I17-2070,W11-0704,0,0.0571994,"Missing"
I17-2070,P11-1038,0,0.0195434,"nd a second character-level encoder to learn the hidden representation of the target non-standard expression. Our model can produce reasonable definitions of new non-standard English expressions given their context with certain confidence. 1 Figure 1: An example Tweet with a non-standard English expression. Our model aims at automatically explaining any newly emerged, non-standard expressions (generating the blue box). tions from Wiktionary to detect satirical news articles. Wang and McKeown (2010) show that using a slang dictionary of 5K terms can help detecting vandalism of Wikipedia edits. Han and Baldwin (2011) make use of the same slang dictionary, and achieve the best performance by combining the dictionary lookup approach with word similarity and context for Twitter and SMS text normalization. However, using a 5K slang dictionary may suffer from the coverage issue, since slang is evolving rapidly in the social media age1 . Recently, Thanapon Noraset (2016) shows that it is possible to use word embeddings to generate plausible definitions. Nonetheless, one weakness is that definition of a word may change within different contexts. In contrast, we take a more radical approach: we aim at building a"
N15-1039,P07-1038,0,0.00899932,"We use the Statistical Toolbox’s linear regression implementation in Matlab, and LibSVM (Chang and Lin, 2011) for 5 http://lucene.apache.org/ http://www.cs.cmu.edu/˜yww/data/meme dataset.zip. 7 memegenerator.net and cheezburger.com 6 training and testing the SVM models. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold cross-validation. Evaluation Metrics: Spearman’s correlation (Hogg and Craig, 1994) and Kendall’s tau (Kendall, 1938) have been widely used in many real-valued prediction (regression) problems in NLP (Albrecht and Hwa, 2007; Yogatama et al., 2011), and here we use them to meaˆ by comparing sure the quality of predicted values y to the vector of ground truth y. Kendall’s tau is a nonparametric statistical metric that have shown to be inexpensive, robust, and representation independent (Lapata, 2006). We use paired two-tailed t-test to measure the statistical significance. 5.1 Comparison with Various Baselines The first two figures in Figure 5 show the learning curve of our system, comparing other baselines. We see that when increasing the amount of training data, our approach clearly dominates all other methods b"
N15-1039,N12-1074,0,0.0117735,"nparanormal model outperforms competitive baselines for predicting and generating popular meme descriptions. In the next section, we outline related work. In Section 3, we introduce the theory of copula, and our nonparanormal approach. In Section 4, we describe the datasets. We show the prediction and generation results in Section 5 and Section 6. Finally, we conclude in Section 7. 2 Related Work Although the language of Internet memes is a relatively new research topic, our work is broadly related to studies on predicting popular social media messages (Hong et al., 2011; Bakshy et al., 2011; Artzi et al., 2012). Most recently, Tan et al. (2014) study the effect on wordings for Tweets. However, none of the above studies have investigated multimodal approaches that combine text and vision. Recently, there has been growing interests in inter-disciplinary research on generating image descriptions. Gupta el al. (2009) have studied the problem of constructing plots from video understanding. The work by Farhadi et al. (2010) is among the first to generate sentences from images. Kulkarni et al. (2011) use linguistic constraints and a conditional random field model for the task, whereas Mitchell et al. (2012"
N15-1039,N10-1138,0,0.0170022,"Missing"
N15-1039,P14-1129,0,0.0151085,"inary research on generating image descriptions. Gupta el al. (2009) have studied the problem of constructing plots from video understanding. The work by Farhadi et al. (2010) is among the first to generate sentences from images. Kulkarni et al. (2011) use linguistic constraints and a conditional random field model for the task, whereas Mitchell et al. (2012) leverage syntactic information and co-occurrence statistics and Dodge et al. (2012) use a large text corpus and CV algorithms for detecting visual text. With the surge of interests in deep learning techniques in NLP (Socher et al., 2013; Devlin et al., 2014) and CV (Krizhevsky et al., 2012; Oquab et al., 2013), there have been several unrefereed manuscripts on parsing images and generating text descriptions lately (Vinyals et al., 2014; Chen 356 and Zitnick, 2014; Donahue et al., 2014; Fang et al., 2014; Karpathy and Fei-Fei, 2014) using neural network models. Although the above studies have shown interesting results, our task is arguably more complex than generating text descriptions: in addition to the visual and textual signals, we have to model the popular votes as a third dimension for learning. For example, we cannot simply train a convolut"
N15-1039,N12-1094,0,0.0358844,"r Tweets. However, none of the above studies have investigated multimodal approaches that combine text and vision. Recently, there has been growing interests in inter-disciplinary research on generating image descriptions. Gupta el al. (2009) have studied the problem of constructing plots from video understanding. The work by Farhadi et al. (2010) is among the first to generate sentences from images. Kulkarni et al. (2011) use linguistic constraints and a conditional random field model for the task, whereas Mitchell et al. (2012) leverage syntactic information and co-occurrence statistics and Dodge et al. (2012) use a large text corpus and CV algorithms for detecting visual text. With the surge of interests in deep learning techniques in NLP (Socher et al., 2013; Devlin et al., 2014) and CV (Krizhevsky et al., 2012; Oquab et al., 2013), there have been several unrefereed manuscripts on parsing images and generating text descriptions lately (Vinyals et al., 2014; Chen 356 and Zitnick, 2014; Donahue et al., 2014; Fang et al., 2014; Karpathy and Fei-Fei, 2014) using neural network models. Although the above studies have shown interesting results, our task is arguably more complex than generating text de"
N15-1039,P05-1045,0,0.0147044,"dency Triples: to better understand the deeper syntactic dependencies of keywords in 1 This is necessary for the normal inversion of the ECDFs, which we will describe in Section 3.2. 357 Figure 3: An example of the standard SIFT keypoints detected on the “doge” meme. memes, we have also extracted typed dependency triples (e.g., subj(I,are)) using the MaltParser (Nivre et al., 2007). • Named Entity Features: after browsing the dataset, we notice that certain names are often mentioned in memes (e.g. “Drake”, “Kenye West”, and “Justin Bieber”), so we utilize the Stanford named entity recognizer (Finkel et al., 2005) to extract lexicalized named entities. • Frame-Semantics Features: SEMAFOR (Das et al., 2010) is a state-of-the-art framesemantics parser that produces FrameNet-style semantic annotation. We use SEMAFOR to extract frame-level semantic features. Visual Features A key insight on viral memes is that the images producing a shared social signal are typically inter-related in style. For example, LOLcats are an early series of memes involving funny cat photos. Similarly, “Bieber memes” involve modified pictures of Bieber. Therefore, we hypothesize that, by extracting visual features, it is of crucia"
N15-1039,J06-4002,0,0.0236745,"Missing"
N15-1039,E12-1076,0,0.0324679,"; Artzi et al., 2012). Most recently, Tan et al. (2014) study the effect on wordings for Tweets. However, none of the above studies have investigated multimodal approaches that combine text and vision. Recently, there has been growing interests in inter-disciplinary research on generating image descriptions. Gupta el al. (2009) have studied the problem of constructing plots from video understanding. The work by Farhadi et al. (2010) is among the first to generate sentences from images. Kulkarni et al. (2011) use linguistic constraints and a conditional random field model for the task, whereas Mitchell et al. (2012) leverage syntactic information and co-occurrence statistics and Dodge et al. (2012) use a large text corpus and CV algorithms for detecting visual text. With the surge of interests in deep learning techniques in NLP (Socher et al., 2013; Devlin et al., 2014) and CV (Krizhevsky et al., 2012; Oquab et al., 2013), there have been several unrefereed manuscripts on parsing images and generating text descriptions lately (Vinyals et al., 2014; Chen 356 and Zitnick, 2014; Donahue et al., 2014; Fang et al., 2014; Karpathy and Fei-Fei, 2014) using neural network models. Although the above studies have"
N15-1039,P02-1040,0,0.107574,"titatively evaluate our system, we compare with both unsupervised and supervised baselines. For the unsupervised baselines, we compare with a compact recurrent neural network language model (RNNLM) (Mikolov, 2012) trained on the 3,008 text descriptions of our meme training set, as well as a full model of RNNLM trained on a large meme corpus of 269K sentences9 . For the supervised baselines, all models are trained on the 3,008 training image-description pairs with labels. All these models can be viewed as different re-ranking methods for the retrieved candidate descriptions. We use BLEU score (Papineni et al., 2002) as the evaluation metric, since the generation task can be viewed as translating raw images into sentences, and it is 9 Note that there are no image features feeding to the unsupervised RNN models. Figure 6: Examples from the meme generation experiment. First row: the chemistry cat meme. Second row: the forever alone meme. Third row: the Batman slaps Robin meme. Left column: human generated topvoted meme descriptions on memegenerator.net at the time of writing. Middle column: generated output from RNNLM. Right column: generated output from NPNs. used in many caption generation studies (Vinyal"
N15-1039,D13-1170,0,0.00285316,"ests in inter-disciplinary research on generating image descriptions. Gupta el al. (2009) have studied the problem of constructing plots from video understanding. The work by Farhadi et al. (2010) is among the first to generate sentences from images. Kulkarni et al. (2011) use linguistic constraints and a conditional random field model for the task, whereas Mitchell et al. (2012) leverage syntactic information and co-occurrence statistics and Dodge et al. (2012) use a large text corpus and CV algorithms for detecting visual text. With the surge of interests in deep learning techniques in NLP (Socher et al., 2013; Devlin et al., 2014) and CV (Krizhevsky et al., 2012; Oquab et al., 2013), there have been several unrefereed manuscripts on parsing images and generating text descriptions lately (Vinyals et al., 2014; Chen 356 and Zitnick, 2014; Donahue et al., 2014; Fang et al., 2014; Karpathy and Fei-Fei, 2014) using neural network models. Although the above studies have shown interesting results, our task is arguably more complex than generating text descriptions: in addition to the visual and textual signals, we have to model the popular votes as a third dimension for learning. For example, we cannot s"
N15-1039,P14-1017,0,0.0136709,"itive baselines for predicting and generating popular meme descriptions. In the next section, we outline related work. In Section 3, we introduce the theory of copula, and our nonparanormal approach. In Section 4, we describe the datasets. We show the prediction and generation results in Section 5 and Section 6. Finally, we conclude in Section 7. 2 Related Work Although the language of Internet memes is a relatively new research topic, our work is broadly related to studies on predicting popular social media messages (Hong et al., 2011; Bakshy et al., 2011; Artzi et al., 2012). Most recently, Tan et al. (2014) study the effect on wordings for Tweets. However, none of the above studies have investigated multimodal approaches that combine text and vision. Recently, there has been growing interests in inter-disciplinary research on generating image descriptions. Gupta el al. (2009) have studied the problem of constructing plots from video understanding. The work by Farhadi et al. (2010) is among the first to generate sentences from images. Kulkarni et al. (2011) use linguistic constraints and a conditional random field model for the task, whereas Mitchell et al. (2012) leverage syntactic information a"
N15-1039,N03-1033,0,0.0262929,"we first explain the visual and textual features used in this study. Then, we introduce the theory of copula, and describe the robust nonparanormal. Finally, we show a simple pipeline for generating meme descriptions. 3.1 Features Textual Features To model the meme descriptions, we take a broad range of textual features into considerations: • Lexical Features: we extract unigrams and bigrams from meme descriptions as surface-level lexical features. • Part-of-Speech Features: to model shallow syntactic cues, we extract lexicalized part-ofspeech features using the Stanford part-ofspeech tagger (Toutanova et al., 2003). • Dependency Triples: to better understand the deeper syntactic dependencies of keywords in 1 This is necessary for the normal inversion of the ECDFs, which we will describe in Section 3.2. 357 Figure 3: An example of the standard SIFT keypoints detected on the “doge” meme. memes, we have also extracted typed dependency triples (e.g., subj(I,are)) using the MaltParser (Nivre et al., 2007). • Named Entity Features: after browsing the dataset, we notice that certain names are often mentioned in memes (e.g. “Drake”, “Kenye West”, and “Justin Bieber”), so we utilize the Stanford named entity rec"
N15-1039,P14-1109,1,0.900212,"oach: we investigate copula methods (Schweizer and Sklar, 1983; Nelsen, 1999), in particular, the nonparanormals (Liu et al., 2009), for joint modeling of raw images, text descriptions, and popular votes. Copula is a statistical framework for analyzing random variables from Statistics (Liu et al., 2012), and often used in Economics (Chen and Fan, 2006). Only until very recently, researchers from the machine learning and information retrieval communities (Ghahramani et al., 2012; Han et al., 2012; Eickhoff et al., 2013). start to understand the theory and the predictive power of copula models. Wang and Hua (2014) are the first to introduce semiparametric Gaussian copula (a.k.a. nonparanormals) for text prediction. However, their approach may be prone to overfitting. In this work, we generalize Wang and Hua’s method to jointly model text and vision features with popular votes, while scaling up the model using effective dropout regularization. 3 Our Approach A key challenge for joint modeling of text and vision is that, because textual features are often relatively sparse and discrete, while visual features are typically dense and continuous, it is difficult to model them jointly in a principled way. To"
N15-1039,D11-1055,0,0.0454159,"Missing"
N15-1064,N10-1138,0,0.0528064,"be expressed on the basis of semantic frames, which encompass three major components: frame (F), frame elements (FE), and lexical units (LU). For example, the frame “food” contains words referring to items of food. A descriptor frame element within the food frame indicates the characteristic of the food. For example, the phrase “low fat milk” should be analyzed with “milk” evoking the food frame and “low fat” filling the descriptor FE of that frame. In our approach, we parse all ASR-decoded utterances in our corpus using SEMAFOR5 , a stateof-the-art semantic parser for frame-semantic parsing (Das et al., 2010; Das et al., 2013), and extract all frames from semantic parsing results as slot candidates, where the LUs that correspond to the frames are extracted for slot filling. For example, Figure 2 shows an example of an ASR-decoded text output parsed by SEMAFOR. SEMAFOR generates three frames (capability, expensiveness, and locale by use) for the utterance, which we consider as slot candidates for training the SLU model. Note that for each slot candidate, SEMAFOR also includes the corresponding lexical unit (can i, cheap, 5 http://www.ark.cs.cmu.edu/SEMAFOR/ 621 Independent Semantic Decoder With ou"
N15-1064,W08-1301,0,0.00761516,"Missing"
N15-1064,J06-2003,0,0.0227167,"walk, are still less-studied for direct inference on knowledge graphs of the spoken contents. In the natural language processing literature, Lao et al. (2011) used a random walk algorithm to construct inference rules on large entity-based knowledge bases, and leveraged syntactic information for reading the Web (Lao et al., 2012). Even though this work has important contributions, the proposed algorithm cannot learn mutually-recursive relations, and does not to consider lexical items— in fact, more and more studies show that, in addition to semantic knowledge graphs, lexical knowledge graphs (Inkpen and Hirst, 2006; Song et al., 2011; Li et al., 2013b) that model surface-level natural language realization, multiword expressions, and context (Li et al., 2013a), are also critical for short text understanding (Song et al., 2011; Wang et al., 2014). From the engineering perspective, quick and easy development turnaround time for domain-specific dialogue applications is also critical (Chen and Rudnicky, 2014). Prior work shows that it is possible to use the frame-semantics theory to automatically in1 http://www.windowsphone.com/en-us/how-to/ wp8/cortana 2 http://www.google.com/landing/now 3 http://www.apple."
N15-1064,D11-1049,0,0.0360983,"ment perspective, empowering the system with a large knowledge base is of crucial significance to modern spoken dialogue systems. On this end, our work clearly aligns with recent studies on leveraging semantic knowledge graphs for SLU modeling (Heck et al., 2013; Hakkani-T¨ur et al., 2013; Hakkani-T¨ur et al., 2014; El-Kahky et al., 2014; Chen et al., 2014a). While leveraging external knowledge is the trend, efficient inference algorithms, such as random walk, are still less-studied for direct inference on knowledge graphs of the spoken contents. In the natural language processing literature, Lao et al. (2011) used a random walk algorithm to construct inference rules on large entity-based knowledge bases, and leveraged syntactic information for reading the Web (Lao et al., 2012). Even though this work has important contributions, the proposed algorithm cannot learn mutually-recursive relations, and does not to consider lexical items— in fact, more and more studies show that, in addition to semantic knowledge graphs, lexical knowledge graphs (Inkpen and Hirst, 2006; Song et al., 2011; Li et al., 2013b) that model surface-level natural language realization, multiword expressions, and context (Li et a"
N15-1064,D12-1093,0,0.00457124,"recent studies on leveraging semantic knowledge graphs for SLU modeling (Heck et al., 2013; Hakkani-T¨ur et al., 2013; Hakkani-T¨ur et al., 2014; El-Kahky et al., 2014; Chen et al., 2014a). While leveraging external knowledge is the trend, efficient inference algorithms, such as random walk, are still less-studied for direct inference on knowledge graphs of the spoken contents. In the natural language processing literature, Lao et al. (2011) used a random walk algorithm to construct inference rules on large entity-based knowledge bases, and leveraged syntactic information for reading the Web (Lao et al., 2012). Even though this work has important contributions, the proposed algorithm cannot learn mutually-recursive relations, and does not to consider lexical items— in fact, more and more studies show that, in addition to semantic knowledge graphs, lexical knowledge graphs (Inkpen and Hirst, 2006; Song et al., 2011; Li et al., 2013b) that model surface-level natural language realization, multiword expressions, and context (Li et al., 2013a), are also critical for short text understanding (Song et al., 2011; Wang et al., 2014). From the engineering perspective, quick and easy development turnaround t"
N15-1064,P14-2050,0,0.0784672,"ings of the raw audio conversation files, and then adapt the FrameNet-style frames to the semantic slots in the target semantic space, so that they can be used practically in the SDSs. Chen et al. formulated the semantic mapping and adaptation problem as a ranking problem to differentiate generic semantic concepts from target semantic space for task-oriented dialogue systems. This paper improves the adaptation process by leveraging distributed word embeddings associated with typed syntactic dependencies between words to infer inter-slot relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014). The proposed framework is shown in Figure 1. In the remainder of the section, we first introduce framesemantic parsing to obtain slot candidates. With slot candidates, then we train the independent semantic decoders. The adaptation process, which is the main focus of this paper, is performed to decide outputted slots. Finally we can build an SLU model based on the learned semantic decoders and induced slots. 3.1 Probabilistic Semantic Parsing FrameNet is a linguistically-principled semantic resource that offers annotations of predicate-argument semantics, and associated lexical units for Eng"
N15-1064,N13-1090,0,0.154004,"from automatic speech recognition (ASR) decodings of the raw audio conversation files, and then adapt the FrameNet-style frames to the semantic slots in the target semantic space, so that they can be used practically in the SDSs. Chen et al. formulated the semantic mapping and adaptation problem as a ranking problem to differentiate generic semantic concepts from target semantic space for task-oriented dialogue systems. This paper improves the adaptation process by leveraging distributed word embeddings associated with typed syntactic dependencies between words to infer inter-slot relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014). The proposed framework is shown in Figure 1. In the remainder of the section, we first introduce framesemantic parsing to obtain slot candidates. With slot candidates, then we train the independent semantic decoders. The adaptation process, which is the main focus of this paper, is performed to decide outputted slots. Finally we can build an SLU model based on the learned semantic decoders and induced slots. 3.1 Probabilistic Semantic Parsing FrameNet is a linguistically-principled semantic resource that offers annotations of predicate-argume"
N15-1064,P13-1045,0,0.00828976,"Missing"
N15-1064,P14-2105,0,0.060405,"ns the word participates in for training embeddings, where the embeddings are less topical but offer more functional similarity compared to original embeddings (Levy and Goldberg, 2014). Table 1 shows the extracted dependency-based contexts for each target word from the example in Figure 4, where headwords and their dependents can form the contexts by following the arc on a word in the dependency tree, and −1 denotes the directionality of the dependency. After replacing original bagof-words contexts with dependency-based contexts, we can train dependency-based embeddings for all target words (Yih et al., 2014; Bordes et al., 2011; Bordes et al., 2013). For training dependency-based word embeddings, each word w is associated with a word vector vw ∈ Rd and each context c is represented as a context vector vc ∈ Rd , where d is the embedding dimensionality. We learn vector representations for both words and contexts such that the dot product vw · vc associated with “good” word-context pairs belonging to the training data D is maximized, leading to the objective function: X 1 arg max log , (5) vw ,vc 1 + exp(−vc · vw ) (w,c)∈D which can be trained using stochastic-gradient updates (Levy and Goldberg, 2"
N15-1064,P98-1013,0,\N,Missing
N15-1064,C98-1013,0,\N,Missing
N15-1064,J14-1002,0,\N,Missing
N16-1008,D07-1069,1,0.692207,"aselines on two publicly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. ("
N16-1008,C08-1040,1,0.865783,"icly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the"
N16-1008,D13-1068,0,0.0170224,"ls for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable models, even though it is difficult to incorporate features and high-dimensional latent states in a HMM-based model. Ahmed et al. (2011) proposed a hierarchical nonparametric model that integrates a Recurrent Chinese Restaurant Process with Latent Dirichlet Allocation to cluster words over time. The main issues with this approach are that it does not generate human-readable sentences, and that scaling nonparametric Bayesian models is often challenging. Similarly, Huang and Huang (2013) introduced a joint mixture-event-aspect model using a generative method. Navarro-Colorado and Saquete (2015) combined temporal information with topic modeling, and obtained the best performance in the crossdocument event ordering task of SemEval 2015. There has been prior work (Wang et al., 2008; Lee et al., 2009) using matrix factorization to perform sentence clustering. A key distinction between our work and this previous work is that our method requires no additional sentence selection steps after sentence clustering, so we avoid error cascades. 60 Zhu and Chen (2007) were among the first"
N16-1008,W04-1013,0,0.0596697,"tent factor models have had huge success in recommender systems. These latent factors, often in the form of low-rank embeddings, capture not only explicit information but also implicit context from the input data. In this work, we propose a novel matrix factorization framework to “recommend” key sentences to a timeline. Figure 2 shows an overview of the framework. More specifically, we formulate this task as a matrix completion problem. Given a news corpus, we assume that there are m total sentences, which are the rows in the matrix. The first column is the metric section, where we use ROUGE (Lin, 2004) as the metric to pre-compute a sentence importance 61 score between a candidate sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we r"
N16-1008,S15-2138,0,0.0165672,"e first to consider latent variable models, even though it is difficult to incorporate features and high-dimensional latent states in a HMM-based model. Ahmed et al. (2011) proposed a hierarchical nonparametric model that integrates a Recurrent Chinese Restaurant Process with Latent Dirichlet Allocation to cluster words over time. The main issues with this approach are that it does not generate human-readable sentences, and that scaling nonparametric Bayesian models is often challenging. Similarly, Huang and Huang (2013) introduced a joint mixture-event-aspect model using a generative method. Navarro-Colorado and Saquete (2015) combined temporal information with topic modeling, and obtained the best performance in the crossdocument event ordering task of SemEval 2015. There has been prior work (Wang et al., 2008; Lee et al., 2009) using matrix factorization to perform sentence clustering. A key distinction between our work and this previous work is that our method requires no additional sentence selection steps after sentence clustering, so we avoid error cascades. 60 Zhu and Chen (2007) were among the first to consider multimodal timeline summarization, but they focus on visualization, and do not make use of images"
N16-1008,P14-1087,0,0.0300454,"example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable models, even though it is difficult to incorporate feature"
N16-1008,nivre-etal-2006-maltparser,0,0.156814,"e sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we run the Stanford part-of-speech tagger (Toutanova et al., 2003) and MaltParser (Nivre et al., 2006) to generate SVO events based on dependency parses. Additional features can easily be incorporated into this framework; we leave the consideration of additional features for future work. Finally, for each sentence, we use an image search engine to retrieve a top-ranked relevant image, and then we use a convolutional neural network (CNN) architecture to extract visual features in an unsupervised fashion. We use a CNN model from Simonyan and Zisserman (2015), which is trained on the ImageNet Challenge 2014 dataset (Russakovsky et al., 2014). In our work, we keep the 16 convolutional layers and m"
N16-1008,radev-etal-2004-mead,1,0.359708,"date sentence in a news article, we take advantage of top-ranked relevant images from the Web and model the image using a convolutional neural network architecture. Finally, we propose a scalable low-rank approximation approach for learning joint embeddings of news stories and images. In experiments, we compare our model to various competitive baselines, and demonstrate the stateof-the-art performance of the proposed textbased and multimodal approaches. 1 To distill key insights from news reports, prior work in summarization often relies on feature engineering, and uses clustering techniques (Radev et al., 2004b) to select important events to be included in the final summary. While this approach is unsupervised, the process of feature engineering is always expensive, and the number of clusters is not easy to estimate. To present a complete summary, researchers from the natural language processing (NLP) community often solely rely on the textual information, while studies in the computer vision (CV) community rely solely on the image and video information. However, even though news images are abundantly available together with news stories, approaches that jointly learn textual and visual representat"
N16-1008,N03-1033,0,0.0186037,"e importance 61 score between a candidate sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we run the Stanford part-of-speech tagger (Toutanova et al., 2003) and MaltParser (Nivre et al., 2006) to generate SVO events based on dependency parses. Additional features can easily be incorporated into this framework; we leave the consideration of additional features for future work. Finally, for each sentence, we use an image search engine to retrieve a top-ranked relevant image, and then we use a convolutional neural network (CNN) architecture to extract visual features in an unsupervised fashion. We use a CNN model from Simonyan and Zisserman (2015), which is trained on the ImageNet Challenge 2014 dataset (Russakovsky et al., 2014). In our work, we ke"
N16-1008,N15-1112,0,0.147479,"rmance. Our main contributions are three-fold: • We propose a novel matrix factorization approach for extractive summarization, leveraging the success of collaborative filtering; • We are among the first to consider representation learning of a joint embedding for text and images in timeline summarization; • Our model significantly outperforms various competitive baselines on two publicly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popul"
N16-1008,D11-1040,0,0.599874,"Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable mo"
N16-1008,W04-3252,0,\N,Missing
N16-4005,P15-1035,1,0.882666,"Missing"
N16-4005,D14-1122,1,0.870886,"Missing"
N18-1113,D14-1181,0,0.00316236,"have two views, the headline and the paragraph. Thus, we construct the two classifiers in co-training based on these two views. Headline Classifier The previous state-of-theart model (Zhou, 2017) for clickbait detection uses 1257 a self-attentive bi-directional gated recurrent unit RNN (biGRU) to model the headlines of the document and train a classifier. Following the same setting, we choose self-attentive biGRU as the headline classifier in co-training. Paragraph Classifier The paragraphs usually have much longer sequences than the headlines. Thus, we utilize the CNN-non-static structure in Kim (2014) as the paragraph classifier to capture the paragraph information. Note that the other three co-training baselines also use the same classifier settings. In our Reinforce Co-Training model, we set the number of unlabeled subsets k as 80. Considering the clickbait detection as a 2-class classification problem (N = 2), the Q-network maps 4-d input Pi1 ||Pi2 in the state representation to a 3-d common embedding space (y = 3), with a further hidden layer of 128 units on top. The dimension k of the softmax layer is also 80. As for the other semi-supervised baselines, Sequence-SSL, Region-SSL and Ad"
N18-1113,D17-1035,0,0.0308153,"that after sample different data partitions, we will also reprocess the unlabeled sets as described in Section 3.1. We then evaluate these 10 classifiers using the same metric. The results are shown in Table 6. The results demonstrate that our learning algorithm is robust to different (seeding) training sets and partitions of the unlabeled set, which again indicates that the Q-agent in our model is able to learn a good and robust data selection policy to select high-quality unlabeled subsets to help the co-training process. 4.4 Discussion about Stability Previous studies (Zhang et al., 2014; Reimers and Gurevych, 2017) show that neural networks can be unstable even with the same training parameters on the same training data. As for our cases, when the two classifiers are initialized with different labeled seeding sets, they can be very unstable. However, after enough iterations with the properly selected unlabeled data, the performance would be stable generally. Usually, the more substantial labeled training datasets will lead to more stable models. However, the problem is that the AGs News and DBpedia have 4 and 14 classes separately, while the Clickbait dataset only has 2 classes. That means the numbers o"
N18-1113,P09-1027,0,0.307834,"eled text corpora available on the web. Semi-supervised methods permit learning improved supervised models by jointly train on a small labeled dataset and a large unlabeled dataset (Zhu, 2006; Chapelle et al., 2009). Co-training is one of the widely used semisupervised methods, where two complementary classifiers utilize large amounts of unlabeled examples to bootstrap the performance of each other iteratively (Blum and Mitchell, 1998; Nigam and Ghani, 2000). Co-training can be readily applied to NLP tasks since data in these tasks naturally have two or more views, such as multi-lingual data (Wan, 2009) and document data (headline and content) (Ghani, 2000; Denis et al., 2003). In the co-training framework, each classifier is trained on one of the two views (aka a subset of features) of both labeled and unlabeled data, under the assumption that either view is sufficient to classify. In each iteration, the co-training algorithm selects high confidence samples scored by each of the classifiers to form an auto-labeled dataset, and the other classifier is then updated with both labeled data and additional auto-labeled set. However, as shown in Figure 1, most of existing co-training methods have"
N18-1129,N15-1021,0,0.473307,"nyms are pronounced using the regular reading rules (for example. YOLO) (b) Initialisms are pronounced letter by letter (for example. BBC). In our work, we propose generative models using a data-driven approach towards generating blends, clippings ,and reduplicatives. We do not consider generative models for alphabetisms since a majority of them can be trivially generated by picking the first letter of each word making up the acronym (for example. laugh out loud → lol). We now outline the datasets considered: 1. Blends. We consider a gold standard dataset Dknight of 400 blends constructed by (Deri and Knight, 2015) from Wikipedia as well as a larger list of 1624 blends manually compiled by (Gangal et al., 2017) called Dlarge , a superset of Dknight . We define Dblind = Dlarge − Dknight . 2. Clippings. We consider a list of 576 human curated clippings constructed by Mattiello (2013) for our analysis of clippings. These were manually collected from a variety of sources including prior work and dictionaries like the Oxford English Dictionary and the Merriam-Webster online dictionary. 2. Blends or portmanteaus, are formed by merging parts of existing words. For example, edutainment is a blend of education a"
N18-1129,D17-1315,0,0.127873,"ed letter by letter (for example. BBC). In our work, we propose generative models using a data-driven approach towards generating blends, clippings ,and reduplicatives. We do not consider generative models for alphabetisms since a majority of them can be trivially generated by picking the first letter of each word making up the acronym (for example. laugh out loud → lol). We now outline the datasets considered: 1. Blends. We consider a gold standard dataset Dknight of 400 blends constructed by (Deri and Knight, 2015) from Wikipedia as well as a larger list of 1624 blends manually compiled by (Gangal et al., 2017) called Dlarge , a superset of Dknight . We define Dblind = Dlarge − Dknight . 2. Clippings. We consider a list of 576 human curated clippings constructed by Mattiello (2013) for our analysis of clippings. These were manually collected from a variety of sources including prior work and dictionaries like the Oxford English Dictionary and the Merriam-Webster online dictionary. 2. Blends or portmanteaus, are formed by merging parts of existing words. For example, edutainment is a blend of education and entertainment. Prior work notes that blend formation does not exhibit rigid rules but only demo"
N18-1129,P12-1074,0,0.0948139,"hile the definition of slang is a controversial issue, we adopt a broad definition including non-standard expressions. 2 While such phenomena are likely present in several languages, in this work we restrict ourselves to slang in English. even within a particular class like B LENDS there are variations in what portions of the components are retained. These word formation mechanisms are not only attractive from a linguistic standpoint in deepening our understanding of slang but also have applications spanning the development of rich conversational agents and tools like brand name generators ¨ (Ozbal and Strapparava, 2012). While such phenomena have been qualitatively studied by Mattiello (2008, 2013), computational models for their generation have not been proposed. In this paper, we propose the first simple models for generating blends, clippings, and reduplicatives3 . Our models incorporate linguistic insights coupled with data-driven analysis to model the above phenomena. In line with “Occam’s razor”, we strive for simplicity. The simplicity of our models not only implies better generalization, more robust estimation of parameters in the wake of small dataset sizes, and better interpret-ability but also yie"
N18-1129,P16-2067,0,0.0292308,"that incorporating structural constraints yields a different view of modeling the problem that can enable better generalization given the small amount of training data. We motivate this by observing the following constraints: 1. Blend length and vocabulary constraints. First, we observe that a majority of blends Equivalent Problem Definition Given a string C = C1 #C2 , consisting of components C1 and C2 , learn E(B), a labeling of each character in C from the label set {C, D}. 3.1.2 Proposed model: C OPY C AT Model Architecture Inline with work on neural sequence labeling (Wang et al., 2015; Plank et al., 2016), our model uses a single layer long 1426 Candidate Generation Our model outputs probability scores over the label set for each element in the sequence. As in previous works (Gangal et al., 2017), we use the output to generate an ordered candidate set T . To construct T , we use a simple top-K decoder which selects the k most probable label sequences. Finding the k most probable tag sequences from the soft-max outputs can be cast as finding the top k shortest simple paths in a directed acyclic graph which can be efficiently solved using (Yen, 1971; Eppstein, 1998). Note that the greedy decoder"
N18-1133,P15-1067,0,0.178089,"ommonly used KGE datasets. 2 2.1 Related Work Knowledge Graph Embeddings A large number of knowledge graph embedding models, which represent entities and relations in a knowledge graph with vectors or matrices, have been proposed in recent years. RESCAL (Nickel et al., 2011) is one of the earliest studies on matrix factorization based knowledge graph embedding models, using a bilinear form as score function. T RANS E (Bordes et al., 2013) is the first model to introduce translation-based embedding. Later variants, such as T RANS H (Wang et al., 2014), T RANS R (Lin et al., 2015) and T RANS D (Ji et al., 2015), extend T RANS E by projecting the embedding vectors of entities into various spaces. D IST M ULT (Yang et al., 2015) simplifies RESCAL by only using a diagonal matrix, and C OMPL E X (Trouillon et al., 2016) extends D IST M ULT into the complex number field. (Nickel et al., 2015) is a comprehensive survey on these models. Some of the more recent models achieve strong performances. M ANIFOLD E (Xiao et al., 2016) embeds a triple as a manifold rather than a point. H OL E (Nickel et al., 2016) employs circular correlation to combine the two entities in a triple. C ONV E (Dettmers et al., 2017)"
N18-1133,W15-4007,0,0.0296494,"the link prediction task with different generators and discriminators. For the generator, we choose two classical probability-based KGE model, D IST M ULT and C OMPL E X, and for the discriminator, we also choose two classical translation-based KGE model, T RANS E and T RANS D, resulting in four possible combinations of generator and discriminator in total. See Table 1 for a brief summary of these models. 4.1 4.1.1 Experimental Settings Datasets We use three common knowledge base completion datasets for our experiment: FB15k-237, WN18 and WN18RR. FB15k-237 is a subset of FB15k introduced by (Toutanova and Chen, 2015), which removed redundant relations in FB15k and greatly reduced the number of relations. Likewise, WN18RR is a subset of WN18 introduced by (Dettmers et al., 2017) which removes reversing relations and dramatically increases the difficulty of reasoning. Both FB15k and WN18 are first introduced by (Bordes et al., 2013) and have been commonly used in knowledge graph researches. Statistics of datasets we used are shown in Table 3. 4.1.2 Evaluation Protocols Following previous works like (Yang et al., 2015) and (Trouillon et al., 2016), for each run, we report two common metrics, mean reciprocal"
N18-1165,D13-1080,0,0.0250084,"Missing"
N18-1165,D14-1044,0,0.435869,"udy in this large KG multi-hop reasoning scenario, where the goal is to design an automated inference model to complete the missing links between existing entities in large KGs. For examples, if the KG contains a fact like president(BarackObama, USA) and spouse(Michelle, BarackObama), then we would like the machines to complete the missing link livesIn(Michelle, USA) automatically. Systems for this task are essential to complex question answering applications. To tackle the multi-hop link prediction problem, various approaches have been proposed. Some earlier works like PRA (Lao et al., 2011; Gardner et al., 2014, 2013) use bounded-depth random walk with restarts to obtain paths. More recently, DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2018), frame the path-finding problem as a Markov Decision Process (MDP) and utilize reinforcement learning (RL) to maximize the expected return. Another line of work along with ours are Chain-of-Reasoning (Das et al., 2016) and Compositional Reasoning (Neelakantan et al., 2015), which take multi-hop chains learned by PRA as input and aim to infer its relation. Here we frame the KG reasoning task as a two sub-steps, i.e. “Path-Finding” and “PathReasoning”."
N18-1165,P17-1097,0,0.0351239,"chieve state-of-the-art results, however, these two models both use heuristic rewards to drive the policy 1824 search, which could make their models sensitive to noises and adversarial examples. 2.3 Variational Auto-encoder Variational Auto-Encoder (Kingma and Welling, 2013) is a very popular algorithm to perform approximate posterior inference in large-scale scenarios, especially in neural networks. Recently, VAE has been successfully applied to various complex machine learning tasks like image generation (Mansimov et al., 2015), machine translation (Zhang et al., 2016), sentence generation (Guu et al., 2017a) and question answering (Zhang et al., 2017). Zhang et al. (2017) is closest to ours, this paper proposes a variational framework to understand the variability of human language about entity referencing. In contrast, our model uses a variational framework to cope with the complex link connections in large KG. Unlike the previous research in VAE, both Zhang et al. (2017) and our model uses discrete variables as the latent representation to infer the semantics of given entity pairs. More specifically, we view the generation of relation as a stochastic process controlled by a latent representat"
N18-1165,P15-1067,0,0.0354605,", the convolution kernel feature size D to 128, we set the hidden size of MLP for both path finder and path reasoner to 400. 1828 2 https://github.com/xwhan/DeepPath Dataset FB15k-237 NELL-995 #Ent 14,505 75,492 #R 237 200 #Triples 310,116 154,213 #Tasks 20 12 Model PRA (Lao et al., 2011) TransE (Bordes et al., 2013) TransR (Lin et al., 2015) MINERVA (Das et al., 2018) DeepPath (Xiong et al., 2017) RNN-Chain (Das et al., 2016) CNN Path-Reasoner MML (Guu et al., 2017b) D IVA Table 1: Dataset statistics. Model RPA (Lao et al., 2011) TransE (Bordes et al., 2013) TransR (Lin et al., 2015) TransD (Ji et al., 2015) TransH (Wang et al., 2014) MINERVA (Das et al., 2018) DeepPath (Xiong et al., 2017) RNN-Chain (Das et al., 2016) CNN Path-Reasoner D IVA 12-rel MAP 67.5 75.0 74.0 77.3 75.1 79.6 79.0 82.0 88.6 9-rel MAP 88.2 80.2 80.2 82.2 87.9 Table 2: MAP results on the NELL dataset. Since MINERVA (Das et al., 2018) only takes 9 relations out of the original 12 relations, we report the known results for both version of NELL-995 dataset. 4.2 Quantitative Results We mainly compare with the embedding-based algorithms (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015; Wang et al., 2014), PRA (Lao et al.,"
N18-1165,D11-1049,0,0.720731,"we situate our study in this large KG multi-hop reasoning scenario, where the goal is to design an automated inference model to complete the missing links between existing entities in large KGs. For examples, if the KG contains a fact like president(BarackObama, USA) and spouse(Michelle, BarackObama), then we would like the machines to complete the missing link livesIn(Michelle, USA) automatically. Systems for this task are essential to complex question answering applications. To tackle the multi-hop link prediction problem, various approaches have been proposed. Some earlier works like PRA (Lao et al., 2011; Gardner et al., 2014, 2013) use bounded-depth random walk with restarts to obtain paths. More recently, DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2018), frame the path-finding problem as a Markov Decision Process (MDP) and utilize reinforcement learning (RL) to maximize the expected return. Another line of work along with ours are Chain-of-Reasoning (Das et al., 2016) and Compositional Reasoning (Neelakantan et al., 2015), which take multi-hop chains learned by PRA as input and aim to infer its relation. Here we frame the KG reasoning task as a two sub-steps, i.e. “Path-Finding”"
N18-1165,P15-1016,0,0.671155,"sential to complex question answering applications. To tackle the multi-hop link prediction problem, various approaches have been proposed. Some earlier works like PRA (Lao et al., 2011; Gardner et al., 2014, 2013) use bounded-depth random walk with restarts to obtain paths. More recently, DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2018), frame the path-finding problem as a Markov Decision Process (MDP) and utilize reinforcement learning (RL) to maximize the expected return. Another line of work along with ours are Chain-of-Reasoning (Das et al., 2016) and Compositional Reasoning (Neelakantan et al., 2015), which take multi-hop chains learned by PRA as input and aim to infer its relation. Here we frame the KG reasoning task as a two sub-steps, i.e. “Path-Finding” and “PathReasoning”. We found that most of the related research is only focused on one step, which leads to major drawbacks—lack of interactions between these two steps. More specifically, DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2018) can be interpreted as enhancing the “Path-Finding” step while compositional reasoning (Neelakantan et al., 2015) and chains of rea1823 Proceedings of NAACL-HLT 2018, pages 1823–1832 c New O"
N18-1165,D15-1174,0,0.827132,"des, since all these models operate solely on latent space, their predictions are barely interpretable. 2.2 Multi-Hop Reasoning The Path-Ranking Algorithm (PRA) method is the first approach to use a random walk with restart mechanism to perform multi-hop reasoning. Later on, some research studies (Gardner et al., 2014, 2013) have revised the PRA algorithm to compute feature similarity in the vector space. These formula-based algorithms can create a large fan-out area, which potentially undermines the inference accuracy. To mitigate this problem, a Convolutional Neural Network(CNN)based model (Toutanova et al., 2015) has been proposed to perform multi-hop reasoning. Recently, DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2018) view the multi-hop reasoning problem as a Markov Decision Process, and leverages REINFORCE (Williams, 1992) to efficiently search for paths in large knowledge graph. These two methods are reported to achieve state-of-the-art results, however, these two models both use heuristic rewards to drive the policy 1824 search, which could make their models sensitive to noises and adversarial examples. 2.3 Variational Auto-encoder Variational Auto-Encoder (Kingma and Welling, 2013) i"
N18-1165,D17-1060,1,0.443441,"plete the missing links between existing entities in large KGs. For examples, if the KG contains a fact like president(BarackObama, USA) and spouse(Michelle, BarackObama), then we would like the machines to complete the missing link livesIn(Michelle, USA) automatically. Systems for this task are essential to complex question answering applications. To tackle the multi-hop link prediction problem, various approaches have been proposed. Some earlier works like PRA (Lao et al., 2011; Gardner et al., 2014, 2013) use bounded-depth random walk with restarts to obtain paths. More recently, DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2018), frame the path-finding problem as a Markov Decision Process (MDP) and utilize reinforcement learning (RL) to maximize the expected return. Another line of work along with ours are Chain-of-Reasoning (Das et al., 2016) and Compositional Reasoning (Neelakantan et al., 2015), which take multi-hop chains learned by PRA as input and aim to infer its relation. Here we frame the KG reasoning task as a two sub-steps, i.e. “Path-Finding” and “PathReasoning”. We found that most of the related research is only focused on one step, which leads to major drawbacks—lack of in"
N18-1165,D16-1050,0,0.0270066,"graph. These two methods are reported to achieve state-of-the-art results, however, these two models both use heuristic rewards to drive the policy 1824 search, which could make their models sensitive to noises and adversarial examples. 2.3 Variational Auto-encoder Variational Auto-Encoder (Kingma and Welling, 2013) is a very popular algorithm to perform approximate posterior inference in large-scale scenarios, especially in neural networks. Recently, VAE has been successfully applied to various complex machine learning tasks like image generation (Mansimov et al., 2015), machine translation (Zhang et al., 2016), sentence generation (Guu et al., 2017a) and question answering (Zhang et al., 2017). Zhang et al. (2017) is closest to ours, this paper proposes a variational framework to understand the variability of human language about entity referencing. In contrast, our model uses a variational framework to cope with the complex link connections in large KG. Unlike the previous research in VAE, both Zhang et al. (2017) and our model uses discrete variables as the latent representation to infer the semantics of given entity pairs. More specifically, we view the generation of relation as a stochastic pro"
N18-2019,D14-1181,0,0.0114147,"Missing"
N18-2019,R15-1086,0,0.135164,"Missing"
N18-2019,W12-2103,0,0.423334,"performing Introduction The rapid rise in user-generated web content has not only yielded a vast increase in information accessibility, but has also given individuals an easy platform on which to share their beliefs and to publicly communicate with others. Unfortunately, this has also led to nefarious uses of online spaces, for instance for the propagation of hate speech. An extensive body of work has focused on the development of automatic hate speech classifiers. A recent survey outlined eight categories of features used in hate speech detection (Schmidt and Wiegand, 2017): simple surface (Warner and Hirschberg, 2012; Waseem and Hovy, 2016), word generalization (Warner and 118 Proceedings of NAACL-HLT 2018, pages 118–123 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 2: The overview of our proposed model. t is the input target Tweet, z denotes intra-user Tweets, and xa is the selected inter-user Tweet. rie is the inter-user representation, ria is the intra-user representation, and rta is the representation of the target Tweet. These three branches respectively correspond to the three branches illustrated in Figure 1. yi is the prediction at the time step"
N18-2019,N16-2013,0,0.295014,"rapid rise in user-generated web content has not only yielded a vast increase in information accessibility, but has also given individuals an easy platform on which to share their beliefs and to publicly communicate with others. Unfortunately, this has also led to nefarious uses of online spaces, for instance for the propagation of hate speech. An extensive body of work has focused on the development of automatic hate speech classifiers. A recent survey outlined eight categories of features used in hate speech detection (Schmidt and Wiegand, 2017): simple surface (Warner and Hirschberg, 2012; Waseem and Hovy, 2016), word generalization (Warner and 118 Proceedings of NAACL-HLT 2018, pages 118–123 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 2: The overview of our proposed model. t is the input target Tweet, z denotes intra-user Tweets, and xa is the selected inter-user Tweet. rie is the inter-user representation, ria is the intra-user representation, and rta is the representation of the target Tweet. These three branches respectively correspond to the three branches illustrated in Figure 1. yi is the prediction at the time step i and si is the state in"
N18-2019,P16-2034,0,0.0225319,"network (LSTM) (Hochreiter and Schmidhuber, 1997) to interactively leverage the similar Tweets from a large Twitter dataset to enhance the performance of the hate speech classifier. An overview of our approach is shown in Figure 1. The main contributions of our work are: 2 Approach Figure 2 illustrates the architecture of our model. It includes three branches, whose details will be described in the following subsections. 2.1 Bidirectional LSTM Given a target Tweet, the baseline approach is to feed the embeddings of the Tweet into a bidirectional LSTM network (Hochreiter and Schmidhuber, 1997; Zhou et al., 2016; Liu et al., 2016) to obtain the prediction. This is shown in the middle branch in Figure 1. However, this method is likely to fail when the target tweet is noisy or the critical words for making predictions are out of vocabulary. 2.2 • We provide a novel perspective on hate speech detection by modeling intra-user Tweet representations. Intra-User Representation The baseline approach does not fully utilize available information, such as the user’s historical Tweets. In our approach, we collect the user’s historical posts through the Twitter API. For a target Tweet t, suppose we collect m Twee"
N18-2019,W17-1101,0,\N,Missing
N18-2125,D17-1103,0,0.0379626,"Missing"
N18-2125,N15-1173,0,0.21422,"ecoder framework comprising multiple hierarchical recurrent neural networks (see Figure 2). Specifically, in the encoding stage, the model has one hierarchical attentive encoder for each input modality, which learns and outputs both the local and global representations of the modality. (In this paper, visual and audio features are used as the input and hence there are two hierarchical attentive encoders as shown in Figure 2; it should be noted, however, that the model seamlessly extends to more than two input modalities.) Among the network architectures for video captioning (Yao et al., 2015; Venugopalan et al., 2015b), sequence-to-sequence models (Venugopalan et al., 2015a) have shown promising results. Pan et al. Pan et al. (2016) introduced a hierarchical recurrent encoder to capture the temporal visual features at different levels. Yu et al. (2016) proposed a hierarchical decoder for paragraph generation, and most recently Wang et al. (2018) invented a hierarchical reinforced framework to generate the caption phrase by phrase. But none had tried to model and align the global In the decoding stage, we employ two crossmodal attentive decoders: the local decoder and the global decoder. The global decoder"
N18-6003,H05-1066,0,0.174527,"Missing"
N18-6003,W02-2020,0,0.0398699,"Missing"
N18-6003,P09-1113,0,0.38883,"Missing"
N18-6003,W14-1611,0,0.0475727,"Missing"
N18-6003,D15-1064,1,0.829291,"d TransE (Bordes et al., 2013). Finally, we discuss DeepPath (Xiong et al., 2017), a novel deep reinforcement learning model that combines the embedding and path-based approaches for the learning to reason problem. Research Impact. Our phrase mining tool, SegPhrase (Liu et al., 2015), won the grand prize of Yelp Dataset Challenge1 and was used by TripAdvisor in productions2 . Our entity recognition and typing system, ClusType (Ren et al., 2015), was shipped as part of the products in Microsoft Bing and U.S. Army Research Lab. We built the first named entity recognizer on Chinese social media (Peng and Dredze, 2015, 2016) and closed the gap between NER on English and Chinese social media. The same technique was applied to build the first relation extractor for cross-sentence, n-ary relation extraction between drug, gene, and mutation (Peng et al., 2017). Duration and Sessions. The duration of the tutorial is flexible: It is expected to be 3 hours, but it can be extended into 6 hours, based on the need of the conference. The outline presented here is for the 3-hour tutorial. For longer duration of the tutorial, we plan to extend entity and relation extraction parts, and add in more case studies and appli"
N18-6003,P11-1055,0,0.138239,"Missing"
N18-6003,P10-1029,0,0.0521991,"Missing"
N18-6003,Q17-1008,1,0.820066,"ing tool, SegPhrase (Liu et al., 2015), won the grand prize of Yelp Dataset Challenge1 and was used by TripAdvisor in productions2 . Our entity recognition and typing system, ClusType (Ren et al., 2015), was shipped as part of the products in Microsoft Bing and U.S. Army Research Lab. We built the first named entity recognizer on Chinese social media (Peng and Dredze, 2015, 2016) and closed the gap between NER on English and Chinese social media. The same technique was applied to build the first relation extractor for cross-sentence, n-ary relation extraction between drug, gene, and mutation (Peng et al., 2017). Duration and Sessions. The duration of the tutorial is flexible: It is expected to be 3 hours, but it can be extended into 6 hours, based on the need of the conference. The outline presented here is for the 3-hour tutorial. For longer duration of the tutorial, we plan to extend entity and relation extraction parts, and add in more case studies and applications. Topics to be covered in this tutorial. The first 2/3 of this tutorial presents a comprehensive overview of the information extraction techniques developed in recent years for constructing knowledge bases (see also Section 2 for a more"
N18-6003,P08-1068,0,0.0888921,"Missing"
N18-6003,D11-1049,0,0.0401887,"xt corpora; (2) entity recognition and typing: preliminaries, challenges, and methodologies; and (3) relation extraction: previous efforts, limitations, recent progress, and a joint entity and relation extraction method using distant supervision; (4) multi-task and multi-domain learning for lowresource information extraction; (5) distill linguistic knowledge into neural models to help low-resource information extraction. The second half of the tutorial presents a comprehensive overview of KB reasoning techniques. For path-based methods, we will first describe the Path-Ranking Algorithm (PRA) (Lao et al., 2011) and briefly describe extensions such as ProPPR (Wang et al., 2013). Our tutorial will also cover the recent integration of Relevance to ACL. Machine “reading” and “reasoning” of large text corpora have long been the interests to CL and NLP communities, especially when people now are exposed to an explosion of information in the form of free text. Extracting structured information is key to understanding messy and scattered raw data, and effective reasoning tools are critical for the use of KBs in downstream tasks like QA. This tutorial will present an organized picture of recent research on k"
N18-6003,P14-1038,0,0.029821,"Missing"
N18-6003,W09-1119,0,0.10705,"Missing"
N18-6003,D16-1144,1,0.814067,"Missing"
N18-6003,D17-1060,1,0.829166,"and help knowledge extraction in low-resource settings with minimal supervision. In the reasoning part, we aim to leverage the existing background knowledge and design various algorithms to fill in the missing link between entities in the KB, given the extracted KBs are likely incomplete. More specifically, this part will introduce two lines of research for KB reasoning: path-based and embedding-based methods. PRA with recurrent neural networks. For the embedding based method, we will briefly describe RESCAL (Nickel et al., 2011) and TransE (Bordes et al., 2013). Finally, we discuss DeepPath (Xiong et al., 2017), a novel deep reinforcement learning model that combines the embedding and path-based approaches for the learning to reason problem. Research Impact. Our phrase mining tool, SegPhrase (Liu et al., 2015), won the grand prize of Yelp Dataset Challenge1 and was used by TripAdvisor in productions2 . Our entity recognition and typing system, ClusType (Ren et al., 2015), was shipped as part of the products in Microsoft Bing and U.S. Army Research Lab. We built the first named entity recognizer on Chinese social media (Peng and Dredze, 2015, 2016) and closed the gap between NER on English and Chines"
N18-6003,P00-1015,0,0.0168191,"Missing"
N18-6003,P10-1040,0,0.141393,"Missing"
N18-6003,P05-1053,0,0.080725,"Missing"
N19-1025,J93-2004,0,0.0652785,"1 at the 21st epoch. For vmf-VAE (Xu and Durrett, 2018), we set the word embedding dimension to be 512 and the hidden units to 1024 for Yahoo, and set both of them to 200 for PTB and Yelp. The temperature κ is set to 80 and is kept constant during training. For all WAE models, we add a small KL divergence term to control the posterior distribution. We found that if we only use RNF with MMD as the distance metric, then the posterior may diverge from the prior such that no reasonable samples can be generated from a standard Gaussian variable. Experimental Results Datasets We use Penn Treebank (Marcus et al., 1993), Yelp 13 reviews (Xu et al., 2016), as in (Xu and Durrett, 2018; Bowman et al., 2015), and Yahoo Answers used in (Xu and Durrett, 2018; Yang et al., 2017) to follow and compare with prior studies. We limit the maximum length of a sample from all datasets to 200 words. The datasets statistics is shown in Table 3. 5.2 Dev 3370 7773 10K Table 3: Datasets statistics; The numbers reflect size of each dataset. Vocab is the vocabulary size. In this section, we investigate WAE’s performance with Riemannian Normalizing Flow over language and text modeling. 5.1 Train 42068 62522 100K Experimental Setup"
N19-1025,D17-1066,0,0.497053,"approach that applies VAE to language modeling tasks. They observe that LSTM decoder in VAE often generates texts without making use of latent representations, rendering the learned codes as useless. This phenomenon is caused by an optimization problem called KL-divergence vanishing when training VAE for text data, where the KL-divergence term in VAE objective collapses to zero. This makes the learned representations meaningless as zero KL-divergence indicates that the latent codes are independent of input texts. Many recent studies are proposed to address this key issue. Yang et al. (2017); Semeniuta et al. (2017) use convolutional neural network as decoder architecture to limit the expressiveness of decoder model. Xu and Durrett (2018); Zhao et al. (2017a,b, 2018) seek to learn different latent space and modify the learning objective. And, even though not designed to tackle KL vanishing at the beginning, recent studies on Normalizing Flows (Rezende and Mohamed, 2015; van den Berg et al., 2018) learn meaningful latent space as it helps to transform an over-simplified latent distribution into more flexible distributions. In this paper, we propose a new type of flow, Recurrent Variational Autoencoder has"
N19-1025,P18-1083,1,0.833749,"al., 2017) and SA-VAE (Kim et al., 2018) are not directly comparable with other current approaches. Here, we compare with models that use LSTM as encoder-decoder and have similar time complexity, while the use of CNN as decoder in CNN-VAE would dramatically change the model expressiveness, and it is known that SAVAE’s time complexity (Kim et al., 2018; He et al., 2019) is much higher than all other existing approaches. 5.4 How Good is Riemannian Latent Representation? Mutual information between Z 0 and X One important question is how useful are latent variables. Since no metrics are perfect (Wang et al., 2018), we should not just look at sample perplexity to judge how good a latent code is. Hence, we also investigate how much information can be encoded into latent codes. We believe that the mutual information term I(z; x) is a better metric regarding the usefulness of latent codes than sample 291 the company said it will be sold to the company ’s promotional programs and UNK the company also said it will sell $ n million of soap eggs turning millions of dollars the company said it will be UNK by the company ’s UNK division n the company said it would n’t comment on the suit and its reorganization p"
N19-1025,D16-1172,0,0.170695,"nd Durrett, 2018), we set the word embedding dimension to be 512 and the hidden units to 1024 for Yahoo, and set both of them to 200 for PTB and Yelp. The temperature κ is set to 80 and is kept constant during training. For all WAE models, we add a small KL divergence term to control the posterior distribution. We found that if we only use RNF with MMD as the distance metric, then the posterior may diverge from the prior such that no reasonable samples can be generated from a standard Gaussian variable. Experimental Results Datasets We use Penn Treebank (Marcus et al., 1993), Yelp 13 reviews (Xu et al., 2016), as in (Xu and Durrett, 2018; Bowman et al., 2015), and Yahoo Answers used in (Xu and Durrett, 2018; Yang et al., 2017) to follow and compare with prior studies. We limit the maximum length of a sample from all datasets to 200 words. The datasets statistics is shown in Table 3. 5.2 Dev 3370 7773 10K Table 3: Datasets statistics; The numbers reflect size of each dataset. Vocab is the vocabulary size. In this section, we investigate WAE’s performance with Riemannian Normalizing Flow over language and text modeling. 5.1 Train 42068 62522 100K Experimental Setup For each model, we set the maximum"
N19-1025,D18-1480,0,0.666804,"ng use of latent representations, rendering the learned codes as useless. This phenomenon is caused by an optimization problem called KL-divergence vanishing when training VAE for text data, where the KL-divergence term in VAE objective collapses to zero. This makes the learned representations meaningless as zero KL-divergence indicates that the latent codes are independent of input texts. Many recent studies are proposed to address this key issue. Yang et al. (2017); Semeniuta et al. (2017) use convolutional neural network as decoder architecture to limit the expressiveness of decoder model. Xu and Durrett (2018); Zhao et al. (2017a,b, 2018) seek to learn different latent space and modify the learning objective. And, even though not designed to tackle KL vanishing at the beginning, recent studies on Normalizing Flows (Rezende and Mohamed, 2015; van den Berg et al., 2018) learn meaningful latent space as it helps to transform an over-simplified latent distribution into more flexible distributions. In this paper, we propose a new type of flow, Recurrent Variational Autoencoder has been widely used for language modeling and text generation tasks. These models often face a difficult optimization problem,"
N19-1025,P18-1101,0,0.0747448,"Missing"
N19-1025,P17-1061,0,0.470376,"emannian Normalizing Flow on Variational Wasserstein Autoencoder for Text Modeling William Yang Wang Prince Zizhuang Wang Department of Computer Science Department of Computer Science University of California, Santa Barbara University of California, Santa Barbara william@cs.ucsb.edu zizhuang wang@ucsb.edu Abstract continuous space of latent representations from high-dimensional data input and makes sampling procedure from such latent space very straightforward. Recent studies also show that VAE learns meaningful representations that encode non-trivial information from input (Gao et al., 2018; Zhao et al., 2017a). Applications of VAE in tasks of Natural Language Processing (Bowman et al., 2015; Zhao et al., 2017b, 2018; Miao et al., 2016) is not as successful as those in Computer Vision. With longshort-term-memory network (LSTM) (Hochreiter and Schmidhuber, 1997) used as encoderdecoder model, the recurrent variational autoencoder (Bowman et al., 2015) is the first approach that applies VAE to language modeling tasks. They observe that LSTM decoder in VAE often generates texts without making use of latent representations, rendering the learned codes as useless. This phenomenon is caused by an optimiz"
N19-1084,P16-2022,0,0.0280537,"mentions are actually pronouns, such as “he” or “it”, this kind of mentions alone provide only limited clues about general entity types (e.g., “he” is a “person”) but little information about fine-grained types. In this case, directly appending representation of pronouns does not provide extra useful information for making fine-grained predictions. Thus, instead of using the concatenation operator, we propose to construct a stronger interaction between the mention and context with an attentionbased matching module, which has shown its effectiveness in recent natural language inference models (Mou et al., 2016; Chen et al., 2017). Formally consider the mention representation M ∈ Rhm and context’s hidden feature Ch ∈ 4.2 Methodology In this section, we first briefly introduce the neural architecture to encode raw text inputs. Then we describe the matching module we use to enhance the interaction between the mention span and the context sentence. Finally, we move to the label decoder, on which we impose the label-relational bias with a graph propagation layer that encodes type co-occurrence statistics and word-level similarities. Figure 1 provides a graphical overview of our model, with 1a) illustrat"
N19-1084,P17-1152,0,0.0259247,"lly pronouns, such as “he” or “it”, this kind of mentions alone provide only limited clues about general entity types (e.g., “he” is a “person”) but little information about fine-grained types. In this case, directly appending representation of pronouns does not provide extra useful information for making fine-grained predictions. Thus, instead of using the concatenation operator, we propose to construct a stronger interaction between the mention and context with an attentionbased matching module, which has shown its effectiveness in recent natural language inference models (Mou et al., 2016; Chen et al., 2017). Formally consider the mention representation M ∈ Rhm and context’s hidden feature Ch ∈ 4.2 Methodology In this section, we first briefly introduce the neural architecture to encode raw text inputs. Then we describe the matching module we use to enhance the interaction between the mention span and the context sentence. Finally, we move to the label decoder, on which we impose the label-relational bias with a graph propagation layer that encodes type co-occurrence statistics and word-level similarities. Figure 1 provides a graphical overview of our model, with 1a) illustrating both the text en"
N19-1084,D14-1162,0,0.0859281,"the label decoder, on which we impose the label-relational bias with a graph propagation layer that encodes type co-occurrence statistics and word-level similarities. Figure 1 provides a graphical overview of our model, with 1a) illustrating both the text encoders and the matching module, and 1b) showing an example of graph propagation. 4.1 Representation Model Our base model to encode the context and the mention span follows existing neural approaches (Shimaoka et al., 2016; Xu and Barbosa, 2018; Choi et al., 2018). To encode the context, we first apply a standard Bi-LSTM, which takes GloVe (Pennington et al., 2014) embeddings and position embeddings (three vectors representing positions before, inside or after the mention span) as inputs and outputs the hidden states at each time step t ∈ [1, lc ]. With the derived hidden states Mention-Context Interaction 2 Please refer to (Shimaoka et al., 2017) and (Choi et al., 2018) for more detailed descriptions. 775 Rlc ×hc , where lc indicates the number of tokens in the context sentence and hm , hc denote feature dimensions. We first project the mention feature M into the same dimension space as Ch with a linear layer (W1 ∈ Rhm ×hc ) and a tanh function3 : mpro"
N19-1084,P18-1009,0,0.199154,"d in entity typing datasets. For benchmarks annotated with the knowledge base (KB) guided distant supervision, this assumption is often valid since all types are from KB ontologies and naturally follow tree-like structures. However, since knowledge bases are inherently incomplete (Min et al., 2013), existing KBs only include a limited set of entity types. Thus, models trained on these datasets fail to generalize to lots of unseen types. In this work, we investigate entity typing in a more open scenario where the type set is not restricted by KB schema and includes over 10,000 free-form types (Choi et al., 2018). As most of the types do not follow any predefined structures, methods that explicitly incorporate type hierarchies cannot be straightforwardly applied here. To effectively capture the underlying label correlations without access to known type structures, we propose a novel label-relational inductive bias, represented by a graph propagation layer that operates in the latent label space. Specifically, this layer learns to incorporate a label affinity matrix derived from global type co-occurrence statistics and word-level type similarities. It can be seamlessly coupled with existing models and"
N19-1084,P17-2052,0,0.160724,"imple cross-entropy loss. Modeling Entity Type Correlations To better capture the underlying label correlations, Shimaoka et al. (2017) employed a hierarchical label encoding method and AFET (Ren et al., 2016a) used the predefined label hierarchy to identify noisy annotations and proposed a partial-label loss to reduce such noise. A recent work (Xu and Barbosa, 2018) proposed hierarchical loss normalization which alleviated the noise of too specific types. Our work differs from these works in that we do not rely on known label structures and aim to learn the underlying correlations from data. Rabinovich and Klein (2017) recently proposed a structure-prediction approach which used type correlation features. The inference on their learned factor graph is approximated by a greedy decoding algorithm, which outperformed unstructured methods on their own dataset. Instead of using an explicit graphical model, we enforce a relational bias on model parameters, which does not introduce extra burden on label decoding. • We impose an effective label-relational bias on entity typing models with an easy-toimplement graph propagation layer, which allows the model to implicitly capture type dependencies; • We augment our gr"
N19-1084,Q14-1037,0,0.0487843,"ierarchy-aware loss functions (Ren et al., 2016b; Xu and Barbosa, 2018) or enhanced type label encodings (Shimaoka et al., 2017) that enable parameter sharing between related types. These methods rely on the assumpIntroduction Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution (Durrett and Klein, 2014), relation extraction (Yaghoobzadeh et al., 2016) and question answering (Lee et al., 2006; Yavuz et al., 2016). 1 https://github.com/xwhan/ Extremely-Fine-Grained-Entity-Typing 773 Proceedings of NAACL-HLT 2019, pages 773–784 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics • Empirically, our model is able to offer significant improvements over previous models on the Ultra-Fine dataset and also reduces the cases of inconsistent type predictions. tion that the underlying type structures are predefined in entity typing datasets. For benchmarks anno"
N19-1084,D16-1144,0,0.588196,"is identified as a “criminal”, then the entity must also be a “person”, but it is less likely for this entity to be a “police officer” at the same time. When ignoring such correlations and considering each type separately, models are often inferior in performance and prone to inconsistent predictions. As shown in Table 1, an existing model that independently predicts different types fails to reject predictions that include apparent contradictions. Existing entity typing research often address this aspect by explicitly utilizing a given type hierarchy to design hierarchy-aware loss functions (Ren et al., 2016b; Xu and Barbosa, 2018) or enhanced type label encodings (Shimaoka et al., 2017) that enable parameter sharing between related types. These methods rely on the assumpIntroduction Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution (Durrett and Klein, 2014), relation extraction ("
N19-1084,W16-1313,0,0.602463,"l., 2009) for entity typing and created one of the early large-scale datasets. Although DS provides an efficient way to annotate training data, later work (Gillick et al., 2014) pointed out that entity type labels induced by DS ignore entities’ local context and may have limited usage in contextaware applications. Most of the following research has since focused on testing in context-dependent scenarios. While early methods (Gillick et al., 2014; Yogatama et al., 2015) on this task rely on well-designed loss functions and a suite of handcraft features that represent both context and entities, Shimaoka et al. (2016) proposed the first attentive neural model which outperformed featurebased methods with a simple cross-entropy loss. Modeling Entity Type Correlations To better capture the underlying label correlations, Shimaoka et al. (2017) employed a hierarchical label encoding method and AFET (Ren et al., 2016a) used the predefined label hierarchy to identify noisy annotations and proposed a partial-label loss to reduce such noise. A recent work (Xu and Barbosa, 2018) proposed hierarchical loss normalization which alleviated the noise of too specific types. Our work differs from these works in that we do"
N19-1084,E17-1119,0,0.698182,"t it is less likely for this entity to be a “police officer” at the same time. When ignoring such correlations and considering each type separately, models are often inferior in performance and prone to inconsistent predictions. As shown in Table 1, an existing model that independently predicts different types fails to reject predictions that include apparent contradictions. Existing entity typing research often address this aspect by explicitly utilizing a given type hierarchy to design hierarchy-aware loss functions (Ren et al., 2016b; Xu and Barbosa, 2018) or enhanced type label encodings (Shimaoka et al., 2017) that enable parameter sharing between related types. These methods rely on the assumpIntroduction Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution (Durrett and Klein, 2014), relation extraction (Yaghoobzadeh et al., 2016) and question answering (Lee et al., 2006; Yavuz et al."
N19-1084,N18-1002,0,0.709,"“criminal”, then the entity must also be a “person”, but it is less likely for this entity to be a “police officer” at the same time. When ignoring such correlations and considering each type separately, models are often inferior in performance and prone to inconsistent predictions. As shown in Table 1, an existing model that independently predicts different types fails to reject predictions that include apparent contradictions. Existing entity typing research often address this aspect by explicitly utilizing a given type hierarchy to design hierarchy-aware loss functions (Ren et al., 2016b; Xu and Barbosa, 2018) or enhanced type label encodings (Shimaoka et al., 2017) that enable parameter sharing between related types. These methods rely on the assumpIntroduction Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution (Durrett and Klein, 2014), relation extraction (Yaghoobzadeh et al., 201"
N19-1084,D16-1015,0,0.134564,"t al., 2017) that enable parameter sharing between related types. These methods rely on the assumpIntroduction Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution (Durrett and Klein, 2014), relation extraction (Yaghoobzadeh et al., 2016) and question answering (Lee et al., 2006; Yavuz et al., 2016). 1 https://github.com/xwhan/ Extremely-Fine-Grained-Entity-Typing 773 Proceedings of NAACL-HLT 2019, pages 773–784 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics • Empirically, our model is able to offer significant improvements over previous models on the Ultra-Fine dataset and also reduces the cases of inconsistent type predictions. tion that the underlying type structures are predefined in entity typing datasets. For benchmarks annotated with the knowledge base (KB) guided distant supervision, this assumption is often valid since all types a"
N19-1084,P15-2048,0,0.101731,"grained entity typing was first thoroughly investigated in (Ling and Weld, 2012), which utilized Freebase-guided distant supervision (DS) (Mintz et al., 2009) for entity typing and created one of the early large-scale datasets. Although DS provides an efficient way to annotate training data, later work (Gillick et al., 2014) pointed out that entity type labels induced by DS ignore entities’ local context and may have limited usage in contextaware applications. Most of the following research has since focused on testing in context-dependent scenarios. While early methods (Gillick et al., 2014; Yogatama et al., 2015) on this task rely on well-designed loss functions and a suite of handcraft features that represent both context and entities, Shimaoka et al. (2016) proposed the first attentive neural model which outperformed featurebased methods with a simple cross-entropy loss. Modeling Entity Type Correlations To better capture the underlying label correlations, Shimaoka et al. (2017) employed a hierarchical label encoding method and AFET (Ren et al., 2016a) used the predefined label hierarchy to identify noisy annotations and proposed a partial-label loss to reduce such noise. A recent work (Xu and Barbo"
N19-1084,P09-1113,0,\N,Missing
N19-1084,N13-1095,0,\N,Missing
N19-1086,D18-1514,0,0.421659,"from previous tasks as anchor points and minimizes the distortion of the anchor points in the embedding space in the lifelong relation extraction. The aligned embedding space is then utilized for relation extraction. Experiment results show that our method outperforms the state-of-the-art significantly in accuracy while remaining efficient. The main contributions of this work include: • We introcduce the lifelong relation detection problem and construct lifelong relation detection benchmarks from two datasets with large relation vocabularies: SimpleQuestions (Bordes et al., 2015) and FewRel (Han et al., 2018). • We propose a simple memory replay approach and find that current popular methods such as EWC and GEM underperform this method. • We propose an alignment model which aims to alleviate the catastrophic forgetting problem by slowing down the fast changes in the embedding space for lifelong learning. 2 To make f perform well on the previous tasks, during the lifelong learning process, we usually allow the learner to maintain and observe a memory M of samples from the previous tasks. Practically, with the growth of the number of tasks, it is difficult to store all the task data1 . Therefore, in"
N19-1086,P82-1020,0,0.719966,"Missing"
N19-1086,N15-2018,0,0.0711611,"Missing"
N19-1086,P15-2123,0,0.126278,"Missing"
N19-1086,D14-1162,0,0.110347,"recent works such as GEM ignore this step and randomly select samples from each task to store in the memory. ka(f (x)) − a(k−1) (f (x))k2 (k) (x,y)∈Dtrain + X Selective Storing Samples in Memory ka(f (x)) − a(k−1) (f (k−1) (x))k2 (k) (x,y)∈Dreplay Embedding Alignment on Relation Detection Model We introduce how to add embedding alignment to relation detection models. The basic model we use is a ranking model that is similar to HR-BiLSTM (Yu et al., 2017). Two BiLSTMs (Hochreiter and Schmidhuber, 1997) are used to encode the sentence and relation respectively given their GloVe word embedding (Pennington et al., 2014). Cosine similarity between the sentence and relation embedding is computed as the score. Relation with maximum score is predicted by the model for the sentence. Ranking loss is used to train the model6 . This base model is our model f , which is trained on a new task k at each step and results in an updated model f (k) . Our proposed approach (Figure 1) inserts an alignment model a to explicitly align to embedding space for stored instances and maintain the embedding space of the current task. Note that the label y (the relation here) also has embedding, so it needs to pass through the alignm"
N19-1086,P15-1128,0,0.0253822,"alignment model to mitigate the sentence embedding distortion of the learned model when training on new data and new relations. Experiment results on multiple benchmarks show that our proposed method significantly outperforms the state-of-the-art lifelong learning approaches. 1 Introduction The task of relation detection/extraction aims to recognize entity pairs’ relationship from given contexts. As an essential component for structured information extraction, it has been widely used in downstream tasks such as automatic knowledgebased completion (Riedel et al., 2013) and question answering (Yih et al., 2015; Yu et al., 2017). Existing relation detection methods always assume a closed set of relations and perform once∗ Co-mentoring Code and dataset can be found in this repository: https://github.com/hongwang600/Lifelong_ Relation_Detection 796 Proceedings of NAACL-HLT 2019, pages 796–806 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics long learning is to learn a classification model f . At each step k, f observes the task T (k) , and optimizes the loss function on its training data with a loss function `(f (x), y). At the same time, we require the m"
N19-1086,N13-1008,0,0.0685422,"g space. Specifically, we utilize an explicit alignment model to mitigate the sentence embedding distortion of the learned model when training on new data and new relations. Experiment results on multiple benchmarks show that our proposed method significantly outperforms the state-of-the-art lifelong learning approaches. 1 Introduction The task of relation detection/extraction aims to recognize entity pairs’ relationship from given contexts. As an essential component for structured information extraction, it has been widely used in downstream tasks such as automatic knowledgebased completion (Riedel et al., 2013) and question answering (Yih et al., 2015; Yu et al., 2017). Existing relation detection methods always assume a closed set of relations and perform once∗ Co-mentoring Code and dataset can be found in this repository: https://github.com/hongwang600/Lifelong_ Relation_Detection 796 Proceedings of NAACL-HLT 2019, pages 796–806 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics long learning is to learn a classification model f . At each step k, f observes the task T (k) , and optimizes the loss function on its training data with a loss function `(f (x"
N19-1086,D16-1022,0,0.167323,"Missing"
N19-1086,P17-2023,0,0.0985245,"Missing"
N19-1120,D18-1549,0,0.627649,"014b; Sutskever et al., 2014). But recent studies (Koehn and Knowles, 2017; Isabelle et al., 2017; Sennrich, 2017) highlight the overreliance of current NMT systems on large parallel corpora. In real-world cases, the majority of language pairs have very little parallel data, so the models need to leverage monolingual data to address this challenge (Gulcehre et al., 2015; Zhang and Zong, 2016; He et al., 2016; Yang et al., 2018). While many studies have explored how to use the monolingual data to improve translation performance with limited supervision, latest approaches (Lample et al., 2018a; Artetxe et al., 2018; Lample et al., 2018b) focus on the fully unsupervised scenario. Back-translation has been dominantly used in these approaches, where pseudo sentence pairs are generated to train the translation systems with a reconstruction loss. However, it is inefficient because the generated pseudo sentence pairs are usually of low quality. During the dual learning of back-translation, the errors could easily accumulate and thus the learned target language distribution would gradually deviate from the real target distribution. This critical drawback hinders the further development of the unsupervised NMT"
N19-1120,Q17-1010,0,0.0348175,"l dataset. The translation results are evaluated on newstest 2014 for en-fr, and newstest 2016 for en-de, en-ro and en-ru. 4.2 Implementation Details We follow previous methods (Koehn et al., 2007; Lample et al., 2018b) to initialize our models. Initialization We use Moses scripts (Koehn et al., 2007) for tokenization. While the system requires cross-lingual BPE embeddings to initialize the shared lookup table for related languages, we set the number of BPE codes as 60, 000. Following the previous preprocessing protocol (Lample et al., 2018b), the embeddings are then generated using fastText (Bojanowski et al., 2017) with an embedding dimension of 512, a context window of size 5 and 10 negative samples. Model Structure In this work, the NMT models can be built upon long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) cells. For LSTM cells, both the encoder and decoder have 3 layers. As for Transformer, we use 4 layers both in the encoder and the decoder. As for both LSTM and Transformer, all encoder parameters are shared across two languages. Similarly, we share all decoder parameters across two languages. Both two model structure are optimized using Adam"
N19-1120,P18-1163,0,0.0253643,"e target language space to guide the translation procedure. This can also be viewed as adding constraints when aligning the two language spaces. 3.3 Evaluate Given a source sentence s, we can translate it as t∗ using the source-to-target translation model Ps→t . Meanwhile, a set M 0 of k sentences can also be generated by the extract-edit approach described above. Although the M 0 may contain potential parallel sentences t0 for s, we cannot directly use (s, t0 ) as ground-truth sentence pairs to train the translation model Vs→t because the NMT system is sensitive to noises (Cho et al., 2014a; Cheng et al., 2018). The rough operation like this will result in sub-optimal translation performance. Therefore, in order to assess the quality of the translated sentence t∗ and train the translation model Vs→t , we introduce an evaluation network R for evaluating the relative similarities between the source and target sentences among all sentence pairs. The evaluation network R is a multilayer perceptron; it takes the target sentence embedding et and source sentence embedding es as inputs, and converts them into the joint embedding space as rt and rs . So the similarity rt · rs α(t|s) = cosine(rt , rs ) = . (4"
N19-1120,W14-4012,0,0.127273,"Missing"
N19-1120,D14-1179,0,0.0355587,"Missing"
N19-1120,Q18-1031,0,0.14848,"semantics of the source sentence s. As described in Figure 2 (b), we employ a maxpooling layer to reserve the more significant features between the source sentence embedding es and the extracted sentence embedding et (t ∈ M ), and then decode it into a new sentence t0 : M 0 = {t0 |t0 = dec(maxpooling(es , et )), t ∈ M }, (3) where M 0 is the set of the extracted-and-edited sentences. Based on the semantic information of the source sentence s, we can further improve the extracted results with this editing mechanism. Unlike other studies using the editing to generate more structural sentences (Guu et al., 2018; Hashimoto et al., 2018), here the revised sentences are designed to serve as better pivotal points in the target language space to guide the translation procedure. This can also be viewed as adding constraints when aligning the two language spaces. 3.3 Evaluate Given a source sentence s, we can translate it as t∗ using the source-to-target translation model Ps→t . Meanwhile, a set M 0 of k sentences can also be generated by the extract-edit approach described above. Although the M 0 may contain potential parallel sentences t0 for s, we cannot directly use (s, t0 ) as ground-truth sentence pa"
N19-1120,L16-1468,0,0.25759,"translation systems with a reconstruction loss. However, it is inefficient because the generated pseudo sentence pairs are usually of low quality. During the dual learning of back-translation, the errors could easily accumulate and thus the learned target language distribution would gradually deviate from the real target distribution. This critical drawback hinders the further development of the unsupervised NMT systems. An alternative solution is to extract real parallel sentences from comparable monolingual corpora, and then use them to train the NMT systems. Recently, neural-based methods (Chu et al., 2016; Grover and Mitra, 2017; Gr´egoire and Langlais, 2018) aim to select potential parallel sentences 1173 Proceedings of NAACL-HLT 2019, pages 1173–1183 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics from monolingual corpora in the same domain. However, these neural models need to be trained on a large parallel dataset first, which is not applicable to language pairs with limited supervision. In this paper, we propose a radically different approach for unsupervised NMT—extract-edit, a powerful alternative to back-translation (see Figure 1). Specif"
N19-1120,W11-1209,0,0.0255742,"ing, and back-translation. However, all these unsupervised models are based on the back-translation learning framework to generate pseudo language pairs for training. Our work leverages the information from real target language sentences. Comparable Corpora Mining Comparable corpora mining aims at extracting parallel sentences from comparable monolingual corpora such as news stories written on the same topic in different languages. Most of the previous methods align the documents based on metadata and then extract parallel sentences using humandefined features (Munteanu and Marcu, 2002, 2006; Hewavitharana and Vogel, 2011). Recent neural-based methods (Chu et al., 2016; Grover and Mitra, 2017; Gr´egoire and Langlais, 2018) learn to identify parallel sentences in the semantic spaces. However, these methods require large amounts of parallel sentence pairs to train the systems first and then test the performance on raw comparable corpora, which does not apply to languages with limited resources. Instead, we explore the corpora mining in an unsupervised fashion and propose a joint training framework with machine translation. Retrieval-Augmented Text Generation Our work is also related to the recent work on applying"
N19-1120,C18-1122,0,0.0298041,"Missing"
N19-1120,P17-3003,0,0.102956,"s with a reconstruction loss. However, it is inefficient because the generated pseudo sentence pairs are usually of low quality. During the dual learning of back-translation, the errors could easily accumulate and thus the learned target language distribution would gradually deviate from the real target distribution. This critical drawback hinders the further development of the unsupervised NMT systems. An alternative solution is to extract real parallel sentences from comparable monolingual corpora, and then use them to train the NMT systems. Recently, neural-based methods (Chu et al., 2016; Grover and Mitra, 2017; Gr´egoire and Langlais, 2018) aim to select potential parallel sentences 1173 Proceedings of NAACL-HLT 2019, pages 1173–1183 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics from monolingual corpora in the same domain. However, these neural models need to be trained on a large parallel dataset first, which is not applicable to language pairs with limited supervision. In this paper, we propose a radically different approach for unsupervised NMT—extract-edit, a powerful alternative to back-translation (see Figure 1). Specifically, to train the sou"
N19-1120,D17-1263,0,0.027188,"Missing"
N19-1120,2005.mtsummit-papers.11,0,0.223486,"Missing"
N19-1120,P07-2045,0,0.012764,"evaluated with BLEU metric on newstest 2014 for en↔frand newstest 2016 for en↔de, en↔ro and en↔ru. The (+) and (−) stand for performance gains and loss separately compared with baseline models with the same NMT cells. German and Russian, all the available sentences are used from the WMT monolingual News Crawl datasets from years 2007 through 2017. As for Romanian, we combine the News Crawl dataset and WMT’16 monolingual dataset. The translation results are evaluated on newstest 2014 for en-fr, and newstest 2016 for en-de, en-ro and en-ru. 4.2 Implementation Details We follow previous methods (Koehn et al., 2007; Lample et al., 2018b) to initialize our models. Initialization We use Moses scripts (Koehn et al., 2007) for tokenization. While the system requires cross-lingual BPE embeddings to initialize the shared lookup table for related languages, we set the number of BPE codes as 60, 000. Following the previous preprocessing protocol (Lample et al., 2018b), the embeddings are then generated using fastText (Bojanowski et al., 2017) with an embedding dimension of 512, a context window of size 5 and 10 negative samples. Model Structure In this work, the NMT models can be built upon long short-term memo"
N19-1120,W17-3204,0,0.183025,"d sentences by t-s s-t Translation Reconstruction Loss Comparative Translation Loss t-s Translation Figure 1: The comparison between two approaches of unsupervised NMT, extract-edit and back-translation. When training the source-to-target (s-t) translation model, instead of using the t-s back-translated sentences to train the model, we directly set the extractededited sentences as pivotal points to guide the training. Introduction Promising results have been achieved in Neural Machine Translation (NMT) by representation learning (Cho et al., 2014b; Sutskever et al., 2014). But recent studies (Koehn and Knowles, 2017; Isabelle et al., 2017; Sennrich, 2017) highlight the overreliance of current NMT systems on large parallel corpora. In real-world cases, the majority of language pairs have very little parallel data, so the models need to leverage monolingual data to address this challenge (Gulcehre et al., 2015; Zhang and Zong, 2016; He et al., 2016; Yang et al., 2018). While many studies have explored how to use the monolingual data to improve translation performance with limited supervision, latest approaches (Lample et al., 2018a; Artetxe et al., 2018; Lample et al., 2018b) focus on the fully unsupervise"
N19-1120,P13-2138,0,0.0280036,"egoire and Langlais, 2018) learn to identify parallel sentences in the semantic spaces. However, these methods require large amounts of parallel sentence pairs to train the systems first and then test the performance on raw comparable corpora, which does not apply to languages with limited resources. Instead, we explore the corpora mining in an unsupervised fashion and propose a joint training framework with machine translation. Retrieval-Augmented Text Generation Our work is also related to the recent work on applying retrieval mechanisms to augment text generation, such as image captioning (Kuznetsova et al., 2013; Mason and Charniak, 2014), dialogue generation (Song et al., 2016; Yan et al., 2016; Wu et al., 2018) and style transfer (Lin et al., 2017; Li et al., 2018). Some editing-based models (Guu et al., 2018; Hashimoto et al., 2018) are proposed to further enhance the retrieved text. Recent work in machine translation (Gu et al., 2018) augments an NMT model with sentence pairs retrieved by an off-the-shelf search engine. However, these methods are two-staged with supervised retrieval first. In our work, the extracted-edited sentences are not directly used as the ground truth to train the translati"
N19-1120,J82-2005,0,0.620017,"Missing"
N19-1120,N18-1169,0,0.057611,"Missing"
N19-1120,W14-1602,0,0.0248918,") learn to identify parallel sentences in the semantic spaces. However, these methods require large amounts of parallel sentence pairs to train the systems first and then test the performance on raw comparable corpora, which does not apply to languages with limited resources. Instead, we explore the corpora mining in an unsupervised fashion and propose a joint training framework with machine translation. Retrieval-Augmented Text Generation Our work is also related to the recent work on applying retrieval mechanisms to augment text generation, such as image captioning (Kuznetsova et al., 2013; Mason and Charniak, 2014), dialogue generation (Song et al., 2016; Yan et al., 2016; Wu et al., 2018) and style transfer (Lin et al., 2017; Li et al., 2018). Some editing-based models (Guu et al., 2018; Hashimoto et al., 2018) are proposed to further enhance the retrieved text. Recent work in machine translation (Gu et al., 2018) augments an NMT model with sentence pairs retrieved by an off-the-shelf search engine. However, these methods are two-staged with supervised retrieval first. In our work, the extracted-edited sentences are not directly used as the ground truth to train the translation model. Instead, we view"
N19-1120,W02-1037,0,0.0853116,"Missing"
N19-1120,P06-1011,0,0.12658,"Missing"
N19-1120,P18-1005,0,0.150559,"ededited sentences as pivotal points to guide the training. Introduction Promising results have been achieved in Neural Machine Translation (NMT) by representation learning (Cho et al., 2014b; Sutskever et al., 2014). But recent studies (Koehn and Knowles, 2017; Isabelle et al., 2017; Sennrich, 2017) highlight the overreliance of current NMT systems on large parallel corpora. In real-world cases, the majority of language pairs have very little parallel data, so the models need to leverage monolingual data to address this challenge (Gulcehre et al., 2015; Zhang and Zong, 2016; He et al., 2016; Yang et al., 2018). While many studies have explored how to use the monolingual data to improve translation performance with limited supervision, latest approaches (Lample et al., 2018a; Artetxe et al., 2018; Lample et al., 2018b) focus on the fully unsupervised scenario. Back-translation has been dominantly used in these approaches, where pseudo sentence pairs are generated to train the translation systems with a reconstruction loss. However, it is inefficient because the generated pseudo sentence pairs are usually of low quality. During the dual learning of back-translation, the errors could easily accumulate"
N19-1120,D16-1160,0,0.132872,"the model, we directly set the extractededited sentences as pivotal points to guide the training. Introduction Promising results have been achieved in Neural Machine Translation (NMT) by representation learning (Cho et al., 2014b; Sutskever et al., 2014). But recent studies (Koehn and Knowles, 2017; Isabelle et al., 2017; Sennrich, 2017) highlight the overreliance of current NMT systems on large parallel corpora. In real-world cases, the majority of language pairs have very little parallel data, so the models need to leverage monolingual data to address this challenge (Gulcehre et al., 2015; Zhang and Zong, 2016; He et al., 2016; Yang et al., 2018). While many studies have explored how to use the monolingual data to improve translation performance with limited supervision, latest approaches (Lample et al., 2018a; Artetxe et al., 2018; Lample et al., 2018b) focus on the fully unsupervised scenario. Back-translation has been dominantly used in these approaches, where pseudo sentence pairs are generated to train the translation systems with a reconstruction loss. However, it is inefficient because the generated pseudo sentence pairs are usually of low quality. During the dual learning of back-translatio"
N19-1120,E17-2060,0,0.126438,"n Loss Comparative Translation Loss t-s Translation Figure 1: The comparison between two approaches of unsupervised NMT, extract-edit and back-translation. When training the source-to-target (s-t) translation model, instead of using the t-s back-translated sentences to train the model, we directly set the extractededited sentences as pivotal points to guide the training. Introduction Promising results have been achieved in Neural Machine Translation (NMT) by representation learning (Cho et al., 2014b; Sutskever et al., 2014). But recent studies (Koehn and Knowles, 2017; Isabelle et al., 2017; Sennrich, 2017) highlight the overreliance of current NMT systems on large parallel corpora. In real-world cases, the majority of language pairs have very little parallel data, so the models need to leverage monolingual data to address this challenge (Gulcehre et al., 2015; Zhang and Zong, 2016; He et al., 2016; Yang et al., 2018). While many studies have explored how to use the monolingual data to improve translation performance with limited supervision, latest approaches (Lample et al., 2018a; Artetxe et al., 2018; Lample et al., 2018b) focus on the fully unsupervised scenario. Back-translation has been do"
N19-1120,P16-1009,0,0.661472,"coder and decoder models in the sequence-to-sequence systems. Vs→t stands for the composition of enc in the source language and dec in the target language, which can be viewed as the source-to-target translation system. Initialization Given the ill-posed nature of the unsupervised NMT task, a suitable initialization method can help model the natural priors over the mapping of two language spaces we expect to reach. There are mainly two initialization methods: (1) bilingual dictionary inference (Conneau et al., 2018; Artetxe et al., 2018; Lample et al., 2018a) and (2) byte-pair encoding (BPE) (Sennrich et al., 2016b; Lample et al., 2018b). As shown in Lample et al. (2018b), the inferred bilingual dictionary can provide a rough word-by-word alignment of semantics, and the BPE can reduce the vocabulary size and eliminate the presence of unknown words in the output results. In our extract-edit approach, to extract potential parallel sentence pairs, we need to compare the semantic similarity of sentences between two languages first. A proper initialization can also help align the semantic spaces and extract potential parallel pairs within them. Thus, following the previous methods, we use the inferred bilin"
N19-1120,P16-1162,0,0.88314,"coder and decoder models in the sequence-to-sequence systems. Vs→t stands for the composition of enc in the source language and dec in the target language, which can be viewed as the source-to-target translation system. Initialization Given the ill-posed nature of the unsupervised NMT task, a suitable initialization method can help model the natural priors over the mapping of two language spaces we expect to reach. There are mainly two initialization methods: (1) bilingual dictionary inference (Conneau et al., 2018; Artetxe et al., 2018; Lample et al., 2018a) and (2) byte-pair encoding (BPE) (Sennrich et al., 2016b; Lample et al., 2018b). As shown in Lample et al. (2018b), the inferred bilingual dictionary can provide a rough word-by-word alignment of semantics, and the BPE can reduce the vocabulary size and eliminate the presence of unknown words in the output results. In our extract-edit approach, to extract potential parallel sentence pairs, we need to compare the semantic similarity of sentences between two languages first. A proper initialization can also help align the semantic spaces and extract potential parallel pairs within them. Thus, following the previous methods, we use the inferred bilin"
N19-1305,W05-0909,0,0.00848568,"zer (Kingma and Ba, 2015). The hyper-parameters of two models are exactly the same. We set the maximum generation length M = 50. The hidden size of the encoders is 64. The size of the word embedding is 200 and that of character embedding is 100. The word embeddings and character embeddings are randomly initialized. Each model is trained for 50 epochs. We report the deciphering results of two models on three testing datasets D, Ds and Dd . 5.2 Experimental Results Quantitative Results: We use equally weighted BLEU score for up to 4-grams (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) to evaluate the decipherment results. The results are shown in Table 1. Figure 4 shows the BLEU score achieved by the two models on three testing datasets D, Ds and Dd during the training process. Both our Seq2Seq model and Variational Decipher achieve reasonable BLEU scores on the testing datasets. The Seq2Seq model outperforms the Variational Decipher on Ds while Variational Decipher outperforms Seq2Seq on Dd . Note that Ds is more than twice the size of Dd . Therefore, Seq2Seq outperforms Variational Decipher on the entire testing dataset D. The different performance of the two models on D"
N19-1305,P10-1079,0,0.015382,"e the design of two neural network models for the decipherment problem. Quantitative and qualitative experimental results are presented in Section 5. Finally, we conclude in Section 6. 2 2.1 Related Work Text Normalization in Social Media The proposed task is related to text normalization focusing on the problems presented by user-generated content in online sources, such as misspelling, rapidly changing out-of-vocabulary slang, short-forms and acronyms, punctuation errors or omissions, etc. These problems usually appear as out-of-vocabulary words. Extensive research has focused on this task (Beaufort et al., 2010; Liu et al., 2011; Gouws et al., 2011; Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012; Chrupała, 2014). However, our task is different from the general text normalization in social media in that instead of the out-of-vocabulary words, we focus on the symbols conveying hateful meaning. These hate symbols can go beyond lexical variants of the vocabulary and thus are more challenging to understand. 2.2 Machine Translation An extensive body of work has been dedicated to machine translation. Knight et al. (2006) study a number of natural language decipherment problems using unsupervised"
N19-1305,P14-2111,0,0.0164696,"s are presented in Section 5. Finally, we conclude in Section 6. 2 2.1 Related Work Text Normalization in Social Media The proposed task is related to text normalization focusing on the problems presented by user-generated content in online sources, such as misspelling, rapidly changing out-of-vocabulary slang, short-forms and acronyms, punctuation errors or omissions, etc. These problems usually appear as out-of-vocabulary words. Extensive research has focused on this task (Beaufort et al., 2010; Liu et al., 2011; Gouws et al., 2011; Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012; Chrupała, 2014). However, our task is different from the general text normalization in social media in that instead of the out-of-vocabulary words, we focus on the symbols conveying hateful meaning. These hate symbols can go beyond lexical variants of the vocabulary and thus are more challenging to understand. 2.2 Machine Translation An extensive body of work has been dedicated to machine translation. Knight et al. (2006) study a number of natural language decipherment problems using unsupervised learning. Ravi and Knight (2011) further frame the task of machine translation as decipherment and tackle it with"
N19-1305,I17-1078,0,0.0235828,"a and Welling, 2014; Salimans et al., 2015), faces (Kingma and Welling, 2014; Rezende et al., 2014), and machine translation (Zhang et al., 2016). Conditional Variational Autoencoder (Larsen et al., 2016; Sohn et al., 2015) extends the original VAE framework by incorporating conditions during generation. In addition to image generation, CVAE has been successfully applied to some NLP tasks. For example, Zhao et al. (2017) apply CVAE to dialog generation, while Guu et al. (2018) use CVAE for sentence generation. 2.4 Hate Speech Analysis Closely related to our work are Pavlopoulos et al. (2017); Gao et al. (2017). Pavlopoulos et al. (2017) build an RNN supplemented by an attention mechanism that outperforms the previous state of the art system in user comment moderation (Wulczyn et al., 2017). Gao et al. (2017) propose a weakly-supervised approach that jointly trains a slur learner and a hate speech classifier. While their work contributes to the automation of harmful content detection and the highlighting of suspicious words, our work builds upon these contributions by providing a learning mechanism that deciphers suspicious hate symbols used by communities of hate to bypass automated content moderat"
N19-1305,W11-2210,0,0.0270872,"for the decipherment problem. Quantitative and qualitative experimental results are presented in Section 5. Finally, we conclude in Section 6. 2 2.1 Related Work Text Normalization in Social Media The proposed task is related to text normalization focusing on the problems presented by user-generated content in online sources, such as misspelling, rapidly changing out-of-vocabulary slang, short-forms and acronyms, punctuation errors or omissions, etc. These problems usually appear as out-of-vocabulary words. Extensive research has focused on this task (Beaufort et al., 2010; Liu et al., 2011; Gouws et al., 2011; Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012; Chrupała, 2014). However, our task is different from the general text normalization in social media in that instead of the out-of-vocabulary words, we focus on the symbols conveying hateful meaning. These hate symbols can go beyond lexical variants of the vocabulary and thus are more challenging to understand. 2.2 Machine Translation An extensive body of work has been dedicated to machine translation. Knight et al. (2006) study a number of natural language decipherment problems using unsupervised learning. Ravi and Knight (2011) furt"
N19-1305,Q18-1031,0,0.0199173,"underlying probability distribution of data. VAE has shown promise in multiple generation tasks, such as handwritten 3007 digits (Kingma and Welling, 2014; Salimans et al., 2015), faces (Kingma and Welling, 2014; Rezende et al., 2014), and machine translation (Zhang et al., 2016). Conditional Variational Autoencoder (Larsen et al., 2016; Sohn et al., 2015) extends the original VAE framework by incorporating conditions during generation. In addition to image generation, CVAE has been successfully applied to some NLP tasks. For example, Zhao et al. (2017) apply CVAE to dialog generation, while Guu et al. (2018) use CVAE for sentence generation. 2.4 Hate Speech Analysis Closely related to our work are Pavlopoulos et al. (2017); Gao et al. (2017). Pavlopoulos et al. (2017) build an RNN supplemented by an attention mechanism that outperforms the previous state of the art system in user comment moderation (Wulczyn et al., 2017). Gao et al. (2017) propose a weakly-supervised approach that jointly trains a slur learner and a hate speech classifier. While their work contributes to the automation of harmful content detection and the highlighting of suspicious words, our work builds upon these contributions"
N19-1305,P11-1038,0,0.0224277,"t problem. Quantitative and qualitative experimental results are presented in Section 5. Finally, we conclude in Section 6. 2 2.1 Related Work Text Normalization in Social Media The proposed task is related to text normalization focusing on the problems presented by user-generated content in online sources, such as misspelling, rapidly changing out-of-vocabulary slang, short-forms and acronyms, punctuation errors or omissions, etc. These problems usually appear as out-of-vocabulary words. Extensive research has focused on this task (Beaufort et al., 2010; Liu et al., 2011; Gouws et al., 2011; Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012; Chrupała, 2014). However, our task is different from the general text normalization in social media in that instead of the out-of-vocabulary words, we focus on the symbols conveying hateful meaning. These hate symbols can go beyond lexical variants of the vocabulary and thus are more challenging to understand. 2.2 Machine Translation An extensive body of work has been dedicated to machine translation. Knight et al. (2006) study a number of natural language decipherment problems using unsupervised learning. Ravi and Knight (2011) further frame the task of m"
N19-1305,D12-1039,0,0.0186212,"and qualitative experimental results are presented in Section 5. Finally, we conclude in Section 6. 2 2.1 Related Work Text Normalization in Social Media The proposed task is related to text normalization focusing on the problems presented by user-generated content in online sources, such as misspelling, rapidly changing out-of-vocabulary slang, short-forms and acronyms, punctuation errors or omissions, etc. These problems usually appear as out-of-vocabulary words. Extensive research has focused on this task (Beaufort et al., 2010; Liu et al., 2011; Gouws et al., 2011; Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012; Chrupała, 2014). However, our task is different from the general text normalization in social media in that instead of the out-of-vocabulary words, we focus on the symbols conveying hateful meaning. These hate symbols can go beyond lexical variants of the vocabulary and thus are more challenging to understand. 2.2 Machine Translation An extensive body of work has been dedicated to machine translation. Knight et al. (2006) study a number of natural language decipherment problems using unsupervised learning. Ravi and Knight (2011) further frame the task of machine translation"
N19-1305,Q16-1002,0,0.028448,"to Sequence (Seq2Seq) learning with Recurrent Neural Networks (RNN). Bahdanau et al. (2015) further improve translation performance using the attention mechanism. Google’s Neural Machine Translation System (GNMT) employs a deep attentional LSTM network with residual connections (Wu et al., 2016). Recently, machine translation techniques have been also applied to explain non-standard English expressions (Ni and Wang, 2017). However, our deciphering task is not the same as machine translation in that hate symbols are short and cannot be modeled as language. Our task is more closely related to (Hill et al., 2016) and (Noraset et al., 2017). Hill et al. (2016) propose using neural language embedding models to map the dictionary definitions to the word representations, which is the inverse of our task. Noraset et al. (2017) propose the definition modeling task. However, in their task, for each word to be defined, its pre-trained word embedding is required as an input, which is actually the prior knowledge of the words. However, such kind of prior knowledge is not available in our decipherment task. Therefore, our task is more challenging and is not simply a definition modeling task. 2.3 Conditional Vari"
N19-1305,P82-1020,0,0.703792,"Missing"
N19-1305,P06-2065,0,0.067365,"r as out-of-vocabulary words. Extensive research has focused on this task (Beaufort et al., 2010; Liu et al., 2011; Gouws et al., 2011; Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012; Chrupała, 2014). However, our task is different from the general text normalization in social media in that instead of the out-of-vocabulary words, we focus on the symbols conveying hateful meaning. These hate symbols can go beyond lexical variants of the vocabulary and thus are more challenging to understand. 2.2 Machine Translation An extensive body of work has been dedicated to machine translation. Knight et al. (2006) study a number of natural language decipherment problems using unsupervised learning. Ravi and Knight (2011) further frame the task of machine translation as decipherment and tackle it without parallel training data. Machine translation using deep learning (Neural Machine Translation) has been proposed in recent years. Sutskever et al. (2014) and Cho et al. (2014) use Sequence to Sequence (Seq2Seq) learning with Recurrent Neural Networks (RNN). Bahdanau et al. (2015) further improve translation performance using the attention mechanism. Google’s Neural Machine Translation System (GNMT) employ"
N19-1305,P12-1109,0,0.0136839,"xperimental results are presented in Section 5. Finally, we conclude in Section 6. 2 2.1 Related Work Text Normalization in Social Media The proposed task is related to text normalization focusing on the problems presented by user-generated content in online sources, such as misspelling, rapidly changing out-of-vocabulary slang, short-forms and acronyms, punctuation errors or omissions, etc. These problems usually appear as out-of-vocabulary words. Extensive research has focused on this task (Beaufort et al., 2010; Liu et al., 2011; Gouws et al., 2011; Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012; Chrupała, 2014). However, our task is different from the general text normalization in social media in that instead of the out-of-vocabulary words, we focus on the symbols conveying hateful meaning. These hate symbols can go beyond lexical variants of the vocabulary and thus are more challenging to understand. 2.2 Machine Translation An extensive body of work has been dedicated to machine translation. Knight et al. (2006) study a number of natural language decipherment problems using unsupervised learning. Ravi and Knight (2011) further frame the task of machine translation as decipherment a"
N19-1305,P11-2013,0,0.0184513,"ral network models for the decipherment problem. Quantitative and qualitative experimental results are presented in Section 5. Finally, we conclude in Section 6. 2 2.1 Related Work Text Normalization in Social Media The proposed task is related to text normalization focusing on the problems presented by user-generated content in online sources, such as misspelling, rapidly changing out-of-vocabulary slang, short-forms and acronyms, punctuation errors or omissions, etc. These problems usually appear as out-of-vocabulary words. Extensive research has focused on this task (Beaufort et al., 2010; Liu et al., 2011; Gouws et al., 2011; Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012; Chrupała, 2014). However, our task is different from the general text normalization in social media in that instead of the out-of-vocabulary words, we focus on the symbols conveying hateful meaning. These hate symbols can go beyond lexical variants of the vocabulary and thus are more challenging to understand. 2.2 Machine Translation An extensive body of work has been dedicated to machine translation. Knight et al. (2006) study a number of natural language decipherment problems using unsupervised learning. Ravi an"
N19-1305,N16-2013,0,0.301988,"broad question we are asking is—What can machine learning and natural language processing do to help and prevent online hate speech? The vast quantity of hate speech on social media can be analyzed to study online abuse. In 1 https://www.fbi.gov/news/stories/2016-hate-crimestatistics 2 https://www.fbi.gov/news/stories/2015-hate-crimestatistics-released recent years, there has been a growing trend of developing computational models of hate speech. However, the majority of the prior studies focus solely on modeling hate speech as a binary or multiclass classification task (Djuric et al., 2015; Waseem and Hovy, 2016; Burnap and Williams, 2016; Wulczyn et al., 2017; Pavlopoulos et al., 2017). While developing new features for hate speech detection certainly has merits, we believe that understanding hate speech requires us to design computational models that can decipher hate symbols that are commonly used by hate groups. Figure 1 shows an example usage of hate symbols in an otherwise seemingly harmless tweet that promotes hate. For example, Aryan Warrior is a longstanding racist prison gang based in the Nevada prison system. WPWW is the acronym for White Pride World Wide. The hate symbols 1488 and 2316 ar"
N19-1305,I17-2070,1,0.861387,"thout parallel training data. Machine translation using deep learning (Neural Machine Translation) has been proposed in recent years. Sutskever et al. (2014) and Cho et al. (2014) use Sequence to Sequence (Seq2Seq) learning with Recurrent Neural Networks (RNN). Bahdanau et al. (2015) further improve translation performance using the attention mechanism. Google’s Neural Machine Translation System (GNMT) employs a deep attentional LSTM network with residual connections (Wu et al., 2016). Recently, machine translation techniques have been also applied to explain non-standard English expressions (Ni and Wang, 2017). However, our deciphering task is not the same as machine translation in that hate symbols are short and cannot be modeled as language. Our task is more closely related to (Hill et al., 2016) and (Noraset et al., 2017). Hill et al. (2016) propose using neural language embedding models to map the dictionary definitions to the word representations, which is the inverse of our task. Noraset et al. (2017) propose the definition modeling task. However, in their task, for each word to be defined, its pre-trained word embedding is required as an input, which is actually the prior knowledge of the wo"
N19-1305,1983.tc-1.13,0,0.112737,"Missing"
N19-1305,P02-1040,0,0.105089,"or training. Both models are optimized using Adam optimizer (Kingma and Ba, 2015). The hyper-parameters of two models are exactly the same. We set the maximum generation length M = 50. The hidden size of the encoders is 64. The size of the word embedding is 200 and that of character embedding is 100. The word embeddings and character embeddings are randomly initialized. Each model is trained for 50 epochs. We report the deciphering results of two models on three testing datasets D, Ds and Dd . 5.2 Experimental Results Quantitative Results: We use equally weighted BLEU score for up to 4-grams (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Banerjee and Lavie, 2005) to evaluate the decipherment results. The results are shown in Table 1. Figure 4 shows the BLEU score achieved by the two models on three testing datasets D, Ds and Dd during the training process. Both our Seq2Seq model and Variational Decipher achieve reasonable BLEU scores on the testing datasets. The Seq2Seq model outperforms the Variational Decipher on Ds while Variational Decipher outperforms Seq2Seq on Dd . Note that Ds is more than twice the size of Dd . Therefore, Seq2Seq outperforms Variational Decipher on the entire testing"
N19-1305,D17-1117,0,0.133279,"anguage processing do to help and prevent online hate speech? The vast quantity of hate speech on social media can be analyzed to study online abuse. In 1 https://www.fbi.gov/news/stories/2016-hate-crimestatistics 2 https://www.fbi.gov/news/stories/2015-hate-crimestatistics-released recent years, there has been a growing trend of developing computational models of hate speech. However, the majority of the prior studies focus solely on modeling hate speech as a binary or multiclass classification task (Djuric et al., 2015; Waseem and Hovy, 2016; Burnap and Williams, 2016; Wulczyn et al., 2017; Pavlopoulos et al., 2017). While developing new features for hate speech detection certainly has merits, we believe that understanding hate speech requires us to design computational models that can decipher hate symbols that are commonly used by hate groups. Figure 1 shows an example usage of hate symbols in an otherwise seemingly harmless tweet that promotes hate. For example, Aryan Warrior is a longstanding racist prison gang based in the Nevada prison system. WPWW is the acronym for White Pride World Wide. The hate symbols 1488 and 2316 are more implicit. 14 symbolizes the 14 words: “WE MUST SECURE THE EXISTENCE O"
N19-1305,D16-1050,0,0.0265802,"edge is not available in our decipherment task. Therefore, our task is more challenging and is not simply a definition modeling task. 2.3 Conditional Variational Autoencoder Unlike the original Seq2Seq model that directly encodes the input into a latent space, the Variational Autoencoder (VAE) (Kingma and Welling, 2014) approximates the underlying probability distribution of data. VAE has shown promise in multiple generation tasks, such as handwritten 3007 digits (Kingma and Welling, 2014; Salimans et al., 2015), faces (Kingma and Welling, 2014; Rezende et al., 2014), and machine translation (Zhang et al., 2016). Conditional Variational Autoencoder (Larsen et al., 2016; Sohn et al., 2015) extends the original VAE framework by incorporating conditions during generation. In addition to image generation, CVAE has been successfully applied to some NLP tasks. For example, Zhao et al. (2017) apply CVAE to dialog generation, while Guu et al. (2018) use CVAE for sentence generation. 2.4 Hate Speech Analysis Closely related to our work are Pavlopoulos et al. (2017); Gao et al. (2017). Pavlopoulos et al. (2017) build an RNN supplemented by an attention mechanism that outperforms the previous state of the art s"
N19-1305,P17-1061,0,0.0212904,"oencoder (VAE) (Kingma and Welling, 2014) approximates the underlying probability distribution of data. VAE has shown promise in multiple generation tasks, such as handwritten 3007 digits (Kingma and Welling, 2014; Salimans et al., 2015), faces (Kingma and Welling, 2014; Rezende et al., 2014), and machine translation (Zhang et al., 2016). Conditional Variational Autoencoder (Larsen et al., 2016; Sohn et al., 2015) extends the original VAE framework by incorporating conditions during generation. In addition to image generation, CVAE has been successfully applied to some NLP tasks. For example, Zhao et al. (2017) apply CVAE to dialog generation, while Guu et al. (2018) use CVAE for sentence generation. 2.4 Hate Speech Analysis Closely related to our work are Pavlopoulos et al. (2017); Gao et al. (2017). Pavlopoulos et al. (2017) build an RNN supplemented by an attention mechanism that outperforms the previous state of the art system in user comment moderation (Wulczyn et al., 2017). Gao et al. (2017) propose a weakly-supervised approach that jointly trains a slur learner and a hate speech classifier. While their work contributes to the automation of harmful content detection and the highlighting of su"
N19-1305,P11-1002,0,0.0261242,"., 2011; Gouws et al., 2011; Han and Baldwin, 2011; Han et al., 2012; Liu et al., 2012; Chrupała, 2014). However, our task is different from the general text normalization in social media in that instead of the out-of-vocabulary words, we focus on the symbols conveying hateful meaning. These hate symbols can go beyond lexical variants of the vocabulary and thus are more challenging to understand. 2.2 Machine Translation An extensive body of work has been dedicated to machine translation. Knight et al. (2006) study a number of natural language decipherment problems using unsupervised learning. Ravi and Knight (2011) further frame the task of machine translation as decipherment and tackle it without parallel training data. Machine translation using deep learning (Neural Machine Translation) has been proposed in recent years. Sutskever et al. (2014) and Cho et al. (2014) use Sequence to Sequence (Seq2Seq) learning with Recurrent Neural Networks (RNN). Bahdanau et al. (2015) further improve translation performance using the attention mechanism. Google’s Neural Machine Translation System (GNMT) employs a deep attentional LSTM network with residual connections (Wu et al., 2016). Recently, machine translation"
N19-1305,W04-1013,0,\N,Missing
N19-1352,D15-1075,0,0.0306916,"Missing"
N19-1352,P17-1152,0,0.0514724,"line selection algorithms, including the frequency-based algorithm (Luong et al., 2015), TF-IDF algorithm (Ramos et al., 2003), and structure lasso algorithm (Friedman et al., 2010), and demonstrate that it can consistently outperform these competing algorithms by a remarkable margin. To show that the conclusions are widely held, our evaluation is based on a wide range of text classification tasks and datasets with different neural networks including Convolutional Neural Network (CNN) (Kim, 2014), Bi-directional Long-Short Term Memory (BiLSTM) (Bahdanau et al., 2014) and Enhanced LSTM (ESIM) (Chen et al., 2017). In summary, our contributions are three-fold: 1. We formally define the vocabulary selection problem, demonstrate its importance, and propose new evaluation metrics for vocabulary selection in text classification tasks. 2. We propose a novel vocabulary selection algorithm based on variational dropout by re-formulating text classification under the Bayesian inference framework. The code will be released in Github1 . 3. We conduct comprehensive experiments to demonstrate the superiority of the proposed vocabulary selection algorithm over a number of strong baselines. 2 2.1 Vocabulary Selection"
N19-1352,N18-2118,0,0.0596795,"correlated with dropout ratio α. Intuitively, the dropout ratio αi is an redundancy indicator for ith word in the vocabulary, with larger αi meaning less performance loss caused by dropping ith word. During training, we use re-parameterization trick (Kingma and We compare the proposed vocabulary selection algorithm against several strong baselines on a wide range of text classification tasks and datasets. 4.1 Datasets & Architectures The main datasets we are using are listed in Table 2, which provides an overview of its description and capacities. Specifically, we follow (Zhang et al., 2015; Goo et al., 2018; Williams et al., 2018) to pre-process the document classification datasets, natural language understanding dataset and natural language inference dataset. We exactly replicate their experiment settings to make our method comparable with theirs. Our models is implemented with TensorFlow (Abadi et al., 2015). In order to evaluate the generalization ability of VVD selection algorithm in deep learning architectures, we study its performance under different established architectures (depicted in Figure 3). In natural language understanding, we use the most recent attention-based model for intenti"
N19-1352,D14-1181,0,0.0829424,"a et al., 2015; Faruqui et al., 2015), nor necessary, as many words may contribute little to the end task and could have been safely removed from the vocabulary. Therefore, how to select the best vocabulary is a problem of both theoretical and practical interests. Somewhat surprisingly, this vocabulary selection problem is largely under-addressed in the literature: The de facto standard practice is to do 3487 Proceedings of NAACL-HLT 2019, pages 3487–3497 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics frequency-based cutoff (Luong et al., 2015; Kim, 2014), and only retain the words more frequent than a certain threshold (Table 1). Although this simple heuristic has demonstrated strong empirical performance, its task-agnostic nature implies that likely it is not the optimal strategy for many tasks (or any task). Task-aware vocabulary selection strategies and a systematic comparison of different strategies are still lacking. In this work, we present the first systematic study of the vocabulary selection problem. Our study will be based on text classification tasks, a broad family of NLP tasks including document classification (DC), natural langu"
N19-1352,D18-1319,0,0.0430096,"Missing"
N19-1352,P16-2058,0,0.0943713,"Missing"
N19-1352,P15-1144,0,0.157818,"Missing"
N19-1352,Q17-1026,0,0.0799327,"have a small number of hyperparameters compared to pruningbased methods. As stated in (Chirkova et al., 2018), Bayesian compression also leads to a higher sparsity level (Molchanov et al., 2017; Neklyudov et al., 2017; Louizos et al., 2017). Our proposed VVD is inspired by these predecessors to specifically tackle the vocabulary redundancy problem in NLP tasks. Vocabulary Reduction An orthogonal line of research for dealing similar vocabulary redundancy problem is the character-based approaches to reduce vocabulary sise (Kim et al., 2016; Zhang et al., 2015; Costa-Juss`a and Fonollosa, 2016; Lee et al., 2017), which decomposes the words into its characters forms for better handling open world inputs. However, these approaches are not applicable to character-free languages like Chinese and Japanese. Moreover, splitting words into characters incurs potential lose of word-level surface form, and thus needs more parameters at the neural network level to recover it to maintain the end task performance (Zhang et al., 2015), which contradicts with our initial motivation of compressing the neural network models for computationor memory-constrained scenarios. 6 Conclusion In this paper, we propose a vocabu"
N19-1352,D15-1166,0,0.150385,"d scenarios (Yogatama et al., 2015; Faruqui et al., 2015), nor necessary, as many words may contribute little to the end task and could have been safely removed from the vocabulary. Therefore, how to select the best vocabulary is a problem of both theoretical and practical interests. Somewhat surprisingly, this vocabulary selection problem is largely under-addressed in the literature: The de facto standard practice is to do 3487 Proceedings of NAACL-HLT 2019, pages 3487–3497 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics frequency-based cutoff (Luong et al., 2015; Kim, 2014), and only retain the words more frequent than a certain threshold (Table 1). Although this simple heuristic has demonstrated strong empirical performance, its task-agnostic nature implies that likely it is not the optimal strategy for many tasks (or any task). Task-aware vocabulary selection strategies and a systematic comparison of different strategies are still lacking. In this work, we present the first systematic study of the vocabulary selection problem. Our study will be based on text classification tasks, a broad family of NLP tasks including document classification (DC), n"
N19-1352,N18-1101,0,0.0680162,"ropout ratio α. Intuitively, the dropout ratio αi is an redundancy indicator for ith word in the vocabulary, with larger αi meaning less performance loss caused by dropping ith word. During training, we use re-parameterization trick (Kingma and We compare the proposed vocabulary selection algorithm against several strong baselines on a wide range of text classification tasks and datasets. 4.1 Datasets & Architectures The main datasets we are using are listed in Table 2, which provides an overview of its description and capacities. Specifically, we follow (Zhang et al., 2015; Goo et al., 2018; Williams et al., 2018) to pre-process the document classification datasets, natural language understanding dataset and natural language inference dataset. We exactly replicate their experiment settings to make our method comparable with theirs. Our models is implemented with TensorFlow (Abadi et al., 2015). In order to evaluate the generalization ability of VVD selection algorithm in deep learning architectures, we study its performance under different established architectures (depicted in Figure 3). In natural language understanding, we use the most recent attention-based model for intention tracking (Goo et al.,"
N19-1352,D14-1162,0,0.081906,"efore, be leveraged for vocabulary selection. We propose to infer the latent dropout probabilities under a Bayesian inference framework. During test time, we select the ˆ by only retaining words with sub vocabulary V dropout probability lower than a certain threshold. For any words deselected using VVD, we will simply regard them as a special token with null vector representation [0, 0, · · · , 0]. Please note that our proposed algorithm needs to re-train a word embedding matrix, thus it is tangential to the research of pre-trained word embedding like Word2Vec (Mikolov et al., 2013) or Glove (Pennington et al., 2014) though we can use them to initialize our embedding. We conduct comprehensive experiments to evaluate the performance of VVD (section 4) on different end classification tasks. Specifically, we compare against an array of strong baseline selection algorithms, including the frequency-based algorithm (Luong et al., 2015), TF-IDF algorithm (Ramos et al., 2003), and structure lasso algorithm (Friedman et al., 2010), and demonstrate that it can consistently outperform these competing algorithms by a remarkable margin. To show that the conclusions are widely held, our evaluation is based on a wide ra"
N19-5001,P18-1044,0,0.0297444,"2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solvi"
N19-5001,C18-1055,0,0.0668385,"th case study of Generative Adversarial Networks for NLP, with a focus on dialogue generation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Good"
N19-5001,P18-2006,0,0.13551,"th case study of Generative Adversarial Networks for NLP, with a focus on dialogue generation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Good"
N19-5001,N19-1337,1,0.835986,"ks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is challenging to deploy GANs for NLP problems, comparing to vision problems. We then focus on introducing SeqGAN (Yu et al., 2017), an early solution of textual models of GAN, with a focus on policy gradient and Monte Carlo Tree Search. Finally, we provide an in-depth case study of deploying two-agent GAN models for conversational AI (Li et al., 2017). We will summarize the lessons lea"
N19-5001,D17-1215,0,0.0493758,"for NLP, with a focus on dialogue generation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is"
N19-5001,D18-1026,0,0.0654804,"Missing"
N19-5001,P18-1225,0,0.0266302,"rning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP p"
N19-5001,P18-1046,1,0.924824,"Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, we start with the gentle introduction to the fundamentals of adver"
N19-5001,P17-1119,0,0.0214364,"s of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learni"
N19-5001,P18-1079,1,0.824017,"adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is challenging to deploy GANs for NLP problems, comparing to vision problems. We then focus on introducing SeqGAN (Yu et al., 2017), an early solution of textual models of GAN, with a focus on policy gradient and Monte Carlo"
N19-5001,D17-1187,0,0.141197,"018a,b; Shi et al., 2018b; Chen et al., 2018; Farag et al., 2018; Ribeiro et al., 2018; Zhao et al., 2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pro"
N19-5001,D18-1077,0,0.122559,"al focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, w"
N19-5001,D18-1428,0,0.138691,"ial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data clean"
N19-5001,D18-1131,0,0.153172,"oduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP"
N19-5001,N18-1122,0,0.0638405,"Missing"
N19-5001,D18-1125,0,0.0412213,"Missing"
N19-5001,N18-1089,0,0.0232576,", 2018; Ribeiro et al., 2018; Zhao et al., 2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the"
N19-5001,C18-1315,0,0.127664,"ation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, we start with the gentle introduction to the fundamentals of adversarial learning. We further Tutorial Description Adversar"
N19-5001,C18-1103,0,0.030798,"et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tuto"
N19-5001,D18-1009,0,0.140633,"negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, we start with the gentle"
N19-5001,D18-1031,0,0.154184,"aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information re"
N19-5001,D18-1316,0,0.0180701,"ation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is challenging to deploy GANs for NLP pr"
N19-5001,C18-1099,0,0.133892,"aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information re"
N19-5001,C18-1037,0,0.0290058,"on natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP"
N19-5001,P18-1083,1,0.909559,"aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information re"
N19-5001,D18-1451,0,0.0867524,"ces in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al.,"
N19-5001,N18-2091,0,0.0348511,"., 2018b; Chen et al., 2018; Farag et al., 2018; Ribeiro et al., 2018; Zhao et al., 2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to p"
P12-1078,W02-1001,0,0.0562306,"e latent variables in SME, we derive the following likelihood function: L= X hlog P (θd |α)i + log P (Zn(d) |θd ) d + Nd X (R) (Q) log P (wn(d) |zn(d) , η, m, yd , yd ) where ck is the true count, and βqjk is the log word likelihood in the original likelihood function. Cqjk is the expected count P Pfrom combinations of time, region and topic. q j hCqjk iβqjk will then be taken the second order derivative to form the Hessian matrix, instead of hCk iβk in the previous SAGE setting. To learn the weight parameters λ(R) and λ(Q) , we can approximate the weights using a multiclass perceptron-style (Collins, 2002) learning method. If P ¯ we say that the notation of V (R) is to marginalize (R) out all other variables in β except η (R) , and P (yd ) is the prior for the region prediction task, we can pre(R) dict the expected region value yˆd of a document d: (R) yˆd n + X (T ) (T ) X (T ) hlog P (τk |γ)i hlog P (ηk |0, τk )i + k + X k (R) (R) hlog P (ηj |0, τj )i + X j + X (R) hlog P (τj j (R)  X ¯ V (R) m + η (Q) (T ) (d) zn +η  (I) (R) yd (Q) ,yd (d) ,zn + λ(R) η (R) (R) yd  (R) P (yd ) If the symbol δ is the hyperprior for the learning (R) rate and y˙ d is the true label, the update procedure for"
P12-1078,W11-1506,0,0.0213169,"ith a robust analysis from qualitative and quantitative standpoints in section 5.2, and we discuss the conclusions of this work in section 6. 2 Related Work Natural Language Processing (NLP) methods for automatically understanding and identifying key information in historical data have not yet been explored until recently. Related research efforts include using the LDA model for topic modeling in historical newspapers (Yang et al., 2011), a rule-based approach to extract verbs in historical Swedish texts (Pettersson and Nivre, 2011), a system for semantic tagging of historical Dutch archives (Cybulska and Vossen, 2011). Despite our historical data domain, our approach is more relevant to text classification and topic modelling. Traditional discriminative methods, such as support vector machine (SVM) and logistic regression, have been very popular in various text categorization tasks (Joachims, 1998; Wang and McKeown, 2010) in the past decades. However, the main problem with these methods is that although they are accurate in classifying documents, they do not aim at helping us to understand the documents. Another problem is lack of expressiveness. For example, SVM does not have latent variables to model the"
P12-1078,D10-1124,0,0.109986,"Missing"
P12-1078,P11-1137,0,0.269275,"ered decisive. While this approach has merit, new technologies should allow extraction of patterns from large samples of opinions. Latent variable models, such as latent Dirichlet allocation (LDA) (Blei et al., 2003) and probabilistic latent semantic analysis (PLSA) (Hofmann, 1999), have been used in the past to facilitate social science research. However, they have numerous drawbacks, as many topics are uninterpretable, overwhelmed by uninformative words, or represent background language use that is unrelated to the dimensions of analysis that qualitative researchers are interested in. SAGE (Eisenstein et al., 2011a), a recently proposed sparse additive generative model of language, addresses many of the drawbacks of LDA. SAGE assumes a background distribution of language use, and enforces sparsity in individual topics. Another advantage, from a social science perspective, is that SAGE can be derived from a standard logit randomutility model of judicial opinion writing, in contrast to LDA. In this work we extend SAGE to the supervised case of joint region and time period prediction. We formulate the resulting sparse mixedeffects (SME) model as being made up of mixed effects that not only contain random"
P12-1078,D11-1051,0,0.0184541,"uments. Another problem is lack of expressiveness. For example, SVM does not have latent variables to model the subtle differences and interactions of features from different domains (e.g. text, links, and date), but rather treats them as a “bag-of-features”. Generative methods, by contrast, can show the causes to effects, have attracted attentions in recent years due to the rich expressiveness of the models and competitive performances in predictive tasks (Wang et al., 2011). For example, Nguyen et al. (2010) study the effect of the context of interaction in blogs using a standard LDA model. Guo and Diab (2011) show the effectiveness of using se741 mantic information in multifaceted topic models for text categorization. Eisenstein et al. (2010) use a latent variable model to predict geolocation information of Twitter users, and investigate geographic variations of language use. Temporally, topic models have been used to show the shift in language use over time in online communities (Nguyen and Ros´e, 2011) and the evolution of topics over time (Shubhankar et al., 2011). When evaluating understandability, however, dense word distributions are a serious issue in many topic models as well as other pred"
P12-1078,W11-0710,0,0.0453866,"Missing"
P12-1078,W11-1512,0,0.0146722,"deling of region, time, and topic in section 4. Experiments are presented in section 5, with a robust analysis from qualitative and quantitative standpoints in section 5.2, and we discuss the conclusions of this work in section 6. 2 Related Work Natural Language Processing (NLP) methods for automatically understanding and identifying key information in historical data have not yet been explored until recently. Related research efforts include using the LDA model for topic modeling in historical newspapers (Yang et al., 2011), a rule-based approach to extract verbs in historical Swedish texts (Pettersson and Nivre, 2011), a system for semantic tagging of historical Dutch archives (Cybulska and Vossen, 2011). Despite our historical data domain, our approach is more relevant to text classification and topic modelling. Traditional discriminative methods, such as support vector machine (SVM) and logistic regression, have been very popular in various text categorization tasks (Joachims, 1998; Wang and McKeown, 2010) in the past decades. However, the main problem with these methods is that although they are accurate in classifying documents, they do not aim at helping us to understand the documents. Another problem"
P12-1078,C10-1129,1,0.696986,"l recently. Related research efforts include using the LDA model for topic modeling in historical newspapers (Yang et al., 2011), a rule-based approach to extract verbs in historical Swedish texts (Pettersson and Nivre, 2011), a system for semantic tagging of historical Dutch archives (Cybulska and Vossen, 2011). Despite our historical data domain, our approach is more relevant to text classification and topic modelling. Traditional discriminative methods, such as support vector machine (SVM) and logistic regression, have been very popular in various text categorization tasks (Joachims, 1998; Wang and McKeown, 2010) in the past decades. However, the main problem with these methods is that although they are accurate in classifying documents, they do not aim at helping us to understand the documents. Another problem is lack of expressiveness. For example, SVM does not have latent variables to model the subtle differences and interactions of features from different domains (e.g. text, links, and date), but rather treats them as a “bag-of-features”. Generative methods, by contrast, can show the causes to effects, have attracted attentions in recent years due to the rich expressiveness of the models and compe"
P12-1078,I11-1032,1,0.823164,"roblem with these methods is that although they are accurate in classifying documents, they do not aim at helping us to understand the documents. Another problem is lack of expressiveness. For example, SVM does not have latent variables to model the subtle differences and interactions of features from different domains (e.g. text, links, and date), but rather treats them as a “bag-of-features”. Generative methods, by contrast, can show the causes to effects, have attracted attentions in recent years due to the rich expressiveness of the models and competitive performances in predictive tasks (Wang et al., 2011). For example, Nguyen et al. (2010) study the effect of the context of interaction in blogs using a standard LDA model. Guo and Diab (2011) show the effectiveness of using se741 mantic information in multifaceted topic models for text categorization. Eisenstein et al. (2010) use a latent variable model to predict geolocation information of Twitter users, and investigate geographic variations of language use. Temporally, topic models have been used to show the shift in language use over time in online communities (Nguyen and Ros´e, 2011) and the evolution of topics over time (Shubhankar et al.,"
P12-1078,W11-1513,0,0.0503736,"nited States court opinion data. We describe our sparse mixed-effects model for joint modeling of region, time, and topic in section 4. Experiments are presented in section 5, with a robust analysis from qualitative and quantitative standpoints in section 5.2, and we discuss the conclusions of this work in section 6. 2 Related Work Natural Language Processing (NLP) methods for automatically understanding and identifying key information in historical data have not yet been explored until recently. Related research efforts include using the LDA model for topic modeling in historical newspapers (Yang et al., 2011), a rule-based approach to extract verbs in historical Swedish texts (Pettersson and Nivre, 2011), a system for semantic tagging of historical Dutch archives (Cybulska and Vossen, 2011). Despite our historical data domain, our approach is more relevant to text classification and topic modelling. Traditional discriminative methods, such as support vector machine (SVM) and logistic regression, have been very popular in various text categorization tasks (Joachims, 1998; Wang and McKeown, 2010) in the past decades. However, the main problem with these methods is that although they are accurate in"
P14-1109,P07-1038,0,0.0598779,"lementation in Matlab, and LibSVM (Chang and Lin, 2011) for training and testing the SVM models. The hyperparameter C in linear SVM, and the γ and C hyperparameters in Gaussian SVM are tuned on the training set using 10-fold crossvalidation. Note that since the kernel density estimation in the proposed copula model is nonparametric, and we only need to learn the Σ in the Gaussian copula, there is no hyperparameters that need to be tuned. Evaluation Metrics: Spearman’s correlation (Hogg and Craig, 1994) and Kendall’s tau (Kendall, 1938) have been widely used in many regression problems in NLP (Albrecht and Hwa, 2007; Yogatama et al., 2011; Wang et al., 2013; Tsai and Wang, 2013), and here we use them to measure the quality of predicted ˆ by comparing to the vector of ground values y truth y. In contrast to Pearson’s correlation, Spearman’s correlation has no assumptions on the relationship of the two measured variables. Kendall’s tau is a nonparametric statistical metric that have shown to be inexpensive, robust, and representation independent (Lapata, 2006). We also use paired two-tailed t-test to measure the statistical significance between the best and the second best approaches. 6.2 Comparing to Vari"
P14-1109,D12-1124,0,0.052079,"lem as a binary classification task. Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market (Bollen et al., 2011; Zhang et al., 2011). Despite our financial domain, our approach is more relevant to text regression. Traditional discriminative models, such as linear regression and linear SVM, have been very popular in various text regression tasks, such as predicting movie revenues from reviews (Joshi et al., 2010), understanding the geographic lexical variation (Eisenstein et al., 2010), and predicting food prices from menus (Chahuneau et al., 2012). The advantage of these models is that the estimation of the parameters is often simple, the results are easy to interpret, and the approach often yields strong performances. While these approaches have merits, they suffer from the problem of not explicitly modeling the correlations and interactions among random variables, which in some sense, corresponding to the impractical assumption of independent and identically distributed (i.i.d) of the data. For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencie"
P14-1109,D10-1124,0,0.146351,"Missing"
P14-1109,N10-1038,0,0.0608829,"have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a binary classification task. Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market (Bollen et al., 2011; Zhang et al., 2011). Despite our financial domain, our approach is more relevant to text regression. Traditional discriminative models, such as linear regression and linear SVM, have been very popular in various text regression tasks, such as predicting movie revenues from reviews (Joshi et al., 2010), understanding the geographic lexical variation (Eisenstein et al., 2010), and predicting food prices from menus (Chahuneau et al., 2012). The advantage of these models is that the estimation of the parameters is often simple, the results are easy to interpret, and the approach often yields strong performances. While these approaches have merits, they suffer from the problem of not explicitly modeling the correlations and interactions among random variables, which in some sense, corresponding to the impractical assumption of independent and identically distributed (i.i.d) of the data. For exa"
P14-1109,N09-1031,0,0.161415,"ansform, our approach moves beyond the standard count-based bag-ofwords models in NLP, and improves previous work on text regression by incorporating the correlation among local features in the form of semiparametric Gaussian copula. In experiments, we show that our model significantly outperforms strong linear and non-linear discriminative baselines on three datasets under various settings. 1 Introduction Predicting the risks of publicly listed companies is of great interests not only to the traders and analysts on the Wall Street, but also virtually anyone who has investments in the market (Kogan et al., 2009). Traditionally, analysts focus on quantitative modeling of historical trading data. Today, even though earnings calls transcripts are abundantly available, their distinctive communicative practices (Camiciottoli, 2010), and correlations with the financial risks, in particular, future stock performances (Price et al., 2012), are not well studied in the past. Earnings calls are conference calls where a listed company discusses the financial performance. Typically, a earnings call contains two parts: the senior executives first report the operational outcomes, as well as the current financial pe"
P14-1109,J06-4002,0,0.0134015,": Spearman’s correlation (Hogg and Craig, 1994) and Kendall’s tau (Kendall, 1938) have been widely used in many regression problems in NLP (Albrecht and Hwa, 2007; Yogatama et al., 2011; Wang et al., 2013; Tsai and Wang, 2013), and here we use them to measure the quality of predicted ˆ by comparing to the vector of ground values y truth y. In contrast to Pearson’s correlation, Spearman’s correlation has no assumptions on the relationship of the two measured variables. Kendall’s tau is a nonparametric statistical metric that have shown to be inexpensive, robust, and representation independent (Lapata, 2006). We also use paired two-tailed t-test to measure the statistical significance between the best and the second best approaches. 6.2 Comparing to Various Baselines In the first experiment, we compare the proposed semiparametric Gaussian copula regression model to three baselines on three datasets with all features. The detailed results are shown in Table 2. On the pre-2009 dataset, we see that the linear regression and linear SVM perform reasonably well, but the Gaussian kernel SVM performs less well, probably due to overfitting. The copula model outperformed all three baselines by a wide margi"
P14-1109,P13-1086,0,0.274154,"ated annual reports, and performs linear SVM regression with -insensitive loss function to predict the measured volatility. Another recent study (Wang et al., 2013) uses exactly the same max-margin regression technique, but with a different focus on the financial sentiment. Using the same dataset, Tsai and Wang (2013) reformulate the regression problem as a text ranking problem. Note that all these regression studies above investigate the SEC-mandated annual reports, which are very different from the earnings calls in many aspects such as length, format, vocabulary, and genre. Most recently, Xie et al. (2013) have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a binary classification task. Broadly speaking, our work is also aligned to recent studies that make use of social media data to predict the stock market (Bollen et al., 2011; Zhang et al., 2011). Despite our financial domain, our approach is more relevant to text regression. Traditional discriminative models, such as linear regression and linear SVM, have been very popular in various text regression tasks, such as predicting movie revenues from reviews"
P14-1109,D11-1055,0,0.0254694,"Missing"
P14-1109,N03-1033,0,0.0331324,"tions above to calculate the measured stock volatility after the earnings call, which is the standard measure of risks in finance, and the dependent variable y of our predictive task. 6 6.1 Experiments Experimental Setup In all experiments throughout this section, we use 80-20 train/test splits on all three datasets. Feature sets: We have extracted lexical, named entity, syntactic, and frame-semantics features, most of which have been shown to perform well in previous work (Xie et al., 2013). We use the unigrams and bigrams to represent lexical features, and the Stanford partof-speech tagger (Toutanova et al., 2003) to extract the lexicalized named entity and part-of-speech features. A probabilistic frame-semantics parser, SEMAFOR (Das et al., 2010), is used to provide the FrameNet-style frame-level semantic annotations. For each of the five sets, we collect the top100 most frequent features, and end up with a total of 500 features. Baselines: The baselines are standard squared-loss linear regression, linear kernel SVM, and non-linear (Gaussian) kernel SVM. They are all standard algorithms in regression problems, and have been shown to have outstanding performances in many recent text regression (Kogan e"
P14-1109,P12-1078,1,0.843915,"impractical assumption of independent and identically distributed (i.i.d) of the data. For example, when bag-of-word-unigrams are present in the feature space, it is easier if one does not explicitly model the stochastic dependencies among the words, even though doing so might hurt the predictive power, while the variance from the correlations among the random variables is not explained. 3 Copula Models for Text Regression In NLP, many statistical machine learning methods that capture the dependencies among random variables, including topic models (Blei et al., 2003; Lafferty and Blei, 2005; Wang et al., 2012), always have to make assumptions with the underlying distributions of the random variables, and make use of informative priors. This might be rather restricting the expressiveness of the model in some sense (Reisinger et al., 2010). On the other hand, once such assumptions are removed, another problem arises — they might be prone to errors, and suffer from the overfitting issue. Therefore, coping with the tradeoff between expressiveness and overfitting, seems to be rather important in statistical approaches that capture stochastic dependency. Our proposed semiparametric copula regression mode"
P14-1109,I13-1097,0,0.323186,"scuss the results and findings in Section 7 and then conclude in Section 8. 2 Related Work Fung et al. (2003) are among the first to study SVM and text mining methods in the market prediction domain, where they align financial news articles with multiple time series to simulate the 33 stocks in the Hong Kong Hang Seng Index. However, text regression in the financial domain have not been explored until recently. Kogan et al. (2009) model the SEC-mandated annual reports, and performs linear SVM regression with -insensitive loss function to predict the measured volatility. Another recent study (Wang et al., 2013) uses exactly the same max-margin regression technique, but with a different focus on the financial sentiment. Using the same dataset, Tsai and Wang (2013) reformulate the regression problem as a text ranking problem. Note that all these regression studies above investigate the SEC-mandated annual reports, which are very different from the earnings calls in many aspects such as length, format, vocabulary, and genre. Most recently, Xie et al. (2013) have proposed the use of frame-level semantic features to understand financial news, but they treat the stock movement prediction problem as a bina"
P14-1109,N10-1138,0,\N,Missing
P15-1035,D14-1164,0,0.0144003,"tems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task sometimes called KB completion (Socher et al., 2013; Wang et al., 2014; West et al., 2014). Pipelines of this sort frequently suffer from error In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, and conclude in Section 7. 1 For example, KBP slot filling is known for its complex pipeline, and the best overall F1 scores (Wiegand and Klakow, 2013; Angeli et al., 2014) for recent competitions are within the range of 30-40. 355 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 355–364, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Work about(X,Z) :- handLabeled(X,Z) about(X,Z) :- sim(X,Y),about(Y,Z) sim(X,Y) :- links(X,Y) sim(X,Y) :hasWord(X,W),hasWord(Y,W), linkedBy(X,Y,W) linkedBy(X,Y,W) :- true In NLP, our work clearly aligns with recent work on joint models of individual text processing task"
P15-1035,W03-2201,0,0.0187715,"inference for IE and SL significantly improve both tasks; that latent context invention further improves the results. 1 • We present a joint model for IE and relational learning in a statistical relational learning setting which outperforms universal schemas (Riedel et al., 2013), a state-of-theart joint method; Introduction • We incorporate latent context into the joint SRL model, bringing additional improvements. Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering (Moll´a et al., 2006), machine translation (Babych and Hartley, 2003), or other applications (Wang and Hua, 2014; Li et al., 2014). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task sometimes called KB completion (Socher et al., 2013; Wang et al., 2014; West et al., 2014). Pipelines of this sort frequently suffer from error In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, an"
P15-1035,D11-1049,1,0.872368,"schemas (Riedel et al., 2013), a state-of-theart joint method; Introduction • We incorporate latent context into the joint SRL model, bringing additional improvements. Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering (Moll´a et al., 2006), machine translation (Babych and Hartley, 2003), or other applications (Wang and Hua, 2014; Li et al., 2014). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task sometimes called KB completion (Socher et al., 2013; Wang et al., 2014; West et al., 2014). Pipelines of this sort frequently suffer from error In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, and conclude in Section 7. 1 For example, KBP slot filling is known for its complex pipeline, and the best overall F1 scores (Wiegand and Klakow, 2013; Angeli et al., 2014) for recent competitions are within the range of 30-40. 355 Proceedings"
P15-1035,D12-1093,1,0.852018,"rom text, without large-scale annotations. In extracting Infobox information from Wikipedia text, Wu and Weld (2007; 2010) also use a similar idea. In an open IE project, Banko et al. (2007) use a seed KB, and utilize weak supervision techniques to extend it. Note that weakly supervised extraction approaches can be noisy, as a pair of entities in context may be associated with one, none, or several of the possible relation labels, a property which complicates the application of distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). Lao et al. (2012) learned syntactic rules for finding relations defined by “lexico-semantic” paths spanning KB relations and text data. Wang et al. (2015) extends the methods used by Lao et al. to learn mutually recursive relations. Recently, Riedel et al. (2013) propose a matrix factorization technique for relation embedding, but their method requires a large amount of negative and unlabeled examples. Weston et al. (2013) connect text with KB embedding by adding a scoring term, though no shared parameters/embeddings are used. All these prior works make use of text and KBs. Unlike these prior works, our method"
P15-1035,N07-4013,0,0.0768777,"Missing"
P15-1035,P14-1016,0,0.0116867,"context invention further improves the results. 1 • We present a joint model for IE and relational learning in a statistical relational learning setting which outperforms universal schemas (Riedel et al., 2013), a state-of-theart joint method; Introduction • We incorporate latent context into the joint SRL model, bringing additional improvements. Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering (Moll´a et al., 2006), machine translation (Babych and Hartley, 2003), or other applications (Wang and Hua, 2014; Li et al., 2014). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task sometimes called KB completion (Socher et al., 2013; Wang et al., 2014; West et al., 2014). Pipelines of this sort frequently suffer from error In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, and conclude in Section 7. 1 For example, KBP slot filling is k"
P15-1035,P09-1113,0,0.0263458,"hasWord(X,W),hasWord(Y,W), linkedBy(X,Y,W) linkedBy(X,Y,W) :- true In NLP, our work clearly aligns with recent work on joint models of individual text processing tasks. For example, Finkel and Manning (2009) work on the problem of joint IE and parsing, where they use tree representations to combine named entities and syntactic chunks. Recently, Devlin et al. (Devlin et al., 2014) use a joint neural network model for machine translation, and obtain an impressive 6.3 BLEU point improvement over a hierarchical phrase-based system. In information extraction, weak supervision (Craven et al., 1999; Mintz et al., 2009) is a common technique for extracting knowledge from text, without large-scale annotations. In extracting Infobox information from Wikipedia text, Wu and Weld (2007; 2010) also use a similar idea. In an open IE project, Banko et al. (2007) use a seed KB, and utilize weak supervision techniques to extend it. Note that weakly supervised extraction approaches can be noisy, as a pair of entities in context may be associated with one, none, or several of the possible relation labels, a property which complicates the application of distant supervision methods (Mintz et al., 2009; Riedel et al., 2010"
P15-1035,U06-1009,0,0.0346256,"Missing"
P15-1035,D13-1136,0,0.0207702,"he possible relation labels, a property which complicates the application of distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). Lao et al. (2012) learned syntactic rules for finding relations defined by “lexico-semantic” paths spanning KB relations and text data. Wang et al. (2015) extends the methods used by Lao et al. to learn mutually recursive relations. Recently, Riedel et al. (2013) propose a matrix factorization technique for relation embedding, but their method requires a large amount of negative and unlabeled examples. Weston et al. (2013) connect text with KB embedding by adding a scoring term, though no shared parameters/embeddings are used. All these prior works make use of text and KBs. Unlike these prior works, our method is posed in an SRL setting, using a scalable probabilistic first-order logic, and allows learning of relational rules that are mutually recursive, thus allowing learning of multi-step inferences. Unlike some prior methods, our method also does not require negative examples, or large numbers of unlabeled examples. 3 # base. # prop. # sim,link. # sim,word. # by(W). Table 1: A simple program in ProPPR. See t"
P15-1035,N13-1008,0,0.334078,"extraction with structure learning (SL) in a scalable probabilistic logic framework. We then propose a latent context invention (LCI) approach to improve the performance. In experiments, we show that our approach outperforms state-of-the-art baselines over three real-world Wikipedia datasets from multiple domains; that joint learning and inference for IE and SL significantly improve both tasks; that latent context invention further improves the results. 1 • We present a joint model for IE and relational learning in a statistical relational learning setting which outperforms universal schemas (Riedel et al., 2013), a state-of-theart joint method; Introduction • We incorporate latent context into the joint SRL model, bringing additional improvements. Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering (Moll´a et al., 2006), machine translation (Babych and Hartley, 2003), or other applications (Wang and Hua, 2014; Li et al., 2014). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task some"
P15-1035,D12-1042,0,0.0307787,"r extracting knowledge from text, without large-scale annotations. In extracting Infobox information from Wikipedia text, Wu and Weld (2007; 2010) also use a similar idea. In an open IE project, Banko et al. (2007) use a seed KB, and utilize weak supervision techniques to extend it. Note that weakly supervised extraction approaches can be noisy, as a pair of entities in context may be associated with one, none, or several of the possible relation labels, a property which complicates the application of distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). Lao et al. (2012) learned syntactic rules for finding relations defined by “lexico-semantic” paths spanning KB relations and text data. Wang et al. (2015) extends the methods used by Lao et al. to learn mutually recursive relations. Recently, Riedel et al. (2013) propose a matrix factorization technique for relation embedding, but their method requires a large amount of negative and unlabeled examples. Weston et al. (2013) connect text with KB embedding by adding a scoring term, though no shared parameters/embeddings are used. All these prior works make use of text and KBs. Unlike these prio"
P15-1035,P10-1013,0,0.0463274,"Missing"
P15-1035,P14-1109,1,0.825976,"tasks; that latent context invention further improves the results. 1 • We present a joint model for IE and relational learning in a statistical relational learning setting which outperforms universal schemas (Riedel et al., 2013), a state-of-theart joint method; Introduction • We incorporate latent context into the joint SRL model, bringing additional improvements. Information extraction (IE) is often an early stage in a pipeline that contains non-trivial downstream tasks, such as question answering (Moll´a et al., 2006), machine translation (Babych and Hartley, 2003), or other applications (Wang and Hua, 2014; Li et al., 2014). Knowledge bases (KBs) populated by IE techniques have also been used as an input to systems that learn rules allowing further inferences to be drawn from the KB (Lao et al., 2011), a task sometimes called KB completion (Socher et al., 2013; Wang et al., 2014; West et al., 2014). Pipelines of this sort frequently suffer from error In next section, we discuss related work. We describe our approach in Section 3. The details of the datasets are introduced in Section 4. We show experimental results in Section 5, discuss in Section 6, and conclude in Section 7. 1 For example, KBP"
P15-1035,N09-1037,0,\N,Missing
P15-1035,P11-1055,0,\N,Missing
P15-1035,P14-1129,0,\N,Missing
P15-1047,J80-3005,0,0.713369,"Missing"
P15-1047,P98-1013,0,0.0995053,"that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matrix Factorization Approach Considering the benefits brought by MF techniques, including 1) modeling the noisy data, 2) modeling hidden semantics, and 3) modeling the 485 (capability, expensiveness, and locale by use) are generated for the utterance, which we consider as slot candidates for a domain-specific dialogue system (Baker et al., 1998). Then we build a slot matrix Fs with binary values based on the induced slots, which also denotes the slot features for the utterances (right part of the matrix in Figure 1(b)). To build the feature model MF , we concatenate two matrices: can i have a cheap restaurant Frame: expensiveness FT LU: cheap Frame: capability Frame: locale by use FT LU: can FE Filler: i FT/FE LU: restaurant Figure 2: An example of probabilistic framesemantic parsing on ASR output. FT: frame target. FE: frame element. LU: lexical unit. long-range dependencies between observations, in this work we apply an MF approach"
P15-1047,N15-1064,1,0.818583,"l., 2013b). Section 4.1 explains the detail of the feature model. In order to consider the additional inter-word and inter-slot relations, we propose a knowledge graph propagation model based on two knowledge graphs, which includes a word relation model (blue block) and a slot relation model (pink block), described in Section 4.2. The method of automatic knowledge graph construction is introduced in Section 5, where we leverage distributed word embeddings associated with typed syntactic dependencies to model the relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014; Chen et al., 2015). Finally, we train the SLU model by learning latent feature vectors for utterances and slot candidates through MF techniques. Combining with a knowledge graph propagation model based on word/slot relations, the trained SLU model estimates the probability that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matrix Factorization Approach Considering the benefits brought by MF techn"
P15-1047,N10-1138,0,0.0189836,"F by integrating a feature model and a knowledge graph propagation model below. p(Mu,x = 1 |θu,x ) = σ(θu,x ) = 4.1 Feature Model First, we build a word pattern matrix Fw with binary values based on observations, where each row represents an utterance and each column refers to an observed unigram. In other words, Fw carries the basic word vectors for the utterances, which is illustrated as the left part of the matrix in Figure 1(b). To induce the semantic elements, we parse all ASR-decoded utterances in our corpus using SEMAFOR2 , a state-of-the-art semantic parser for frame-semantic parsing (Das et al., 2010; Das et al., 2013), and extract all frames from semantic parsing results as slot candidates (Chen et al., 2013b; Dinarelli et al., 2009). Figure 2 shows an example of an ASR-decoded output parsed by SEMAFOR. Three FrameNet-defined frames 2 (2) Knowledge Graph Propagation Model Since SEMAFOR was trained on FrameNet annotation, which has a more generic frame-semantic context, not all the frames from the parsing results can be used as the actual slots in the domainspecific dialogue systems. For instance, in Figure 2, we see that the frames “expensiveness” and “locale by use” are essentially the"
P15-1047,W09-0505,0,0.0287041,"First, we build a word pattern matrix Fw with binary values based on observations, where each row represents an utterance and each column refers to an observed unigram. In other words, Fw carries the basic word vectors for the utterances, which is illustrated as the left part of the matrix in Figure 1(b). To induce the semantic elements, we parse all ASR-decoded utterances in our corpus using SEMAFOR2 , a state-of-the-art semantic parser for frame-semantic parsing (Das et al., 2010; Das et al., 2013), and extract all frames from semantic parsing results as slot candidates (Chen et al., 2013b; Dinarelli et al., 2009). Figure 2 shows an example of an ASR-decoded output parsed by SEMAFOR. Three FrameNet-defined frames 2 (2) Knowledge Graph Propagation Model Since SEMAFOR was trained on FrameNet annotation, which has a more generic frame-semantic context, not all the frames from the parsing results can be used as the actual slots in the domainspecific dialogue systems. For instance, in Figure 2, we see that the frames “expensiveness” and “locale by use” are essentially the key slots for the purpose of understanding in the restaurant query domain, whereas the “capability” frame does not convey particularly va"
P15-1047,P93-1008,0,0.0867577,"Missing"
P15-1047,N13-1008,0,0.0514169,"tic slots per utterance. 4.4.2 det Figure 4: The dependency parsing result. ln p(Mu |θ) − λθ , where Mu is the vector corresponding to the utterance u from Mu,x in (1), because we assume that each utterance is independent of others. To avoid treating unobserved facts as designed negative facts, we consider our positive-only data as implicit feedback. Bayesian Personalized Ranking (BPR) is an optimization criterion that learns from implicit feedback for MF, which uses a variant of the ranking: giving observed true facts higher scores than unobserved (true or false) facts (Rendle et al., 2009). Riedel et al. (2013) also showed that BPR learns the implicit relations for improving the relation extraction task. dobj can i have a cheap restaurant u∈U = arg max u∈U nsubj u∈U = arg max 4.4.1 ccomp Relation Weight Estimation For the edges in the knowledge graphs, we model the relations between two connected nodes xi and xj as rˆ(xi , xj ), where x is either a slot s or a word pattern w. Since the weights are measured based on the relations between nodes regardless of the directions, we combine the scores of two directional dependencies: Optimization To maximize the objective in (6), we employ a stochastic grad"
P15-1047,J92-1004,0,0.103938,"ed MF approaches produce better SLU models that are able to predict semantic slots and word patterns taking into account their relations and domain-specificity in a joint manner. 1 Introduction A key component of a spoken dialogue system (SDS) is the spoken language understanding (SLU) module—it parses the users’ utterances into semantic representations; for example, the utterance “find a cheap restaurant” can be parsed into (price=cheap, target=restaurant) (Pieraccini et al., 1992). To design the SLU module of a SDS, most previous studies relied on predefined slots1 for training the decoder (Seneff, 1992; Dowding 1 A slot is defined as a basic semantic unit in SLU, such as “price” and “target” in the example. 483 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 483–494, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a knowledge graph propagation based model, fusing both a word-based lexical knowledge graph and a slot-based semantic graph. In fact, as it is shown in the Netflix challenge, MF is credited as the most useful technique for reco"
P15-1047,P13-1045,0,0.0113493,"Missing"
P15-1047,P14-2050,0,0.239037,"n Figure 1(a)) (Chen et al., 2013b). Section 4.1 explains the detail of the feature model. In order to consider the additional inter-word and inter-slot relations, we propose a knowledge graph propagation model based on two knowledge graphs, which includes a word relation model (blue block) and a slot relation model (pink block), described in Section 4.2. The method of automatic knowledge graph construction is introduced in Section 5, where we leverage distributed word embeddings associated with typed syntactic dependencies to model the relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014; Chen et al., 2015). Finally, we train the SLU model by learning latent feature vectors for utterances and slot candidates through MF techniques. Combining with a knowledge graph propagation model based on word/slot relations, the trained SLU model estimates the probability that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matrix Factorization Approach Considering the benefits"
P15-1047,N13-1090,0,0.0868124,"ugh frame-semantic parsing (the yellow block in Figure 1(a)) (Chen et al., 2013b). Section 4.1 explains the detail of the feature model. In order to consider the additional inter-word and inter-slot relations, we propose a knowledge graph propagation model based on two knowledge graphs, which includes a word relation model (blue block) and a slot relation model (pink block), described in Section 4.2. The method of automatic knowledge graph construction is introduced in Section 5, where we leverage distributed word embeddings associated with typed syntactic dependencies to model the relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014; Chen et al., 2015). Finally, we train the SLU model by learning latent feature vectors for utterances and slot candidates through MF techniques. Combining with a knowledge graph propagation model based on word/slot relations, the trained SLU model estimates the probability that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matri"
P15-1047,P14-2105,0,0.021105,"ns the word participates in for training embeddings, where the embeddings are less topical but offer more functional similarity compared to original embeddings (Levy and Goldberg, 2014). Table 1 shows the extracted dependency-based contexts for each target word from the example in Figure 4, where headwords and their dependents can form the contexts by following the arc on a word in the dependency tree, and −1 denotes the directionality of the dependency. After replacing original bag-of-words contexts with dependencybased contexts, we can train dependency-based embeddings for all target words (Yih et al., 2014; Bordes et al., 2011; Bordes et al., 2013). For training dependency-based word embeddings, each target x is associated with a vector vx ∈ Rd and each context c is represented as a context vector vc ∈ Rd , where d is the embedding dimensionality. We learn vector representations for both targets and contexts such that the dot product vx · vc associated with “good” targetcontext pairs belonging to the training data D is maximized, leading to the objective function: arg max Target Word restaurant cheap locale by use expansiveness Experiments Experimental Setup In this experiment, we used the Camb"
P15-1047,C98-1013,0,\N,Missing
P15-1047,J14-1002,0,\N,Missing
P15-1047,H93-1008,0,\N,Missing
P15-1047,P14-1004,0,\N,Missing
P17-2067,D14-1181,0,0.0326859,"to crowdsourced datasets, the instances in LIAR are collected in a grounded, more natural context, such as political debate, TV ads, Facebook posts, tweets, interview, news release, etc. In each case, the labeler provides a lengthy analysis report to ground each judgment, and the links to all supporting documents are also provided. Empirically, we have evaluated several popular learning based methods on this dataset. The baselines include logistic regression, support vector machines, long short-term memory networks (Hochreiter and Schmidhuber, 1997), and a convolutional neural network model (Kim, 2014). We further introduce a neural network architecture to integrate text and meta-data. Our experiment suggests that this approach improves the performance of a strong text-only convolutional neural networks baseline. 2 LIAR : Statement: “The last quarter, it was just announced, our gross domestic product was below zero. Who ever heard of this? Its never below zero.” Speaker: Donald Trump Context: presidential announcement speech Label: Pants on Fire Justification: According to Bureau of Economic Analysis and National Bureau of Economic Research, the growth in the gross domestic product has been"
P17-2067,P09-2078,0,0.0240119,"Missing"
P17-2067,P11-1032,0,0.365371,"ar Pants on Fire”: A New Benchmark Dataset for Fake News Detection William Yang Wang Department of Computer Science University of California, Santa Barbara Santa Barbara, CA 93106 USA william@cs.ucsb.edu Abstract ing young children as sex slaves as part of a childabuse ring led by Hillary Clinton”1 . The man was later arrested by police, and he was charged for firing an assault rifle in the restaurant (Kang and Goldman, 2016). The broadly-related problem of deception detection (Mihalcea and Strapparava, 2009) is not new to the natural language processing community. A relatively early study by Ott et al. (2011) focuses on detecting deceptive review opinions in sentiment analysis, using a crowdsourcing approach to create training data for the positive class, and then combine with truthful opinions from TripAdvisor. Recent studies have also proposed stylometric (Feng et al., 2012), semi-supervised learning (Hai et al., 2016), and linguistic approaches (P´erez-Rosas and Mihalcea, 2015) to detect deceptive text on crowdsourced datasets. Even though crowdsourcing is an important approach to create labeled training data, there is a mismatch between training and testing. When testing on real-world review d"
P17-2067,D15-1133,0,0.218759,"Missing"
P17-2067,W14-2508,0,0.782041,"reate labeled training data, there is a mismatch between training and testing. When testing on real-world review datasets, the results could be suboptimal since the positive training data was created in a completely different, simulated platform. The problem of fake news detection is more challenging than detecting deceptive reviews, since the political language on TV interviews, posts on Facebook and Twitters are mostly short statements. However, the lack of manually labeled fake news dataset is still a bottleneck for advancing computational-intensive, broadcoverage models in this direction. Vlachos and Riedel (2014) are the first to release a public fake news detection and fact-checking dataset, but it only includes 221 statements, which does not permit machine learning based assessments. To address these issues, we introduce the LIAR Automatic fake news detection is a challenging problem in deception detection, and it has tremendous real-world political and social impacts. However, statistical approaches to combating fake news has been dramatically limited by the lack of labeled benchmark datasets. In this paper, we present LIAR: a new, publicly available dataset for fake news detection. We collected a"
P17-2067,D15-1306,1,0.309576,"quivalent to f-measures on this balanced dataset. Benchmark Evaluation Experimental Settings We used five baselines: a majority baseline, a regularized logistic regression classifier (LR), a support vector machine classifier (SVM) (Crammer and Singer, 2001), a bi-directional long short-term memory networks model (Bi-LSTMs) (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005), and a convolutional neural network model (CNNs) (Kim, 2014). For LR and SVM, we used the L IB S HORT T EXT toolkit6 , which was shown to provide very strong performances on short text classification problems (Wang and Yang, 2015). For Bi-LSTMs and CNNs, we used TensorFlow for the implementation. We used pretrained 300-dimensional word2vec embeddings from Google News (Mikolov et al., 2013) to warm-start the text embeddings. We strictly tuned all the hyperparameters on the validation dataset. The best filter sizes for the CNN model was (2,3,4). In all cases, each size has 128 filters. The dropout keep probabilities was optimized to 0.8, 6 Test 0.208 0.255 0.247 0.233 0.270 Table 2: The evaluation results on the LIAR dataset. The top section: text-only models. The bottom: text + meta-data hybrid models. In this section,"
P17-2067,P12-2034,0,0.11476,"ng led by Hillary Clinton”1 . The man was later arrested by police, and he was charged for firing an assault rifle in the restaurant (Kang and Goldman, 2016). The broadly-related problem of deception detection (Mihalcea and Strapparava, 2009) is not new to the natural language processing community. A relatively early study by Ott et al. (2011) focuses on detecting deceptive review opinions in sentiment analysis, using a crowdsourcing approach to create training data for the positive class, and then combine with truthful opinions from TripAdvisor. Recent studies have also proposed stylometric (Feng et al., 2012), semi-supervised learning (Hai et al., 2016), and linguistic approaches (P´erez-Rosas and Mihalcea, 2015) to detect deceptive text on crowdsourced datasets. Even though crowdsourcing is an important approach to create labeled training data, there is a mismatch between training and testing. When testing on real-world review datasets, the results could be suboptimal since the positive training data was created in a completely different, simulated platform. The problem of fake news detection is more challenging than detecting deceptive reviews, since the political language on TV interviews, post"
P17-2067,N16-1138,0,0.506675,"cometping-pong-pizza-shooting-fake-news-consequences.html 422 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 422–426 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2067 dataset, which includes 12,836 short statements labeled for truthfulness, subject, context/venue, speaker, state, party, and prior history. With such volume and a time span of a decade, LIAR is an order of magnitude larger than the currently available resources (Vlachos and Riedel, 2014; Ferreira and Vlachos, 2016) of similiar type. Additionally, in contrast to crowdsourced datasets, the instances in LIAR are collected in a grounded, more natural context, such as political debate, TV ads, Facebook posts, tweets, interview, news release, etc. In each case, the labeler provides a lengthy analysis report to ground each judgment, and the links to all supporting documents are also provided. Empirically, we have evaluated several popular learning based methods on this dataset. The baselines include logistic regression, support vector machines, long short-term memory networks (Hochreiter and Schmidhuber, 1997)"
P17-2067,D16-1187,0,0.029811,"r arrested by police, and he was charged for firing an assault rifle in the restaurant (Kang and Goldman, 2016). The broadly-related problem of deception detection (Mihalcea and Strapparava, 2009) is not new to the natural language processing community. A relatively early study by Ott et al. (2011) focuses on detecting deceptive review opinions in sentiment analysis, using a crowdsourcing approach to create training data for the positive class, and then combine with truthful opinions from TripAdvisor. Recent studies have also proposed stylometric (Feng et al., 2012), semi-supervised learning (Hai et al., 2016), and linguistic approaches (P´erez-Rosas and Mihalcea, 2015) to detect deceptive text on crowdsourced datasets. Even though crowdsourcing is an important approach to create labeled training data, there is a mismatch between training and testing. When testing on real-world review datasets, the results could be suboptimal since the positive training data was created in a completely different, simulated platform. The problem of fake news detection is more challenging than detecting deceptive reviews, since the political language on TV interviews, posts on Facebook and Twitters are mostly short s"
P18-1046,P05-1045,0,0.0269065,"the discriminator during training. The accuracy is calculated from the negative set N D . At the beginning of adversarial learning, the Evaluation and Implementation Details The Reidel dataset2 (Riedel et al., 2010) is a commonly-used distant supervision relation extraction dataset. Freebase is a huge knowledge base including billions of triples: the entity pair and the specific relationship between them. Given these triples, the sentences of each entity pair are selected from the New York Times corpus(NYT). Entity mentions of NYT corpus are recognized by the Stanford named entity recognizer (Finkel et al., 2005). There are 52 actual relationships and a special relation N A which indicates there is no relation between head and tail entities. Entity pairs of 2 Value 3, 100 50, 114042 5 1e-5, 1e-4 3 http://iesl.cs.umass.edu/riedel/ecml/ 501 https://github.com/thunlp/NRE /people/person/place_lived /business/person/company 1 1 0.97 Accuracy 0.95 0.92 0.89 0.94 0.99 0.91 0.98 Accuracy 0.98 Accuracy /location/neighborhood/neighborhood_of 1.01 1.01 0.88 0.85 0.97 0.96 0.82 0.86 0.95 0.79 0.83 0.94 0.76 0.8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0.73 1 2 3 4 5 Bag Sequence 7 8 9 10 11 12 13 14 15 16 17 0.9"
P18-1046,P05-1053,0,0.0637435,"th et al., 2013). Most of the current state-of-the-art methods (Zeng et al., 2015; Lin et al., 2016) make the denoising operation in the sentence bag of entity pair, and integrate this process into the distant supervision relation exIntroduction Relation extraction is a crucial task in the field of natural language processing (NLP). It has a wide range of applications including information retrieval, question answering, and knowledge base completion. The goal of relation extraction system is to predict relation between entity pair in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005). For exam496 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 496–505 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics describe our adversarial learning strategy in Section 3. In Section 4, we show the stability analyses of DSGAN and the empirical evaluation results. And finally, we conclude in Section 5. traction. Indeed, these methods can filter a substantial number of noise samples; However, they overlook the case that all sentences of an entity pair are false positive, which is also the c"
P18-1046,P11-1055,0,0.922036,"iven a sentence “The [owl]e1 held the mouse in its [claw]e2 .”, a relation classifier should figure out the relation Component-Whole between entity owl and claw. With the infinite amount of facts in real world, it is extremely expensive, and almost impossible for human annotators to annotate training dataset to meet the needs of all walks of life. This problem has received increasingly attention. Fewshot learning and Zero-shot Learning (Xian et al., 2017) try to predict the unseen classes with few labeled data or even without labeled data. Differently, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) is to efficiently generate relational data from plain text for unseen relations with distant supervision (DS). However, it naturally brings with some defects: the resulted distantly-supervised training samples are often very noisy (shown in Figure 1), which is the main problem of impeding the performance (Roth et al., 2013). Most of the current state-of-the-art methods (Zeng et al., 2015; Lin et al., 2016) make the denoising operation in the sentence bag of entity pair, and integrate this process into the distant supervision relation exIntroduction Relation extraction"
P18-1046,P16-1200,0,0.645499,"g (Xian et al., 2017) try to predict the unseen classes with few labeled data or even without labeled data. Differently, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) is to efficiently generate relational data from plain text for unseen relations with distant supervision (DS). However, it naturally brings with some defects: the resulted distantly-supervised training samples are often very noisy (shown in Figure 1), which is the main problem of impeding the performance (Roth et al., 2013). Most of the current state-of-the-art methods (Zeng et al., 2015; Lin et al., 2016) make the denoising operation in the sentence bag of entity pair, and integrate this process into the distant supervision relation exIntroduction Relation extraction is a crucial task in the field of natural language processing (NLP). It has a wide range of applications including information retrieval, question answering, and knowledge base completion. The goal of relation extraction system is to predict relation between entity pair in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005). For exam496 Proceedings of the 56th Annual Meeting of the Association for Com"
P18-1046,P09-1113,0,0.990085,"elation type. ple, given a sentence “The [owl]e1 held the mouse in its [claw]e2 .”, a relation classifier should figure out the relation Component-Whole between entity owl and claw. With the infinite amount of facts in real world, it is extremely expensive, and almost impossible for human annotators to annotate training dataset to meet the needs of all walks of life. This problem has received increasingly attention. Fewshot learning and Zero-shot Learning (Xian et al., 2017) try to predict the unseen classes with few labeled data or even without labeled data. Differently, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) is to efficiently generate relational data from plain text for unseen relations with distant supervision (DS). However, it naturally brings with some defects: the resulted distantly-supervised training samples are often very noisy (shown in Figure 1), which is the main problem of impeding the performance (Roth et al., 2013). Most of the current state-of-the-art methods (Zeng et al., 2015; Lin et al., 2016) make the denoising operation in the sentence bag of entity pair, and integrate this process into the distant supervision relation exIntroducti"
P18-1046,P15-2060,0,0.0345205,"ve functions and gradient calculation. Because the generator involves a discrete sampling step, we introduce a policy gradient method to calculate gradients for the generator. 3.1 3.2 Generative Adversarial Training for Distant Supervision Relation Extraction The generator and the discriminator of DSGAN are both modeled by simple CNN, because CNN performs well in understanding sentence (Zeng et al., 2014), and it has less parameters than RNNbased networks. For relation extraction, the input information consists of the sentences and entity pairs; thus, as the common setting (Zeng et al., 2014; Nguyen and Grishman, 2015), we use both word embedding and position embedding to convert input instances into continuous real-valued vectors. What we desire the generator to do is to accurately recognize true positive samples. Unlike the generator applied in computer vision field (Im et al., 2016) that generates new image from the input noise, our generator just needs to discover true positive samples from the noisy DS positive dataset. Thus, it is to realize the “sampling from a probability distribution” process of the discrete GANs (Figure 2). For a input sentence sj , we define the probability of being true positive"
P18-1046,D15-1203,0,0.858258,"d Zero-shot Learning (Xian et al., 2017) try to predict the unseen classes with few labeled data or even without labeled data. Differently, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) is to efficiently generate relational data from plain text for unseen relations with distant supervision (DS). However, it naturally brings with some defects: the resulted distantly-supervised training samples are often very noisy (shown in Figure 1), which is the main problem of impeding the performance (Roth et al., 2013). Most of the current state-of-the-art methods (Zeng et al., 2015; Lin et al., 2016) make the denoising operation in the sentence bag of entity pair, and integrate this process into the distant supervision relation exIntroduction Relation extraction is a crucial task in the field of natural language processing (NLP). It has a wide range of applications including information retrieval, question answering, and knowledge base completion. The goal of relation extraction system is to predict relation between entity pair in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005). For exam496 Proceedings of the 56th Annual Meeting of the"
P18-1046,C14-1220,0,0.883456,"Related Work To address the above-mentioned data sparsity issue, Mintz et al. (2009) first align unlabeled text corpus with Freebase by distant supervision. However, distant supervision inevitably suffers from the wrong labeling problem. Instead of explicitly removing noisy instances, the early works intend to suppress the noise. Riedel et al. (2010) adopt multi-instance single-label learning in relation extraction; Hoffmann et al. (2011) and Surdeanu et al. (2012) model distant supervision relation extraction as a multi-instance multi-label problem. Recently, some deep-learning-based models (Zeng et al., 2014; Shen and Huang, 2016) have been proposed to solve relation extraction. Naturally, some works try to alleviate the wrong labeling problem with deep learning technique, and their denoising process is integrated into relation extraction. Zeng et al. (2015) select one most plausible sentence to represent the relation between entity pairs, which inevitably misses some valuable information. Lin et al. (2016) calculate a series of soft attention weights for all sentences of one entity pair and the incorrect sentences can be down-weighted; Base on the same idea, Ji et al. (2017) bring the useful ent"
P18-1046,P15-1061,0,0.0376203,"the absence of the corresponding labeled dataset, there is not a ground-truth test dataset to evaluate the performance of distant supervision relation extraction system. Under this circumstance, the previous work adopt the held-out evaluation to evaluate their systems, which can provide an approximate measure of precision without requiring costly human evaluation. It builds a test set where entity pairs are also extracted from Freebase. Similarly, relation facts that discovered from test articles are automatically compared with those in Freebase. CNN is widely used in relation classification (Santos et al., 2015; Qin et al., 2017), thus the generator and the discriminator are both modeled as a simple CNN with the window size cw and the kernel size ck . Word embedding is directly from the released word embedding matrix by Lin et al. (2016)3 . Position embedding has the same setting with the previous works: the maximum distance of -30 and 30. Some detailed hyperparameter settings are displayed in Table 1. Experiments This paper proposes an adversarial learning strategy to detect true positive samples from the noisy distant supervision dataset. Due to the absence of supervised information, we define a g"
P18-1046,C16-1238,0,0.0142979,"ress the above-mentioned data sparsity issue, Mintz et al. (2009) first align unlabeled text corpus with Freebase by distant supervision. However, distant supervision inevitably suffers from the wrong labeling problem. Instead of explicitly removing noisy instances, the early works intend to suppress the noise. Riedel et al. (2010) adopt multi-instance single-label learning in relation extraction; Hoffmann et al. (2011) and Surdeanu et al. (2012) model distant supervision relation extraction as a multi-instance multi-label problem. Recently, some deep-learning-based models (Zeng et al., 2014; Shen and Huang, 2016) have been proposed to solve relation extraction. Naturally, some works try to alleviate the wrong labeling problem with deep learning technique, and their denoising process is integrated into relation extraction. Zeng et al. (2015) select one most plausible sentence to represent the relation between entity pairs, which inevitably misses some valuable information. Lin et al. (2016) calculate a series of soft attention weights for all sentences of one entity pair and the incorrect sentences can be down-weighted; Base on the same idea, Ji et al. (2017) bring the useful entity information into th"
P18-1046,D12-1042,0,0.946244,"wl]e1 held the mouse in its [claw]e2 .”, a relation classifier should figure out the relation Component-Whole between entity owl and claw. With the infinite amount of facts in real world, it is extremely expensive, and almost impossible for human annotators to annotate training dataset to meet the needs of all walks of life. This problem has received increasingly attention. Fewshot learning and Zero-shot Learning (Xian et al., 2017) try to predict the unseen classes with few labeled data or even without labeled data. Differently, distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) is to efficiently generate relational data from plain text for unseen relations with distant supervision (DS). However, it naturally brings with some defects: the resulted distantly-supervised training samples are often very noisy (shown in Figure 1), which is the main problem of impeding the performance (Roth et al., 2013). Most of the current state-of-the-art methods (Zeng et al., 2015; Lin et al., 2016) make the denoising operation in the sentence bag of entity pair, and integrate this process into the distant supervision relation exIntroduction Relation extraction is a crucial task in the"
P18-1046,P12-1076,0,0.183272,"5) select one most plausible sentence to represent the relation between entity pairs, which inevitably misses some valuable information. Lin et al. (2016) calculate a series of soft attention weights for all sentences of one entity pair and the incorrect sentences can be down-weighted; Base on the same idea, Ji et al. (2017) bring the useful entity information into the calculation of the attention weights. However, compared to these soft attention weight assignment strategies, recognizing the true positive samples from distant supervision dataset before relation extraction is a better choice. Takamatsu et al. (2012) build a noise-filtering strategy based on the linguistic features extracted from many NLP tools, including NER and dependency tree, which inevitably suffers the error propagation problem; while we just utilize word embedding as the input information. In this work, we learn a true-positive identifier (the generator) which is independent of the relation prediction of entity pairs, so it can be directly applied on top of any existing relation extraction classifiers. Then, we redistribute the false positive samples into the negative set, in which way to make full use of the distantly labeled reso"
P18-1053,D13-1135,0,0.468683,"e important information for interpreting the zero pronoun are called the antecedents. In recent years, deep learning models for Chinese zero pronoun resolution have been widely investigated (Chen and Ng, 2016; Yin et al., 2017a,b). These solutions concentrate on anaphoric zero pronoun resolution, applying numerous neural network models to zero pronouncandidate antecedent prediction. Neural network models have demonstrated their capabilities to learn vector-space semantics of zero pronouns and their antecedents (Yin et al., 2017a,b), and substantially surpass classic models (Zhao and Ng, 2007; Chen and Ng, 2013, 2015), obtaining stateof-the-art results on the benchmark dataset. However, these models are heavily making local coreference decisions. They simply consider the coreference chain between the zero pronoun and one single candidate antecedent one link at a time while overlooking their impacts on future decisions. Intuitively, antecedents provide key linguistic cues for explaining the zero pronoun, it is therefore reasonable to leverage useful information provided by previously predicted antecedents as cues for predicting the later zero pronoun-candidate antecedent pairs. For instance, given a"
P18-1053,P15-2053,0,0.0423527,"atures. It is the first time that machine learning techniques are applied for this task. To better explore syntactics, Kong and Zhou (2010) employed the tree kernel technique in their model. Chen and Ng (2013) extended Zhao and Ng (2007)’s model further by integrating innovative features and coreference chains between zero pronoun as bridges to find antecedents. In contrast, unsupervised techniques have been proposed and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement lea"
P18-1053,P16-1074,0,0.516974,"our technique surpasses the state-of-the-art models. 1 A zero pronoun can be an anaphoric zero pronoun if it coreferes to one or more mentions in the associated text, or unanaphoric, if there are no such mentions. In this example, the second zero pronoun “ 2 ” is anaphoric and corefers to the mention “S ã ∫ N ö /Litigant Li Yading” while the zero pronoun “ 1 ” is unanaphoric. These mentions that contain the important information for interpreting the zero pronoun are called the antecedents. In recent years, deep learning models for Chinese zero pronoun resolution have been widely investigated (Chen and Ng, 2016; Yin et al., 2017a,b). These solutions concentrate on anaphoric zero pronoun resolution, applying numerous neural network models to zero pronouncandidate antecedent prediction. Neural network models have demonstrated their capabilities to learn vector-space semantics of zero pronouns and their antecedents (Yin et al., 2017a,b), and substantially surpass classic models (Zhao and Ng, 2007; Chen and Ng, 2013, 2015), obtaining stateof-the-art results on the benchmark dataset. However, these models are heavily making local coreference decisions. They simply consider the coreference chain between t"
P18-1053,D16-1245,0,0.0652603,"llconnected hidden layers and one sof tmax layer, the agent maps the state vector into the probability distribution over actions that indicates the coreference likelihood of the input zero pronouncandidate antecedent pair. 1 (st ) + bi ) where p(a|zp, npt ; ✓) indicates the probability of selecting action a. Intuitively, the estimation of the gradient might have very high variance. One commonly used remedy to reduce the variance is to subtract a baseline value b from the reward. Hence, we utilize the gradient estimate as follows: r✓ J(✓) = r✓ X log p(a|zp, npt ; ✓)(R(at ) bt ) t (6) Following Clark and Manning (2016), we intorduce the baseline b and get the value of bt at time t by Eat0 ⇠p R(a1 , ..., at0 , ..., aT ). 2.3 Pretraining Pretraining is crucial in reinforcement learning techniques (Clark and Manning, 2016; Xiong et al., 2017). In this work, we pretrain the model by using the loss function from Yin et al. (2017a): (2) where Wi and bi are the parameters of the ith hidden layer; si represents the state vector. After going through all the layers, we can get the representative vector for the zero pronoun-candidate antecedent pair (zp, npt ). We then feed it into a scoring-layer to get their corefer"
P18-1053,P06-1079,0,0.101425,"iques have been proposed and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradient algorithm to trai"
P18-1053,P11-1081,0,0.0509601,"ind antecedents. In contrast, unsupervised techniques have been proposed and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They ut"
P18-1053,I11-1085,0,0.0228345,"and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradient algorithm to train the model and achieves better re"
P18-1053,D15-1260,0,0.0238565,"Missing"
P18-1053,D17-1060,1,0.907609,"ormation of current candidate antecedent and 3) antecedent information generated by antecedents predicted in previous states. In particular, our reinforcement learning agent is designed as a policy network ⇡✓ (s, a) = p(a|s; ✓), where s represents the state; a indicates the action and ✓ represents the parameters of the model. The parameters ✓ are trained using stochastic gradient descent. Compared with Deep Q-Network (Mnih et al., 2013) that commonly learns a greedy policy, policy network is able to learn a stochastic policy that prevents the agent from getting stuck at an intermediate state (Xiong et al., 2017). Additionally, the learned policy is more explainable, comparing to learned value functions in Deep Q-Network. We here introduce the definitions of components of our reinforcement learning model, namely, state, action 571 and reward. 2.1.1 any function. To encourage the agent to find accurate antecedents, we regard the F-score for the selected antecedents as the reward for each action in a path. State Given a zero pronoun zp with its representation vzp and all of its candidate antecedents representations {vnp1 , vnp2 , ..., vnpn }, our model generate coreference decisions for zero pronoun-can"
P18-1053,D16-1132,0,0.0721111,"Missing"
P18-1053,D17-1135,1,0.393412,"sses the state-of-the-art models. 1 A zero pronoun can be an anaphoric zero pronoun if it coreferes to one or more mentions in the associated text, or unanaphoric, if there are no such mentions. In this example, the second zero pronoun “ 2 ” is anaphoric and corefers to the mention “S ã ∫ N ö /Litigant Li Yading” while the zero pronoun “ 1 ” is unanaphoric. These mentions that contain the important information for interpreting the zero pronoun are called the antecedents. In recent years, deep learning models for Chinese zero pronoun resolution have been widely investigated (Chen and Ng, 2016; Yin et al., 2017a,b). These solutions concentrate on anaphoric zero pronoun resolution, applying numerous neural network models to zero pronouncandidate antecedent prediction. Neural network models have demonstrated their capabilities to learn vector-space semantics of zero pronouns and their antecedents (Yin et al., 2017a,b), and substantially surpass classic models (Zhao and Ng, 2007; Chen and Ng, 2013, 2015), obtaining stateof-the-art results on the benchmark dataset. However, these models are heavily making local coreference decisions. They simply consider the coreference chain between the zero pronoun an"
P18-1053,W03-1024,0,0.0976102,"trast, unsupervised techniques have been proposed and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradien"
P18-1053,D10-1086,0,0.401465,"s, our model learns to predict potential antecedents incrementally, selecting global-optimal antecedents in a sequential manner. In the end, our model successfully predicts “y/She” as the result. 4 4.1 Related Work Zero Pronoun Resolution A wide variety of techniques for machine learning models for Chinese zero pronoun resolution have been proposed. Zhao and Ng (2007) utilized the decision tree to learn the anaphoric zero pronoun resolver by using syntactical and positional features. It is the first time that machine learning techniques are applied for this task. To better explore syntactics, Kong and Zhou (2010) employed the tree kernel technique in their model. Chen and Ng (2013) extended Zhao and Ng (2007)’s model further by integrating innovative features and coreference chains between zero pronoun as bridges to find antecedents. In contrast, unsupervised techniques have been proposed and shown their efficiency. Chen and Ng (2014) proposed an unsupervised model, where a model trained on manually resolved pronoun was employed for the resolution of zero pronoun. Chen and Ng (2015) proposed an unsupervised anaphoric zero pronoun resolver, using the salience model to deal with the issue. Besides, ther"
P18-1053,D07-1057,0,0.897073,"ons that contain the important information for interpreting the zero pronoun are called the antecedents. In recent years, deep learning models for Chinese zero pronoun resolution have been widely investigated (Chen and Ng, 2016; Yin et al., 2017a,b). These solutions concentrate on anaphoric zero pronoun resolution, applying numerous neural network models to zero pronouncandidate antecedent prediction. Neural network models have demonstrated their capabilities to learn vector-space semantics of zero pronouns and their antecedents (Yin et al., 2017a,b), and substantially surpass classic models (Zhao and Ng, 2007; Chen and Ng, 2013, 2015), obtaining stateof-the-art results on the benchmark dataset. However, these models are heavily making local coreference decisions. They simply consider the coreference chain between the zero pronoun and one single candidate antecedent one link at a time while overlooking their impacts on future decisions. Intuitively, antecedents provide key linguistic cues for explaining the zero pronoun, it is therefore reasonable to leverage useful information provided by previously predicted antecedents as cues for predicting the later zero pronoun-candidate antecedent pairs. For"
P18-1053,D16-1127,0,0.0241677,"extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradient algorithm to train the model and achieves better results compared with the counterpart neural network model. Narasimhan et al. (2016) introduced a deep Q-learning based slot-filling technique, where the agent’s action is to retrieve or reconcile content from a new document. Xiong et al. (2017) proposed an innovative reinforcement learning framework for learning multi-"
P18-1053,P17-1010,1,0.821947,"hm and “Pre” presents the model without reinforcement learning. “dev” shows the performance of our reinforcement learning model on the development dataset. 3.3 Case Study Lastly, we show a case to illustrate the effectiveness of our proposed model, as is shown in Figure 4. In this case, we can see that our model correctly predict mentions “£✏W/The Xiaohui” 575 那 小穗 她 本来 就是 好 , , been extensively studied for zero pronoun resolution. Chen and Ng (2016) introduced a deep neural network resolver for this task. In their work, zero pronoun and candidates are encoded by a feedforward neural network. Liu et al. (2017) explored to produce pseudo dataset for anaphoric zero pronoun resolution. They trained their deep learning model by adopting a two-step learning method that overcomes the discrepancy between the generated pseudo dataset and the real one. To better utilize vector-space semantics, Yin et al. (2017b) employed recurrent neural network to encode zero pronoun and antecedents. In particular, a twolayer antecedent encoder was employed to generate the hierarchical representation of antecedents. Yin et al. (2017a) developed an innovative deep memory network resolver, where zero pronouns are encoded by"
P18-1053,D15-1001,0,0.0198445,"Besides, there has been extensive work on zero anaphora for other languages. Efforts for zero pronoun resolution fall into two major categories, namely, (1) heuristic techniques (Han, 2006); and (2) learning-based models (Iida and Poesio, 2011; Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011; Iida and Poesio, 2011; Iida et al., 2015, 2016). In recent years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradient algorithm to train the model and achieves better results compared with the counterpart neural network model. Narasimhan et al. (2016) introduced a deep Q-learning based slot-filling technique, where the agent’s action is to retrieve or reconcile content from a new document. Xiong et al. (2017) proposed an innovative reinforcement learning framework f"
P18-1053,D16-1261,0,0.0376312,"nt years, deep learning techniques have 4.2 Deep Reinforcement Learning Recent advances in deep reinforcement learning have shown promise results in a variety of natural language processing tasks (Branavan et al., 2012; Narasimhan et al., 2015; Li et al., 2016). In recent time, Clark and Manning (2016) proposed a deep reinforcement learning model for coreference resolution, where an agent is utilized for linking mentions to their potential antecedents. They utilized the policy gradient algorithm to train the model and achieves better results compared with the counterpart neural network model. Narasimhan et al. (2016) introduced a deep Q-learning based slot-filling technique, where the agent’s action is to retrieve or reconcile content from a new document. Xiong et al. (2017) proposed an innovative reinforcement learning framework for learning multi-hop relational paths. Deep reinforcement learning is a natural choice for tasks that require making incremental decisions. By combin576 ing non-linear function approximations with reinforcement learning, the deep reinforcement learning paradigm can integrate vector-space semantic into a robust joint learning and reasoning process. Moreover, by optimizing the po"
P18-1053,D17-1035,0,0.0195247,"nforcement learning. We can see from the table that our model with reinforcement learning achieves better performance than the model without this all across the board. With the help of reinforcement learning, our model learns to choose effective actions in sequential decisions. It empowers the model to directly optimize the overall evaluation metrics, which brings a more effective and natural way of dealing with the task. Moreover, by seeing that the performance on development dataset stops increasing with iterations bigger than 70, we therefore set the pretraining iterations to 70. Following Reimers and Gurevych (2017), to illustrate the impact of randomness in our reinforcement learning model, we run our model with different random seed values. Table 4 shows the performance of our model with different random seeds on the test dataset. We report the minimum, the maximum, the median F-scores results and the standard deviation of F-scores. We run Moreover, on purpose of better illustrating the effectiveness of the proposed reinforcement learning model, we run a set of experiments with different settings. In particular, we compare the model with and without the proposed reinforcement learning process using dif"
P18-1083,W05-0909,0,0.674381,"these methods are trained by maximizing the likelihood of the observed data pairs, they are restricted to generate simple and plain description with limited expressive patterns. In order to cope with the challenges and produce more human-like descriptions, Rennie et al. (2016) have proposed a reinforcement learning framework. However, in the scenario of visual storytelling, the common reinforced captioning methods are facing great challenges since the hand-crafted rewards based on string matches are either too biased or too sparse to drive the policy search. For instance, we used the METEOR (Banerjee and Lavie, 2005) score as the reward to reinforce our policy and found that though the METEOR score is significantly improved, the other scores are severely harmed. Here we showcase an adversarial example with an average METEOR score as high as 40.2: • We propose an adversarial reward learning framework and apply it to boost visual story generation. • We evaluate our approach on the Visual Storytelling (VIST) dataset and achieve the state-of-the-art results on automatic metrics. • We empirically demonstrate that automatic metrics are not perfect for either training or evaluation. We had a great time to have a"
P18-1083,W17-5534,0,0.0365203,"Missing"
P18-1083,E06-1032,0,0.0644834,"utomatic Metrics Automatic metrics, including BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely applied to the sequence generation tasks. Using automatic metrics can ensure rapid prototyping and testing new models with fewer expensive human evaluation. However, they have been criticized to be biased and correlate poorly with human judgments, especially in many generative tasks like response generation (Lowe et al., 2017; Liu et al., 2016), dialogue system (Bruni and Fern´andez, 2017) and machine translation (Callison-Burch et al., 2006). The naive overlap-counting methods are not able to reflect many semantic properties in natural language, such as coherence, expressiveness, etc. min max Images Images references 3.1 [log D(x)] + E [log D(G(z))] , z∼pz where G is the generator and D is the discriminator, and z is the latent variable. Recently, GAN has quickly been adopted to tackle discrete problems (Yu et al., 2017a; Dai et al., 2017; Wang et al., 2018a). The basic idea is to use Monte Carlo policy gradient estimation (Williams, 1992) to update the parameters of the generator. 901 Our Approach Problem Statement Here we consi"
P18-1083,D14-1181,0,0.00429273,"Missing"
P18-1083,N18-1154,1,0.808523,"tories (Huang et al., 2016). Yu et al. (2017b) proposes a multi-task learning algorithm for both album summarization and paragraph generation, achieving the best results on the VIST dataset. But these methods are still based on behavioral cloning and lack the ability to generate more structured stories. 900 Reinforcement Learning in Sequence Generation Recently, reinforcement learning (RL) has gained its popularity in many sequence generation tasks such as machine translation (Bahdanau et al., 2016), visual captioning (Ren et al., 2017; Wang et al., 2018b), summarization (Paulus et al., 2017; Chen et al., 2018), etc. The common wisdom of using RL is to view generating a word as an action and aim at maximizing the expected return by optimizing its policy. As pointed in (Ranzato et al., 2015), traditional maximum likelihood algorithm is prone to exposure bias and label bias, while the RL agent exposes the generative model to its own distribution and thus can perform better. But these works usually utilize hand-crafted metric scores as the reward to optimize the model, which fails to learn more implicit semantics due to the limitations of automatic metrics. Environment Adversarial Objective Reward G E"
P18-1083,D16-1230,0,0.0206962,"ning two models to play a min-max two-player game: D Reward Model Inverse RL Rethinking Automatic Metrics Automatic metrics, including BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely applied to the sequence generation tasks. Using automatic metrics can ensure rapid prototyping and testing new models with fewer expensive human evaluation. However, they have been criticized to be biased and correlate poorly with human judgments, especially in many generative tasks like response generation (Lowe et al., 2017; Liu et al., 2016), dialogue system (Bruni and Fern´andez, 2017) and machine translation (Callison-Burch et al., 2006). The naive overlap-counting methods are not able to reflect many semantic properties in natural language, such as coherence, expressiveness, etc. min max Images Images references 3.1 [log D(x)] + E [log D(G(z))] , z∼pz where G is the generator and D is the discriminator, and z is the latent variable. Recently, GAN has quickly been adopted to tackle discrete problems (Yu et al., 2017a; Dai et al., 2017; Wang et al., 2018a). The basic idea is to use Monte Carlo policy gradient estimation (William"
P18-1083,P17-1103,0,0.0230201,"by alternately training two models to play a min-max two-player game: D Reward Model Inverse RL Rethinking Automatic Metrics Automatic metrics, including BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely applied to the sequence generation tasks. Using automatic metrics can ensure rapid prototyping and testing new models with fewer expensive human evaluation. However, they have been criticized to be biased and correlate poorly with human judgments, especially in many generative tasks like response generation (Lowe et al., 2017; Liu et al., 2016), dialogue system (Bruni and Fern´andez, 2017) and machine translation (Callison-Burch et al., 2006). The naive overlap-counting methods are not able to reflect many semantic properties in natural language, such as coherence, expressiveness, etc. min max Images Images references 3.1 [log D(x)] + E [log D(G(z))] , z∼pz where G is the generator and D is the discriminator, and z is the latent variable. Recently, GAN has quickly been adopted to tackle discrete problems (Yu et al., 2017a; Dai et al., 2017; Wang et al., 2018a). The basic idea is to use Monte Carlo policy gradient"
P18-1083,P02-1040,0,0.102086,"using Boltzmann distribution pθ (x) ∝ exp(−Eθ (x)). Inspired by these methods, we propose a practical AREL approach for visual storytelling to uncover a robust reward function from human demonstrations and thus help produce human-like stories. 3 Generative Adversarial Network Generative adversarial network (GAN) (Goodfellow et al., 2014) is a very popular approach for estimating intractable probabilities, which sidestep the difficulty by alternately training two models to play a min-max two-player game: D Reward Model Inverse RL Rethinking Automatic Metrics Automatic metrics, including BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004), have been widely applied to the sequence generation tasks. Using automatic metrics can ensure rapid prototyping and testing new models with fewer expensive human evaluation. However, they have been criticized to be biased and correlate poorly with human judgments, especially in many generative tasks like response generation (Lowe et al., 2017; Liu et al., 2016), dialogue system (Bruni and Fern´andez, 2017) and machine translation (Callison-Burch et al., 2006). The naive overlap-counting methods are not a"
P18-1083,N18-2125,1,0.86793,"Missing"
P18-1083,D17-1101,0,0.100846,"Missing"
P18-1104,W16-6208,0,0.0646034,"Missing"
P18-1104,D17-1169,0,0.384232,"control the target emotion of the response, we investigate several encoder-decoder generation models, including a standard attention-based SEQ 2 SEQ model as the base model, and a more sophisticated CVAE model (Kingma and Welling, 2013; Sohn et al., 2015), as VAE is recently found convenient in dialog generation (Zhao et al., 2017). To explicitly improve emotion expression, we then experiment with several extensions to the CVAE model, including a hybrid objective with policy gradient. The performance in emotion expression is automatically evaluated by a separate sentence-to-emoji classifier (Felbo et al., 2017). Additionally, we conducted a human evaluation to assess the quality of the generated emotional text. Results suggest that our method is capable of generating state-of-the-art emotional text at scale. Our main contributions are three-fold: • We provide a publicly available, large-scale dataset of Twitter conversation-pairs naturally labeled with fine-grained emojis. • We are the first to use naturally labeled emojis for conducting large-scale emotional response generation for dialog. • We apply several state-of-the-art generative models to train an emotional response generation system, and an"
P18-1104,D17-2013,0,0.0391785,"ational Linguistics (Long Papers), pages 1128–1137 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics are able to imitate Twitter users’ language style when using those emojis, we claim that, to some extent, we have captured those emotions. Using a large collection of Twitter conversations, we then trained a conditional generative model to automatically generate the emotional responses. Figure 1 shows an example. To generate emotional responses in dialogs, another technical challenge is to control the target emotion labels. In contrast to existing work (Huang et al., 2017) that uses information retrieval to generate emotional responses, the research question we are pursuing in this paper, is to design novel techniques that can generate abstractive responses of any given arbitrary emotions, without having human annotators to label a huge amount of training data. To control the target emotion of the response, we investigate several encoder-decoder generation models, including a standard attention-based SEQ 2 SEQ model as the base model, and a more sophisticated CVAE model (Kingma and Welling, 2013; Sohn et al., 2015), as VAE is recently found convenient in dialog"
P18-1104,D16-1127,0,0.260059,"Missing"
P18-1104,D17-1230,0,0.321851,"onal agents, machines must also have the ability to learn to generate emotional sentences. One of the major challenges is the lack of largescale, manually labeled emotional text datasets. Due to the cost and complexity of manual annotation, most prior research studies primarily focus on small-sized labeled datasets (Pang et al., 2002; Maas et al., 2011; Socher et al., 2013), which are not ideal for training deep learning models with a large number of parameters. In recent years, a handful of medium to large scale, emotional corpora in the area of emotion analysis (Go et al., 2016) and dialog (Li et al., 2017b) are proposed. However, all of them are limited to a traditional, small set of labels, for example, “happiness,” “sadness,” “anger,” etc. or simply binary “positive” and “negative.” Such coarse-grained classification labels make it difficult to capture the nuances of human emotion. To avoid the cost of human annotation, we propose the use of naturally-occurring emoji-rich Twitter data. We construct a dataset using Twitter conversations with emojis in the response. The fine-grained emojis chosen by the users in the response can be seen as the natural label for the emotion of the response. We"
P18-1104,I17-1099,0,0.143856,"onal agents, machines must also have the ability to learn to generate emotional sentences. One of the major challenges is the lack of largescale, manually labeled emotional text datasets. Due to the cost and complexity of manual annotation, most prior research studies primarily focus on small-sized labeled datasets (Pang et al., 2002; Maas et al., 2011; Socher et al., 2013), which are not ideal for training deep learning models with a large number of parameters. In recent years, a handful of medium to large scale, emotional corpora in the area of emotion analysis (Go et al., 2016) and dialog (Li et al., 2017b) are proposed. However, all of them are limited to a traditional, small set of labels, for example, “happiness,” “sadness,” “anger,” etc. or simply binary “positive” and “negative.” Such coarse-grained classification labels make it difficult to capture the nuances of human emotion. To avoid the cost of human annotation, we propose the use of naturally-occurring emoji-rich Twitter data. We construct a dataset using Twitter conversations with emojis in the response. The fine-grained emojis chosen by the users in the response can be seen as the natural label for the emotion of the response. We"
P18-1104,D15-1166,0,0.0201431,"istribution of emoji labels within the corpus is presented in Table 1. 4 Generative Models In this work, our goal is to generate emotional responses to tweets with the emotion specified by an emoji label. We assembled several generative models and trained them on our dataset. 4.1 Base: Attention-Based Sequence-to-Sequence Model Traditional studies use deep recurrent architecture and encoder-decoder models to generate conversation responses, mapping original texts to target responses. Here we use a sequence-to-sequence (SEQ 2 SEQ) model (Sutskever et al., 2014) with global attention mechanism (Luong et al., 2015) as our base model (See Figure 3). We use randomly initialized embedding vectors to represent each word. To specifically model the Figure 3: From bottom to top is a forward pass of data during training. Left: the base model encodes the original tweets in vo , and generates responses by decoding from the concatenation of vo and the embedded emoji, ve . Right: In the CVAE model, all additional components (outlined in gray) can be added incrementally to the base model. A separate encoder encodes the responses in x. Recognition network inputs x and produces the latent variable z by reparameterizat"
P18-1104,P11-1015,0,0.081944,"ve and generate human emotions. In the past decade, there has been significant progress in sentiment analysis (Pang et al., 2002, 2008; Liu, 2012) and natural language understanding—e.g., classifying the sentiment of online reviews. To build empathetic conversational agents, machines must also have the ability to learn to generate emotional sentences. One of the major challenges is the lack of largescale, manually labeled emotional text datasets. Due to the cost and complexity of manual annotation, most prior research studies primarily focus on small-sized labeled datasets (Pang et al., 2002; Maas et al., 2011; Socher et al., 2013), which are not ideal for training deep learning models with a large number of parameters. In recent years, a handful of medium to large scale, emotional corpora in the area of emotion analysis (Go et al., 2016) and dialog (Li et al., 2017b) are proposed. However, all of them are limited to a traditional, small set of labels, for example, “happiness,” “sadness,” “anger,” etc. or simply binary “positive” and “negative.” Such coarse-grained classification labels make it difficult to capture the nuances of human emotion. To avoid the cost of human annotation, we propose the"
P18-1104,W02-1011,0,0.0317885,"nvestigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate highquality abstractive conversation responses in accordance with designated emotions. 1 Introduction A critical research problem for artificial intelligence is to design intelligent agents that can perceive and generate human emotions. In the past decade, there has been significant progress in sentiment analysis (Pang et al., 2002, 2008; Liu, 2012) and natural language understanding—e.g., classifying the sentiment of online reviews. To build empathetic conversational agents, machines must also have the ability to learn to generate emotional sentences. One of the major challenges is the lack of largescale, manually labeled emotional text datasets. Due to the cost and complexity of manual annotation, most prior research studies primarily focus on small-sized labeled datasets (Pang et al., 2002; Maas et al., 2011; Socher et al., 2013), which are not ideal for training deep learning models with a large number of parameters"
P18-1104,D13-1170,0,0.00480102,"an emotions. In the past decade, there has been significant progress in sentiment analysis (Pang et al., 2002, 2008; Liu, 2012) and natural language understanding—e.g., classifying the sentiment of online reviews. To build empathetic conversational agents, machines must also have the ability to learn to generate emotional sentences. One of the major challenges is the lack of largescale, manually labeled emotional text datasets. Due to the cost and complexity of manual annotation, most prior research studies primarily focus on small-sized labeled datasets (Pang et al., 2002; Maas et al., 2011; Socher et al., 2013), which are not ideal for training deep learning models with a large number of parameters. In recent years, a handful of medium to large scale, emotional corpora in the area of emotion analysis (Go et al., 2016) and dialog (Li et al., 2017b) are proposed. However, all of them are limited to a traditional, small set of labels, for example, “happiness,” “sadness,” “anger,” etc. or simply binary “positive” and “negative.” Such coarse-grained classification labels make it difficult to capture the nuances of human emotion. To avoid the cost of human annotation, we propose the use of naturally-occur"
P18-1104,P17-1061,0,0.478617,"formation retrieval to generate emotional responses, the research question we are pursuing in this paper, is to design novel techniques that can generate abstractive responses of any given arbitrary emotions, without having human annotators to label a huge amount of training data. To control the target emotion of the response, we investigate several encoder-decoder generation models, including a standard attention-based SEQ 2 SEQ model as the base model, and a more sophisticated CVAE model (Kingma and Welling, 2013; Sohn et al., 2015), as VAE is recently found convenient in dialog generation (Zhao et al., 2017). To explicitly improve emotion expression, we then experiment with several extensions to the CVAE model, including a hybrid objective with policy gradient. The performance in emotion expression is automatically evaluated by a separate sentence-to-emoji classifier (Felbo et al., 2017). Additionally, we conducted a human evaluation to assess the quality of the generated emotional text. Results suggest that our method is capable of generating state-of-the-art emotional text at scale. Our main contributions are three-fold: • We provide a publicly available, large-scale dataset of Twitter conversa"
P18-1199,D17-1063,0,0.030306,"rom the dynamic interaction between these two parts (Arulkumaran et al., 2017). First, the prerequisite of reinforcement learning is that the external environment should be modeled as a Markov decision process (MDP). However, the traditional setting of relation extraction cannot satisfy this condition: the input sentences are independent of each other. In other words, we cannot merely use the information of the sentence being processed as the state. Thus, we add the information from the early states into the representation of the current state, in which way to model our task as a MDP problem (Fang et al., 2017). The other component, RL agent, is parameterized with a policy network πθ (s, a) = p(a|s; θ). The probability distribution of actions A = {aremove , aremain } is calculated by policy network based on state vectors. What needs to be noted is that, Deep Q Network (DQN) (Mnih et al., 2013) is also a widelyused RL method; however, it is not suitable for our case, even if our action space is small. First, we cannot compute the immediate reward for every operation; In contrast, the accurate reward can only be obtained after finishing processing the whole training dataset. Second, the stochastic pol"
P18-1199,P05-1045,0,0.0564392,"icy-based RL method to generate a series of relation indicators and use them to re2142 distribute training dataset by moving false positive samples to negative sample set. Therefore, our experiments are intended to demonstrate that our RL agents possess this capability. 4.1 Datast and Evaluation Metrics We evaluate the proposed method on a commonlyused dataset2 , which is first presented in Riedel et al. (2010). This dataset is generated by aligning entity pairs from Freebase with New York Times corpus(NYT). Entity mentions of NYT corpus are recognized by the Stanford named entity recognizer (Finkel et al., 2005). The sentences from the years 2005-2006 are used as the training corpus and sentences from 2007 are used as the testing corpus. There are 52 actual relations and a special relation N A which indicates there is no relation between the head and tail entities. The sentences of N A are from the entity pairs that exist in the same sentence of the actual relations but do not appear in the Freebase. Similar to the previous works, we adopt the held-out evaluation to evaluate our model, which can provide an approximate measure of the classification ability without costly human evaluation. Similar to t"
P18-1199,P05-1053,0,0.22463,"rried to Michelle Obama.”, a relation classifier aims at predicting the relation of “spouse”. In downstream applications, relation extraction is the key module for constructing knowledge graphs, and it is a vital component of many natural language processing applications such as structured search, sentiment analysis, question answering, and summarization. Introduction Relation extraction is a core task in information extraction and natural language understanding. The goal of relation extraction is to predict relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005). For example, given a sentence A major issue encountered in the early development of relation extraction algorithms is the data sparsity issue—It is extremely expensive, and almost impossible for human annotators to go through a large corpus of millions of sentences to provide a large amount of labeled training instances. Therefore, distant supervision relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) becomes popular, because it uses entity pairs from knowledge bases to select a set of noisy instances from unlabeled data. In recent years, neural network ap"
P18-1199,W09-2415,0,0.0609166,"Missing"
P18-1199,P11-1055,0,0.958418,"on extraction and natural language understanding. The goal of relation extraction is to predict relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005). For example, given a sentence A major issue encountered in the early development of relation extraction algorithms is the data sparsity issue—It is extremely expensive, and almost impossible for human annotators to go through a large corpus of millions of sentences to provide a large amount of labeled training instances. Therefore, distant supervision relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) becomes popular, because it uses entity pairs from knowledge bases to select a set of noisy instances from unlabeled data. In recent years, neural network approaches (Zeng et al., 2014, 2015) have been proposed to train the relation extractor under these noisy conditions. To suppress the noisy(Roth et al., 2013), recent stud2137 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2137–2147 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ies (Lin et al., 2016) have propos"
P18-1199,P15-2060,0,0.0278513,"c policy of the policy network is capable of preventing the agent from getting stuck in an intermediate state. The following subsections detailedly introduce the definitions of the fundamental components in the proposed RL method. States In order to satisfy the condition of MDP, the state s includes the information from the current sentence and the sentences that have been removed in early states. The semantic and syntactic information of sentence is represented by a continuous real-valued vector. According to some state-of-the-art supervised relation extraction approaches (Zeng et al., 2014; Nguyen and Grishman, 2015), we utilize both word embedding and position embedding to convert sentence into vector. With this sentence vector, the current state is the concatenation of the current sentence vector and the average vector of the removed sentences in early states. We give relatively larger weight for the vector of the current sentence, in which way to magnify the dominating influence of the current sentence information for the decision of action. Actions At each step, our agent is required to determine whether the instance is false positive for target relation type. Each relation type has a agent1 . There a"
P18-1199,P15-1061,0,0.036274,"ntinuous space, which is more reasonable than a binary reward (−1 and 1), because this setting can reflect the number of wrong-labeled instance that the agent has removed. In order to avoid the randomness of F1 , we use the average F1 of last five epochs to calculate the reward. Policy Network For each input sentence, our policy network is to determine whether it expresses the target relation type and then make removal action if it is irrelevant to the target relation type. Thus, it is analogous to a binary relation classifier. CNN is commonly used to construct relation classification system (Santos et al., 2015; Xu et al., 2015; Shen and Huang, 2016), so we adopt a simple CNN with window size cw and kernel size ck , to model policy network π(s; θ). The reason why we do not choice the variants of CNN (Zeng et al., 2015; Lin et al., 2016) that are well-designed for distant supervision is that these two models belong to bag-level models (dealing with a bag of sentences simultaneously) and deal with the multi-classification problem; We just need a model to do binary sentencelevel classification. Naturally, the simpler network is adopted. 3.1 Training Policy-based Agent Unlike the goal of distant supervi"
P18-1199,C16-1238,0,0.0156937,"le than a binary reward (−1 and 1), because this setting can reflect the number of wrong-labeled instance that the agent has removed. In order to avoid the randomness of F1 , we use the average F1 of last five epochs to calculate the reward. Policy Network For each input sentence, our policy network is to determine whether it expresses the target relation type and then make removal action if it is irrelevant to the target relation type. Thus, it is analogous to a binary relation classifier. CNN is commonly used to construct relation classification system (Santos et al., 2015; Xu et al., 2015; Shen and Huang, 2016), so we adopt a simple CNN with window size cw and kernel size ck , to model policy network π(s; θ). The reason why we do not choice the variants of CNN (Zeng et al., 2015; Lin et al., 2016) that are well-designed for distant supervision is that these two models belong to bag-level models (dealing with a bag of sentences simultaneously) and deal with the multi-classification problem; We just need a model to do binary sentencelevel classification. Naturally, the simpler network is adopted. 3.1 Training Policy-based Agent Unlike the goal of distant supervision relation extraction, our agent is t"
P18-1199,D12-1042,0,0.928909,"al language understanding. The goal of relation extraction is to predict relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005). For example, given a sentence A major issue encountered in the early development of relation extraction algorithms is the data sparsity issue—It is extremely expensive, and almost impossible for human annotators to go through a large corpus of millions of sentences to provide a large amount of labeled training instances. Therefore, distant supervision relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) becomes popular, because it uses entity pairs from knowledge bases to select a set of noisy instances from unlabeled data. In recent years, neural network approaches (Zeng et al., 2014, 2015) have been proposed to train the relation extractor under these noisy conditions. To suppress the noisy(Roth et al., 2013), recent stud2137 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2137–2147 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ies (Lin et al., 2016) have proposed the use of attention"
P18-1199,P16-1200,0,0.34163,", 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) becomes popular, because it uses entity pairs from knowledge bases to select a set of noisy instances from unlabeled data. In recent years, neural network approaches (Zeng et al., 2014, 2015) have been proposed to train the relation extractor under these noisy conditions. To suppress the noisy(Roth et al., 2013), recent stud2137 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2137–2147 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ies (Lin et al., 2016) have proposed the use of attention mechanisms to place soft weights on a set of noisy sentences, and select samples. However, we argue that only selecting one example or based on soft attention weights are not the optimal strategy: To improve the robustness, we need a systematic solution to make use of more instances, while removing false positives and placing them in the right place. In this paper, we investigate the possibility of using dynamic selection strategies for robust distant supervision. More specifically, we design a deep reinforcement learning agent, whose goal is to learn to cho"
P18-1199,D15-1062,0,0.0380208,"Missing"
P18-1199,P09-1113,0,0.986501,"re task in information extraction and natural language understanding. The goal of relation extraction is to predict relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005). For example, given a sentence A major issue encountered in the early development of relation extraction algorithms is the data sparsity issue—It is extremely expensive, and almost impossible for human annotators to go through a large corpus of millions of sentences to provide a large amount of labeled training instances. Therefore, distant supervision relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) becomes popular, because it uses entity pairs from knowledge bases to select a set of noisy instances from unlabeled data. In recent years, neural network approaches (Zeng et al., 2014, 2015) have been proposed to train the relation extractor under these noisy conditions. To suppress the noisy(Roth et al., 2013), recent stud2137 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2137–2147 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ies (Lin et"
P18-1199,D15-1203,0,0.670838,"we use the average F1 of last five epochs to calculate the reward. Policy Network For each input sentence, our policy network is to determine whether it expresses the target relation type and then make removal action if it is irrelevant to the target relation type. Thus, it is analogous to a binary relation classifier. CNN is commonly used to construct relation classification system (Santos et al., 2015; Xu et al., 2015; Shen and Huang, 2016), so we adopt a simple CNN with window size cw and kernel size ck , to model policy network π(s; θ). The reason why we do not choice the variants of CNN (Zeng et al., 2015; Lin et al., 2016) that are well-designed for distant supervision is that these two models belong to bag-level models (dealing with a bag of sentences simultaneously) and deal with the multi-classification problem; We just need a model to do binary sentencelevel classification. Naturally, the simpler network is adopted. 3.1 Training Policy-based Agent Unlike the goal of distant supervision relation extraction, our agent is to determine whether an annotated sentence expresses the target relation type rather than predict the relationship of entity pair, so sentences are treated independently de"
P18-1199,C14-1220,0,0.847545,"ple, given a sentence A major issue encountered in the early development of relation extraction algorithms is the data sparsity issue—It is extremely expensive, and almost impossible for human annotators to go through a large corpus of millions of sentences to provide a large amount of labeled training instances. Therefore, distant supervision relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012) becomes popular, because it uses entity pairs from knowledge bases to select a set of noisy instances from unlabeled data. In recent years, neural network approaches (Zeng et al., 2014, 2015) have been proposed to train the relation extractor under these noisy conditions. To suppress the noisy(Roth et al., 2013), recent stud2137 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2137–2147 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ies (Lin et al., 2016) have proposed the use of attention mechanisms to place soft weights on a set of noisy sentences, and select samples. However, we argue that only selecting one example or based on soft attention weights are not the optima"
P18-5007,D17-1106,0,0.0203824,", Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (Wang et al., 2018b),"
P18-5007,N18-1113,1,0.878952,"Missing"
P18-5007,D17-1060,1,0.831141,"2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical deep reinforcement learning for video captioning (Wang et al., 2018b), discussing the techniques of leveraging hierarchies in DRL for NLP generation problems. This tutorial aims at introducing deep reinforcement learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in reinforcement learning. The intended length of the tutorial is 3 hours, including a coffee break. Many Natural Language Processing (NLP) tasks (including generation, language grounding, reasoning, information extraction, coreferenc"
P18-5007,D15-1001,0,0.0312245,"forcement Learning for NLP William Yang Wang UC Santa Barbara william@cs.ucsb.edu Jiwei Li Shannon.ai jiwei li@shannonai.com Abstract their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we d"
P18-5007,P18-1053,1,0.885681,"Missing"
P18-5007,D16-1261,0,0.0216518,"om Abstract their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study"
P18-5007,P18-1104,1,0.822012,"b.edu Jiwei Li Shannon.ai jiwei li@shannonai.com Abstract their modern deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong"
P18-5007,D17-1103,0,0.0282946,"etworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical deep reinforcement learning for video captioning ("
P18-5007,P18-1199,1,0.823639,"deep learning extensions such as Deep QNetworks (Mnih et al., 2015), Policy Networks (Silver et al., 2016), and Deep Hierarchical Reinforcement Learning (Kulkarni et al., 2016). We outline the applications of deep reinforcement learning in NLP, including dialog (Li et al., 2016), semi-supervised text classification (Wu et al., 2018), coreference (Clark and Manning, 2016; Yin et al., 2018), knowledge graph reasoning (Xiong et al., 2017), text games (Narasimhan et al., 2015; He et al., 2016a), social media (He et al., 2016b; Zhou and Wang, 2018), information extraction (Narasimhan et al., 2016; Qin et al., 2018), language and vision (Pasunuru and Bansal, 2017; Misra et al., 2017; Wang et al., 2018a,b,c; Xiong et al., 2018), etc. We further discuss several critical issues in DRL solutions for NLP tasks, including (1) The efficient and practical design of the action space, state space, and reward functions; (2) The trade-off between exploration and exploitation; and (3) The goal of incorporating linguistic structures in DRL. To address the model design issue, we discuss several recent solutions (He et al., 2016b; Li et al., 2016; Xiong et al., 2017). We then focus on a new case study of hierarchical de"
P19-1159,Q18-1041,0,0.0301316,"scope of this paper is limited. There is a long history of gender stereotype study in law, psychology, media study, and many other disciplines which we do not discuss. Similar issues of algorithmic bias have also been discussed extensively in artificial intelligence, machine learning, data mining, and several other application fields (e.g., (Calders and Verwer, 2010; Feldman et al., 2015; Hardt et al., 2016; Misra et al., 2016; Kleinberg et al., 2016; Pleiss et al., 2017; Beutel et al., 2017; Misra et al., 2016)). Other important aspects such as model/data transparency (Mitchell et al., 2019; Bender and Friedman, 2018) and privacy preservation (Reddy and Knight, 2016; Elazar and Goldberg, 2018; Li et al., 2018) are also not covered in this literature survey. Besides, we refer the readers to Hovy and Spruit (2016) for a more general discussion of ethical concern in NLP. The study of gender bias in NLP is still relatively nascent and consequently lacks unified metrics and benchmarks for evaluation. We urge researchers in related fields to work together to create standardized metrics that rigorously measure the gender bias in NLP applications. However, we recognize that different applications may require diffe"
P19-1159,D18-1002,0,0.0272669,"study in law, psychology, media study, and many other disciplines which we do not discuss. Similar issues of algorithmic bias have also been discussed extensively in artificial intelligence, machine learning, data mining, and several other application fields (e.g., (Calders and Verwer, 2010; Feldman et al., 2015; Hardt et al., 2016; Misra et al., 2016; Kleinberg et al., 2016; Pleiss et al., 2017; Beutel et al., 2017; Misra et al., 2016)). Other important aspects such as model/data transparency (Mitchell et al., 2019; Bender and Friedman, 2018) and privacy preservation (Reddy and Knight, 2016; Elazar and Goldberg, 2018; Li et al., 2018) are also not covered in this literature survey. Besides, we refer the readers to Hovy and Spruit (2016) for a more general discussion of ethical concern in NLP. The study of gender bias in NLP is still relatively nascent and consequently lacks unified metrics and benchmarks for evaluation. We urge researchers in related fields to work together to create standardized metrics that rigorously measure the gender bias in NLP applications. However, we recognize that different applications may require different metrics and there are trade-offs between different notions of biases (B"
P19-1159,W19-3621,0,0.168175,"hey first build a linear support vector machine to classify words into a set of gender-specific and a set of gender-neutral words based on a training set of hand-selected gender-specific words. The authors then identify a gender direction by aggregating ten gender pairs (e.g. she-he, her-his, woman-man, etc.) and using principal component analysis to find a single eigenvector that exhibits significantly greater variance than the rest. Manzini et al. (2019) extend this method and their approach can be used to find non-binary gender bias by aggregating n-tuples instead of gender pairs. However, Gonen and Goldberg (2019) note that the above method fails to capture the full picture of gender bias in vector spaces. Specifically, even after the projections of word embeddings representing gender-neutral words onto the gender subspace have been removed, word embeddings representing words with similar biases still cluster together. They further introduce the notion of cluster bias. Cluster bias of a word w can be measured as the percentage of male or female stereotypical words among the k nearest neighbors of w’s embedding where the male or female stereotypical words are obtained through human annotation. 2.3 Measu"
P19-1159,W12-1008,0,0.27664,"Missing"
P19-1159,P16-2096,0,0.0603932,"have also been discussed extensively in artificial intelligence, machine learning, data mining, and several other application fields (e.g., (Calders and Verwer, 2010; Feldman et al., 2015; Hardt et al., 2016; Misra et al., 2016; Kleinberg et al., 2016; Pleiss et al., 2017; Beutel et al., 2017; Misra et al., 2016)). Other important aspects such as model/data transparency (Mitchell et al., 2019; Bender and Friedman, 2018) and privacy preservation (Reddy and Knight, 2016; Elazar and Goldberg, 2018; Li et al., 2018) are also not covered in this literature survey. Besides, we refer the readers to Hovy and Spruit (2016) for a more general discussion of ethical concern in NLP. The study of gender bias in NLP is still relatively nascent and consequently lacks unified metrics and benchmarks for evaluation. We urge researchers in related fields to work together to create standardized metrics that rigorously measure the gender bias in NLP applications. However, we recognize that different applications may require different metrics and there are trade-offs between different notions of biases (Barocas et al., 2018; Chouldechova and Roth, 2018). Gender debiasing methods in NLP are not sufficient to debias models end"
P19-1159,S18-2005,0,0.42654,"model’s prediction should not be heavily influenced by the gender of the entity mentions or contexts in the input. To evaluate whether or not this is the case, consider two sentences that act as the inputs to a model for which the only differences are the words that correspond to gender, such as “He went to the park” vs “She went to the park”. We refer to changing the gender of the gendered nouns as gender-swapping. Gender-swapping can be generalized to sentences by swapping each male-definitional word with its respective female equivalent and vice-versa (Zhao et al., 2018a; Lu et al., 2018; Kiritchenko and Mohammad, 2018). If the model does not make decisions based on genders, it should perform equally for both sentences. Otherwise, the difference in evaluation scores reflects the extent of gender bias found in the system. For example, Dixon et al. (2017) introduce two metrics to measure these performance differences – False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED) – that have been used to measure gender bias in abusive language detection (Park et al., 2018). These are defined as the differences in the false positive and false negative rates, respectively, of prediction"
P19-1159,D17-1018,0,0.0257358,"E2 after applying gender-swapping and name anonymization for data augmentation. This removes gender associations with named entities in sentences. The model is then trained on the union of the original data set with name-anonymization and the augmented data set. The identification of gender-specific words and their equivalent opposite gender word requires lists typically created by crowd workers. Data augmentation has been shown to be flexible; it can mitigate gender bias in several different models in many different tasks. When applied to a neural network based coreference resolution model (Lee et al., 2017, 2018) originally trained on OntoNotes 5.0 which was tested on WinoBias, gender augmentation lowered the difference between F1 scores on pro-stereotypical and antistereotypical test sets significantly, which indicates the model was less inclined to make genderbiased predictions (Zhao et al., 2018a, 2019). In hate speech detection, data augmentation reduced FNED and FPED differences between male and female predictions of a Convolutional Neural Network by a wide margin (Park et al., 2018). Data augmentation without name-anonymization has also been used to debias knowledge graphs built from Boll"
P19-1159,N18-2108,0,0.0251649,"Missing"
P19-1159,P18-2005,0,0.0498437,"Missing"
P19-1159,N19-1062,0,0.159781,"gender subspace of a word embedding representing a gender-neutral word and that word’s bias rating, as rated by crowd workers. To identify the gender subspace, they first build a linear support vector machine to classify words into a set of gender-specific and a set of gender-neutral words based on a training set of hand-selected gender-specific words. The authors then identify a gender direction by aggregating ten gender pairs (e.g. she-he, her-his, woman-man, etc.) and using principal component analysis to find a single eigenvector that exhibits significantly greater variance than the rest. Manzini et al. (2019) extend this method and their approach can be used to find non-binary gender bias by aggregating n-tuples instead of gender pairs. However, Gonen and Goldberg (2019) note that the above method fails to capture the full picture of gender bias in vector spaces. Specifically, even after the projections of word embeddings representing gender-neutral words onto the gender subspace have been removed, word embeddings representing words with similar biases still cluster together. They further introduce the notion of cluster bias. Cluster bias of a word w can be measured as the percentage of male or fe"
P19-1159,N19-1063,0,0.0399345,"skan et al., 2017). The authors confirm that human biases found through IAT tests exist in GloVe and Word2Vec embeddings. Finally, the authors demonstrate a positive correlation between the strength of association of an occupation word embedding with the female gender and the percentage of females in that occupation in United States, with the percentages taken from Bureau of Labor Statistics labor force participation data. Notably, Garg et al. (2018) show that bias in word 1631 embeddings can be used to track social changes such as increased or decreased female participation in the workforce. May et al. (2019) extend WEAT to create the Sentence Encoder Association Test (SEAT), capable of testing sentence encoders (e.g., ELMo (Peters et al., 2018)) for human biases found in IAT tests. 2.2 Analyzing Gender Sub-space in Embeddings Bolukbasi et al. (2016) define gender bias as the correlation between the magnitude of the projection onto the gender subspace of a word embedding representing a gender-neutral word and that word’s bias rating, as rated by crowd workers. To identify the gender subspace, they first build a linear support vector machine to classify words into a set of gender-specific and a set"
P19-1159,D18-1302,0,0.353775,"s Language Model Word Embedding Example of Representation Bias in the Context of Gender Translating “He is a nurse. She is a doctor.” to Hungarian and back to English results in “She is a nurse. He is a doctor.” (Douglas, 2017) An image captioning model incorrectly predicts the agent to be male because there is a computer nearby (Burns et al., 2018). Automatic speech detection works better with male voices than female voices (Tatman, 2017). Sentiment Analysis Systems rank sentences containing female noun phrases to be indicative of anger more often than sentences containing male noun phrases (Park et al., 2018). “He is doctor” has a higher conditional likelihood than “She is doctor” (Lu et al., 2018). Analogies such as “man : woman :: computer programmer : homemaker” are automatically generated by models trained on biased word embeddings (Bolukbasi et al., 2016). D S X R X X X U X X X X X X X X X X Table 1: Following the talk by Crawford (2017), we categorize representation bias in NLP tasks into the following four categories: (D)enigration, (S)tereotyping, (R)ecognition, (U)nder-representation. Briefly, denigration refers to the use of culturally or historically derogatory terms; stereotyping reinf"
P19-1159,N18-1202,0,0.041898,"uthors demonstrate a positive correlation between the strength of association of an occupation word embedding with the female gender and the percentage of females in that occupation in United States, with the percentages taken from Bureau of Labor Statistics labor force participation data. Notably, Garg et al. (2018) show that bias in word 1631 embeddings can be used to track social changes such as increased or decreased female participation in the workforce. May et al. (2019) extend WEAT to create the Sentence Encoder Association Test (SEAT), capable of testing sentence encoders (e.g., ELMo (Peters et al., 2018)) for human biases found in IAT tests. 2.2 Analyzing Gender Sub-space in Embeddings Bolukbasi et al. (2016) define gender bias as the correlation between the magnitude of the projection onto the gender subspace of a word embedding representing a gender-neutral word and that word’s bias rating, as rated by crowd workers. To identify the gender subspace, they first build a linear support vector machine to classify words into a set of gender-specific and a set of gender-neutral words based on a training set of hand-selected gender-specific words. The authors then identify a gender direction by ag"
P19-1159,W16-5603,0,0.0226761,"ry of gender stereotype study in law, psychology, media study, and many other disciplines which we do not discuss. Similar issues of algorithmic bias have also been discussed extensively in artificial intelligence, machine learning, data mining, and several other application fields (e.g., (Calders and Verwer, 2010; Feldman et al., 2015; Hardt et al., 2016; Misra et al., 2016; Kleinberg et al., 2016; Pleiss et al., 2017; Beutel et al., 2017; Misra et al., 2016)). Other important aspects such as model/data transparency (Mitchell et al., 2019; Bender and Friedman, 2018) and privacy preservation (Reddy and Knight, 2016; Elazar and Goldberg, 2018; Li et al., 2018) are also not covered in this literature survey. Besides, we refer the readers to Hovy and Spruit (2016) for a more general discussion of ethical concern in NLP. The study of gender bias in NLP is still relatively nascent and consequently lacks unified metrics and benchmarks for evaluation. We urge researchers in related fields to work together to create standardized metrics that rigorously measure the gender bias in NLP applications. However, we recognize that different applications may require different metrics and there are trade-offs between dif"
P19-1159,W04-2401,0,0.0250953,"Missing"
P19-1159,N18-2002,0,0.220199,"Missing"
P19-1159,Q18-1042,0,0.223567,"g gender bias. For one, these data sets often also contain biases (such as containing more male references than female references), so evaluation on them might not reveal gender bias. Furthermore, predictions made by systems performing complex NLP tasks depend on many factors; we must carefully design data sets to isolate the effect of gender of the output in order to be able to probe gender bias. We name these data sets Gender Bias Evaluation Testsets (GBETs). 1632 The goal of designing GBETs is to provide Data Set Winogender Schemas (Rudinger et al., 2018) WinoBias (Zhao et al., 2018a) GAP (Webster et al., 2018) EEC (Kiritchenko and Mohammad, 2018) Task Coreference Resolution Coreference Resolution Coreference Resolution Sentiment Analysis Probing Concept Occupation Occupation Names Emotion Size 720 English Sentences 3,160 English Sentences 4,454 English Contexts 8,640 English Sentences Table 2: Summary of GBETs. GBETs evaluate models trained for specific tasks for gender bias. GBETs use differences in values of the probing concept or prediction accuracies relating to the probing concept between gender-swapped data points to measure bias. check that NLP systems avoid making mistakes due to gender bia"
P19-1159,N19-1064,1,0.865318,"Missing"
P19-1159,D17-1323,1,0.814881,"aining set and within the algorithm itself. Introduction Gender bias is the preference or prejudice toward one gender over the other (Moss-Racusin et al., 2012). Gender bias is exhibited in multiple parts of a Natural Language Processing (NLP) system, including the training data, resources, pretrained models (e.g. word embeddings), and algorithms themselves (Zhao et al., 2018a; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018). NLP systems containing bias in any of these parts can produce gender biased predictions and sometimes even amplify biases present in the training sets (Zhao et al., 2017). The propagation of gender bias in NLP algorithms poses the danger of reinforcing damaging * Equal Contribution. stereotypes in downstream applications. This has real-world consequences; for example, concerns have been raised about automatic resume filtering systems giving preference to male applicants when the only distinguishing factor is the applicants’ gender. One way to categorize bias is in terms of allocation and representation bias (Crawford, 2017). Allocation bias can be framed as an economic issue in which a system unfairly allocates resources to certain groups over others, while re"
P19-1159,N18-2003,1,0.110244,"1: Observation and evaluation of gender bias in NLP. Bias observation occurs in both the training sets and the test sets specifically for evaluating the gender bias of a given algorithm’s predictions. Debiasing gender occurs in both the training set and within the algorithm itself. Introduction Gender bias is the preference or prejudice toward one gender over the other (Moss-Racusin et al., 2012). Gender bias is exhibited in multiple parts of a Natural Language Processing (NLP) system, including the training data, resources, pretrained models (e.g. word embeddings), and algorithms themselves (Zhao et al., 2018a; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018). NLP systems containing bias in any of these parts can produce gender biased predictions and sometimes even amplify biases present in the training sets (Zhao et al., 2017). The propagation of gender bias in NLP algorithms poses the danger of reinforcing damaging * Equal Contribution. stereotypes in downstream applications. This has real-world consequences; for example, concerns have been raised about automatic resume filtering systems giving preference to male applicants when the only distinguishing factor is the applicants’"
P19-1159,D18-1521,1,0.103647,"1: Observation and evaluation of gender bias in NLP. Bias observation occurs in both the training sets and the test sets specifically for evaluating the gender bias of a given algorithm’s predictions. Debiasing gender occurs in both the training set and within the algorithm itself. Introduction Gender bias is the preference or prejudice toward one gender over the other (Moss-Racusin et al., 2012). Gender bias is exhibited in multiple parts of a Natural Language Processing (NLP) system, including the training data, resources, pretrained models (e.g. word embeddings), and algorithms themselves (Zhao et al., 2018a; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018). NLP systems containing bias in any of these parts can produce gender biased predictions and sometimes even amplify biases present in the training sets (Zhao et al., 2017). The propagation of gender bias in NLP algorithms poses the danger of reinforcing damaging * Equal Contribution. stereotypes in downstream applications. This has real-world consequences; for example, concerns have been raised about automatic resume filtering systems giving preference to male applicants when the only distinguishing factor is the applicants’"
P19-1159,D18-1301,0,0.0330824,"and Verwer, 2010; Feldman et al., 2015; Hardt et al., 2016; Misra et al., 2016; Kleinberg et al., 2016; Pleiss et al., 2017; Beutel et al., 2017; Kilbertus et al., 2017). Many of these technical methods could be applicable to NLP yet to our knowledge have not been studied. Additionally, mitigating gender bias in NLP is both a sociological and an engineering problem. To completely debias effectively, it is important to understand how machine learning methods encode biases and how humans perceive biases. A few interdisciplinary studies (Herbelot et al., 2012; Avin et al., 2015; Fu et al., 2016; Schluter, 2018) have emerged, and we urge more interdisciplinary discussions in terms of gender bias. Approaches from other technical fields may improve current debiasing methods in NLP or inspire the development of new, more effective methods even if the properties of the data or Conclusion and Future Directions In this paper, we summarize recent literature about recognizing and mitigating gender bias in NLP. We acknowledge that the scope of this paper is limited. There is a long history of gender stereotype study in law, psychology, media study, and many other disciplines which we do not discuss. Similar i"
P19-1159,W17-1606,0,0.0782787,"Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Task Machine Translation Caption Generation Speech Recognition Sentiment Analysis Language Model Word Embedding Example of Representation Bias in the Context of Gender Translating “He is a nurse. She is a doctor.” to Hungarian and back to English results in “She is a nurse. He is a doctor.” (Douglas, 2017) An image captioning model incorrectly predicts the agent to be male because there is a computer nearby (Burns et al., 2018). Automatic speech detection works better with male voices than female voices (Tatman, 2017). Sentiment Analysis Systems rank sentences containing female noun phrases to be indicative of anger more often than sentences containing male noun phrases (Park et al., 2018). “He is doctor” has a higher conditional likelihood than “She is doctor” (Lu et al., 2018). Analogies such as “man : woman :: computer programmer : homemaker” are automatically generated by models trained on biased word embeddings (Bolukbasi et al., 2016). D S X R X X X U X X X X X X X X X X Table 1: Following the talk by Crawford (2017), we categorize representation bias in NLP tasks into the following four categories:"
P19-1159,N16-2013,0,0.0453347,"related task. Bias fine-tuning incorporates transfer learning from an unbiased data set to ensure that a model contains minimal bias before fine-tuning the model on a more biased data set used to train for the target task directly (Park et al., 2018). This allows models to avoid learning biases from training sets while still being adequately trained to perform a task. Bias fine-tuning has been shown to be relatively effective. Park et al. (2018) use transfer learning from a gender unbiased abusive tweets data set (Founta et al., 2018) and fine-tuning on a genderbiased sexist tweets data set (Waseem and Hovy, 2016) to train a Convolutional Neural Network (CNN). They evaluate the CNN using a GBET evaluation set (which is private, so not mentioned in 2.3). They tested the same model after training it on gender-swapped data sets as well. Park et al. (2018) find that gender-swapping was more effective at both removing bias and retaining performance than bias fine-tuning. However, transfer learning may have been ineffective in this case because abusive language detection data sets and sexist language detection data sets have significant differences. For more similar data sets, bias finetuning may be more eff"
P19-1214,W04-3252,0,0.0620575,"ces But the flight was cancelled due to the weather. But I lost my passport. The meeting was cancelled. The weather is good today. Figure 1: An example for the Mask pre-training task. A sentence is masked in the original paragraph, and the model is required to predicted the missing sentence from the candidate sentences. Introduction Extractive summarization aims at shortening the original article while retaining the key information through the way of selection sentences from the original articles. This paradigm has been proven effective by many previous systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015). In order to decide whether to choose a particular sentence, the system should have a global view of the document context, e.g., the subject and structure of the document. However, previous works (Nallapati et al., 2017; Al-Sabahi et al., 2018; Zhou et al., 2018; Zhang et al., 2018) usually directly build an end-to-end training system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to le"
P19-1214,K16-1028,0,0.0886086,"Missing"
P19-1214,W17-5308,0,0.0545636,"Missing"
P19-1214,P07-1010,0,0.0362907,"o learn the contextualized sentence representation with self-supervision. Self-supervised learning (Raina et al., 2007; Doersch et al., 2015; Agrawal et al., 2015; Wang and Gupta, 2015) is a newly emerged paradigm, which aims to learn from the intrinsic structure of the raw data. The general framework is to construct training signals directly from the structured raw data, and use it to train the model. The structure information learned through the process can then be easily transformed and benefit other tasks. Thus self-supervised learning has been widely applied in structured data like text (Okanohara and Tsujii, 2007; Collobert and Weston, 2008; Peters et al., 2018; Devlin et al., 2018; Wu et al., 2019) and images (Doersch et al., 2015; Agrawal et al., 2015; Wang and Gupta, 2015; Lee et al., 2017). 2221 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2221–2227 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Since documents are well organized and structured, it is intuitive to employ the power of selfsupervised learning to learn the intrinsic structure of the document and model the document-level context for the summ"
P19-1214,N18-1049,0,0.0259131,"ystem can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this 1 Code can be found in this repository: https:// github.com/hongwang600/Summarization task. In recent years, extensive works (Pennington et al., 2014; Nie and Bansal, 2017; Lin et al., 2017; Peters et al., 2018; Devlin et al., 2018; Subramanian et al., 2018; Cer et al., 2018; Logeswaran and Lee, 2018; Pagliardini et al., 2018) have been done in learning the word or sentence representations, but most of them only use a sentence or a few sentences when learning the representation, and the document context can hardly be included in the representation. Hence, we introduce new pre-training methods that take the whole document into consideration to learn the contextualized sentence representation with self-supervision. Self-supervised learning (Raina et al., 2007; Doersch et al., 2015; Agrawal et al., 2015; Wang and Gupta, 2015) is a newly emerged paradigm, which aims to learn from the intrinsic structure of the raw data"
P19-1214,D14-1162,0,0.0949777,"8; Zhang et al., 2018) usually directly build an end-to-end training system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this 1 Code can be found in this repository: https:// github.com/hongwang600/Summarization task. In recent years, extensive works (Pennington et al., 2014; Nie and Bansal, 2017; Lin et al., 2017; Peters et al., 2018; Devlin et al., 2018; Subramanian et al., 2018; Cer et al., 2018; Logeswaran and Lee, 2018; Pagliardini et al., 2018) have been done in learning the word or sentence representations, but most of them only use a sentence or a few sentences when learning the representation, and the document context can hardly be included in the representation. Hence, we introduce new pre-training methods that take the whole document into consideration to learn the contextualized sentence representation with self-supervision. Self-supervised learning ("
P19-1214,N18-1202,0,0.0365077,"ing system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this 1 Code can be found in this repository: https:// github.com/hongwang600/Summarization task. In recent years, extensive works (Pennington et al., 2014; Nie and Bansal, 2017; Lin et al., 2017; Peters et al., 2018; Devlin et al., 2018; Subramanian et al., 2018; Cer et al., 2018; Logeswaran and Lee, 2018; Pagliardini et al., 2018) have been done in learning the word or sentence representations, but most of them only use a sentence or a few sentences when learning the representation, and the document context can hardly be included in the representation. Hence, we introduce new pre-training methods that take the whole document into consideration to learn the contextualized sentence representation with self-supervision. Self-supervised learning (Raina et al., 2007; Doersch et al., 2015; Agrawal et al., 201"
P19-1214,P19-1375,1,0.839455,"Missing"
P19-1214,D18-1088,0,0.056554,"troduction Extractive summarization aims at shortening the original article while retaining the key information through the way of selection sentences from the original articles. This paradigm has been proven effective by many previous systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015). In order to decide whether to choose a particular sentence, the system should have a global view of the document context, e.g., the subject and structure of the document. However, previous works (Nallapati et al., 2017; Al-Sabahi et al., 2018; Zhou et al., 2018; Zhang et al., 2018) usually directly build an end-to-end training system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this 1 Code can be found in this repository: https:// github.com/hongwang600/Summarization task. In recent years, extensive works (Pennington et al., 201"
P19-1214,P18-1061,0,0.365384,"idate sentences. Introduction Extractive summarization aims at shortening the original article while retaining the key information through the way of selection sentences from the original articles. This paradigm has been proven effective by many previous systems (Carbonell and Goldstein, 1998; Mihalcea and Tarau, 2004; McDonald, 2007; Cao et al., 2015). In order to decide whether to choose a particular sentence, the system should have a global view of the document context, e.g., the subject and structure of the document. However, previous works (Nallapati et al., 2017; Al-Sabahi et al., 2018; Zhou et al., 2018; Zhang et al., 2018) usually directly build an end-to-end training system to learn to choose sentences without explicitly modeling the document context, counting on that the system can automatically learn the document-level context. We argue that it is hard for these end-to-end systems to learn to leverage the document context from scratch due to the challenges of this task, and a well pre-trained embedding model that incorporates document context should help on this 1 Code can be found in this repository: https:// github.com/hongwang600/Summarization task. In recent years, extensive works (P"
P19-1214,P17-1099,0,0.0573945,"ng these selected sentences, i.e., each selected sentence will be put in another position within the same document. Let Cs be the set of positions where the sentences are switched. Similarly, we use a linear layer fs to predict if a sentence is switched and minimize the MSE loss: `s = MSE(fs (Di ), yis ) where yis = 1 if i ∈ Cs , otherwise yis = 0. 3 Experiment To show the effectiveness of the pre-training method (Mask, Replace and Switch), we conduct experiments on the commonly used dataset CNN/DM (Hermann et al., 2015; Nallapati et al., 2016), and compare them with a popular baseline Lead3 (See et al., 2017), which selects first three sentences as the summary, and the state-of-theart extractive summarization method N EU S UM (Zhou et al., 2018), which jointly scores and selects sentences using pointer network. 3.1 On CNN/DM Dataset Model and training details We use the rulebased system from (Zhou et al., 2018) to label sentences in a document, e.g., sentences to be extracted will be labeled as 1. Rouge score3 (Lin, 2004) is used to evaluate the performance of the model, and we report Rouge-1, Rouge-2, and Rouge-L as in prior work. We use the pretrained glove embedding (Pennington et al., 2014) wi"
P19-1214,W04-1013,0,\N,Missing
P19-1214,D18-2029,0,\N,Missing
P19-1214,N19-1423,0,\N,Missing
P19-1360,D18-1547,0,0.10399,"On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics. 1 Introduction Conversational artificial intelligence (Young et al., 2013) is one of the critical milestones in artificial intelligence. Recently, there have been increasing interests in industrial companies to build task-oriented conversational agents (Wen et al., 2017; Li et al., 2017; Rojas-Barahona et al., 2017) to solve pre-defined tasks such as restaurant or flight bookings, etc (see Figure 1 for an example dialog from MultiWOZ (Budzianowski et al., 2018)). Traditional agents are built based on slotfilling techniques, which requires significant human handcraft efforts. And it is hard to generate naturally sounding utterances in a generalizable and scalable manner. Therefore, different semantically controlled neural language generation models have been developed (Wen et al., 2015, 2016a,b; Dusek and Jurc´ıcek, 2016) to replace the traditional systems, where an explicit semantic representation (dialog act) are used to influence the RNN generation. The canonical approach is proposed in (Wen et al., 2015) to encode each individual dialog act as a"
P19-1360,D18-1038,1,0.831883,"t their inter-relationships, which greatly reduces the sample complexity and improves generalization, (ii) we propose to incorporate the structure prior in semantic space to design HDSA to explicitly model the semantics of neural generation, and outperforms baselines. 2 Related Work & Background Canonical task-oriented dialog systems are built as pipelines of separately trained modules: (i) user intention classification (Shi et al., 2016; Goo et al., 2018), which is for understanding human intention. (ii) belief state tracker (Williams et al., 2013; Mrksic et al., 2017a,b; Zhong et al., 2018; Chen et al., 2018), which is used to track user’s query constraint and formulate DB query to retrieve entries from a large database. (iii) dialog act prediction (Wen et al., 2017), which is applied to classify the system action. (iv) response generation (Rojas-Barahona et al., 2017; Wen et al., 2016b; Li et al., 2017; Lei et al., 2018) to realize language surface form given the semantic constraint. In order to handle the massive number of entities in the response, Rojas-Barahona et al. (2017); Wen et al. (2016b, 2015) suggest to break response generation into two steps: first generate delexicalized sentences wi"
P19-1360,W16-3622,0,0.0440253,"Missing"
P19-1360,W17-5506,0,0.033051,"aurant where food=‘korean’ and area=’north’ Food: Korean Area: North Price: * Stars: * DB Execution I want to find a Korean restaurant in the north of the town. Name Location Price Food Stars Little Seoul north low Korean 4 Figure 3: Illustration of the neural dialog system. We decompose it into two parts: the lower part describes the dialog state tracking and DB query, and the upper part denotes the Dialog Action Prediction and Response Generation. In this paper, we are mainly interested in improving the performance of the upper part. 2016), CamRes767 (Rojas-Barahona et al., 2017) and KVRET (Eric et al., 2017), etc. However, a recently introduced multi-domain and large-scale dataset MultiWOZ (Budzianowski et al., 2018) poses great challenges to these approaches due to the large number of slots and complex ontology. Dealing with such a large semantic space remains a challenging research problem. We follow the nomenclature proposed in RojasBarahona et al. (2017) to visualize the overview of the pipeline system in Figure 3, and then decompose it into two parts: the lower part (blue rectangle) contains state tracking and symbolic DB execution, the upper part consists of dialog act prediction and respon"
P19-1360,N18-2118,0,0.0351149,"eases the sample complexity on seen cases. In summary, our contributions include: (i) we propose a hierarchical graph representation of dialog acts to exploit their inter-relationships, which greatly reduces the sample complexity and improves generalization, (ii) we propose to incorporate the structure prior in semantic space to design HDSA to explicitly model the semantics of neural generation, and outperforms baselines. 2 Related Work & Background Canonical task-oriented dialog systems are built as pipelines of separately trained modules: (i) user intention classification (Shi et al., 2016; Goo et al., 2018), which is for understanding human intention. (ii) belief state tracker (Williams et al., 2013; Mrksic et al., 2017a,b; Zhong et al., 2018; Chen et al., 2018), which is used to track user’s query constraint and formulate DB query to retrieve entries from a large database. (iii) dialog act prediction (Wen et al., 2017), which is applied to classify the system action. (iv) response generation (Rojas-Barahona et al., 2017; Wen et al., 2016b; Li et al., 2017; Lei et al., 2018) to realize language surface form given the semantic constraint. In order to handle the massive number of entities in the r"
P19-1360,P17-1163,0,0.0607181,"Missing"
P19-1360,Q17-1022,0,0.0603362,"Missing"
P19-1360,P02-1040,0,0.104096,"ent neural networks to compare their performances. The experimental results (measured in F1 scores) are reported in Table 2. Experimental results show that fine-tuning the pretrained BERT (Devlin et al., 2018) can lead to significantly better performance than the other models. Therefore, we will use it as the dialog act prediction model in the following experiments. Instead of jointly training the predictor and the response generator, we simply fix the trained predictor when learning the generator Pβ (y). 5.1 Automatic Evaluation We follow Budzianowski et al. (2018) to use delexicalized-BLEU (Papineni et al., 2002), inform rate and request success as three basic metrics to compare the delexicalized generation against the delexicalized reference. We further propose Entity F1 (Rojas-Barahona et al., 2017) to evaluate the entity coverage accuracy (including all slot values, days, numbers, and reference, etc), and restore-BLEU to compare the restored generation against the raw reference. The evaluation metrics are detailed in the supplementary material. Before diving into the experiments, we first list all the models we experiment with as follows: 1. Without Dialog Act, we use the official code 7 : (i) LSTM"
P19-1360,E17-1042,0,0.331229,"man intention. (ii) belief state tracker (Williams et al., 2013; Mrksic et al., 2017a,b; Zhong et al., 2018; Chen et al., 2018), which is used to track user’s query constraint and formulate DB query to retrieve entries from a large database. (iii) dialog act prediction (Wen et al., 2017), which is applied to classify the system action. (iv) response generation (Rojas-Barahona et al., 2017; Wen et al., 2016b; Li et al., 2017; Lei et al., 2018) to realize language surface form given the semantic constraint. In order to handle the massive number of entities in the response, Rojas-Barahona et al. (2017); Wen et al. (2016b, 2015) suggest to break response generation into two steps: first generate delexicalized sentences with placeholders like &lt;Res.Name&gt;, and then post-process the sentence by replacing the placeholders with the DB record. The existing modularized neural models have achieved promising performance on limiteddomain datasets like DSTC (Williams et al., 3697 Utterance Understanding Delexicialized Response Generation Dialog Act Prediction I recommend &lt;Res.Name&gt;, which has a &lt;Res.Price&gt; price. Post-Processing 1. Restaurant-Recommend-Name 2. Restaurant-Recommend-Price History: sys res"
P19-1360,N16-1176,0,0.0561454,"Missing"
P19-1360,P18-1133,0,0.546307,"dialog systems are built as pipelines of separately trained modules: (i) user intention classification (Shi et al., 2016; Goo et al., 2018), which is for understanding human intention. (ii) belief state tracker (Williams et al., 2013; Mrksic et al., 2017a,b; Zhong et al., 2018; Chen et al., 2018), which is used to track user’s query constraint and formulate DB query to retrieve entries from a large database. (iii) dialog act prediction (Wen et al., 2017), which is applied to classify the system action. (iv) response generation (Rojas-Barahona et al., 2017; Wen et al., 2016b; Li et al., 2017; Lei et al., 2018) to realize language surface form given the semantic constraint. In order to handle the massive number of entities in the response, Rojas-Barahona et al. (2017); Wen et al. (2016b, 2015) suggest to break response generation into two steps: first generate delexicalized sentences with placeholders like &lt;Res.Name&gt;, and then post-process the sentence by replacing the placeholders with the DB record. The existing modularized neural models have achieved promising performance on limiteddomain datasets like DSTC (Williams et al., 3697 Utterance Understanding Delexicialized Response Generation Dialog A"
P19-1360,D18-1548,0,0.235152,"?( Disentangled Self-Attention History ? ?&apos; ?# ?# ?&quot; ?&quot; Dialog Act Graph hotel-inform-location hotel-inform-name ?$ Disentangled Self-Attention Figure 5: The left figure describes the dialog act predictor and HDSA, and the right figure describes the details of DSA. The predicted hierarchical dialog acts are used to control the switch in HDSA at each layer. Here we use L = 3 layers, the head numbers at each layer are H = (4, 3, 6) heads, the hierarchical graph representation A=[[0, 1, 0, 0], [0, 1, 0], [0, 0, 1, 1, 0, 0]]. We use m to denote the dialog history length and n for response. et al. (2018); Rojas-Barahona et al. (2017) to use one-hot vector vkb and vbf for representing the DB records and belief state (see the original papers for details). For convenience, we use θ to collect all the parameters of the utterance encoder and action predictor. At training time, we propose to maximize the cross-entropy objective L(θ) as follows: L(θ) =A · log(fθ (¯ u, vkb , vbf )+ (1 − A) · log(1 − fθ (¯ u, vkb , vbf )) (3) where · denotes the inner product between two vectors. At test time, we predict the dialog acts Aˆ = {I(Pθ (A)i &gt; T )|1 ≤ i ≤ H0 }, where T is the threshold and I is the indicato"
P19-1360,I17-1074,0,0.156629,"graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics. 1 Introduction Conversational artificial intelligence (Young et al., 2013) is one of the critical milestones in artificial intelligence. Recently, there have been increasing interests in industrial companies to build task-oriented conversational agents (Wen et al., 2017; Li et al., 2017; Rojas-Barahona et al., 2017) to solve pre-defined tasks such as restaurant or flight bookings, etc (see Figure 1 for an example dialog from MultiWOZ (Budzianowski et al., 2018)). Traditional agents are built based on slotfilling techniques, which requires significant human handcraft efforts. And it is hard to generate naturally sounding utterances in a generalizable and scalable manner. Therefore, different semantically controlled neural language generation models have been developed (Wen et al., 2015, 2016a,b; Dusek and Jurc´ıcek, 2016) to replace the traditional systems, where an explicit"
P19-1360,D16-1233,0,0.0756983,"Missing"
P19-1360,N16-1015,0,0.144468,"e naturally sounding utterances in a generalizable and scalable manner. Therefore, different semantically controlled neural language generation models have been developed (Wen et al., 2015, 2016a,b; Dusek and Jurc´ıcek, 2016) to replace the traditional systems, where an explicit semantic representation (dialog act) are used to influence the RNN generation. The canonical approach is proposed in (Wen et al., 2015) to encode each individual dialog act as a unique vector and use it as an extra input feature into the cell of long short-term memory (LSTM) to influence the generation. As pointed in (Wen et al., 2016b), these models though achieving good performance on limited domains, suffer from scalability problem as the possible dialog acts grow combinatorially with the number of domains. In order to alleviate such issue, we propose a hierarchical graph representation by leveraging the structural property of dialog acts. Specifically, we first build a multi-layer tree to represent the entire dialog act space based on their interrelationships. Then, we merge the tree nodes with the same semantic meaning to construct an acyclic multi-layered graph, where each dialog act is interpreted as a root-to-leaf"
P19-1360,D15-1199,0,0.292871,"Missing"
P19-1360,W13-4065,0,0.0356162,"ropose a hierarchical graph representation of dialog acts to exploit their inter-relationships, which greatly reduces the sample complexity and improves generalization, (ii) we propose to incorporate the structure prior in semantic space to design HDSA to explicitly model the semantics of neural generation, and outperforms baselines. 2 Related Work & Background Canonical task-oriented dialog systems are built as pipelines of separately trained modules: (i) user intention classification (Shi et al., 2016; Goo et al., 2018), which is for understanding human intention. (ii) belief state tracker (Williams et al., 2013; Mrksic et al., 2017a,b; Zhong et al., 2018; Chen et al., 2018), which is used to track user’s query constraint and formulate DB query to retrieve entries from a large database. (iii) dialog act prediction (Wen et al., 2017), which is applied to classify the system action. (iv) response generation (Rojas-Barahona et al., 2017; Wen et al., 2016b; Li et al., 2017; Lei et al., 2018) to realize language surface form given the semantic constraint. In order to handle the massive number of entities in the response, Rojas-Barahona et al. (2017); Wen et al. (2016b, 2015) suggest to break response gene"
P19-1375,P18-5002,0,0.097459,"oduction In recent years, dialogue systems have achieved fruitful results with neural conversation models in both open-domain generation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2016b, 2017; Xu et al., 2017; Zhang et al., 2018b) and task-oriented completion (Wen et al., 2015, 2017; Williams et al., 2017; Bordes et al., 2017; Su et al., 2018). These methods empower lots of real-world dialogue applications such as Google Home and Apple Siri. However, the utterance generation from dialogue systems still faces some critical challenges, including utterance blandness and incoherence (Gao et al., 2018). They are mainly caused by the objective function of the dialogue systems that prefer utterances with unconditionally high probability (Li et al., 2016a). We argue that in a meaningful and coherent dialogue, the change of utterance order will lead to a low-quality dialogue. However, most existing neural-based dialogue systems either encode the full dialogue history (Li et al., 2017; Xu et al., 2017) or only the current utterance (Liu and Lane, 2018). None of them explicitly models the sequential order and studies its criticality to the dialogue learning problem. In this paper, we explore the"
P19-1375,N16-1014,0,0.149197,"dialogue history. Furthermore, we design a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and taskoriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets. 1 Introduction In recent years, dialogue systems have achieved fruitful results with neural conversation models in both open-domain generation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2016b, 2017; Xu et al., 2017; Zhang et al., 2018b) and task-oriented completion (Wen et al., 2015, 2017; Williams et al., 2017; Bordes et al., 2017; Su et al., 2018). These methods empower lots of real-world dialogue applications such as Google Home and Apple Siri. However, the utterance generation from dialogue systems still faces some critical challenges, including utterance blandness and incoherence (Gao et al., 2018). They are mainly caused by the objective function of the dialogue systems that prefer utterances with unconditionally high probability (Li et al., 2016a). We argue that in a meani"
P19-1375,D16-1127,0,0.169223,"dialogue history. Furthermore, we design a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and taskoriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets. 1 Introduction In recent years, dialogue systems have achieved fruitful results with neural conversation models in both open-domain generation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2016b, 2017; Xu et al., 2017; Zhang et al., 2018b) and task-oriented completion (Wen et al., 2015, 2017; Williams et al., 2017; Bordes et al., 2017; Su et al., 2018). These methods empower lots of real-world dialogue applications such as Google Home and Apple Siri. However, the utterance generation from dialogue systems still faces some critical challenges, including utterance blandness and incoherence (Gao et al., 2018). They are mainly caused by the objective function of the dialogue systems that prefer utterances with unconditionally high probability (Li et al., 2016a). We argue that in a meani"
P19-1375,D17-1230,0,0.0739859,"of real-world dialogue applications such as Google Home and Apple Siri. However, the utterance generation from dialogue systems still faces some critical challenges, including utterance blandness and incoherence (Gao et al., 2018). They are mainly caused by the objective function of the dialogue systems that prefer utterances with unconditionally high probability (Li et al., 2016a). We argue that in a meaningful and coherent dialogue, the change of utterance order will lead to a low-quality dialogue. However, most existing neural-based dialogue systems either encode the full dialogue history (Li et al., 2017; Xu et al., 2017) or only the current utterance (Liu and Lane, 2018). None of them explicitly models the sequential order and studies its criticality to the dialogue learning problem. In this paper, we explore the sequential order within the dialogue as the self-supervised signal to guide meaningful and coherent dialogue learning. We introduce a self-supervised learning task, inconsistent order detection, to explicitly capture the order signal of the dialogue. The task is defined as, given a target utterance pair triple, the model is required to predict whether the triple is correctly ordered"
P19-1375,N19-1423,0,0.0178878,"successfully applied in computer vision. Many self-supervised tasks have been introduced to use non-visual but intrinsically correlated features to guide the visual feature learning (Doersch et al., 2015; Wang and Gupta, 2015; Pathak et al., 2016). As for natural language processing, predicting nearby words (Mikolov et al., 2013b,a) is a self-supervised task to learn word embeddings. The language modeling is another line of self-supervision where a language model learns to predict the next word given the previous sequence (Bengio et al., 2003; Dai and Le, 2015; Peters et al., 2018). Recently, Devlin et al. (2019) further proposes two self-supervised tasks, the masked language model and next sentence prediction, to learn sentence embeddings. Lample and Conneau (2019); Liu et al. (2019) further extend these two tasks into multi-lingual and multi-task paradigms. Wang et al. (2019) consider them at the sentence-level for extractive summarization. Our work is the first to consider the sequential order as the self-supervised signal in dialogue and we propose the self-supervised task of inconsistent order detection towards more coherent and relevant dialogue learning. 3 Methods In this section, we systematic"
P19-1375,W18-5041,0,0.087241,"Siri. However, the utterance generation from dialogue systems still faces some critical challenges, including utterance blandness and incoherence (Gao et al., 2018). They are mainly caused by the objective function of the dialogue systems that prefer utterances with unconditionally high probability (Li et al., 2016a). We argue that in a meaningful and coherent dialogue, the change of utterance order will lead to a low-quality dialogue. However, most existing neural-based dialogue systems either encode the full dialogue history (Li et al., 2017; Xu et al., 2017) or only the current utterance (Liu and Lane, 2018). None of them explicitly models the sequential order and studies its criticality to the dialogue learning problem. In this paper, we explore the sequential order within the dialogue as the self-supervised signal to guide meaningful and coherent dialogue learning. We introduce a self-supervised learning task, inconsistent order detection, to explicitly capture the order signal of the dialogue. The task is defined as, given a target utterance pair triple, the model is required to predict whether the triple is correctly ordered or not. For instance, the utterance pair triple h(Q1 , A1 ), (Q4 , A"
P19-1375,P19-1441,0,0.0246967,"ing (Doersch et al., 2015; Wang and Gupta, 2015; Pathak et al., 2016). As for natural language processing, predicting nearby words (Mikolov et al., 2013b,a) is a self-supervised task to learn word embeddings. The language modeling is another line of self-supervision where a language model learns to predict the next word given the previous sequence (Bengio et al., 2003; Dai and Le, 2015; Peters et al., 2018). Recently, Devlin et al. (2019) further proposes two self-supervised tasks, the masked language model and next sentence prediction, to learn sentence embeddings. Lample and Conneau (2019); Liu et al. (2019) further extend these two tasks into multi-lingual and multi-task paradigms. Wang et al. (2019) consider them at the sentence-level for extractive summarization. Our work is the first to consider the sequential order as the self-supervised signal in dialogue and we propose the self-supervised task of inconsistent order detection towards more coherent and relevant dialogue learning. 3 Methods In this section, we systematically describe how to utilize the internal sequential order of utterances as self-supervision for dialogue learning. In Section 3.1, we first introduce the task of inconsistent"
P19-1375,D15-1166,0,0.0195014,"Automatic Evaluation for explanations. cal LSTM, and the Monte Carlo search is implemented to obtain rewards for every generation step to update the generator G. • AEL (Xu et al., 2017): The discriminator D only encodes the currently generated utterance by a CNN model and the generator G is optimized using an approximate embedding layer. Implementation Details We follow the most of parameters in Li et al. (2017); Xu et al. (2017) to make a fair comparison. For the generator model G, we adopt the same S EQ 2S EQ model (Sutskever et al., 2014) with an attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) for our approach and baselines. We approximate the dialogue history for G using the concatenation of two preceding utterances following the Li et al. (2017). To train the generator G, we use the REINFORCE algorithm (Williams, 1992) to maximize the expected reward of generated utterances. We also implement the Monte Carlo search to give rewards for each generation step. To accelerate the sampling process, we use multiple GPUs to parallelize and distribute the jobs. As for the SSN , it first gets pre-trained using sampled data from OpenSubtitiles, and then iteratively updated during the min-max"
P19-1375,P18-1203,0,0.0307316,"ponses to 5 judges and ask them to decide which one of the three results is the be.ts Ties are not permitted. We consider both single-turn and multiturn for the evaluation. The results are shown in Table 4. Evidently, the generator trained in our method shows a significant improvement in the quality of generated sentences. The gain is even higher in the multi-turn setting than the single-turn setting. This is because when only considering the single-turn dialogue, the information encoded in three methods will be similar. 4.3 Task-Oriented Dialogue Learning Dataset Following the previous work (Peng et al., 2018; Su et al., 2018), we use the same Movie-Ticket Booking dataset collected from Amazon Mechanical Turk for evaluation. The dataset is manually labeled based on a schema defined by domain experts consisting of 11 intents and 16 slots in the full domain setting. In total, the dataset has 280 annotated dialogues with an average length of approximately 11 turns. In this scenario, the goal of dialogue systems is to help the user complete the tasks through the conversation. Baselines We compare our SSN -based discriminator within the state-of-the-art task-oriented dialogue policy learning approach,"
P19-1375,N18-1202,0,0.0174026,"btained automatically, has been successfully applied in computer vision. Many self-supervised tasks have been introduced to use non-visual but intrinsically correlated features to guide the visual feature learning (Doersch et al., 2015; Wang and Gupta, 2015; Pathak et al., 2016). As for natural language processing, predicting nearby words (Mikolov et al., 2013b,a) is a self-supervised task to learn word embeddings. The language modeling is another line of self-supervision where a language model learns to predict the next word given the previous sequence (Bengio et al., 2003; Dai and Le, 2015; Peters et al., 2018). Recently, Devlin et al. (2019) further proposes two self-supervised tasks, the masked language model and next sentence prediction, to learn sentence embeddings. Lample and Conneau (2019); Liu et al. (2019) further extend these two tasks into multi-lingual and multi-task paradigms. Wang et al. (2019) consider them at the sentence-level for extractive summarization. Our work is the first to consider the sequential order as the self-supervised signal in dialogue and we propose the self-supervised task of inconsistent order detection towards more coherent and relevant dialogue learning. 3 Method"
P19-1375,N15-1020,0,0.0627302,"Missing"
P19-1375,D18-1416,0,0.381978,"ng through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and taskoriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets. 1 Introduction In recent years, dialogue systems have achieved fruitful results with neural conversation models in both open-domain generation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2016b, 2017; Xu et al., 2017; Zhang et al., 2018b) and task-oriented completion (Wen et al., 2015, 2017; Williams et al., 2017; Bordes et al., 2017; Su et al., 2018). These methods empower lots of real-world dialogue applications such as Google Home and Apple Siri. However, the utterance generation from dialogue systems still faces some critical challenges, including utterance blandness and incoherence (Gao et al., 2018). They are mainly caused by the objective function of the dialogue systems that prefer utterances with unconditionally high probability (Li et al., 2016a). We argue that in a meaningful and coherent dialogue, the change of utterance order will lead to a low-quality dialogue. However, most existing neural-based dialogue systems either encod"
P19-1375,D11-1054,0,0.129803,"Missing"
P19-1375,P19-1214,1,0.780474,"processing, predicting nearby words (Mikolov et al., 2013b,a) is a self-supervised task to learn word embeddings. The language modeling is another line of self-supervision where a language model learns to predict the next word given the previous sequence (Bengio et al., 2003; Dai and Le, 2015; Peters et al., 2018). Recently, Devlin et al. (2019) further proposes two self-supervised tasks, the masked language model and next sentence prediction, to learn sentence embeddings. Lample and Conneau (2019); Liu et al. (2019) further extend these two tasks into multi-lingual and multi-task paradigms. Wang et al. (2019) consider them at the sentence-level for extractive summarization. Our work is the first to consider the sequential order as the self-supervised signal in dialogue and we propose the self-supervised task of inconsistent order detection towards more coherent and relevant dialogue learning. 3 Methods In this section, we systematically describe how to utilize the internal sequential order of utterances as self-supervision for dialogue learning. In Section 3.1, we first introduce the task of inconsistent order detection, where the model needs to predict whether one sampled triple of the dialogue i"
P19-1375,D15-1199,0,0.0275607,"Missing"
P19-1375,E17-1042,0,0.0740121,"Missing"
P19-1375,P17-1062,0,0.0232804,"s more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and taskoriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets. 1 Introduction In recent years, dialogue systems have achieved fruitful results with neural conversation models in both open-domain generation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2016b, 2017; Xu et al., 2017; Zhang et al., 2018b) and task-oriented completion (Wen et al., 2015, 2017; Williams et al., 2017; Bordes et al., 2017; Su et al., 2018). These methods empower lots of real-world dialogue applications such as Google Home and Apple Siri. However, the utterance generation from dialogue systems still faces some critical challenges, including utterance blandness and incoherence (Gao et al., 2018). They are mainly caused by the objective function of the dialogue systems that prefer utterances with unconditionally high probability (Li et al., 2016a). We argue that in a meaningful and coherent dialogue, the change of utterance order will lead to a low-quality dialogue. However, most existing neu"
P19-1375,D17-1065,0,0.0694121,"ermore, we design a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and taskoriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets. 1 Introduction In recent years, dialogue systems have achieved fruitful results with neural conversation models in both open-domain generation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2016b, 2017; Xu et al., 2017; Zhang et al., 2018b) and task-oriented completion (Wen et al., 2015, 2017; Williams et al., 2017; Bordes et al., 2017; Su et al., 2018). These methods empower lots of real-world dialogue applications such as Google Home and Apple Siri. However, the utterance generation from dialogue systems still faces some critical challenges, including utterance blandness and incoherence (Gao et al., 2018). They are mainly caused by the objective function of the dialogue systems that prefer utterances with unconditionally high probability (Li et al., 2016a). We argue that in a meaningful and coherent dialo"
P19-1375,P18-1102,0,0.0988792,"a joint learning framework where SSN can guide the dialogue systems towards more coherent and relevant dialogue learning through adversarial training. We demonstrate that the proposed methods can be applied to both open-domain and taskoriented dialogue scenarios, and achieve the new state-of-the-art performance on the OpenSubtitiles and Movie-Ticket Booking datasets. 1 Introduction In recent years, dialogue systems have achieved fruitful results with neural conversation models in both open-domain generation (Ritter et al., 2011; Sordoni et al., 2015b; Li et al., 2016b, 2017; Xu et al., 2017; Zhang et al., 2018b) and task-oriented completion (Wen et al., 2015, 2017; Williams et al., 2017; Bordes et al., 2017; Su et al., 2018). These methods empower lots of real-world dialogue applications such as Google Home and Apple Siri. However, the utterance generation from dialogue systems still faces some critical challenges, including utterance blandness and incoherence (Gao et al., 2018). They are mainly caused by the objective function of the dialogue systems that prefer utterances with unconditionally high probability (Li et al., 2016a). We argue that in a meaningful and coherent dialogue, the change of u"
P19-1417,P17-1171,0,0.483694,"the relevant text piece. Knowledge bases (KBs) are considered as an essential resource for answering factoid questions. However, accurately constructing KB with a welldesigned and complicated schema requires lots of human efforts, which inevitably limits the coverage of KBs (Min et al., 2013). As a matter of fact, KBs are often incomplete and insufficient to cover full evidence required by open-domain questions. On the other hand, the vast amount of unstructured text on the Internet can easily cover a wide range of evolving knowledge, which is commonly used for open-domain question answering (Chen et al., 2017; Wang et al., 2018). Therefore, to improve the coverage of KBs, it is straightforward to augment KB with text data. Recently, text-based QA models along (Seo et al., 2016; Xiong et al., 2017; Yu et al., 2018) have achieved remarkable performance when dealing with a single passage that is guaranteed to include the answer. However, they are still insufficient when multiple documents https://github.com/xwhan/Knowledge-Aware-Reader. Cameron Newton (born May 11, 1989) plays for the Carolina Panthers of the National Football League (NFL) … Answer: Carolina Panthers Introduction 1 Document Retrieval"
P19-1417,P18-1076,0,0.0234033,"ear gate function P as γ e = g(~e, (ei ,ri )∈Ne s˜(ri ,ei ) σ(We [~ ri ; e~i ]))3 , which controls how much information in the original entity representation should be retained.4 3.2 Knowledge-Aware Text Reader Knowledge-aware Passage Enhancement To encode the retrieved passages, we use a standard bi-LSTM, which takes several token-level features5 . With the entity linking annotations in passages, we fuse the entity knowledge with the token-level features in a similar fashion as the query reformulation process. However, instead of applying a standard gating mechanism (Yang and Mitchell, 2017; Mihaylov and Frank, 2018), we propose a new conditional gating function that explicitly conditions on the question q~0 . This simple modification allows the reader to dynamically select the inputs according to their relevance to the question. Considering a passage token wid with its token features f~wd i and its linked entity ewi 6 , we define the conditional gating function as: ~idw = γ d e~0 wi + (1 − γ d )f~wd , where i i γ d = sigmoid(Wgd [~ q · e~0 w ; ~ q · f~wd ]). i i e~0 wi denotes the entity embedding learned by our SGR EADER. With the learned KB embeddings, our model enhances text reading with KAR EADER. Br"
P19-1417,D16-1147,0,0.0398533,"3 47.7 49.2 21.3 34.3 33.5 46.7 66.7 66.5 38.6 62.4 58.0 KV-KB+T EXT GN-LF GN-EF SGR EADER + KAR EADER (Ours) 24.6 29.8 31.5 33.6 14.4 17.0 17.7 18.9 27.0 39.1 40.7 42.6 17.7 25.9 25.2 27.1 32.5 46.2 49.9 52.7 23.6 35.6 34.7 36.1 40.5 65.4 67.8 67.2 30.9 56.8 60.4 57.3 GN-LF+EF (ensemble) 33.3 19.3 42.5 26.7 52.3 37.4 68.7 62.3 Table 1: Comparisons with Key-Value Memory Networks and GRAFT-Nets under different KB settings. downsampled to different extents. For a fair comparison, the retrieved document set is the same as the previous work. Baselines and Evaluation Key-Value (KV) Memory Network (Miller et al., 2016) is a simple baseline that treats KB triples and documents as memory cells. Specifically, we consider its two variants, KV-KB and KV-KB+Text. The former is a KB-only model while the latter uses both KB and text. We also compare to the latest method GraftNet (GN) (Sun et al., 2018), which treats documents as a special genre of nodes in KBs and utilizes graph convolution (Kipf and Welling, 2016) to aggregate the information. Similar to the KV-based baselines, we denote GN-KB as the KB-only version. Further, both GN-LF (late fusion) and GN-EF (early fusion) consider both KB and text. The former o"
P19-1417,N13-1095,0,0.0354997,"tball player Textual evidence: Cam Newton plays for Panthers Figure 1: A real example from WebQSP. Here the answer cannot be directly found in the KB. But the knowledge provided by the KB, i.e., Cam Newton is a football player, indicates he signed with the team he plays for. This knowledge can be essential for recognizing the relevant text piece. Knowledge bases (KBs) are considered as an essential resource for answering factoid questions. However, accurately constructing KB with a welldesigned and complicated schema requires lots of human efforts, which inevitably limits the coverage of KBs (Min et al., 2013). As a matter of fact, KBs are often incomplete and insufficient to cover full evidence required by open-domain questions. On the other hand, the vast amount of unstructured text on the Internet can easily cover a wide range of evolving knowledge, which is commonly used for open-domain question answering (Chen et al., 2017; Wang et al., 2018). Therefore, to improve the coverage of KBs, it is straightforward to augment KB with text data. Recently, text-based QA models along (Seo et al., 2016; Xiong et al., 2017; Yu et al., 2018) have achieved remarkable performance when dealing with a single pa"
P19-1417,P18-1150,0,0.0594365,"Missing"
P19-1417,D18-1455,0,0.206151,"Missing"
P19-1417,P17-1132,0,0.0248211,"meter calculated by a linear gate function P as γ e = g(~e, (ei ,ri )∈Ne s˜(ri ,ei ) σ(We [~ ri ; e~i ]))3 , which controls how much information in the original entity representation should be retained.4 3.2 Knowledge-Aware Text Reader Knowledge-aware Passage Enhancement To encode the retrieved passages, we use a standard bi-LSTM, which takes several token-level features5 . With the entity linking annotations in passages, we fuse the entity knowledge with the token-level features in a similar fashion as the query reformulation process. However, instead of applying a standard gating mechanism (Yang and Mitchell, 2017; Mihaylov and Frank, 2018), we propose a new conditional gating function that explicitly conditions on the question q~0 . This simple modification allows the reader to dynamically select the inputs according to their relevance to the question. Considering a passage token wid with its token features f~wd i and its linked entity ewi 6 , we define the conditional gating function as: ~idw = γ d e~0 wi + (1 − γ d )f~wd , where i i γ d = sigmoid(Wgd [~ q · e~0 w ; ~ q · f~wd ]). i i e~0 wi denotes the entity embedding learned by our SGR EADER. With the learned KB embeddings, our model enhances text"
P19-1417,P14-2105,0,0.0349953,"ts of our model consist of a graph-attention based KB reader (§3.1) and a knowledge-aware text reader (§3.2). The interaction between the modules is shown in Figure 2. 3.1 SubGraph Reader This section describes the KB subgraph reader (SGR EADER), which employs graph-attention techniques to accumulate knowledge of each subgraph entity (e) from its linked neighbors (Ne ). The graph attention mechanism is particularly designed to take into account two important aspects: (1) whether the neighbor relation is relevant to the question; (2) whether the neighbor entity is a topic 2 Annotated by STAGG (Yih et al., 2014). entity mentioned by the question. After the propagation, the SGR EADER finally outputs a vectorized representation for each entity, encoding the knowledge indicated by its linked neighbors. Question-Relation Matching To match the question and KB relation in an isomorphic latent space, we apply a shared LSTM to encode the question {w1q , w2q , ..., wlqq } and the tokenized relation {w1r , w2r , ..., wlrr }. With the derived hidden states hq ∈ Rlq ×dh and hr ∈ Rlr ×dh for each word, we first compute the representation of relations with a self-attentive encoder: ~r = X αi~hri , αi ∝ exp(w~r · ~"
P19-1417,P16-2033,0,0.206707,"Missing"
P19-1417,P17-1053,1,\N,Missing
P19-1496,W11-2107,0,0.0849864,"Missing"
P19-1496,P11-2008,0,0.0618779,"Missing"
P19-1496,P17-1044,0,0.0754159,"Missing"
P19-1496,D14-1108,0,0.0518288,"Penn Treebank (Marcus et al., 1993). In recent 3 https://developer.twitter.com/ years, with the increasing usage of social media platforms, several NLP techniques and datasets for processing social media text have been proposed. For example, Gimpel et al. (2011) build a Twitter part-of-speech tagger based on 1,827 manually annotated tweets. Ritter et al. (2011) annotated 800 tweets, and performed an empirical study for partof-speech tagging and chunking on a new Twitter dataset. They also investigated the task of Twitter Named Entity Recognition, utilizing a dataset of 2,400 annotated tweets. Kong et al. (2014) annotated 929 tweets, and built the first dependency parser for tweets, whereas Wang et al. (2014) built the Chinese counterpart based on 1,000 annotated Weibo posts. To the best of our knowledge, question answering and reading comprehension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets. Reading Comprehension Machine reading comprehension (RC) aims to answer questions by comprehending evidence from passages. This direction has recently drawn much attention due to the f"
P19-1496,D17-1082,0,0.0442583,"ension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets. Reading Comprehension Machine reading comprehension (RC) aims to answer questions by comprehending evidence from passages. This direction has recently drawn much attention due to the fast development of deep learning techniques and large-scale datasets. The early development of the RC datasets focuses on either the cloze-style (Hermann et al., 2015; Hill et al., 2015) or quiz-style problems (Richardson et al., 2013; Lai et al., 2017). The former one aims to generate single-token answers from automatically constructed pseudo-questions while the latter requires choosing from multiple answer candidates. However, such unnatural settings make them fail to serve as the standard QA benchmarks. Instead, researchers started to ask human annotators to create questions and answers given passages in a crowdsourced way. Such efforts give the rise of large-scale human-annotated RC datasets, many of which are quite popular in the community such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016), NewsQA (Trischler et al.,"
P19-1496,J93-2004,0,0.0681079,"Missing"
P19-1496,D16-1147,0,0.0464777,"s for core natural language understanding tasks involving syntactic and semantic analysis have been developed for noisy social media text (Gimpel et al., 2011; Ritter et al., 2011; Kong et al., 2014; Wang et al., 2014), there is little work on question answering or reading comprehension over social media, with the primary bottleneck being the lack of available datasets. We observe that recently proposed QA datasets usually focus on formal domains, e.g. CNN/DAILY M AIL (Hermann et al., 2015) and NewsQA (Trischler et al., 2016) on news articles; SQuAD (Rajpurkar et al., 2016) and W IKI M OVIES (Miller et al., 2016) that use Wikipedia. In this paper, we propose the first large-scale dataset for QA over social media data. Rather than naively obtaining tweets from Twitter using 5020 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5020–5031 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the Twitter API3 which can yield irrelevant tweets with no valuable information, we restrict ourselves only to tweets which have been used by journalists in news articles thus implicitly implying that such tweets contain useful and re"
P19-1496,D18-1009,0,0.0434984,"On pretraining model Although BERT was demonstrated to be a powerful tool for reading comprehension, this is the first time a detailed analysis has been done on its reasoning skills. From the results, the huge improvement of BERT mainly comes from two types. The first is paraphrasing, which is not surprising because that a well pretrained language model is expected to be able to better encode sentences. Thus the derived embedding space could work better for sentence comparison. The second type is commonsense, which is consistent with the good performance of BERT (Devlin et al., 2018) on SWAG (Zellers et al., 2018). We believe that this provides further evidence about the connection between largescaled deep neural language model and certain kinds of commonsense. 6 Conclusion We present the first dataset for QA on social media data by leveraging news media and crowdsourcing. The proposed dataset informs us of the distinctiveness of social media from formal domains in the context of QA. Specifically, we find that QA on social media requires systems to comprehend social media specific linguistic patterns like informality, hashtags, usernames, and authorship. These distinguishing linguistic factors bring up"
P19-1496,P02-1040,0,0.103171,"Missing"
P19-1496,N18-1202,0,0.0112655,"is further modeled by an RNN layer to make the span predictions. Since our T WEET QA does not have labeled answer spans as in SQuAD, we need to use the human-written answers to retrieve the answerspan labels for training. To get the approximate answer spans, we consider the same matching approach as in the query matching baseline. But instead of using questions to do matching, we use the human-written answers to get the spans that achieve the best BLEU-1 scores. Fine-Tuning BERT This is another extractive RC model that benefits from the recent advance in pretrained general language encoders (Peters et al., 2018; Devlin et al., 2018). In our work, we select the BERT model (Devlin et al., 2018) which has achieved the best performance on SQuAD. In our experiments, we use the PyTorch reimple5026 5.2 Evaluation on Dev/Test Data Models BLEU-1 METEOR ROUGE-L H UMAN E XTRACT-UB 76.4|78.2 79.5|80.3 63.7|66.7 68.8|69.8 70.9|73.5 74.3|75.6 Query-Matching 30.3|29.4 12.0|12.1 17.0|17.4 BiDAF Generative BERT Neural Baselines 48.3|48.7 31.6|31.4 53.4|53.7 32.1|31.8 67.3|69.6 56.9|58.6 38.9|38.6 39.5|39.0 62.6|64.1 Reasoning Types Paraphrasing Sentence relations Authorship Oral/Tweet habits UserIDs&Hashtags Commons"
P19-1496,D16-1264,0,0.454066,"s-2017/ In recent years, while several tools for core natural language understanding tasks involving syntactic and semantic analysis have been developed for noisy social media text (Gimpel et al., 2011; Ritter et al., 2011; Kong et al., 2014; Wang et al., 2014), there is little work on question answering or reading comprehension over social media, with the primary bottleneck being the lack of available datasets. We observe that recently proposed QA datasets usually focus on formal domains, e.g. CNN/DAILY M AIL (Hermann et al., 2015) and NewsQA (Trischler et al., 2016) on news articles; SQuAD (Rajpurkar et al., 2016) and W IKI M OVIES (Miller et al., 2016) that use Wikipedia. In this paper, we propose the first large-scale dataset for QA over social media data. Rather than naively obtaining tweets from Twitter using 5020 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5020–5031 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics the Twitter API3 which can yield irrelevant tweets with no valuable information, we restrict ourselves only to tweets which have been used by journalists in news articles thus implicitly implyin"
P19-1496,D13-1020,0,0.157474,"ering and reading comprehension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets. Reading Comprehension Machine reading comprehension (RC) aims to answer questions by comprehending evidence from passages. This direction has recently drawn much attention due to the fast development of deep learning techniques and large-scale datasets. The early development of the RC datasets focuses on either the cloze-style (Hermann et al., 2015; Hill et al., 2015) or quiz-style problems (Richardson et al., 2013; Lai et al., 2017). The former one aims to generate single-token answers from automatically constructed pseudo-questions while the latter requires choosing from multiple answer candidates. However, such unnatural settings make them fail to serve as the standard QA benchmarks. Instead, researchers started to ask human annotators to create questions and answers given passages in a crowdsourced way. Such efforts give the rise of large-scale human-annotated RC datasets, many of which are quite popular in the community such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016), NewsQA"
P19-1496,D11-1141,0,0.115207,"Missing"
P19-1496,D14-1122,1,0.853261,"reasing usage of social media platforms, several NLP techniques and datasets for processing social media text have been proposed. For example, Gimpel et al. (2011) build a Twitter part-of-speech tagger based on 1,827 manually annotated tweets. Ritter et al. (2011) annotated 800 tweets, and performed an empirical study for partof-speech tagging and chunking on a new Twitter dataset. They also investigated the task of Twitter Named Entity Recognition, utilizing a dataset of 2,400 annotated tweets. Kong et al. (2014) annotated 929 tweets, and built the first dependency parser for tweets, whereas Wang et al. (2014) built the Chinese counterpart based on 1,000 annotated Weibo posts. To the best of our knowledge, question answering and reading comprehension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets. Reading Comprehension Machine reading comprehension (RC) aims to answer questions by comprehending evidence from passages. This direction has recently drawn much attention due to the fast development of deep learning techniques and large-scale datasets. The early development of the"
P19-1560,D17-1169,0,0.0229517,"apable of generating emotional texts and capturing greater diversity than traditional SEQ2SEQ models. We give an example of the structure of CVAE+GEF in Figure 2. For space consideration, we leave out the detailed structure of CVAE, and will elaborate it in the supplementary materials. In this architecture, golden explanations eg and generated explanations ec are both composed of three text comments: positive comments, negative comments, and neutral comments, which are ﬁnegrained explanations for the ﬁnal overall rating. The classiﬁer is a skip-connected model of bidirectional GRU-RNN layers (Felbo et al., 2017). It takes three kinds of comments as inputs, and outputs the probability distribution over the predicted classiﬁcations. 3.4.2 Case 2: Numerical Explanations Another frequently employed form of the ﬁnegrained explanations for the overall results is numerical scores. For example, when a user wants to rate a product, s/he may ﬁrst rate some attributes of the product, like the packaging, price, etc. After rating all the attributes, s/he will give an overall rating for the product. So we can say that the rating for the attributes can somewhat explain 4.1 PCMag Review Dataset This dataset is crawl"
P19-1560,P16-1154,0,0.0336764,"g series is a stylish 27-inch monitor offering good color reproduction and sharp image quality. however, it ’s more expensive than most tn monitors and has a limited feature set. Table 9: Examples of our generated explanations. Some key points are underlined. Second, there are UNKs in the generated explanations. Since we are generating abstractive comments for product reviews, there may exist some domain-speciﬁc words. The frequency of these special words is low, so it is relatively hard for GEF to learn to embed and generated these words. A substituted way is that we can use copymechanism (Gu et al., 2016) to generate these domain-speciﬁc words. 6 Related Work Our work is closely aligned with Explainable Artiﬁcial Intelligence (Gunning, 2017), which is claimed to be essential if users are to understand, and effectively manage this incoming generation of artiﬁcially intelligent partners. In artiﬁcial intelligence, providing an explanation of individual decisions has attracted attention in recent years. The traditional way of explaining the results is to build connections between the input and output, and ﬁgure out how much each dimension or element contributes to the ﬁnal output. Some previous w"
P19-1560,P18-1175,0,0.05296,"Missing"
P19-1560,D14-1181,0,0.00320295,"an overall rating score. We regard the three short comments as ﬁne-grained information for the long review text. Besides, we also conduct experiments on the Skytrax User Reviews Dataset2 , where each case consists of three parts: a review text for a ﬂight, ﬁve sub-ﬁeld rating scores (seat comfortability, cabin stuff, food, in-ﬂight environment, ticket value) and an overall rating score. As for this dataset, we regard the ﬁve sub-ﬁeld rating scores as ﬁne-grained information for the ﬂight review text. Empirically, we evaluate our model-agnostic method on several neural network baseline models (Kim, 2014; Liu et al., 2016; Zhou and Wang, 2018) for both datasets. Experimental results suggest that our approach substantially improves the performance over baseline systems, illustrating the advantage of utilizing ﬁne-grained information. Meanwhile, by providing the ﬁne-grained information as explanations for the classiﬁcation results, our model is an understandable system that is worth trusting. Our major contributions are three-fold: • We are the ﬁrst to leverage the generated ﬁnegrained information for building a generative explanation framework for text classiﬁcation, propose an explanation fac"
P19-1560,D16-1011,0,0.0430271,"o ways: evaluating the sensitivity of output if input changes and analyzing the result from a mathematical perspective by redistributing the prediction function backward (Samek et al., 2018). There are some works connecting the result with the classiﬁcation model. Ribeiro et al. (2016) selects a set of representative instances with explanations via submodular optimization. Although the method is promising and mathematically reasonable, they cannot generate explanations in natural forms. They focus on how to interpret the result. Some of the previous works have similar motivations as our work. Lei et al. (2016) rationalize neural prediction by extracting the phrases from the input texts as explanations. They conduct their work in an extractive way, and focus on rationalizing the predictions. However, our work aims not only to predict the results but also to generate abstractive explanations, and our framework can generate explanations both in the forms of texts and numerical scores. Hancock et al. (2018) proposes to use a classiﬁer with natural language explanations that are annotated by human beings to do the classiﬁcation. Our work is different from theirs in that we use the natural attributes as"
P19-1560,P14-5010,0,0.00241325,"is cheap , but too heavy . CVAE ec ec1: pricy . ec2: heavy . ec3: just good . ve Pclassified p1=0.5 p2=0.2 p3=0.1 p4=0.2 ௦௦ௗ = 0.2 Ppred classifier Pgold eg eg1: cheap . eg2: heavy . eg3: worthy to buy . p1=0.2 p2=0.3 p3=0.3 p4=0.2 4 p1=0.3 p2=0.1 p3=0.2 p4=0.4 We conduct experiments on two datasets where we use texts and numerical ratings to represent ﬁnegrained information respectively. The ﬁrst one is crawled from a website called PCMag, and the other one is the Skytrax User Reviews Dataset. Note that all the texts in the two datasets are preprocessed by the Stanford Tokenizer3 (Manning et al., 2014). y=4 ௗ = 0.3 Dataset ௗ = 0.1  |= ܵ ܨܧ௦௦ௗ െ ௗ |+ |௦௦ௗ െ ௗ | = 0.2 െ 0.3 + 0.2 െ 0.1 = 0.2 Figure 2: Structure of CVAE+GEF. There are totally 4 categories for the classiﬁcation, and the ground-truth category is 2 in this example. We assume that the pretrained classiﬁer is a ”perfect” classiﬁer that will correctly predict the ﬁnal label to be 2 when taking eg as input. So we wish the classiﬁer can also predict the ﬁnal result as label 2 when taking ec as input. This is why we focus on p˜classif ied and p˜gold . is found to be capable of generatin"
P19-1560,P02-1040,0,0.103532,"Missing"
P19-1560,D14-1162,0,0.0819626,"2.9 1.2 1.5 Table 4: BLEU scores for generated explanations. Pos., Neg., Neu. respectively stand for positive, negative and neural explanations. The low BLEU-3 and BLEU-4 scores are because the target explanations contain many domain-speciﬁc words with low frequency, which makes it hard for the model to generate accurate explanations. Experiments and Analysis Experimental Settings As the goal of this study is to propose an explanation framework, in order to test the effectiveness of proposed GEF, we use the same experimental settings on the base model and on the base model+GEF. We use GloVe (Pennington et al., 2014) word embedding for PCMag dataset and minimize the objective function using Adam (Kingma and Ba, 2014). The hyperparameter settings for both datasets are listed in Table 3. Meanwhile, since the generation loss is larger than classiﬁcation loss for text explanations, we stop updating the predictor after classiﬁcation loss reaches a certain threshold (adjusted based on dev set) to avoid overﬁtting. 5.2 Neu. CVAE CVAE+GEF CVAE CVAE+GEF CVAE CVAE+GEF Experimental Results 5.2.1 Results of Text Explanations We use BLEU (Papineni et al., 2002) scores to evaluation the quality of generated text explan"
P19-1560,N18-1202,0,0.0228838,"nd the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and ﬁne-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time. 1 Introduction Deep learning methods have produced state-ofthe-art results in many natural language processing (NLP) tasks (Vaswani et al., 2017; Yin et al., 2018; Peters et al., 2018; Wang et al., 2018; Hancock et al., 2018; Ma et al., 2018). Though these deep neural network models achieve impressive performance, it is relatively difﬁcult to convince people to trust the predictions of such neural networks since they are actually black boxes for human beings (Samek et al., 2018). For instance, if an essay scoring system only tells the scores of a given essay without providing explicit reasons, the users can hardly be convinced of the judgment. Therefore, the ability to explain the rationale is essential for a NLP system, a need which requires traditional NLP models to prov"
P19-1560,N16-3020,0,0.677724,"ey are actually black boxes for human beings (Samek et al., 2018). For instance, if an essay scoring system only tells the scores of a given essay without providing explicit reasons, the users can hardly be convinced of the judgment. Therefore, the ability to explain the rationale is essential for a NLP system, a need which requires traditional NLP models to provide human-readable explanations. In recent years, lots of works have been done to solve text classiﬁcation problems, but just a few of them have explored the explainability of their systems (Camburu et al., 2018; Ouyang et al., 2018). Ribeiro et al. (2016) try to identify an interpretable model over the interpretable representation that is locally faithful to the classiﬁer. Samek et al. (2018) use heatmap to visualize how much each hidden element contributes to the predicted results. Although these systems are somewhat promising, they typically do not consider ﬁnegrained information that may contain information for interpreting the behavior of models. However, if a human being wants to rate a product, s/he may ﬁrst write down some reviews, and then score or summarize some attributes of the product, like price, packaging, and quality. Finally, t"
P19-1560,D15-1167,0,0.0268387,"aset, which means that ec can be in the form of texts or in the form of numerical scores. We apply GEF to both forms of explanations using different base models. 3.4.1 Case 1: Text Explanations To test the performance of GEF on generating text explanations, we apply GEF to Conditional Variational Autoencoder (CVAE) (Sohn et al., 2015). We here utilize CVAE because we want to generate explanations conditioned on different emotions (positive, negative and neural) and CVAE 5573 why the user gives the overall rating. LSTM and CNN are shown to achieve great performance in text classiﬁcation tasks (Tang et al., 2015), so we use LSTM and CNN models as the encoder E respectively. The numerical explanations are also regarded as a classiﬁcation problem in this example. it is cheap , but too heavy . CVAE ec ec1: pricy . ec2: heavy . ec3: just good . ve Pclassified p1=0.5 p2=0.2 p3=0.1 p4=0.2 ௦௦ௗ = 0.2 Ppred classifier Pgold eg eg1: cheap . eg2: heavy . eg3: worthy to buy . p1=0.2 p2=0.3 p3=0.3 p4=0.2 4 p1=0.3 p2=0.1 p3=0.2 p4=0.4 We conduct experiments on two datasets where we use texts and numerical ratings to represent ﬁnegrained information respectively. The ﬁrst one is crawled from a website cal"
P19-1560,P18-1130,0,0.0138858,"ore reasonable explanations. We construct two new datasets that contain summaries, rating scores, and ﬁne-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time. 1 Introduction Deep learning methods have produced state-ofthe-art results in many natural language processing (NLP) tasks (Vaswani et al., 2017; Yin et al., 2018; Peters et al., 2018; Wang et al., 2018; Hancock et al., 2018; Ma et al., 2018). Though these deep neural network models achieve impressive performance, it is relatively difﬁcult to convince people to trust the predictions of such neural networks since they are actually black boxes for human beings (Samek et al., 2018). For instance, if an essay scoring system only tells the scores of a given essay without providing explicit reasons, the users can hardly be convinced of the judgment. Therefore, the ability to explain the rationale is essential for a NLP system, a need which requires traditional NLP models to provide human-readable explanations. In recent years, lots of w"
P19-1560,P18-1158,0,0.0160601,"raining approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and ﬁne-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time. 1 Introduction Deep learning methods have produced state-ofthe-art results in many natural language processing (NLP) tasks (Vaswani et al., 2017; Yin et al., 2018; Peters et al., 2018; Wang et al., 2018; Hancock et al., 2018; Ma et al., 2018). Though these deep neural network models achieve impressive performance, it is relatively difﬁcult to convince people to trust the predictions of such neural networks since they are actually black boxes for human beings (Samek et al., 2018). For instance, if an essay scoring system only tells the scores of a given essay without providing explicit reasons, the users can hardly be convinced of the judgment. Therefore, the ability to explain the rationale is essential for a NLP system, a need which requires traditional NLP models to provide human-readable"
P19-1560,P18-1053,1,0.830324,"plainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and ﬁne-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time. 1 Introduction Deep learning methods have produced state-ofthe-art results in many natural language processing (NLP) tasks (Vaswani et al., 2017; Yin et al., 2018; Peters et al., 2018; Wang et al., 2018; Hancock et al., 2018; Ma et al., 2018). Though these deep neural network models achieve impressive performance, it is relatively difﬁcult to convince people to trust the predictions of such neural networks since they are actually black boxes for human beings (Samek et al., 2018). For instance, if an essay scoring system only tells the scores of a given essay without providing explicit reasons, the users can hardly be convinced of the judgment. Therefore, the ability to explain the rationale is essential for a NLP system, a need which requires tradition"
P19-1560,P18-1104,1,0.843571,"egard the three short comments as ﬁne-grained information for the long review text. Besides, we also conduct experiments on the Skytrax User Reviews Dataset2 , where each case consists of three parts: a review text for a ﬂight, ﬁve sub-ﬁeld rating scores (seat comfortability, cabin stuff, food, in-ﬂight environment, ticket value) and an overall rating score. As for this dataset, we regard the ﬁve sub-ﬁeld rating scores as ﬁne-grained information for the ﬂight review text. Empirically, we evaluate our model-agnostic method on several neural network baseline models (Kim, 2014; Liu et al., 2016; Zhou and Wang, 2018) for both datasets. Experimental results suggest that our approach substantially improves the performance over baseline systems, illustrating the advantage of utilizing ﬁne-grained information. Meanwhile, by providing the ﬁne-grained information as explanations for the classiﬁcation results, our model is an understandable system that is worth trusting. Our major contributions are three-fold: • We are the ﬁrst to leverage the generated ﬁnegrained information for building a generative explanation framework for text classiﬁcation, propose an explanation factor, and introduce minimum risk training"
P19-1646,D18-1418,0,0.0226284,"to the trade-off between the expected information gain and the cost of asking a question. 2.2 Dialogue Generation and Visual Dialogue Dialogue generation is an important research topic in NLP, thus many approaches have been proposed to address this task. Most earlier works made use of a predefined template (Lemon et al., 2006; Wang and Lemon, 2013) to generate dialogues. More recently, deep neural networks have been used for building end-to-end architectures capable of generating questions (Vinyals and Le, 2015; Sordoni et al., 2015) and also for the task of goal-oriented dialogue generation (Rajendran et al., 2018; Bordes et al., 2017). Visual dialogue focuses on having a conversation about an image with either one or both of the agents being a machine. Since its inception (Das et al., 2017), different approaches have been proposed to address this problem (Massiceti et al., 2018; Lu et al., 2017; Das et al., 2017). Goaloriented Visual Dialogue, on the other hand, is an area that has only been introduced fairly recently. De Vries et al. (De Vries et al., 2017) proposed the GuessWhat?! dataset for goal-oriented visual dialogue while Strub et al. (Strub et al., 2017) developed a reinforcement learning app"
P19-1646,N15-1020,0,0.0802839,"Missing"
P19-1646,E06-2009,0,0.0276769,"while others resort to Bayesian inference. Coenen et al. (Coenen et al., 2017), for instance, came up with nine important questions about human inquiry, while one recent model called Rational Speech Act (RSA) (Hawkins et al., 2015) considers questions as a distribution that is proportional to the trade-off between the expected information gain and the cost of asking a question. 2.2 Dialogue Generation and Visual Dialogue Dialogue generation is an important research topic in NLP, thus many approaches have been proposed to address this task. Most earlier works made use of a predefined template (Lemon et al., 2006; Wang and Lemon, 2013) to generate dialogues. More recently, deep neural networks have been used for building end-to-end architectures capable of generating questions (Vinyals and Le, 2015; Sordoni et al., 2015) and also for the task of goal-oriented dialogue generation (Rajendran et al., 2018; Bordes et al., 2017). Visual dialogue focuses on having a conversation about an image with either one or both of the agents being a machine. Since its inception (Das et al., 2017), different approaches have been proposed to address this problem (Massiceti et al., 2018; Lu et al., 2017; Das et al., 2017"
P19-1646,W13-4067,0,0.0331746,"to Bayesian inference. Coenen et al. (Coenen et al., 2017), for instance, came up with nine important questions about human inquiry, while one recent model called Rational Speech Act (RSA) (Hawkins et al., 2015) considers questions as a distribution that is proportional to the trade-off between the expected information gain and the cost of asking a question. 2.2 Dialogue Generation and Visual Dialogue Dialogue generation is an important research topic in NLP, thus many approaches have been proposed to address this task. Most earlier works made use of a predefined template (Lemon et al., 2006; Wang and Lemon, 2013) to generate dialogues. More recently, deep neural networks have been used for building end-to-end architectures capable of generating questions (Vinyals and Le, 2015; Sordoni et al., 2015) and also for the task of goal-oriented dialogue generation (Rajendran et al., 2018; Bordes et al., 2017). Visual dialogue focuses on having a conversation about an image with either one or both of the agents being a machine. Since its inception (Das et al., 2017), different approaches have been proposed to address this problem (Massiceti et al., 2018; Lu et al., 2017; Das et al., 2017). Goaloriented Visual"
W11-2018,E09-1004,0,0.0171781,", Mean, Median, SDev, MAS Min, Max, Mean, SDev Prosodic Events Pitch accents, intermediate phrase, and intonational boundaries. Table 1: Feature Sets. RAP: Relative Average Perturbation. PPQ5: five-point Period Perturbation Quotient. APQn: n-point Amplitude Perturbation Quotient. NHR: Noise-to-Harmonics Ratio. MAS: Mean Absolute Slope. own emotional state. Its pleasantness (EE) score indicates the negative or positive valence of a word, rated on a scale from 1 to 3. For example, “abandon” scores 1.0, implying a fairly low level of pleasC(t, u) |U | V (t, u) = ∗log P antness. A previous study (Agarwal et al., 2009) C(v, u) u(t) ∗ (1 − |M eanLOI|) notes that one of the advantages of this dictionary Here, the Mean LOI score ranging from (0,1) is is that it has different scores for various forms of a the label of each utterance. Instead of summing root word. For example, the words “affect” and “afthe u(t) scores directly, we now assign a weight to fection” have very different meanings; if they were each utterance. The weight is (1 − |M eanLOI|) in given the same score, the lexical affect quantificaour task. The overall IDF score of words important tion might not be discriminative. To calculate an to identi"
W11-2018,P96-1038,1,0.340624,"le 1). Prosodic Event Features To examine the contribution of higher-level prosodic events, we have also experimented with AuToBI (Rosenberg, 2010) to automatically detect pitch accents, word boundaries, intermediate phrase boundaries, and intonational boundaries in utterances. AuToBI requires annotated word boundary information; since we do not have hand-annotated boundaries, we use the Penn Phonetics Lab Forced Aligner (Yuan and Liberman, 2008) to align each utterance with its transcription. We use AuToBI’s models, which were trained on the spontaneous speech Boston Directions Corpus (BDC) (Hirschberg and Nakatani, 1996), to identify prosodic events in our corpus. 3.3 Fusion Learning Approaches Assuming that our various lexical, acoustic and prosodic feature streams are informative to some extent when tested separately, we want to combine information from the streams in different domains to improve prediction. We experimented with several approaches, including Bag-of-Features, Sum Rule combination, Hierarchical Fusion, and a new approach. We present here results of each on our LOI prediction task. In the Bag-of-Features approach, a simple classification method includes all features in a single classifier. A p"
W11-2018,W00-1308,0,0.0736547,"Missing"
W11-2018,C10-1129,1,0.916195,"Missing"
W12-1603,W08-0105,1,0.870302,"Tickle-Degnen and Rosenthal (1990) model provides a starting point by outlining the components of rapport, including the finding that positivity decreases over the course of a relationship. The popularity of this model, however, has not diminished the disproportionate attention that positivity and politeness receive in analyses of rapport (Brown and Levinson, 1978), including in the vast majority of computational approaches to rapport-building in dialogue (Stronks et al., 2002; Johnson and Rizzo, 2004; Bickmore and Picard, 2005; Gratch et al., 2006; McLaren et al., 2007; Cassell et al., 2007; Baker et al., 2008; Bickmore et al., 2011). The creation and expression of rapport is complex, and can also be signaled through negative, or impolite, exchanges (Straehle, 1993; Watts, 2003; SpencerOatey, 2008) that communicate affection and relationship security among intimates who can flout common social norms (Culpeper, 2011; Kienpointner, 1997). However, it is an open question as to whether such rudeness is likely to impress a new student on the first day of class. We must better understand how and when impoliteness and other negative dialogue moves can contribute to the development and expression of the ra"
W12-1603,W07-1906,1,0.946194,"predictive power of our models under various settings, and compare our sparse models with standard non-sparse solutions. Our experiments demonstrate that our models are more accurate than non-sparse models quantitatively, and that teens use unexpected kinds of language to do relationship work such as signaling rapport, but friends and strangers, tutors and tutees, carry out this work in quite different ways from one another. 1 Introduction and Related Work Rapport, the harmonious synchrony between interlocutors, has numerous benefits for a range of dialogue types, including direction giving (Cassell et al., 2007) or contributing to patient recovery (Vowles and Thompson, 2012). In peer tutoring, an educational paradigm in which students of similar ability tutor one another, friendship among tutors and tutees leads to better learning (Gartner et al., 1971). With the burgeoning use of spoken dialogue systems in education, understanding the process by which two humans build and signal rapport during learning becomes a vital step for implementing spoken dialogue systems (SDSs) that can initiate (and, as importantly, maintain) a successful relationship with students over time. However, implementing a tutori"
W12-1603,W11-2003,0,0.061362,"Missing"
W12-1603,P04-1045,0,0.0392667,"ons of Lasso and ridge estimators, and enforces composite penalty. In addition to the model comparisons, by varying the different sizes of feature windows (number of turns in the dialogue history), we empirically show that our proposed sparse log-linear model is flexible, enabling the model to capture long-range dependency. This approach also allows us to extend previous work on speaker state prediction. Although speaker state prediction has attracted much attention in the dialogue research community, most studies have focused on the analysis of anger, frustration, and other classic emotions (Litman and Forbes-Riley, 2004; Liscombe et al., 2005; Devillers and Vidrascu, 2006; Ai et al., 2006; Grimm et al., 2007; Gupta and Nitendra., 2007; Metallinou et al., 2011). Recently, Wang and Hirschberg (2011) proposed a hierarchical model that detects level of interest of speakers in dialogue, using a multistream prediction feedback technique. However, to the best of our knowledge, we are among the first to study the problem of automatic impoliteness and positivity prediction in dialogue. Because our ultimate goal is to build an SDS that responds to users’ language use over time, the features from the user’s target turn"
W12-1603,D11-1139,0,0.0500211,"MLE training method not only produces dense models, but may also overestimates lower frequency features that might be unreliable signals and overfit to a particular set of speakers. In recent studies on speaker state prediction that use lexical features, it has been shown that MLE estimators demonstrate large performance gaps between non-overlapping speaker datasets (Jeon et al., 2010; Wang et al., 2012a). On the other hand, recent studies on `1 /`2 based group penalty for evaluating dialogue systems (Gonz´alez-Brenes and Mostow, 2011), structured sparsity for linguistic structure prediction (Martins et al., 2011), and discovering historical legal opinions with a sparse mixed-effects latent variable model (Wang et al., 2012b) have all shown concrete benefits of modeling sparsity in languagerelated predictive tasks. We therefore apply sparsitysensitive models that can prevent less frequent features from overfitting. We start with the `1 regularized Lasso (Tibshirani, 1994) model, since, compared to other covariance matrix based sparse models, such as sparse Principal Component Analysis (PCA) and sparse Canonical Correlation Analysis (CCA), the Lasso model is straightforward and requires fewer computing"
W12-1603,W11-2018,1,0.838614,"gue history), we empirically show that our proposed sparse log-linear model is flexible, enabling the model to capture long-range dependency. This approach also allows us to extend previous work on speaker state prediction. Although speaker state prediction has attracted much attention in the dialogue research community, most studies have focused on the analysis of anger, frustration, and other classic emotions (Litman and Forbes-Riley, 2004; Liscombe et al., 2005; Devillers and Vidrascu, 2006; Ai et al., 2006; Grimm et al., 2007; Gupta and Nitendra., 2007; Metallinou et al., 2011). Recently, Wang and Hirschberg (2011) proposed a hierarchical model that detects level of interest of speakers in dialogue, using a multistream prediction feedback technique. However, to the best of our knowledge, we are among the first to study the problem of automatic impoliteness and positivity prediction in dialogue. Because our ultimate goal is to build an SDS that responds to users’ language use over time, the features from the user’s target turn that the model is aiming to predict are not observable, which renders the task more difficult than previous speaker state detection tasks. Our main contributions are three-fold: (1"
W12-1603,C10-1129,1,0.79665,"actually explainin urself..”) (Kienpointner, 1997). 3.2 Automated Features To compare the performance between what could be automatically extracted from dialogue and hand annotation, we extracted 2,872 unigram and 12,016 bigram features from the text corpus. Using the Stanford PoS tagger4 with its attached model, we also extracted 46 common part-of-speech tags from the text. In addition to the above lexical and syntactic features, we automatically extracted the capitalization features[Ca] that have at least one full word (eg. “CALM DOWN”) (Chovanec, 2009). Since a recent text prediction task (Wang and McKeown, 2010) observed benefits from modeling punctuation features[P], we extracted the expressive punctuation that included at least one exclamation point or more than one question-mark (eg. “I don’t get it?!??!”) (Crystal, 2001). We used a smiley dictionary5 to extract the emoticons[E] that convey emotional states (S´anchez et al., 2006) from text. 4 Sparse Log-Linear Models We formulate our impoliteness and positivity prediction problems as binary classifications. To do this, ˆ First, we we estimate the label yˆt ∼ Bernoulli(θ). introduce a standard log-linear parametrization6 to our predictive tasks: P"
W12-1603,P12-1078,1,0.839762,"atively rarely in data of this sort. Training discriminative models with maximum likelihood estimators (MLE) on such datasets usually results in assigning too much weight on less frequent signals. This standard MLE training method not only produces dense models, but may also overestimates lower frequency features that might be unreliable signals and overfit to a particular set of speakers. In recent studies on speaker state prediction that use lexical features, it has been shown that MLE estimators demonstrate large performance gaps between non-overlapping speaker datasets (Jeon et al., 2010; Wang et al., 2012a). On the other hand, recent studies on `1 /`2 based group penalty for evaluating dialogue systems (Gonz´alez-Brenes and Mostow, 2011), structured sparsity for linguistic structure prediction (Martins et al., 2011), and discovering historical legal opinions with a sparse mixed-effects latent variable model (Wang et al., 2012b) have all shown concrete benefits of modeling sparsity in languagerelated predictive tasks. We therefore apply sparsitysensitive models that can prevent less frequent features from overfitting. We start with the `1 regularized Lasso (Tibshirani, 1994) model, since, compa"
Y95-1001,J86-2003,0,0.0880973,"Missing"
