2020.acl-main.711,P19-1470,0,0.263513,"s. We make the code and data from our experiments publicly available. 1 2. Retrieval of Commonsense Context: Adding commonsense context could be important to make explicit the semantic incongruity factor (e.g., GenSarc4 vs. GenSarc3 in Table 1), or could enhance the humorous effect of the generated sarcastic message (e.g., GenSarc2 vs. GenSarc1 in Table 1). We propose an approach where retrieved relevant commonsense context sentences are to be added to the generated sarcastic message. At first, we use a pre-trained language model fine-tuned on the ConceptNet (Speer et al., 2017) called COMET (Bosselut et al., 2019) to generate relevant commonsense knowledge. COMET gives us that, “inherited unfavorable genes from my mother” causes “to be ugly” or that “getting sick from fast food” causes “stomach ache” (Section 4.2.1). The derived commonsense concept is then used to retrieve relevant sentences — from a corpus — that could be added to the sentence obtained through reversal of valence (e.g., “Stomach ache is just an additional side effect” in Table 1) (Section 4.2.2). 3. Ranking of Semantic Incongruity: The previous module generates a list of candidate commonsense contexts. Next, we measure contradiction b"
2020.acl-main.711,D15-1075,0,0.0150114,"Corrections System 3 (Zhao et al., 2019) to correct any pronoun or gender specific errors introduced by the replacements. 4.3 Ranking for Semantic Incongruity After the grammatical error correction, the next step is to select the best context sentence from the retrieved results. Since we expect the context sentences to be incongruous with the sentence generated by the reversal of valence approach (Section 4.1), we rank the context sentences by semantic incongruity scores and select the best candidate. We frame the problem of semantic incongruity based on the Natural Language Inference (NLI) (Bowman et al., 2015) task. The Multi-Genre NLI (Williams et al., 2018) covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization, making it an ideal choice as our NLI Dataset. We first fine-tune RoBERTa-large (Liu et al., 2019), a state-of-the-art pre-trained language model for a 3-way classification (i.e., contradiction, entailment, and neutral) by training on the Multi-NLI dataset. Next, for each retrieved sentence, we treat it as the premise and the sentence generated by the reversal of valence as the hypothesis, and thus, obtain a contradiction score from the t"
2020.acl-main.711,W17-5523,1,0.83734,"eakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the context, which can include shared commonsense or world knowledge between the speaker and the addressee; 4) be aimed at some ta"
2020.acl-main.711,P11-2102,1,0.783483,"rous effect. Introduction Studies have shown that the use of sarcasm or verbal irony, can increase creativity on both the speakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the"
2020.acl-main.711,W10-2914,0,0.103038,"create incongruity or enhance the humorous effect. Introduction Studies have shown that the use of sarcasm or verbal irony, can increase creativity on both the speakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based"
2020.acl-main.711,esuli-sebastiani-2006-sentiwordnet,0,0.0124467,"onic criticism (Kreuz and Link, 2002)). This observation is also supported by research on sarcasm detection, particularly on social media. Hence, for our sarcasm generation task, we focus on transforming a literal utterance with negative valence into positive valence. To implement the reversal of valence, as highlighted in the yellow background in Figure 1, we first identify the evaluative words and replace them with their lexical antonyms using WordNet (Miller, 1995). As we expect the evaluative words to be negative words, we rely on the word level negative scores obtained from SentiWordNet (Esuli and Sebastiani, 2006). In the absence of words with negative polarity, we check if there is the negation word not or words ending with n’t and remove these words. In case there are both negative words and not (or words ending in n’t), we handle only one of them. Given the non sarcastic example “zero visibility in fog makes driving difficult” shown in Figure 1 and which we use as our running example, the reversal of valence module generates “zero visibility in fog makes driving easy”. 4.2 Retrieval of Commonsense Context As discussed before, a straightforward reversal of valence might not generate sarcastic message"
2020.acl-main.711,D17-1050,0,0.0364701,"eativity on both the speakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the context, which can include shared commonsense or world knowledge between the speaker and the addressee; 4)"
2020.acl-main.711,J18-4009,1,0.883065,"Missing"
2020.acl-main.711,D15-1116,1,0.901471,"shown that the use of sarcasm or verbal irony, can increase creativity on both the speakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the context, which can include shared common"
2020.acl-main.711,2020.scil-1.10,1,0.796904,"Missing"
2020.acl-main.711,P15-2124,0,0.229843,"of sarcasm or verbal irony, can increase creativity on both the speakers and the addressees (Huang et al., 2015), and can serve different communicative purposes such as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the context, which can include shared commonsense or world knowl"
2020.acl-main.711,N18-1169,0,0.0885691,"Missing"
2020.acl-main.711,2021.ccl-1.108,0,0.0836509,"Missing"
2020.acl-main.711,D19-1636,0,0.326336,"h as evoking humor and diminishing or enhancing critique (Burgers et al., 2012). Thus, developing computational models that generate sarcastic messages could impact many downstream applications, such as better conversational agents and creative or humorous content creation. While most computational work has focused on sarcasm detection (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Ghosh et al., 2015; Joshi et al., 2015b; Muresan et al., 2016; Ghosh and Veale, 2017; Ghosh et al., 2017, 2018), research on sarcasm generation is in its infancy (Joshi et al., 2015a; Mishra et al., 2019). Sarcasm generation ∗ The research was conducted when the author was at USC/ISI. is a challenging problem since the generated utterance should have at least five characteristics (a.k.a. “sarcasm factors”) (Burgers et al., 2012): 1) be evaluative; 2) be based on a reversal of valence between the literal and intended meaning; 3) be based on a semantic incongruity with the context, which can include shared commonsense or world knowledge between the speaker and the addressee; 4) be aimed at some target, and 5) be relevant to the communicative situation in some way. To simplify the problem, we foc"
2020.acl-main.711,P02-1040,0,0.112297,"oRV): This model only retrieves commonsense context and ranks them based on semantic incongruity. 4. No Semantic Incongruity (NSI): This model relies only on the reversal of valence and retrieval of commonsense context, without ranking based on semantic incongruity. A randomly selected retrieved sentence is used. 5. MTS2019: We make use of the model released by Mishra et al. (2019) as it is the stateof-the-art sarcasm generation system.6 6. Human (Gold) Sarcasm: As described in Section 5.1, we have gold sarcasm created by humans for every non-sarcastic utterance. 5.3 Evaluation Criteria BLEU (Papineni et al., 2002) is one of the most widely used automatic evaluation metric for generation tasks such as Machine Translation. However, for creative text generation, it is not ideal to expect significant n-gram overlaps between the machinegenerated and the gold-standard utterances. Hence, we performed a human evaluation. We evaluate a total of 900 generated utterances since our ablation study consisted of six different systems with 150 utterances each. Sarcasm is often linked with intelligence, creativity, and wit; thus we propose a set of 4 criteria to evaluate the generated output: (1) Creativity (“How creat"
2020.acl-main.711,N19-1014,0,0.0394513,"ve any pronoun, but the retrieved sentence does, we simply change that pronoun to “I”. For example, if the non-sarcastic input sentence is “Ignoring texts is literally the worst part of communication.” and the retrieved commonsense sentence is “He has never suffered the torment of rejection.”, we modify the retrieved sentence to “I have never suffered the torment of rejection.” to have consistency among the pronoun use. After correcting the pronouns and proper names (in the same way as pronoun correction), we feed the corrected sentences into the Neural Grammatical Error Corrections System 3 (Zhao et al., 2019) to correct any pronoun or gender specific errors introduced by the replacements. 4.3 Ranking for Semantic Incongruity After the grammatical error correction, the next step is to select the best context sentence from the retrieved results. Since we expect the context sentences to be incongruous with the sentence generated by the reversal of valence approach (Section 4.1), we rank the context sentences by semantic incongruity scores and select the best candidate. We frame the problem of semantic incongruity based on the Natural Language Inference (NLI) (Bowman et al., 2015) task. The Multi-Genr"
2020.acl-main.711,P17-1155,0,0.251405,"odule 1) and select the commonsense context that received the highest contradiction score. Finally, we concatenate the selected context to the sentence obtained through reversal of valence. Here, conceptually, contradiction detection is aimed to capture the semantic incongruity between the output of valence reversal and its 2 2.1 Related Work Sarcasm Generation Research on sarcasm generation is in its infancy. Joshi et al. (2015a) proposed SarcasmBot, a sarcasm generation system that implements eight rulebased sarcasm generators, each of which generates a certain type of sarcastic expression. Peled and Reichart (2017) introduced a novel task of sarcasm interpretation, defined as the generation of a nonsarcastic utterance conveying the same message as the original sarcastic one. They use supervised machine translation models for the same in presence of parallel data. However, it is impractical to assume the existence of large corpora for training supervised generative models using deep neural nets; we hence resort to unsupervised approaches. Mishra et al. (2019) employed reinforced neural seq2seq learning and information retrieval based approaches to generate sarcasm. Their models are trained using only unl"
2020.acl-main.711,D13-1066,0,0.137191,"Missing"
2020.acl-main.711,W18-0902,0,0.0391012,"text generation, it is not ideal to expect significant n-gram overlaps between the machinegenerated and the gold-standard utterances. Hence, we performed a human evaluation. We evaluate a total of 900 generated utterances since our ablation study consisted of six different systems with 150 utterances each. Sarcasm is often linked with intelligence, creativity, and wit; thus we propose a set of 4 criteria to evaluate the generated output: (1) Creativity (“How creative are the utterances ?”), (2) Sarcasticness (“How sarcastic are the utterances ?”), (3) Humour (“How funny are the sentences ?”) (Skalicky and Crossley, 2018), and (4) Grammaticality (“How grammatical are the sentences ?”). We design a MTurk task where Turkers were asked to rate outputs from all the six systems. Each Turker was given the non-sarcastic utterance as well as a group of sarcastic utterances generated by all the six systems (randomly shuffled). Each criteria was rated on a scale from 1 (not at all) to 5 (very). Finally, each utterance was rated by three individual Turkers. 55, 59, 66, and 60 Turkers 7981 6 https://github.com/TarunTater/sarcasm generation System State-of-the-art (Mishra et al., 2019) Human Generated Reversal of Valence ("
2020.acl-main.711,J18-4010,0,0.0295381,"Missing"
2020.acl-main.711,N18-1101,0,0.0779299,"re not concerned with the fifth characteristic, while the first and to some degree, the fourth are specified by the input (literal) utterances. 7976 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7976–7986 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Given the lack of “training” data for the sarcasm generation task, we propose a novel unsupervised approach that has three main modules guided by the above mentioned sarcasm factors: context. Contradiction scores are obtained from a model trained on the Multi-Genre NLI Corpus (Williams et al., 2018) (Section 4.3). 1. Reversal of Valence: To generate sarcastic utterances that satisfy the second characteristic we identify the evaluative word and use negation or lexical antonyms to generate the sarcastic utterance by reversing the valence (Section 4.1). For example, given, “I hate getting sick from fast food” this module will generate “I love getting sick from fast food” (GenSarc1 in Table 1). We test our approach on 150 non-sarcastic utterances randomly sampled from two existing data sets. We conduct human evaluation using several criteria: 1) how sarcastic is the generated message; 2) how"
2020.acl-main.761,D15-1075,0,0.0539384,"have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations. Other participants generated adversarial claims using implicative phrases such as “not clear” (Kim and Allan, 2019) or GPT-2 (Niewinski et al., 2019). In comparison, we present a diverse set of attacks motivated by"
2020.acl-main.761,W18-5521,1,0.935823,"· · · pt−1 ) for evidence ep at time t < k. At time t, we update the hidden state zt of the pointer network decoder. Then we compute the weighted average hqt of the entire evidence set using q hops over the evidence (Vinyals et al., 2016; Sukhbaatar et al., 2015):4 Figure 3: Pointer network architecture. Claim and evidence (page title or sentence) are embedded with BERT and evidence is sequentially predicted (for sentence selection the relation sequence is jointly predicted). (1a) by combining the top M pages from a TF-IDF search using DrQA (Chen et al., 2017) with pages from the approach of Chakrabarty et al. (2018), which provides results from Google search and predicted named entities and noun phrases. Then, we perform document ranking by selecting the top D < M pages with a pointer network (1b). Next, an N -long sequence of evidence sentences (2) and veracity relation labels (3) are predicted jointly by another pointer network. Prior to training, we fine-tune BERT for document and sentence ranking on claim/title and claim/sentence pairs, respectively. Each claim and evidence pair in the FEVER 1.0 dataset has both the title of the Wikipedia article and at least one sentence associated with the evidence"
2020.acl-main.761,P17-1171,0,0.0283485,"y by computing the extraction probability P (pt |p0 · · · pt−1 ) for evidence ep at time t < k. At time t, we update the hidden state zt of the pointer network decoder. Then we compute the weighted average hqt of the entire evidence set using q hops over the evidence (Vinyals et al., 2016; Sukhbaatar et al., 2015):4 Figure 3: Pointer network architecture. Claim and evidence (page title or sentence) are embedded with BERT and evidence is sequentially predicted (for sentence selection the relation sequence is jointly predicted). (1a) by combining the top M pages from a TF-IDF search using DrQA (Chen et al., 2017) with pages from the approach of Chakrabarty et al. (2018), which provides results from Google search and predicted named entities and noun phrases. Then, we perform document ranking by selecting the top D < M pages with a pointer network (1b). Next, an N -long sequence of evidence sentences (2) and veracity relation labels (3) are predicted jointly by another pointer network. Prior to training, we fine-tune BERT for document and sentence ranking on claim/title and claim/sentence pairs, respectively. Each claim and evidence pair in the FEVER 1.0 dataset has both the title of the Wikipedia arti"
2020.acl-main.761,P18-1063,0,0.0241854,"d evaluate on FV1/FV2-dev/test (Section 3). We report accuracy (percentage of correct labels) and recall (whether the gold evidence is contained in selected evidence at k = 5). We also report the FEVER score, the percentage of correct evidence sentences (for S and R) that also have correct labels, and potency, the inverse FEVER score (subtracted from one) for evaluating adversarial claims. Our Baseline-RL: For baseline experiments, to compare different loss functions, we use the approach of Chakrabarty et al. (2018) for document selection and ranking, the reinforcement learning (RL) method of Chen and Bansal (2018) for sentence selection, and BERT (Devlin et al., 2019) for relation prediction. The RL approach using a pointer network is detailed by Chen and Bansal (2018) for extractive summarization, with the only difference that we use our fine-tuned BERT on claim/gold sentence pairs to represent each evidence sentence in the pointer network (as with our full system) and use the FEVER score as a reward. The reward is obtained by selecting sentences with the pointer network and then predicting the relation using an MLP (updated during training) and the concatenation of all claim/predicted sentence repres"
2020.acl-main.761,N19-1423,0,0.0681987,"m to 1) make sequential decisions to handle multiple propositions, 2) support temporal reasoning, and 3) handle ambiguity and complex lexical relations. To address the first requirement we make use of a pointer network (Vinyals et al., 2015) in two novel ways: i) to rerank candidate documents and ii) to jointly predict a sequence of evidence sentences and veracity relations in order to compose evidence (Figure 3). To address the second we add a post-processing step for simple temporal reasoning. To address the third we use rich, contextualized representations. Specifically, we fine-tune BERT (Devlin et al., 2019) as this model has shown excellent performance on related tasks and was pre-trained on Wikipedia. Figure 2: Our FEVER pipeline: 1) Retrieving Wikipedia pages by selecting an initial candidate set (1a) and ranking the top D (1b); 2) Identifying the top N sentences; 3) Predicting supports, refutes, or not enough info. Dashed arrows indicate fine-tuning steps. Our full pipeline is presented in Figure 2. We first identify an initial candidate set of documents 8596 features for every claim c and evidence ep pair by summing the [CLS] embedding for the top 4 layers (as recommended by Devlin et al. (2"
2020.acl-main.761,P18-2103,0,0.0149649,"ays make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations. Other participants generated adversarial claims using implicative phrases such as “not clear” (Kim and Allan, 2019) or GPT-2 (Niewinski et al., 2019). In comparison, we present a diverse set of attacks motivated by realistic, challenging categories and further develop models to address those attacks. 3 Problem Formulation and Datasets We address the end-to-end fact-checking p"
2020.acl-main.761,W18-5516,0,0.150254,"g the veracity of naturallyoccurring claims have focused on statements factchecked by journalists or organizations such as PolitiFact.org (Vlachos and Riedel, 2014; Alhindi et al., 2018), news articles (Pomerleau and Rao, 2017), or answers in community forums (Mihaylova et al., 2018, 2019). However, those datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieva"
2020.acl-main.761,W18-5525,1,0.929968,"uited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in"
2020.acl-main.761,P16-1135,1,0.838677,"Our Baseline-RL, S: Our System. *: p < 0.05 **: p < 0.01 ***: p < 0.001 by approximate randomization test diction), simple temporal reasoning (by rule-based date handling), and ambiguity and variation (by fine-tuned contextualized representations). There are many unaddressed vulnerabilities that are relevant for fact-checking. The Facebook bAbI tasks (Weston et al., 2016) include other types of reasoning (e.g. positional or size-based). The DROP dataset (Dua et al., 2019) requires mathematical operations for question answering such as addition or counting. Propositions with causal relations (Hidey and McKeown, 2016), which are eventbased rather than attribute-based as in FEVER, are also challenging. Finally, many verifiable claims are non-experiential (Park and Cardie, 2014), e.g. personal testimonies, which would require predicting whether a reported event was actually possible. Finally, our system could be improved in many ways. Future work in multi-hop reasoning could represent the relation between consecutive pieces of evidence and future work in temporal reasoning could incorporate numerical operations with BERT (Andor et al., 2019). One limitation of our system is the pipeline nature, which may req"
2020.acl-main.761,D17-1215,0,0.140729,"question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have b"
2020.acl-main.761,D19-6615,0,0.254756,"factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations. Other participants generated adversarial claims using implicative phrases such as “not clear” (Kim and Allan, 2019) or GPT-2 (Niewinski et al., 2019). In comparison, we present a diverse set of attacks motivated by realistic, challenging categories and further develop models to address those attacks. 3 Problem Formulation and Datasets We address the end-to-end fact-checking problem in the context of FEVER (Thorne et al., 2018), a task where a system is required to verify a claim by providing evidence from Wikipedia. To be successful, a system needs to predict both the correct veracity relation– supported (S), refuted (R), or not enough information (NEI)– and the correct set of evidence sentences (not appli"
2020.acl-main.761,W18-5517,0,0.45819,"for predicting the veracity of naturallyoccurring claims have focused on statements factchecked by journalists or organizations such as PolitiFact.org (Vlachos and Riedel, 2014; Alhindi et al., 2018), news articles (Pomerleau and Rao, 2017), or answers in community forums (Mihaylova et al., 2018, 2019). However, those datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framew"
2020.acl-main.761,S19-2149,0,0.0456459,"re experts examine ”check-worthy” claims (Hassan et al., 2017) published by others for their “shades” of truth (e.g., FactCheck.org or PolitiFact). However, this process is time-consuming, and thus building computational models for automatic fact-checking has become an active area of research (Graves, 2018). Advances were made possible by new open source datasets and shared tasks: the Fact Extraction and Verification Shared Task (FEVER) 1.0 and 2.0 (Thorne et al., 2018; Thorne ∗ Work completed in part at Amazon and Vlachos, 2019), SemEval 2019 Shared Task 8: Fact-Checking in Community Forums (Mihaylova et al., 2019), and LIAR(+) datasets with claims from PolitiFact (Wang, 2017; Alhindi et al., 2018). The FEVER 1.0 shared task dataset (Thorne et al., 2018) has enabled the development of endto-end fact-checking systems, requiring document retrieval and evidence sentence extraction to corroborate a veracity relation prediction (supports, refutes, not enough info). An example is given in Figure 1. Since the claims in FEVER 1.0 were manually written using information from Wikipedia, the dataset may lack linguistic challenges that occur in verifying naturally occurring check-worthy claims, such as temporal rea"
2020.acl-main.761,C16-1007,0,0.0648274,"Missing"
2020.acl-main.761,P14-1095,0,0.161316,"acks for lexical variation, where words may be inserted or replaced or changed with some other edit operation, have been shown to be effective for similar tasks such as natural language inference (Nie et al., 2019b) and question answering (Jia and Liang, 2017), so we include these types of attacks as well. For the fact-checking task, models must match words and entities across claim and evidence to make a veracity prediction. As claims often contain ambiguous entities (Thorne and Vlachos, 2018) or lexical features indicative 2 As determined by NER using Spacy: https://spacy.io of credibility (Nakashole and Mitchell, 2014), we desire models resilient to minor changes in entities (Hanselowski et al., 2018) and words (Alzantot et al., 2018). We thus create an adversarial dataset of 1000 examples, with 417 multi-propositional, 313 temporal and 270 lexically variational. Representative examples are provided in Appendix A. Multiple Propositions Check-worthy claims often consist of multiple propositions (Graves, 2018). In the FEVER task, checking these claims may require retrieving evidence sequentially after resolving entities and events, understanding discourse connectives, and evaluating each proposition. Consider"
2020.acl-main.761,P19-1225,0,0.016203,"ers in community forums (Mihaylova et al., 2018, 2019). However, those datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model"
2020.acl-main.761,W14-2105,0,0.0198557,"andling), and ambiguity and variation (by fine-tuned contextualized representations). There are many unaddressed vulnerabilities that are relevant for fact-checking. The Facebook bAbI tasks (Weston et al., 2016) include other types of reasoning (e.g. positional or size-based). The DROP dataset (Dua et al., 2019) requires mathematical operations for question answering such as addition or counting. Propositions with causal relations (Hidey and McKeown, 2016), which are eventbased rather than attribute-based as in FEVER, are also challenging. Finally, many verifiable claims are non-experiential (Park and Cardie, 2014), e.g. personal testimonies, which would require predicting whether a reported event was actually possible. Finally, our system could be improved in many ways. Future work in multi-hop reasoning could represent the relation between consecutive pieces of evidence and future work in temporal reasoning could incorporate numerical operations with BERT (Andor et al., 2019). One limitation of our system is the pipeline nature, which may require addressing each type of attack individually as adversaries adjust their techniques. An end-to-end approach or a query reformulation step (re-writing claims t"
2020.acl-main.761,D17-1317,0,0.040915,"val may resolve references, the model must understand the meaning of “lived long enough to see” and evaluate the comparative statement. To create claims of this type, we mine Wikipedia by selecting a page X and extracting sentences with the pattern “is/was/named the A of Y ” (e.g. A is “first governor”) where Y links to another page. Then we manually create temporal claims by examining dates on X and Y and describing the relation between the entities and events. Named Entity Ambiguity and Lexical Variation As fact-checking systems are sensitive to lexical choice (Nakashole and Mitchell, 2014; Rashkin et al., 2017), we consider how variations in entities and words may affect veracity relation prediction. E NTITY DISAMBIGUATION has been shown to be important for retrieving the correct page for an entity among multiple candidates (Hanselowski et al., 2018). To create examples that contain ambiguous entities we selected claims from FV1-dev where at least one Wikipedia disambiguation page was returned by the Wikipedia python API.3 We then created a new claim using one of the documents returned from the disambiguation list. For example the claim “Patrick Stewart is someone who does acting for a living.” retu"
2020.acl-main.761,P19-1103,0,0.0291828,"cting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations."
2020.acl-main.761,P18-1079,0,0.0385504,"n architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline"
2020.acl-main.761,D19-6616,0,0.0367825,"Missing"
2020.acl-main.761,N18-1081,0,0.0288541,"evidence, i.e. “teacher forcing” (Williams and Zipser, 1989), and the model prediction from the previous step, so that we have training data for NEI. Combining Equations 3 and 5, our loss is: L(θ) = λL(θptr ) + L(θrel seq ) (6) Finally, to predict a relation at inference, we ensemble the sequence of predicted labels by averaging the probabilities over every time step.5 Post-processing for Simple Temporal Reasoning As neural models are unreliable for handling numerical statements, we introduce a rule-based step to extract and reason about dates. We use the Open Information Extraction system of Stanovsky et al. (2018) to extract tuples. For example, given the claim “The Latvian Soviet Socialist Republic was a republic of the Soviet Union 3 years after 2009,” the system would identify ARG0 as preceding the verb was and ARG1 following. After identifying tuples in claims and predicted sentences, we discard those lacking dates (e.g. ARG0). Given more than one candidate sentence, we select the one ranked higher by the pointer network. Once we have both the claim and evidence date-tuple we apply one of three rules to resolve the relation prediction based on the corresponding temporal phrase. We either evaluate w"
2020.acl-main.761,D19-6604,0,0.106964,"ral language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations. Other participants generated adversarial claims using implicative phrases such as “not clear” (Kim and Allan, 2019) or GPT-2 (Niewinski et al., 2019). In comparison, we present a diverse set of attacks motivated by realistic, challenging categories and further develop models to address those attacks. 3 Problem Formulation and Datasets We address the end-to-end fact-checking problem in the context of FEVER (Thorne et al., 2018), a task where a system is required to verify a claim by providing evidence from Wikipedia. To be successful, a system needs to predict both the correct veracity relation– supported (S), refuted (R), or not enough information (NEI)– and the correct set of evidence sentences (not applicable for NEI). The FEVER 1.0 data"
2020.acl-main.761,E17-3010,0,0.0265089,", composition of multiple propositions may also be necessary for NEI, as the relation of the claim and evidence may be changed by more general/specific phrases. We thus add ADDITIONAL UNVERIFIABLE PROPOSITIONS that change the gold label to NEI. We selected claims from FV1-dev and added propositions which have no evidence in Wikipedia (e.g. for the claim “Duff McKagan is an American citizen,” we can add the reduced relative clause “born in Seattle“). 8595 Temporal Reasoning Many check-worthy claims contain dates or time periods and to verify them requires models that handle temporal reasoning (Thorne and Vlachos, 2017). In order to evaluate the ability of current systems to handle temporal reasoning we modify claims from FV1-dev. More specifically, using claims with the phrase ”in <date>” we automatically generate seven modified claims using simple DATE MANIP ULATION heuristics: arithmetic (e.g., “in 2001” → “4 years before 2005”), range (“in 2001” → “before 2008”), and verbalization (“in 2001” → “in the first decade of the 21st century”). We also create examples requiring MULTI - HOP TEMPORAL REASONING , where the system must evaluate an event in relation to another. Consider the S claim “The first governo"
2020.acl-main.761,C18-1283,0,0.123658,"Missing"
2020.acl-main.761,N18-1074,0,0.390308,"rom newswire to social media (Vosoughi et al., 2018). To overcome this challenge, fact-checking has emerged as a necessary part of journalism, where experts examine ”check-worthy” claims (Hassan et al., 2017) published by others for their “shades” of truth (e.g., FactCheck.org or PolitiFact). However, this process is time-consuming, and thus building computational models for automatic fact-checking has become an active area of research (Graves, 2018). Advances were made possible by new open source datasets and shared tasks: the Fact Extraction and Verification Shared Task (FEVER) 1.0 and 2.0 (Thorne et al., 2018; Thorne ∗ Work completed in part at Amazon and Vlachos, 2019), SemEval 2019 Shared Task 8: Fact-Checking in Community Forums (Mihaylova et al., 2019), and LIAR(+) datasets with claims from PolitiFact (Wang, 2017; Alhindi et al., 2018). The FEVER 1.0 shared task dataset (Thorne et al., 2018) has enabled the development of endto-end fact-checking systems, requiring document retrieval and evidence sentence extraction to corroborate a veracity relation prediction (supports, refutes, not enough info). An example is given in Figure 1. Since the claims in FEVER 1.0 were manually written using inform"
2020.acl-main.761,N19-1230,0,0.0125915,"yoccurring claims have focused on statements factchecked by journalists or organizations such as PolitiFact.org (Vlachos and Riedel, 2014; Alhindi et al., 2018), news articles (Pomerleau and Rao, 2017), or answers in community forums (Mihaylova et al., 2018, 2019). However, those datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model ev"
2020.acl-main.761,W14-2508,0,0.237851,"tion for Computational Linguistics butions using pointer networks: 1) a document ranking model; and 2) a joint model for evidence sentence selection and veracity relation prediction framed as a sequence labeling task (Section 5). Our new system achieves state-of-the-art results for FEVER and we present an evaluation of our models including ablation studies (Section 6). Data and code will be released to the community.1 2 Related Work Approaches for predicting the veracity of naturallyoccurring claims have focused on statements factchecked by journalists or organizations such as PolitiFact.org (Vlachos and Riedel, 2014; Alhindi et al., 2018), news articles (Pomerleau and Rao, 2017), or answers in community forums (Mihaylova et al., 2018, 2019). However, those datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other wor"
2020.acl-main.761,D19-1221,0,0.0277601,"prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and impro"
2020.acl-main.761,P17-2067,0,0.0613097,"others for their “shades” of truth (e.g., FactCheck.org or PolitiFact). However, this process is time-consuming, and thus building computational models for automatic fact-checking has become an active area of research (Graves, 2018). Advances were made possible by new open source datasets and shared tasks: the Fact Extraction and Verification Shared Task (FEVER) 1.0 and 2.0 (Thorne et al., 2018; Thorne ∗ Work completed in part at Amazon and Vlachos, 2019), SemEval 2019 Shared Task 8: Fact-Checking in Community Forums (Mihaylova et al., 2019), and LIAR(+) datasets with claims from PolitiFact (Wang, 2017; Alhindi et al., 2018). The FEVER 1.0 shared task dataset (Thorne et al., 2018) has enabled the development of endto-end fact-checking systems, requiring document retrieval and evidence sentence extraction to corroborate a veracity relation prediction (supports, refutes, not enough info). An example is given in Figure 1. Since the claims in FEVER 1.0 were manually written using information from Wikipedia, the dataset may lack linguistic challenges that occur in verifying naturally occurring check-worthy claims, such as temporal reasoning or lexical generalization/specification. Thorne and Vla"
2020.acl-main.761,N18-1101,0,0.0608056,"ransformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations. Other participants generated adversarial claims using implicative phrases such as “not clear” (Kim and Allan, 2019) or GPT-2 (Niewinski et al., 2019). In comparison, we present a diverse set of attacks motivated by realistic, challenging c"
2020.acl-main.761,D18-1010,0,0.0845557,"e datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other r"
2020.acl-main.761,W18-5515,0,0.0524187,"Missing"
2020.acl-main.761,P19-1085,0,0.566224,"and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we fo"
2020.coling-main.540,P11-2103,0,0.0126555,"n features derived from predictive models are useful in the downstream task of document-level news vs. opinion classification; 2. We show that argumentation features transfer well to articles from unseen publishers or domains, highlighting their generality for this task. These models can be used to flag content to readers or fact-checkers who seek to check verifiable factual information and not personal opinions. 2 Related Work Subjectivity detection has been studied extensively in previous work, especially in the context of sentiment analysis (e.g., (Pang and Lee, 2008; Liu and others, 2010; Abdul-Mageed et al., 2011)). The majority of these approaches study subjectivity at the sentence level, while some previous work considered document-level classification on genres such as newspaper articles (Wiebe et al., 2004). Detection of subjective language in newspaper articles focused on lexical features of subjectivity, while observing that subjective words also appear in objective contexts and vice versa (Wiebe et al., 2004; Yu and Hatzivassiloglou, 2003; Toprak and Gurevych, 2009). This lead to approaches that do not generalize across publishers or topics and therefore a contextualized view of the problem is n"
2020.coling-main.540,C16-1324,0,0.379884,"ises). We can see that claims are more prevalent in the opinion article, while the news story contains more premises to support a small amount of claims. We study the predictive power of such coarse-grained argumentation features (claims and premises) for the task of news articles classification into news stories and opinion pieces. For short, we will refer to this binary task as news vs. opinion classification. To train our sentence-level argument component classification model (claim, premises, none), we use the corpus of editorial news labeled with argumentation strategies introduced by Al Khatib et al. (2016). We compare our approach that uses argumentation features to models using discrete linguistic features from previous work (Kr¨uger et al., 2017) and to document-level transformer-based models such as BERT (Devlin et al., 2019) fine-tuned for the document-level news vs. opinion classification task. We focus in particular on the transferability of these This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6139 Proceedings of the 28th International Conference on Computational Linguistics, pages 6139–6"
2020.coling-main.540,D17-1141,0,0.0193136,"orpus for our task is the news editorial corpus of 300 articles from three publishers (Al Khatib et al., 2016). The authors annotate argumentative strategies at the token level into one of six possible strategies. They report agreements and differences in argumentative writing between publishers. Editorials across different publishers consist of a majority of assumptions (claims), while they employ evidence supporting strategies differently. This corpus has been used to analyze persuasion in editorials (El Baff et al., 2020), and to study patterns of argumentative strategies across topics (Al Khatib et al., 2017). We hypothesize that predicting argumentative strategies of newspaper articles is also useful in predicting the overall type of the article to be news or opinion. Wachsmuth et al. (2014) used argumentation to predict sentiment of reviews, however, to the best of our knowledge, there is no previous work on using argumentation features for news vs. opinion classification. 3 Data In our experiments on news vs. opinion classification, we use two data collections that aim to test the generalizability of the modeling approaches. Details about sizes, publishers, and data set splits in both collectio"
2020.coling-main.540,D17-1218,0,0.0487295,"Missing"
2020.coling-main.540,N19-1423,0,0.0322596,"es (claims and premises) for the task of news articles classification into news stories and opinion pieces. For short, we will refer to this binary task as news vs. opinion classification. To train our sentence-level argument component classification model (claim, premises, none), we use the corpus of editorial news labeled with argumentation strategies introduced by Al Khatib et al. (2016). We compare our approach that uses argumentation features to models using discrete linguistic features from previous work (Kr¨uger et al., 2017) and to document-level transformer-based models such as BERT (Devlin et al., 2019) fine-tuned for the document-level news vs. opinion classification task. We focus in particular on the transferability of these This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6139 Proceedings of the 28th International Conference on Computational Linguistics, pages 6139–6149 Barcelona, Spain (Online), December 8-13, 2020 (a) News Story (b) Opinion Article Figure 1: Sentences Tagged as Claims or Premises in a News story and Opinion Articles classifiers, as this task is particularly sensitive to"
2020.coling-main.540,2020.acl-main.287,0,0.0597809,"Missing"
2020.coling-main.540,W17-5102,1,0.810735,"in text from argument components (claim, 6140 Data Collection WSJ-NYT Multi-Publisher Type train test test test train test test Publisher WSJ WSJ NYT-Defense NYT-Medicine 10 publishers 10 publishers The Metro - Winnipeg News 1751 500 1000 1000 3193 353 418 Opinion 1751 500 1000 1000 3193 353 418 Total 3502 1000 2000 2000 6386 706 836 Table 1: Details of All Datasets from the Two Data Collections. premises) to relations (support, attack) (Stede and Schneider, 2018). There are many argument mining corpora available on text from multiple genres (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Hidey et al., 2017). One relevant corpus for our task is the news editorial corpus of 300 articles from three publishers (Al Khatib et al., 2016). The authors annotate argumentative strategies at the token level into one of six possible strategies. They report agreements and differences in argumentative writing between publishers. Editorials across different publishers consist of a majority of assumptions (claims), while they employ evidence supporting strategies differently. This corpus has been used to analyze persuasion in editorials (El Baff et al., 2020), and to study patterns of argumentative strategies ac"
2020.coling-main.540,D14-1006,0,0.12565,"a field concerned with finding argument structure in text from argument components (claim, 6140 Data Collection WSJ-NYT Multi-Publisher Type train test test test train test test Publisher WSJ WSJ NYT-Defense NYT-Medicine 10 publishers 10 publishers The Metro - Winnipeg News 1751 500 1000 1000 3193 353 418 Opinion 1751 500 1000 1000 3193 353 418 Total 3502 1000 2000 2000 6386 706 836 Table 1: Details of All Datasets from the Two Data Collections. premises) to relations (support, attack) (Stede and Schneider, 2018). There are many argument mining corpora available on text from multiple genres (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Hidey et al., 2017). One relevant corpus for our task is the news editorial corpus of 300 articles from three publishers (Al Khatib et al., 2016). The authors annotate argumentative strategies at the token level into one of six possible strategies. They report agreements and differences in argumentative writing between publishers. Editorials across different publishers consist of a majority of assumptions (claims), while they employ evidence supporting strategies differently. This corpus has been used to analyze persuasion in editorials (El Baff et al., 2020), and t"
2020.coling-main.540,C14-1053,0,0.0268409,"e of six possible strategies. They report agreements and differences in argumentative writing between publishers. Editorials across different publishers consist of a majority of assumptions (claims), while they employ evidence supporting strategies differently. This corpus has been used to analyze persuasion in editorials (El Baff et al., 2020), and to study patterns of argumentative strategies across topics (Al Khatib et al., 2017). We hypothesize that predicting argumentative strategies of newspaper articles is also useful in predicting the overall type of the article to be news or opinion. Wachsmuth et al. (2014) used argumentation to predict sentiment of reviews, however, to the best of our knowledge, there is no previous work on using argumentation features for news vs. opinion classification. 3 Data In our experiments on news vs. opinion classification, we use two data collections that aim to test the generalizability of the modeling approaches. Details about sizes, publishers, and data set splits in both collections are shown in Table 1. 3.1 WSJ–NYT For this dataset, we use the setup introduced by Kr¨uger et al. (2017) for their work on news vs. opinion classification. This consists of data from t"
2020.coling-main.540,J04-3002,0,0.147501,"ishers or domains, highlighting their generality for this task. These models can be used to flag content to readers or fact-checkers who seek to check verifiable factual information and not personal opinions. 2 Related Work Subjectivity detection has been studied extensively in previous work, especially in the context of sentiment analysis (e.g., (Pang and Lee, 2008; Liu and others, 2010; Abdul-Mageed et al., 2011)). The majority of these approaches study subjectivity at the sentence level, while some previous work considered document-level classification on genres such as newspaper articles (Wiebe et al., 2004). Detection of subjective language in newspaper articles focused on lexical features of subjectivity, while observing that subjective words also appear in objective contexts and vice versa (Wiebe et al., 2004; Yu and Hatzivassiloglou, 2003; Toprak and Gurevych, 2009). This lead to approaches that do not generalize across publishers or topics and therefore a contextualized view of the problem is necessary. Kr¨uger et al. (2017) focus on the task of document-level classification of news articles into the broad categories of opinion and news stories. Opinion articles can be further split in multi"
2020.coling-main.540,H05-1044,0,0.0415889,"sentence (question marks, exclamation points, commas, and semicolons), ratio of quoted text, ratio of verb tense outside quoted text (past, present, future:will, modal verbs) of all verbs in the article, sentiment of text outside quotes, sentiment of adjectives outside quotes. We ignore features that require parsing to simplify feature extraction as they did not show significant gains in this task. Sentiment is represented by a numerical value that captures the degree (‘weak-subj’: 0.1, ‘strongsubj’: 1.0) and polarity of the sentiment that are extracted using the MPQA Sentiment Clues Lexicon (Wilson et al., 2005). Our reproduction of Kr¨uger et al. (2017) yields different results which are due to our more strict pre-processing that removes trivial cues from the data and a slight difference in how the data was sampled and split. 4.2 Document-level Contextualized Embeddings We fine-tune bert-base-cased models (Devlin et al., 2019) on each of the two data collections to obtain a contextualized representation of the article. We use the top layer of the [CLS] token to represent the article. We experimented with using each of the top four layers, sum and average of all four layers to represent the [CLS]. Th"
2020.coling-main.540,W03-1017,0,0.455944,"ctivity detection has been studied extensively in previous work, especially in the context of sentiment analysis (e.g., (Pang and Lee, 2008; Liu and others, 2010; Abdul-Mageed et al., 2011)). The majority of these approaches study subjectivity at the sentence level, while some previous work considered document-level classification on genres such as newspaper articles (Wiebe et al., 2004). Detection of subjective language in newspaper articles focused on lexical features of subjectivity, while observing that subjective words also appear in objective contexts and vice versa (Wiebe et al., 2004; Yu and Hatzivassiloglou, 2003; Toprak and Gurevych, 2009). This lead to approaches that do not generalize across publishers or topics and therefore a contextualized view of the problem is necessary. Kr¨uger et al. (2017) focus on the task of document-level classification of news articles into the broad categories of opinion and news stories. Opinion articles can be further split in multiple types such as: editorials – which express the opinion of journalist or the editorial board, op-eds – which expresses the opinion of a contributor, and letters-to-the-editor – which express the opinion of the readers to an editorial. Kr"
2020.emnlp-main.391,W13-3520,0,0.0754815,"Missing"
2020.emnlp-main.391,C04-1080,0,0.257443,"universal POS tags. We rely on off-the-shelf taggers to tag the source text prior to projecting the annotations as described next. POS Projection using Token and Type Constraints. To project the POS tags from the source to the target language, we use token and type constraints based on the mapping induced by the wordlevel alignments. The idea of using both token and type constraints was first introduced by T¨ackstr¨om et al. (2013). Type constraints define the set of POS tags a word type can receive. In a semisupervised leaning setup, type constraints can be obtained from an annotated corpus (Banko and Moore, 2004) or from a resource that serves as a POS lookup such as the Wiktionary 3 (Li et al., 2012; T¨ackstr¨om et al., 2013). For the extraction of type constraints in an unsupervised fashion, we follow the approach of (Buys and Botha, 2016), where we define a tag distribution for each word type on the target side by accumulating the counts of the different POS tags of the source-side tokens that align with the target-side tokens of that word type. The POS tags whose probability is equal to or greater than some threshold β constitute the type constraints of the underlying word type. As token constrain"
2020.emnlp-main.391,A00-1031,0,0.718325,"Missing"
2020.emnlp-main.391,J92-4003,0,0.540298,"ns into one or more high-resource Our first contribution is a robust approach for selecting training instances via cross-lingual annotation projection that exploits and expands all these best practices: coupling type and token constraints obtained in an unsupervised way, wordalignment confidence together with the density of the projected POS, and (optionally) multi-source projection (Sub-section 2.1). Our second contribution is a BiLSTM (Hochreiter and Schmidhuber, 1997) neural architecture that uses pre-trained contextualized word embeddings, affix embeddings and hierarchical Brown clusters (Brown et al., 1992). As contextualized embeddings, we show gains by exploiting the mul4820 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4820–4831, c November 16–20, 2020. 2020 Association for Computational Linguistics tilingual XML-R model (Conneau et al., 2019), while affix embeddings are particularly useful for morphologically-rich languages, and word clusters have been shown to be useful for non-neural POS tagging (Kupiec, 1992; T¨ackstr¨om et al., 2013; Owoputi et al., 2012). Moreover, in addition to the single-source setups, we propose an approach that utiliz"
2020.emnlp-main.391,P16-1184,0,0.231995,"tion on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages. 1 Unsupervised cross-lingual POS tagging via annotation projection has a long research history (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Duong et al., 2013; Agi´c et al., 2015, 2016; Buys and Botha, 2016). In contrast to our work, these approaches either use large and/or in-domain parallel data or rely on a large number of source languages for projection. However, since projection could suffer from bad translation, alignment mistakes or wrong assumptions, a key consideration for all these approaches is how to obtain high-quality training instances for the target language (i.e., sentences with accurate POS tags projected from the source-language(s)). Coupling token and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Buys and Botha, 2016), word-alignment confidence (Duong et al"
2020.emnlp-main.391,P19-4007,0,0.0617345,"Missing"
2020.emnlp-main.391,D17-1078,0,0.0146932,"ng out sentences of low density and alignment confidence is crucial for training the model. While choosing the sentences with top alignment scores has proved successful in previous research (Duong et al., 2013), we add the density factor as our Bi-LSTM model benefits from longer contiguous labeled sequences. 2.2 Neural POS Tagging The architecture of our POS tagger is a bidirectional long short-term memory (BiLSTM) neuralnetwork model (Hochreiter and Schmidhuber, 1997). BiLSTMs have been widely used for POS tagging (Huang et al., 2015; Wang et al., 2015; Plank et al., 2016; Ma and Hovy, 2016; Cotterell and Heigold, 2017) and other sequence-labeling tasks. The input to our BiLSTM model is a labeled sentence where the word representation is the concatenation of word and sub-word information, namely pre-trained and randomly initialized word embeddings, affix embeddings and word clusters. Figure 1 shows the complete structure of our neural architecture. 4 . Word and Affix Embeddings We use two types of word-embedding features: pre-trained contextualized embeddings (PT) and randomly initialized embeddings (RI). For the pre-trained contextualized embeddings, we use the final layer of the multilingual XLM-RoBERTa mo"
2020.emnlp-main.391,P11-1061,0,0.0190467,", affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages. 1 Unsupervised cross-lingual POS tagging via annotation projection has a long research history (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Duong et al., 2013; Agi´c et al., 2015, 2016; Buys and Botha, 2016). In contrast to our work, these approaches either use large and/or in-domain parallel data or rely on a large number of source languages for projection. However, since projection could suffer from bad translation, alignment mistakes or wrong assumptions, a key consideration for all these approaches is how to obtain high-quality training instances for the target language (i.e., sentences with accurate POS tags projected from the source-language(s)). Coupling token and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al."
2020.emnlp-main.391,N19-1423,0,0.00647488,"eural architecture. 4 . Word and Affix Embeddings We use two types of word-embedding features: pre-trained contextualized embeddings (PT) and randomly initialized embeddings (RI). For the pre-trained contextualized embeddings, we use the final layer of the multilingual XLM-RoBERTa model, XLM-R (Conneau et al., 2019) 5 XLM-R is a transformer-based multilingual masked language model that is pre-trained on texts of 100 languages, and its performance is competitive with strong monolingual models when tested on a variety of NLP tasks. It also shows better performance than multilingual BERT, mBERT (Devlin et al., 2019), particularly for low-resource languages. We use the average of the embedding vectors of the first and last sub-tokens of each word to represent its pre-trained embeddings. It is worth noting that when using our architecture for a target language that is not present in the XLM-R model, one can consider training a custom XLM transformer-based model 6 given the availability of monolingual data and suitable computational resources, and thus our architecture is not limited to the languages available in the XLM-R model. The randomly initialized embeddings are learned as part of training the model."
2020.emnlp-main.391,P13-2112,0,0.11992,"hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages. 1 Unsupervised cross-lingual POS tagging via annotation projection has a long research history (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Duong et al., 2013; Agi´c et al., 2015, 2016; Buys and Botha, 2016). In contrast to our work, these approaches either use large and/or in-domain parallel data or rely on a large number of source languages for projection. However, since projection could suffer from bad translation, alignment mistakes or wrong assumptions, a key consideration for all these approaches is how to obtain high-quality training instances for the target language (i.e., sentences with accurate POS tags projected from the source-language(s)). Coupling token and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Buys and Bot"
2020.emnlp-main.391,K16-1018,0,0.0350024,"Missing"
2020.emnlp-main.391,I05-1075,0,0.0804695,"tualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages. 1 Unsupervised cross-lingual POS tagging via annotation projection has a long research history (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Duong et al., 2013; Agi´c et al., 2015, 2016; Buys and Botha, 2016). In contrast to our work, these approaches either use large and/or in-domain parallel data or rely on a large number of source languages for projection. However, since projection could suffer from bad translation, alignment mistakes or wrong assumptions, a key consideration for all these approaches is how to obtain high-quality training instances for the target language (i.e., sentences with accurate POS tags projected from the source-language(s)). Coupling token and type constraints (Das and Petrov, 20"
2020.emnlp-main.391,W19-1425,0,0.0326517,"Missing"
2020.emnlp-main.391,L18-1293,0,0.0468848,"Missing"
2020.emnlp-main.391,D12-1127,0,0.0407781,"Missing"
2020.emnlp-main.391,P16-1101,0,0.0378867,") (dS +aS ) Filtering out sentences of low density and alignment confidence is crucial for training the model. While choosing the sentences with top alignment scores has proved successful in previous research (Duong et al., 2013), we add the density factor as our Bi-LSTM model benefits from longer contiguous labeled sequences. 2.2 Neural POS Tagging The architecture of our POS tagger is a bidirectional long short-term memory (BiLSTM) neuralnetwork model (Hochreiter and Schmidhuber, 1997). BiLSTMs have been widely used for POS tagging (Huang et al., 2015; Wang et al., 2015; Plank et al., 2016; Ma and Hovy, 2016; Cotterell and Heigold, 2017) and other sequence-labeling tasks. The input to our BiLSTM model is a labeled sentence where the word representation is the concatenation of word and sub-word information, namely pre-trained and randomly initialized word embeddings, affix embeddings and word clusters. Figure 1 shows the complete structure of our neural architecture. 4 . Word and Affix Embeddings We use two types of word-embedding features: pre-trained contextualized embeddings (PT) and randomly initialized embeddings (RI). For the pre-trained contextualized embeddings, we use the final layer of t"
2020.emnlp-main.391,D17-1036,0,0.0285607,"Missing"
2020.emnlp-main.391,mayer-cysouw-2014-creating,0,0.0294466,"2013), multi-source projection (Agi´c et al., 2016) and coverage (percentage of tokens covered by multisource projection) (Plank and Agi´c, 2018) have shown to lead to training instances of better quality. However, only one or two of these have been usually employed. Introduction Majority of world’s languages do not have annotated datasets even for the most simple NLP tasks such as part-of-speech (POS) tagging. However, efforts in documenting low-resource languages often contain translations, usually of religious text, into other high-resource languages. One such parallel corpus is the Bible (Mayer and Cysouw, 2014): 484 languages have a complete Bible translation, while 2551 have a part of the Bible translated. Our goal is to learn POS taggers for a diverse set of target languages in a truly low-resource scenario, where only a limited and possibly out-of-domain set of translations into one or more high-resource Our first contribution is a robust approach for selecting training instances via cross-lingual annotation projection that exploits and expands all these best practices: coupling type and token constraints obtained in an unsupervised way, wordalignment confidence together with the density of the p"
2020.emnlp-main.391,J03-1002,0,0.0206054,"o induce a neural POS tagger for a target language of interest without any direct supervision. Instead, we rely on parallel translations between the target and one or more source languages for which POS taggers are accessible. This section describes our approach: 1) cross-lingual annotation projection via word alignments to prepare the training instances of the target language, and 2) neural POS tagging for the target language. 2.1 Cross-Lingual Projection via Word Alignments Given sentence-aligned parallel data, we align the text of the source and target sides at the word level using GIZA++ (Och and Ney, 2003), while sentences of more than 80 tokens are eliminated. We construct bidirectional word alignments, by only considering the intersecting source-to-target and target-to-source alignments, and exclude the alignment points where the average of the alignment probabilities in the two directions is below some threshold α. Tagging of Source Languages. Since crosslingual projection requires a common POS tagset for all languages, we use the universal POS tagset of the Universal Dependencies (UD) project 2 , which consists of 17 universal POS tags. We rely on off-the-shelf taggers to tag the source tex"
2020.emnlp-main.391,pasha-etal-2014-madamira,1,0.816624,"Missing"
2020.emnlp-main.391,P16-2067,0,0.0217121,"core(S) = 2×(dS ×aS ) (dS +aS ) Filtering out sentences of low density and alignment confidence is crucial for training the model. While choosing the sentences with top alignment scores has proved successful in previous research (Duong et al., 2013), we add the density factor as our Bi-LSTM model benefits from longer contiguous labeled sequences. 2.2 Neural POS Tagging The architecture of our POS tagger is a bidirectional long short-term memory (BiLSTM) neuralnetwork model (Hochreiter and Schmidhuber, 1997). BiLSTMs have been widely used for POS tagging (Huang et al., 2015; Wang et al., 2015; Plank et al., 2016; Ma and Hovy, 2016; Cotterell and Heigold, 2017) and other sequence-labeling tasks. The input to our BiLSTM model is a labeled sentence where the word representation is the concatenation of word and sub-word information, namely pre-trained and randomly initialized word embeddings, affix embeddings and word clusters. Figure 1 shows the complete structure of our neural architecture. 4 . Word and Affix Embeddings We use two types of word-embedding features: pre-trained contextualized embeddings (PT) and randomly initialized embeddings (RI). For the pre-trained contextualized embeddings, we use t"
2020.emnlp-main.391,2020.acl-demos.14,0,0.0288331,"solate), Bulgarian (IE, Slavic), Finnish (Uralic, Finnic), Hindi (IE, Hindi), Indonesian (Austronesian, MalayoSumbawan), Lithuanian (IE, Baltic), Persian (IE, Iranian), Portuguese (IE, Romance), Telugu (Dravidian, South Central) and Turkish (Turkic, Southwestern). We use the multilingual parallel Bible corpus 10 (Christodouloupoulos and Steedman, 2015) as the source of our parallel data, where we perform the alignment on the verse and word levels. The Bible text is available in full for our source and target languages except Basque, where only the new testament is available. We use Stanza 11 (Qi et al., 2020) to tag the source-side text of the source languages except for Arabic, for which we apply MADAMIRA (Pasha et al., 2014) for performance gain. However, since MADAMIRA was trained on PTB tags and was not designed to follow the UD guidelines, we mapped the Arabic PTB tags into their UD cognates and manually corrected the analyses of the most frequent 2,500 Arabic POS and lemma pairs by selecting the most likely analysis for each. We evaluate our models in terms of POS accuracy on the test sets of the Universal Dependencies, UD v2.5 (Zeman et al., 2019) 12 . We also report our results on older ve"
2020.emnlp-main.391,W96-0213,0,0.77217,". Coupling both the randomly initialized embeddings and the pre-trained ones is essential when the domain of the training data is different from the one of the pre-trained embeddings, which is the case in our learning setup, where we use the Bible data for training, while the XLM-R model is trained on text from Wikipedia 7 and a CommonCrawl corpus (See Conneau et al. (2019) for more details). In addition to word embeddings, we use randomly initialized prefix and suffix n-gram character embeddings, where n is in {1, 2, 3, 4}, as the use of affix information has proved effective in POS tagging (Ratnaparkhi, 1996; Martins and Kreutzer, 4 We also experimented with BiLSTM+CRF, but the CRF layer did not improve the model, which is in line with previous research (Yang et al., 2018; Plank and Agi´c, 2018). 5 We get better results when using the XLM-R embeddings as features as opposed to performing fine tuning, where the latter is more suitable to sentence-level predictions. 6 https://github.com/facebookresearch/XLM 7 https://wikipedia.org 4822 POSn POS3 POS2 POS1 Custom Softmax Activation BiLSTM Encoding Layer h1 h2 h3 h1 h2 h3 h1 h2 h3 hn Backward Layer hn Forward Layer hn Word Clusters 1 Word Clusters 2"
2020.emnlp-main.391,W17-2213,0,0.0487393,"Missing"
2020.emnlp-main.391,Q13-1001,0,0.0605263,"Missing"
2020.emnlp-main.391,petrov-etal-2012-universal,0,0.135086,"Missing"
2020.emnlp-main.391,C18-1327,0,0.0124817,"ined embeddings, which is the case in our learning setup, where we use the Bible data for training, while the XLM-R model is trained on text from Wikipedia 7 and a CommonCrawl corpus (See Conneau et al. (2019) for more details). In addition to word embeddings, we use randomly initialized prefix and suffix n-gram character embeddings, where n is in {1, 2, 3, 4}, as the use of affix information has proved effective in POS tagging (Ratnaparkhi, 1996; Martins and Kreutzer, 4 We also experimented with BiLSTM+CRF, but the CRF layer did not improve the model, which is in line with previous research (Yang et al., 2018; Plank and Agi´c, 2018). 5 We get better results when using the XLM-R embeddings as features as opposed to performing fine tuning, where the latter is more suitable to sentence-level predictions. 6 https://github.com/facebookresearch/XLM 7 https://wikipedia.org 4822 POSn POS3 POS2 POS1 Custom Softmax Activation BiLSTM Encoding Layer h1 h2 h3 h1 h2 h3 h1 h2 h3 hn Backward Layer hn Forward Layer hn Word Clusters 1 Word Clusters 2 Word Clusters 3 Word Clusters n Affix Embeddings 1 Affix Embeddings 2 Affix Embeddings 3 Affix Embeddings n RI Word Embeddings 1 RI Word Embeddings 2 RI Word Embedding"
2020.emnlp-main.391,P19-1493,0,0.0149119,"mbination, further improves the tagging accuracy for most target languages. Our experiments, using limited and outof-domain parallel data, demonstrate significant improvements over previous work (both unsupervised and semi-supervised), even when comparing our single-source setups to other multi-source ones. We also investigate how much gold data is needed to develop supervised taggers comparable to our best unsupervised models. In addition, we show that cross-lingual annotation projection generalizes across languages of different typologies better than the zero-shot model-transfer approach by Pires et al. (2019). Finally, our tagging scripts and models are made publicly available 1 . 2 Approach Our goal is to induce a neural POS tagger for a target language of interest without any direct supervision. Instead, we rely on parallel translations between the target and one or more source languages for which POS taggers are accessible. This section describes our approach: 1) cross-lingual annotation projection via word alignments to prepare the training instances of the target language, and 2) neural POS tagging for the target language. 2.1 Cross-Lingual Projection via Word Alignments Given sentence-aligne"
2020.emnlp-main.391,H01-1035,0,0.373127,"ecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology. In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work. In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages. 1 Unsupervised cross-lingual POS tagging via annotation projection has a long research history (Yarowsky et al., 2001; Fossum and Abney, 2005; Das and Petrov, 2011; Duong et al., 2013; Agi´c et al., 2015, 2016; Buys and Botha, 2016). In contrast to our work, these approaches either use large and/or in-domain parallel data or rely on a large number of source languages for projection. However, since projection could suffer from bad translation, alignment mistakes or wrong assumptions, a key consideration for all these approaches is how to obtain high-quality training instances for the target language (i.e., sentences with accurate POS tags projected from the source-language(s)). Coupling token and type constra"
2020.emnlp-main.391,D18-1061,0,0.0281103,"Missing"
2020.emnlp-main.391,K17-3001,0,0.0587746,"Missing"
2020.emnlp-main.524,P19-1470,0,0.253793,"k as a style-transfer problem we make three contributions: 4 Automatic creation of a parallel corpus of [literal sentence, simile] pairs. Our constructed corpus contains 87,843 such pairs. As a first step, we use distant supervision to automatically collect a set of self-labeled similes using the phrase like a. We then convert these similes to their literal versions by removing the COMPARATOR and replacing the VEHICLE with the associated PROPERTY 4 Code & Data at https://github.com/ tuhinjubcse/SimileGeneration-EMNLP2020 by leveraging the structured common sense knowledge achieved from COMET (Bosselut et al., 2019), a language model fine-tuned on ConceptNet (Speer et al., 2017). For example, for the simile “Love is like a unicorn” our method will generate “Love is rare” (Section 2.1). Transfer learning from a pre-trained model for generating high quality similes. Our system SCOPE, fine-tunes BART (Lewis et al., 2019) — a state of the art pre-trained denoising autoencoder built with a sequence to sequence model, on our automatically collected parallel corpus of [literal sentence, simile] pairs (Section 2.2) to generate similes. Human evaluations show that this approach generates similes that are better 3"
2020.emnlp-main.524,N16-1098,0,0.0526042,"Missing"
2020.emnlp-main.524,W17-7403,0,0.0709637,"Missing"
2020.emnlp-main.524,D14-1215,0,0.0294031,"cativeness. Experimental results from Table 7 prove that effective usage of similes can improve evocativeness of machine generated stories. 7 Related Work Simile generation is a relatively new task. Most prior work has focused on detection of similes. The closest task in NLP to simile generation is generating metaphors. However, it should be noted that 6462 the overlap between the expressive range of similes and metaphors is known to be only partial: there are similes that cannot be rephrased as metaphors, similarly the other way around (Israel et al., 2004). 7.1 Simile Detection and Analysis Niculae and Danescu-Niculescu-Mizil (2014) proposed frameworks for annotating similes from product reviews by considering their semantic and syntactic characteristics as well as the challenges inherent to the automatic detection of similes. Qadir et al. (2015, 2016) built computational models to recognize affective polarity and implicit properties in similes. Unlike these works, we focus on generating similes by transforming a literal sentence while still being faithful to the property in context. 7.2 Metaphor Generation 8 Conclusion We establish a new task for NLG: simile generation from literal sentences. We propose a novel way of c"
2020.emnlp-main.524,N19-4009,0,0.0323896,"Missing"
2020.emnlp-main.524,P02-1040,0,0.107509,"ores (BERT-S) and Novelty. Boldface denotes the best results. Baseline Systems 1. BART: This is the pre-trained BART model. Since BART is a pre-trained sequence to sequence model, it can still be used for conditional text generation. To this end we use the same literal sentence (For example The city was beautiful) as an input to the encoder and force the decoder to begin with same prefix by removing the adjective/adverb at the end and appending the comparator and the article (The city was like a) and generate a simile. B-1 0.0 3.25 3.73 8.03 3.2 Evaluation Criteria Automatic evaluation. BLEU (Papineni et al., 2002) is one of the most widely used automatic evaluation metric for generation tasks such as Machine Translation. However, for creative text generation, it is not ideal to expect significant n-gram overlaps between the machine-generated and the gold-standard sentences. We still report the BLEU scores for generated VEHICLE after discarding the common prefix with the gold. BERTScore (Zhang et al., 2019) has been used recently for evaluating text generation using contextualized embeddings and it is said to somewhat ameliorate the problems with BLEU. It computes a similarity score using contextual emb"
2020.emnlp-main.524,W18-1505,1,0.906961,"Missing"
2020.emnlp-main.524,D15-1019,0,0.0223654,"f similes. The closest task in NLP to simile generation is generating metaphors. However, it should be noted that 6462 the overlap between the expressive range of similes and metaphors is known to be only partial: there are similes that cannot be rephrased as metaphors, similarly the other way around (Israel et al., 2004). 7.1 Simile Detection and Analysis Niculae and Danescu-Niculescu-Mizil (2014) proposed frameworks for annotating similes from product reviews by considering their semantic and syntactic characteristics as well as the challenges inherent to the automatic detection of similes. Qadir et al. (2015, 2016) built computational models to recognize affective polarity and implicit properties in similes. Unlike these works, we focus on generating similes by transforming a literal sentence while still being faithful to the property in context. 7.2 Metaphor Generation 8 Conclusion We establish a new task for NLG: simile generation from literal sentences. We propose a novel way of creating parallel corpora and a transfer-learning approach for generating similes. Human and automatic evaluations show that our best model is successful at generating similes. Our experimental results further show tha"
2020.emnlp-main.524,N16-1146,0,0.0398205,"Missing"
2020.emnlp-main.524,D19-1322,0,0.0225191,"tructure is: “[The city/TOPIC] [was/EVENT] [like/COMPARATOR] [a painting/VEHICLE]” (PROPERTY is implicit). Unlike metaphors, the semantic context of similes tends to be very shallow, transferring a single property (Hanks, 2013). Moreover, the explicit syntactic structure of similes allows, in exchange, for more lexical creativity (Niculae and DanescuNiculescu-Mizil, 2014). We focus on the task of generating a simile starting from a literal utterance that contains the TOPIC, EVENT and PROPERTY. We frame this task as a style-transfer problem (Shen et al., 2017; Fu et al., 2017; Li et al., 2018; Sudhakar et al., 2019), where the author’s intent is to make the description of the TOPIC more emphatic by introducing a comparison with the VEHICLE via a shared PROPERTY (See Figure 1 for examples of literal descriptive sentences and the generated similes). We call our approach SCOPE (Style transfer through COmmonsense PropErty). There are two main challenges we need to address: 1) the lack of training data that consists of pairs of literal utterances and their equivalent simile in order to train a supervised model; 2) ensuring that the generated simile makes a meaningful comparison between the TOPIC and the VEHIC"
2020.emnlp-main.524,N19-1014,0,0.022565,"EVENT, and a PROPERTY if stated explicitly. We take the top 5 properties from COMET to form 5 possible literal versions for a particular simile. To rank these literal versions and select the best one, we rely on perplexity scores obtained from a pre-trained language model GPT (Radford et al., 2018). Table 1 shows human written similes collected from Reddit, the top 5 common sense properties associated with the VEHICLE, and the literal version created by taking the best PROPERTY. To correct any grammatical errors introduced by this manipulation, we rely on a grammatical error correction model (Zhao et al., 2019). Test Data Collection. Our task is to generate a simile given a literal input. The automaticallygenerated parallel data might contain stylistic biases. To truly measure the effectiveness of our approach, we need to evaluate on a dataset independent of our training and validation data. Towards this end, we again scrape WRITINGPROMPTS subreddits for sentences which are this time literal in nature (without any comparators like, as). Since literal utterances contains the description of TOPIC via a PROPERTY and usually the PROPERTY is an adjective or adverb, we restrict the last word of our litera"
2020.emnlp-main.636,C18-1139,0,0.0449853,"Missing"
2020.emnlp-main.636,D19-1539,0,0.0210273,"(∼146M) CNN-DM (∼138M) Wiki+Books (∼192M) Mean F1 92.26±0.11 91.22±0.21 85.54±0.19 88.02±0.18 93.5 Results on CONLL-2012 dataset Model CVT BERTBase Pre-BERTBase APBERTBase BERT-MRC+DSC Unlabeled Data CNN-DM (∼18M) Wiki+Books (∼192M) CNN-DM (∼146M) CNN-DM (∼138M) Wiki+Books (∼192M) Mean F1 89.26±0.1 89.0±0.23 84.20±0.19 85.88±0.17 92.07 Table 3: Model performance for NER. The same unlabeled dataset is used for training CVT, Pre-BERTBase and APBERTBase , and Unlabeled Data indicates the approximate number of sentences seen by each model during training, until convergence criteria is met. Cloze (Baevski et al., 2019) and BERT-MRC+DSC (Li et al., 2019) are SOTA baselines for CONLL-2003 and CONLL-2012, respectively, for this task. Baevski et al. (2019) also use subsampled Common Crawl and News Crawl datasets but do not provide exact splits for these. Performance Metrics We report mean F1 (with standard deviation) on the labeled test splits for each task over 5 randomized runs, and compare the models using statistical significance tests over these runs. Further, we report the approximate number of unlabeled sentences seen by each model. Table 2 shows the results for the OTE detection task. Here, out of the 3"
2020.emnlp-main.636,D18-1217,0,0.237422,"hese are then reused in supervised training on a downstream task. These pre-trained models, particularly the ones based on the Transformer architecture (Vaswani et al., 2017)1 have ∗ Smaranda Muresan is an Amazon Scholar and a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT) (Clark et al., 2018) is a semi-supervised approach that uses unlabeled data in a task-specific manner, rather than trying to learn general representations that can be used for many downstream tasks. Inspired by selflearning (McClosky et al., 2006; Yarowsky, 1995) and multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013), the key idea is that the primary prediction module, which has an unrestricted view of the data, trains on the task using labeled examples, and makes task-specific predictions on unlabeled data. The auxiliary modules, with restricted views of the unlabeled data, attempt to replicate the p"
2020.emnlp-main.636,N19-1423,0,0.186313,"ing tasks, with lesser financial and environmental impact. 1 Introduction Exploiting unlabeled data to improve performance has become the foundation for many natural language processing tasks. The question we investigate in this paper is how to effectively use unlabeled data: in a task-agnostic or a task-specific way. An example of the former is training models on language model (LM) like objectives on a large unlabeled corpus to learn general representations, as in ELMo (Embeddings from Language Models) (Peters et al., 2018) and BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019). These are then reused in supervised training on a downstream task. These pre-trained models, particularly the ones based on the Transformer architecture (Vaswani et al., 2017)1 have ∗ Smaranda Muresan is an Amazon Scholar and a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT"
2020.emnlp-main.636,2020.acl-main.740,0,0.0365544,"Missing"
2020.emnlp-main.636,N16-1030,1,0.615518,"Missing"
2020.emnlp-main.636,P19-1524,0,0.0474345,"anguage model (LM) like objectives on a large unlabeled corpus to learn general representations, as in ELMo (Embeddings from Language Models) (Peters et al., 2018) and BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019). These are then reused in supervised training on a downstream task. These pre-trained models, particularly the ones based on the Transformer architecture (Vaswani et al., 2017)1 have ∗ Smaranda Muresan is an Amazon Scholar and a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT) (Clark et al., 2018) is a semi-supervised approach that uses unlabeled data in a task-specific manner, rather than trying to learn general representations that can be used for many downstream tasks. Inspired by selflearning (McClosky et al., 2006; Yarowsky, 1995) and multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013), the key idea is that the prima"
2020.emnlp-main.636,2021.ccl-1.108,0,0.102216,"Missing"
2020.emnlp-main.636,W18-5711,0,0.0765426,"tasks. We find that the CVT-based approach using less unlabeled data achieves similar performance with BERT-based models, while being superior in terms of financial and environmental cost as well. 2 Background, Tasks and Datasets Before presenting the models and their training setups, we discuss the relevant literature and introduce the tasks and datasets used for our experiments. We focus on three tasks: opinion target expression (OTE) detection; named entity recognition (NER), and slot-labeling, each of which can be modeled as a sequence tagging problem (Xu et al., 2018; Liu et al., 2019a; Louvan and Magnini, 2018). The IOB sequence tagging scheme (Ramshaw and Marcus, 1999) is used for each of these tasks. Related Work. The usefulness of continued training of large transformer-based models on domain/task-related unlabeled data has been shown recently (Gururangan et al., 2020; Rietzler et al., 2019; Xu et al., 2019), with a varied use of terminology for the process. Xu et al. (2019) and Rietzler et al. (2019) show gains of further tuning BERT using in-domain unlabeled data and refer to this as Post-training, and LM finetuning, respectively. More recently, Gururangan et al. (2020) use the term Domain-Adap"
2020.emnlp-main.636,P16-1101,0,0.107731,"Missing"
2020.emnlp-main.636,P11-1015,0,0.0628885,"Missing"
2020.emnlp-main.636,N06-1020,0,0.0187733,"a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT) (Clark et al., 2018) is a semi-supervised approach that uses unlabeled data in a task-specific manner, rather than trying to learn general representations that can be used for many downstream tasks. Inspired by selflearning (McClosky et al., 2006; Yarowsky, 1995) and multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013), the key idea is that the primary prediction module, which has an unrestricted view of the data, trains on the task using labeled examples, and makes task-specific predictions on unlabeled data. The auxiliary modules, with restricted views of the unlabeled data, attempt to replicate the primary module predictions. This helps to learn better representations for the task. We present an experimental study that investigates different task-agnostic and task-specific approaches to use unsupervised data and evaluates"
2020.emnlp-main.636,D14-1162,0,0.0925881,"Missing"
2020.emnlp-main.636,N18-1202,0,0.0237559,"ecture and we show that it achieves similar performance to BERT on a set of sequence tagging tasks, with lesser financial and environmental impact. 1 Introduction Exploiting unlabeled data to improve performance has become the foundation for many natural language processing tasks. The question we investigate in this paper is how to effectively use unlabeled data: in a task-agnostic or a task-specific way. An example of the former is training models on language model (LM) like objectives on a large unlabeled corpus to learn general representations, as in ELMo (Embeddings from Language Models) (Peters et al., 2018) and BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019). These are then reused in supervised training on a downstream task. These pre-trained models, particularly the ones based on the Transformer architecture (Vaswani et al., 2017)1 have ∗ Smaranda Muresan is an Amazon Scholar and a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmenta"
2020.emnlp-main.636,S16-1002,0,0.0496953,"Missing"
2020.emnlp-main.636,S14-2004,0,0.0914221,"Missing"
2020.emnlp-main.636,W12-4501,0,0.0498685,"Missing"
2020.emnlp-main.636,P19-1355,0,0.0551668,"BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019). These are then reused in supervised training on a downstream task. These pre-trained models, particularly the ones based on the Transformer architecture (Vaswani et al., 2017)1 have ∗ Smaranda Muresan is an Amazon Scholar and a Research Scientist at the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT) (Clark et al., 2018) is a semi-supervised approach that uses unlabeled data in a task-specific manner, rather than trying to learn general representations that can be used for many downstream tasks. Inspired by selflearning (McClosky et al., 2006; Yarowsky, 1995) and multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013), the key idea is that the primary prediction module, which has an unrestricted view of the data, trains on the task using labeled examples, and makes task-specific predictions on unlabeled data. The auxil"
2020.emnlp-main.636,N19-1242,0,0.0170219,"rature and introduce the tasks and datasets used for our experiments. We focus on three tasks: opinion target expression (OTE) detection; named entity recognition (NER), and slot-labeling, each of which can be modeled as a sequence tagging problem (Xu et al., 2018; Liu et al., 2019a; Louvan and Magnini, 2018). The IOB sequence tagging scheme (Ramshaw and Marcus, 1999) is used for each of these tasks. Related Work. The usefulness of continued training of large transformer-based models on domain/task-related unlabeled data has been shown recently (Gururangan et al., 2020; Rietzler et al., 2019; Xu et al., 2019), with a varied use of terminology for the process. Xu et al. (2019) and Rietzler et al. (2019) show gains of further tuning BERT using in-domain unlabeled data and refer to this as Post-training, and LM finetuning, respectively. More recently, Gururangan et al. (2020) use the term Domain-Adaptive Pretraining and show benefits over RoBERTa (Liu et al., 2019b). There have also been efforts to reduce model sizes for BERT, such as DistilBERT (Sanh et al., 2019), although these come at significant losses in performance. Opinion Target Expression (OTE) Detection: An integral component of fine-grain"
2020.emnlp-main.636,P18-2094,0,0.107342,"he labeled datasets, for the various tasks. We find that the CVT-based approach using less unlabeled data achieves similar performance with BERT-based models, while being superior in terms of financial and environmental cost as well. 2 Background, Tasks and Datasets Before presenting the models and their training setups, we discuss the relevant literature and introduce the tasks and datasets used for our experiments. We focus on three tasks: opinion target expression (OTE) detection; named entity recognition (NER), and slot-labeling, each of which can be modeled as a sequence tagging problem (Xu et al., 2018; Liu et al., 2019a; Louvan and Magnini, 2018). The IOB sequence tagging scheme (Ramshaw and Marcus, 1999) is used for each of these tasks. Related Work. The usefulness of continued training of large transformer-based models on domain/task-related unlabeled data has been shown recently (Gururangan et al., 2020; Rietzler et al., 2019; Xu et al., 2019), with a varied use of terminology for the process. Xu et al. (2019) and Rietzler et al. (2019) show gains of further tuning BERT using in-domain unlabeled data and refer to this as Post-training, and LM finetuning, respectively. More recently, Gur"
2020.emnlp-main.636,P95-1026,0,0.25807,"t the Data Science Institute, Columbia University 1 Not only BERT, but other models like RoBERTa (Liu et al., 2019b) and BART (Lewis et al., 2019) achieved state-of-the-art results in a variety of NLP tasks, but come at a great cost financially and environmentally (Strubell et al., 2019; Schwartz et al., 2019). In contrast, Cross-View Training (CVT) (Clark et al., 2018) is a semi-supervised approach that uses unlabeled data in a task-specific manner, rather than trying to learn general representations that can be used for many downstream tasks. Inspired by selflearning (McClosky et al., 2006; Yarowsky, 1995) and multi-view learning (Blum and Mitchell, 1998; Xu et al., 2013), the key idea is that the primary prediction module, which has an unrestricted view of the data, trains on the task using labeled examples, and makes task-specific predictions on unlabeled data. The auxiliary modules, with restricted views of the unlabeled data, attempt to replicate the primary module predictions. This helps to learn better representations for the task. We present an experimental study that investigates different task-agnostic and task-specific approaches to use unsupervised data and evaluates them in terms of"
2020.figlang-1.1,2020.figlang-1.10,0,0.0611491,"Missing"
2020.figlang-1.1,K16-1017,0,0.19068,"last decade, the problem of irony and sarcasm detection has attracted a considerable interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed in Response. The shared task is designed to benchmark the usefulness of modeling"
2020.figlang-1.1,2020.figlang-1.9,0,0.040498,"Missing"
2020.figlang-1.1,2020.figlang-1.15,0,0.0496774,"Interested readers can refer to the individual teams’ papers for more details. But first, we discuss the baseline classification model that we used. 4.1 System Descriptions ad6398 (Kumar and Anand, 2020): Report results comparing multiple transformer architectures (BERT, SpanBERT (Joshi et al., 2020), RoBERTa (Liu et al., 2019)) both in single sentence classification (with concatenated context and response string) and sentence pair classification (with context and response being separate inputs to a Siamese type architecture). Their best result was with using RoBERTa + LSTM model. aditya604 (Avvaru et al., 2020): Used BERT on simple concatenation of last-k context texts and response text. The authors included details of data cleaning (de-emojification, hashtag text extraction, apostrophe expansion) as well experiments on other architectures (LSTM, CNN, XLNet (Yang et al., 2019)) and varying size of context (5, 7, complete) in their report. The best results were obtained by BERT with 7 length context for Twitter dataset and BERT with 5 context for Reddit dataset. Baseline Classifier We use prior published work as the baseline that used conversation context to detect sarcasm from social media platforms"
2020.figlang-1.1,W16-0425,0,0.108561,"dicting the sarcastic or non-sarcastic label. Initial approaches used feature-based machine learning models that rely on different types of features from lexical (e.g., sarcasm markers, word embeddings) to pragmatic such as emoticons or learned patterns of contrast between positive sentiment and negative situations (Davidov et al., 2010; Veale and Hao, 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Ghosh and Muresan, 2018). Recently, deep learning methods have been applied for this task (Ghosh and Veale, 2016; Tay et al., 2018). For excellent surveys on sarcasm and irony detection see (Wallace, 2015; Joshi et al., 2017). However, when recognizing sarcastic intent even humans have difficulties sometimes when considering an utterance in isolation (Wallace et al., 2014). Recently an increasing number of researchers have started to explore the role of contextual information for irony and sarcasm analysis. The term context loosely refers to any information that is available beyond the utterance itself (Joshi et al., 2017). A few researchers have examined author context (Bamman and Smith, 2015; Khattri"
2020.figlang-1.1,2020.figlang-1.12,0,0.0249815,"rganize shared-tasks (Leong et al., 2018) because it is easy to use, provides easy communication with the participants (e.g., allows mass-emailing) as well as tracks all the submissions updating the leaderboard in real-time. The metrics used for evaluation is the average F1 score between the two categories - sarcastic and non-sarcastic. The leaderboards displayed the Precision, Recall, and F1 scores in the descending order of the F1 scores, separately for the two tracks - Twitter and Reddit. 4 4.2 We describe the participating systems in the following section (in alphabetical order). abaruah (Baruah et al., 2020): Fine-tuned a BERT model (Devlin et al., 2018) and reported results on varying maximum sequence length (corresponding to varying level of context inclusion from just response to entire context). They also reported results of BiLSTM with FastText embeddings (of response and entire context) and SVM based on char n-gram features (again on both response and entire context). One interesting result was SVM with discrete features performed better than BiLSTM. They achieved best results with BERT on response and most immediate context. Systems The shared task started on January 19, 2020, when the tra"
2020.figlang-1.1,D17-1050,0,0.623718,"arcasm detection has attracted a considerable interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed in Response. The shared task is designed to benchmark the usefulness of modeling the entire conversation context (i.e., all"
2020.figlang-1.1,P19-1239,0,0.0120467,"cognizing sarcastic intent even humans have difficulties sometimes when considering an utterance in isolation (Wallace et al., 2014). Recently an increasing number of researchers have started to explore the role of contextual information for irony and sarcasm analysis. The term context loosely refers to any information that is available beyond the utterance itself (Joshi et al., 2017). A few researchers have examined author context (Bamman and Smith, 2015; Khattri et al., 2015; Rajadesingan et al., 2015; Amir et al., 2016; Ghosh and Veale, 2017), multi-modal context (Schifanella et al., 2016; Cai et al., 2019; Castro et al., 2019), eye-tracking information (Mishra et al., 2016), or conversation context (Bamman and Smith, 2015; Wang et al., 2015; Joshi et al., 2016; Zhang et al., 2016; Ghosh et al., 2017; Ghosh and Veale, 2017). Related to shared tasks on figurative language analysis, recently, Van Hee et al. (2018) have con3.1 3.1.1 Datasets Reddit Training Dataset Khodak et al. (2017) introduced the self-annotated Reddit Corpus which is a very large collection of sarcastic and non-sarcastic posts (over one million) curated from different subreddits such as politics, religion, sports, technology,"
2020.figlang-1.1,J18-4009,1,0.882883,"nation of last-k context texts and response text. The authors included details of data cleaning (de-emojification, hashtag text extraction, apostrophe expansion) as well experiments on other architectures (LSTM, CNN, XLNet (Yang et al., 2019)) and varying size of context (5, 7, complete) in their report. The best results were obtained by BERT with 7 length context for Twitter dataset and BERT with 5 context for Reddit dataset. Baseline Classifier We use prior published work as the baseline that used conversation context to detect sarcasm from social media platforms such as Twitter and Reddit (Ghosh et al., 2018). Ghosh et al. (2018) proposed a dual LSTM architecture with hierarchical attention where one LSTM models the conversation context and the other models sarcastic response. The hierarchical attention (Yang et al., 2016) implements two levels of attention – one at the word level and another at the sentence level. We used their system based on only the immediate conversation context (i.e., the immediate prior turn). 2 This is denoted as LST Mattn in Table 3 and Table 4. amitjena40 (Jena et al., 2020): Used a timeseries analysis inspired approach for integrating context. Each text in conversationa"
2020.figlang-1.1,P19-1455,0,0.0257673,"Missing"
2020.figlang-1.1,W17-5523,1,0.950144,"le interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed in Response. The shared task is designed to benchmark the usefulness of modeling the entire conversation context (i.e., all the prior dialogue turns) for sarcasm de"
2020.figlang-1.1,D15-1116,1,0.94501,"peakers’ intended sentiments and beliefs. Consequently, in the last decade, the problem of irony and sarcasm detection has attracted a considerable interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed in Response. The sh"
2020.figlang-1.1,D18-2029,0,0.032603,"Missing"
2020.figlang-1.1,D17-1070,0,0.0118553,"s were available about the depth of context usage (full vs. immediate). Additionally, they only experimented with Twitter data and no submission was made to the Reddit track. They provided details of data cleaning measures for their experiments which involved stopword removal, lowercasing, stemming, punctuation removal and spelling normalization. burtenshaw (Lemmens et al., 2020): Employed an ensemble of four models - LSTM (on word, emoji and hashtag representations), CNNLSTM (on GloVe embeddings with discrete punctuation and sentiment features), MLP (on sentence embeddings through Infersent (Conneau et al., 2017)) and SVM (on character and stylometric features). The first three models (except SVM) used the last two immediate contexts along with the response. duke DS (Gregory et al., 2020): Here the authors have conducted extensive set of experiments using discrete features, DNNs, as well as transformer models, however, reporting only the results on the Twitter track. Regarding discrete features, one of novelties in their approach is including a predictor to identify whether the tweet is political or not, since many sarcastic tweets are on political topics. Regarding the models, the best performing mod"
2020.figlang-1.1,2020.figlang-1.6,0,0.0386357,"Missing"
2020.figlang-1.1,W10-2914,0,0.302085,"osite of what they say. Recognizing whether a speaker is ironic or sarcastic is essential to downstream applications for correctly understanding speakers’ intended sentiments and beliefs. Consequently, in the last decade, the problem of irony and sarcasm detection has attracted a considerable interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the s"
2020.figlang-1.1,P11-2102,1,0.835353,"ironic or sarcastic is essential to downstream applications for correctly understanding speakers’ intended sentiments and beliefs. Consequently, in the last decade, the problem of irony and sarcasm detection has attracted a considerable interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation conte"
2020.figlang-1.1,2020.figlang-1.37,0,0.0236927,"provided details of data cleaning measures for their experiments which involved stopword removal, lowercasing, stemming, punctuation removal and spelling normalization. burtenshaw (Lemmens et al., 2020): Employed an ensemble of four models - LSTM (on word, emoji and hashtag representations), CNNLSTM (on GloVe embeddings with discrete punctuation and sentiment features), MLP (on sentence embeddings through Infersent (Conneau et al., 2017)) and SVM (on character and stylometric features). The first three models (except SVM) used the last two immediate contexts along with the response. duke DS (Gregory et al., 2020): Here the authors have conducted extensive set of experiments using discrete features, DNNs, as well as transformer models, however, reporting only the results on the Twitter track. Regarding discrete features, one of novelties in their approach is including a predictor to identify whether the tweet is political or not, since many sarcastic tweets are on political topics. Regarding the models, the best performing model is an ensemble of five transformers: BERTbase-uncased, RoBERTa-base, XLNet-base-cased, RoBERTa-large, and ALBERT-base-v2. andy3223 (Dong et al., 2020): Used the transformer-bas"
2020.figlang-1.1,2020.figlang-1.38,0,0.0141076,"the response. duke DS (Gregory et al., 2020): Here the authors have conducted extensive set of experiments using discrete features, DNNs, as well as transformer models, however, reporting only the results on the Twitter track. Regarding discrete features, one of novelties in their approach is including a predictor to identify whether the tweet is political or not, since many sarcastic tweets are on political topics. Regarding the models, the best performing model is an ensemble of five transformers: BERTbase-uncased, RoBERTa-base, XLNet-base-cased, RoBERTa-large, and ALBERT-base-v2. andy3223 (Dong et al., 2020): Used the transformer-based architecture for sarcasm detection, reporting the performance of three architecture, BERT, RoBERTa, and ALBERT (Lan et al., 2019). They considered two models, the targetoriented where only the target (i.e., sarcastic response) is modeled and context-aware, where the context is also modeled with the target. The authors conducted extensive hyper-parameter search, and set the learning rate to 3e-5, the number of epochs to 30, and use different seed values, 21, 42, 63, for three runs. Additionally, they set the maximum sequence length 128 for the target-oriented models"
2020.figlang-1.1,C18-1156,0,0.05654,"putational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed in Response. The shared task is designed to benchmark the usefulness of modeling the entire conversation context (i.e., all the prior dialogue turns) for sarcasm detection. Section 2 disc"
2020.figlang-1.1,D17-1169,0,0.0142139,"tracted a considerable interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed in Response. The shared task is designed to benchmark the usefulness of modeling the entire conversation context (i.e., all the prior dialogue t"
2020.figlang-1.1,2020.figlang-1.11,0,0.0288441,"Missing"
2020.figlang-1.1,2020.figlang-1.8,0,0.0161972,"d conversation context to detect sarcasm from social media platforms such as Twitter and Reddit (Ghosh et al., 2018). Ghosh et al. (2018) proposed a dual LSTM architecture with hierarchical attention where one LSTM models the conversation context and the other models sarcastic response. The hierarchical attention (Yang et al., 2016) implements two levels of attention – one at the word level and another at the sentence level. We used their system based on only the immediate conversation context (i.e., the immediate prior turn). 2 This is denoted as LST Mattn in Table 3 and Table 4. amitjena40 (Jena et al., 2020): Used a timeseries analysis inspired approach for integrating context. Each text in conversational thread (context and response) was individually scored using BERT and Simple Exponential Smoothing (SES) was utilized to get probability of final response being sarcastic. They used the final response label as a pseudo-label for scoring the context entries, which is not theoretically grounded. If final response is sarcastic, the previous context dialogue cannot be assumed to be sarcastic (with respect to its preceding dialogue). However, the effect of this error is attenuated due to exponentially"
2020.figlang-1.1,W18-0907,0,0.0318553,"els (both Reddit and Twitter). Participants can choose to partition the training data further to a validation set for preliminary evaluations and/or tuning of hyper-parameters. Likewise, they can also elect to perform cross-validation on the training data. 3.3 Evaluation Phase In the second phase, instances for evaluation are released. Each participating system generated predictions for the evaluation instances, for up to N 3 models. 1 Predictions are submitted to the CodaLab site and evaluated automatically against the gold labels. CodaLab is an established platform to organize shared-tasks (Leong et al., 2018) because it is easy to use, provides easy communication with the participants (e.g., allows mass-emailing) as well as tracks all the submissions updating the leaderboard in real-time. The metrics used for evaluation is the average F1 score between the two categories - sarcastic and non-sarcastic. The leaderboards displayed the Precision, Recall, and F1 scores in the descending order of the F1 scores, separately for the two tracks - Twitter and Reddit. 4 4.2 We describe the participating systems in the following section (in alphabetical order). abaruah (Baruah et al., 2020): Fine-tuned a BERT m"
2020.figlang-1.1,P15-2124,0,0.666457,"ntiments and beliefs. Consequently, in the last decade, the problem of irony and sarcasm detection has attracted a considerable interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed in Response. The shared task is designe"
2020.figlang-1.1,W13-1605,0,0.0783871,"Missing"
2020.figlang-1.1,K16-1015,0,0.0184264,"number of researchers have started to explore the role of contextual information for irony and sarcasm analysis. The term context loosely refers to any information that is available beyond the utterance itself (Joshi et al., 2017). A few researchers have examined author context (Bamman and Smith, 2015; Khattri et al., 2015; Rajadesingan et al., 2015; Amir et al., 2016; Ghosh and Veale, 2017), multi-modal context (Schifanella et al., 2016; Cai et al., 2019; Castro et al., 2019), eye-tracking information (Mishra et al., 2016), or conversation context (Bamman and Smith, 2015; Wang et al., 2015; Joshi et al., 2016; Zhang et al., 2016; Ghosh et al., 2017; Ghosh and Veale, 2017). Related to shared tasks on figurative language analysis, recently, Van Hee et al. (2018) have con3.1 3.1.1 Datasets Reddit Training Dataset Khodak et al. (2017) introduced the self-annotated Reddit Corpus which is a very large collection of sarcastic and non-sarcastic posts (over one million) curated from different subreddits such as politics, religion, sports, technology, etc. This corpus contains self-labeled sarcastic posts where users label their posts as sarcastic by marking “/s” to the end of sarcastic posts. For any such"
2020.figlang-1.1,2020.tacl-1.5,0,0.0145229,"for the Reddit track and 38 systems for the Twitter track, respectively. Out of all submissions, 14 shared task system papers were submitted. In the following section we summarize each system paper. We also put forward a comparative analysis based on their performance and the choice of features/models in Section 5. Interested readers can refer to the individual teams’ papers for more details. But first, we discuss the baseline classification model that we used. 4.1 System Descriptions ad6398 (Kumar and Anand, 2020): Report results comparing multiple transformer architectures (BERT, SpanBERT (Joshi et al., 2020), RoBERTa (Liu et al., 2019)) both in single sentence classification (with concatenated context and response string) and sentence pair classification (with context and response being separate inputs to a Siamese type architecture). Their best result was with using RoBERTa + LSTM model. aditya604 (Avvaru et al., 2020): Used BERT on simple concatenation of last-k context texts and response text. The authors included details of data cleaning (de-emojification, hashtag text extraction, apostrophe expansion) as well experiments on other architectures (LSTM, CNN, XLNet (Yang et al., 2019)) and varyi"
2020.figlang-1.1,2021.ccl-1.108,0,0.0206321,"Missing"
2020.figlang-1.1,2020.figlang-1.7,0,0.0283253,"Missing"
2020.figlang-1.1,W15-2905,0,0.0178498,"le, 2016; Tay et al., 2018). For excellent surveys on sarcasm and irony detection see (Wallace, 2015; Joshi et al., 2017). However, when recognizing sarcastic intent even humans have difficulties sometimes when considering an utterance in isolation (Wallace et al., 2014). Recently an increasing number of researchers have started to explore the role of contextual information for irony and sarcasm analysis. The term context loosely refers to any information that is available beyond the utterance itself (Joshi et al., 2017). A few researchers have examined author context (Bamman and Smith, 2015; Khattri et al., 2015; Rajadesingan et al., 2015; Amir et al., 2016; Ghosh and Veale, 2017), multi-modal context (Schifanella et al., 2016; Cai et al., 2019; Castro et al., 2019), eye-tracking information (Mishra et al., 2016), or conversation context (Bamman and Smith, 2015; Wang et al., 2015; Joshi et al., 2016; Zhang et al., 2016; Ghosh et al., 2017; Ghosh and Veale, 2017). Related to shared tasks on figurative language analysis, recently, Van Hee et al. (2018) have con3.1 3.1.1 Datasets Reddit Training Dataset Khodak et al. (2017) introduced the self-annotated Reddit Corpus which is a very large collection of"
2020.figlang-1.1,maynard-greenwood-2014-cares,0,0.228596,"wnstream applications for correctly understanding speakers’ intended sentiments and beliefs. Consequently, in the last decade, the problem of irony and sarcasm detection has attracted a considerable interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarc"
2020.figlang-1.1,2020.figlang-1.13,0,0.0215421,"eddit track and 1070 for the Twitter track. The CodaLab leaderboard showcases results from 39 systems for the Reddit track and 38 systems for the Twitter track, respectively. Out of all submissions, 14 shared task system papers were submitted. In the following section we summarize each system paper. We also put forward a comparative analysis based on their performance and the choice of features/models in Section 5. Interested readers can refer to the individual teams’ papers for more details. But first, we discuss the baseline classification model that we used. 4.1 System Descriptions ad6398 (Kumar and Anand, 2020): Report results comparing multiple transformer architectures (BERT, SpanBERT (Joshi et al., 2020), RoBERTa (Liu et al., 2019)) both in single sentence classification (with concatenated context and response string) and sentence pair classification (with context and response being separate inputs to a Siamese type architecture). Their best result was with using RoBERTa + LSTM model. aditya604 (Avvaru et al., 2020): Used BERT on simple concatenation of last-k context texts and response text. The authors included details of data cleaning (de-emojification, hashtag text extraction, apostrophe expa"
2020.figlang-1.1,P16-1104,0,0.389484,"roblem of irony and sarcasm detection has attracted a considerable interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed in Response. The shared task is designed to benchmark the usefulness of modeling the entire conversat"
2020.figlang-1.1,2020.figlang-1.2,0,0.285999,"Missing"
2020.figlang-1.1,2020.figlang-1.36,0,0.0128986,"al., 2014). Using BERT as a feature extraction method as opposed to fine-tuning it was not beneficial and Logisitic Regression over GloVe embeddings outperformed them in their experiment. Context was used in their best model but no details were available about the depth of context usage (full vs. immediate). Additionally, they only experimented with Twitter data and no submission was made to the Reddit track. They provided details of data cleaning measures for their experiments which involved stopword removal, lowercasing, stemming, punctuation removal and spelling normalization. burtenshaw (Lemmens et al., 2020): Employed an ensemble of four models - LSTM (on word, emoji and hashtag representations), CNNLSTM (on GloVe embeddings with discrete punctuation and sentiment features), MLP (on sentence embeddings through Infersent (Conneau et al., 2017)) and SVM (on character and stylometric features). The first three models (except SVM) used the last two immediate contexts along with the response. duke DS (Gregory et al., 2020): Here the authors have conducted extensive set of experiments using discrete features, DNNs, as well as transformer models, however, reporting only the results on the Twitter track."
2020.figlang-1.1,P19-1275,0,0.0140463,"ask has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed in Response. The shared task is designed to benchmark the usefulness of modeling the entire conversation context (i.e., all the prior dialogue turns) for sarcasm detection. Section 2 discusses the current state of research on sa"
2020.figlang-1.1,D14-1162,0,0.0847811,"ion of prior turns and response) LST Mattn BERT-Large (concatenation of response and its immediate prior turn) 2 6 8 9 12 16 17 21 26 31 32 andy3223 taha tanvidadu nclabj ad6398 kalaivani.A amitjena40 burtenshaw salokr adithya604 baseline abaruah Table 3: Performance of the best system per team and baseline for the Reddit track. We include two ranks - ranks from the submitted systems as well as the Leaderboard ranks from the CodaLab site AnandKumaR (Khatri and P, 2020): Experimented with using traditional ML classifiers like SVM and Logisitic Regression over embeddings through BERT and GloVe (Pennington et al., 2014). Using BERT as a feature extraction method as opposed to fine-tuning it was not beneficial and Logisitic Regression over GloVe embeddings outperformed them in their experiment. Context was used in their best model but no details were available about the depth of context usage (full vs. immediate). Additionally, they only experimented with Twitter data and no submission was made to the Reddit track. They provided details of data cleaning measures for their experiments which involved stopword removal, lowercasing, stemming, punctuation removal and spelling normalization. burtenshaw (Lemmens et"
2020.figlang-1.1,P14-2084,0,0.246667,"rectly understanding speakers’ intended sentiments and beliefs. Consequently, in the last decade, the problem of irony and sarcasm detection has attracted a considerable interest from computational linguistics researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed"
2020.figlang-1.1,N18-1202,0,0.0671992,"Missing"
2020.figlang-1.1,D13-1066,0,0.646664,"Missing"
2020.figlang-1.1,N16-1174,0,0.0351354,"Net (Yang et al., 2019)) and varying size of context (5, 7, complete) in their report. The best results were obtained by BERT with 7 length context for Twitter dataset and BERT with 5 context for Reddit dataset. Baseline Classifier We use prior published work as the baseline that used conversation context to detect sarcasm from social media platforms such as Twitter and Reddit (Ghosh et al., 2018). Ghosh et al. (2018) proposed a dual LSTM architecture with hierarchical attention where one LSTM models the conversation context and the other models sarcastic response. The hierarchical attention (Yang et al., 2016) implements two levels of attention – one at the word level and another at the sentence level. We used their system based on only the immediate conversation context (i.e., the immediate prior turn). 2 This is denoted as LST Mattn in Table 3 and Table 4. amitjena40 (Jena et al., 2020): Used a timeseries analysis inspired approach for integrating context. Each text in conversational thread (context and response) was individually scored using BERT and Simple Exponential Smoothing (SES) was utilized to get probability of final response being sarcastic. They used the final response label as a pseud"
2020.figlang-1.1,C16-1231,0,0.0532053,"rs have started to explore the role of contextual information for irony and sarcasm analysis. The term context loosely refers to any information that is available beyond the utterance itself (Joshi et al., 2017). A few researchers have examined author context (Bamman and Smith, 2015; Khattri et al., 2015; Rajadesingan et al., 2015; Amir et al., 2016; Ghosh and Veale, 2017), multi-modal context (Schifanella et al., 2016; Cai et al., 2019; Castro et al., 2019), eye-tracking information (Mishra et al., 2016), or conversation context (Bamman and Smith, 2015; Wang et al., 2015; Joshi et al., 2016; Zhang et al., 2016; Ghosh et al., 2017; Ghosh and Veale, 2017). Related to shared tasks on figurative language analysis, recently, Van Hee et al. (2018) have con3.1 3.1.1 Datasets Reddit Training Dataset Khodak et al. (2017) introduced the self-annotated Reddit Corpus which is a very large collection of sarcastic and non-sarcastic posts (over one million) curated from different subreddits such as politics, religion, sports, technology, etc. This corpus contains self-labeled sarcastic posts where users label their posts as sarcastic by marking “/s” to the end of sarcastic posts. For any such sarcastic post, the"
2020.figlang-1.1,2020.figlang-1.14,0,0.0259529,"Missing"
2020.figlang-1.1,P18-1093,0,0.223221,"researchers. The task has been usually framed as a binary classification task (sarcastic vs. non-sarcastic) using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Tsur et al., 2010; Gonz´alezIb´an˜ ez et al., 2011; Riloff et al., 2013; Maynard and Greenwood, 2014; Wallace et al., 2014; Ghosh et al., 2015; Joshi et al., 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Oprea and Magdy, 2019; Majumder et al., 2019; Castro et al., 2019; Ghosh et al., 2019). In this paper, we report on the shared task on sarcasm detection that we conducted as part of the Table 1 and Table 2 show examples of three turn dialogues, where Response is the sarcastic reply. Without using the conversation context Contexti , it is difficult to identify the sarcastic intent expressed in Response. The shared task is designed to benchmark the usefulness of modeling the entire conversation context (i.e., all the prior dialogue turns) for sarcasm detection. Section 2 discusses the current"
2020.figlang-1.1,S18-1005,0,0.110328,"Missing"
2020.lrec-1.879,W02-0603,0,0.495662,"els are inefficient in detecting the morpheme wa due to its high degree of ambiguity. For the analysis of the common segmentation phenomena seen in both Mexicanero and Nahuatl, see Eskander et al. (2019). 5. Related Work Unsupervised morphological segmentation was first performed by expensive manual rule engineering. An early use of machine learning for morphological segmentation was proposed by Goldsmith (2001) through the use of the Minimum Description Length (MDL) approach. The approach, however, requires some manual work that makes it challenging to generalize across languages. Morfessor (Creutz and Lagus, 2002), is a commonly used unsupervised and semi-supervised morphologicalsegmentation framework that utilizes the MDL principal, along with an HMM model, where the morphemes have a hierarchical structure. Another variation of Morfessor is Morfessor FlatCat (Gr¨onroos et al., 2014), which predicts both segmentation and morpheme categories. Log-linear models have proved successful for the problem of unsupervised morphological segmentation (Poon et al., 2009) with the use of global and contextual features. Another log-linear model is proposed by Narasimhan et al. (2015), where they arrange the words in"
2020.lrec-1.879,N09-1019,0,0.0971274,"Missing"
2020.lrec-1.879,C16-1086,1,0.299902,"hological segmentation has been an active research topic for decades as it is beneficial for many natural language processing tasks. With the high cost of manually labeling data for morphology and the increasing interest in low-resource languages, unsupervised morphological segmentation has become essential for processing a typologically diverse set of languages, whether high-resource or low-resource. In this paper, we present and release MorphAGram, a publicly available framework for unsupervised morphological segmentation that uses Adaptor Grammars (AG) and is based on the work presented by Eskander et al. (2016). We conduct an extensive quantitative and qualitative evaluation of this framework on 12 languages and show that the framework achieves state-of-the-art results across languages of different typologies (from fusional to polysynthetic and from high-resource to low-resource). Keywords: Unsupervised Morphological Segmentation Framework, Low-Resource Languages, Qualitative and Qualitative Evaluation, Adaptor Grammars, Language Typology 1. Introduction Many natural language processing tasks profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al.,"
2020.lrec-1.879,W19-4222,1,0.726857,"nds the set of distributions over linguistic structures that can be characterized by a grammar, better matching the occurrences of trees and sub-trees observed in actual corpora. AGs define a framework to implement Bayesian nonparametric learning of grammars and are usually trained in an unsupervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in"
2020.lrec-1.879,J01-2001,0,0.656015,"hile the AG-LI and AG-SS models tend to over-segment in Yorem Nokki, MorphoChain under-segments most of the time, which is why it cannot produce many of the short common morphemes. Finally, all the models are inefficient in detecting the morpheme wa due to its high degree of ambiguity. For the analysis of the common segmentation phenomena seen in both Mexicanero and Nahuatl, see Eskander et al. (2019). 5. Related Work Unsupervised morphological segmentation was first performed by expensive manual rule engineering. An early use of machine learning for morphological segmentation was proposed by Goldsmith (2001) through the use of the Minimum Description Length (MDL) approach. The approach, however, requires some manual work that makes it challenging to generalize across languages. Morfessor (Creutz and Lagus, 2002), is a commonly used unsupervised and semi-supervised morphologicalsegmentation framework that utilizes the MDL principal, along with an HMM model, where the morphemes have a hierarchical structure. Another variation of Morfessor is Morfessor FlatCat (Gr¨onroos et al., 2014), which predicts both segmentation and morpheme categories. Log-linear models have proved successful for the problem"
2020.lrec-1.879,C14-1111,0,0.0487354,"Missing"
2020.lrec-1.879,P11-2094,0,0.0282008,"c learning of grammars and are usually trained in an unsupervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in terms of known affixes (Section 3). We conduct an extensive quantitative and qualitative evaluation of this framework (Section 4) for a set of 12 languages across a language typology continuum (Section 2), namely English, German, Fin"
2020.lrec-1.879,C10-1060,0,0.0222975,"and sub-trees observed in actual corpora. AGs define a framework to implement Bayesian nonparametric learning of grammars and are usually trained in an unsupervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in terms of known affixes (Section 3). We conduct an extensive quantitative and qualitative evaluation of this framework (Section 4) for a set"
2020.lrec-1.879,W08-0704,0,0.638478,"atching the occurrences of trees and sub-trees observed in actual corpora. AGs define a framework to implement Bayesian nonparametric learning of grammars and are usually trained in an unsupervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in terms of known affixes (Section 3). We conduct an extensive quantitative and qualitative evalua"
2020.lrec-1.879,P08-1046,0,0.173545,"atching the occurrences of trees and sub-trees observed in actual corpora. AGs define a framework to implement Bayesian nonparametric learning of grammars and are usually trained in an unsupervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in terms of known affixes (Section 3). We conduct an extensive quantitative and qualitative evalua"
2020.lrec-1.879,N18-1005,0,0.283831,"Missing"
2020.lrec-1.879,W10-2211,0,0.027971,"d subtrees that correspond to the adapted nonterminals in addition to the segmentation output of the input vocabulary. Text segmentation can then be performed in two different modes; transductive and inductive. In the transductive mode, the word should be present in the vocabulary list provided to the learner, where the segmentation output serves as a segmentation lookup. In contrast, the inductive mode is suitable for words that were not processed by the learner, Evaluation and Results Data The data for English, German, Finnish and Turkish is from the Morpho Challenge competition 2 (MC2010) (Kurimo et al., 2010), where we select the most frequent 50,000 words for training after filtering out those words that have foreign letters. In addition, the development sets are collected from all the years of the competition, where we filter out the German words in which the stem receives transformation. The Estonian training and development sets are the ones used by Sirts and Goldwater (2013) 3 , where we filter out words containing foreign letters. The data is based on the Sega corpus 4 , where the gold segmentation in the development set is collected from the Estonian Morphologically Disambiguated Corpus 5 ."
2020.lrec-1.879,D14-1095,0,0.349557,"itative and qualitative evaluation of this framework on 12 languages and show that the framework achieves state-of-the-art results across languages of different typologies (from fusional to polysynthetic and from high-resource to low-resource). Keywords: Unsupervised Morphological Segmentation Framework, Low-Resource Languages, Qualitative and Qualitative Evaluation, Adaptor Grammars, Language Typology 1. Introduction Many natural language processing tasks profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). Many of the languages of the world are low-resource and/or endangered, where they lack adequate morphologically annotated resources. Thus, open-source unsupervised morphological segmentation frameworks could be an important resource for the computational linguistics community. In addition, we argue that frameworks that enable the use of linguistic knowledge to guide the learning process could be particularly beneficial when working on low-resource or endangered languages, where even unsegmentated data might be minimal. We present MorphAGram 1 , an publicly available framework for unsupervise"
2020.lrec-1.879,Q15-1012,0,0.141547,"rStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu2a+SM Sch. PrStSu2a+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Table 3: The best language-independent (standard/cascaded) setup (AG-LI) and the best scholarseeded setup (AG-SS) per language. Std.=Standard, Casc.=Cascaded, and Sch.=Scholar-Seeded We conduct the evaluation in a transductive learning scenario, where the unsegmented test words are included in our training set, which is common in the evaluation of unsupervised morphological segmentation (Poon et al., 2009; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Eskander et al., 2016). However, we do not see gains in the performance when using the inductive learning approach instead, where the unsegmented test words are separate from the training set. We run the learners for 500 iterations for all languages, and we compute the results as the average of five runs since the samplers are non-deterministic. No annealing is used as it does not improve the results, and all parameters are automatically inferred. 4.3. Language Ave L(M) Ave M/W Max M/W Ambiguity English 5.30 2.39 6 0.48 German 5.16 2.94 8 0.43 5.74 3.48 9 0.53 Finnish Estonian 5.63 1.93 7 0."
2020.lrec-1.879,C10-1092,0,0.0406524,"presented by Eskander et al. (2016). We conduct an extensive quantitative and qualitative evaluation of this framework on 12 languages and show that the framework achieves state-of-the-art results across languages of different typologies (from fusional to polysynthetic and from high-resource to low-resource). Keywords: Unsupervised Morphological Segmentation Framework, Low-Resource Languages, Qualitative and Qualitative Evaluation, Adaptor Grammars, Language Typology 1. Introduction Many natural language processing tasks profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). Many of the languages of the world are low-resource and/or endangered, where they lack adequate morphologically annotated resources. Thus, open-source unsupervised morphological segmentation frameworks could be an important resource for the computational linguistics community. In addition, we argue that frameworks that enable the use of linguistic knowledge to guide the learning process could be particularly beneficial when working on low-resource or endangered languages, where even unsegmentated data might be minimal. We"
2020.lrec-1.879,N09-1024,0,0.674735,"n in Table 3. Best AG-SS Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu2a+SM Sch. PrStSu2a+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Sch. PrStSu+SM Table 3: The best language-independent (standard/cascaded) setup (AG-LI) and the best scholarseeded setup (AG-SS) per language. Std.=Standard, Casc.=Cascaded, and Sch.=Scholar-Seeded We conduct the evaluation in a transductive learning scenario, where the unsegmented test words are included in our training set, which is common in the evaluation of unsupervised morphological segmentation (Poon et al., 2009; Sirts and Goldwater, 2013; Narasimhan et al., 2015; Eskander et al., 2016). However, we do not see gains in the performance when using the inductive learning approach instead, where the unsegmented test words are separate from the training set. We run the learners for 500 iterations for all languages, and we compute the results as the average of five runs since the samplers are non-deterministic. No annealing is used as it does not improve the results, and all parameters are automatically inferred. 4.3. Language Ave L(M) Ave M/W Max M/W Ambiguity English 5.30 2.39 6 0.48 German 5.16 2.94 8 0"
2020.lrec-1.879,C10-1116,0,0.0240946,"PrStSu+SM Casc. PrStSu+SM Casc. PrStSu+SM Casc. PrStSu+SM Std. PrStSu+SM Std. PrStSu+SM Casc. PrStSu+SM Std. PrStSu+SM Std. PrStSu+SM Std. PrStSu+SM Std. PrStSu+SM Evaluation Metrics We evaluate the performance of our morphologicalsegmentation framework using two metrics: Boundary Precision and Recall (BPR) and EMMA-2 (Virpioja et al., 2011). BPR is the classical evaluation method for morphological segmentation, where the boundaries in the proposed segmentation are compared to the boundaries in the reference. In contrast, EMMA-2 is based on matching the morphemes, and is a variation of EMMA (Spiegler and Monson, 2010). In EMMA, each proposed morpheme is matched to each morpheme in the gold segmentation through one-to-one mappings. However, EMMA-2 allows for shorter computation times as it replaces the one-to-one assignment problem in EMMA by two many-to-one assignment problems, where two or more proposed morphemes can be mapped to one reference morpheme. EMMA-2 also results in higher precision and recall as it tolerates failing to join two allomorphs or to distinguish between identical syncretic morphemes. 4.4. Baselines We evaluate our system versus two state-of-the-art baselines: Morfessor (Creutz and La"
2020.lrec-1.879,C10-1115,0,0.0768193,"Missing"
2020.lrec-1.879,D12-1064,0,0.0119657,"upervised manner using sampling techniques. AGs have been used successfully for unsupervised morphological segmentation, where a grammar is a morphological grammar that specifies word structure (Sirts 1 https://github.com/rnd2110/MorphAGram and Goldwater, 2013; Eskander et al., 2016; Eskander et al., 2018; Eskander et al., 2019). AGs have been also applied to other NLP applications such as word segmentation (Johnson, 2008a; Johnson, 2008c; Johnson and Demuth, 2010), named-entity clustering (Elsner et al., 2009), transliteration of names (Huang et al., 2011) and native-language identification (Wong et al., 2012). In this paper, we release MorphAGram, a publicly available framework for unsupervised morphological segmentation. The framework is also suitable for semi-supervised learning setups where it allows linguistic knowledge to be specified at two levels: designing the grammars and using scholar-seeded knowledge in terms of known affixes (Section 3). We conduct an extensive quantitative and qualitative evaluation of this framework (Section 4) for a set of 12 languages across a language typology continuum (Section 2), namely English, German, Finnish, Estonian, Georgian, Turkish, Arabic, Zulu, Mexica"
2020.scil-1.10,K16-1017,0,0.0119928,"positive sentiment words and the negative situation in this example). Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is equally important to understand how the hearer interprets the speaker’s sarcastic/iro"
2020.scil-1.10,P19-1455,0,0.0204735,"Missing"
2020.scil-1.10,W04-3205,0,0.0111958,"Missing"
2020.scil-1.10,W10-2914,0,0.252119,"racteristic of verbal irony is called semantic incongruity — incongruity between the literal evaluation and the context (e.g., between the positive sentiment words and the negative situation in this example). Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recogn"
2020.scil-1.10,P08-1118,0,0.0335518,"Missing"
2020.scil-1.10,P16-1047,0,0.0367951,"Missing"
2020.scil-1.10,D17-1169,0,0.0123339,"mple). Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is equally important to understand how the hearer interprets the speaker’s sarcastic/ironic message. For the above utterance, the strength of negative s"
2020.scil-1.10,W16-0425,0,0.0130159,"ing to the inter-subjective account of negation types (Verhagen, 2005). Sentential negation leads the addressee to open up an alternative mental space where an opposite predication is at stake. 7 Related Work Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Gonz´alezIb´an˜ ez et al., 2011; Liebrecht et al., 2013; Wallace et al., 2014; Zhang et al., 2016; Ghosh and Veale, 2016; Schifanella et al., 2016; Xiong et al., 2019; Castro et al., 2019). Unlike this line of work, our research focuses on how the hearer interprets an ironic message. The findings from our study could have multiple impacts on the sarcasm detection task. First, interpretation strategies open up a scope of “graded interpretation” of irony instead of only a binary decision (i.e., predicting the strength of irony). Second, nature of semantic incongruence and stereotype irony situations can be useful features in irony detection. Recently, Peled and Reichart (2017) proposed a computational model based"
2020.scil-1.10,D17-1050,0,0.251319,"e situation in this example). Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is equally important to understand how the hearer interprets the speaker’s sarcastic/ironic message. For the above utterance, the st"
2020.scil-1.10,J18-4009,1,0.876441,"rcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is equally important to understand how the hearer interprets the speaker’s sarcastic/ironic message. For the above utterance, the strength of negative sentiment perceived by the hearer depends on whether they interpret the speaker’s"
2020.scil-1.10,D15-1116,1,0.954605,"n than in the former. Kreuz (2000) noted that most studies in linguistics and psychology have conducted experiments analyzing reaction times (Gibbs, 1986; Katz et al., 2004) or situational context (Ivanko and Pexman, 2003), featuring a setup with in vitro data aimed at testing the validity of specific theories of irony. Instead, our study adopts a naturalistic approach to understand hearers’ reception of irony looking at what linguistic strategies are recurrently used by hearers to interpret the non-literal meaning underlying ironic utterances. We leverage the crowdsourcing task introduced by Ghosh et al. (2015) for their work on detecting whether a word has a literal or sarcastic interpretation, later adopted by Peled and Reichart (2017). The task is framed as follows: given a speaker’s ironic message, five annotators (e.g., Turkers on Amazon Mechanical Turk (MTurk)) are asked to verbalize their interpretation of the speaker’s ironic message (i.e., their understanding of the speaker’s intended meaning) (see Table 1; Sim denotes the speaker’s ironic message, while Hint denotes the hearer’s interpretation of that ironic message). The crowdsourcing experiments are reported in Section 2. The paper makes"
2020.scil-1.10,W17-5523,1,0.887237,"arch on verbal irony or sarcasm has focused on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is equally important to understand how the hearer interprets the speaker’s sarcastic/ironic message. For the above utterance, the strength of negative sentiment perceived b"
2020.scil-1.10,P11-2102,1,0.786712,"they are simply omitted in the interpretations. This trend can be explained according to the inter-subjective account of negation types (Verhagen, 2005). Sentential negation leads the addressee to open up an alternative mental space where an opposite predication is at stake. 7 Related Work Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Gonz´alezIb´an˜ ez et al., 2011; Liebrecht et al., 2013; Wallace et al., 2014; Zhang et al., 2016; Ghosh and Veale, 2016; Schifanella et al., 2016; Xiong et al., 2019; Castro et al., 2019). Unlike this line of work, our research focuses on how the hearer interprets an ironic message. The findings from our study could have multiple impacts on the sarcasm detection task. First, interpretation strategies open up a scope of “graded interpretation” of irony instead of only a binary decision (i.e., predicting the strength of irony). Second, nature of semantic incongruence and stereotype irony situations can be useful features in"
2020.scil-1.10,C18-1156,0,0.0271066,"or sarcasm has focused on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is equally important to understand how the hearer interprets the speaker’s sarcastic/ironic message. For the above utterance, the strength of negative sentiment perceived by the hearer depends on"
2020.scil-1.10,P15-2124,0,0.0714567,"between the literal evaluation and the context (e.g., between the positive sentiment words and the negative situation in this example). Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is equally important to"
2020.scil-1.10,E17-1025,0,0.0388994,"Missing"
2020.scil-1.10,P07-2045,0,0.00734643,"Missing"
2020.scil-1.10,W13-1605,0,0.0374211,"Missing"
2020.scil-1.10,maynard-greenwood-2014-cares,0,0.0211125,"irony is called semantic incongruity — incongruity between the literal evaluation and the context (e.g., between the positive sentiment words and the negative situation in this example). Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic"
2020.scil-1.10,P16-1104,0,0.0139396,"words and the negative situation in this example). Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is equally important to understand how the hearer interprets the speaker’s sarcastic/ironic message. For the"
2020.scil-1.10,J13-3004,0,0.0172538,"Sim -Hint has no overlap with the dev set of 500 Sim -Hint pairs used to identify the strategies (Section 4). Agreement between the annotators for both sets is high with  &gt; 0.9. In SIGNtest , 79 instances were just copies of the original message, which we eliminated, thus the SIGNtest contains only 421 instances. 5.1 Computational Methods Lexical Antonyms. To detect whether an Sim Hint pair uses the lexical antonyms strategy, we first need to build a resource of lexical antonyms. We use the MPQA sentiment Lexicon (Wilson et al., 2005), Hu and Liu (2004)’s opinion lexicon, antonym pairs from Mohammad et al. (2013), antonyms from WordNet, and pairs of opposite verbs from Verbocean (Chklovski and Pantel, 2004). Given this lexicon of lexical antonyms, the task is now to detect whether a given Sim -Hint pair Strategies Lex ant Simple neg AN weaksent ANI!D AN desiderative AntPhrase+PragInf P 89.0 92.0 93.6 53.1 100.0 86.2 dev R 95.7 89.4 87.9 65.4 92.9 53.2 F1 92.2 90.7 90.7 58.6 96.3 65.8 P 97.2 88.3 95.0 80.0 100.0 70.7 test R 89.9 88.3 91.9 0.44 100.0 85.3 F1 93.4 88.3 93.4 57.2 100.0 77.4 SIGNtest P R F1 89.4 97.9 93.5 93.3 91.2 92.2 93.3 87.5 90.3 85.7 70.6 77.4 100.0 66.7 80.0 89.5 68.0 77.3 Table 3:"
2020.scil-1.10,J04-4002,0,0.0753307,"Missing"
2020.scil-1.10,P19-1275,0,0.0115257,"ting ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is equally important to understand how the hearer interprets the speaker’s sarcastic/ironic message. For the above utterance, the strength of negative sentiment perceived by the hearer depends on whether they interpret the speaker’s actual meaning as “pictu"
2020.scil-1.10,P17-1155,0,0.431604,"Missing"
2020.scil-1.10,D16-1078,0,0.0642819,"Missing"
2020.scil-1.10,P18-2085,0,0.026606,"Missing"
2020.scil-1.10,W15-2914,0,0.0359071,"Missing"
2020.scil-1.10,D13-1066,0,0.146747,"Missing"
2020.scil-1.10,J11-2001,0,0.0734719,"Missing"
2020.scil-1.10,P18-1093,0,0.0143923,"on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is equally important to understand how the hearer interprets the speaker’s sarcastic/ironic message. For the above utterance, the strength of negative sentiment perceived by the hearer depends on whether they inte"
2020.scil-1.10,P14-2084,0,0.0631366,"ngruity — incongruity between the literal evaluation and the context (e.g., between the positive sentiment words and the negative situation in this example). Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating ⇤ Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Davidov et al., 2010; Maynard and Greenwood, 2014; Wallace et al., 2014; Joshi et al., 2015; Bamman and Smith, 2015; Muresan et al., 2016; Amir et al., 2016; Mishra et al., 2016; Ghosh and Veale, 2017; Felbo et al., 2017; Ghosh et al., 2017; Hazarika et al., 2018; Tay et al., 2018; Ghosh et al., 2018; Oprea and Magdy, 2019). Such approaches have focused their analysis on the speakers’ beliefs and intentions for using irony (Attardo, 2000). However, sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer (Haverkate, 1990). Thus, we argue that, besides recognizing the speaker’s sarcastic/ironic intent, it is"
2020.scil-1.10,H05-1044,0,0.0215467,"dataset (i.e., denoted by SIGNtest ), respectively. Note, the test set for the Sim -Hint has no overlap with the dev set of 500 Sim -Hint pairs used to identify the strategies (Section 4). Agreement between the annotators for both sets is high with  &gt; 0.9. In SIGNtest , 79 instances were just copies of the original message, which we eliminated, thus the SIGNtest contains only 421 instances. 5.1 Computational Methods Lexical Antonyms. To detect whether an Sim Hint pair uses the lexical antonyms strategy, we first need to build a resource of lexical antonyms. We use the MPQA sentiment Lexicon (Wilson et al., 2005), Hu and Liu (2004)’s opinion lexicon, antonym pairs from Mohammad et al. (2013), antonyms from WordNet, and pairs of opposite verbs from Verbocean (Chklovski and Pantel, 2004). Given this lexicon of lexical antonyms, the task is now to detect whether a given Sim -Hint pair Strategies Lex ant Simple neg AN weaksent ANI!D AN desiderative AntPhrase+PragInf P 89.0 92.0 93.6 53.1 100.0 86.2 dev R 95.7 89.4 87.9 65.4 92.9 53.2 F1 92.2 90.7 90.7 58.6 96.3 65.8 P 97.2 88.3 95.0 80.0 100.0 70.7 test R 89.9 88.3 91.9 0.44 100.0 85.3 F1 93.4 88.3 93.4 57.2 100.0 77.4 SIGNtest P R F1 89.4 97.9 93.5 93.3"
2020.scil-1.10,C16-1231,0,0.0153272,"be explained according to the inter-subjective account of negation types (Verhagen, 2005). Sentential negation leads the addressee to open up an alternative mental space where an opposite predication is at stake. 7 Related Work Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context, author context, visual context, or cognitive features (Gonz´alezIb´an˜ ez et al., 2011; Liebrecht et al., 2013; Wallace et al., 2014; Zhang et al., 2016; Ghosh and Veale, 2016; Schifanella et al., 2016; Xiong et al., 2019; Castro et al., 2019). Unlike this line of work, our research focuses on how the hearer interprets an ironic message. The findings from our study could have multiple impacts on the sarcasm detection task. First, interpretation strategies open up a scope of “graded interpretation” of irony instead of only a binary decision (i.e., predicting the strength of irony). Second, nature of semantic incongruence and stereotype irony situations can be useful features in irony detection. Recently, Peled and Reichart (2017) proposed a co"
2020.sltu-1.13,W13-2507,0,0.0263974,"00 Vocabulary 48,259 43,646 66,870 59,333 Training of Bilingual Embeddings 2.1.1. Intrinsic Evaluation Word Translation Task. An important intrinsic evaluation task for learning bilingual embeddings is the word translation task a.k.a. bilingual dictionary induction which assesses how good bilingual embeddings are at detecting word pairs that are semantically similar across languages by checking if translationally equivalent words in different languages are nearby in the embedding space. As our evaluation dictionaries, we use bilingual dictionaries derived from Wiktionary using Wikt2Dict tool (Acs et al., 2013) which has polysemous entries in both directions. We generate Swahili-English, Tagalog-English. Somali-English and German-English dictionaries (the sizes are given in Table 2). We argue that these dictionaries are more reliable as evaluation dictionaries compared to Google Translate dictionaries, which are generally used only for evaluation. We calculate precision at k, where k = 1 and k = 10) (P @1, P @10) for both source-to-target and target-to-source directions and take an average of these scores as the final accuracy. We take the definition of the task from (Ammar et al., 2016). In conjunc"
2020.sltu-1.13,P18-1073,0,0.0770596,"Missing"
2020.sltu-1.13,Q17-1010,0,0.229362,"Missing"
2020.sltu-1.13,D18-1366,0,0.0194386,"and others, 2017) is to use character n-grams. In addition to whole words, several sizes of n-grams, i.e. three to six, are used during training of the skip-gram model. This approach is languageagnostic and can be adapted to new languages easily. Another approach is to have morphological segmentation as a preprocessing step before training the embeddings (Luong et al., 2013). Other techniques predict both the word and its morphological tag (Cotterell and Sch¨utze, 2015) however, all these approaches are monolingual and work on one language at a time. The most closely related work to ours is (Chaudhary et al., 2018) which uses the fastText (Bojanowski and others, 2017) approach to include morphological information when learning cross-lingual embeddings by combining the high-resource and low resource corpora and training using the skip-gram objective. Their evaluation is limited to named-entity-recognition and machine translation and requires detailed linguistically tagged words on a large monolingual corpus for related languages. Our approach incorporates supervision through small amount of parallel corpora while training on subwords for any two languages including unrelated ones. 4.2. Bilingual Embeddin"
2020.sltu-1.13,N15-1140,0,0.031491,"Missing"
2020.sltu-1.13,N19-1423,0,0.0163792,"adapted BiSkip to learn embeddings jointly. This eliminates the need for having pre103 Figure 6: Swahili Analogy Reasoning Task Semantic and Syntactic Categories trained monolingual embeddings and it has been shown to have better accuracy than comparable corpora based approaches (Upadhyay et al., 2016). In addition, our intrinsic evaluations of semi-supervised and unsupervised embeddings did not perform well. Recently, pre-trained contextual embeddings have been extended to other languages, e.g. XLM (Lample and Conneau, 2019), cross-lingual ELMo (Schuster et al., 2019) and multilingual BERT (Devlin et al., 2019) shown to have promising results on a variety of tasks. However, they are not as amenable in low resource scenarios where they tend to overfit. They are also not good at fine-grained linguistic tasks (Liu et al., 2019) and geared toward sentence level tasks. In addition, if a pretrained model is not available, it requires lots of computing power and data to be trained from scratch. For instance, XLM model uses 200K for low resource and 18 million for German. For parallel data, they use 165K for Swahili and 9 million for German. 5. Conclusions and Future Work We present a new cross-lingual embe"
2020.sltu-1.13,N13-1073,0,0.0331528,"by the IARPA MATERIAL program1 . Data statistics for each language i.e. size of parallel corpora, vocabulary and dictionaries, are listed in Table 2. For German, we use the Europarl dataset (Koehn, 2005). Since the size of this parallel dataset is much larger than the others (1,908,920), we select a random subset of 100K parallel sentence to imitate a low-resource scenario. This is important as parallel corpora is more costly to obtain than other bilingual resources, such as dictionaries. For all the models, symmetric word alignments from parallel corpora are learned via the fast align tool (Dyer et al., 2013). For aligning segments of the words, we compute word and stembased alignments and between the two, aligning based on stem performs better across all languages and dimensions. We train embeddings with different dimensions, d = 40 and d = 300, for 20 iterations. Our code for training MultiSeg embeddings, pre-trained cross-lingual embeddings and evaluation scripts such as word translation score and coverage will be publicly available2 . We evaluate our approach both intrinsically and extrinsically on various monolingual and cross-lingual tasks and compare the performance to the BiSkip baseline."
2020.sltu-1.13,W18-5808,1,0.919076,"lingual constraint given by the parallel data. We propose a combined approach that integrates subword information directly when learning bilingual embeddings leveraging the two extensions of the SGNS approach. Our model extends the BiSkip model that uses parallel data by learning representations of subwords and then representing words as the sum of the subword vectors (as was done in the monolingual case for character n-grams (Bojanowski and others, 2017)). As subwords, we consider character ngrams , morphemes obtained using a state-of-the-art unsupervised morphological segmentation approach (Eskander et al., 2018) and byte pair encoding (BPE) (Sennrich et al., 2016). 2. Methodology Our proposed method to learn bilingual embeddings uses both parallel data and information about the internal structure of words in both languages during training. In SGNS, given a sequence of words w1 , ..., wT , the objective is to maximize average log probability where c represents the context: T X X 1/T logp(wc |wt ), (1) t=1 c This probability can be calculated with a softmax function as below: P uT vw e wt c logp(wc |wt ) = PW uT v (2) e wt w where W is the size of the vocabulary, and uwt and vwc are the corresponding w"
2020.sltu-1.13,W19-4222,1,0.805075,"roach both intrinsically and extrinsically on various monolingual and cross-lingual tasks and compare the performance to the BiSkip baseline. Recall, that BiSkip does not use any subword information when training the bilingual embeddings. Figure 2: Alignment algorithm that are computed by merging most frequent adjacent pairs of characters in the corpora. When considering morphemes as subwords, we either split the words into prefix, stem and suffix, or we consider all morphemes, that is the stem and all affixes. We use an unsupervised morphological segmentation approach (Eskander et al., 2018; Eskander et al., 2019) based on Adaptor Grammars that has been shown to produce state-of-the-art results for a variety of morphologically rich languages (e.g., Turkish, Arabic, and 4 UtoAztecan languages which are low resource and polysynthetic). We denote our proposed method using each subword type as MultiSegCN (uses char n-gram as representation during training), MultiSegM (uses prefix, stem, suffix morphemes), MultiSegmorphall (uses all morphemes), MultiSegBP E (uses byte pair encondings), MultiSegAll (uses all subword types as representations during training). Figure 1a shows all possible segmentations for a g"
2020.sltu-1.13,P12-1092,0,0.0605839,"are opposite, superlative, plural nouns and past tense. Word Similarity Task. Word similarity datasets contain word pairs which are assigned similarity ratings by humans. These rankings are then compared with cosine similarity between the word vectors based on the Spearman’s rank correlation coefficient to estimate how well they capture semantic relatedness. In our evaluations, we use three word similarity datasets: WordSimilarity-353 (WS353) (Finkelstein et al., 2001), Stanford Rare Word (RW) similarity dataset (Luong et al., 2013), and Stanford’s Contextual Word Similarities (SCWS) dataset (Huang et al., 2012). 2.1.2. Extrinsic Evaluation As extrinsic evaluation of our embeddings in a downstream semantic task, we use Cross-Language Document Classification (CLDC)3 (Klementiev et al., 2012). In this task, a document classifier is trained using the document representations derived from the cross-lingual embeddings for language l1 , and then the trained model is tested on documents from language l2 . The classifier is trained using the averaged perceptron algorithm and the document vectors are the averaged vector of words in the document weighted by their idf values. For this task, we only have dataset"
2020.sltu-1.13,C12-1089,0,0.104246,"rankings are then compared with cosine similarity between the word vectors based on the Spearman’s rank correlation coefficient to estimate how well they capture semantic relatedness. In our evaluations, we use three word similarity datasets: WordSimilarity-353 (WS353) (Finkelstein et al., 2001), Stanford Rare Word (RW) similarity dataset (Luong et al., 2013), and Stanford’s Contextual Word Similarities (SCWS) dataset (Huang et al., 2012). 2.1.2. Extrinsic Evaluation As extrinsic evaluation of our embeddings in a downstream semantic task, we use Cross-Language Document Classification (CLDC)3 (Klementiev et al., 2012). In this task, a document classifier is trained using the document representations derived from the cross-lingual embeddings for language l1 , and then the trained model is tested on documents from language l2 . The classifier is trained using the averaged perceptron algorithm and the document vectors are the averaged vector of words in the document weighted by their idf values. For this task, we only have dataset for German-English, and we report results where we train on 1, 000 documents and test on 5, 000 to be consistent with the original BiSkip setup. 3. Results The performance on the wo"
2020.sltu-1.13,2005.mtsummit-papers.11,0,0.0700224,"een 3 and 6 as in fastText), or as morphemes, or as byte pair enconding (BPE) 98 2.1. This section describes the data used for training our bilingual word embeddings and our evaluation setup, including the evaluation datasets and measures. We build bilingual embeddings for Swahili-English, Tagalog-English, Somali-English and German-English. For Swahili, Tagalog and Somali, we use parallel corpora provided by the IARPA MATERIAL program1 . Data statistics for each language i.e. size of parallel corpora, vocabulary and dictionaries, are listed in Table 2. For German, we use the Europarl dataset (Koehn, 2005). Since the size of this parallel dataset is much larger than the others (1,908,920), we select a random subset of 100K parallel sentence to imitate a low-resource scenario. This is important as parallel corpora is more costly to obtain than other bilingual resources, such as dictionaries. For all the models, symmetric word alignments from parallel corpora are learned via the fast align tool (Dyer et al., 2013). For aligning segments of the words, we compute word and stembased alignments and between the two, aligning based on stem performs better across all languages and dimensions. We train e"
2020.sltu-1.13,N19-1112,0,0.0231318,"ve better accuracy than comparable corpora based approaches (Upadhyay et al., 2016). In addition, our intrinsic evaluations of semi-supervised and unsupervised embeddings did not perform well. Recently, pre-trained contextual embeddings have been extended to other languages, e.g. XLM (Lample and Conneau, 2019), cross-lingual ELMo (Schuster et al., 2019) and multilingual BERT (Devlin et al., 2019) shown to have promising results on a variety of tasks. However, they are not as amenable in low resource scenarios where they tend to overfit. They are also not good at fine-grained linguistic tasks (Liu et al., 2019) and geared toward sentence level tasks. In addition, if a pretrained model is not available, it requires lots of computing power and data to be trained from scratch. For instance, XLM model uses 200K for low resource and 18 million for German. For parallel data, they use 165K for Swahili and 9 million for German. 5. Conclusions and Future Work We present a new cross-lingual embedding training method for low resource languages, MultiSeg, that incorporates subword information (given as character n-grams, morphemes, or BPEs) during training from parallel corpora. The morphemes are obtained from"
2020.sltu-1.13,W13-3512,0,0.645576,"ults show that our method that leverages subword information outperforms the BiSkip approach, both in intrinsic and extrinsic evaluations of the learned embeddings (Section 3.). Specifically, analogy reasoning results show that using subwords helps capture syntactic characteristics. Qualitative and intrinsic analysis also shows better-quality cross-lingual embeddings particularly for morphological variants. Considering the internal word structure when learning monolingual word embeddings has shown to produce better quality word representations, particularly for morphologically rich languages (Luong et al., 2013; Bojanowski and others, 2017). However, the most popular approaches for learning cross-lingual embeddings have yet to use subword information directly during learning in the cross-lingual space. One of the most widely used approaches for monolingual embeddings (fastText) (Bojanowski and others, 2017) extends the continuous skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013a) to learn subword information given as character n-grams and then representing words as the sum of the n-gram vectors. SGNS has also been used to learn bilingual embeddings using parallel data, the most no"
2020.sltu-1.13,W15-1521,0,0.154211,"most popular approaches for learning cross-lingual embeddings have yet to use subword information directly during learning in the cross-lingual space. One of the most widely used approaches for monolingual embeddings (fastText) (Bojanowski and others, 2017) extends the continuous skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013a) to learn subword information given as character n-grams and then representing words as the sum of the n-gram vectors. SGNS has also been used to learn bilingual embeddings using parallel data, the most notable approach being BiSkip (a.k.a, BiVec) (Luong et al., 2015a). This joint model learns bilingual word representations by exploiting both the context co-occurrence information through the monolingual component and the meaning equivalent signals from the bilingual constraint given by the parallel data. We propose a combined approach that integrates subword information directly when learning bilingual embeddings leveraging the two extensions of the SGNS approach. Our model extends the BiSkip model that uses parallel data by learning representations of subwords and then representing words as the sum of the subword vectors (as was done in the monolingual c"
2020.sltu-1.13,N13-1090,0,0.72612,"hological variants. Considering the internal word structure when learning monolingual word embeddings has shown to produce better quality word representations, particularly for morphologically rich languages (Luong et al., 2013; Bojanowski and others, 2017). However, the most popular approaches for learning cross-lingual embeddings have yet to use subword information directly during learning in the cross-lingual space. One of the most widely used approaches for monolingual embeddings (fastText) (Bojanowski and others, 2017) extends the continuous skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013a) to learn subword information given as character n-grams and then representing words as the sum of the n-gram vectors. SGNS has also been used to learn bilingual embeddings using parallel data, the most notable approach being BiSkip (a.k.a, BiVec) (Luong et al., 2015a). This joint model learns bilingual word representations by exploiting both the context co-occurrence information through the monolingual component and the meaning equivalent signals from the bilingual constraint given by the parallel data. We propose a combined approach that integrates subword information directly when learnin"
2020.sltu-1.13,N19-1162,0,0.0284943,"and others, 2018). Among these techniques, we adapted BiSkip to learn embeddings jointly. This eliminates the need for having pre103 Figure 6: Swahili Analogy Reasoning Task Semantic and Syntactic Categories trained monolingual embeddings and it has been shown to have better accuracy than comparable corpora based approaches (Upadhyay et al., 2016). In addition, our intrinsic evaluations of semi-supervised and unsupervised embeddings did not perform well. Recently, pre-trained contextual embeddings have been extended to other languages, e.g. XLM (Lample and Conneau, 2019), cross-lingual ELMo (Schuster et al., 2019) and multilingual BERT (Devlin et al., 2019) shown to have promising results on a variety of tasks. However, they are not as amenable in low resource scenarios where they tend to overfit. They are also not good at fine-grained linguistic tasks (Liu et al., 2019) and geared toward sentence level tasks. In addition, if a pretrained model is not available, it requires lots of computing power and data to be trained from scratch. For instance, XLM model uses 200K for low resource and 18 million for German. For parallel data, they use 165K for Swahili and 9 million for German. 5. Conclusions and Fut"
2020.sltu-1.13,P16-1162,0,0.0196931,"pose a combined approach that integrates subword information directly when learning bilingual embeddings leveraging the two extensions of the SGNS approach. Our model extends the BiSkip model that uses parallel data by learning representations of subwords and then representing words as the sum of the subword vectors (as was done in the monolingual case for character n-grams (Bojanowski and others, 2017)). As subwords, we consider character ngrams , morphemes obtained using a state-of-the-art unsupervised morphological segmentation approach (Eskander et al., 2018) and byte pair encoding (BPE) (Sennrich et al., 2016). 2. Methodology Our proposed method to learn bilingual embeddings uses both parallel data and information about the internal structure of words in both languages during training. In SGNS, given a sequence of words w1 , ..., wT , the objective is to maximize average log probability where c represents the context: T X X 1/T logp(wc |wt ), (1) t=1 c This probability can be calculated with a softmax function as below: P uT vw e wt c logp(wc |wt ) = PW uT v (2) e wt w where W is the size of the vocabulary, and uwt and vwc are the corresponding word vector representations for wc and wt in R . BiSki"
2020.sltu-1.13,P16-1157,0,0.0394039,"Missing"
2020.sltu-1.13,P15-2118,0,0.032654,"Missing"
2021.acl-long.165,W18-5513,1,0.675764,"c or scientific lexical relationships. For instance in (C2, EV2) we see that both bolded phrases in red and blue refer to the same phenomena, but immune dysregulation is “a breakdown of immune system processes” and restraining it can be seen as the same concept as correcting immune abormalities, but the model is not able to capture such complex domain specific knowledge. 5 Related Work Fact-Checking. Approaches for predicting the veracity of naturally-occurring claims have focused 2123 on statements fact-checked by journalists or organizations such as PolitiFact.org (Vlachos and Riedel, 2014; Alhindi et al., 2018), news articles (Pomerleau and Rao, 2017), or answers in community forums (Mihaylova et al., 2018, 2019). Mixeddomain large scale datasets such as UKP Snopes (Hanselowski et al., 2019), MultiFC (Augenstein et al., 2019), and FEVER (Thorne et al., 2018, 2019) rely on Wikipedia and fact-checking websites to obtain evidences for their claims. Even though these datasets contain many claims, due do domain mismatch they may be difficult to apply for COVID-19 related misinformation detection. SciFact (Wadden et al., 2020) introduced the task of scientific fact-checking, generating a dataset of 1.4K s"
2021.acl-long.165,J08-4004,0,0.0917943,"Missing"
2021.acl-long.165,D19-1475,0,0.0457154,"Missing"
2021.acl-long.165,D17-1218,0,0.0215217,"ng models targeting even more specific types of disinformation, like disinformation about antivirals or PPE/masks. In our study, the titles of the post are considered candidate claims and the associated sources are considered evidence documents. Posts from the r/COVID19 subreddit are extracted via the Pushshift Reddit API.1 Two issues still need to be addressed: 1) ensure that titles are well-formed claims; 2) ensure the highest trustworthiness of the posts and their associated sources. Filtering for well-formed claims. The definition of a claim can vary depending on domain, register or task (Daxenberger et al., 2017). For our work, we consider a claim to be a proposition whose truthfulness can only be determined by additional evidence. In addition, a well-formed claim has to be a full sentence. Thus, to filter out most of the titles that are not well-formed claims, we employ a simple syntax-based approach to remove questions and consider statements that have at least a main verb. This filtering steps allows us to remove titles such as ”B cell memory: understanding COVID-19” and consider titles such as the ones in Figure 1 and Table 1. In addition, we ask three volunteer computer science students with back"
2021.acl-long.165,N19-1423,0,0.277151,"h is geared only towards scientific claims. • Automatic generation of counter-claims (Section 2.2). An end-to-end fact-checking system requires both true and false claims for training. Following FEVER and SciFact, to obtain false claims, we aim to generate counter-claims of the original true claim. The advantage is that we obtain evidence documents/sentences for free. However, unlike FEVER and SciFact, we propose a novel approach to automatically generate counter-claims from a given claim using two steps: 1) select salient words from the true claim using attention scores obtained from a BERT (Devlin et al., 2019) model fine-tuned on the SciFact dataset, and 2) replace those words with their opposites using Masked Language Model infilling • Evidence sentence selection using text similarity and crowdsourcing (Section 2.3). For evidence sentence selection, we calculate the semantic similarity between the original true claim and the sentences in source evidence documents using sentence-BERT (SBERT) (Reimers and Gurevych, 2019), retrieve top five sentences and use crowdsourcing for final validation. Table 1 shows examples of evidence sentences that support the true claims and refute the corresponding count"
2021.acl-long.165,2020.acl-main.225,0,0.0354665,"Missing"
2021.acl-long.165,K19-1046,0,0.0806209,"at are propagated online in news articles or social media. Ideally, a fact-checking pipeline will address several tasks: 1) Consider real-world claims, 2) Retrieve relevant documents not bounded to a known document collection (e.g., Wikipedia) and which contain information to validate the claim, 3) Select evidence sentences that can support or refute the claim and 4) Predict the claim veracity based on this evidence. Recent work on end-to-end factchecking, including models and datasets, has advanced the field by addressing several tasks in the pipeline, but not all (Thorne et al., 2018, 2019; Hanselowski et al., 2019; Augenstein et al., 2019; Diggelmann et al., 2021; Wadden et al., 2020). One line of work that includes FEVER (Thorne et al., 2018, 2019) and SciFact (Wadden et al., 2020) addresses tasks 2, 3 and 4, but assumes a given document collection for task 2 (Wikipedia or CORD-19, respectively) and does not address task 1. Moreover, the refuted claims in these datasets are manually generated by asking humans to produce counter-claims for a given claim supported by a source document. Another line of work that includes Multi-FC (Augenstein et al., 2019) addresses tasks 1, 2 and 4, but not 3. It provide"
2021.acl-long.165,2020.acl-main.761,1,0.844643,"n .. ..improves the effect .. ..blocks SARS-CoV-2.. ..inhibits the effect .. ..enchanced SARS-CoV-2.. ..are not fit for purpose .. .. the final stage .. .. shows positive results. ..are good fit for .. .. the first stage .. .. shows no results. Table 2: A detailed look into what parts of speech are replaced, and in what direction the claims are reversed. We omitted full claims due to space constraint. The first 3 claims show nouns, the next 2 show verbs and the final 3 show adjective modifications. 2.3 Evidence Sentence Selection To select evidence sentences we follow the approach proposed by Hidey et al. (2020). Given the true claims and the 5 evidence documents for each claim (Section 2.1) we use cosine similarity on SBERT sentence embeddings (Reimers and Gurevych, 2019) to extract the top 5 sentences most similar to the true claim. Note that we only need to do this step for true claims, as automatically the evidence sentences that support the true claim will be the evidence sentences that refute the corresponding counter-claims. Sentences containing the claim itself were discarded. The collected five sentences will serve as candidate evidence sentences for future human validation described below."
2021.acl-long.165,2020.nlpcovid19-2.11,0,0.0348333,"cking. However, due to difficult and expensive methods employed for generation of FEVER, it can be difficult to extrapolate this method to assemble a COVID-19 specific dataset. COVID-19 related NLP tasks. Numerous NLP approaches were employed to aid the battle with the COVID-19 pandemic. Notably Wang et al. (2020) released CORD-19, a dataset containing 140K papers about COVID-19 and related topics while Zhang et al. (2020a) created a neural search engine COVIDEX for information retrieval. To combat misinformation Lee et al. (2020) proposed a hypothesis that misinformation has high perplexity. Hossain et al. (2020) released COVIDLIES: a dataset of 6761 expert-annotated tweets matched with their stance on known COVID-19 misconceptions. The dataset provides a comprehensive evaluation of misconception retrieval but does not analyze evidence retrieval and prediction of veracity of claims based on presented evidence. Poliak et al. (2020) collected 24,000 Question with expert answers from 40 trusted websites to help NLP research with COVID related information. COVID-Fact, on the other hand, deals with real world claims and presents an end-to-end fact checking system to fight misinformation. 6 Conclusion We re"
2021.acl-long.165,2020.findings-emnlp.309,0,0.0833975,"Missing"
2021.acl-long.165,2020.acl-main.703,0,0.0188017,"Missing"
2021.acl-long.165,2021.ccl-1.108,0,0.0730948,"Missing"
2021.acl-long.165,S19-2149,0,0.0447248,"Missing"
2021.acl-long.165,N19-4009,0,0.0325574,"Missing"
2021.acl-long.165,2020.nlpcovid19-2.31,0,0.0380407,"CORD-19, a dataset containing 140K papers about COVID-19 and related topics while Zhang et al. (2020a) created a neural search engine COVIDEX for information retrieval. To combat misinformation Lee et al. (2020) proposed a hypothesis that misinformation has high perplexity. Hossain et al. (2020) released COVIDLIES: a dataset of 6761 expert-annotated tweets matched with their stance on known COVID-19 misconceptions. The dataset provides a comprehensive evaluation of misconception retrieval but does not analyze evidence retrieval and prediction of veracity of claims based on presented evidence. Poliak et al. (2020) collected 24,000 Question with expert answers from 40 trusted websites to help NLP research with COVID related information. COVID-Fact, on the other hand, deals with real world claims and presents an end-to-end fact checking system to fight misinformation. 6 Conclusion We release a dataset of 4,086 claims concerning the COVID-19 pandemic, together with supporting and refuting evidence. The dataset contains real-world true claims obtained from the r/COVID19 subreddit as well as automatically generated counterclaims. Our experiments reveal that our dataset outperforms zero-shot baselines traine"
2021.acl-long.165,D19-1410,0,0.306527,"opose a novel approach to automatically generate counter-claims from a given claim using two steps: 1) select salient words from the true claim using attention scores obtained from a BERT (Devlin et al., 2019) model fine-tuned on the SciFact dataset, and 2) replace those words with their opposites using Masked Language Model infilling • Evidence sentence selection using text similarity and crowdsourcing (Section 2.3). For evidence sentence selection, we calculate the semantic similarity between the original true claim and the sentences in source evidence documents using sentence-BERT (SBERT) (Reimers and Gurevych, 2019), retrieve top five sentences and use crowdsourcing for final validation. Table 1 shows examples of evidence sentences that support the true claims and refute the corresponding counter-claims. • COVID-Fact dataset of 4,086 real-world claims annotated with sentence-level evidence and a baseline on this task. Our results show that models trained on current datasets (FEVER, SciFact) do not perform well on our data (Section 4). Moreover, we show the usefulness of our dataset through zero-shot performance on the scientific claim verification task on SciFact (Wadden et al., 2020) data (Section 4). 2"
2021.acl-long.165,D19-1322,0,0.0602655,"Missing"
2021.acl-long.165,N18-1074,0,0.335618,"g the veracity of claims that are propagated online in news articles or social media. Ideally, a fact-checking pipeline will address several tasks: 1) Consider real-world claims, 2) Retrieve relevant documents not bounded to a known document collection (e.g., Wikipedia) and which contain information to validate the claim, 3) Select evidence sentences that can support or refute the claim and 4) Predict the claim veracity based on this evidence. Recent work on end-to-end factchecking, including models and datasets, has advanced the field by addressing several tasks in the pipeline, but not all (Thorne et al., 2018, 2019; Hanselowski et al., 2019; Augenstein et al., 2019; Diggelmann et al., 2021; Wadden et al., 2020). One line of work that includes FEVER (Thorne et al., 2018, 2019) and SciFact (Wadden et al., 2020) addresses tasks 2, 3 and 4, but assumes a given document collection for task 2 (Wikipedia or CORD-19, respectively) and does not address task 1. Moreover, the refuted claims in these datasets are manually generated by asking humans to produce counter-claims for a given claim supported by a source document. Another line of work that includes Multi-FC (Augenstein et al., 2019) addresses tasks 1"
2021.acl-long.165,D19-6601,0,0.0242183,"h gold evidence, as well as Top-5 and Top-1 evidences ranked by SBERT cosine similarity with the original claim. 3.3 Experiments Besides evaluating our baseline pipeline on the COVID-Fact dataset, we perform several additional experiments outlined below. All hyperparameters can be found in Appendix A. Adequacy of Existing Datasets for COVID-Fact. For the task of veracity prediction, we evaluate 4 Results and Analysis Table 5 summarizes the results for the evidence retrieval evaluation. Our pipeline provides a strong baseline with F1 score of ≈ 32. For comparison, the baseline system in FEVER (Thorne et al., 2019) achieves the F1 score of 18.26. Note Top 5 evidence retrieval performs worse than gold since we evaluate how the system performs with automatically negated claims as well, for which we re-run the Google+SBERT method. Table 4 summarizes the results for the veracity prediction task using gold and retrieved evidence. We observe that, given the gold evidences, fine-tuning on COVID-Fact led to performance improvement of 25 F1-score and 35 F1-score compared to training solely on SciFact and FEVER respectively. This indicates that the COVID-Fact dataset is challenging and cannot be solved using popu"
2021.acl-long.165,W14-2508,0,0.0146489,"nowledge of domain-specific or scientific lexical relationships. For instance in (C2, EV2) we see that both bolded phrases in red and blue refer to the same phenomena, but immune dysregulation is “a breakdown of immune system processes” and restraining it can be seen as the same concept as correcting immune abormalities, but the model is not able to capture such complex domain specific knowledge. 5 Related Work Fact-Checking. Approaches for predicting the veracity of naturally-occurring claims have focused 2123 on statements fact-checked by journalists or organizations such as PolitiFact.org (Vlachos and Riedel, 2014; Alhindi et al., 2018), news articles (Pomerleau and Rao, 2017), or answers in community forums (Mihaylova et al., 2018, 2019). Mixeddomain large scale datasets such as UKP Snopes (Hanselowski et al., 2019), MultiFC (Augenstein et al., 2019), and FEVER (Thorne et al., 2018, 2019) rely on Wikipedia and fact-checking websites to obtain evidences for their claims. Even though these datasets contain many claims, due do domain mismatch they may be difficult to apply for COVID-19 related misinformation detection. SciFact (Wadden et al., 2020) introduced the task of scientific fact-checking, generat"
2021.acl-long.165,2020.emnlp-main.609,0,0.0351519,"Missing"
2021.acl-long.165,W18-5446,0,0.0723084,"Missing"
2021.acl-long.165,2020.nlpcovid19-acl.1,0,0.221284,"ention-based salient word selection. 2119 2.2.2 Masked Language Model Infilling with Entailment-based Quality Control After selecting salient words from the true claims for replacement, we need to provide only paraphrases that are opposite in meaning and consider the context in which these words occur. Language models have been used previously for infilling tasks (Donahue et al., 2020) and have also been used for automatic claim mutation in fact checking (Jiang et al., 2020). Inspired by these approaches, we use the Masked Language Model (MLM) RoBERTa (Liu et al., 2019) fine-tuned on CORD-19 (Wang et al., 2020) for infilling. The fine-tuned RoBERTa is available on Huggingface 3 . We generate a large number (10-30) of candidate counter-claims with replaced keywords per each original claim. After generating multiple candidate counterclaims based on MLM infilling, we select the ones that have the highest contradiction score with the original claim. To compute the contradiction score we use the RoBERTa (Liu et al., 2019) model trained on Multi-NLI (Williams et al., 2018) due to its size and diversity. The scores are in the range from 0 to 1. We first set the minimum score threshold and then select top t"
2021.acl-long.165,N18-1101,0,0.0807862,"Missing"
2021.acl-long.165,2020.nlpcovid19-acl.2,0,0.0759618,"examples). Our counter-claim generation consists of two stages: 1) select salient words from the true claims, and 2) replace those words with their opposite using Mask Language Model infilling with entailment-based quality control. We discuss these steps below. 2.2.1 Salient Words Selection Salient words (keywords) are essential to the overall semantics of a sentence. For example, in the claim ”Oxford vaccine triggers immune response”, a salient word would be “triggers”. By changing the word ”triggers” to ”inhibits” we change the meaning of above claim to its opposite (counterclaim). Recently Zhang et al. (2020b) used YAKE (Campos et al., 2018, 2020), an unsupervised automatic keyword extraction method for selecting salient words to guide their text generation process. For selecting salient words from a claim, we experiment with YAKE as one of our methods. In addition, we explore an attention-based method described below. Attention-Based Salience. Recently, Sudhakar et al. (2019) use self-attention scores from BERT (Devlin et al., 2019) to delete keywords from an input sequence for the task of Style Transfer. They use a novel method to extract a specific attention head and layer combination that enc"
2021.acl-long.165,2020.emnlp-main.698,0,0.0201467,"Missing"
2021.acl-long.524,W19-0606,0,0.0127048,"roxy for a conceptual metaphoric mapping. 6725 We first train FrameNet frame embeddings and employ evaluation metrics to ensure their quality. We then apply transformations between domains to literal verbs to generate metaphors grounded in conceptual metaphor theory. 3.1.1 Figure 2: Lexical generation process Learning Frame Embeddings In order to exploit FrameNet frames as conceptual domains, we will embed them in vector space. While lexical and contextualized embeddings have proven effective, the field of embedding concepts from lexical resources is less well explored (Sikos and Pad´o, 2018; Alhoshan et al., 2019). These methods involve tagging raw corpora using automatic FrameNet parsing and then inputting some combination of the original text and the FrameNet information into standard embedding algorithms. To train and evaluate frame embeddings, we use 211k sentences of Gold annotations used to train the Open-SESAME parser (Swayamdipta et al., 2017), along with a variety of other automatically tagged datasets: 250k individual sentence from the Gutenberg Poetry Corpus (Jacobs, 2018), 17k from various fiction section of the Brown Corpus (Francis and Kucera, 1979), and 80k sentences randomly selected fr"
2021.acl-long.524,P98-1013,0,0.356034,"ith metaphoric counterparts. This can be done by employing vector spaces, identifying the word most likely to fit in an appropriate context and subjecting them to some constraints of metaphoricity. We build on this paradigm by incorporating facets of conceptual metaphor theory. Our procedure is as follows: we learn a joint embedded representations for domains and lexical items. We then use the linear transformation between two domains as a mapping, which can be applied to input words from the target domain to generate a word from the source domain. As a proxy for domains, we utilize FrameNet (Baker et al., 1998), which contains semantic frames along with the set of lexical units that evoke them. Frames can be defined as related systems of concepts (Fillmore, 1982), which is exchangeable with the term “domain” used in conceptual metaphor theory (Cruse and Croft, 2004). Thus, we consider the transformation from one frame to another as a proxy for a conceptual metaphoric mapping. 6725 We first train FrameNet frame embeddings and employ evaluation metrics to ensure their quality. We then apply transformations between domains to literal verbs to generate metaphors grounded in conceptual metaphor theory. 3"
2021.acl-long.524,Q17-1010,0,0.0358182,"a hierarchy of connected frames. Starting with the assumption that frames connected in the structure should be more similar, we also calculate a structural similarity metric str. We follow the same process as above, taking the distance between the mean embedding of the local frames n ∈ N , where N is the immediate neighbors of f , to the mean embedding of a sample k of distant frames n ∈ / N. str(f ) = P cos(En ,Ef ) n∈N |N | k cos(E ,E ) P n f − n6∈N k We experiment with three lexical embeddings models: word2vec skip-gram (Mikolov et al., 2013), Glove (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). We experiment with 50, 100, and 300 dimensional representations; we find the 50 dimensional word2vec embeddings perform best for both evaluation metrics.4 3.1.2 Embedding Mappings To apply these embeddings to generate metaphors based on conceptual mappings, we learn mappings between frames and apply the mappings directly to lexical items to facilitate lexical replacement. We define a mapping m as the pointwise distance between the target frame embedding and the source frame embedding. Following the approach for learning connections between concrete and poetic themes of Gagliano et al. (2016)"
2021.acl-long.524,P19-1470,0,0.0569006,"horic verbs, and replacing them with infilling from a language model. We use a BERT-based metaphor classification model trained on the VUA metaphor corpus (Steen et al., 2010) to identify metaphoric verbs in a sentence (i.e “died” in The house where love had died). Then we convert it to a literal sentence (The house where love had ended) using infillings from pre-trained BERT (Devlin et al., 2019). To ensure the literal sentence with replacements convey the same semantic meaning as the metaphorical sentence they are then filtered using symbolic meaning (SymbolOf relation) obtained from COMET (Bosselut et al., 2019), a GPT based language model fine-tuned on ConceptNet (Speer et al., 2017). COMET returns top 5 symbolic beams of (loss, loneliness, despair, sadness and sorrow) for the sentence “The house where love had died” whereas it replaces sorrow with life for the literal version. While Chakrabarty et al. (2021) filter down to only those candidates with an exact match between the top 5 symbolic beams for the literal and metaphorical sentences returned by the COMET model, we ease the restriction to cases where at least four of five symbols are the same. In order to learn more direct metaphoric informati"
2021.acl-long.524,W15-1405,0,0.022996,"ctional preferences of verbs. Shaikh et al. (2014a) builds ”conceptual spaces” for source domains, using rule-based extraction of relations between lexical items. These conceptual spaces are then used to find new conceptual metaphors. This process is extended to build a repository of linguistic and conceptual metaphors (Shaikh et al., 2014b). Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metaphoric domains has been explored in the MetaNet project (Dodge et al., 2015). We however, restrict our work to FrameNet due to the coverage and availability of reliable automatic parsing. Metaphor Generation. Early work in metaphor generation was based in heuristics, learning to generate relatively simple ”A is like B” representations (Abe et al., 2006; Terai and Nakagawa, 2010). In a similar vein, Veale (2016) uses template-like structures to generate creative and metaphoric tweets. Other works focus on identifying metaphoric mappings using WordNet clustering and selectional preferences (Mason, 2004; Gandy et al., 2013), syntactic relations to build proposition datab"
2021.acl-long.524,P18-1082,0,0.0223628,"she left. where hEOT i and hV i are delimiters, DEATH is the source frame, and CAUSE TO END the target frame. The decoding target is the metaphoric text “The party died as soon as she left”, which evokes the CAUSE TO END IS DEATH mapping. Note that our training data differs only at the level of a single verb. We use the generative BART seq2seq model to generate metaphoric paraphrases, 6727 but due to the nature of the training data and the importance of verbs in metaphoric expressions, this is often realized in the output as lexical replacement. Post fine-tuning, we use top-k (k=5) sampling (Fan et al., 2018) to generate metaphors conditioned on the input literal sentence and source and target domains for the required metaphoric mapping.5 We evaluate the lexical model (CM-Lex) and the sequence-to-sequence model (CM-BART) under two experimental settings. 4 Experimental Setup We evaluate our metaphor generation methods against two previous approaches to metaphoric paraphrase generation: the MERMAID system (Chakrabarty et al., 2021) and the metaphor masking model (MetMask) (Stowe et al., 2020). We explore two tasks: generating against gold standard metaphoric expressions, and using rare and unseen me"
2021.acl-long.524,W16-0203,0,0.138294,"janowski et al., 2017). We experiment with 50, 100, and 300 dimensional representations; we find the 50 dimensional word2vec embeddings perform best for both evaluation metrics.4 3.1.2 Embedding Mappings To apply these embeddings to generate metaphors based on conceptual mappings, we learn mappings between frames and apply the mappings directly to lexical items to facilitate lexical replacement. We define a mapping m as the pointwise distance between the target frame embedding and the source frame embedding. Following the approach for learning connections between concrete and poetic themes of Gagliano et al. (2016), we sum the embedding of the target verb and the mapping m for the selected conceptual mapping, and select the most similar word to the resulting vector. This word is then delemmatized using fitbert (Havens and Stal, 2019) and inserted into the original sentence (Figure 2). Note that these resulting words are generated without context, as they rely only on the input word and the conceptual mappings. This approach has benefits: we require no labeled metaphor data, using only embeddings trained on FrameNet-tagged corpora. However, ignoring context is likely detrimental. In order to better use c"
2021.acl-long.524,2020.acl-main.703,0,0.22681,"6726 4 For full frame embedding evaluation, see Appendix A. Literal (filled from LM) That tyranny is destroyed The house where love had ended As the moments passed on What I learned my senses fraught Target Frame DESTRUCTION CAUSE TO END PROCESS END COMING TO BELIEVE Metaphoric (original) That tyranny is slain The house where love had died As the moments roll on What I bear my senses fraught Source Frame KILLING DEATH CAUSE MOTION BRINGING Table 1: Sample of extracted pairs from the data collection process. 3.2 CM-BART For sequence-to-sequence learning, we fine-tune a pre-trained BART model (Lewis et al., 2020), adding source and target information to guide generation towards the intended metaphors. We first outline a procedure for generating semi-supervised paired data, then detail the training and generation process. 3.2.1 Method for Creating Parallel Data In order to train sequence-to-sequence models for metaphor generation, we require large scale parallel corpora. We follow the approach of Chakrabarty et al. (2021) and build a corpus of literal/metaphoric paraphrases by starting with the Gutenberg Poetry corpus (Jacobs, 2018), identifying and masking metaphoric verbs, and replacing them with inf"
2021.acl-long.524,P18-1113,0,0.0555435,"Missing"
2021.acl-long.524,J04-1002,0,0.189424,"taphoricity, provide a strong signal for which domains to generate in. This highlights a possible benefit to the interaction between deep, pre-trained models such as BART and available lexical resources: by combining these, we are able to leverage the strength of each to build a powerful metaphor generation system. 6 Related Work We broadly cover two areas of related work: previous computational approaches to CMT, and previous approaches to metaphor generation. Computational Approaches to CMT. There are a variety of approaches to identifying conceptual metaphors themselves. The CorMet system (Mason, 2004) was built to extract conceptual metaphors based on selectional preferences of verbs. Shaikh et al. (2014a) builds ”conceptual spaces” for source domains, using rule-based extraction of relations between lexical items. These conceptual spaces are then used to find new conceptual metaphors. This process is extended to build a repository of linguistic and conceptual metaphors (Shaikh et al., 2014b). Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metap"
2021.acl-long.524,S16-2003,0,0.180733,"ings. For the former, we build a gold test set of metaphoric paraphrases that evoke a particular source/target mapping. For the latter, we apply a variety of source/target mappings to literal inputs for which we do not have gold outputs. 4.1 Building a Test Set For a test set, we use the same procedure as our data collection approach from Section 3.2.1. We apply this procedure to two datasets: a sample of the Gutenberg Poetry Corpus and a sample of fiction from the Brown Corpus (Francis and Kucera, 1979). This generates an initial set of literal/metaphoric pairs. We also tagged the pairs from Mohammad et al. (2016) with FrameNet tags, as these generally contain novel, well-formed metaphors. These three datasets each have different properties with regard to metaphor. The Gutenberg Poetry corpus has consistent, novel metaphors, but often unconventional syntactic constructions, due to the poetic nature of the text. The Mohammad 2016 corpus contains manually constructed metaphors which are novel, following relatively basic syntactic patterns. The Brown Corpus is standard fiction texts, so the metaphors within tend to be very conventional. From these sources, we draw pairs randomly, checking that they reflec"
2021.acl-long.524,C14-1165,0,0.0248913,"CMT, and previous approaches to metaphor generation. Computational Approaches to CMT. There are a variety of approaches to identifying conceptual metaphors themselves. The CorMet system (Mason, 2004) was built to extract conceptual metaphors based on selectional preferences of verbs. Shaikh et al. (2014a) builds ”conceptual spaces” for source domains, using rule-based extraction of relations between lexical items. These conceptual spaces are then used to find new conceptual metaphors. This process is extended to build a repository of linguistic and conceptual metaphors (Shaikh et al., 2014b). Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metaphoric domains has been explored in the MetaNet project (Dodge et al., 2015). We however, restrict our work to FrameNet due to the coverage and availability of reliable automatic parsing. Metaphor Generation. Early work in metaphor generation was based in heuristics, learning to generate relatively simple ”A is like B” representations (Abe et al., 2006; Terai and Nakagawa, 2010). In a similar vein, Veale (2016) uses te"
2021.acl-long.524,N19-4009,0,0.042058,"Missing"
2021.acl-long.524,D14-1162,0,0.0858506,"en frames (eg. used-by, uses), yielding a hierarchy of connected frames. Starting with the assumption that frames connected in the structure should be more similar, we also calculate a structural similarity metric str. We follow the same process as above, taking the distance between the mean embedding of the local frames n ∈ N , where N is the immediate neighbors of f , to the mean embedding of a sample k of distant frames n ∈ / N. str(f ) = P cos(En ,Ef ) n∈N |N | k cos(E ,E ) P n f − n6∈N k We experiment with three lexical embeddings models: word2vec skip-gram (Mikolov et al., 2013), Glove (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). We experiment with 50, 100, and 300 dimensional representations; we find the 50 dimensional word2vec embeddings perform best for both evaluation metrics.4 3.1.2 Embedding Mappings To apply these embeddings to generate metaphors based on conceptual mappings, we learn mappings between frames and apply the mappings directly to lexical items to facilitate lexical replacement. We define a mapping m as the pointwise distance between the target frame embedding and the source frame embedding. Following the approach for learning connections between concrete and"
2021.acl-long.524,D19-1410,1,0.846699,"Missing"
2021.acl-long.524,W14-4725,0,0.0741936,"Missing"
2021.acl-long.524,W18-3813,0,0.0520256,"Missing"
2021.acl-long.524,W16-1105,0,0.0462399,"Mohler et al. (2014) focus on identifying appropriate source domains for metaphoric expressions, using vector-based approaches for metaphor interpretation. The idea of using frames to represent metaphoric domains has been explored in the MetaNet project (Dodge et al., 2015). We however, restrict our work to FrameNet due to the coverage and availability of reliable automatic parsing. Metaphor Generation. Early work in metaphor generation was based in heuristics, learning to generate relatively simple ”A is like B” representations (Abe et al., 2006; Terai and Nakagawa, 2010). In a similar vein, Veale (2016) uses template-like structures to generate creative and metaphoric tweets. Other works focus on identifying metaphoric mappings using WordNet clustering and selectional preferences (Mason, 2004; Gandy et al., 2013), syntactic relations to build proposition databases (Ovchinnikova et al., 2014), and embedding based approaches to identify poetic relationships (Gagliano et al., 2016). However, the goal of these works is to generate mappings, rather than linguistic expressions that evoke them. Amongst deep learning approaches Yu and Wan (2019) identify literal and metaphoric words in corpora based"
2021.acl-long.524,D19-1221,0,0.0577942,"f-the-art performance in metaphor generation by both automatic and human evaluations. Future work can expand these models to go beyond verbs, incorporating nominal and other types of metaphors. The next necessary step is to go beyond lexicalized metaphors: good, consistent conceptual metaphors often span long stretches of text, and we need to design models that can learn and generate metaphors over larger texts. Ethical Considerations Although we use language models trained on data collected from the Web, which have been shown to have issues with bias and abusive language (Sheng et al., 2019; Wallace et al., 2019), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, BART is a conditional language model, which provides more control of the generated output. It should also be noted that our CM-BART model is fine-tuned on the poetry corpus which is devoid of harmful and toxic text especially targeted at marginalized communities Advances in generative AI inherently come with concerns about models’ ability to deceive, persuade, and misinform. Metaphorical language has been shown to express and elicit stronger emotion than literal language (Citron and"
2021.acl-long.524,N19-1092,0,0.214231,"domain from which we draw the metaphorical expressions, while the target domain is the conceptual domain that we try to understand. A classical mapping is ARGUMENT IS WAR, in which we conceptualize the target argumentation domain as the more concrete source domain of war: Introduction Recent neural models have led to important progress in natural language generation (NLG) tasks. While pre-trained models have facilitated advances in many areas of generation, the field of metaphor generation remains relatively unexplored. Moreover, the few existing deep learning models for metaphor generation (Yu and Wan, 2019; Stowe et al., 2020; Chakrabarty et al., 2020) lack any conceptualization of the meaning of the metaphors. This work proposes the first step towards metaphor generation informed by the conceptual metaphor theory (CMT) (Lakoff and Johnson, 1980; Lakoff, 1993; Reddy, 1979). CMT holds • They fought against the contract. • They defended their new proposal. We focus on verbs, as they are often the key component of metaphoric expressions (Steen et al., 2010; Martin, 2006). When used metaphorically, verbs typically evoke source domains (e.g. fought, defended in the above examples): they are concrete"
2021.acl-short.133,D18-1217,0,0.0255797,"Missing"
2021.acl-short.133,N19-1423,0,0.00507731,"et. The sizes for the train/valid/test sets are 746, 173, and 186 respectively. Data Pre-processing Following the advice in (Zirikly et al., 2019), we replace all human names and URLs in the Reddit posts with special tokens ” PERSON ” and ” URL ”, respectively. We also remove punctuation and stop words besides lowercasing. Due to the limitation of GPU memory, we split those large posts to be passages with no more than 128 words1 and make sure that the split point is not in the middle of the sentence2 . Such passages are treated as separate posts. Model Architecture Our architecture is a BERT (Devlin et al., 2019) model. We also experimented with other state-of-the-art pre-trained language models (PLMs), including RoBERTa (Liu et al., 2019) and XLNET (Yang et al., 2019), but found BERT to work the best and thus consider it as our baseline architecture (more details can be found in Appendix A). Each post xi,k is fed into BERT (Devlin et al., 2019) and we get post embedding ~ei,k = BERT(xi,k ). Then we do simple mean-pooling to obtain the user embedding Pn(i) ~e i,k ~ui = k=1 . Finally, we feed ~ui to a fullyn(i) connected layer and use the Softmax layer to predict the risk level probability P˜ (yi |Ci )"
2021.acl-short.133,2020.acl-main.740,0,0.0519044,"Missing"
2021.eacl-main.171,P12-2041,0,0.0399648,"and Liu, 2004) to identify sentiment in the turns; (d) hedge features, since they are often used to mitigate speaker’s commitment (Tan et al., 2016); (e) PDTB discourse markers because claims often start with discourse markers such as therefore, so. We discard markers from the temporal relation; (f) modal verbs because they signal the degree of certainty when expressing a claim (Stab and Gurevych, 2014); (g) pronouns, since they dialogically point to the previous speaker’s stance; (h) textual entailment: captures whether a position expressed in the prior turn is accepted in the current turn (Cabrio and Villata, 2012; Menini and Tonelli, 2016)3 ; (i) lemma overlap to determine topical alignment between the prior and current turn (Somasundaran and Wiebe, 2010). We compute lemma overlap of noun, verbs, and adjectives between the turns, and (j) negation to extract explicit negation cues (e.g., “not”, “don’t”) that often signal disagreement. Sarcasm-relevant features (SarcF ). As sarcasm-relevant features we use: (a) Linguistic Inquiry Word Count (LIWC) (Pennebaker et al., 2001) features to capture the linguistic, social, individual, and psychological processes; (b) measuring sentiment incongruity, that is, c"
2021.eacl-main.171,D19-1291,1,0.852342,"gumentative relations. Our argumentative features in the feature-based model are based on the above works (Section 4.1). We show that additional features that are useful in sarcasm detection (Joshi et al., 2015; Ghosh and Muresan, 2018) enhance the performance on the argumentative relation identification and classification tasks. In addition to feature-based models, deep learning models have been recently used for these tasks. Potash et al. (2017) proposed a pointer network, and Hou and Jochim (2017) offered LSTM+Attention network to predict argument components and relations jointly, whereas (Chakrabarty et al., 2019) exploited adaptive pretraining (Gururangan et al., 2020) for BERT to identify argument relations. We use two multitask learning objectives (argumentative relation identification/classification and sarcasm detection), as our goal is to investigate whether identifying sarcasm can help in modeling the disagreement space. Majumder et al. (2019); Chauhan et al. (2020) used multitask learning for sarcasm & sentiment and sarcasm, sentiment, & emotion, respectively, where a direct link between the corresponding tasks is 1999 evident. Finally, analyzing the role of sarcasm and verbal irony in argument"
2021.eacl-main.171,D17-1050,0,0.0267024,"Missing"
2021.eacl-main.171,J18-4009,1,0.846202,"the individual model and LRArgF +SarcF (i.e., model that uses both ArgF and SarcF features) is the joint model. 4.2 Dual LSTM and Multitask Learning LSTMs are able to learn long-term dependencies (Hochreiter and Schmidhuber, 1997) and have been shown to be effective in Natural Language Inference (NLI) research, where the task is to establish the relationship between multiple inputs (Rockt¨aschel et al., 2015). This type of architecture is often denoted as the dual architecture since one LSTM models the premise and the other models the hypothesis (in Recognizing Textual Entailment(RTE) tasks). Ghosh et al. (2018) used the dual LSTM architecture with hierarchical attention (HAN) (Yang et al., 2016) for sarcasm detection to model the conversation context, and we use their approach in this paper to model the current turn ct and the prior turn pt. HAN implements attention both at the word level and sentence level. The distinct characteristics of this attention is that the word/sentence-representations are weighted by measuring similarity with a word/sentence level context vector, respectively, which are randomly initialized and jointly learned during training (Yang et al., 2016). We compute the vector rep"
2021.eacl-main.171,W17-5523,1,0.849432,"et al., 2015; Muresan et al., 2016; Ghosh and Muresan, 2018) with state-of-theart argument features leads to better performance for the argumentative relation classification task (agree/disagree/none) (Section 5). For the deep learning approaches, we hypothesize that multitask learning, which allows representations to be shared between multiple tasks (e.g., here, the tasks of argumentative relation classification and sarcasm detection), lead to better generalizations. We investigate the impact of multitask learning for a dual Long Short-Term Memory (LSTM) Network with hierarchical attention (Ghosh et al., 2017) (Section 4.2) and BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), including an optional joint multitask learning objective with uncertainty-based weighting of task-specific losses (Kendall et al., 2018) (Section 4.3). We demonstrate that multitask learning improves the performance of the argumentative relation classification task for all settings (Section 5). We provide a detailed qualitative analysis (Section 5.1) to give insights into when and how modeling sarcasm helps. We make the code from our experiments publicly available.1 The Internet Argument Co"
2021.eacl-main.171,P16-2089,1,0.846351,"linguistics, focusing on the detection of argumentative structures in a text (see Stede and Schneider (2018) for an overview). This paper focuses on two subtasks: argumentative relation identification and classification (i.e., agree/disagree/none). Some of the earlier work on argumentative relation identification and classification has relied on feature-based machine learning models, focusing on online discussions (Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Wacholder et al., 2014) and monologues (Stab and Gurevych, 2014, 2017; Persing and Ng, 2016; Ghosh et al., 2016). Stab and Gurevych (2014) proposed a set of lexical, syntactic, semantic, and discourse features to classify them. On the same essay dataset, Nguyen and Litman (2016) utilized contextual information to improve the accuracy. Both Stab and Gurevych (2017) and Persing and Ng (2016) used Integer Linear Programming (ILP) based joint modeling to detect argument components and relations. Rosenthal and McKeown (2015) introduced sentence similarity and accommodation features, whereas Menini and Tonelli (2016) presented how entailment between text pairs can discover argumentative relations. Our argumen"
2021.eacl-main.171,W14-2106,1,0.92159,"elation classification task (agree/disagree/none) in all setups. 1 Table 1: Sarcastic turns that disagree, agree or have no argumentative relation with their prior turns. Introduction User-generated conversational data such as discussion forums provide a wealth of naturally occurring arguments. The ability to automatically detect and classify argumentative relations (e.g., agree/disagree) in threaded discussions is useful to understand how collective opinions form, how conflict arises and is resolved (van Eemeren et al., 1993; Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Rosenthal and McKeown, 2015; Stede and Schneider, 2018). Linguistic and argumentation theories have thoroughly studied the use of sarcasm in argumentation, including its effectiveness as a persuasive device or as a means to express an ad hominem ∗ Equal Contribution. fallacy (attacking the opponent instead of her/his argument) (Tindale and Gough, 1987; van Eemeren and Grootendorst, 1992; Gibbs and Izett, 2005; Averbeck, 2013). We propose an experimental setup to further our understanding of the role of sarcasm in shaping up the disagreement space in online interactions. The disagreement spac"
2021.eacl-main.171,2020.figlang-1.1,1,0.891341,"Missing"
2021.eacl-main.171,P11-2102,1,0.660945,"istory in linguistics (Tindale and Gough, 1987; Gibbs and Izett, 2005; Averbeck, 2013; van Eemeren and Grootendorst, 1992). We propose joint modeling of argumentative relation detection and sarcasm detection to empirically validate sarcasm’s role in shaping the disagreement space in online conversations. While the focus of our paper is not to provide a state-of-the-art sarcasm detection model, our feature-based models, along with the deep learning models for sarcasm detection are based on state-ofthe-art approaches. We implemented discrete features such as pragmatic features (Gonz´alez-Ib´an˜ ez et al., 2011; Muresan et al., 2016), diverse sarcasm markers (Ghosh and Muresan, 2018), and incongruity detection features (Riloff et al., 2013; Joshi et al., 2015). The LSTM models are influenced by Ghosh and Veale (2017); Ghosh et al. (2018), where the function of contextual knowledge is used to detect sarcasm. Lastly, transformer models such as BERT and RoBERTa have been used in the winning entries for the recent shared task on sarcasm detection (Ghosh et al., 2020). In our research, for both kinds of deep-learning models, the best results are obtained by using the multitask setup, showing that multita"
2021.eacl-main.171,2020.acl-main.740,0,0.0606576,"Missing"
2021.eacl-main.171,N18-1036,0,0.0608077,"Missing"
2021.eacl-main.171,W17-5107,0,0.0163424,"ccommodation features, whereas Menini and Tonelli (2016) presented how entailment between text pairs can discover argumentative relations. Our argumentative features in the feature-based model are based on the above works (Section 4.1). We show that additional features that are useful in sarcasm detection (Joshi et al., 2015; Ghosh and Muresan, 2018) enhance the performance on the argumentative relation identification and classification tasks. In addition to feature-based models, deep learning models have been recently used for these tasks. Potash et al. (2017) proposed a pointer network, and Hou and Jochim (2017) offered LSTM+Attention network to predict argument components and relations jointly, whereas (Chakrabarty et al., 2019) exploited adaptive pretraining (Gururangan et al., 2020) for BERT to identify argument relations. We use two multitask learning objectives (argumentative relation identification/classification and sarcasm detection), as our goal is to investigate whether identifying sarcasm can help in modeling the disagreement space. Majumder et al. (2019); Chauhan et al. (2020) used multitask learning for sarcasm & sentiment and sarcasm, sentiment, & emotion, respectively, where a direct l"
2021.eacl-main.171,P15-2124,0,0.408731,", it is not argumentative. It can be noticed that none of the current turns contain explicit lexical terms that could signal an argumentative relation with the prior turn. Instead, the argumentative move is being implicitly expressed using sarcasm. We study whether modeling sarcasm can improve the detection and classification of argumentative relations in online discussions. We propose a thorough experimental setup to answer this question using feature-based machine learning approaches and deep learning models. For the former, we show that combining features that are useful to detect sarcasm (Joshi et al., 2015; Muresan et al., 2016; Ghosh and Muresan, 2018) with state-of-theart argument features leads to better performance for the argumentative relation classification task (agree/disagree/none) (Section 5). For the deep learning approaches, we hypothesize that multitask learning, which allows representations to be shared between multiple tasks (e.g., here, the tasks of argumentative relation classification and sarcasm detection), lead to better generalizations. We investigate the impact of multitask learning for a dual Long Short-Term Memory (LSTM) Network with hierarchical attention (Ghosh et al.,"
2021.eacl-main.171,P19-1441,0,0.0269456,"this dual LSTM architecture is denoted as LST Mattn . To measure the impact of sarcasm in argumentative relation detection, we use a multitask learning approach. Multitask learning aims to leverage useful information in multiple related tasks to im3 We used the textual entailment toolkit (AllenNLP) (Gardner et al., 2017). 2001 4 https://pypi.org/project/skll/ Argumentative Relation Sarcasm Dense + SoftMax v pt v ct Figure 1: Sentence-level Multitask Attention Network for prior turn pt and current turn ct. Figure is inspired by Yang et al. (2016). prove each task’s performance (Caruana, 1997; Liu et al., 2019). We use a simple hard parameter sharing network. The architecture is a replica of the LST Mattn , with a modification of employing two loss functions, one for sarcasm detection (i.e., training using the S and N S labels) and another for the argumentative relation classification task (i.e., training using the A, D, and N labels). Figure 1 shows the high-level architecture of the dual LSTM and multitask learning (LST MM T ). The prior turn pt (left) and the current turn ct (right) are read by two separate LSTMs (i.e., LST Mpt and LST Mct ). In case of LST MM T the concatenation of vpt and vct i"
2021.eacl-main.171,C16-1232,0,0.136011,"2014; Wacholder et al., 2014) and monologues (Stab and Gurevych, 2014, 2017; Persing and Ng, 2016; Ghosh et al., 2016). Stab and Gurevych (2014) proposed a set of lexical, syntactic, semantic, and discourse features to classify them. On the same essay dataset, Nguyen and Litman (2016) utilized contextual information to improve the accuracy. Both Stab and Gurevych (2017) and Persing and Ng (2016) used Integer Linear Programming (ILP) based joint modeling to detect argument components and relations. Rosenthal and McKeown (2015) introduced sentence similarity and accommodation features, whereas Menini and Tonelli (2016) presented how entailment between text pairs can discover argumentative relations. Our argumentative features in the feature-based model are based on the above works (Section 4.1). We show that additional features that are useful in sarcasm detection (Joshi et al., 2015; Ghosh and Muresan, 2018) enhance the performance on the argumentative relation identification and classification tasks. In addition to feature-based models, deep learning models have been recently used for these tasks. Potash et al. (2017) proposed a pointer network, and Hou and Jochim (2017) offered LSTM+Attention network to"
2021.eacl-main.171,W13-4006,0,0.0928513,"oves the argumentative relation classification task (agree/disagree/none) in all setups. 1 Table 1: Sarcastic turns that disagree, agree or have no argumentative relation with their prior turns. Introduction User-generated conversational data such as discussion forums provide a wealth of naturally occurring arguments. The ability to automatically detect and classify argumentative relations (e.g., agree/disagree) in threaded discussions is useful to understand how collective opinions form, how conflict arises and is resolved (van Eemeren et al., 1993; Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Rosenthal and McKeown, 2015; Stede and Schneider, 2018). Linguistic and argumentation theories have thoroughly studied the use of sarcasm in argumentation, including its effectiveness as a persuasive device or as a means to express an ad hominem ∗ Equal Contribution. fallacy (attacking the opponent instead of her/his argument) (Tindale and Gough, 1987; van Eemeren and Grootendorst, 1992; Gibbs and Izett, 2005; Averbeck, 2013). We propose an experimental setup to further our understanding of the role of sarcasm in shaping up the disagreement space in online interactions. T"
2021.eacl-main.171,P16-1107,0,0.0237318,": argumentative relation identification and classification (i.e., agree/disagree/none). Some of the earlier work on argumentative relation identification and classification has relied on feature-based machine learning models, focusing on online discussions (Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Wacholder et al., 2014) and monologues (Stab and Gurevych, 2014, 2017; Persing and Ng, 2016; Ghosh et al., 2016). Stab and Gurevych (2014) proposed a set of lexical, syntactic, semantic, and discourse features to classify them. On the same essay dataset, Nguyen and Litman (2016) utilized contextual information to improve the accuracy. Both Stab and Gurevych (2017) and Persing and Ng (2016) used Integer Linear Programming (ILP) based joint modeling to detect argument components and relations. Rosenthal and McKeown (2015) introduced sentence similarity and accommodation features, whereas Menini and Tonelli (2016) presented how entailment between text pairs can discover argumentative relations. Our argumentative features in the feature-based model are based on the above works (Section 4.1). We show that additional features that are useful in sarcasm detection (Joshi et"
2021.eacl-main.171,W16-3604,0,0.0161698,"back through the model. This allows BERT to model the nuances of both tasks and their interdependence simultaneously. Dynamic Loss: Similar to the LSTM architecture, here, too, we experiment with dynamic multitask loss. We denote this variation as BERTM T uncert . Alternate Multitask Learning. We employ another multitask learning technique where we attempt to enrich the learning with fine-tuning of labeled additional material from the sarcasm detection task. Notably, we exploit “sarcasm V2”, a sarcasm detection dataset that was also curated from the original corpus of IAC and was released by Oraby et al. (2016). We pre-process the “sarcasm V2” dataset by removing duplicates that appear in IACorig and we end up selecting 3513 trainingv2 instances and 423 devv2 instances balanced between S/NS categories for experiments and merged them to the sarcasm dataset (training and dev, respectively) from IACorig . Note, unlike the original multitask setting, this time we have more sarcastic instances (a total of 11,495) than instances labeled with argumentative roles (7,982 instances as before) for the training purpose, while keeping the test set from IACorig unchanged. Since the training data is now unequal be"
2021.eacl-main.171,P16-1205,0,0.020729,"earch in computational linguistics, focusing on the detection of argumentative structures in a text (see Stede and Schneider (2018) for an overview). This paper focuses on two subtasks: argumentative relation identification and classification (i.e., agree/disagree/none). Some of the earlier work on argumentative relation identification and classification has relied on feature-based machine learning models, focusing on online discussions (Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Wacholder et al., 2014) and monologues (Stab and Gurevych, 2014, 2017; Persing and Ng, 2016; Ghosh et al., 2016). Stab and Gurevych (2014) proposed a set of lexical, syntactic, semantic, and discourse features to classify them. On the same essay dataset, Nguyen and Litman (2016) utilized contextual information to improve the accuracy. Both Stab and Gurevych (2017) and Persing and Ng (2016) used Integer Linear Programming (ILP) based joint modeling to detect argument components and relations. Rosenthal and McKeown (2015) introduced sentence similarity and accommodation features, whereas Menini and Tonelli (2016) presented how entailment between text pairs can discover argumentative r"
2021.eacl-main.171,D17-1143,0,0.0157437,"d McKeown (2015) introduced sentence similarity and accommodation features, whereas Menini and Tonelli (2016) presented how entailment between text pairs can discover argumentative relations. Our argumentative features in the feature-based model are based on the above works (Section 4.1). We show that additional features that are useful in sarcasm detection (Joshi et al., 2015; Ghosh and Muresan, 2018) enhance the performance on the argumentative relation identification and classification tasks. In addition to feature-based models, deep learning models have been recently used for these tasks. Potash et al. (2017) proposed a pointer network, and Hou and Jochim (2017) offered LSTM+Attention network to predict argument components and relations jointly, whereas (Chakrabarty et al., 2019) exploited adaptive pretraining (Gururangan et al., 2020) for BERT to identify argument relations. We use two multitask learning objectives (argumentative relation identification/classification and sarcasm detection), as our goal is to investigate whether identifying sarcasm can help in modeling the disagreement space. Majumder et al. (2019); Chauhan et al. (2020) used multitask learning for sarcasm & sentiment and sarcasm"
2021.eacl-main.171,D13-1066,0,0.0950446,"Missing"
2021.eacl-main.171,W15-4625,0,0.401843,"on task (agree/disagree/none) in all setups. 1 Table 1: Sarcastic turns that disagree, agree or have no argumentative relation with their prior turns. Introduction User-generated conversational data such as discussion forums provide a wealth of naturally occurring arguments. The ability to automatically detect and classify argumentative relations (e.g., agree/disagree) in threaded discussions is useful to understand how collective opinions form, how conflict arises and is resolved (van Eemeren et al., 1993; Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Rosenthal and McKeown, 2015; Stede and Schneider, 2018). Linguistic and argumentation theories have thoroughly studied the use of sarcasm in argumentation, including its effectiveness as a persuasive device or as a means to express an ad hominem ∗ Equal Contribution. fallacy (attacking the opponent instead of her/his argument) (Tindale and Gough, 1987; van Eemeren and Grootendorst, 1992; Gibbs and Izett, 2005; Averbeck, 2013). We propose an experimental setup to further our understanding of the role of sarcasm in shaping up the disagreement space in online interactions. The disagreement space, defined in the context of"
2021.eacl-main.171,W10-0214,0,0.0541326,"l., 2016); (e) PDTB discourse markers because claims often start with discourse markers such as therefore, so. We discard markers from the temporal relation; (f) modal verbs because they signal the degree of certainty when expressing a claim (Stab and Gurevych, 2014); (g) pronouns, since they dialogically point to the previous speaker’s stance; (h) textual entailment: captures whether a position expressed in the prior turn is accepted in the current turn (Cabrio and Villata, 2012; Menini and Tonelli, 2016)3 ; (i) lemma overlap to determine topical alignment between the prior and current turn (Somasundaran and Wiebe, 2010). We compute lemma overlap of noun, verbs, and adjectives between the turns, and (j) negation to extract explicit negation cues (e.g., “not”, “don’t”) that often signal disagreement. Sarcasm-relevant features (SarcF ). As sarcasm-relevant features we use: (a) Linguistic Inquiry Word Count (LIWC) (Pennebaker et al., 2001) features to capture the linguistic, social, individual, and psychological processes; (b) measuring sentiment incongruity, that is, capturing the number of times the difference in sentiment polarity between the prior turn pt and the current turn ct occurs and number of positive"
2021.eacl-main.171,D14-1006,0,0.144258,"mining is a growing area of research in computational linguistics, focusing on the detection of argumentative structures in a text (see Stede and Schneider (2018) for an overview). This paper focuses on two subtasks: argumentative relation identification and classification (i.e., agree/disagree/none). Some of the earlier work on argumentative relation identification and classification has relied on feature-based machine learning models, focusing on online discussions (Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Wacholder et al., 2014) and monologues (Stab and Gurevych, 2014, 2017; Persing and Ng, 2016; Ghosh et al., 2016). Stab and Gurevych (2014) proposed a set of lexical, syntactic, semantic, and discourse features to classify them. On the same essay dataset, Nguyen and Litman (2016) utilized contextual information to improve the accuracy. Both Stab and Gurevych (2017) and Persing and Ng (2016) used Integer Linear Programming (ILP) based joint modeling to detect argument components and relations. Rosenthal and McKeown (2015) introduced sentence similarity and accommodation features, whereas Menini and Tonelli (2016) presented how entailment between text pairs"
2021.eacl-main.171,J17-3005,0,0.0226208,"Some of the earlier work on argumentative relation identification and classification has relied on feature-based machine learning models, focusing on online discussions (Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Wacholder et al., 2014) and monologues (Stab and Gurevych, 2014, 2017; Persing and Ng, 2016; Ghosh et al., 2016). Stab and Gurevych (2014) proposed a set of lexical, syntactic, semantic, and discourse features to classify them. On the same essay dataset, Nguyen and Litman (2016) utilized contextual information to improve the accuracy. Both Stab and Gurevych (2017) and Persing and Ng (2016) used Integer Linear Programming (ILP) based joint modeling to detect argument components and relations. Rosenthal and McKeown (2015) introduced sentence similarity and accommodation features, whereas Menini and Tonelli (2016) presented how entailment between text pairs can discover argumentative relations. Our argumentative features in the feature-based model are based on the above works (Section 4.1). We show that additional features that are useful in sarcasm detection (Joshi et al., 2015; Ghosh and Muresan, 2018) enhance the performance on the argumentative relati"
2021.eacl-main.171,walker-etal-2012-corpus,0,0.229899,"modeling sarcasm improves the argumentative relation classification task (agree/disagree/none) in all setups. 1 Table 1: Sarcastic turns that disagree, agree or have no argumentative relation with their prior turns. Introduction User-generated conversational data such as discussion forums provide a wealth of naturally occurring arguments. The ability to automatically detect and classify argumentative relations (e.g., agree/disagree) in threaded discussions is useful to understand how collective opinions form, how conflict arises and is resolved (van Eemeren et al., 1993; Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Rosenthal and McKeown, 2015; Stede and Schneider, 2018). Linguistic and argumentation theories have thoroughly studied the use of sarcasm in argumentation, including its effectiveness as a persuasive device or as a means to express an ad hominem ∗ Equal Contribution. fallacy (attacking the opponent instead of her/his argument) (Tindale and Gough, 1987; van Eemeren and Grootendorst, 1992; Gibbs and Izett, 2005; Averbeck, 2013). We propose an experimental setup to further our understanding of the role of sarcasm in shaping up the disagreement space"
2021.eacl-main.171,H05-1044,0,0.0233146,"ArgF ) and sarcasmrelevant (SarcF ) features. Unless mentioned, all features were extracted from the current turn ct. Argument-relevant features (ArgF ). We first evaluate the features that are reported as being useful for identifying and classifying argumentative relations: (a) n-grams (e.g., unigram, bigram, trigram) created based on the full vocabulary of the IAC corpus; (b) argument lexicons: two lists of twenty words representing agreement (e.g., “agree”, “accord”) and disagreement (e.g., “differ”, “oppose”), respectively (Rosenthal and McKeown, 2015) (c) sentiment lexicons such as MPQA (Wilson et al., 2005) and opinion lexicon (Hu and Liu, 2004) to identify sentiment in the turns; (d) hedge features, since they are often used to mitigate speaker’s commitment (Tan et al., 2016); (e) PDTB discourse markers because claims often start with discourse markers such as therefore, so. We discard markers from the temporal relation; (f) modal verbs because they signal the degree of certainty when expressing a claim (Stab and Gurevych, 2014); (g) pronouns, since they dialogically point to the previous speaker’s stance; (h) textual entailment: captures whether a position expressed in the prior turn is accept"
2021.eacl-main.171,D15-1284,0,0.0250959,"uous terms “genesis” ↔ “evolution”, the strength of the relation (i.e., attention weight) is comparatively lower than BERTM T uncert (See Figure 4). On the contrary, between Figure 6 and Figure 7, BERTM T uncert model is attending multiple words in ct from the word “religion” in pt, but the BERTALT model attends only two words ‘anti” and “ignorance”, with high weights from “religion” (pt to ct). Humor by word repetition. Often the current turn ct sarcastically taunts the prior turn pt by word repetition and rhyme, imposing a humorous comic effect, also regarded as the phonetic style of humor (Yang et al., 2015). For the pair, “genetics 2005 In the case of argumentative turns (agree and disagree) that are wrongly classified as none by all models, we found two common patterns: the use of concessions (e.g., “it’s a consideration, but I doubt we should be promoting this . . . ”) and arguments with uncommitted beliefs (e.g., “it is possible that”, “that could probably be”, “possibly, I must admit”). Figure 8: BERTM T uncert (right) attending coreferenced words in a humorous example missed by the BERTOrig model (left) (disagree relation) has nothing to do with it” (pt) ↔ “are saying that genetics has noth"
2021.eacl-main.171,P18-4013,0,0.0122886,"y by the multitask models (LRArgF +SarcF , LST MM T uncert , BERTM T uncert , and BERTALT ) and not by the corresponding individual models (LRArgF , LST Mattn , and BERTOrig ). For both Transformer and LSTM-based models, we explore how attention heads behave and whether common patterns exist (e.g., attending words with opposite meaning when incongruity occurs). We display the heat maps of the attention weights for a pair of prior and current turns (LSTM-based models) (Figure 3) whereas for BERT we display word-toword attentions (Figures 4, 5, 6, 7, and 8) using visualization tools (Vig, 2019; Yang and Zhang, 2018).5 All the examples presented in this section are argumentative moves (i.e., turns with A or D) correctly identified by our multitask learning models but wrongly predicted as none (N ) by the individual models. Moreover, the multitask learning models also correctly predict that these turns are instances of sarcasm. Incongruity between prior turn and current turn. Semantic incongruity, which can appear between conversation context pt and the current turn ct is an inherent characteristic of sarcasm (Joshi et al., 2015). This characteristic highlights the inconsistency between expectations and re"
2021.eacl-main.171,N16-1174,0,0.231518,"res) is the joint model. 4.2 Dual LSTM and Multitask Learning LSTMs are able to learn long-term dependencies (Hochreiter and Schmidhuber, 1997) and have been shown to be effective in Natural Language Inference (NLI) research, where the task is to establish the relationship between multiple inputs (Rockt¨aschel et al., 2015). This type of architecture is often denoted as the dual architecture since one LSTM models the premise and the other models the hypothesis (in Recognizing Textual Entailment(RTE) tasks). Ghosh et al. (2018) used the dual LSTM architecture with hierarchical attention (HAN) (Yang et al., 2016) for sarcasm detection to model the conversation context, and we use their approach in this paper to model the current turn ct and the prior turn pt. HAN implements attention both at the word level and sentence level. The distinct characteristics of this attention is that the word/sentence-representations are weighted by measuring similarity with a word/sentence level context vector, respectively, which are randomly initialized and jointly learned during training (Yang et al., 2016). We compute the vector representation for the current turn ct and prior turn pt and concatenate vectors from the"
2021.eacl-main.171,P19-3007,0,0.0167683,"edicted only by the multitask models (LRArgF +SarcF , LST MM T uncert , BERTM T uncert , and BERTALT ) and not by the corresponding individual models (LRArgF , LST Mattn , and BERTOrig ). For both Transformer and LSTM-based models, we explore how attention heads behave and whether common patterns exist (e.g., attending words with opposite meaning when incongruity occurs). We display the heat maps of the attention weights for a pair of prior and current turns (LSTM-based models) (Figure 3) whereas for BERT we display word-toword attentions (Figures 4, 5, 6, 7, and 8) using visualization tools (Vig, 2019; Yang and Zhang, 2018).5 All the examples presented in this section are argumentative moves (i.e., turns with A or D) correctly identified by our multitask learning models but wrongly predicted as none (N ) by the individual models. Moreover, the multitask learning models also correctly predict that these turns are instances of sarcasm. Incongruity between prior turn and current turn. Semantic incongruity, which can appear between conversation context pt and the current turn ct is an inherent characteristic of sarcasm (Joshi et al., 2015). This characteristic highlights the inconsistency betw"
2021.eacl-main.171,W14-4918,1,0.800808,"e.ucsc.edu/iac2 2 Related Work Argument mining is a growing area of research in computational linguistics, focusing on the detection of argumentative structures in a text (see Stede and Schneider (2018) for an overview). This paper focuses on two subtasks: argumentative relation identification and classification (i.e., agree/disagree/none). Some of the earlier work on argumentative relation identification and classification has relied on feature-based machine learning models, focusing on online discussions (Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Wacholder et al., 2014) and monologues (Stab and Gurevych, 2014, 2017; Persing and Ng, 2016; Ghosh et al., 2016). Stab and Gurevych (2014) proposed a set of lexical, syntactic, semantic, and discourse features to classify them. On the same essay dataset, Nguyen and Litman (2016) utilized contextual information to improve the accuracy. Both Stab and Gurevych (2017) and Persing and Ng (2016) used Integer Linear Programming (ILP) based joint modeling to detect argument components and relations. Rosenthal and McKeown (2015) introduced sentence similarity and accommodation features, whereas Menini and Tonelli (2016) pres"
2021.eacl-main.171,N12-1072,0,0.192873,"modeling sarcasm improves the argumentative relation classification task (agree/disagree/none) in all setups. 1 Table 1: Sarcastic turns that disagree, agree or have no argumentative relation with their prior turns. Introduction User-generated conversational data such as discussion forums provide a wealth of naturally occurring arguments. The ability to automatically detect and classify argumentative relations (e.g., agree/disagree) in threaded discussions is useful to understand how collective opinions form, how conflict arises and is resolved (van Eemeren et al., 1993; Abbott et al., 2011; Walker et al., 2012b; Misra and Walker, 2013; Ghosh et al., 2014; Rosenthal and McKeown, 2015; Stede and Schneider, 2018). Linguistic and argumentation theories have thoroughly studied the use of sarcasm in argumentation, including its effectiveness as a persuasive device or as a means to express an ad hominem ∗ Equal Contribution. fallacy (attacking the opponent instead of her/his argument) (Tindale and Gough, 1987; van Eemeren and Grootendorst, 1992; Gibbs and Izett, 2005; Averbeck, 2013). We propose an experimental setup to further our understanding of the role of sarcasm in shaping up the disagreement space"
2021.emnlp-main.504,2020.acl-main.399,0,0.0186401,"cit premise by a BART model (zero-shot), by a BART model fine-tuned on ART dataset, and a BART model fine-tuned on ART augmented with discourse-aware commonsense knowledge derived from PARA-COMET. We make the code available at https://github.com/ tuhinjubcse/EnthymemesEMNLP2021. O1 O2 H Alex had his heart set on an ivy league college Alex ended up achieving his dream of getting into the school. Alex applied to Harvard Table 2: Instances from the ART dataset. models that aim to generate an implicit premise given an enthymeme, using abductive reasoning and discourse-aware commonsense knowledge. Alshomary et al. (2020) introduce a closely related task of generating an argument’s conclusion from its premises. Specifically, they focus on the subtask of inferring the conclusion’s target from the premises. They develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network. Unlike this paper, our work focuses on the new task of generating an implicit premise given an enthymeme that consists of a stated conclusion and a stated premise. 3"
2021.emnlp-main.504,2020.lrec-1.282,0,0.0194636,"pothesis is prepended by And since in bolded blue. Test datasets. We test our models on three different datasets of incomplete arguments (enthymeme) annotated with human-generated implicit/missing premises. First, we use the Argument Reasoning Comprehension Task dataset released by Habernal et al. (2018) (D1), which contains 1654 {claim, premise, warrant(implicit premise)} triples. Second, we used the dataset introduced by Boltuži´c and Šnajder (2016), which contains 494 enthymemes from an online debate forum with human annotated implicit premises (D2). Third, we use the dataset introduced by Becker et al. (2020) (D3), which contains implicit premises annotated for each arguments from the MicroText Corpus (Peldszus and Stede). For D3, we focus only arguments that are in a support relation since this corresponds to our task. Moreover, we choose the cases where there is only one implicit premise, rather than a chain of linked premises. This results in a total of 112 enthymemes for D3. For all datasets, we apply automatic filtering to keep only full-formed sentences as claim and premises (e.g., remove cases where the stated premise/claim consists of a nounphrase, a partial clauses, or many sentences). 4"
2021.emnlp-main.504,W16-2815,0,0.0672571,"Missing"
2021.emnlp-main.504,P19-1470,0,0.0267223,"f a nounphrase, a partial clauses, or many sentences). 4 Method Fine-tuning BART on ART. To fine-tune BART on the ART dataset (Section 3), we concatenate O1 and O2 with a special delimiter [SEP] as input to BART encoder as shown in Table 3 Row 1. For decoding, we focus on reconstructing the entire argument given an enthymeme. To encourage fluency and coherence in our generated argument, we prepend the plausible hypothesis (implicit premise) with a discourse marker And since (Table 3 Row 3) during fine-tuning. Fine-tuning BART on PARA-COMET enhanced ART. Adapted knowledge models such as COMET (Bosselut et al., 2019) have been shown to generate implicit commonsense inferences along several dimensions (depending on what knowledge graphs they were pre-trained on). PARA-COMET (Gabriel et al., 2021), is an extension of COMET pre-trained on ATOMIC (Sap et al., 2019) that is able to generate discourse-aware common sense knowledge. ATOMIC is a knowledge graph that contains 9 relations related to social commonsense knowledge, including dynamic aspects of events such as causes and effects, if-then conditional statements, and mental states. Given a text with T sentences S1 , S2 ...ST , PARA-COMET generates a set of"
2021.emnlp-main.504,D19-1109,0,0.0480755,"Missing"
2021.emnlp-main.504,N18-1175,0,0.365299,"i2 , and Smaranda Muresan1,3 1 Department of Computer Science, Columbia University 2 College of Computing, Georgia Tech 3 Data Science Institute, Columbia University atrivedi@gatech.edu, {tuhin.chakr,smara}@cs.columbia.edu Abstract Reason Claim Enthymemes are defined as arguments where a premise or conclusion is left implicit. We tackle the task of generating the implicit premise in an enthymeme, which requires not only an understanding of the stated conclusion and premise, but also additional inferences that could depend on commonsense knowledge. The largest available dataset for enthymemes (Habernal et al., 2018) consists of 1.7k samples, which is not large enough to train a neural text generation model. To address this issue, we take advantage of a similar task and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020). However, we show that simply using a state-of-the-art seq2seq model fine-tuned on this data might not generate meaningful implicit premises associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and"
2021.emnlp-main.504,2020.acl-main.703,0,0.503728,"ses associated with the given enthymemes. We demonstrate that encoding discourse-aware commonsense during fine-tuning improves the quality of the generated implicit premises and outperforms all other baselines both in automatic and human evaluations on three different datasets. ZeroShot Fine-tuned on ART Fine-tuned on ART +PARA-C Vaccinations save lives Vaccination should be mandatory for all children Vaccines save lives, they save money Vaccinations are the best way to protect children. Vaccinations are the best way to prevent childhood diseases. Table 1: Implicit Premise Generation by BART (Lewis et al., 2020) in three different setting for an input enthymeme from dataset by Habernal et al. (2018) The missing premise in this case is the generalization “Dogs generally bark when a person enters an area unless the dog knows the person well.&quot; While there has been work on identification (i.e., classification) and reconstruction of implicit premises in enthymemes (Rajendran et al., 2016; Habernal et al., 2018; Reisert et al., 2015; Boltuži´c and Šnajder, 2016; Razuvayevskaya and Teufel, 2017), to our knowledge, automatically generating an implicit premise from a given enthymeme is a new task. There are t"
2021.emnlp-main.504,N16-1098,0,0.029856,"ur work focuses on the new task of generating an implicit premise given an enthymeme that consists of a stated conclusion and a stated premise. 3 Datasets Training dataset. Based on the theoretical connection between enthymemes and abductive reasoning, we use the Abductive Reasoning in narrative Text (ART) data developed for the abductive NLG task (Bhagavatula et al., 2020) to train our models. The task is framed as: given two observations (O1 and O2) from a narrative, generate the most plausible explanation (hypothesis) (Table 2). The observations O1, O2 in ART are drawn from the ROCStories (Mostafazadeh et al., 2016) dataset, 2 Related Work a large collection of short, manually curated five Prior work on enthymeme reconstruction has fo- sentence stories. The beginning and ending of each cused primarily on the identification (i.e., classifi- story maps to the first (O1) and second (O2) obcation) of implicit premises in enthymemes (Ra- servations in ART, respectively. Bhagavatula et al. jendran et al., 2016; Habernal et al., 2018; Reis- (2020) presented O1 and O2 as narrative context to ert et al., 2015; Boltuži´c and Šnajder, 2016; Razu- crowdworkers and prompted them to generate plauvayevskaya and Teufel,"
2021.emnlp-main.504,L18-1258,1,0.826464,"e notice that adding commonsense beams from PARA-COMET makes the generated implicit premise more plausible. For instance, for the stated claim and premise from D3 in Table 6, we see that PARA-COMET adds a beam to feel better. Similarly it adds a beam to learn more for the stated claim and premise from D1 for both examples shown in Table 6. We posit that adding these in combination with the stated claim and premise, leads our model to infer more plausible implicit premises compared to the ones generated by BART fine-tuned on ART. Finally, given that D3 has been annotated with argument schemes (Musi et al., 2018), we can explore their role in enthymeme reconstruction. We notice that most of the generated plausible implicit premises beResults. While pre-trained language models of- long to enthymemes annotated with Practical Evalten contain structured commonsense (Davison uation argument scheme, where “the premise is et al., 2019; Zhou et al., 2020) Table 4 shows that an evaluation about something being ‘good’ or pre-trained BART cannot generate plausible im- ‘bad’, while the claim expresses a recommendaplicit premises. Fine-tuning on the ART dataset im- tion/advice about stopping/continuing an action&quot;"
2021.emnlp-main.504,P02-1040,0,0.109355,"Missing"
2021.emnlp-main.504,W16-2804,0,0.154915,"be mandatory for all children Vaccines save lives, they save money Vaccinations are the best way to protect children. Vaccinations are the best way to prevent childhood diseases. Table 1: Implicit Premise Generation by BART (Lewis et al., 2020) in three different setting for an input enthymeme from dataset by Habernal et al. (2018) The missing premise in this case is the generalization “Dogs generally bark when a person enters an area unless the dog knows the person well.&quot; While there has been work on identification (i.e., classification) and reconstruction of implicit premises in enthymemes (Rajendran et al., 2016; Habernal et al., 2018; Reisert et al., 2015; Boltuži´c and Šnajder, 2016; Razuvayevskaya and Teufel, 2017), to our knowledge, automatically generating an implicit premise from a given enthymeme is a new task. There are two main challenges that need to be addressed: 1) lack of large scale data of incomplete arguments together with annotated 1 Introduction missing premises needed to train a sequence-tosequence model (the largest such set contains 1.7K In argumentation theory, an enthymeme is defined as an incomplete argument found in discourse, instances (Habernal et al., 2018)); and 2) the in"
2021.emnlp-main.504,W15-0507,0,0.028116,"es, they save money Vaccinations are the best way to protect children. Vaccinations are the best way to prevent childhood diseases. Table 1: Implicit Premise Generation by BART (Lewis et al., 2020) in three different setting for an input enthymeme from dataset by Habernal et al. (2018) The missing premise in this case is the generalization “Dogs generally bark when a person enters an area unless the dog knows the person well.&quot; While there has been work on identification (i.e., classification) and reconstruction of implicit premises in enthymemes (Rajendran et al., 2016; Habernal et al., 2018; Reisert et al., 2015; Boltuži´c and Šnajder, 2016; Razuvayevskaya and Teufel, 2017), to our knowledge, automatically generating an implicit premise from a given enthymeme is a new task. There are two main challenges that need to be addressed: 1) lack of large scale data of incomplete arguments together with annotated 1 Introduction missing premises needed to train a sequence-tosequence model (the largest such set contains 1.7K In argumentation theory, an enthymeme is defined as an incomplete argument found in discourse, instances (Habernal et al., 2018)); and 2) the inherent need to model commonsense or word know"
2021.emnlp-main.504,D19-1339,0,0.0222231,"of discourse aware commonsense. 6 Conclusions knowledge-enhanced model by encoding discourseaware commonsense that outperforms all existing baselines in terms of automatic metrics as well as plausibility judgements from crowdworkers. Future work includes exploring other sources for commonsense knowledge, experimenting with improved decoding techniques, as well as studying the role of argument schemes in enthymemes reconstruction. 7 Ethical Considerations Although we use language models trained on data collected from the Web, which have been shown to have issues with bias and abusive language (Sheng et al., 2019; Wallace et al., 2019), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, BART is a conditional language model, which provides more control of the generated output. Finally, we finetune our model on the ART dataset, which is built on five sentence short stories which is devoid of harmful and toxic text especially targeted at marginalized communities. While dual-use concerns are certainly possible here, we think that open-sourcing this technology will help to facilitate understanding of arguments with more balanced and better reasoni"
2021.emnlp-main.504,D19-1221,0,0.0285262,"ommonsense. 6 Conclusions knowledge-enhanced model by encoding discourseaware commonsense that outperforms all existing baselines in terms of automatic metrics as well as plausibility judgements from crowdworkers. Future work includes exploring other sources for commonsense knowledge, experimenting with improved decoding techniques, as well as studying the role of argument schemes in enthymemes reconstruction. 7 Ethical Considerations Although we use language models trained on data collected from the Web, which have been shown to have issues with bias and abusive language (Sheng et al., 2019; Wallace et al., 2019), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, BART is a conditional language model, which provides more control of the generated output. Finally, we finetune our model on the ART dataset, which is built on five sentence short stories which is devoid of harmful and toxic text especially targeted at marginalized communities. While dual-use concerns are certainly possible here, we think that open-sourcing this technology will help to facilitate understanding of arguments with more balanced and better reasoning. The technology shou"
2021.emnlp-main.577,N19-1388,0,0.0259889,"m/ https://digitaldante.columbia.edu/ https://www.poetryinternational.org/ https://lyricstranslate.com/ https://www.poetryinternational.org/ https://lyricstranslate.com/ Train Valid Test 37,746 2059 536 50,001 4186 548 15,199 699 140 17,000 1,050 1295 34,534 1,997 528 23,403 1,000 159 Table 2: Dataset source and statistics collect a multilingual parallel corpus consisting of more than 190,000 lines of poetry spanning over six languages. We try to tackle the second challenge by leveraging multilingual pre-training (e.g., mBART (Liu et al., 2020)) and multilingual finetuning (Tang et al., 2020; Aharoni et al., 2019) that have recently led to advances in neural machine translation for low-resource languages. Moreover, it has been shown that adaptive pre-training and/or fine-tuning on in-domain data always lead to improved performance on the end task (Gururangan et al., 2020). Since poetry translation falls into the lowresource (no or little parallel data) and in-domain translation scenarios, we present an empirical investigation on whether advances in these areas bring us a step closer to poetry translation systems that don’t go far off in terms of faithfulness (i.e., keeping the meaning and poetic style"
2021.emnlp-main.577,D16-1162,0,0.0158218,"nonpoetic model, introduction of archaic language where it is not appropriate, and change in meaning. An example output is provided in Table 8. Gold M M+ST What fun it is, with feet in sharp steel shod, How fun it is to wear iron-clad shoes, Their iron shoes are saucy fun, Table 8: Style transfer example. M=Multi(OPUS) 7 Analysis It is well-known that occasionally NMT systems have a tendency to generate translations that are grammatically correct but unrelated to the source sentence particularly for low-resource settings (e.g., hallucinate words that are not mentioned in the source language) (Arthur et al., 2016; Koehn and Knowles, 2017). Pre-trained multilingual language models and techniques like multilingual training or fine-tuning can indeed be effective for dealing with 6 Shortcomings of Style Transfer low-resource data such as poetry as seen in Figures Techniques as a Post-Editing tool 1, 2, 3, showing examples of poetic translations by Poetic All and Multi(OPUS) configurations. HowWe evaluate whether style transfer techniques could help attenuate the shortcomings of translation mod- ever, it is surprising that even a model trained on 6M parallel lines from OPUS(100) performs worse els trained"
2021.emnlp-main.577,W15-0713,0,0.0190004,"ion of automatic literary translation, automatic poetry translation is still in its infancy. Genzel et al. (2010) produce poetry translations with meter and rhyme using phrase-based statistical MT approaches. Ghazvininejad et al. (2018) present a neural poetry translation system that focuses on form rather than meaning. They also only focus on poetry translation from French to English, and their code or data is not publicly available. In our work, the focus is on faithfulness and the ability to preserve figurative language in translation across multiple languages. 9 Conclusion and Future Work Besacier and Schwartz (2015) conduct a pilot study We release poetic parallel corpora for 6 language of how suitable an MT+PE (machine translation + pairs. Our work shows the clear benefit of domain post-editing) pipeline would be for literary trans- adaptation for poetry translation. It further shows lation, concluding that their SMT approach could that improvements can be achieved by leveragproduce “acceptable and rather readable” transla- ing multilingual fine-tuning, and that the improvetions. Matusov (2019) found that adapting NMT ments transfer to unseen languages. Future direcsystems to literary content leads to i"
2021.emnlp-main.577,P17-2061,0,0.0181712,"ed parallel corpora of low-resource languages. For example, Currey et al. (2017) propose copying the target data to Poetic and literary translation Jones and Irvine the source side to incorporate monolingual training (2013) discuss the difficulties of faithful machine data for low-resource languages. Back-translation translation of literary text in terms of the compethas been used for synthetic parallel corpora gen- ing objectives of staying faithful to the original eration (Sennrich et al., 2016). To improve per- text but, on the other hand, trying to convey the formance on specific domains, Chu et al. (2017) experience of reading a literary piece to the reader. 7260 Original: Met steeds speelser gemak sla ik de aanvallen af op mijn zwaarbevochten onverschilligheid. De hemelen druipen af, met pracht en al, de bomen laten moedeloos hangen hun fonkelend loof. Geen oog, geen oor. Een brede glimlach. Je brengt wijn en jezelf: mijn laatste zwakheden. Ik zal mij verzadigen tot herhaling ongewenst wordt, en het vuil nog eenmaal van mijn harde handen spoelen. Gold: With ever greater playful ease I counter the attacks on my hard-won indiﬀerence. The heavens fall back, glory and all, trees let dangle in dej"
2021.emnlp-main.577,C18-1111,0,0.0209744,"s. Multi(OPUS): Round, icy of its oceans, transparent as a cell under the microscope but horizontally with mountains set high above the meadows with the tongue of the rivers and the low sea. I only sometimes suspect vertigo: We spin faster. Sleeping I scream “cado” And there I feel space, black, stars on my neck The scare that vomits itself into a thousand balls. Figure 2: Example Italian-English translation augment corpora with tags to indicate specific domains. A conventional model-centric approach is fine-tuning on in-domain parallel corpora or on mixed in-domain and out-of-domain corpora (Chu and Wang, 2018). In our work, we deal with a model-centric approach where we leverage a multilingual pre-trained model (mBART) and then finetune it multilingually on in-domain corpus. Recently, Hu et al. (2019) introduced a domain adaptation technique using lexicon induction, where large amounts of monolingual data are leveraged to find translations of in-domain unseen words. However, word-level lexicon induction might not be the most useful augmentation technique in our case, since poetic text deals with multi-word unseen phenomena such as metaphors. Domain adaptation in neural machine translation Chu and W"
2021.emnlp-main.577,W17-4715,0,0.0216458,"tion technique using lexicon induction, where large amounts of monolingual data are leveraged to find translations of in-domain unseen words. However, word-level lexicon induction might not be the most useful augmentation technique in our case, since poetic text deals with multi-word unseen phenomena such as metaphors. Domain adaptation in neural machine translation Chu and Wang (2018) categorize domain adaptation for NMT in two groups: data centric and model centric. Data centric techniques mostly focus on data augmentation for limited parallel corpora of low-resource languages. For example, Currey et al. (2017) propose copying the target data to Poetic and literary translation Jones and Irvine the source side to incorporate monolingual training (2013) discuss the difficulties of faithful machine data for low-resource languages. Back-translation translation of literary text in terms of the compethas been used for synthetic parallel corpora gen- ing objectives of staying faithful to the original eration (Sennrich et al., 2016). To improve per- text but, on the other hand, trying to convey the formance on specific domains, Chu et al. (2017) experience of reading a literary piece to the reader. 7260 Ori"
2021.emnlp-main.577,2020.emnlp-main.475,0,0.034068,"Missing"
2021.emnlp-main.577,2020.lrec-1.468,0,0.0371045,"Missing"
2021.emnlp-main.577,D10-1016,0,0.0400055,"Missing"
2021.emnlp-main.577,P19-1286,0,0.0151798,"sometimes suspect vertigo: We spin faster. Sleeping I scream “cado” And there I feel space, black, stars on my neck The scare that vomits itself into a thousand balls. Figure 2: Example Italian-English translation augment corpora with tags to indicate specific domains. A conventional model-centric approach is fine-tuning on in-domain parallel corpora or on mixed in-domain and out-of-domain corpora (Chu and Wang, 2018). In our work, we deal with a model-centric approach where we leverage a multilingual pre-trained model (mBART) and then finetune it multilingually on in-domain corpus. Recently, Hu et al. (2019) introduced a domain adaptation technique using lexicon induction, where large amounts of monolingual data are leveraged to find translations of in-domain unseen words. However, word-level lexicon induction might not be the most useful augmentation technique in our case, since poetic text deals with multi-word unseen phenomena such as metaphors. Domain adaptation in neural machine translation Chu and Wang (2018) categorize domain adaptation for NMT in two groups: data centric and model centric. Data centric techniques mostly focus on data augmentation for limited parallel corpora of low-resour"
2021.emnlp-main.577,N18-2011,0,0.271582,"uning on poetic data outperforms bilingual fine-tuning on poetic data. 1 Human GCK Google Il ny avait que sable et boue Où sétait ouverte la tombe. Le long des murs de la prison On ne voyait aucune tombe For where a grave had opened wide, There was no grave at all: Only a stretch of mud and sand By the hideous prison-wall, But there was only sand and mud. To where the grave was laid. Along the walls of prison wall. We saw no masquerade There was only sand and mud Where the grave had opened. Along the prison walls No tomb could be seen. Table 1: A French poem accompanied by human translation, (Ghazvininejad et al., 2018) (GCK) system translation, and Google Translate.2 et al., 2021; Van de Cruys, 2020; Ghazvininejad et al., 2016; Li et al., 2020; Hämäläinen and Alnajjar, 2019; Yi et al., 2020; Deng et al., 2019; Chen et al., 2019; Yang et al., 2018), research on poetry translation is in its infancy (Ghazvininejad et al., 2018; Genzel et al., 2010). For example, Ghazvininejad et al. (2018) em1 Introduction ploys a constrained decoding technique to maintain rhyme in French to English poetry translaAmerican poet Robert Frost once defined poetry tion. However, while keeping the poetic style and as “that which get"
2021.emnlp-main.577,D16-1126,0,0.0299813,"et boue Où sétait ouverte la tombe. Le long des murs de la prison On ne voyait aucune tombe For where a grave had opened wide, There was no grave at all: Only a stretch of mud and sand By the hideous prison-wall, But there was only sand and mud. To where the grave was laid. Along the walls of prison wall. We saw no masquerade There was only sand and mud Where the grave had opened. Along the prison walls No tomb could be seen. Table 1: A French poem accompanied by human translation, (Ghazvininejad et al., 2018) (GCK) system translation, and Google Translate.2 et al., 2021; Van de Cruys, 2020; Ghazvininejad et al., 2016; Li et al., 2020; Hämäläinen and Alnajjar, 2019; Yi et al., 2020; Deng et al., 2019; Chen et al., 2019; Yang et al., 2018), research on poetry translation is in its infancy (Ghazvininejad et al., 2018; Genzel et al., 2010). For example, Ghazvininejad et al. (2018) em1 Introduction ploys a constrained decoding technique to maintain rhyme in French to English poetry translaAmerican poet Robert Frost once defined poetry tion. However, while keeping the poetic style and as “that which gets lost out of both prose and verse fluency, the translation might diverge in terms of in translation” (Frost,"
2021.emnlp-main.577,W13-2305,0,0.0330206,"ed the use of BLEU through a systematic study of 4380 machine translation systems and recommend use of a pre-trained metric COMET (Rei et al., 2020). COMET leverages recent breakthroughs in crosslingual pre-trained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. We rely on the recommended model wmt-large-da-estimator-1719, which is trained to minimize the mean squared error between the predicted scores and the DA (Graham et al., 2013) quality assessments. Notice that these scores are normalized per annotator and hence not bounded between 0 and 1, allowing negative scores to occur; higher score means better translation. BERTScore in the automatic evaluation. We chose a subset of the test set for human evaluation: 1044 sentences spanning across 80 poems in 6 languages (204 lines in Russian, 173 lines in Italian, 140 lines in Portuguese, 220 lines in Spanish, 148 lines in German, and 159 lines in Dutch with corresponding human translations). Human judges were also provided with gold translations to make the judgement easier."
2021.emnlp-main.577,2020.acl-main.740,0,0.0339761,"Missing"
2021.emnlp-main.577,D19-1617,0,0.0245827,"des murs de la prison On ne voyait aucune tombe For where a grave had opened wide, There was no grave at all: Only a stretch of mud and sand By the hideous prison-wall, But there was only sand and mud. To where the grave was laid. Along the walls of prison wall. We saw no masquerade There was only sand and mud Where the grave had opened. Along the prison walls No tomb could be seen. Table 1: A French poem accompanied by human translation, (Ghazvininejad et al., 2018) (GCK) system translation, and Google Translate.2 et al., 2021; Van de Cruys, 2020; Ghazvininejad et al., 2016; Li et al., 2020; Hämäläinen and Alnajjar, 2019; Yi et al., 2020; Deng et al., 2019; Chen et al., 2019; Yang et al., 2018), research on poetry translation is in its infancy (Ghazvininejad et al., 2018; Genzel et al., 2010). For example, Ghazvininejad et al. (2018) em1 Introduction ploys a constrained decoding technique to maintain rhyme in French to English poetry translaAmerican poet Robert Frost once defined poetry tion. However, while keeping the poetic style and as “that which gets lost out of both prose and verse fluency, the translation might diverge in terms of in translation” (Frost, 1961). Indeed, the task is meaning w.r.t. the in"
2021.emnlp-main.577,W18-2703,0,0.0220056,"aluation in terms of preference between multilingual fine-tuning on Non-Poetic data vs Poetic data, in terms of faithfulness. Significant difference (α &lt; 0.05) via Wilcoxon signed-rank test. et al., 2020) or domain repair (Wei et al., 2020) could improve the performance of model trained on Poetic data. We leave this for future work. While we show that multilingual fine-tuning is an effective way to improve performance on low Zero-Shot Performance on Unseen Languages resource poetic data, we believe techniques like We test the generalization capabilities of our model iterative backtranslation (Hoang et al., 2018) with fine-tuned on poetic data using poetry written in sophisticated techniques for data selection (Dou languages not seen during fine-tuning. We compare 7258 Ukranian Romanian Swedish M1 M2 M3 M1 M2 M3 M1 M2 M3 BLEU 9.2 9.1 15.1 30.1 24.4 29.9 14.3 16.6 19.5 BERTScore 64.2 65.0 67.3 74.7 73.6 76.1 68.0 66.4 71.3 COMET -39.61 -40.46 -32.10 13.71 9.43 18.15 -24.21 -30.47 -14.97 Table 6: Zero-shot experiments. M1=NonPoetic Multi(ML50); M2=Non-Poetic Multi(OPUS); M3=Poetic All. Significant (α &lt; 0.005) via Wilcoxon signed-rank test the zero-shot performance of our model fine-tuned multilingually"
2021.emnlp-main.577,P17-1016,0,0.0242064,"whereas the original poem talks about “tomb&quot;. 2011). But even though poetry is destined to lose Meanwhile, state-of-the-art machine translation its accuracy, integrity, and beauty even in human translation, the process conceives new opportuni- systems trained on large non-poetic data might preserve meaning and fluency, but not the poetic style ties to stress-test the ability of machine translation (e.g., Google Translate’s output in Table 1). models to deal with figurative language. While most computational work has focused on Two main challenges exist for automatic poetry poetry generation (Hopkins and Kiela, 2017; Uthus translation: the lack of open-sourced multilingual parallel poetic corpora and the intrinsic complexi1 The italics part of the title is the translation of a poem by ties involved in preserving the semantics, style and Pablo Neruda with the same name. 2 Example taken from (Ghazvininejad et al., 2018). figurative nature of poetry. To address the first, we 7253 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7253–7265 c November 7–11, 2021. 2021 Association for Computational Linguistics Language Pair Spanish-English Russian-English Portugese-E"
2021.emnlp-main.577,W13-2713,0,0.0804759,"Missing"
2021.emnlp-main.577,2021.acl-long.66,0,0.0172506,"dish poetry given the fact that our model is fine-tuned on poetry belonging to languages from the Slavic, Romance, and Germanic families. Table 6 shows that our multilingually fine-tuned poetic model outperforms the other two multilingual models fine-tuned on NonPoetic data, even though the languages were not contained in the fine-tuning data. This suggests that performance improvements of poetic fine-tuning are not only due to language-specific training data, but rather to multilinguality, presence of language family related data, as well as poetic style. These corroborate recent findings by Ko et al. (2021) who adapt high-resource NMT models to translate lowresource related languages without parallel data. They exploit the fact that some low-resource languages are linguistically related or similar to highresource languages, and often share many lexical or syntactic features. RU ES PT IT DE NL BLEU 5.75 (-7.07) 6.42 (-21.58) 5.11 (-4.09) 6.13 (-16.77) 5.98 (-11.82) 6.96 (-19.14) BERTScore 59.91 (-7.29) 62.11 (-13.49) 58.24 (-5.76) 58.72 (-12.38) 60.07 (-10.83) 60.95 (-11.95) Table 7: BLEU and BERTScore after style transfer applied to the Multi(OPUS) configuration. Value in parenthesis reports dec"
2021.emnlp-main.577,W17-3204,0,0.0236378,"oduction of archaic language where it is not appropriate, and change in meaning. An example output is provided in Table 8. Gold M M+ST What fun it is, with feet in sharp steel shod, How fun it is to wear iron-clad shoes, Their iron shoes are saucy fun, Table 8: Style transfer example. M=Multi(OPUS) 7 Analysis It is well-known that occasionally NMT systems have a tendency to generate translations that are grammatically correct but unrelated to the source sentence particularly for low-resource settings (e.g., hallucinate words that are not mentioned in the source language) (Arthur et al., 2016; Koehn and Knowles, 2017). Pre-trained multilingual language models and techniques like multilingual training or fine-tuning can indeed be effective for dealing with 6 Shortcomings of Style Transfer low-resource data such as poetry as seen in Figures Techniques as a Post-Editing tool 1, 2, 3, showing examples of poetic translations by Poetic All and Multi(OPUS) configurations. HowWe evaluate whether style transfer techniques could help attenuate the shortcomings of translation mod- ever, it is surprising that even a model trained on 6M parallel lines from OPUS(100) performs worse els trained on non-poetic data. We use"
2021.emnlp-main.577,2020.emnlp-main.55,0,0.032634,"indeed be effective for dealing with 6 Shortcomings of Style Transfer low-resource data such as poetry as seen in Figures Techniques as a Post-Editing tool 1, 2, 3, showing examples of poetic translations by Poetic All and Multi(OPUS) configurations. HowWe evaluate whether style transfer techniques could help attenuate the shortcomings of translation mod- ever, it is surprising that even a model trained on 6M parallel lines from OPUS(100) performs worse els trained on non-poetic data. We use the romantic than models trained on in-domain data that is 35X poetry style transfer model provided by Krishna et al. (2020) to paraphrase our non-poetic transla- smaller. tions. This is the only available poetic style transfer Table 9 shows how model fine-tuned multimodel to our knowledge.To control for faithful- lingually on non-poetic data suffer from loss of ness, we generate 20 outputs for each input (i.e., metaphoric expression in poetry, while a model non-poetic translations) using nucleus sampling fine-tuned multilingually on Poetic data is able to (p = 0.6), we then select the sentence that has the capture it. Table 10 shows how every model except highest similarity score with input using the SIM our best"
2021.emnlp-main.577,W19-7301,0,0.0354502,"Missing"
2021.emnlp-main.577,2020.acl-main.703,0,0.0999319,"Missing"
2021.emnlp-main.577,2020.acl-main.68,0,0.0142866,"a tombe. Le long des murs de la prison On ne voyait aucune tombe For where a grave had opened wide, There was no grave at all: Only a stretch of mud and sand By the hideous prison-wall, But there was only sand and mud. To where the grave was laid. Along the walls of prison wall. We saw no masquerade There was only sand and mud Where the grave had opened. Along the prison walls No tomb could be seen. Table 1: A French poem accompanied by human translation, (Ghazvininejad et al., 2018) (GCK) system translation, and Google Translate.2 et al., 2021; Van de Cruys, 2020; Ghazvininejad et al., 2016; Li et al., 2020; Hämäläinen and Alnajjar, 2019; Yi et al., 2020; Deng et al., 2019; Chen et al., 2019; Yang et al., 2018), research on poetry translation is in its infancy (Ghazvininejad et al., 2018; Genzel et al., 2010). For example, Ghazvininejad et al. (2018) em1 Introduction ploys a constrained decoding technique to maintain rhyme in French to English poetry translaAmerican poet Robert Frost once defined poetry tion. However, while keeping the poetic style and as “that which gets lost out of both prose and verse fluency, the translation might diverge in terms of in translation” (Frost, 1961). Indeed, th"
2021.emnlp-main.577,2020.tacl-1.47,0,0.25399,"om/ https://www.poetryinternational.org/ https://lyricstranslate.com/ https://digitaldante.columbia.edu/ https://www.poetryinternational.org/ https://lyricstranslate.com/ https://www.poetryinternational.org/ https://lyricstranslate.com/ Train Valid Test 37,746 2059 536 50,001 4186 548 15,199 699 140 17,000 1,050 1295 34,534 1,997 528 23,403 1,000 159 Table 2: Dataset source and statistics collect a multilingual parallel corpus consisting of more than 190,000 lines of poetry spanning over six languages. We try to tackle the second challenge by leveraging multilingual pre-training (e.g., mBART (Liu et al., 2020)) and multilingual finetuning (Tang et al., 2020; Aharoni et al., 2019) that have recently led to advances in neural machine translation for low-resource languages. Moreover, it has been shown that adaptive pre-training and/or fine-tuning on in-domain data always lead to improved performance on the end task (Gururangan et al., 2020). Since poetry translation falls into the lowresource (no or little parallel data) and in-domain translation scenarios, we present an empirical investigation on whether advances in these areas bring us a step closer to poetry translation systems that don’t go far of"
2021.emnlp-main.577,W19-7302,0,0.0125082,"serve figurative language in translation across multiple languages. 9 Conclusion and Future Work Besacier and Schwartz (2015) conduct a pilot study We release poetic parallel corpora for 6 language of how suitable an MT+PE (machine translation + pairs. Our work shows the clear benefit of domain post-editing) pipeline would be for literary trans- adaptation for poetry translation. It further shows lation, concluding that their SMT approach could that improvements can be achieved by leveragproduce “acceptable and rather readable” transla- ing multilingual fine-tuning, and that the improvetions. Matusov (2019) found that adapting NMT ments transfer to unseen languages. Future direcsystems to literary content leads to improved auto- tions include addition of new languages and larger matic evaluation metrics on literary prose as com- corpora, adapting low-resource machine translapared to general domain NMT systems. Kuzman tion techniques for poetry translation, translating et al. (2019) found that Goolge NMT outperformed to languages that are morphologically richer than bespoke NMT models tailored to literature for English, as well as working on better evaluation English-Slovene literary translations"
2021.emnlp-main.577,P02-1040,0,0.112749,"(e.g., Ru-En, Es-En, It-En) on poetic data described in Section 2.1. • Poetic All: multilingually fine-tuned mBARTlarge-50-many-to-one on all poetic data combined. • Poetic LangFamily: multilingually finetuned mBART-large-50-many-to-one on poetic data for all languages belonging to the same language family. For instance, Pt, Es, It belong to the Romance language family, while De and Nl are both Germanic languages. 4.1 Automatic Evaluation Setup For the automatic evaluation, we compare the performance of all the above mentioned models in terms of three metrics: BLEU, BERTScore and COMET. BLEU (Papineni et al., 2002) is one of the most widely used automatic evaluation metrics for Machine Translation. We use the SacreBLEU (Post, 2018) python library to compute BLEU scores between the system output and the human written gold reference. BERTScore (Zhang et al., 2019) has been used recently for evaluating text generation systems us4 Experimental Setting ing contextualized embeddings, and it is said to somewhat ameliorate the problems with BLEU. We experiment with several systems to evaluate BERTScore also has better correlation with human performance across several dimensions: poetic vs non-poetic data; multi"
2021.emnlp-main.577,W18-6319,0,0.0123829,"-one on all poetic data combined. • Poetic LangFamily: multilingually finetuned mBART-large-50-many-to-one on poetic data for all languages belonging to the same language family. For instance, Pt, Es, It belong to the Romance language family, while De and Nl are both Germanic languages. 4.1 Automatic Evaluation Setup For the automatic evaluation, we compare the performance of all the above mentioned models in terms of three metrics: BLEU, BERTScore and COMET. BLEU (Papineni et al., 2002) is one of the most widely used automatic evaluation metrics for Machine Translation. We use the SacreBLEU (Post, 2018) python library to compute BLEU scores between the system output and the human written gold reference. BERTScore (Zhang et al., 2019) has been used recently for evaluating text generation systems us4 Experimental Setting ing contextualized embeddings, and it is said to somewhat ameliorate the problems with BLEU. We experiment with several systems to evaluate BERTScore also has better correlation with human performance across several dimensions: poetic vs non-poetic data; multilingual fine-tuning vs. bilin- judgements (Zhang et al., 2019). It computes a similarity score using contextual embeddi"
2021.emnlp-main.577,D19-1339,0,0.0280152,"glish- tributions spark interest in the machine translation Catalan translation of novels, finding that domain- community to take up this rather challenging task. specific models lead to performance improvements Additionally, by open-sourcing our work we hope judging by all evaluation techniques. Fonteyne et al. to provide a helpful resource for professional trans(2020) conduct a document-level evaluation of the lators. 7261 Ethical Considerations Although we use language models trained on data collected from the Web, which have been shown to have issues with gender bias and abusive language (Sheng et al., 2019; Wallace et al., 2019) even in the multilingual space (Zhao et al., 2020), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, mBART is a conditional language model, which provides more control of the generated output. Our poetic parallel corpora are unlikely to contain toxic text and underwent manual inspection by the authors. Technological advances in machine translation have had both positive and negative effects. Translation technology can diminish translators’ professional autonomy as well as endanger professional translators’ li"
2021.emnlp-main.577,tiedemann-2012-parallel,0,0.00891133,"Evening and the glow stick’s in your eyes and you are looking its orange snapped into your eyes the liquid light Table 3: Parallel Poetic translations written by humans from our multilingual datasets. the number of lines from the original poems. We collect approximately 190K (with 177K in training) parallel poetic lines spanning 6 different languages (see Table 3 for examples). This data is further split into train and validation. 2.2 Non-Poetic Training Data We also benchmark the quality of poetry translations obtained by models trained on non-poetic data. For this we rely on OPUS100 corpus (Tiedemann, 2012) as well as the ML50 corpus(Tang et al., 2020) designed to demonstrate the impact of multilingual fine-tuning. Each of the language pairs in OPUS100 have 1 million parallel sentences in their training set, several orders of magnitude larger than our poetic parallel data. For example, Portuguese-English non-poetic data is 65 times larger than the poetic data, while the RussianEnglish non-poetic data is 18 times larger than the poetic data. The size of the smallest non-poetic parallel corpus is about 6 times larger than all our poetic parallel data combined. For ML50 (Tang et al., 2020), benchma"
2021.emnlp-main.577,2020.acl-main.223,0,0.0770139,"Missing"
2021.emnlp-main.577,2021.naacl-main.92,0,0.0256311,"uch as the use of figurative language and style. pair. For the recently developed metric COMET, We conduct human evaluation by recruiting three we see that the best models are the multilingually bilingual speakers as volunteers for each language. fine-tuned poetic models, which is consistent with NMT systems are susceptible to producing highly the results obtained using the other two metrics. pathological translations that are completely unreHowever, when comparing the bilingually finelated to the source input often termed as halluci- tuned models (Poetic vs. Non-Poetic Bi(Opus)) the nations (Raunak et al., 2021)(e.g., the word Lungs pattern is not as clear based on automatic metrics. in Table 10). To account for these effects, we use We see comparable performance, but not a clear faithfulness as a measure that combines both mean- winner across languages and metrics. However, as ing preservation and poetic style. with the multilingual case, the size of Poetic data We evaluate the best translations from multilin- is much smaller than the Non-Poetic data (20X gual models trained on poetic and non-poetic data. to 50X smaller depending on the language). We Human judges were asked to evaluate on a binary a"
2021.emnlp-main.577,D19-1221,0,0.012924,"ark interest in the machine translation Catalan translation of novels, finding that domain- community to take up this rather challenging task. specific models lead to performance improvements Additionally, by open-sourcing our work we hope judging by all evaluation techniques. Fonteyne et al. to provide a helpful resource for professional trans(2020) conduct a document-level evaluation of the lators. 7261 Ethical Considerations Although we use language models trained on data collected from the Web, which have been shown to have issues with gender bias and abusive language (Sheng et al., 2019; Wallace et al., 2019) even in the multilingual space (Zhao et al., 2020), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, mBART is a conditional language model, which provides more control of the generated output. Our poetic parallel corpora are unlikely to contain toxic text and underwent manual inspection by the authors. Technological advances in machine translation have had both positive and negative effects. Translation technology can diminish translators’ professional autonomy as well as endanger professional translators’ livelihood. Moreover, giv"
2021.emnlp-main.577,2020.emnlp-main.213,0,0.0363561,"for gual fine-tuning; language-family-specific models each token in the system output with each token in vs. mixed-language-family models. the reference. We report F1-Score of BERTScore. • Non-Poetic Bi (OPUS): fine-tuned We use the latest implementation to date which remBART50 on Non-Poetic data from places BERT with deberta-large-mnli, which is a 7256 DeBERTa model (He et al., 2020) fine-tuned on MNLI (Williams et al., 2017). Recently Kocmi et al. (2021) criticized the use of BLEU through a systematic study of 4380 machine translation systems and recommend use of a pre-trained metric COMET (Rei et al., 2020). COMET leverages recent breakthroughs in crosslingual pre-trained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. We rely on the recommended model wmt-large-da-estimator-1719, which is trained to minimize the mean squared error between the predicted scores and the DA (Graham et al., 2013) quality assessments. Notice that these scores are normalized per annotator and hence not bounded between 0 and 1, allowing nega"
2021.emnlp-main.577,2020.emnlp-main.474,0,0.0312391,"Missing"
2021.emnlp-main.577,P16-1009,0,0.0366689,"r NMT in two groups: data centric and model centric. Data centric techniques mostly focus on data augmentation for limited parallel corpora of low-resource languages. For example, Currey et al. (2017) propose copying the target data to Poetic and literary translation Jones and Irvine the source side to incorporate monolingual training (2013) discuss the difficulties of faithful machine data for low-resource languages. Back-translation translation of literary text in terms of the compethas been used for synthetic parallel corpora gen- ing objectives of staying faithful to the original eration (Sennrich et al., 2016). To improve per- text but, on the other hand, trying to convey the formance on specific domains, Chu et al. (2017) experience of reading a literary piece to the reader. 7260 Original: Met steeds speelser gemak sla ik de aanvallen af op mijn zwaarbevochten onverschilligheid. De hemelen druipen af, met pracht en al, de bomen laten moedeloos hangen hun fonkelend loof. Geen oog, geen oor. Een brede glimlach. Je brengt wijn en jezelf: mijn laatste zwakheden. Ik zal mij verzadigen tot herhaling ongewenst wordt, en het vuil nog eenmaal van mijn harde handen spoelen. Gold: With ever greater playful e"
2021.emnlp-main.577,P19-1427,0,0.0224033,"ns. This is the only available poetic style transfer Table 9 shows how model fine-tuned multimodel to our knowledge.To control for faithful- lingually on non-poetic data suffer from loss of ness, we generate 20 outputs for each input (i.e., metaphoric expression in poetry, while a model non-poetic translations) using nucleus sampling fine-tuned multilingually on Poetic data is able to (p = 0.6), we then select the sentence that has the capture it. Table 10 shows how every model except highest similarity score with input using the SIM our best poetic model fine-tuned multilingually sufmodel by Wieting et al. (2019). fer from hallucinations. The Non-Poetic model, 7259 Original: Люблю я пышное природы увяданье, В багрец и в золото одетые леса, В их сенях ветра шум и свежее дыханье, И мглой волнистою покрыты небеса, И редкий солнца луч, и первые морозы, И отдаленные седой зимы угрозы. Gold: I love the lavish withering of nature, The gold and scarlet raiment of the woods, The crisp wind rustling o'er their threshold, The sky engulfed by tides of rippled gloom, The sun's scarce rays, approaching frosts, And gray-haired winter threatening from afar. Poetic All: I love the luxuriant decay of nature, The forest"
2021.emnlp-main.577,D18-1430,0,0.0273548,"re was no grave at all: Only a stretch of mud and sand By the hideous prison-wall, But there was only sand and mud. To where the grave was laid. Along the walls of prison wall. We saw no masquerade There was only sand and mud Where the grave had opened. Along the prison walls No tomb could be seen. Table 1: A French poem accompanied by human translation, (Ghazvininejad et al., 2018) (GCK) system translation, and Google Translate.2 et al., 2021; Van de Cruys, 2020; Ghazvininejad et al., 2016; Li et al., 2020; Hämäläinen and Alnajjar, 2019; Yi et al., 2020; Deng et al., 2019; Chen et al., 2019; Yang et al., 2018), research on poetry translation is in its infancy (Ghazvininejad et al., 2018; Genzel et al., 2010). For example, Ghazvininejad et al. (2018) em1 Introduction ploys a constrained decoding technique to maintain rhyme in French to English poetry translaAmerican poet Robert Frost once defined poetry tion. However, while keeping the poetic style and as “that which gets lost out of both prose and verse fluency, the translation might diverge in terms of in translation” (Frost, 1961). Indeed, the task is meaning w.r.t. the input. Table 1 shows how the so complex that translators often have to “creat"
2021.emnlp-main.577,2020.acl-main.260,0,0.0253137,"ation of novels, finding that domain- community to take up this rather challenging task. specific models lead to performance improvements Additionally, by open-sourcing our work we hope judging by all evaluation techniques. Fonteyne et al. to provide a helpful resource for professional trans(2020) conduct a document-level evaluation of the lators. 7261 Ethical Considerations Although we use language models trained on data collected from the Web, which have been shown to have issues with gender bias and abusive language (Sheng et al., 2019; Wallace et al., 2019) even in the multilingual space (Zhao et al., 2020), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, mBART is a conditional language model, which provides more control of the generated output. Our poetic parallel corpora are unlikely to contain toxic text and underwent manual inspection by the authors. Technological advances in machine translation have had both positive and negative effects. Translation technology can diminish translators’ professional autonomy as well as endanger professional translators’ livelihood. Moreover, given the fact that most people resort to models train"
2021.findings-acl.297,2020.scil-1.10,1,0.831546,"Missing"
2021.findings-acl.297,P18-2103,0,0.0209024,"arious types of figurative language, they fail on cases where the interpretation relies on pragmatic inference and reasoning about world knowledge. We release the code and the data. 1 2 Related Work We follow recent work that test for an expanded range of inference patterns in RTE systems (Bernardy and Chatzikyriakidis, 2019) by evaluating how well RTE models capture specific linguistic phenomena, such as pragmatic inferences (Jeretic et al., 2020), veridicality (Ross and Pavlick, 2019), and others (Pavlick and CallisonBurch, 2016; White et al., 2017; Dasgupta et al., 2018; Naik et al., 2018; Glockner et al., 2018; Kim et al., 2019; Kober et al., 2019; Richardson et al., 2020; Yanaka et al., 2020; Vashishtha et al., 2020; Poliak, 2020). We are not the first to explore figurative language in RTE. Agerri (2008) analyze examples in the Pascal RTE-1 (Dagan et al., 2006) and RTE-2 (BarHaim et al., 2006) datasets that require understanding metaphors and Agerri et al. (2008) present an approach for RTE systems to process metaphors. Poliak et al. (2018)’s diverse collection of RTE datasets includes examples based on figurative language, but focuses only on identifying puns. 3 Dataset Creation We create RTE tes"
2021.findings-acl.297,P16-1018,0,0.0170908,"y with their respective antonyms and use the original (Literal, Simile) pairs as Entailed. For instance, in the case of an existing contexthypothesis pair expressing Entailment - “Hitler skittered off like an enthusiastic sloth” → “Hitler skittered off slowly&quot; - we alter “slowly&quot; to “fast&quot; to make it a pair of Not-Entailment (NE) instance. 3.2 Metaphor Metaphors express deep feelings and complex attitudes (Veale et al., 2016). Understanding metaphors requires comprehending abstract concepts and making connections between seemingly unrelated ideas to appropriately deviate from literal meaning (Gutierrez et al., 2016; Mohammad et al., 2016; Kintsch and Bowles, 2002; Glucksberg, 1998).When generating metaphoric paraphrases, Chakrabarty et al. (2021) create a diverse test set of 150 literal sentences curated from different domains and genres and asked two expert annotators to create metaphorical sentences, resulting in a total 2 Note, such re-framing task (content generation task) does not involve assigning a label to a text fragment, thus, computing inter-annotator agreement is not applicable here. 3355 Genre PairID Slate 143311e Example I Praise from a stranger is like a glass of water served at a restaur"
2021.findings-acl.297,2020.acl-main.768,0,0.0121615,"l RTE models capture these aspects of figurative language. Our results demonstrate that, although, systems trained on a popular RTE dataset may capture some aspects of various types of figurative language, they fail on cases where the interpretation relies on pragmatic inference and reasoning about world knowledge. We release the code and the data. 1 2 Related Work We follow recent work that test for an expanded range of inference patterns in RTE systems (Bernardy and Chatzikyriakidis, 2019) by evaluating how well RTE models capture specific linguistic phenomena, such as pragmatic inferences (Jeretic et al., 2020), veridicality (Ross and Pavlick, 2019), and others (Pavlick and CallisonBurch, 2016; White et al., 2017; Dasgupta et al., 2018; Naik et al., 2018; Glockner et al., 2018; Kim et al., 2019; Kober et al., 2019; Richardson et al., 2020; Yanaka et al., 2020; Vashishtha et al., 2020; Poliak, 2020). We are not the first to explore figurative language in RTE. Agerri (2008) analyze examples in the Pascal RTE-1 (Dagan et al., 2006) and RTE-2 (BarHaim et al., 2006) datasets that require understanding metaphors and Agerri et al. (2008) present an approach for RTE systems to process metaphors. Poliak et a"
2021.findings-acl.297,S19-1026,1,0.895618,"Missing"
2021.findings-acl.297,W19-0409,0,0.0409999,"Missing"
2021.findings-acl.297,2021.ccl-1.108,0,0.0896758,"Missing"
2021.findings-acl.297,S16-2003,0,0.0259135,"antonyms and use the original (Literal, Simile) pairs as Entailed. For instance, in the case of an existing contexthypothesis pair expressing Entailment - “Hitler skittered off like an enthusiastic sloth” → “Hitler skittered off slowly&quot; - we alter “slowly&quot; to “fast&quot; to make it a pair of Not-Entailment (NE) instance. 3.2 Metaphor Metaphors express deep feelings and complex attitudes (Veale et al., 2016). Understanding metaphors requires comprehending abstract concepts and making connections between seemingly unrelated ideas to appropriately deviate from literal meaning (Gutierrez et al., 2016; Mohammad et al., 2016; Kintsch and Bowles, 2002; Glucksberg, 1998).When generating metaphoric paraphrases, Chakrabarty et al. (2021) create a diverse test set of 150 literal sentences curated from different domains and genres and asked two expert annotators to create metaphorical sentences, resulting in a total 2 Note, such re-framing task (content generation task) does not involve assigning a label to a text fragment, thus, computing inter-annotator agreement is not applicable here. 3355 Genre PairID Slate 143311e Example I Praise from a stranger is like a glass of water served at a restaurant in: You drink it wa"
2021.findings-acl.297,C18-1198,0,0.0283222,"e some aspects of various types of figurative language, they fail on cases where the interpretation relies on pragmatic inference and reasoning about world knowledge. We release the code and the data. 1 2 Related Work We follow recent work that test for an expanded range of inference patterns in RTE systems (Bernardy and Chatzikyriakidis, 2019) by evaluating how well RTE models capture specific linguistic phenomena, such as pragmatic inferences (Jeretic et al., 2020), veridicality (Ross and Pavlick, 2019), and others (Pavlick and CallisonBurch, 2016; White et al., 2017; Dasgupta et al., 2018; Naik et al., 2018; Glockner et al., 2018; Kim et al., 2019; Kober et al., 2019; Richardson et al., 2020; Yanaka et al., 2020; Vashishtha et al., 2020; Poliak, 2020). We are not the first to explore figurative language in RTE. Agerri (2008) analyze examples in the Pascal RTE-1 (Dagan et al., 2006) and RTE-2 (BarHaim et al., 2006) datasets that require understanding metaphors and Agerri et al. (2008) present an approach for RTE systems to process metaphors. Poliak et al. (2018)’s diverse collection of RTE datasets includes examples based on figurative language, but focuses only on identifying puns. 3 Dataset Cre"
2021.findings-acl.297,P16-1204,0,0.05753,"Missing"
2021.findings-acl.297,P17-1155,0,0.0632876,"Missing"
2021.findings-acl.297,D14-1162,0,0.0937727,"Missing"
2021.findings-acl.297,2020.eval4nlp-1.10,1,0.768935,"orld knowledge. We release the code and the data. 1 2 Related Work We follow recent work that test for an expanded range of inference patterns in RTE systems (Bernardy and Chatzikyriakidis, 2019) by evaluating how well RTE models capture specific linguistic phenomena, such as pragmatic inferences (Jeretic et al., 2020), veridicality (Ross and Pavlick, 2019), and others (Pavlick and CallisonBurch, 2016; White et al., 2017; Dasgupta et al., 2018; Naik et al., 2018; Glockner et al., 2018; Kim et al., 2019; Kober et al., 2019; Richardson et al., 2020; Yanaka et al., 2020; Vashishtha et al., 2020; Poliak, 2020). We are not the first to explore figurative language in RTE. Agerri (2008) analyze examples in the Pascal RTE-1 (Dagan et al., 2006) and RTE-2 (BarHaim et al., 2006) datasets that require understanding metaphors and Agerri et al. (2008) present an approach for RTE systems to process metaphors. Poliak et al. (2018)’s diverse collection of RTE datasets includes examples based on figurative language, but focuses only on identifying puns. 3 Dataset Creation We create RTE test sets that focus on similes, metaphors, and irony. We provide further background for these types of figurative language and"
2021.findings-acl.297,W18-5441,1,0.896715,"Missing"
2021.findings-acl.297,D19-1228,0,0.0236343,"figurative language. Our results demonstrate that, although, systems trained on a popular RTE dataset may capture some aspects of various types of figurative language, they fail on cases where the interpretation relies on pragmatic inference and reasoning about world knowledge. We release the code and the data. 1 2 Related Work We follow recent work that test for an expanded range of inference patterns in RTE systems (Bernardy and Chatzikyriakidis, 2019) by evaluating how well RTE models capture specific linguistic phenomena, such as pragmatic inferences (Jeretic et al., 2020), veridicality (Ross and Pavlick, 2019), and others (Pavlick and CallisonBurch, 2016; White et al., 2017; Dasgupta et al., 2018; Naik et al., 2018; Glockner et al., 2018; Kim et al., 2019; Kober et al., 2019; Richardson et al., 2020; Yanaka et al., 2020; Vashishtha et al., 2020; Poliak, 2020). We are not the first to explore figurative language in RTE. Agerri (2008) analyze examples in the Pascal RTE-1 (Dagan et al., 2006) and RTE-2 (BarHaim et al., 2006) datasets that require understanding metaphors and Agerri et al. (2008) present an approach for RTE systems to process metaphors. Poliak et al. (2018)’s diverse collection of RTE d"
2021.findings-acl.297,S18-1005,0,0.0473921,"Missing"
2021.findings-acl.297,2020.findings-emnlp.363,1,0.857866,"Missing"
2021.findings-acl.297,I17-1100,0,0.061315,"Missing"
2021.findings-acl.297,N18-1101,0,0.0193676,"ronic tweets (training and test) released by Van Hee et al. (2018) to generate 4,598 RTE pairs. Akin to Poliak et al. (2018), we 3356 Testset Model NBoW InferSent RoBERTa-large Simile Metaphor 51.17 55.01 85.47 54.81 65.75 88.09 sm − im 86.37 71.62 94.76 IMeaning SIGN2000 71.50 68.84 93.42 IIntent 61.72 11.72 52.81 Table 4: Accuracy of different models on our datasets focusing on similes, metaphors, and irony. replace Name with names sampled from a distribution of names based on the US census data.3 . The templates are a) Name tweeted that tweet, b) Name was ironic. 4 Experimental Setup MNLI (Williams et al., 2018) is one of the widely used large-scale corpora that contains instances of figurative language (Table 3). Following recent work, we evaluate RTE models trained on MNLI (Williams et al., 2018) using three standard neural models: bag of words (NBoW) model, InferSent (Conneau et al., 2017), and RoBERTalarge (Liu et al., 2019). In NBoW, word embeddings for contexts and hypotheses are averaged separately, and their concatenation is passed to a logistic regression softmax classifier. InferSent encodes the context and hypotheses independently using a BiLSTM, then their sentence representations are fed"
2021.findings-acl.297,2020.acl-main.543,0,0.0111525,"s on pragmatic inference and reasoning about world knowledge. We release the code and the data. 1 2 Related Work We follow recent work that test for an expanded range of inference patterns in RTE systems (Bernardy and Chatzikyriakidis, 2019) by evaluating how well RTE models capture specific linguistic phenomena, such as pragmatic inferences (Jeretic et al., 2020), veridicality (Ross and Pavlick, 2019), and others (Pavlick and CallisonBurch, 2016; White et al., 2017; Dasgupta et al., 2018; Naik et al., 2018; Glockner et al., 2018; Kim et al., 2019; Kober et al., 2019; Richardson et al., 2020; Yanaka et al., 2020; Vashishtha et al., 2020; Poliak, 2020). We are not the first to explore figurative language in RTE. Agerri (2008) analyze examples in the Pascal RTE-1 (Dagan et al., 2006) and RTE-2 (BarHaim et al., 2006) datasets that require understanding metaphors and Agerri et al. (2008) present an approach for RTE systems to process metaphors. Poliak et al. (2018)’s diverse collection of RTE datasets includes examples based on figurative language, but focuses only on identifying puns. 3 Dataset Creation We create RTE test sets that focus on similes, metaphors, and irony. We provide further background fo"
2021.findings-acl.347,2020.lrec-1.879,1,0.907959,"2013; Narasimhan et al., 2014; Eskander et al., 2016, 2018, 2019). In this work, we show how linguistic priors effectively boost morphological-segmentation performance in a minimally-supervised manner that does not require segmented words for training. We integrate our priors within Adaptor Grammars (Johnson et al., 2007), a type of nonparametric Bayesian models that generalize Probabilistic Context-Free Grammars (PCFGs). Adaptor Grammars have proved successful for unsupervised morphological segmentation, achieving state-of-the-art results across a variety of typologically diverse languages (Eskander et al., 2020). We introduce two types of linguistic priors: 1) grammar definition, where we design a languagespecific grammar that is tailored for the language of interest by modeling specific morphological phenomena, and 2) linguist-provided affixes, where an expert in the underlying language compiles a list of carefully selected affixes and seeds it into the grammars prior to training the segmentation model. We use Japanese and Georgian as case studies for priors 1 and 2, respectively. As our goal is to develop a robust approach that benefits low-resource and/or endangered languages of high morphological"
2021.findings-acl.347,W19-4222,1,0.807706,"vised and 1 The training and evaluation datasets, linguistic priors and models for both Japanese and Georgian are available at https://github.com/rnd2110/MorphAGram/data. 2 https://github.com/rnd2110/MorphAGram 3969 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3969–3974 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Language-independent PrStSu+SM grammar (left side) vs. its Japanese cognate (right side) minimally-supervised morphological segmentation, outperforming the competing discriminative models (Sirts and Goldwater, 2013; Eskander et al., 2019, 2020). Adaptor Grammars are non-parametric Bayesian models that are composed of two main components: 1) a Probabilistic Context-Free Grammar (PCFG) whose definition relies on the underlying task (in the case of morphological segmentation, a PCFG models word structure); and 2) an adaptor that is based on the Pitman-Yor process (Pitman, 1995). The adaptor keeps the posterior probability of a subtree proportional to the number of times that subtree is utilized to parse the input data and manages the caching of the subtrees. The learning process is Markov Chain Monte Carlo sampling (MCMC) (Andri"
2021.findings-acl.347,C16-1086,1,0.906633,"a Probabilistic Context-Free Grammar (PCFG) whose definition relies on the underlying task (in the case of morphological segmentation, a PCFG models word structure); and 2) an adaptor that is based on the Pitman-Yor process (Pitman, 1995). The adaptor keeps the posterior probability of a subtree proportional to the number of times that subtree is utilized to parse the input data and manages the caching of the subtrees. The learning process is Markov Chain Monte Carlo sampling (MCMC) (Andrieu et al., 2003) that does the inference of the PCFG probabilities and the hyperparameters of the model. Eskander et al. (2016) define a set of languageindependent grammars and three learning settings for Adaptor Grammars: 1) Standard, fully unsupervised; 2) Scholar-Seeded, minimally-supervised by manually seeding affixes into the grammar prior to training the segmentation model, and 3) Cascaded, fully unsupervised by approximating the ScholarSeeded setting using automatically generated affixes from an initial round of learning. We next present two ways of including linguistic priors in Adaptor Grammars: 1) defining a language-specific grammar; and 2) using linguist-provided affixes in the Scholar-Seeded learning setu"
2021.findings-acl.347,D14-1095,0,0.0201565,"Missing"
2021.findings-acl.347,N09-1024,0,0.0336939,"tation models for Japanese in the Standard (STD) and Cascaded (CAS)5 settings, both with generic and language-specific (LS) grammar definitions. For Georgian, we evaluate our morphologicalsegmentation models in the Standard (STD), Cascaded (CAS) and Scholar-Seeded (SS) settings, in addition to the proposed Scholar-Seeded setting with linguist-provided affixes (SS-Ling). We perform the evaluation in a transductive manner, where the unsegmented words in the gold standard are part of the training sets; this is common in evaluating unsupervised and minimally-supervised morphological segmentation (Poon et al., 2009; Sirts and Goldwater, 2013; Narasimhan et al., 2014; Eskander et al., 2016, 2019, 2020). For the metrics, we use Boundary Precision and Recall (BPR) and EMMA-2 (Virpioja et al., 2011). BPR is the classical metric for evaluating morphological segmentation; it compares the boundaries in the proposed segmentation to those in the reference. EMMA-2 5 For the Cascaded setup, we use the high-precision grammar PrStSu2a+SM defined by Eskander et al. (2016) as the base grammar. 3971 Language Japanese Georgian Setting Prec. BPR Recall F1-Score Prec. EMMA-2 Recall F1-Score Morfessor AG STD AG CAS AG STD-"
2021.findings-acl.347,E14-4017,0,0.0160353,"Missing"
2021.findings-acl.347,Q13-1021,0,0.0875462,"oved successful for unsupervised and 1 The training and evaluation datasets, linguistic priors and models for both Japanese and Georgian are available at https://github.com/rnd2110/MorphAGram/data. 2 https://github.com/rnd2110/MorphAGram 3969 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3969–3974 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Language-independent PrStSu+SM grammar (left side) vs. its Japanese cognate (right side) minimally-supervised morphological segmentation, outperforming the competing discriminative models (Sirts and Goldwater, 2013; Eskander et al., 2019, 2020). Adaptor Grammars are non-parametric Bayesian models that are composed of two main components: 1) a Probabilistic Context-Free Grammar (PCFG) whose definition relies on the underlying task (in the case of morphological segmentation, a PCFG models word structure); and 2) an adaptor that is based on the Pitman-Yor process (Pitman, 1995). The adaptor keeps the posterior probability of a subtree proportional to the number of times that subtree is utilized to parse the input data and manages the caching of the subtrees. The learning process is Markov Chain Monte Carlo"
2021.findings-acl.347,W18-5808,1,0.876435,"Missing"
2021.findings-acl.347,J01-2001,0,0.713923,"Missing"
2021.findings-acl.347,N18-1005,0,0.04374,"Missing"
2021.findings-acl.348,2020.acl-main.289,0,0.0390728,"al., 2019; Mohammad et al., 2018). Earlier work, including some of the above, focused on featurebased machine learning models that could leverage emotion lexicons (Mohammad and Turney, 2013)), while recent work explores deep learning models (e.g., Bi-LSTM and BERT) and multi-task learning (Xu et al., 2018; Demszky et al., 2020). However, comparatively few researchers have looked at the semantic roles related to emotion such as the cause, the target or the experiencer, with few exceptions for Chinese (Gui et al., 2016; Chen et al., 2018; Xia and Ding, 2019; Xia et al., 2019; Fan et al., 2020; Wei et al., 2020; Ding et al., 2020), English (Mohammad et al., 2014; Ghazi et al., 2015; Kim and Klinger, 2018; Bostan et al., 2020; Oberländer et al., 2020; Oberländer and Klinger, 2020) and Italian (Russo et al., 2011). We highlight some of these works here and draw connection to our work. Most recent work on emotion-cause detection has been carried out on a Chinese dataset compiled by Gui et al. (2016). This dataset characterizes the emotion and cause detection problems as clause-level pair extraction problem – i.e., of all the clauses in the input, one is selected to contain the expression of an emotion,"
2021.findings-acl.348,2020.emnlp-demos.6,0,0.0962293,"Missing"
2021.naacl-main.230,N19-1213,0,0.0164695,"et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou et al. (2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better studied in settings where researchers have access to some physiological signals (e.g., Zuo e"
2021.naacl-main.230,2020.acl-main.372,0,0.119555,"es the stress classification problem in terms of the author and the time–i.e., a post is labeled stressful only if the Pre-training and fine-tuning are another type of poster themselves is currently expressing stress. transfer learning where multiple tasks are trained in sequence rather than at the same time. Pre-trained Because this dataset is small for training a deep language models are perhaps the most widely used learning model, we also experiment with larger example, where a large neural language model can datasets to provide auxiliary information. We se2896 lect the GoEmotions dataset (Demszky et al., 2020), which consists of 58,009 Reddit comments labeled by crowd workers with one or more of 27 emotions (or Neutral), for its larger size and genre similarity to Dreaddit. In this paper, we refer to the dataset in this form as GoEmotionsall or GoEmotionsA . The authors also published two relabelings of this dataset, achieved by agglomerative clustering: one where labels are clustered together into the Ekman 6 basic emotions (anger, disgust, fear, joy, sadness, surprise, neutral) (Ekman, 1992) (GoEmotionsEkman/E ), and one into simple polarity (positive, negative, ambiguous, neutral) (GoEmotionssen"
2021.naacl-main.230,N19-1423,0,0.0194252,"rning has been successfully applied to many domains across NLP (Sun et al., 2019; Kiperwasser and Ballesteros, 2018; Liu et al., 2019); we are especially interested in instances where it has improved semantic and emotion-related tasks, such as Xu et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou et al. (2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji det"
2021.naacl-main.230,D17-1169,0,0.0326887,"e-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou et al. (2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better studied in settings where researchers have access to some physiological signals (e.g., Zuo et al. (2012); Allen et al. (2014); Al-Shargie et al. (2016); Kumar et al. (2020); Jaiswal et al. (2020)). This work is not as relevant to our task, since we have only text data available when detecting stress from online posts. 3 Data A comparison of all the datasets we use in this work can be seen in Table 1. T"
2021.naacl-main.230,2020.acl-main.740,0,0.014103,"nd emotion-related tasks, such as Xu et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou et al. (2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better studied in settings where researchers have access to s"
2021.naacl-main.230,P18-1031,0,0.0204595,"sted in instances where it has improved semantic and emotion-related tasks, such as Xu et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou et al. (2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better st"
2021.naacl-main.230,2020.lrec-1.187,0,0.0428233,"sk learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better studied in settings where researchers have access to some physiological signals (e.g., Zuo et al. (2012); Allen et al. (2014); Al-Shargie et al. (2016); Kumar et al. (2020); Jaiswal et al. (2020)). This work is not as relevant to our task, since we have only text data available when detecting stress from online posts. 3 Data A comparison of all the datasets we use in this work can be seen in Table 1. The primary dataset we use for this work is Dreaddit (Turcan and McKeown, 2019), a dataset of 3,553 segments of Reddit posts from various support communities where the authors believe posters are likely to express stress. The stress detection problem as expressed in this dataset is a binary classification problem, with crowdsourced annotations aggregated as the majority vote from five ann"
2021.naacl-main.230,2020.louhi-1.16,0,0.0417352,"a et al. (2018); Lin et al. (2017)) to assign labels. Much of the work that has been done on psychological stress detection focuses either on establishing baseline models with little advancement in computational modeling, or on using external information about the text (e.g., author, time of posting, number of replies), which is usually, but not always available and may differ in meaning or importance across platforms and domains. There has also been a substantial amount of work on detecting related mental health concerns such as anxiety (e.g., Shen and Rudzicz (2017); Gruda and Hasan (2019); Jiang et al. (2020)), but these are distinct from the generalized experience of stress. The most similar work to ours is Turcan and McKeown (2019), our prior work publishing a dataset of psychological stress collected from the social media website Reddit and labeled by crowd workers, and presenting baselines with several basic non-neural and BERT-based models on this data. We use this dataset in our current work; however, we focus on exploring interpretable frameworks for this sensitive task and connecting the stress detection task concretely with emotion detection. The models we propose in this work rely on two"
2021.naacl-main.230,Q18-1017,0,0.0177964,"ents to the neural representation learned by models like BERT: multi-task learning and pre-training or fine-tuning. Multi-task learning is an increasingly popular framework in which some parameters in a model are shared between or used to inform multiple different tasks. Hard parameter sharing (Caruana, 1993), the variant we employ, uses some set of parameters as a shared base representation and then allows each task to have some private parameters on top and perform their own separate predictions. Multi-task learning has been successfully applied to many domains across NLP (Sun et al., 2019; Kiperwasser and Ballesteros, 2018; Liu et al., 2019); we are especially interested in instances where it has improved semantic and emotion-related tasks, such as Xu et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target d"
2021.naacl-main.230,D19-6213,1,0.903496,"on focuses either on establishing baseline models with little advancement in computational modeling, or on using external information about the text (e.g., author, time of posting, number of replies), which is usually, but not always available and may differ in meaning or importance across platforms and domains. There has also been a substantial amount of work on detecting related mental health concerns such as anxiety (e.g., Shen and Rudzicz (2017); Gruda and Hasan (2019); Jiang et al. (2020)), but these are distinct from the generalized experience of stress. The most similar work to ours is Turcan and McKeown (2019), our prior work publishing a dataset of psychological stress collected from the social media website Reddit and labeled by crowd workers, and presenting baselines with several basic non-neural and BERT-based models on this data. We use this dataset in our current work; however, we focus on exploring interpretable frameworks for this sensitive task and connecting the stress detection task concretely with emotion detection. The models we propose in this work rely on two types of enhancements to the neural representation learned by models like BERT: multi-task learning and pre-training or fine-t"
2021.naacl-main.230,2020.emnlp-demos.6,0,0.0310945,"Missing"
2021.naacl-main.230,W18-6243,0,0.0230015,"framework in which some parameters in a model are shared between or used to inform multiple different tasks. Hard parameter sharing (Caruana, 1993), the variant we employ, uses some set of parameters as a shared base representation and then allows each task to have some private parameters on top and perform their own separate predictions. Multi-task learning has been successfully applied to many domains across NLP (Sun et al., 2019; Kiperwasser and Ballesteros, 2018; Liu et al., 2019); we are especially interested in instances where it has improved semantic and emotion-related tasks, such as Xu et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou e"
2021.naacl-main.230,zuo-etal-2012-multilingual,0,0.0324675,"2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better studied in settings where researchers have access to some physiological signals (e.g., Zuo et al. (2012); Allen et al. (2014); Al-Shargie et al. (2016); Kumar et al. (2020); Jaiswal et al. (2020)). This work is not as relevant to our task, since we have only text data available when detecting stress from online posts. 3 Data A comparison of all the datasets we use in this work can be seen in Table 1. The primary dataset we use for this work is Dreaddit (Turcan and McKeown, 2019), a dataset of 3,553 segments of Reddit posts from various support communities where the authors believe posters are likely to express stress. The stress detection problem as expressed in this dataset is a binary classifi"
2021.naacl-main.336,N18-2014,0,0.0544286,"Missing"
2021.naacl-main.336,P19-1470,0,0.471899,"arising from perception are used in conceptual tasks such as representing propositions and abstract concepts. Philosopher Susanne Langer in her essay “Expressiveness and Symbolism” stated “A metaphor is not language, it is an idea expressed by language, an idea that in its turn functions as a symbol to express something”. Our approach has two steps: 1) identify a set of sentences that contains metaphorical verbs from an online poetry corpus; 2) convert these metaphorical sentences to their literal versions using Masked Language Models and structured common sense knowledge achieved from COMET (Bosselut et al., 2019), a language model fine-tuned on ConceptNet (Speer et al., 2017). For the later, we exploit the SymbolOf relation to make sure the generated sentence that contains the literal sense of the verb has the same symbol as the metaphorical sentence. For example, for the metaphorical sentence “The turbulent feelings that surged through his soul"" our method will generate “The turbulent feelings that continued through his soul"" maintaining the common symbolic meaning of (love, loss, despair, sorrow, loneliness) between the two (Section 2). • A task-based evaluation to improve the quality of human writt"
2021.naacl-main.336,2020.emnlp-main.524,1,0.542084,"h attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create parallel data and (Liu et al., 2019) to rescore our generated hypothesis during decoding. 7.2 Metaphor Generation Some early works made contributions to use template and heuristic-based methods (Abe et al., 2006; Terai and Nakagawa, 2010) to generate “A is like B” sentences, more popularly referred to as similes. Chakrabarty et al. (2020) concentrated on simile generation, applying seq2seq model to paraphrase a literal sentence into a simile. Other attempts learned from the mappings of different domains and generated conceptual metaphors of pattern “A is B” (Hervás et al., 2007; Mason, 2004; Gero and Chilton, 2019). These works paid attention to the relationship between nouns and concepts to create elementary figurative expressions. Recent metaphor generation works focus mainly on verbs. Yu and Wan (2019) proposed an unsupervised metaphor extraction method, and developed a neural generation model to generate metaphorical sente"
2021.naacl-main.336,P18-1082,0,0.0677038,"Missing"
2021.naacl-main.336,D18-1060,0,0.0119118,"hoose the verb with highest literal probability. Our goal is to see if re-written poems are qualitatively better than the original forms. To do this, we hire Turkers from Amazon Mechanical Turk and present them with hits where the task is to choose the better version between the original Quatrain and the re-written version. 15 Turkers were recruited for the task. Each Quatrain was evaluated by 3 distinct Turkers. Table 7 shows metaphorical transformations by a MERMAID Figure 4 shows that poems rewritten by MERMAID were considered better by the Turkers. With advent of deep learning approaches, Gao et al. (2018) used BiLSTM models based on GloVe (Pennington et al., 2014) and ELMo word vectors (Peters et al., 2018) to detect metaphoric verbs. Inspired by the linguistic theories, MIP (Semino et al., 2007; Steen, 2010) and SPV (Wilks, 1975, 1978), Mao et al. (2019) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create paral"
2021.naacl-main.336,2020.figlang-1.21,0,0.0242976,"s that poems rewritten by MERMAID were considered better by the Turkers. With advent of deep learning approaches, Gao et al. (2018) used BiLSTM models based on GloVe (Pennington et al., 2014) and ELMo word vectors (Peters et al., 2018) to detect metaphoric verbs. Inspired by the linguistic theories, MIP (Semino et al., 2007; Steen, 2010) and SPV (Wilks, 1975, 1978), Mao et al. (2019) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create parallel data and (Liu et al., 2019) to rescore our generated hypothesis during decoding. 7.2 Metaphor Generation Some early works made contributions to use template and heuristic-based methods (Abe et al., 2006; Terai and Nakagawa, 2010) to generate “A is like B” sentences, more popularly referred to as similes. Chakrabarty et al. (2020) concentrated on simile generation, applying seq2seq model to paraphrase a literal sentence into a simile. Other attempts learned from the mappings of d"
2021.naacl-main.336,P18-1152,0,0.0155536,"goal, we fine-tune BART (Lewis et al., 2019), a pre-trained conditional language model that combines bidirectional and autoregressive transformers, on the collected parallel corpora. Specifically, we fine-tune BART by treating the literal input as encoder source and the metaphorical output as the the decoder target (Figure 1). One issue of the pre-trained language models is that they have a tendency to generate literal tokens over metaphorical ones. To overcome this, we introduce a rescoring model during the decoding process to favor more metaphorical verbs. The rescoring model is inspired by Holtzman et al. (2018); Goldfarb-Tarrant et al. (2020) and detailed in the next section. 3.2 Discriminative Decoding We have a base metaphor generation model p(z|x) which is learned by fine-tuning BART (Lewis et al., 2019) on pairs of literal (x) and metaphorical (z) sentences. We propose to modify the decoding objective to incorporate a Metaphor detection rescoring model a and re-rank the base, or “naive"" BART generated hypotheses, bringing the metaphoric representation closer to the rescoring model’s specialty 4253 DECODER TARGET ENCODER TARGET SOURCE BART The tax cut will help the economy Black desert covered in"
2021.naacl-main.336,W13-0907,0,0.072719,"Missing"
2021.naacl-main.336,W14-2302,0,0.0149977,"on of metaphor, while metaphor genera- mask the metaphorical verbs as input, and the origtion is relatively under-studied. inal metaphorical sentences as output. However, this model face challenges in transferring the literal 7.1 Metaphor Detection sentences to metaphorical ones, while maintainFor metaphor detection, researchers focused on ing the same meaning. We, on the contrary, focus variety of features, including unigrams, imageabil- on maintaining the same meaning through parallel ity, sensory features, WordNet, bag-of-words fea- data creation focusing on symbolism. Additionally, tures (Klebanov et al., 2014; Tsvetkov et al., 2014; we incorporate a metaphor detection model as a Shutova et al., 2016; Tekiro˘glu et al., 2015; Hovy discriminator to improve decoding during generaet al., 2013; Köper and im Walde, 2016). tion. 4257 8 Conclusion We show how to transform literal sentences to metaphorical ones. We propose a novel way of creating parallel corpora and an approach for generating metaphors that benefits from transfer learning and discriminative decoding. Human and automatic evaluations show that our best model is successful at generating metaphors. We further show that leveraging symbolic mea"
2021.naacl-main.336,N16-1039,0,0.058103,"Missing"
2021.naacl-main.336,2020.acl-main.703,0,0.0795396,"Missing"
2021.naacl-main.336,2021.ccl-1.108,0,0.0195425,"Missing"
2021.naacl-main.336,P19-1378,0,0.0139253,"tter version between the original Quatrain and the re-written version. 15 Turkers were recruited for the task. Each Quatrain was evaluated by 3 distinct Turkers. Table 7 shows metaphorical transformations by a MERMAID Figure 4 shows that poems rewritten by MERMAID were considered better by the Turkers. With advent of deep learning approaches, Gao et al. (2018) used BiLSTM models based on GloVe (Pennington et al., 2014) and ELMo word vectors (Peters et al., 2018) to detect metaphoric verbs. Inspired by the linguistic theories, MIP (Semino et al., 2007; Steen, 2010) and SPV (Wilks, 1975, 1978), Mao et al. (2019) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create parallel data and (Liu et al., 2019) to rescore our generated hypothesis during decoding. 7.2 Metaphor Generation Some early works made contributions to use template and heuristic-based methods (Abe et al., 2006; Terai and Nakagawa, 2010) to generate “A is lik"
2021.naacl-main.336,J04-1002,0,0.169501,"rbs to create parallel data and (Liu et al., 2019) to rescore our generated hypothesis during decoding. 7.2 Metaphor Generation Some early works made contributions to use template and heuristic-based methods (Abe et al., 2006; Terai and Nakagawa, 2010) to generate “A is like B” sentences, more popularly referred to as similes. Chakrabarty et al. (2020) concentrated on simile generation, applying seq2seq model to paraphrase a literal sentence into a simile. Other attempts learned from the mappings of different domains and generated conceptual metaphors of pattern “A is B” (Hervás et al., 2007; Mason, 2004; Gero and Chilton, 2019). These works paid attention to the relationship between nouns and concepts to create elementary figurative expressions. Recent metaphor generation works focus mainly on verbs. Yu and Wan (2019) proposed an unsupervised metaphor extraction method, and developed a neural generation model to generate metaphorical sentences from literal-metaphorical verb pairs. They however do not focus on literal to metaphorical sentence transfer , but generate a sentence given 7 Related Work a metaphorical fit word. The closest to our work is that of Stowe et al. (2020), who focus on bu"
2021.naacl-main.336,S16-2003,0,0.0229048,"effect of transfer learning from a large generative pre-trained model, which also accounts for context unlike the retrieval based methods. 4.2 Test Data To measure the effectiveness of our approach, we need to evaluate our model on a dataset that is independent of our automatically created parallel data and that is diverse across various domains, genres and types. Hence we rely on test data from multiple sources. As our first source, we randomly sample literal and metaphorical sentences with high confidence (> 0.7) and unique verbs from the existing 4 Experimental Setup dataset introduced by Mohammad et al. (2016). For To compare the quality of the generated metaphors, the metaphorical sentences from Mohammad et al. we benchmark our MERMAID model against human (2016) we convert them to their literal equivalent 4254 the same way as discussed in Section 2.2 without the use of COMET as we do not need it. To ensure diversity in genre, as our second source we scrape W RITING P ROMPT and O CPOETRY subreddits for sentences with length up to 12 words, which are literal in nature based on prediction from our model described in Section 2.1. We collate 500 such sentences combined from all sources and randomly sam"
2021.naacl-main.336,D17-1238,0,0.0119809,"here 1 denotes the worst and 5 be the best. Boldface denotes the best results overall and underscore denotes the best among computational models. evaluate on four dimensions for 900 utterances, we have a total of 3600 evaluations. Each criteria was rated on a likert scale from 1 (not at all) to 5 (very). Each group of utterances was rated by three separate Turkers, resulted in 42, 48, 44 and 53 Turkers for the four evaluation tasks respectively. We pay them at a rate of $15 per hour. Human evaluation. Since automatic evaluation is known to have significant limitations for creative generation (Novikova et al., 2017), we further conduct human evaluation on a total of 900 utterances, 600 generated from 4 systems and 300 generated by the two human experts. We propose a 5 Results set of four criteria to evaluate the generated output: Based on the semantic similarity metric shown in (1) Fluency (Flu) (“How fluent, grammatical, well formed and easy to understand are the generated ut- column 1 of Table 4, our system MERMAID is better in preserving the meaning of the input than terances?”), (2) Meaning (Mea) (“Are the input and the output referring or meaning the same thing?"") the other baselines. As mentioned,"
2021.naacl-main.336,N19-4009,0,0.0221409,"Missing"
2021.naacl-main.336,P02-1040,0,0.109425,"ture who is the author of a novel — to write corresponding metaphors for each of these 150 inputs for evaluation and comparison. 4.3 Evaluation Criteria Automatic evaluation. One important aspect in evaluating the quality of the generated metaphors is whether they are faithful to the input: while we change literal sentences to metaphorical ones, it should still maintain the same denotation as the input. To this end, we calculate the Semantic Similarity between the metaphorical output and the input using sentence-BERT (SBERT) (Reimers and Gurevych, 2019). We also calculate corpus-level BLEU-2 (Papineni et al., 2002) and BERTScore (Zhang et al., 2019) with human written references. System LEXREP META_M BART MERMAID HUMAN1 HUMAN2 Similarity ↑ 79.6 73.2 83.6 85.0 86.6 84.2 BLEU-2↑ 68.7 61.0 65.0 66.7 - BertScore↑ 0.56 0.62 0.65 0.71 - Table 4: Automatic evaluation results on test set where MERMAID significantly outperforms other automatic methods for 2 out of 3 metrics (p &lt; .001) according to approximate randomization test). BLEU-2 and BertScore is calculated w.r.t to Human references (HUMAN1 & HUMAN2). Corpus level BLEU-2 and Semantic Similarity are in range of (0-100) while BertScore is in range (0-1) Sys"
2021.naacl-main.336,D14-1162,0,0.0893441,"goal is to see if re-written poems are qualitatively better than the original forms. To do this, we hire Turkers from Amazon Mechanical Turk and present them with hits where the task is to choose the better version between the original Quatrain and the re-written version. 15 Turkers were recruited for the task. Each Quatrain was evaluated by 3 distinct Turkers. Table 7 shows metaphorical transformations by a MERMAID Figure 4 shows that poems rewritten by MERMAID were considered better by the Turkers. With advent of deep learning approaches, Gao et al. (2018) used BiLSTM models based on GloVe (Pennington et al., 2014) and ELMo word vectors (Peters et al., 2018) to detect metaphoric verbs. Inspired by the linguistic theories, MIP (Semino et al., 2007; Steen, 2010) and SPV (Wilks, 1975, 1978), Mao et al. (2019) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create parallel data and (Liu et al., 2019) to rescore our generated hyp"
2021.naacl-main.336,N18-1202,0,0.00759152,"vely better than the original forms. To do this, we hire Turkers from Amazon Mechanical Turk and present them with hits where the task is to choose the better version between the original Quatrain and the re-written version. 15 Turkers were recruited for the task. Each Quatrain was evaluated by 3 distinct Turkers. Table 7 shows metaphorical transformations by a MERMAID Figure 4 shows that poems rewritten by MERMAID were considered better by the Turkers. With advent of deep learning approaches, Gao et al. (2018) used BiLSTM models based on GloVe (Pennington et al., 2014) and ELMo word vectors (Peters et al., 2018) to detect metaphoric verbs. Inspired by the linguistic theories, MIP (Semino et al., 2007; Steen, 2010) and SPV (Wilks, 1975, 1978), Mao et al. (2019) proposed two detection models consisting of BiLSTM with attention mechanisms that relied on GloVe and ELMo embeddings. Recent work on metaphor detection have also used pretrained language models (Su et al., 2020; Gong et al., 2020). While we focus on metaphor generation , we use (Devlin et al., 2018) to detect metaphoric verbs to create parallel data and (Liu et al., 2019) to rescore our generated hypothesis during decoding. 7.2 Metaphor Genera"
2021.naacl-main.336,D19-1410,0,0.0138251,"uter science who is also a poet, and a student in comparative literature who is the author of a novel — to write corresponding metaphors for each of these 150 inputs for evaluation and comparison. 4.3 Evaluation Criteria Automatic evaluation. One important aspect in evaluating the quality of the generated metaphors is whether they are faithful to the input: while we change literal sentences to metaphorical ones, it should still maintain the same denotation as the input. To this end, we calculate the Semantic Similarity between the metaphorical output and the input using sentence-BERT (SBERT) (Reimers and Gurevych, 2019). We also calculate corpus-level BLEU-2 (Papineni et al., 2002) and BERTScore (Zhang et al., 2019) with human written references. System LEXREP META_M BART MERMAID HUMAN1 HUMAN2 Similarity ↑ 79.6 73.2 83.6 85.0 86.6 84.2 BLEU-2↑ 68.7 61.0 65.0 66.7 - BertScore↑ 0.56 0.62 0.65 0.71 - Table 4: Automatic evaluation results on test set where MERMAID significantly outperforms other automatic methods for 2 out of 3 metrics (p &lt; .001) according to approximate randomization test). BLEU-2 and BertScore is calculated w.r.t to Human references (HUMAN1 & HUMAN2). Corpus level BLEU-2 and Semantic Similarit"
2021.naacl-main.336,D19-1339,1,0.714841,"mapping. 9 Ethics Our data is collected from Reddit and we understand and respect user privacy. Our models are fine-tuned on sentence level data obtained from user posts. These do not contain any explicit detail which leaks information about a users name, health, negative financial status, racial or ethnic origin, religious or philosophical affiliation or beliefs, sexual orientation, trade union membership, alleged or actual commission of crime. Second, although we use language models trained on data collected from the Web, which have been shown to have issues with bias and abusive language (Sheng et al., 2019; Wallace et al., 2019), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, BART is a conditional language model, which provides more control of the generated output. Furthermore, we specifically encode writing style from a poetic corpus in our models and train on parallel data in the direction of literal to metaphorical style. Open-sourcing this technology will help to generate metaphoric text assisting creative writing practitioners or non native language speakers to improve their writing. We do not envision any dual-use that can ca"
2021.naacl-main.336,N16-1020,0,0.0185437,"is relatively under-studied. inal metaphorical sentences as output. However, this model face challenges in transferring the literal 7.1 Metaphor Detection sentences to metaphorical ones, while maintainFor metaphor detection, researchers focused on ing the same meaning. We, on the contrary, focus variety of features, including unigrams, imageabil- on maintaining the same meaning through parallel ity, sensory features, WordNet, bag-of-words fea- data creation focusing on symbolism. Additionally, tures (Klebanov et al., 2014; Tsvetkov et al., 2014; we incorporate a metaphor detection model as a Shutova et al., 2016; Tekiro˘glu et al., 2015; Hovy discriminator to improve decoding during generaet al., 2013; Köper and im Walde, 2016). tion. 4257 8 Conclusion We show how to transform literal sentences to metaphorical ones. We propose a novel way of creating parallel corpora and an approach for generating metaphors that benefits from transfer learning and discriminative decoding. Human and automatic evaluations show that our best model is successful at generating metaphors. We further show that leveraging symbolic meanings helps us learn better abstract representations and better preservation of the denotati"
2021.naacl-main.336,C10-1113,0,0.119001,"Missing"
2021.naacl-main.336,K19-1034,0,0.0223201,"Missing"
2021.naacl-main.336,N19-1092,0,0.16384,"rs allow us to communicate not just information, but also feelings and complex attitudes (Veale et al., 2016). While most computational work has focused on metaphor detection (Gao et al., 2018; Stowe et al., 2019; Shutova et al., 2010; Tsvetkov et al., 2014; Veale et al., 2016; Stowe and Palmer, 2018), research on metaphor generation is ∗ The wildfire spread through the forest at an amazing speed. The wildfire danced through the forest at an amazing speed. The window panes were rattling as the wind blew through them The window panes were trembling as the wind blew through them under-explored (Yu and Wan, 2019; Stowe et al., 2020). Generating metaphors could impact many downstream applications such as creative writing assistance, literary or poetic content creation. Relevant statistics demonstrate that the most frequent type of metaphor is expressed by verbs (Steen, 2010; Martin, 2006). We therefore focus on the task of generating a metaphor starting from a literal utterance (Stowe et al., 2020), where we transform a literal verb to a metaphorical verb. Table 1 shows examples of literal sentences and the generated metaphors. To tackle the metaphor generation problem we need to address three challen"
2021.naacl-main.336,D19-1221,0,0.042146,"ur data is collected from Reddit and we understand and respect user privacy. Our models are fine-tuned on sentence level data obtained from user posts. These do not contain any explicit detail which leaks information about a users name, health, negative financial status, racial or ethnic origin, religious or philosophical affiliation or beliefs, sexual orientation, trade union membership, alleged or actual commission of crime. Second, although we use language models trained on data collected from the Web, which have been shown to have issues with bias and abusive language (Sheng et al., 2019; Wallace et al., 2019), the inductive bias of our models should limit inadvertent negative impacts. Unlike model variants such as GPT, BART is a conditional language model, which provides more control of the generated output. Furthermore, we specifically encode writing style from a poetic corpus in our models and train on parallel data in the direction of literal to metaphorical style. Open-sourcing this technology will help to generate metaphoric text assisting creative writing practitioners or non native language speakers to improve their writing. We do not envision any dual-use that can cause harm for the use of"
2021.naacl-main.394,2021.ccl-1.108,0,0.0405142,"Missing"
2021.naacl-main.394,2020.emnlp-main.602,0,0.0361797,"ent Learning, ensuring that a generated text is logically implied by the ground-truth text. Holtzman et al. (2018) utilize a discriminative model trained on SNLI (Bowman et al., 2015) to complement an RNN generator and guide the decoding process to improve contradictions in generation. Although we experimented with both of these approaches, including the approach of Holtzman et al. (2018) with MNLI to account for entailment in text generation, none of them yielded better results than our method. Other approaches have explored “vocab boosting” (Ghosh et al., 2017) for tasks such as de-biasing (Ma et al., 2020), which involves increasing the values of certain words; however, as these values are on the simplex, the softmax function necessarily decreases the values of other logits which are key to fluency such as function words. The effects of lexical framing have been studied for social and political issues, although our work is the first to use lexical framing in generation for positive framing effects (less partisan, no appeal to fear fallacy). Demszky et al. (2019) and Tyagi et al. (2020) study political polarization and how this manifests in differences in word choice among different groups; Khud"
2021.naacl-main.394,D17-1103,0,0.170463,"but found these techniques to be less effective than our method. As pre-trained sequence-to-sequence language models are good at copying input and generating naturalsounding output, we hypothesize that our approach will better allow us to change connotative meaning without affecting fluency and denotation. In contrast, approaches such as “vocab boosting” (Ghosh et al., 2017) increase the logits of key connotative words, which would necessarily decrease the probabilities of functional words and words necessary for maintaining denotative meaning. Other approaches such as reinforcement learning (Pasunuru and Bansal, 2017) may further decrease these desired qualities, while trying to maximize another objective. 4 Evaluation Tasks and Test Data To evaluate our methods for argument reframing we need to look beyond our automatically labeled data. We consider two tasks: 1) reframing an argument that contains partisan language to a less partisan argument; and 2) reframing an appeal to fear or prejudice fallacy to an argument without this fallacy. Recently Webson et al. (2020), proposed resources and methods to disentangle denotation Post fine-tuning at the decoding step, we use a and connotation in vector spaces. Th"
2021.naacl-main.394,D19-1410,0,0.0162879,"te steps. Our training data for this method includes only arguments labeled with their attribute (e.g., positive or negative). Arguments containing lexical connotations catering to trust are positive, while those not catering to trust are negative. 5.2 Evaluation Criteria Automatic evaluation. One important criterion is to measure if the reframed arguments are faithful to the input. Even though we are changing the argument for connotations, it should still maintain the same denotative meaning as the input. To this end we calculate Semantic Similarity with our input using SENTENCE BERT(SBERT) (Reimers and Gurevych, 2019). ical are the utterances?”), (2) Meaning Preservation (M) (“How well does the reframed argument capture the same denotative meaning as the input argument?”), (3) Trustworthiness/Presence of Fear(T/PF). For the 100 input arguments reflecting partisan view we ask Turkers to rate reframed arguments based on trustworthiness with respect to the input. For the 50 Appeal to Fear or Prejudice fallacies we ask Turkers to rate reframed arguments based on presence of fear (the intention behind this being that we want to rank systems which portray the least amount of fear). In both of these ratings we st"
2021.naacl-main.394,D19-1339,0,0.02126,"cial or ethnic ence on Natural Language Processing (EMNLPorigin, religious or philosophical affiliation or beIJCNLP), pages 2922–2932, Hong Kong, China. Asliefs, sexual orientation, trade union membership, sociation for Computational Linguistics. alleged or actual commission of crime. Emily Allaway and Kathleen McKeown. 2020. A uniSecond, although we use language models fied feature representation for lexical connotations. trained on data collected from the Web, which have been shown to have issues with bias and abusive Aristotle and R. Bartlett. 2019. Aristotle’s &quot;art of rhetoric&quot;. language (Sheng et al., 2019; Wallace et al., 2019), the inductive bias of our models should limit inTal August, Nigini Oliveira, Chenhao Tan, Noah A. advertent negative impacts. Unlike model variSmith, and Katharina Reinecke. 2018. Framing efants such as GPT, BART is a conditional language fects: Choice of slogans used to advertise online experiments can boost recruitment and lead to sample model, which provides more control of the generbiases. PACMHCI, 2(CSCW):22:1–22:19. ated output. We have two levels of control on our generation approach: lexical replacements via con- Samuel R Bowman, Gabor Angeli, Christopher Potts"
2021.naacl-main.394,D19-1322,0,0.254449,"and appears prominently in manipulative text such as propaganda (Da San Martino et al., 2019). On the other hand, arguments with trusted language align with the Aristotelian modes of persuasion, specifically ethos (Aristotle and Bartlett, 2019). In our work, we leverage such a lexical resource for connotations (Allaway and McKeown, 2020) to reframe arguments to be more trustworthy (e.g., less partisan, no appeal to fear fallacy), while maintaining the same denotative meaning. While retrieve-and-replace methods perform well on other attribute transfer tasks such as sentiment (Li et al., 2018a; Sudhakar et al., 2019a), our task is more dependent on broader context within a sentence even though we are performing localized replacement. Thus, there are two main challenges we need to address: 1) the lack of a parallel dataset of negatively and positively framed arguments (naturallyoccurring); and 2) a generation approach that can not only change the connotative meaning but also keep the same denotative meaning of the input argument. We introduce our approach called ENTRUST : ArgumENT Reframing with langUage modelS and enTailment, with the following contributions: 1) A Connotation-guided Masked Language Model"
2021.naacl-main.394,D17-1238,0,0.0118305,".65, 0.51, 0.46, respectively. 6 Results Automatic Evaluation. As can be seen in Table 4 our model ENTRUST maintains the denotative meaning with the input better than other systems (p &lt; .001 using approximate randomization tests) and only marginally behind humans when it comes to arguments with partisan collocations. For Appeal to Fear or Prejudice our system maintains better denotative meaning than all systems except LEXREP (p &lt; .001). The automatic metric somewhat penalizes humans for changing more content than just targeted words; this unreliability is a known issue with automated metrics (Novikova et al., 2017) and strongly implies a need for human evaluation. Human Evaluation. Table 5 shows the results of our human-based evaluations. For fluency, meaning preservation, trustworthiness, and reduction of fear the ENTRUST model is better than all the baselines (p &lt; .001 using approximate randomization tests). It is further encouraging to see that the entailment step helps us maintain better denotative meaning (See Table 5 Col3: Row 4 vs Row Human evaluation. We use Amazon Mechan- 7). For Presence of Fear, Turkers often rate our ical Turk to evaluate on a total of 900 utter- ENTRUST model to be the leas"
2021.naacl-main.394,N19-4009,0,0.0234101,"Missing"
2021.naacl-main.394,P16-2032,0,0.0299561,"rformance on several evaluation criteria such as fluency, meaning, trustworthiness/reduction in fear. Code, data, and models available at https://github.com/ tuhinjubcse/ArgReframingNAACL2021 same denotation but different connotative meaning. Selection of naturally-occurring arguments. Since our goal is to re-write arguments, it is essential to identify an abundant source of naturallyoccurring arguments. The Change My View subreddit, an argumentative discussion forum intended for persuasion on diverse topics, has been used extensively in computational argumentation research (Tan et al., 2016; Wei et al., 2016; Musi et al., 2018; Chakrabarty et al., 2019a,b; Hidey et al., 2017). We collect sentences from the same source and classify them as claim, premise, or non-argument using the fine-tuned BERT model released by Chakrabarty et al. (2019b). This results in 301,166 arguments labeled as premises. We consider only premises to create our parallel data because argumentative appeals occur within justifications (premises) for or against the speaker’s claim. Connotation-guided Masked Language Model. Allaway and McKeown (2020) provide a resource with words labeled for lexical connotations, using the aspec"
2021.naacl-main.394,N18-1101,0,0.0278292,"ut. While in most cases BART is able to generate content which is semantically similar to the input, it sometimes contradicts the input. For example, Table 2 shows that BART changes soft power to military strength. Here the denotative meaning changes. To control for this, we introduce an additional post-processing step. We generate multiple outputs by varying the value of k (between 5 and 50) while conducting topk sampling. We then calculate the entailment scores of these outputs with the input argument respectively using a RoBERTa (Liu et al., 2019) model fine-tuned on the Multi-NLI dataset (Williams et al., 2018) and then select the output having the best entailment score. We also experimented with other methods for incorporating entailment during decoding based on prior work (Section 8), but found these techniques to be less effective than our method. As pre-trained sequence-to-sequence language models are good at copying input and generating naturalsounding output, we hypothesize that our approach will better allow us to change connotative meaning without affecting fluency and denotation. In contrast, approaches such as “vocab boosting” (Ghosh et al., 2017) increase the logits of key connotative wor"
2021.sigdial-1.40,2020.coling-main.540,1,0.78397,"s articles formulated as a sentence classification task and as a sentence ranking task. We showed that providing an argumentative discourse context along with the target sentence when fine-tuning BERT improves over baselines of the target sentence alone or with its local discourse context, especially on the argumentative part of the articles. In future work, we want to compare using the gold annotations of argument structure with predicted argument components and relations by training another model that generate argumentation fea387 tures to be used for the main task as done in previous work (Alhindi et al., 2020). Also, we want to explore the use of other linguistic features tested in previous work and other variations of argumentation context and features such as counts of relations for the target argumentative segment. BERT is pretrained on the next sentence prediction task, which makes an out-of-order argumentation context to be further away from the distribution of the pretraining data. To remedy this, we plan to adaptively pretrain BERT on more argumentation context extracted from multiple argumentation corpora. Finally, we want to study the relation of check-worthiness to intrinsic clause types"
2021.sigdial-1.40,W18-5513,1,0.852213,", 2018; Graves, 2018), the majority of which is work on predicting the veracity of claims either by comparing them against evidence from Wikipedia (Thorne et al., 2018), trusted news outlets (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (Joty et al., 2018), or debate websites (Chen et al., 2019), or by analyzing the linguistic properties of false and true claim 1 The annotated dataset, guidelines, and code are available here: https://github.com/Tariq60/whatToFactcheck (P´erez-Rosas et al., 2018; Rashkin et al., 2017) in addition to the speaker’s history (Wang, 2017; Alhindi et al., 2018). Other work focuses on estimating the credibility of sources by using an external list of bias per publisher (Baly et al., 2018) or by modeling conflicting reports on a claim from different sources (Zhang et al., 2019). However, all of these methods either report bias at the publisher level or start with a list of claims to fact-check. Previous work on detecting check-worthy claims focus on text from the political domain. The two main existing systems for check-worthy claim detection are ClaimBuster (Hassan et al., 2017) and ClaimRank (Jaradat et al., 2018). ClaimBuster is trained on sentence"
2021.sigdial-1.40,D18-1389,0,0.0166055,"from Wikipedia (Thorne et al., 2018), trusted news outlets (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (Joty et al., 2018), or debate websites (Chen et al., 2019), or by analyzing the linguistic properties of false and true claim 1 The annotated dataset, guidelines, and code are available here: https://github.com/Tariq60/whatToFactcheck (P´erez-Rosas et al., 2018; Rashkin et al., 2017) in addition to the speaker’s history (Wang, 2017; Alhindi et al., 2018). Other work focuses on estimating the credibility of sources by using an external list of bias per publisher (Baly et al., 2018) or by modeling conflicting reports on a claim from different sources (Zhang et al., 2019). However, all of these methods either report bias at the publisher level or start with a list of claims to fact-check. Previous work on detecting check-worthy claims focus on text from the political domain. The two main existing systems for check-worthy claim detection are ClaimBuster (Hassan et al., 2017) and ClaimRank (Jaradat et al., 2018). ClaimBuster is trained on sentences from political debates and uses sentence level features such as TF-IDF weights and sentiment. ClaimRank extends this to Arabic"
2021.sigdial-1.40,P13-4021,0,0.0165186,"Missing"
2021.sigdial-1.40,N19-1053,0,0.0131276,"gumentative discourse structure provides a slight but statistically significant improvement over a BERT model that uses just local discourse context (Sections 5 and 6). 2 Related Work Previous work on fact-checking has focused on different steps of the fact-checking pipeline (Thorne and Vlachos, 2018; Graves, 2018), the majority of which is work on predicting the veracity of claims either by comparing them against evidence from Wikipedia (Thorne et al., 2018), trusted news outlets (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (Joty et al., 2018), or debate websites (Chen et al., 2019), or by analyzing the linguistic properties of false and true claim 1 The annotated dataset, guidelines, and code are available here: https://github.com/Tariq60/whatToFactcheck (P´erez-Rosas et al., 2018; Rashkin et al., 2017) in addition to the speaker’s history (Wang, 2017; Alhindi et al., 2018). Other work focuses on estimating the credibility of sources by using an external list of bias per publisher (Baly et al., 2018) or by modeling conflicting reports on a claim from different sources (Zhang et al., 2019). However, all of these methods either report bias at the publisher level or start"
2021.sigdial-1.40,N19-1423,0,0.0273467,"guistics Figure 1: Fact-Checked Segments and Argument Components and Relations in one Article Our contributions in this paper are as follows1 : 1. We introduce a new dataset of 95 climate change news articles with annotations of factchecked segments (Section 3.1). 2. We annotate the argumentative discourse structure of these 95 articles (Section 3.2), thus introducing the first multi-layer annotated corpus both for argumentative discourse structure and check-worthy statements that allows us to deepen our understating of the connection between the two (Section 4). 3. We show that a BERT model (Devlin et al., 2019) that incorporates information about argumentative discourse structure provides a slight but statistically significant improvement over a BERT model that uses just local discourse context (Sections 5 and 6). 2 Related Work Previous work on fact-checking has focused on different steps of the fact-checking pipeline (Thorne and Vlachos, 2018; Graves, 2018), the majority of which is work on predicting the veracity of claims either by comparing them against evidence from Wikipedia (Thorne et al., 2018), trusted news outlets (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (J"
2021.sigdial-1.40,N16-1138,0,0.166275,"er in highly controversial topics such as climate change. An endto-end automatic fact-checking system needs to accomplish three main tasks: 1) find claims that are worth fact-checking, 2) retrieve relevant evidence from credible sources, and 3) determine the veracity of that claim given the retrieved evidence. Most previous attempts at automating factchecking focus on the latter two steps by comparing a manually prepared list of claims against automatically- or manually-retrieved evidences from (trusted) sources such as Wikipedia or news articles from credible publishers (Thorne et al., 2018; Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017). However, less attention is given to automatically compiling a list of check-worthy statements that can then be inspected and fact-checked by a human fact-checker (or by a fact-checking system). A small number of previous studies developed datasets and models for identifying checkworthy statements in political news and debates (Hassan et al., 2017; Jaradat et al., 2018; Arslan et al., 2020). We look at the problem of deciding what sentences to fact-check in news articles and in particular in the climate change domain. We hypothesize that selecting segments for fact-c"
2021.sigdial-1.40,W17-5102,1,0.830031,"eck.org. We on the other hand work on a dataset from a different genre: news articles, and from a different domain: climate change, and investigate the question whether argumentative discourse structure helps in detecting check-worthy statements. Argument mining is a field concerned with finding argument structure in text from argument components (claim, premises) to relations (support, attack) as covered extensively by Lawrence and Reed (2020). Several argumentation corpora are available on texts from multiple genres such as student essays (Stab and Gurevych, 2014), and social-media threads (Hidey et al., 2017), which have been used in applications such as writing assistance (Zhang and Litman, 2016) and essay scoring (Somasundaran et al., 2016). Freeman (2000) has argued that statements have different types which affects the type of evidence they need or lack thereof. This was empirically explored by works that attempted to identify the appropriate type of support for statements in user comments (Park and Cardie, 2014) and controversial topics in the social media (Addawood and Bashir, 2016). In this work, we provide a resource and a model that aims to deepen our understanding of the relations betwee"
2021.sigdial-1.40,N18-5006,0,0.0231196,"Missing"
2021.sigdial-1.40,D18-1452,0,0.0361899,"Missing"
2021.sigdial-1.40,W02-0109,0,0.034332,"from very low to very high by the fact-checkers in addition to the segment-level annotation. Table 1 shows the number of articles in each of the nine degrees of credibility for news articles. The annotations of fact-checked segments vary in length from a fragment of a sentence to multiple sentences. We thus map this to binary labels at the sentence-level: factchecked sentences or non-fact-checked sentences. Each sentence is labeled as ’fact-checked’ if it was fact-checked, or it has a fact-checked fragment, or it is part of multi-sentence fact-checked segment. We use NLTK sentence segmenter (Loper and Bird, 2002) to split both the original articles and the factchecked segments into a list of sentences. There are a total of 134 articles that are factchecked by climatefeedback.org at the time of crawling this data (May 2020). However, we only include articles that have segment-level annotations and thus the final dataset has a total of 95 articles. We split the dataset to 68 articles in the training set (4,353 sentences in total, 824 are fact-checked), 7 articles in the development set (249 sentences in total, 55 are fact-checked), and 20 articles in the test set (970 sentences in total, 220 are factche"
2021.sigdial-1.40,W14-2105,0,0.0325435,"nsively by Lawrence and Reed (2020). Several argumentation corpora are available on texts from multiple genres such as student essays (Stab and Gurevych, 2014), and social-media threads (Hidey et al., 2017), which have been used in applications such as writing assistance (Zhang and Litman, 2016) and essay scoring (Somasundaran et al., 2016). Freeman (2000) has argued that statements have different types which affects the type of evidence they need or lack thereof. This was empirically explored by works that attempted to identify the appropriate type of support for statements in user comments (Park and Cardie, 2014) and controversial topics in the social media (Addawood and Bashir, 2016). In this work, we provide a resource and a model that aims to deepen our understanding of the relations between argumentative discourse structure and check-worthiness. 3 Multi-Layer Annotated Corpus We describe below the dataset, its fact-checked segment annotation by climate scientists, and our argumentative discourse structures annotation on the same dataset. 3.1 Credibility very-low very-low/low low neutral Count 23 7 10 7 Credibility high high/very-high very-high mixed Count 21 8 18 1 Table 1: Number of articles per"
2021.sigdial-1.40,C18-1287,0,0.0408487,"Missing"
2021.sigdial-1.40,D17-1317,0,0.0250647,"sed on different steps of the fact-checking pipeline (Thorne and Vlachos, 2018; Graves, 2018), the majority of which is work on predicting the veracity of claims either by comparing them against evidence from Wikipedia (Thorne et al., 2018), trusted news outlets (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (Joty et al., 2018), or debate websites (Chen et al., 2019), or by analyzing the linguistic properties of false and true claim 1 The annotated dataset, guidelines, and code are available here: https://github.com/Tariq60/whatToFactcheck (P´erez-Rosas et al., 2018; Rashkin et al., 2017) in addition to the speaker’s history (Wang, 2017; Alhindi et al., 2018). Other work focuses on estimating the credibility of sources by using an external list of bias per publisher (Baly et al., 2018) or by modeling conflicting reports on a claim from different sources (Zhang et al., 2019). However, all of these methods either report bias at the publisher level or start with a list of claims to fact-check. Previous work on detecting check-worthy claims focus on text from the political domain. The two main existing systems for check-worthy claim detection are ClaimBuster (Hassan et al., 2017)"
2021.sigdial-1.40,C16-1148,0,0.020577,"e, and investigate the question whether argumentative discourse structure helps in detecting check-worthy statements. Argument mining is a field concerned with finding argument structure in text from argument components (claim, premises) to relations (support, attack) as covered extensively by Lawrence and Reed (2020). Several argumentation corpora are available on texts from multiple genres such as student essays (Stab and Gurevych, 2014), and social-media threads (Hidey et al., 2017), which have been used in applications such as writing assistance (Zhang and Litman, 2016) and essay scoring (Somasundaran et al., 2016). Freeman (2000) has argued that statements have different types which affects the type of evidence they need or lack thereof. This was empirically explored by works that attempted to identify the appropriate type of support for statements in user comments (Park and Cardie, 2014) and controversial topics in the social media (Addawood and Bashir, 2016). In this work, we provide a resource and a model that aims to deepen our understanding of the relations between argumentative discourse structure and check-worthiness. 3 Multi-Layer Annotated Corpus We describe below the dataset, its fact-checked"
2021.sigdial-1.40,D14-1006,0,0.0321685,"cked by various factchecking agencies such as FactCheck.org. We on the other hand work on a dataset from a different genre: news articles, and from a different domain: climate change, and investigate the question whether argumentative discourse structure helps in detecting check-worthy statements. Argument mining is a field concerned with finding argument structure in text from argument components (claim, premises) to relations (support, attack) as covered extensively by Lawrence and Reed (2020). Several argumentation corpora are available on texts from multiple genres such as student essays (Stab and Gurevych, 2014), and social-media threads (Hidey et al., 2017), which have been used in applications such as writing assistance (Zhang and Litman, 2016) and essay scoring (Somasundaran et al., 2016). Freeman (2000) has argued that statements have different types which affects the type of evidence they need or lack thereof. This was empirically explored by works that attempted to identify the appropriate type of support for statements in user comments (Park and Cardie, 2014) and controversial topics in the social media (Addawood and Bashir, 2016). In this work, we provide a resource and a model that aims to d"
2021.sigdial-1.40,J17-3005,0,0.0607811,"Missing"
2021.sigdial-1.40,C18-1283,0,0.0121929,"es (Section 3.2), thus introducing the first multi-layer annotated corpus both for argumentative discourse structure and check-worthy statements that allows us to deepen our understating of the connection between the two (Section 4). 3. We show that a BERT model (Devlin et al., 2019) that incorporates information about argumentative discourse structure provides a slight but statistically significant improvement over a BERT model that uses just local discourse context (Sections 5 and 6). 2 Related Work Previous work on fact-checking has focused on different steps of the fact-checking pipeline (Thorne and Vlachos, 2018; Graves, 2018), the majority of which is work on predicting the veracity of claims either by comparing them against evidence from Wikipedia (Thorne et al., 2018), trusted news outlets (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (Joty et al., 2018), or debate websites (Chen et al., 2019), or by analyzing the linguistic properties of false and true claim 1 The annotated dataset, guidelines, and code are available here: https://github.com/Tariq60/whatToFactcheck (P´erez-Rosas et al., 2018; Rashkin et al., 2017) in addition to the speaker’s history (Wang, 2017; Alhind"
2021.sigdial-1.40,N18-1074,0,0.0363608,"Missing"
2021.sigdial-1.40,P17-2067,0,0.0274855,"and Vlachos, 2018; Graves, 2018), the majority of which is work on predicting the veracity of claims either by comparing them against evidence from Wikipedia (Thorne et al., 2018), trusted news outlets (Ferreira and Vlachos, 2016; Pomerleau and Rao, 2017), discussion forums (Joty et al., 2018), or debate websites (Chen et al., 2019), or by analyzing the linguistic properties of false and true claim 1 The annotated dataset, guidelines, and code are available here: https://github.com/Tariq60/whatToFactcheck (P´erez-Rosas et al., 2018; Rashkin et al., 2017) in addition to the speaker’s history (Wang, 2017; Alhindi et al., 2018). Other work focuses on estimating the credibility of sources by using an external list of bias per publisher (Baly et al., 2018) or by modeling conflicting reports on a claim from different sources (Zhang et al., 2019). However, all of these methods either report bias at the publisher level or start with a list of claims to fact-check. Previous work on detecting check-worthy claims focus on text from the political domain. The two main existing systems for check-worthy claim detection are ClaimBuster (Hassan et al., 2017) and ClaimRank (Jaradat et al., 2018). ClaimBuster"
C10-2102,W05-0602,0,0.026145,"or LWFGs assumes that the semantic composition constraints are learnable. In this paper, we show what are the properties and principles the semantic representation and grammar formalism require, in order to be able to learn these constraints from examples, and give a learning algorithm. We also introduce a LWFG parser as a deductive system, used as an inference engine during LWFG induction. An example for learning a grammar for noun compounds is given. 1 Introduction Recently, several machine learning approaches have been proposed for mapping sentences to their formal meaning representations (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Muresan, 2008; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009). However, only few of them integrate the semantic representation with a grammar formalism: λ-expressions and Combinatory Categorial Grammars (CCGs) (Steedman, 1996) are used by Zettlemoyer and Collins (2005;2009), and ontology-based representations and Lexicalized Well-Founded Grammars (LWFGs) (Muresan and Rambow, 2007) are used by Muresan (2008). An advantage of the LWFG formalism, compared to most constraint-based grammar formalisms developed for deep language understanding, is that it is ac"
C10-2102,P07-1121,0,0.0364633,"nable. In this paper, we show what are the properties and principles the semantic representation and grammar formalism require, in order to be able to learn these constraints from examples, and give a learning algorithm. We also introduce a LWFG parser as a deductive system, used as an inference engine during LWFG induction. An example for learning a grammar for noun compounds is given. 1 Introduction Recently, several machine learning approaches have been proposed for mapping sentences to their formal meaning representations (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Muresan, 2008; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009). However, only few of them integrate the semantic representation with a grammar formalism: λ-expressions and Combinatory Categorial Grammars (CCGs) (Steedman, 1996) are used by Zettlemoyer and Collins (2005;2009), and ontology-based representations and Lexicalized Well-Founded Grammars (LWFGs) (Muresan and Rambow, 2007) are used by Muresan (2008). An advantage of the LWFG formalism, compared to most constraint-based grammar formalisms developed for deep language understanding, is that it is accompanied by a learnability guarantee, the search space for LWFG indu"
C10-2102,P09-1110,0,0.068539,"e show what are the properties and principles the semantic representation and grammar formalism require, in order to be able to learn these constraints from examples, and give a learning algorithm. We also introduce a LWFG parser as a deductive system, used as an inference engine during LWFG induction. An example for learning a grammar for noun compounds is given. 1 Introduction Recently, several machine learning approaches have been proposed for mapping sentences to their formal meaning representations (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Muresan, 2008; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009). However, only few of them integrate the semantic representation with a grammar formalism: λ-expressions and Combinatory Categorial Grammars (CCGs) (Steedman, 1996) are used by Zettlemoyer and Collins (2005;2009), and ontology-based representations and Lexicalized Well-Founded Grammars (LWFGs) (Muresan and Rambow, 2007) are used by Muresan (2008). An advantage of the LWFG formalism, compared to most constraint-based grammar formalisms developed for deep language understanding, is that it is accompanied by a learnability guarantee, the search space for LWFG induction being a complete grammar l"
C10-2102,P07-1105,1,0.311596,"a grammar for noun compounds is given. 1 Introduction Recently, several machine learning approaches have been proposed for mapping sentences to their formal meaning representations (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Muresan, 2008; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009). However, only few of them integrate the semantic representation with a grammar formalism: λ-expressions and Combinatory Categorial Grammars (CCGs) (Steedman, 1996) are used by Zettlemoyer and Collins (2005;2009), and ontology-based representations and Lexicalized Well-Founded Grammars (LWFGs) (Muresan and Rambow, 2007) are used by Muresan (2008). An advantage of the LWFG formalism, compared to most constraint-based grammar formalisms developed for deep language understanding, is that it is accompanied by a learnability guarantee, the search space for LWFG induction being a complete grammar lattice (Muresan and Rambow, 2007). Like other constraint-based grammar formalisms, the semantic structures in LWFG are composed by constraint solving, semantic composition being realized through constraints at the grammar rule level. Moreover, semantic interpretation is also realized through constraints at the grammar ru"
C10-2102,W08-2002,1,0.804642,"raints are learnable. In this paper, we show what are the properties and principles the semantic representation and grammar formalism require, in order to be able to learn these constraints from examples, and give a learning algorithm. We also introduce a LWFG parser as a deductive system, used as an inference engine during LWFG induction. An example for learning a grammar for noun compounds is given. 1 Introduction Recently, several machine learning approaches have been proposed for mapping sentences to their formal meaning representations (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Muresan, 2008; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009). However, only few of them integrate the semantic representation with a grammar formalism: λ-expressions and Combinatory Categorial Grammars (CCGs) (Steedman, 1996) are used by Zettlemoyer and Collins (2005;2009), and ontology-based representations and Lexicalized Well-Founded Grammars (LWFGs) (Muresan and Rambow, 2007) are used by Muresan (2008). An advantage of the LWFG formalism, compared to most constraint-based grammar formalisms developed for deep language understanding, is that it is accompanied by a learnability guarantee, the sea"
C12-2039,W10-2924,0,0.259112,"ple entities (e.g., [Academy awards]Other is a named entity in sentence (1), but it is not part of the Live_In relation); and 2) each relation has a concept of directionality. This is because the arguments in a relation often take different roles and need to be distinguished ( Live_in([Actress Angie Dickinson] Pers , [Kulm,N.D.] Loc ] vs. Live_in([Modesto] Loc ,[George Lucas] Pers ). Identifying the right directionality is key to the task of relation extraction. While few recent work on relation extraction has modeled the directionality of relations (Roth and Yih, 2004; Giuliano et al., 2007; Kate and Mooney, 2010; Zhang et al., 2008), these studies have only reported averaged results. A key contribution of this paper is an in-depth study of relation directionality, showing how various factors might contribute to the accuracy of results for each relation direction. In this paper, we explore a novel approach of creating substring sequences from corpora annotated with entities for relation extraction. We use intra-sentential information between the entities to create string sequences, which we call entity sequences. In our approach, we assume that entity boundaries are known, but the types of entities ar"
C12-2039,W04-2401,0,0.5745,"ing structured information from unstructured text. Two major sub-tasks of IE are extracting entities such as [John Smith] Pers , [New York] Loc and [Google Inc]Or g and the relation between these entities, such as Work_For relation between [John Smith] Pers and [Google Inc.]Or g , and Live_In relation between [John Smith] Pers and [New York] Loc . Extracting relations between entities is still a significantly harder task than recognizing entities, and current state-of-the-art systems achieve inferior results. Consider the following examples of a Live_In relation from the corpus introduced by (Roth and Yih, 2004): (1) (2) [Actress Angie Dickinson] Pers , who was born in [Kulm,N.D.] Loc donated a coat she wore to the 1966 [Academy Awards]Other [Modesto] Loc , native [George Lucas] Per ’s film was released... Our task is to extract the Live_In relation from the above sentences where the involved named entities are [Actress Angie Dickinson] Per and [Kulm, N.D.] Loc in example (1) and [George Lucas] Per and [Modesto] Loc in example (2). These two examples are illustrative of two key challenges: 1) a sentence can contain multiple entities (e.g., [Academy awards]Other is a named entity in sentence (1), but"
C12-2039,I08-2119,0,0.0745971,"kernes, and only consider the entity boundaries as given, and not entity types as in (Bunescu and Mooney, 2005a). Bunescu and Mooney (2005b) present a shortest path (between the entities) dependency tree kernel and evaluate it on the ACE 2002 dataset. However, as pointed out by (Giuliano et al., 2007) due to the varied datasets (e.g. ACE, SemEval) employed for these research it is a hard task to compare one against another. The generic trend is usually similar — sequence kernels have more flexibility and thus gap sequence kernels find similar subsequences and often results in a higher recall (Wang, 2008). 6 Conclusion We have presented an approach for relation extraction using semantic and syntactic features augmented with an entity sequence kernel. To the best of our knowledge, this paper presents the first in depth study of how the order of the candidate entities influences relation directionality and how various factors might contribute to the accuracy of results for each relation direction. Our proposed entity sequence kernel outperforms state-of-the-art methods for three out of the five relations under study. We plan to further explore the shortest path dependency kernel with different k"
D15-1116,P05-1074,0,0.0112758,"re to the parallel 1004 monolingual dataset used in Barzilay and McKeown (2001), and thus lexical and contextual information from tweets can be used to extract the candidate targets words for LSSD. For instance, we can align the [SM] and [IM3 ] (from the above examples), where except for the words happy and unhappy, the majority of the words in the two messages are anchor words and thus happy and unhappy can be extracted as paraphrases via cotraining. To model contextual information, such as part of speech tagging for the co-training algorithm, we used Tweet NLP (Gimpel et al., 2011). Second, Bannard and Callison-Burch (2005) noticed that the co-training method proposed byBarzilay and McKeown (2001) requires identical bounding substrings and has bias towards single words while extracting paraphrases. This apparent limitation, however, is advantageous to us because we are specifically interested in extracting target words. Co-training resulted in 367 extracted pairs of paraphrases. We also considered a statistical machine translation (SMT) alignment method - IBM Model 4 with HMM alignment implemented in Giza++ (Och and Ney, 2000). We used Moses software(Koehn et al., 2007) to extract lexical translations by alignin"
D15-1116,P14-2131,0,0.0345481,"due to our approach of selecting the target words. Low-dimensional text representation, such as WTMF, have been successful in WSD disambiguation research and in computing similarity between short texts (Guo and Diab, 2012a; Guo and Diab, 2012b). word2vec and GloVe representations have provided state-of-the-art results on various word similarity and analogy detection task (Mikolov et al., 2013c; Mikolov et al., 2013b; Pennington et al., 2014). Word embedding based models are also used for other NLP tasks such as dependency parsing, semantic role labeling, POS tagging, NER, question-answering (Bansal et al., 2014; Collobert et al., 2011; Weston et al., 2015) and our work on LSSD is a novel application of word embeddings. 6 Conclusion and Future Work We proposed a reframing of the sarcasm detection task as a type of word sense disambiguation problem, where the sense of a word is its sarcastic or literal sense. Using a crowdsourcing experiment and unsupervised methods for detecting semantically opposite phrases, we collected a set of target words to be used in the LSSD task. We compared several distributional semantics methods, and showed that using word embeddings in a modified SVM kernel achieves the"
D15-1116,P01-1008,0,0.0854266,"identical bounding substrings and has bias towards single words while extracting paraphrases. This apparent limitation, however, is advantageous to us because we are specifically interested in extracting target words. Co-training resulted in 367 extracted pairs of paraphrases. We also considered a statistical machine translation (SMT) alignment method - IBM Model 4 with HMM alignment implemented in Giza++ (Och and Ney, 2000). We used Moses software(Koehn et al., 2007) to extract lexical translations by aligning the dataset of 5,000 SM-IM pairs. From the set of 367 extracted paraphrases using Barzilay and McKeown (2001)’s approach, we selected only those paraphrases where the lexical translation scores φ (resulted after running Moses) are ≥ 0.8. After filtering via translation scores and manual inspection, we obtained a set of 80 semantically opposite paraphrases. Given this set of semantically opposite words, the words that appear in the sarcastic messages were consider our target words for LSSD (70 target words after lemmatization). They range from verbs, such as “love” and “like”, adjectives, such as “brilliant”, “genius”, and adverbs, such as “really”. 3 Literal/Sarcastic Sense Disambiguation Our Literal"
D15-1116,J90-1003,0,0.330394,"one for the literal sense L using the literal sense training data for t (~ vl ).3 Given a test message u containing a target word t, we first represent the target word as a vector v~u using all the context words inside u. To predict whether t is used in a literal or sarcastic sense in the test message u we simply apply geometric techniques (e.g., cosine similarity) between v~u and the two sense vectors v~s and v~l , choosing the one with the maximum score. To create the two sense vectors v~s and v~l for each of the target words t, we use the positive pointwise mutual information model (PPMI) (Church and Hanks, 1990). Based on t’s context words ck in a window of 10 words, we separately computed PPMI for sarcastic and literal senses using t’s training data. The size of the context widow used in DSMs is generally between 5 and 10, and in our experiments we used a window of 10 words since tweets often include meaningful words/tokens at the end of the tweets (e.g., interjections, such as “yay”, “ohh”; upper-case words, such as, “GREAT”; novel hashtags, such as “#notreally”, “#lolol”; emoticons, such as “:(”). We sorted the context words based on the PPMI scores and for each target word t we selected a maximum"
D15-1116,W10-2914,0,0.456982,"Missing"
D15-1116,P11-2008,0,0.0327313,"Missing"
D15-1116,P11-2102,1,0.927897,"olumns of M or max = 0 [line 21]. 3.2.2 Classification Approaches The second approach for our LSSD task is to treat it as a binary classification task to identify the sarcastic or literal sense of a target word t. We have two classification tasks: S vs. L and S vs. Lsent for each of the 37 target words. We use the libSVM toolkit (Chang and Lin, 2011). Development data is used for tuning parameters. SVM Baseline: The SVM baseline for LSSD tasks uses n-grams and lexicon-based binaryvalued features that are commonly used in existing state-of-the-art sarcasm detection approaches (Gonz´alez-Ib´an˜ ez et al., 2011; Tchokni et al., 2014). They are derived from i) bag-of-words (BoW) representations of words, ii) LIWC dictionary (Pennebaker et al., 2001), and iii) a list of interjections (e.g., “ah”, “oh”, “yeah”), punctuations (e.g., “!”, “?”), and emoticons collected from Wikipedia. CMU Tweet Tokenizer is employed for tokenization. 5 We kept unigrams unchanged when all the characters are uppercase (e.g., “NEVER” in “A shooting in Oakland? That NEVER happens! #sarcasm”) but otherwise words are converted to lower case. We also change all numbers to a generic number token “22”. To avoid any bias during exp"
D15-1116,P12-2028,1,0.931224,"ximum score. All vector elements are given by the tf-idf values of the corresponding words. This approach, denoted as the “PPMI baseline”, is the baseline for our DSM experiments. Context Vectors with Word Embedding: The above method considers that the context vectors v~s and v~l of each target word t contain the cooccurring words selected by their PPMI values. We enhance the representation of context vectors to represent each word in the context vector by its word embedding. We experiment with three different methods of obtaining word embeddings: Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b); word2vec that implements the skip-gram and continuous bag-of-words models (CBOW) of Mikolov et al. (2013a), and GloVe (Pennington et al., 2014), a log-bilinear regression model based upon global word-word cooccurrence count in the training corpora. After removing the tweets that are used as test sets, we build the three word embedding models in an unsupervised fashion with the remaining 2,482,763 tweets from our original data collection (Section 3.1). In each of the three models, each word w is represented by its d-dimensional vector w ~ of real numbers, where d=100 for all of the embeddin"
D15-1116,P12-1091,1,0.618277,"ximum score. All vector elements are given by the tf-idf values of the corresponding words. This approach, denoted as the “PPMI baseline”, is the baseline for our DSM experiments. Context Vectors with Word Embedding: The above method considers that the context vectors v~s and v~l of each target word t contain the cooccurring words selected by their PPMI values. We enhance the representation of context vectors to represent each word in the context vector by its word embedding. We experiment with three different methods of obtaining word embeddings: Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012b); word2vec that implements the skip-gram and continuous bag-of-words models (CBOW) of Mikolov et al. (2013a), and GloVe (Pennington et al., 2014), a log-bilinear regression model based upon global word-word cooccurrence count in the training corpora. After removing the tweets that are used as test sets, we build the three word embedding models in an unsupervised fashion with the remaining 2,482,763 tweets from our original data collection (Section 3.1). In each of the three models, each word w is represented by its d-dimensional vector w ~ of real numbers, where d=100 for all of the embeddin"
D15-1116,P15-2124,0,0.366466,"), we have introduced a reframing of this task as a type of word sense disambiguation problem, where the sense of a word is sarcastic or literal. Our SVM baseline uses the lexical features proposed in previous research on sarcasm detection (e.g., LIWC lexicon, interjections, pragmatic features) (Liebrecht et al., 2013; Gonz´alez-Ib´an˜ ez et al., 2011; Reyes et al., 2013). Our analysis of target words where the sarcastic sense is the opposite of the literal sense is related to the idea of “positive sentiment toward a negative situation” proposed by Riloff et al. (2013) and recently studied by Joshi et al. (2015). In our approach, we chose distributional semantic approaches that learn contextual information of targets effectively from a large corpus containing both literal and sarcastic uses of words and show that word embedding are highly accurate in predicting the sarcastic or literal sense of a word (Tables 4 and 5). This approach has the potential to capture more nuanced cases of sarcasm, beyond “positive sentiment towards a negative situation” (e.g., one of our target words was “shocked” which is negative). However, our current framing is still inherently limited to cases where sarcasm is charact"
D15-1116,P07-2045,0,0.038242,"impel et al., 2011). Second, Bannard and Callison-Burch (2005) noticed that the co-training method proposed byBarzilay and McKeown (2001) requires identical bounding substrings and has bias towards single words while extracting paraphrases. This apparent limitation, however, is advantageous to us because we are specifically interested in extracting target words. Co-training resulted in 367 extracted pairs of paraphrases. We also considered a statistical machine translation (SMT) alignment method - IBM Model 4 with HMM alignment implemented in Giza++ (Och and Ney, 2000). We used Moses software(Koehn et al., 2007) to extract lexical translations by aligning the dataset of 5,000 SM-IM pairs. From the set of 367 extracted paraphrases using Barzilay and McKeown (2001)’s approach, we selected only those paraphrases where the lexical translation scores φ (resulted after running Moses) are ≥ 0.8. After filtering via translation scores and manual inspection, we obtained a set of 80 semantically opposite paraphrases. Given this set of semantically opposite words, the words that appear in the sarcastic messages were consider our target words for LSSD (70 target words after lemmatization). They range from verbs,"
D15-1116,D14-1162,0,0.0980506,"Missing"
D15-1116,D13-1066,0,0.494563,"Missing"
D15-1116,Q15-1016,0,0.0226407,"Missing"
D15-1116,W13-1605,0,0.27337,"Missing"
D15-1116,maynard-greenwood-2014-cares,0,0.21527,"oduction Recognizing sarcasm is important for understanding people’s actual sentiments and beliefs. For example, failing to recognize the following message as being sarcastic “I love that I have to go back to the emergency room”, will lead a sentiment and opinion analysis system to infer that the author’s sentiment is positive towards the event of “going to the emergency room”. Current approaches have framed the sarcasm detection task as predicting whether a full utterance is sarcastic or not (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014). We propose a re-framing of sarcasm detection as a type of word sense disambiguation problem: given an utterance and a target word, identify whether the sense of the target word is literal or sarcastic. We call this the Literal/Sarcastic Sense Disambiguation (LSSD) task. In the above utterance, the word “love” is used in a sarcastic, nonliteral sense (the author’s intended meaning being most likely the opposite of the original literal meaning - a negative sentiment, such as “hate”). Two key challenges need to be addressed: 1) how to collect a set of target words that can have a literal or a s"
D19-1291,W11-0702,0,0.057683,"components in a pipeline. A full end-to-end neural sequence tagging model was developed by Eger et al. (2017) that pre1 https://github.com/chridey/change-my-view-modes https://github.com/tuhinjubcse/AMPERSANDEMNLP2019 2 dicts argumentative discourse units (ADUs), components, and relations at the token level. In contrast, we assume we have ADUs and predict components and relations at that level. Argument mining on discussion forums (online discussion) Most computational work related to argumentation as a process has focused on the detection of agreement and disagreement in online interactions (Abbott et al., 2011; Sridhar et al., 2015; Rosenthal and McKeown, 2015; Walker et al., 2012). However, these approaches do not identify the argument components that the (dis)agreement has scope over (i.e., what has been targeted by a disagreement or agreement move). Also in these approaches, researchers predict the type of relation (e.g. agreement) given that a relationship exists. On the contrary, we predict both argument components as well as the existence of ˇ a relation between them. Boltuˇzi´c and Snajder (2014) and Murakami and Raymond (2010) address relations between complete arguments but without the mic"
D19-1291,C18-1081,0,0.0142663,"eters are discussed in Appendix A. 5.1 Argumentative Component Classification For baseline experiments on argumentative component classification we rely on the handcrafted features used by Stab and Gurevych (2017): lexical (unigrams), structural (token statistics and position), indicator (I, me, my), syntactic (POS, modal verbs), discourse relation (PDTB), and word embedding features. Hidey et al. (2017) show that emotional appeal or pathos is strongly correlated with persuasion and appears in premises. This motivated us to augment the work of Stab and Gurevych (2017) with emotion embeddings (Agrawal et al., 2018) which capture emotion-enriched word representations and show improved performance over generic embeddings (denoted in the table as EWE). We also compare our results to several neural models - a joint model using pointer networks (Morio and Fujita, 2018), a model that leverages context fine-tuning (Chakrabarty et al., 2019), and a BERT baseline (Devlin et al., 2019) using only the pre-trained model without our additional finetuning step. Table 2 shows that our best model gains statistically significant improvement over all the other models (p &lt; 0.001 with a Chi-squared test). To compare direct"
D19-1291,W14-2107,0,0.0392392,"Missing"
D19-1291,N19-1054,1,0.841295,"al verbs), discourse relation (PDTB), and word embedding features. Hidey et al. (2017) show that emotional appeal or pathos is strongly correlated with persuasion and appears in premises. This motivated us to augment the work of Stab and Gurevych (2017) with emotion embeddings (Agrawal et al., 2018) which capture emotion-enriched word representations and show improved performance over generic embeddings (denoted in the table as EWE). We also compare our results to several neural models - a joint model using pointer networks (Morio and Fujita, 2018), a model that leverages context fine-tuning (Chakrabarty et al., 2019), and a BERT baseline (Devlin et al., 2019) using only the pre-trained model without our additional finetuning step. Table 2 shows that our best model gains statistically significant improvement over all the other models (p &lt; 0.001 with a Chi-squared test). To compare directly to the work of Chakrabarty et al. (2019), we also test our model on the binary claim detection task and obtain a Claim F-Score of 70.0 with fine-tuned BERT, which is a 5-point improvement in F-score over pre-trained BERT and a 12point improvement over Chakrabarty et al. (2019). These results show that fine-tuning on the"
D19-1291,N19-1423,0,0.297667,"e relations in threads from the Change 2933 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2933–2943, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics My View (CMV) subreddit. In addition to the CMV dataset we also introduce two novel distantly-labeled data-sets that incorporate the macro- and micro-level context (further described in Section 3). We also propose a new transfer learning approach to fine-tune a pre-trained language model (Devlin et al., 2019) on the distantlabeled data (Section 4) and demonstrate improvement on both argument component classification and relation prediction (Section 5). We further show that using discourse relations based on Rhetorical Structure Theory (Mann and Thompson, 1988) improves the results for relation identification both for inter-turn and intra-turn relations. Overall, our approach for argument mining obtains statistically significant improvement over a state-of-the-art model based on pointer networks (Morio and Fujita, 2018) and a strong baseline using a pre-trained language model (Devlin et al., 2019)."
D19-1291,P17-1002,0,0.0653242,"ve relations. They however rely on handcrafted (structural, lexical, indicator, and discourse) features. We on the other hand use a transfer learning approach for argument mining in discussion forums. Recent work has examined neural models for argument mining. Potash et al. (2017) use pointer networks (Vinyals et al., 2015) to predict both argumentative components and relations in a joint model. For our work, we focus primarily on relation prediction but conduct experiments with predicting argumentative components in a pipeline. A full end-to-end neural sequence tagging model was developed by Eger et al. (2017) that pre1 https://github.com/chridey/change-my-view-modes https://github.com/tuhinjubcse/AMPERSANDEMNLP2019 2 dicts argumentative discourse units (ADUs), components, and relations at the token level. In contrast, we assume we have ADUs and predict components and relations at that level. Argument mining on discussion forums (online discussion) Most computational work related to argumentation as a process has focused on the detection of agreement and disagreement in online interactions (Abbott et al., 2011; Sridhar et al., 2015; Rosenthal and McKeown, 2015; Walker et al., 2012). However, these"
D19-1291,W19-4512,0,0.0110451,"possible pairs, using the labeled relations in the CMV data. For inter-turn relation prediction, we fine-tune first on the QR dataset, where the dialogue context more closely represents our labeled inter-post relations. Then, we fine-tune on inter-turn relation predic2937 tion using all possible pairs as training. This process is indicated in Figure 2, where we use the appropriate context fine-tuning to obtain a relation classifier. Discourse Relations Rhetorical Structure Theory was originally developed to offer an explanation of the coherence of texts. Musi et al. (2018) and, more recently Hewett et al. (2019), showed that discourse relations from RST often correlate with argumentative relations. We thus derive features from RST trees and train a classifier using these features to predict an argumentative relation. To extract features from a pair of argumentative components, we first concatenate the two components so that they form a single text input. We then use a state-of-the-art RST discourse parser (Ji and Eisenstein, 2014)3 to create parse trees and take the predicted discourse relation at the root of the parse tree as a categorical feature in a classifier. There are 28 unique discourse relat"
D19-1291,W17-5102,1,0.948027,"hat takes into consideration the constraints of thread structure. Their model discriminates between types of arguments (e.g., claims or premises) and both intra-turn and inter-turn relations, simultaneously. Their dataset, which is not publically available, is three times larger than ours, so instead we focus on transfer learning approaches that take advantage of discourse and dialogue context and use their model as our baseline. 3 3.1 Data Labeled Persuasive Forum Data To learn to predict relations, we use a corpus of 78 threads from the CMV subreddit annotated with argumentative components (Hidey et al., 2017). The authors annotate claims (“propositions that express the speakers stance on a certain matter”), and premises (“propositions that express a justification provided by the speaker in support of a claim”). In this data, the main claim, or the central position of the original poster, is always the title of the original post. We expand this corpus by annotating the argumentative relations among these propositions (both inter-turn and intra-turn) and extend the corpus by annotating additional argument components (using the guidelines of the authors) for a total of 112 threads. It is to be noted"
D19-1291,P14-1002,0,0.0830666,"on classifier. Discourse Relations Rhetorical Structure Theory was originally developed to offer an explanation of the coherence of texts. Musi et al. (2018) and, more recently Hewett et al. (2019), showed that discourse relations from RST often correlate with argumentative relations. We thus derive features from RST trees and train a classifier using these features to predict an argumentative relation. To extract features from a pair of argumentative components, we first concatenate the two components so that they form a single text input. We then use a state-of-the-art RST discourse parser (Ji and Eisenstein, 2014)3 to create parse trees and take the predicted discourse relation at the root of the parse tree as a categorical feature in a classifier. There are 28 unique discourse relations predicted in the data, including Circumstance, Purpose, and Antithesis. We use a one-hot encoding of these relations as features and train an XGBoost Classifier (Chen and Guestrin, 2016) to predict whether an argument relation exists. This classifier with discourse relations, as indicated in Figure 2, is then ensembled with our predictions from the BERT classifier by predicting a relation if either one of the classifie"
D19-1291,W18-5202,0,0.311025,"ropose a new transfer learning approach to fine-tune a pre-trained language model (Devlin et al., 2019) on the distantlabeled data (Section 4) and demonstrate improvement on both argument component classification and relation prediction (Section 5). We further show that using discourse relations based on Rhetorical Structure Theory (Mann and Thompson, 1988) improves the results for relation identification both for inter-turn and intra-turn relations. Overall, our approach for argument mining obtains statistically significant improvement over a state-of-the-art model based on pointer networks (Morio and Fujita, 2018) and a strong baseline using a pre-trained language model (Devlin et al., 2019). We make our data,1 code, and trained models publicly available.2 2 Related Work Argument mining on monologues Most prior work in argument mining (AM) has focused on monologues or essays. Stab and Gurevych (2017) and Persing and Ng (2016) used pipeline approaches for AM, combining parts of the pipeline using integer linear programming (ILP) to classify argumentative components. Stab and Gurevych (2017) represented relations of argument components in essays as tree structures. Both Stab and Gurevych (2017) and Persi"
D19-1291,C10-2100,0,0.0194913,"d on the detection of agreement and disagreement in online interactions (Abbott et al., 2011; Sridhar et al., 2015; Rosenthal and McKeown, 2015; Walker et al., 2012). However, these approaches do not identify the argument components that the (dis)agreement has scope over (i.e., what has been targeted by a disagreement or agreement move). Also in these approaches, researchers predict the type of relation (e.g. agreement) given that a relationship exists. On the contrary, we predict both argument components as well as the existence of ˇ a relation between them. Boltuˇzi´c and Snajder (2014) and Murakami and Raymond (2010) address relations between complete arguments but without the micro-structure of arguments as in Stab and Gurevych (2017). Ghosh et al. (2014) introduce a scheme to annotate inter-turn relations between two posts as “target-callout”, and intraturn relations as “stance-rationale”. However, their empirical study is reduced to predicting the type of inter-turn relations as agree/disagree/other. Our computational model on the other hand handles both macro- and micro- level structures of arguments (argument components and relations). Budzynska et al. (2014) focused on building foundations for extra"
D19-1291,P11-1099,0,0.0371479,": Main Claim) process of argument in a dialogue (Bentahar et al., 2010) have received less attention. Introduction Argument mining is a field of corpus-based discourse analysis that involves the automatic identification of argumentative structures in text. Most current studies have focused on monologues or micro-level models of argument that aim to identify the structure of a single argument by identifying the argument components (classes such as “claim” and “premise”) and relations between them (“support” or “attack”) (Somasundaran et al., 2007; Stab and Gurevych, 2014; Swanson et al., 2015; Feng and Hirst, 2011; Habernal and Gurevych, 2017; Peldszus and Stede, 2015). Macro-level models (or dialogical models) and rhetorical models which focus on the We propose a novel approach to automatically identify the argument structure in persuasive dialogues that brings together the micro-level and the macro-level models of argumentation. Our approach identifies argument components in a full discussion thread and two kinds of argument relations: inter-turn relations (argumentative relations to support or attack the other person’s argument) and intra-turn relations (to support one’s claim). Figure 1 shows a thr"
D19-1291,L18-1258,1,0.851443,"on the relation prediction task on all possible pairs, using the labeled relations in the CMV data. For inter-turn relation prediction, we fine-tune first on the QR dataset, where the dialogue context more closely represents our labeled inter-post relations. Then, we fine-tune on inter-turn relation predic2937 tion using all possible pairs as training. This process is indicated in Figure 2, where we use the appropriate context fine-tuning to obtain a relation classifier. Discourse Relations Rhetorical Structure Theory was originally developed to offer an explanation of the coherence of texts. Musi et al. (2018) and, more recently Hewett et al. (2019), showed that discourse relations from RST often correlate with argumentative relations. We thus derive features from RST trees and train a classifier using these features to predict an argumentative relation. To extract features from a pair of argumentative components, we first concatenate the two components so that they form a single text input. We then use a state-of-the-art RST discourse parser (Ji and Eisenstein, 2014)3 to create parse trees and take the predicted discourse relation at the root of the parse tree as a categorical feature in a classif"
D19-1291,W14-2106,1,0.914783,"Missing"
D19-1291,P17-1091,0,0.282636,"mics of the dialogue itself. By combining recent advances in theoretical understanding of inferential anchors in dialogue with grammatical techniques for automatic recognition of pragmatic features, they produced results for illocutionary structure parsing which are comparable with existing techniques acting at a similar level such as rhetorical structure parsing. Furthermore, Visser et al. (2019) presented US2016, the largest publicly available set of corpora of annotated dialogical argumentation. Their annotation covers argumentative relations, dialogue acts and pragmatic features. Although Niculae et al. (2017) tried relation prediction between arguments in user comments on web discussion forums using structured SVM 2934 and RNN the work closest to our task is that of Morio and Fujita (2018). They propose a pointer network model that takes into consideration the constraints of thread structure. Their model discriminates between types of arguments (e.g., claims or premises) and both intra-turn and inter-turn relations, simultaneously. Their dataset, which is not publically available, is three times larger than ours, so instead we focus on transfer learning approaches that take advantage of discourse"
D19-1291,J17-1004,0,0.0564278,"of argument in a dialogue (Bentahar et al., 2010) have received less attention. Introduction Argument mining is a field of corpus-based discourse analysis that involves the automatic identification of argumentative structures in text. Most current studies have focused on monologues or micro-level models of argument that aim to identify the structure of a single argument by identifying the argument components (classes such as “claim” and “premise”) and relations between them (“support” or “attack”) (Somasundaran et al., 2007; Stab and Gurevych, 2014; Swanson et al., 2015; Feng and Hirst, 2011; Habernal and Gurevych, 2017; Peldszus and Stede, 2015). Macro-level models (or dialogical models) and rhetorical models which focus on the We propose a novel approach to automatically identify the argument structure in persuasive dialogues that brings together the micro-level and the macro-level models of argumentation. Our approach identifies argument components in a full discussion thread and two kinds of argument relations: inter-turn relations (argumentative relations to support or attack the other person’s argument) and intra-turn relations (to support one’s claim). Figure 1 shows a thread structure consisting of m"
D19-1291,D15-1110,0,0.0732525,"ntahar et al., 2010) have received less attention. Introduction Argument mining is a field of corpus-based discourse analysis that involves the automatic identification of argumentative structures in text. Most current studies have focused on monologues or micro-level models of argument that aim to identify the structure of a single argument by identifying the argument components (classes such as “claim” and “premise”) and relations between them (“support” or “attack”) (Somasundaran et al., 2007; Stab and Gurevych, 2014; Swanson et al., 2015; Feng and Hirst, 2011; Habernal and Gurevych, 2017; Peldszus and Stede, 2015). Macro-level models (or dialogical models) and rhetorical models which focus on the We propose a novel approach to automatically identify the argument structure in persuasive dialogues that brings together the micro-level and the macro-level models of argumentation. Our approach identifies argument components in a full discussion thread and two kinds of argument relations: inter-turn relations (argumentative relations to support or attack the other person’s argument) and intra-turn relations (to support one’s claim). Figure 1 shows a thread structure consisting of multiple posts with argument"
D19-1291,N16-1164,0,0.166102,"ucture Theory (Mann and Thompson, 1988) improves the results for relation identification both for inter-turn and intra-turn relations. Overall, our approach for argument mining obtains statistically significant improvement over a state-of-the-art model based on pointer networks (Morio and Fujita, 2018) and a strong baseline using a pre-trained language model (Devlin et al., 2019). We make our data,1 code, and trained models publicly available.2 2 Related Work Argument mining on monologues Most prior work in argument mining (AM) has focused on monologues or essays. Stab and Gurevych (2017) and Persing and Ng (2016) used pipeline approaches for AM, combining parts of the pipeline using integer linear programming (ILP) to classify argumentative components. Stab and Gurevych (2017) represented relations of argument components in essays as tree structures. Both Stab and Gurevych (2017) and Persing and Ng (2016) propose joint inference models using ILP to detect argumentative relations. They however rely on handcrafted (structural, lexical, indicator, and discourse) features. We on the other hand use a transfer learning approach for argument mining in discussion forums. Recent work has examined neural models"
D19-1291,D17-1143,0,0.0845745,"hes for AM, combining parts of the pipeline using integer linear programming (ILP) to classify argumentative components. Stab and Gurevych (2017) represented relations of argument components in essays as tree structures. Both Stab and Gurevych (2017) and Persing and Ng (2016) propose joint inference models using ILP to detect argumentative relations. They however rely on handcrafted (structural, lexical, indicator, and discourse) features. We on the other hand use a transfer learning approach for argument mining in discussion forums. Recent work has examined neural models for argument mining. Potash et al. (2017) use pointer networks (Vinyals et al., 2015) to predict both argumentative components and relations in a joint model. For our work, we focus primarily on relation prediction but conduct experiments with predicting argumentative components in a pipeline. A full end-to-end neural sequence tagging model was developed by Eger et al. (2017) that pre1 https://github.com/chridey/change-my-view-modes https://github.com/tuhinjubcse/AMPERSANDEMNLP2019 2 dicts argumentative discourse units (ADUs), components, and relations at the token level. In contrast, we assume we have ADUs and predict components and"
D19-1291,W15-4625,1,0.861548,"neural sequence tagging model was developed by Eger et al. (2017) that pre1 https://github.com/chridey/change-my-view-modes https://github.com/tuhinjubcse/AMPERSANDEMNLP2019 2 dicts argumentative discourse units (ADUs), components, and relations at the token level. In contrast, we assume we have ADUs and predict components and relations at that level. Argument mining on discussion forums (online discussion) Most computational work related to argumentation as a process has focused on the detection of agreement and disagreement in online interactions (Abbott et al., 2011; Sridhar et al., 2015; Rosenthal and McKeown, 2015; Walker et al., 2012). However, these approaches do not identify the argument components that the (dis)agreement has scope over (i.e., what has been targeted by a disagreement or agreement move). Also in these approaches, researchers predict the type of relation (e.g. agreement) given that a relationship exists. On the contrary, we predict both argument components as well as the existence of ˇ a relation between them. Boltuˇzi´c and Snajder (2014) and Murakami and Raymond (2010) address relations between complete arguments but without the micro-structure of arguments as in Stab and Gurevych ("
D19-1291,2007.sigdial-1.5,0,0.0594341,"e model. 1 Figure 1: Annotated Discussion Thread (C: Claim, P: Premise, MC: Main Claim) process of argument in a dialogue (Bentahar et al., 2010) have received less attention. Introduction Argument mining is a field of corpus-based discourse analysis that involves the automatic identification of argumentative structures in text. Most current studies have focused on monologues or micro-level models of argument that aim to identify the structure of a single argument by identifying the argument components (classes such as “claim” and “premise”) and relations between them (“support” or “attack”) (Somasundaran et al., 2007; Stab and Gurevych, 2014; Swanson et al., 2015; Feng and Hirst, 2011; Habernal and Gurevych, 2017; Peldszus and Stede, 2015). Macro-level models (or dialogical models) and rhetorical models which focus on the We propose a novel approach to automatically identify the argument structure in persuasive dialogues that brings together the micro-level and the macro-level models of argumentation. Our approach identifies argument components in a full discussion thread and two kinds of argument relations: inter-turn relations (argumentative relations to support or attack the other person’s argument) an"
D19-1291,P15-1012,0,0.0193909,"ine. A full end-to-end neural sequence tagging model was developed by Eger et al. (2017) that pre1 https://github.com/chridey/change-my-view-modes https://github.com/tuhinjubcse/AMPERSANDEMNLP2019 2 dicts argumentative discourse units (ADUs), components, and relations at the token level. In contrast, we assume we have ADUs and predict components and relations at that level. Argument mining on discussion forums (online discussion) Most computational work related to argumentation as a process has focused on the detection of agreement and disagreement in online interactions (Abbott et al., 2011; Sridhar et al., 2015; Rosenthal and McKeown, 2015; Walker et al., 2012). However, these approaches do not identify the argument components that the (dis)agreement has scope over (i.e., what has been targeted by a disagreement or agreement move). Also in these approaches, researchers predict the type of relation (e.g. agreement) given that a relationship exists. On the contrary, we predict both argument components as well as the existence of ˇ a relation between them. Boltuˇzi´c and Snajder (2014) and Murakami and Raymond (2010) address relations between complete arguments but without the micro-structure of argume"
D19-1291,D14-1006,0,0.0392804,"ted Discussion Thread (C: Claim, P: Premise, MC: Main Claim) process of argument in a dialogue (Bentahar et al., 2010) have received less attention. Introduction Argument mining is a field of corpus-based discourse analysis that involves the automatic identification of argumentative structures in text. Most current studies have focused on monologues or micro-level models of argument that aim to identify the structure of a single argument by identifying the argument components (classes such as “claim” and “premise”) and relations between them (“support” or “attack”) (Somasundaran et al., 2007; Stab and Gurevych, 2014; Swanson et al., 2015; Feng and Hirst, 2011; Habernal and Gurevych, 2017; Peldszus and Stede, 2015). Macro-level models (or dialogical models) and rhetorical models which focus on the We propose a novel approach to automatically identify the argument structure in persuasive dialogues that brings together the micro-level and the macro-level models of argumentation. Our approach identifies argument components in a full discussion thread and two kinds of argument relations: inter-turn relations (argumentative relations to support or attack the other person’s argument) and intra-turn relations (t"
D19-1291,J17-3005,0,0.566224,"tions based on Rhetorical Structure Theory (Mann and Thompson, 1988) improves the results for relation identification both for inter-turn and intra-turn relations. Overall, our approach for argument mining obtains statistically significant improvement over a state-of-the-art model based on pointer networks (Morio and Fujita, 2018) and a strong baseline using a pre-trained language model (Devlin et al., 2019). We make our data,1 code, and trained models publicly available.2 2 Related Work Argument mining on monologues Most prior work in argument mining (AM) has focused on monologues or essays. Stab and Gurevych (2017) and Persing and Ng (2016) used pipeline approaches for AM, combining parts of the pipeline using integer linear programming (ILP) to classify argumentative components. Stab and Gurevych (2017) represented relations of argument components in essays as tree structures. Both Stab and Gurevych (2017) and Persing and Ng (2016) propose joint inference models using ILP to detect argumentative relations. They however rely on handcrafted (structural, lexical, indicator, and discourse) features. We on the other hand use a transfer learning approach for argument mining in discussion forums. Recent work"
D19-1291,W15-4631,0,0.0448125,"Claim, P: Premise, MC: Main Claim) process of argument in a dialogue (Bentahar et al., 2010) have received less attention. Introduction Argument mining is a field of corpus-based discourse analysis that involves the automatic identification of argumentative structures in text. Most current studies have focused on monologues or micro-level models of argument that aim to identify the structure of a single argument by identifying the argument components (classes such as “claim” and “premise”) and relations between them (“support” or “attack”) (Somasundaran et al., 2007; Stab and Gurevych, 2014; Swanson et al., 2015; Feng and Hirst, 2011; Habernal and Gurevych, 2017; Peldszus and Stede, 2015). Macro-level models (or dialogical models) and rhetorical models which focus on the We propose a novel approach to automatically identify the argument structure in persuasive dialogues that brings together the micro-level and the macro-level models of argumentation. Our approach identifies argument components in a full discussion thread and two kinds of argument relations: inter-turn relations (argumentative relations to support or attack the other person’s argument) and intra-turn relations (to support one’s claim)"
D19-1291,walker-etal-2012-corpus,0,0.0480209,"l was developed by Eger et al. (2017) that pre1 https://github.com/chridey/change-my-view-modes https://github.com/tuhinjubcse/AMPERSANDEMNLP2019 2 dicts argumentative discourse units (ADUs), components, and relations at the token level. In contrast, we assume we have ADUs and predict components and relations at that level. Argument mining on discussion forums (online discussion) Most computational work related to argumentation as a process has focused on the detection of agreement and disagreement in online interactions (Abbott et al., 2011; Sridhar et al., 2015; Rosenthal and McKeown, 2015; Walker et al., 2012). However, these approaches do not identify the argument components that the (dis)agreement has scope over (i.e., what has been targeted by a disagreement or agreement move). Also in these approaches, researchers predict the type of relation (e.g. agreement) given that a relationship exists. On the contrary, we predict both argument components as well as the existence of ˇ a relation between them. Boltuˇzi´c and Snajder (2014) and Murakami and Raymond (2010) address relations between complete arguments but without the micro-structure of arguments as in Stab and Gurevych (2017). Ghosh et al. (2"
D19-1291,D18-1116,0,0.0636399,"Missing"
D19-5013,E12-1059,0,0.0218768,"lair framework (Akbik et al., 2018, 2019) that combines character level embeddings with different kinds of word embeddings as input to a BiLSTM-CRF model (Ma and Hovy, 2016; Lample et al., 2016). Akbik et al. (2018) have shown that stacking multiple pre-trained embeddings as input to the LSTM improves performance on the downstream sequence labeling task. We combine Glove embeddings (Pennington et al., 2014) with Urban Dictionary2 embeddings3 . Due to the small-size of our data set we additionally include one-hot-encoded features based on dictionary look-ups from the UBY dictionary provided by Gurevych et al. (2012). These features are based on concepts associated with the specific word such as offensive, vulgar, coarse, or ethnic slur. In total, 30 concept features were added as additional dimensions to the embedding representations. We also experimented with stacking BERT embeddings with all or some of the embeddings mentioned above. However, this resulted on lower Methods In the following we explain the details of our approach for the SLC and FLC tasks. 3.1 Fragment Level Classification (FLC) Sentence Level Classification (SLC) We fine-tuned BERT (Devlin et al., 2019) for the binary sentence-level cla"
D19-5013,N16-1030,0,0.0463072,"r the eighteen propaganda techniques can be found in Da San Martino et al. (2019b). However, the results on the shared task data are not directly comparable as more articles were added to shared task’s data. Da San Martino et al. (2019a) should be referred to for an accurate comparison between participants who all used the same development and test sets. 3 3.2 Our architecture for the sequence labeling task builds on the flair framework (Akbik et al., 2018, 2019) that combines character level embeddings with different kinds of word embeddings as input to a BiLSTM-CRF model (Ma and Hovy, 2016; Lample et al., 2016). Akbik et al. (2018) have shown that stacking multiple pre-trained embeddings as input to the LSTM improves performance on the downstream sequence labeling task. We combine Glove embeddings (Pennington et al., 2014) with Urban Dictionary2 embeddings3 . Due to the small-size of our data set we additionally include one-hot-encoded features based on dictionary look-ups from the UBY dictionary provided by Gurevych et al. (2012). These features are based on concepts associated with the specific word such as offensive, vulgar, coarse, or ethnic slur. In total, 30 concept features were added as addi"
D19-5013,P16-1101,0,0.0218784,"nnotation scheme for the eighteen propaganda techniques can be found in Da San Martino et al. (2019b). However, the results on the shared task data are not directly comparable as more articles were added to shared task’s data. Da San Martino et al. (2019a) should be referred to for an accurate comparison between participants who all used the same development and test sets. 3 3.2 Our architecture for the sequence labeling task builds on the flair framework (Akbik et al., 2018, 2019) that combines character level embeddings with different kinds of word embeddings as input to a BiLSTM-CRF model (Ma and Hovy, 2016; Lample et al., 2016). Akbik et al. (2018) have shown that stacking multiple pre-trained embeddings as input to the LSTM improves performance on the downstream sequence labeling task. We combine Glove embeddings (Pennington et al., 2014) with Urban Dictionary2 embeddings3 . Due to the small-size of our data set we additionally include one-hot-encoded features based on dictionary look-ups from the UBY dictionary provided by Gurevych et al. (2012). These features are based on concepts associated with the specific word such as offensive, vulgar, coarse, or ethnic slur. In total, 30 concept featu"
D19-5013,N19-4010,0,0.0204239,"Missing"
D19-5013,C18-1139,0,0.0132574,"use we do not have access to the gold labels of the official dev and test sets of the shared task. More details about the dataset and the annotation scheme for the eighteen propaganda techniques can be found in Da San Martino et al. (2019b). However, the results on the shared task data are not directly comparable as more articles were added to shared task’s data. Da San Martino et al. (2019a) should be referred to for an accurate comparison between participants who all used the same development and test sets. 3 3.2 Our architecture for the sequence labeling task builds on the flair framework (Akbik et al., 2018, 2019) that combines character level embeddings with different kinds of word embeddings as input to a BiLSTM-CRF model (Ma and Hovy, 2016; Lample et al., 2016). Akbik et al. (2018) have shown that stacking multiple pre-trained embeddings as input to the LSTM improves performance on the downstream sequence labeling task. We combine Glove embeddings (Pennington et al., 2014) with Urban Dictionary2 embeddings3 . Due to the small-size of our data set we additionally include one-hot-encoded features based on dictionary look-ups from the UBY dictionary provided by Gurevych et al. (2012). These feat"
D19-5013,D14-1162,0,0.0913886,"an Martino et al. (2019a) should be referred to for an accurate comparison between participants who all used the same development and test sets. 3 3.2 Our architecture for the sequence labeling task builds on the flair framework (Akbik et al., 2018, 2019) that combines character level embeddings with different kinds of word embeddings as input to a BiLSTM-CRF model (Ma and Hovy, 2016; Lample et al., 2016). Akbik et al. (2018) have shown that stacking multiple pre-trained embeddings as input to the LSTM improves performance on the downstream sequence labeling task. We combine Glove embeddings (Pennington et al., 2014) with Urban Dictionary2 embeddings3 . Due to the small-size of our data set we additionally include one-hot-encoded features based on dictionary look-ups from the UBY dictionary provided by Gurevych et al. (2012). These features are based on concepts associated with the specific word such as offensive, vulgar, coarse, or ethnic slur. In total, 30 concept features were added as additional dimensions to the embedding representations. We also experimented with stacking BERT embeddings with all or some of the embeddings mentioned above. However, this resulted on lower Methods In the following we e"
D19-5013,D17-1317,0,0.0599152,"5th out of 26 teams on the sentencelevel classification task and 5th out of 11 teams on the fragment-level classification task based on our scores on the blind test set. We present our models, a discussion of our ablation studies and experiments, and an analysis of our performance on all eighteen propaganda techniques present in the corpus of the shared task. 1 Introduction Propaganda aims at influencing a target audience with a specific group agenda using faulty reasoning and/or emotional appeals (Miller, 1939). Automatic detection of propaganda has been studied mainly at the article level (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019). However, in order to build computational models that can explain why an article is propagandistic, the model would need to detect specific techniques present at sentence or even token level. The NLP4IF shared task on fine-grained propaganda detection aims to produce models capable of spotting propaganda techniques in sentences and text fragments in news articles (Da San Martino et al., 2019a). The data for this task consist of news articles that were labeled at the fragment level with one of eighteen propaganda techniques. There are two sub-tasks in this shared"
D19-5013,D19-5024,0,0.360415,"Pfeiffer∗ Smaranda Muresan†‡ † Department of Computer Science, Columbia University ‡ Data Science Institute, Columbia University ∗ Ubiquitous Knowledge Processing Lab, Technische Universitat Darmstadt {tariq.a, smara}@columbia.edu pfeiffer@ukp.informatik.tu-darmstadt.de Abstract pressed in a text fragment together with the beginning and the end of that text fragment. This task is evaluated based on the prediction of the type of propaganda technique and the intersection between the gold and the predicted spans. The details to the evaluation measure used for the FLC task are explained in Da San Martino et al. (2019a). Both sub-tasks were automatically evaluated on a unified development set. The system performance was centrally assessed without distributing the gold labels, however allowing for an unlimited number of submissions. The final performance on the test set was similarly evaluated, with the difference that the feedback was given only after the submission was closed, simultaneously concluding the shared-task. In this paper, we describe the data in Section 2, our proposed methods for both sub-tasks in Section 3, and analyze the results and errors of our models in Section 4. This paper presents th"
D19-5013,D19-1565,0,0.11118,"Missing"
D19-5013,N19-1423,0,\N,Missing
J18-4009,D15-1075,0,0.0293266,"rm dependencies (Hochreiter and Schmidhuber 1997). Recently, LSTMs have been shown to be effective in NLI tasks such as Recognizing Textual Entailment, where the goal is to establish the relationship between two inputs (e.g., a premise and a hypothesis) (Bowman et al. 2015; Parikh et al. 2016; Rockt¨aschel et al. 2016). LSTMs address the vanishing gradient problem commonly found in recurrent neural networks by incorporating gating functions into their state dynamics (Hochreiter and Schmidhuber 1997). We introduce some notations and terminology standard in the LSTM literature (Tai, Socher, and Manning 2015). The LSTM unit at each time step t is defined as a collection of vectors: an input gate it , a forget gate ft , an output gate ot , a memory cell ct , and a hidden state ht . The LSTM transition equations are: it = σ(Wi ∗ [ht−1 , xt ] + bi ) ft = σ(Wf ∗ [ht−1 , xt ] + bf ) ot = σ(Wo ∗ [ht−1 , xt ] + bo ) C˜ t = tanh(Wc ∗ [ht−1 , xt ] + bc ) (1) ct = ft ct−1 + it C˜ t ht = ot tanh(ct ) where xt is the input at the current time step, σ is the logistic sigmoid function, and denotes element-wise multiplication. The input gate controls how much each unit is updated, the forget gate controls the ex"
J18-4009,P09-2041,0,0.0892955,"Missing"
J18-4009,W10-2914,0,0.410283,"Missing"
J18-4009,W16-0425,0,0.467235,"rony and sarcasm are a type of interactional phenomenon with specific perlocutionary effects on the hearer (Haverkate 1990), such as to break their pattern of expectation. For the current report, we do not make a clear distinction between sarcasm and verbal irony. Most computational models for sarcasm detection have considered ˜ utterances in isolation (Davidov, Tsur, and Rappoport 2010; Gonz´alez-Ib´anez, Muresan, and Wacholder 2011; Liebrecht, Kunneman, and Van den Bosch 2013; Riloff et al. 2013; Maynard and Greenwood 2014; Ghosh Guo, and Muresan 2015; Joshi, Sharma, and Bhattacharyya 2015; Ghosh and Veale 2016; Joshi et al. 2016b). In many instances, however, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al. 2014). Thus, to detect the speaker’s sarcastic intent, it is necessary (even if maybe not sufficient) to consider their utterance(s) in the larger conversation context. Consider the Twitter conversation example in Table 1. Without the context of userA’s statement, the sarcastic intent of userB’s response might not be detected. In this article, we investigate the role of conversation context for the detection of sarcasm in soci"
J18-4009,D17-1050,0,0.574914,"Missing"
J18-4009,W17-5523,1,0.362395,"Missing"
J18-4009,D15-1116,1,0.931298,"Missing"
J18-4009,P11-2008,0,0.065323,"Missing"
J18-4009,P11-2102,1,0.940738,"Missing"
J18-4009,P15-2124,0,0.290547,"Missing"
J18-4009,K16-1015,0,0.579782,"type of interactional phenomenon with specific perlocutionary effects on the hearer (Haverkate 1990), such as to break their pattern of expectation. For the current report, we do not make a clear distinction between sarcasm and verbal irony. Most computational models for sarcasm detection have considered ˜ utterances in isolation (Davidov, Tsur, and Rappoport 2010; Gonz´alez-Ib´anez, Muresan, and Wacholder 2011; Liebrecht, Kunneman, and Van den Bosch 2013; Riloff et al. 2013; Maynard and Greenwood 2014; Ghosh Guo, and Muresan 2015; Joshi, Sharma, and Bhattacharyya 2015; Ghosh and Veale 2016; Joshi et al. 2016b). In many instances, however, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al. 2014). Thus, to detect the speaker’s sarcastic intent, it is necessary (even if maybe not sufficient) to consider their utterance(s) in the larger conversation context. Consider the Twitter conversation example in Table 1. Without the context of userA’s statement, the sarcastic intent of userB’s response might not be detected. In this article, we investigate the role of conversation context for the detection of sarcasm in social media discussion"
J18-4009,D16-1104,0,0.212248,"type of interactional phenomenon with specific perlocutionary effects on the hearer (Haverkate 1990), such as to break their pattern of expectation. For the current report, we do not make a clear distinction between sarcasm and verbal irony. Most computational models for sarcasm detection have considered ˜ utterances in isolation (Davidov, Tsur, and Rappoport 2010; Gonz´alez-Ib´anez, Muresan, and Wacholder 2011; Liebrecht, Kunneman, and Van den Bosch 2013; Riloff et al. 2013; Maynard and Greenwood 2014; Ghosh Guo, and Muresan 2015; Joshi, Sharma, and Bhattacharyya 2015; Ghosh and Veale 2016; Joshi et al. 2016b). In many instances, however, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al. 2014). Thus, to detect the speaker’s sarcastic intent, it is necessary (even if maybe not sufficient) to consider their utterance(s) in the larger conversation context. Consider the Twitter conversation example in Table 1. Without the context of userA’s statement, the sarcastic intent of userB’s response might not be detected. In this article, we investigate the role of conversation context for the detection of sarcasm in social media discussion"
J18-4009,W15-2905,0,0.0374692,"olation. However, even humans have difficulty sometimes in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al. 2014). Recently, an increasing number of researchers have started using contextual information for irony and sarcasm detection. The term context loosely refers to any information that is available beyond the utterance itself (Joshi, Bhattacharyya, and Carman 2017). There are two major research directions— author context and conversation context—and we briefly discuss them here. Author Context. Researchers often examined the author-specific context (Khattri et al. 2015; Rajadesingan, Zafarani, and Liu 2015). For instance, Khattri et al. (2015) studied the historical tweets of a particular author to learn about the author’s prevailing sentiment toward particular targets (e.g., named entities). Here, historical tweets are considered as the author’s context. Khattri et al. hypothesized that altering sentiment toward a particular target in the candidate tweet may represent sarcasm. Rajadesingan, Zafarani, and Liu (2015) create features based on authors’ previous tweets, for instance, an author’s familiarity with sarcasm. Finally, Amir et al. (2016) enhanced Raj"
J18-4009,L18-1102,0,0.302619,"Missing"
J18-4009,W13-1605,0,0.187686,"Missing"
J18-4009,maynard-greenwood-2014-cares,0,0.464782,"tive language use such as sarcasm and irony. Recognizing sarcasm and verbal ∗ The research was carried out while Debanjan was a Ph.D. candidate at Rutgers University. Submission received: 15 October 2017; revised version received: 5 May 2018; accepted for publication: 20 August 2018. doi:10.1162/coli a 00336 © 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 4 irony is critical for understanding people’s actual sentiments and beliefs (Maynard and Greenwood 2014). For instance, the utterance “I love waiting at the doctor’s office for hours . . . ” is ironic, expressing a negative sentiment toward the situation of “waiting for hours at the doctor’s office,” even if the speaker uses positive sentiment words such as “love.” Verbal irony and sarcasm are a type of interactional phenomenon with specific perlocutionary effects on the hearer (Haverkate 1990), such as to break their pattern of expectation. For the current report, we do not make a clear distinction between sarcasm and verbal irony. Most computational models for sarcasm detection have considered"
J18-4009,P16-1104,0,0.302143,"t-patterns (Veale and Hao 2010), specific hashtags (Maynard and Greenwood 2014) as well as semi-supervised approach (Davidov, Tsur, and Rappoport 2010). Researchers have also examined different characteristics of sarcasm, such as sarcasm detection as a sense-disambiguation problem (Ghosh, Guo, and Muresan 2015) and sarcasm as a contrast between a positive sentiment and negative situation (Riloff et al. 2013; Joshi, Sharma, and Bhattacharyya 2015). Apart from linguistically motivated contextual knowledge, cognitive features, such as eye-tracking information, are also used in sarcasm detection (Mishra et al. 2016). Schifanella et al. (2016) propose a multimodal approach, where textual and visual features are combined for sarcasm detection. Some studies present approaches for sarcasm detection in languages other than English. For example, Pt´acˇ ek, Habernal, and 1 We use Theano Python library for the LSTM-based experiments. Code available at https://github.com/ debanjanghosh/sarcasm context and https://github.com/Alex-Fabbri/deep learning nlp sarcasm/. 758 Ghosh, Fabbri, and Muresan Sarcasm Analysis Using Conversation Context Hong (2014) use various n-grams, including unigrams, bigrams, and trigrams, a"
J18-4009,W17-5537,0,0.329534,"ddit threads and were allowed to utilize additional context for sarcasm labeling. They also use a lexical classifier to automatically identify sarcastic comments and show that the model often fails to recognize the same examples for which the annotators requested more context. Bamman and Smith (2015) considered conversation context in addition to “author and addressee” features, which are derived from the author’s historical tweets, profile information, and historical communication between the author and the addressee. Their results show only a minimal impact of modeling conversation context. Oraby et al. (2017) have studied the “pre” and “post” messages from debate forums as well as Twitter to identify whether rhetorical questions are used sarcastically or not. For both corpora, adding “pre” and “post” messages do not seem to significantly affect the F1 scores, even though using the “post” message as context seems to improve for the sarcastic class (Oraby et al. 2017). Unlike these approaches that model the utterance and context together, Wang et al. (2015) and Joshi et al. (2016a) use a sequence labeling approach and show that conversation helps in sarcasm detection. Inspired by this idea of modeli"
J18-4009,W16-3604,0,0.482244,"he unit of analysis (i.e., what we label as sarcastic or not sarcastic) is a message/turn in a social media conversation (i.e., a tweet in Twitter or a post/comment in discussion forums). We call this unit current turn (C TURN). The conversation context that we consider is the prior turn (P TURN), and, when available, also the succeeding turn (S TURN), which is the reply to the current turn. Table 1 shows some examples of sarcastic messages (C TURNs), together with their respective prior turns (P TURN) taken from Twitter and two discussion forum corpora: the Internet Argument Corpus (IACv2 ) (Oraby et al. 2016) and Reddit (Khodak, Saunshi, and Vodrahalli 2018). Table 2 shows examples from the IACv2 corpus of sarcastic messages (C TURNs; userB’s post) and the conversation context given by the prior turn (P TURN; userA’s post) as well as the succeeding turn (S TURN; userC’s post). We address three specific questions: 1. Does modeling of conversation context help in sarcasm detection? 2. Can humans and computational models identify what part of the prior turn (P TURN) triggered the sarcastic reply (C TURN) (e.g., which sentence(s) from userC’s turn triggered userD’s sarcastic reply in Table 1)? 3. Give"
J18-4009,D16-1244,0,0.0844183,"Missing"
J18-4009,C14-1022,0,0.0957607,"Missing"
J18-4009,D13-1066,0,0.569399,"Missing"
J18-4009,C16-1270,0,0.0238408,"incongruity might become diffuse if the inputs are combined too soon (i.e., using one LSTM on combined current turn and context). LSTM for Natural Language Inference (NLI) Tasks and Sarcasm Detection. LSTM networks are a particular type of recurrent neural networks that have been shown to be effective in NLI tasks, especially where the task is to establish the relationship between multiple inputs. For instance, in recognizing textual entailment research, LSTM networks, especially the attention-based models, are highly accurate (Bowman et al. 2015; Parikh et al. 2016; Rockt¨aschel et al. 2016; Sha et al. 2016). Rockt¨aschel et al. (2016) presented various word-based and conditional attention models that show how the entailment relationship between the hypothesis and the premise can be effectively derived. Parikh et al. (2016) use attention to decompose the RTE problem into subproblems that can be solved separately and Sha et al. (2016) presented an altered version (“re-read LSTM”) of LSTM that is similar to word attention models of Rockt¨aschel et al. (2016). Likewise, recently LSTMs are used in sarcasm detection research (Ghosh and Veale 2017; Huang, Huang, and Chen 2017; Oraby et al. 2017). Oraby"
J18-4009,strapparava-valitutti-2004-wordnet,0,0.255385,"Missing"
J18-4009,P15-1150,0,0.137452,"Missing"
J18-4009,walker-etal-2012-corpus,0,0.0416999,"mselves labeled their posts as sarcastic) in the case of Twitter and Reddit data. On the other hand, we have labels obtained via crowdsourcing as is the case for the IAC (Oraby et al. 2016). We first introduce the different data sets we use and then point out some differences between them that could impact results and modeling choices. IACv2 ). Internet Argument Corpus (IAC) is a publicly Internet Argument Corpus V2 (IAC available corpus of online forum conversations on a range of social and political topics, from gun control debates and marijuana legalization to climate change and evolution (Walker et al. 2012). The corpus comes with annotations of different types of pragmatic categories, such as agreement/disagreement (between a pair of online posts), nastiness, and sarcasm. There are different versions of IAC and we use a specific subset of IAC in this research. Oraby et al. (2016) have introduced a subset of the Internet Argument Corpus V2 that contains 9,400 posts labeled as sarcastic or non-sarcastic, called Sarcasm Corpus V2 (balanced data set). To obtain the gold labels, Oraby et al. (2016) first used a weakly supervised pattern learner to learn sarcastic and non-sarcastic patterns from the I"
J18-4009,P14-2084,0,0.687018,"the current report, we do not make a clear distinction between sarcasm and verbal irony. Most computational models for sarcasm detection have considered ˜ utterances in isolation (Davidov, Tsur, and Rappoport 2010; Gonz´alez-Ib´anez, Muresan, and Wacholder 2011; Liebrecht, Kunneman, and Van den Bosch 2013; Riloff et al. 2013; Maynard and Greenwood 2014; Ghosh Guo, and Muresan 2015; Joshi, Sharma, and Bhattacharyya 2015; Ghosh and Veale 2016; Joshi et al. 2016b). In many instances, however, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al. 2014). Thus, to detect the speaker’s sarcastic intent, it is necessary (even if maybe not sufficient) to consider their utterance(s) in the larger conversation context. Consider the Twitter conversation example in Table 1. Without the context of userA’s statement, the sarcastic intent of userB’s response might not be detected. In this article, we investigate the role of conversation context for the detection of sarcasm in social media discussions (Twitter conversations and discussion forums). The unit of analysis (i.e., what we label as sarcastic or not sarcastic) is a message/turn in a social medi"
J18-4009,H05-1044,0,0.172508,"Missing"
J18-4009,N16-1174,0,0.0464111,"es vary for each vector element, the model can learn to represent information over multiple time scales. As our goal is to explore the role of contextual information (e.g., prior turn and/or succeeding turn) for recognizing whether the current turn is sarcastic or not, we will use multiple LSTMs: one that reads the current turn and one (or two) that read(s) the context (e.g., one LSTM will read the prior turn and one will read the succeeding turn when available). Attention-based LSTM Networks. Attentive neural networks have been shown to perform well on a variety of NLP tasks (Xu et al. 2015; Yang et al. 2016; Yin et al. 2016). Using attention-based LSTM will accomplish two goals: (1) test whether they achieve higher performance than simple LSTM models and (2) use the attention weights produced by the LSTM models to perform the qualitative analyses that enable us to answer the latter two questions we want to address (e.g., which portions of context triggers the sarcastic reply). Yang et al. (2016) have included two levels of attention mechanisms, one at the word level and another at the sentence level, where the sentences are in turn produced by attentions over words (i.e., the hierarchical model)"
J18-4009,Q16-1019,0,0.0352524,"ector element, the model can learn to represent information over multiple time scales. As our goal is to explore the role of contextual information (e.g., prior turn and/or succeeding turn) for recognizing whether the current turn is sarcastic or not, we will use multiple LSTMs: one that reads the current turn and one (or two) that read(s) the context (e.g., one LSTM will read the prior turn and one will read the succeeding turn when available). Attention-based LSTM Networks. Attentive neural networks have been shown to perform well on a variety of NLP tasks (Xu et al. 2015; Yang et al. 2016; Yin et al. 2016). Using attention-based LSTM will accomplish two goals: (1) test whether they achieve higher performance than simple LSTM models and (2) use the attention weights produced by the LSTM models to perform the qualitative analyses that enable us to answer the latter two questions we want to address (e.g., which portions of context triggers the sarcastic reply). Yang et al. (2016) have included two levels of attention mechanisms, one at the word level and another at the sentence level, where the sentences are in turn produced by attentions over words (i.e., the hierarchical model). We experiment wi"
J18-4009,Q18-1009,0,0.0564345,"Missing"
L18-1258,W16-2803,0,0.0783999,"akers and later professionally translated into English. The argument structure was annotated according to the scheme proposed by Peldszus and Stede (2013), which builds on the ideas of Freeman (2011). Briefly, the texts are segmented into argumentative units, each unit has an argumentative role and is related to another unit, except for the single main claim (resulting overall in a tree structure). Furthermore, the corpus was enriched with discourse structure information based on 1629 – if the effect is the case, the cause is probably the case RST and SDRT theories by Stede et al. (2016), and Becker et al. (2016) have provided the additional layer of situation entity types (Becker et al., 2016). Annotating the argument schemes, covering the underlying inferential moves, will provide a valuable annotation layer for studying the mechanics of argumentation from a theoretical, yet empirically grounded perspective, and for argumentation mining. 2.2. – if a quality characterizes the cause, then such quality characterizes the effect too – if the realization of the goal necessitates the means x, x must be adopted – if an action does not allow to achieve the goal, it should not be undertaken Annotation guideli"
L18-1258,P11-1099,0,0.0258086,"d corpora that focus on the first three steps, with genres ranging from persuasive essays and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-available data. In this paper, we report on an annotation project that adds information about inferential rules, in the shape of argument schemes, to an existing corpus that already holds annotations of argumentation structure as well as discourse structure based on Rhetorical Structure Theory (RST) and Segmented Discourse Representation Theory(SDRT) (Stede et al., 2016). Our emphasis thus is on a multi-layer resource that allows for correlating different lev"
L18-1258,W14-2106,1,0.881402,"mpasses the following steps (Peldszus and Stede, 2013): 1. the segmentation of texts into argumentative discourse units(ADUs); 2. the classification of the role (e.g., claim, premise) played by each ADU; 3. the analysis of the relations linking ADUs (e.g., support, attack); and 4. the identification argument schemes, namely the implicit and explicit inferential relations within and across ADUs. Annotation efforts have so far provided corpora that focus on the first three steps, with genres ranging from persuasive essays and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-available data. In this"
L18-1258,W17-5109,0,0.0169349,"s and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-available data. In this paper, we report on an annotation project that adds information about inferential rules, in the shape of argument schemes, to an existing corpus that already holds annotations of argumentation structure as well as discourse structure based on Rhetorical Structure Theory (RST) and Segmented Discourse Representation Theory(SDRT) (Stede et al., 2016). Our emphasis thus is on a multi-layer resource that allows for correlating different levels and for studying dependencies between discourse relations and argument str"
L18-1258,J17-1004,0,0.101946,"schemes both for support and attack relations, and a new user-friendly annotation tool. The multi-layer annotated corpus allows us to conduct an initial study of dependencies between discourse relations (according to Rhetorical Structure Theory (Mann and Thompson, 1988)) and argument schemes. Our main contribution is that of offering the first resource for the combined study of (argumentative) discourse relations and inferential moves. Keywords: argumentation mining, argument schemes, discourse relations 1. Introduction Recent interest in Argumentation Mining (e.g., (Lippi and Torroni, 2016; Habernal and Gurevych, 2017) has brought to the fore the need for corpora annotated with argument information, which can be used as training data. Generally, the automatic search for arguments encompasses the following steps (Peldszus and Stede, 2013): 1. the segmentation of texts into argumentative discourse units(ADUs); 2. the classification of the role (e.g., claim, premise) played by each ADU; 3. the analysis of the relations linking ADUs (e.g., support, attack); and 4. the identification argument schemes, namely the implicit and explicit inferential relations within and across ADUs. Annotation efforts have so far pr"
L18-1258,W15-0501,0,0.0196503,"earch for arguments encompasses the following steps (Peldszus and Stede, 2013): 1. the segmentation of texts into argumentative discourse units(ADUs); 2. the classification of the role (e.g., claim, premise) played by each ADU; 3. the analysis of the relations linking ADUs (e.g., support, attack); and 4. the identification argument schemes, namely the implicit and explicit inferential relations within and across ADUs. Annotation efforts have so far provided corpora that focus on the first three steps, with genres ranging from persuasive essays and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-avai"
L18-1258,W16-2810,1,0.894866,", it should not be terminated The annotation consists of two subtasks: 1) given a SUPPORT or REBUT relation, identify the argument scheme among the 8 middle level schemes (DEFINI TIONAL , CAUSAL , MEREOLOGICAL , ANALOGY , OPPOSI TION , PRACTICAL EVALUATION , AUTHORITY ) or NONE if no reasoning is present; and 2) identify the associated inference rule. An early pilot annotation testing the first version of guidelines had been carried out on top of 30 persuasive essays from the corpus of Stab and Gurevych (2014), obtaining a fair inter-annotator agreement with trained but non-expert annotators (Musi et al., 2016). The guidelines contain identification questions, linguistic clues and inferential rules for each argument scheme. Annotators are asked to first browse the identification yes/no questions and check whether inferential rules apply and linguistic clues are indeed present. The description of CAUSAL argument schemes contains, for example, the following information: – if something has a positive value, it should be supported/continued/promoted/maintained • Identification Question: is x a cause/effect of y or is it a means to obtain y? • Other clues: Evaluations about actions play a role as common"
L18-1258,C14-1142,0,0.118212,"g steps (Peldszus and Stede, 2013): 1. the segmentation of texts into argumentative discourse units(ADUs); 2. the classification of the role (e.g., claim, premise) played by each ADU; 3. the analysis of the relations linking ADUs (e.g., support, attack); and 4. the identification argument schemes, namely the implicit and explicit inferential relations within and across ADUs. Annotation efforts have so far provided corpora that focus on the first three steps, with genres ranging from persuasive essays and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-available data. In this paper, we report on an an"
L18-1258,L16-1167,1,0.572851,"Argument Schemes to Discourse Relations Elena Musi§ , Tariq Alhindi§ , Manfred Stede † , Leonard Kriese † , Smaranda Muresan § , Andrea Rocci Columbia University§ , Potsdam University† , Universita della Svizzera italiana 475 Riverside Drive New York (USA) § , Karl-Liebknecht-Str. 24-25 Potsdam (Germany) † , Via Buffi 13 Lugano (Switzerland) em3202/ta2509/sm761@columbia.edu § , stede@uni-potsdam.de, leonard.kriese@hotmail.de, andrea.rocci@usi.ch Abstract We present a multi-layer annotated corpus of 112 argumentative microtexts encompassing not only argument structure and discourse relations (Stede et al., 2016), but also argument schemes — the inferential relations linking premises to claims. We propose a set of guidelines for the annotation of argument schemes both for support and attack relations, and a new user-friendly annotation tool. The multi-layer annotated corpus allows us to conduct an initial study of dependencies between discourse relations (according to Rhetorical Structure Theory (Mann and Thompson, 1988)) and argument schemes. Our main contribution is that of offering the first resource for the combined study of (argumentative) discourse relations and inferential moves. Keywords: argu"
L18-1258,walker-etal-2012-corpus,0,0.0626493,"de, 2013): 1. the segmentation of texts into argumentative discourse units(ADUs); 2. the classification of the role (e.g., claim, premise) played by each ADU; 3. the analysis of the relations linking ADUs (e.g., support, attack); and 4. the identification argument schemes, namely the implicit and explicit inferential relations within and across ADUs. Annotation efforts have so far provided corpora that focus on the first three steps, with genres ranging from persuasive essays and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-available data. In this paper, we report on an annotation project that"
muresan-klavans-2002-method,W01-0513,0,\N,Missing
muresan-klavans-2002-method,A00-2018,0,\N,Missing
muresan-klavans-2002-method,J93-1007,0,\N,Missing
muresan-klavans-2002-method,J87-3003,1,\N,Missing
P07-1105,C04-1180,0,0.0573198,"ntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar. 1 Introduction There is considerable interest in learning computational grammars.1 While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto1 This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank"
P07-1105,W05-0602,0,0.0930758,"atural language strings annotated with their semantics, along with basic assumptions about natural language syntax. We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar. 1 Introduction There is considerable interest in learning computational grammars.1 While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto1 This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank Judith Klavans for her contributions over the course"
P07-1105,P06-1111,0,0.020267,"rning computational grammars.1 While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005). Learning both syntax and semantics is arguably more difficult than learning syntax alone. In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993). Haghighi and Klein (2006) show that using a handful of “proto1 This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs). We would like to thank Judith Klavans for her contributions over the course of this research, Kathy McKeown for her input, and several anonymous reviewers for very useful feedback on earlier drafts of this paper. In this paper, we present a new grammar formalism and a new learning method which together address the problem of learning a syntactic-semantic grammar in the presence of a r"
P07-1105,P99-1013,0,0.846434,"Missing"
P08-1115,J96-1002,0,0.0180756,"Missing"
P08-1115,J90-2002,0,0.449742,"te state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for ChineseEnglish and Arabic-English translation. 1 Introduction When Brown and colleagues introduced statistical machine translation in the early 1990s, their key insight – harkening back to Weaver in the late 1940s – was that translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990). They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models). Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007). Why, however, should this advantage be limited to translation from sp"
P08-1115,P05-1033,0,0.398479,"for Arabic — across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input. In addition, we generalize lattice decoding algorithmically, extending it for the first time to hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Formally, the approach we take can be thought of as a “noisier channel”, where an observed signal o gives rise to a set of source-language strings f 0 ∈ F(o) and we seek eˆ = arg max max P r(e, f 0 |o) e eˆ = arg max P r(e|f ) e (1) An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f . There, Bertoldi and others have recently found that, rather than translating a single-best transcription f , it is advantageous to allow the MT decoder to f 0 ∈F (o) = arg max max P r(e)P r(f 0 |e,"
P08-1115,J07-2003,0,0.845516,"cross a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input. In addition, we generalize lattice decoding algorithmically, extending it for the first time to hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Formally, the approach we take can be thought of as a “noisier channel”, where an observed signal o gives rise to a set of source-language strings f 0 ∈ F(o) and we seek eˆ = arg max max P r(e, f 0 |o) e eˆ = arg max P r(e|f ) e (1) An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f . There, Bertoldi and others have recently found that, rather than translating a single-best transcription f , it is advantageous to allow the MT decoder to f 0 ∈F (o) = arg max max P r(e)P r(f 0 |e, o) e f 0 ∈F (o"
P08-1115,W07-0721,0,0.0249901,"Missing"
P08-1115,W07-0729,1,0.806539,"ximum probability value over all possible paths in the lattice for each jump considered, which is similar to the approach we have taken. Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases. Applications of source lattices outside of the domain of spoken language translation have been far more limited. Costa-juss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simpli(Source Type) cs hs ss hs+ss hs+ss+cs hs+ss+cs."
P08-1115,N04-1001,0,0.0109403,"ical phrase-based translation model, using our modified version of Hiero (Chiang, 2005; Chiang, 2007). These two translation model types illustrate the applicability of the theoretical contributions presented in Section 2 and Section 3. We observed that the coverage of named entities (NEs) in our baseline systems was rather poor. Since names in Chinese can be composed of relatively long strings of characters that cannot be translated individually, when generating the segmentation lattices that included cs arcs, we avoided segmenting NEs of type PERSON, as identified using a Chinese NE tagger (Florian et al., 2004). The results are summarized in Table 4. We see that using word lattices improves BLEU scores both in the phrase-based model and hierarchical model as compared to the single-best segmentation approach. All results using our word-lattice decoding for the hierarchical models (hs+ss and hs+ss+cs) are significantly better than the best segmentation (ss).4 For the phrase-based model, we obtain significant gains using our word-lattice decoder using all three segmentations on MT05. The other results, while better than the best segmentation (hs) by at least 0.3 BLEU points, are not statistically signi"
P08-1115,J99-4004,0,0.0214313,"in this table is a triple hFij , pij , Rij i 2.2 Parsing word lattices Chiang (2005) introduced hierarchical phrase-based translation models, which are formally based on synchronous context-free grammars (SCFGs). Translation proceeds by parsing the input using the source language side of the grammar, simultaneously building a tree on the target language side via the target side of the synchronized rules. Since decoding is equivalent to parsing, we begin by presenting a parser for word lattices, which is a generalization of a CKY parser for lattices given in Cheppalier et al. (1999). Following Goodman (1999), we present our lattice parser as a deductive proof system in Figure 2. The parser consists of two kinds of items, the first with the form [X → α • β, i, j] representing rules that have yet to be completed and span node i to node j. The other items have the form [X, i, j] and indicate that non-terminal X spans [i, j]. As with sentence parsing, the goal is a deduction that covers the spans of the entire input lattice [S, 0, |V |− 1]. The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an -edge without advancing the dot in an incompl"
P08-1115,N06-2013,0,0.0347615,"mentation variant we made use of. The limitation of this approach is that as the amount and variety of training data increases, the optimal segmentation strategy changes: more aggressive segmentation results 2 The segmentation process is ambiguous, even for native speakers of Chinese. 1 硬 硬质 0 号 合金 质 2 金 合 4 5 称 工 6 号称 "" 7 8 工业 业 10 牙 9 牙齿 齿 11 "" 12 3 硬质合金 Figure 5: Sample Chinese segmentation lattice using three segmentations. in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). Lattices allow the decoder to make decisions about what granularity of segmentation to use subsententially. 4.1 Chinese Word Segmentation Experiments In our experiments we used two state-of-the-art Chinese word segmenters: one developed at Harbin Institute of Technology (Zhao et al., 2001), and one developed at Stanford University (Tseng et al., 2005). In addition, we used a character-based segmentation. In the remaining of this paper, we use cs for character segmentation, hs for Harbin segmentation and ss for Stanford segmentation. We built two types of lattices: one that combines the Harbi"
P08-1115,N03-1017,0,0.215202,"bic-English and Chinese-English translation. In Section 5 we discuss relevant prior work, and we conclude in Section 6. 2 Decoding Most statistical machine translation systems model translational equivalence using either finite state transducers or synchronous context free grammars (Lopez, to appear 2008). In this section we discuss the issues associated with adapting decoders from both classes of formalism to process word lattices. The first decoder we present is a SCFG-based decoder similar to the one described in Chiang (2007). The second is a phrase-based decoder implementing the model of Koehn et al. (2003). 2.1 Word lattices A word lattice G = hV, Ei is a directed acyclic graph that formally is a weighted finite state automaton (FSA). We further stipulate that exactly one node has no outgoing edges and is designated the ‘end node’. Figure 1 illustrates three classes of word lattices. 1013 c 2 a x 0 m=1 b 1 b 1 ε 1 x 0 3 2 d c 3 2 b c 3 y a Figure 1: Three examples of word lattices: (a) sentence, (b) confusion network, and (c) non-linear word lattice. A word lattice is useful for our purposes because it permits any finite set of strings to be represented and allows for substrings common to multi"
P08-1115,2005.iwslt-1.8,0,0.0223693,". 3.1 MT05 0.3063 0.3176 Experimental results We tested the effect of the distance metric on translation quality using Chinese word segmentation lattices (Section 4.1, below) using both a hierarchical and phrase-based system modified to translate word lattices. We compared the shortest-path distance metric with a baseline which uses the difference in node number as the distortion distance. For an additional datapoint, we added a lexicalized reordering model that models the probability of each phrase pair appearing in three different orientations (swap, monotone, other) in the training corpus (Koehn et al., 2005). Table 2 summarizes the results of the phrasebased systems. On both test sets, the shortest path metric improved the BLEU scores. As expected, the lexicalized reordering model improved translation quality over the baseline; however, the improvement was more substantial in the model that used the shortest-path distance metric (which was already a higher baseline). Table 3 summarizes the results of our experiment comparing the performance of two distance metrics to determine whether a rule has exceeded the decoder’s span limit. The pattern is the same, showing a clear increase in BLEU for the s"
P08-1115,P07-2045,0,0.0604115,"translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990). They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models). Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007). Why, however, should this advantage be limited to translation from spoken input? Even for text, there are often multiple ways to derive a sequence of words from the input string. Segmentation of Chinese, decompounding in German, morphological analysis for Arabic — across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation"
P08-1115,W04-3250,0,0.288436,"Missing"
P08-1115,2000.eamt-1.5,0,0.135111,"Missing"
P08-1115,P02-1038,0,0.179596,"Missing"
P08-1115,2005.iwslt-1.18,0,0.522209,"o the approach we have taken. Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases. Applications of source lattices outside of the domain of spoken language translation have been far more limited. Costa-juss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simpli(Source Type) cs hs ss hs+ss hs+ss+cs hs+ss+cs.lexRo MT05 BLEU 0.2833 0.2905 0.2894 0.2938 0.2993 0.3072 MT06 BLEU 0.2694 0.2835 0.2801 0.2870 0.2865 0.2992 (S"
P08-1115,E06-1006,0,0.0617151,"ss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simpli(Source Type) cs hs ss hs+ss hs+ss+cs hs+ss+cs.lexRo MT05 BLEU 0.2833 0.2905 0.2894 0.2938 0.2993 0.3072 MT06 BLEU 0.2694 0.2835 0.2801 0.2870 0.2865 0.2992 (Source Type) cs hs ss hs+ss hs+ss+cs (a) Phrase-based model MT05 BLEU 0.2904 0.3008 0.3071 0.3132 0.3176 MT06 BLEU 0.2821 0.2907 0.2964 0.3006 0.3043 (b) Hierarchical model Table 4: Chinese Word Segmentation Results (Source Type) surface morph morph+surface MT05 BLEU 0.4682 0.5087 0.5225 MT06 BLEU 0.3512 0.3841 0.4008 (Source Type) surface morph morph+surface (a) Phrase-ba"
P08-1115,2005.iwslt-1.2,0,0.0659287,"Missing"
P08-1115,zhang-etal-2004-interpreting,0,\N,Missing
P08-1115,I05-3027,0,\N,Missing
P11-2102,W07-0101,0,0.366339,"astic and non-sarcastic utterances in Twitter and in Amazon product reviews. In this paper, we consider the somewhat harder problem 581 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 581–586, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics of distinguishing sarcastic tweets from nonsarcastic tweets that directly convey positive and negative attitudes (we do not consider neutral utterances at all). Our approach of looking at lexical features for identification of sarcasm was inspired by the work of Kreuz and Caucci (2007). In addition, we also look at pragmatic features, such as establishing common ground between speaker and hearer (Clark and Gerring, 1984), and emoticons. 3 express sarcasm, such as “lol thanks. I can always count on you for comfort :) #sarcasm”. Our final corpus consists of 900 tweets in each of the three categories, sarcastic, positive and negative. Examples of tweets in our corpus that are labeled with the #sarcasm hashtag include the following: 1) @UserName That must suck. 2) I can't express how much I love shopping on black Friday. 3) @UserName that's what I love about Miami. Attention to"
P11-2102,pak-paroubek-2010-twitter,0,0.135364,"udges on this task. Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well. 1 2 Introduction Automatic detection of sarcasm is still in its infancy. One reason for the lack of computational models has been the absence of accurately-labeled naturally occurring utterances that can be used to train machine learning systems. Microblogging platforms such as Twitter, which allow users to communicate feelings, opinions and ideas in short messages and to assign labels to their own messages, have been recently exploited in sentiment and opinion analysis (Pak and Paroubek, 2010; Davidov et al., 2010). In Twitter, messages can be anRelated Work Sarcasm and irony are well-studied phenomena in linguistics, psychology and cognitive science (Gibbs, 1986; Gibbs and Colston 2007; Kreuz and Glucksberg, 1989; Utsumi, 2002). But in the text mining literature, automatic detection of sarcasm is considered a difficult problem (Nigam & Hurst, 2006 and Pang & Lee, 2008 for an overview) and has been addressed in only a few studies. In the context of spoken dialogues, automatic detection of sarcasm has relied primarily on speech-related cues such as laughter and prosody (Tepperman e"
P11-2102,strapparava-valitutti-2004-wordnet,0,\N,Missing
P16-2089,C14-1142,0,0.418635,"or Computational Learning Systems, Columbia University, NY, USA debanjan.ghosh@rutgers.edu, {ak3654,yh2635,smara@columbia.edu} Abstract work has begun to explore the impact of highlevel persuasion-related features, such as opinions and their targets, thesis clarity and argumentation schemes (Farra et al., 2015; Song et al., 2014; Ong et al., 2014; Persing and Ng, 2015). In this paper, we investigate whether argumentation features derived from a coarse-grained, general argumentative structure of essays are good predictors of holistic essay scores. We use the argumentative structure proposed by Stab and Gurevych (2014a): argument components (major claims, claims, premises) and argument relations (support, attack). Figure 1(i) shows an extract from an essay written in response to the above prompt, labeled with a claim and two premises. The advantage of having a simple annotation scheme is two-fold: it allows for more reliable human annotations and it enables better performance for argumentation mining systems designed to automatically identify the argumentative structure (Stab and Gurevych, 2014b). The paper has two main contributions. First, we introduce a set of argumentation features related to three mai"
P16-2089,D14-1006,0,0.462749,"or Computational Learning Systems, Columbia University, NY, USA debanjan.ghosh@rutgers.edu, {ak3654,yh2635,smara@columbia.edu} Abstract work has begun to explore the impact of highlevel persuasion-related features, such as opinions and their targets, thesis clarity and argumentation schemes (Farra et al., 2015; Song et al., 2014; Ong et al., 2014; Persing and Ng, 2015). In this paper, we investigate whether argumentation features derived from a coarse-grained, general argumentative structure of essays are good predictors of holistic essay scores. We use the argumentative structure proposed by Stab and Gurevych (2014a): argument components (major claims, claims, premises) and argument relations (support, attack). Figure 1(i) shows an extract from an essay written in response to the above prompt, labeled with a claim and two premises. The advantage of having a simple annotation scheme is two-fold: it allows for more reliable human annotations and it enables better performance for argumentation mining systems designed to automatically identify the argumentative structure (Stab and Gurevych, 2014b). The paper has two main contributions. First, we introduce a set of argumentation features related to three mai"
P16-2089,W15-0608,0,0.566576,"ting premises. Figure 2 shows examples of a T reeh&gt;1 structure, a Chain structure, and a T reeh=1 structure. The dark nodes represent claims (C), lighter nodes can be either claims or premises (C/P) and white nodes are premises (P). Figure 1 shows an extract from an essays and the corresponding Chain structure. To measure the effectiveness of the above features in predicting the holistic essay scores (high/medium/low) we use Logistic Regression (LR) learners and evaluate the learners using quadratic-weighted kappa (QWK) against the human scores, a methodology generally used for essay scoring (Farra et al., 2015). QWK corrects for chance agreement between the system prediction and the human prediction, and it takes into account the extent of the disagreement between labels. Table 2 reports the performance for the three feature groups as well as their combination. Our baseline feature (bl) is the number of sentences in the essay, since essay length has been shown to be generally highly correlated with essay scores (Chodorow and Burstein, 2004). We found that all three feature groups individually are strongly correlated with the human scores, much better than 551 4 Feature Type All features top100 Autom"
P16-2089,W14-2104,0,0.0639692,"ned Argumentation Features for Scoring Persuasive Essays Debanjan Ghosh§ , Aquila Khanam† , Yubo Han† and Smaranda Muresan‡ § School of Communication and Information, Rutgers University, NJ, USA † Department of Computer Science, Columbia University, NY, USA ‡ Center for Computational Learning Systems, Columbia University, NY, USA debanjan.ghosh@rutgers.edu, {ak3654,yh2635,smara@columbia.edu} Abstract work has begun to explore the impact of highlevel persuasion-related features, such as opinions and their targets, thesis clarity and argumentation schemes (Farra et al., 2015; Song et al., 2014; Ong et al., 2014; Persing and Ng, 2015). In this paper, we investigate whether argumentation features derived from a coarse-grained, general argumentative structure of essays are good predictors of holistic essay scores. We use the argumentative structure proposed by Stab and Gurevych (2014a): argument components (major claims, claims, premises) and argument relations (support, attack). Figure 1(i) shows an extract from an essay written in response to the above prompt, labeled with a claim and two premises. The advantage of having a simple annotation scheme is two-fold: it allows for more reliable human annot"
P16-2089,P13-1026,0,0.171157,"Missing"
P16-2089,P15-1053,0,0.638658,"Features for Scoring Persuasive Essays Debanjan Ghosh§ , Aquila Khanam† , Yubo Han† and Smaranda Muresan‡ § School of Communication and Information, Rutgers University, NJ, USA † Department of Computer Science, Columbia University, NY, USA ‡ Center for Computational Learning Systems, Columbia University, NY, USA debanjan.ghosh@rutgers.edu, {ak3654,yh2635,smara@columbia.edu} Abstract work has begun to explore the impact of highlevel persuasion-related features, such as opinions and their targets, thesis clarity and argumentation schemes (Farra et al., 2015; Song et al., 2014; Ong et al., 2014; Persing and Ng, 2015). In this paper, we investigate whether argumentation features derived from a coarse-grained, general argumentative structure of essays are good predictors of holistic essay scores. We use the argumentative structure proposed by Stab and Gurevych (2014a): argument components (major claims, claims, premises) and argument relations (support, attack). Figure 1(i) shows an extract from an essay written in response to the above prompt, labeled with a claim and two premises. The advantage of having a simple annotation scheme is two-fold: it allows for more reliable human annotations and it enables b"
P16-2089,prasad-etal-2008-penn,0,0.209974,"Missing"
P16-2089,W14-2110,0,0.05304,"gument relations; and 2) due to short paragraphs, the percentage of N S instances are less than in the S&G dataset, hence the Lexical features (i.e., word-pairs between Arg1 and Arg2 ) perform very well. 2 In future work, we plan to use the authors’ improved approach and larger dataset released after the acceptance of this paper (Stab and Gurevych, 2016). 552 Feature Type All features top100 S 84.3 89.0 NS 95.0 97.1 and Ng, 2015). Farra et al. (2015) investigate the impact of opinion and target features on TOEFL essays scores. Our work looks a step further by exploring argumentation features. Song et al. (2014) show that adding features related to argumentation schemes (from manual annotation) as part of an automatic scoring system increases the correlation with human scores. We show that argumentation features are good predictors of human scores for TOEFL essays, both when the coarsegrained argumentative structure is manually annotated and automatically predicted. Persing and Ng (2015) proposed a feature-rich approach for modeling argument strength in student essays, where the features are related to argument components. Our work explores features related to argument components, relations and typol"
P16-2089,miltsakaki-etal-2004-penn,0,\N,Missing
P16-2089,J17-3005,0,\N,Missing
P18-2125,Q15-1014,0,0.0291737,"n image what color either might refer to. If the reference color is a deep navy blue, then we imagine the target to be much closer to navy than, for example, a sky blue. We propose a new paradigm of learning to ground comparative adjectives within the realm of color descriptions: given a reference RGB color and a comparative term (e.g. ‘lighter’, ‘paler’), Introduction Multimodal approaches to object recognition have achieved a degree of success by grounding adjectives and nouns from descriptive text in image features (Farhadi et al., 2009; Lampert et al., 2009; Russakovsky and Fei-Fei, 2010; Lazaridou et al., 2015). One limitation of this approach, particularly for fine-grained object recognition, is when objects are differentiated not by having unique sets of attributes but by a difference in the strengths of their shared attributes (Wang et al., 2009; Duan et al., 2012; Maji et al., 2013; Vedaldi et al., 2014). In text, this difference is described using comparative adjectives. For example, the sexual dimorphism of the American black duck is described with the phrase “females tend to be slightly paler 1 https://www.allaboutbirds.org/guide/ American_Black_Duck/id 790 Proceedings of the 56th Annual Meet"
P18-2125,Q15-1008,0,0.407222,"pink data, applied to a teal sample Figure 1: Grounding ‘darker’ than males, with duller olive bills”.1 In a recent study of pragmatic referring expression interpretation in the context of color selection, Monroe et al. (2017) found that speakers almost always used comparative adjectives when the target color was very similar to a distractor, rather than using multiple positive form adjectives to create a highly specific description of the color independent of its surroundings. Though color has been studied in terms of its contextual dependence and vagueness in grounding (Egr´e et al., 2013; McMahan and Stone, 2015; Monroe et al., 2016, 2017), no approaches have focused explicitly on learning to ground comparative adjective; in this work we focus on comparative color descriptions. The presence of distractors in the Monroe et al. (2017) study is important - comparatives describe a change in a feature with respect to a reference point. While the description light blue can be understood to represent a particular subset of colors in RGB, for example, neither ‘lighter’ nor ‘lighter blue’ have explicit representations; it is only with a reference that we can image what color either might refer to. If the refe"
P18-2125,N13-1090,0,0.0437782,"on for why this representation is appropriate; our output w ~ g corresponds to the rate of change across the color bar, indicating the direction along with the degree of the compared property increases. All points along this line are representations of w in respect to rc . The network architecture consists of two fully connected layers, shown in Fig 2. The comparative is represented as a bi-gram to account for comparatives which necessitate using ‘more’ (e.g. “more electric”); single-word comparatives are preceded by the zero vector. We used the Google pre-trained word embeddings4 with d=300 (Mikolov et al., 2013a,b). As these vectors are two orders of magnitude larger than the reference RGB color, we input the reference directly into both layers of the network, helping to mitigate the loss of Data We utilize the labeled RGB color data originally collected by Munroe (2010), through an online survey asking participants to provide free-form labels to various RGB samples. This data was then cleaned by McMahan and Stone (2015)2 . The cleaned data contains 821 color labels, averaging 600 RGB datapoints per label. These labels do not contain comparative adjectives, but many start with adjectives in the posi"
P18-2125,D16-1243,0,0.193244,"teal sample Figure 1: Grounding ‘darker’ than males, with duller olive bills”.1 In a recent study of pragmatic referring expression interpretation in the context of color selection, Monroe et al. (2017) found that speakers almost always used comparative adjectives when the target color was very similar to a distractor, rather than using multiple positive form adjectives to create a highly specific description of the color independent of its surroundings. Though color has been studied in terms of its contextual dependence and vagueness in grounding (Egr´e et al., 2013; McMahan and Stone, 2015; Monroe et al., 2016, 2017), no approaches have focused explicitly on learning to ground comparative adjective; in this work we focus on comparative color descriptions. The presence of distractors in the Monroe et al. (2017) study is important - comparatives describe a change in a feature with respect to a reference point. While the description light blue can be understood to represent a particular subset of colors in RGB, for example, neither ‘lighter’ nor ‘lighter blue’ have explicit representations; it is only with a reference that we can image what color either might refer to. If the reference color is a deep"
S19-2194,S17-2083,0,0.269474,"factors, are now publicly available.2 2 Github repository: https://github.com/ joelau94/rumour2019-experiments 1110 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1110–1114 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics 2 Related Work 3.1 Rumour Detection. Recently there has been a growing interest on developing methods for the task of rumour detection (Zubiaga et al., 2018), including a shared task in 2017 (Derczynski et al., 2017), which established a strong baseline for stance classification — task A(Kochkina et al., 2017), while (Enayet and El-Beltagy, 2017) established the same for veracity — task B. Dungs et al. (2018) discuss how stance information can facilitate veracity classification, while (Zubiaga et al., 2017) explore the use of contextual information for rumour detection. These results show that stance information and context information are important for rumour verification. Multi-task Learning. Text classification tasks invariably suffer from the weak supervision signal due to loss of information in projecting text to task labels. There has been a growing number of works that explore multi-task lea"
S19-2194,C18-1288,0,0.035992,"ore the use of contextual information for rumour detection. These results show that stance information and context information are important for rumour verification. Multi-task Learning. Text classification tasks invariably suffer from the weak supervision signal due to loss of information in projecting text to task labels. There has been a growing number of works that explore multi-task learning for text classification (Zhang et al., 2017; Xiao et al., 2018). For the task of rumour detection specifically, there were attempts in jointly train for stance classification and rumour verification (Kochkina et al., 2018). Our muti-task approach uses a different, more advanced sentence embedding approach and uses the same LSTM for both tasks but with hidden states from different levels, which can be considered as different level representations of sentences. Empirically we found that higher levels of representation performs better for stance classification, while lower levels are better for veracity classification. Transfer Learning with pre-trained Language Models. To alleviate the problem of data scarcity, researchers have proposed various approaches for pre-training language models on large-scale monolingua"
S19-2194,D14-1162,0,0.0954117,"enables the most important information to flow in when trying to decide the stance of a post. 3. Rumour verification is incorporated as a task being jointly learnt together with stance classification, yet exploiting information at a different level from stance classification. In practice, hidden states at different levels of LSTM is being used for different tasks. Figure 1 shows our overall model architecture, which we describe in more detail below. Word Embeddings. The word embedding space is adjustable in our model. We initialize the word embedding matrix with pre-trained GloVe embeddings (Pennington et al., 2014). While we fix most word embedding vectors, we also keep some of the most frequent word embeddings trainable, allowing the word embedding space to adjust itself. Sentence Embeddings. We consider each post as a sentence and we encode it with a bidirectional LSTM. We then take the last hidden state of the forward LSTM and first hidden state of the backward LSTM and concatenate them. The resulting vector can be considered as a dynamically generated sentence embedding. Stacked Branch Encoder. To capture the interaction between posts in a branch of conversation, we use a stacked Bi-LSTM to encode t"
S19-2194,N18-1202,0,0.0531106,"e LSTM for both tasks but with hidden states from different levels, which can be considered as different level representations of sentences. Empirically we found that higher levels of representation performs better for stance classification, while lower levels are better for veracity classification. Transfer Learning with pre-trained Language Models. To alleviate the problem of data scarcity, researchers have proposed various approaches for pre-training language models on large-scale monolingual corpora, such as ELMo, ULMFiT, BERT, GPT, and have shown their effectiveness on several NLP tasks (Peters et al., 2018; Ruder and Howard, 2018; Devlin et al., 2018; Radford, 2018). In our work we use ULMFiT (Ruder and Howard, 2018) for stance classification. 3 System Description We propose two system configurations: 1. System1: A joint-learning for task A and task B without using language model fine-tuning. 2. System2: Language model fine-tuning for task A. System1: Joint Training for Stance Classification and Rumour Verification We formulate the joint learning of Task A and B as follows: Given a branch of conversation X containing n posts X = (x1 , x2 , · · · , xn ), where each post xk is a sequence of mk wo"
S19-2194,P18-1031,0,0.0944756,"support, deny, querying, comment) of responses to a rumourous statement are predicted, and task B (verification), in which the statement’s veracity is to be predicted. Size of training data provided for Task A is 5,217 and for Task B is 327. In this paper, we proposed several methods to alleviate data sparsity and unleash the power of sophisticated neural models: 1. Jointly learning to perform rumour verification and stance detection. Training a neural * Equal contribution. 3. Using language model fine-tuning for stance classification. We use the Universal Language Model Fine-tuning (ULMFiT) (Ruder and Howard, 2018) to improve our stance classifier. We begin with a generic language model trained on the Wikitext 103 dataset (Merity et al., 2016). This dataset consists of a large collection of pre-processed English Wikipedia articles. This enables the language model to properly model the general properties of language. Next, we fine-tune this language model on task specific data: RumourEval2019 dataset. Finally, a classification layer is added and the model is initialized with parameters from the fined-tuned language model. Our system, which relies on these three key factors, are now publicly available.2 2"
S19-2194,N18-2114,0,0.0241514,"he same for veracity — task B. Dungs et al. (2018) discuss how stance information can facilitate veracity classification, while (Zubiaga et al., 2017) explore the use of contextual information for rumour detection. These results show that stance information and context information are important for rumour verification. Multi-task Learning. Text classification tasks invariably suffer from the weak supervision signal due to loss of information in projecting text to task labels. There has been a growing number of works that explore multi-task learning for text classification (Zhang et al., 2017; Xiao et al., 2018). For the task of rumour detection specifically, there were attempts in jointly train for stance classification and rumour verification (Kochkina et al., 2018). Our muti-task approach uses a different, more advanced sentence embedding approach and uses the same LSTM for both tasks but with hidden states from different levels, which can be considered as different level representations of sentences. Empirically we found that higher levels of representation performs better for stance classification, while lower levels are better for veracity classification. Transfer Learning with pre-trained Lang"
S19-2194,D17-1070,0,0.0230847,"ined on Wikitext 103 dataset, we fine-tuned the LM on RumourEval2019 dataset. Pre-processing was inspired by BranchLSTM system (Kochkina et al., 2017). Tweets along a particular branch were concatenated starting from the source tweet till the target node and considered as one training instance. The SDQC label of the last node concatenated was considered to be the label of the training instance. Here, replies are being referred to as nodes. For instance, source + reply 1 + reply 2, label of reply 2 was one training instance. Finally, we used a BiLSTM max pooling network which was presented in (Conneau et al., 2017) and is shown in Figure 3. This model was initialized with parameters from the fined tuned Figure 2: Max pooling in System2 4 Results and Analysis For System1, we achieved an F1-score of 22.44 on task B and an F1-score of 34.04 on task A. For System2, we achieved F1-score of 36.25 on task A (System2 is only applied to task A). Performance of System1 on task A is slightly lower than the performance of System2, because we only treat task A as an auxiliary task for task B and did not apply ULMFiT to task A. Our final submission consisted of using System 2 for task A and System1 for task B. Our fi"
S19-2194,C18-1284,0,0.111582,"iments 1110 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1110–1114 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics 2 Related Work 3.1 Rumour Detection. Recently there has been a growing interest on developing methods for the task of rumour detection (Zubiaga et al., 2018), including a shared task in 2017 (Derczynski et al., 2017), which established a strong baseline for stance classification — task A(Kochkina et al., 2017), while (Enayet and El-Beltagy, 2017) established the same for veracity — task B. Dungs et al. (2018) discuss how stance information can facilitate veracity classification, while (Zubiaga et al., 2017) explore the use of contextual information for rumour detection. These results show that stance information and context information are important for rumour verification. Multi-task Learning. Text classification tasks invariably suffer from the weak supervision signal due to loss of information in projecting text to task labels. There has been a growing number of works that explore multi-task learning for text classification (Zhang et al., 2017; Xiao et al., 2018). For the task of rumour detecti"
S19-2194,C16-1230,0,0.0263695,"on. To predict the stance of a post, we want to selectively pay attention to some other posts that are relevant to this post. We use a QKV-style attention (Query, Key, Value) (Vaswani et al., 2017) to summarize the post context into a single vector (where in practice one attention head is usually enough). In addition, we use representations at different levels for different tasks. Introduction The ubiquity of social media is allowing unverified news and rumours to spread easily. Efforts have been made to explore automated methods for rumour detection and verification (Derczynski et al., 2017; Zubiaga et al., 2016, 2018), and has shown promising potential to tackle this issue at scale. RumourEval 2019 Shared Task 7 tackles the problem of predicting the veracity of rumours and stance of replies. It consists of two subtasks: task A (SDQC), in which stance (support, deny, querying, comment) of responses to a rumourous statement are predicted, and task B (verification), in which the statement’s veracity is to be predicted. Size of training data provided for Task A is 5,217 and for Task B is 327. In this paper, we proposed several methods to alleviate data sparsity and unleash the power of sophisticated neu"
S19-2200,W03-1017,0,0.310646,"Subtask A is a question classification task, where the questions types are: • Factual: The question is asking for factual information, which can be answered by checking various information sources, and it is not ambiguous (e.g., “What is Ooredoo customer service number?”). • Opinion: The question asks for an opinion or an advice, not for a fact. (e.g., “Can anyone recommend a good Vet in Doha?””) • Non-Factual: When the answer does not provide factual information to the question; it can be an opinion or an advice that cannot be verified (e.g., “It’s better to buy a new one.”). 2 Related Work Yu and Hatzivassiloglou (2003) separated opinions from fact, at both the document and sentence level. (Mihaylova et al., 2018) were the first to propose a novel multi-faceted model for fact checking of answers on community question answering forums. Their proposed model captures information 1144 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1144–1148 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics OPINION 586 FACTUAL 311 SOCIALIZING 254 TRUE 166 Table 1: Size of Subtask A dataset (question types). Data One of key challenges for both"
S19-2200,P18-1031,0,0.293671,"ated by altering sentences extracted from Wikipedia, and the task is to classify claims as SUPPORTED, REFUTED or NOTENOUGHINFO. However, the verification of the claims is limited to a particular database (namely Wikipedia) unlike Subtask B. Also, the claims are inherently less noisy as compared to answers in Community Question Answering forums. Pre-trained language models have been recently used to achieve state-of-the-art results on a wide range of NLP tasks (e.g., sequence labeling and sentence classification). Some of the recent works that have employed pre-trained language models include (Howard and Ruder, 2018), (Peters et al., 2018), (Yang et al., 2018), and (Radford et al., 2018). In this paper, we show the effectiveness of the Universal Language Model Fine-tunig (ULMFiT) method (Howard and Ruder, 2018) for both question classification and answer fact checking. 3 FALSE 135 Labeled Data Subtask A has a total of 1,118 questions divided into three types. Table 1 show the class distribution. Subtask B has a total of 495 answers divided QUESTIONS 189,941 ANSWERS 1,894,456 Table 3: External unannoted questions and answers. into three types. Table 2 shows the class distribution. 3.2 Unlabeled Data The ta"
S19-2200,nakov-etal-2017-trust,0,0.110541,"tics OPINION 586 FACTUAL 311 SOCIALIZING 254 TRUE 166 Table 1: Size of Subtask A dataset (question types). Data One of key challenges for both Subtask A and B is the limited amount of annotated data. This poses a challenge to apply state-of-the-art neural discrimination models without using additional data. 3.1 NON-FACTUAL 194 Table 2: Size of Subtask B dataset (answer types). from the answer content (what is said and how), from the author profile (who says it), from the rest of the community forum (where it is said), and from external authoritative sources of information (external support). (Nakov et al., 2017) proposed models for credibility assessment in community question answering forums. However, credibility is different from veracity as it is a subjective perception about whether a statement is credible, rather than verifying whether it is true/false as a matter of fact. Recently there has been a lot of attention on building models for fact checking. (Thorne et al., 2018) introduce a new publicly available dataset for fact extraction and verification (FEVER Shared Task). The dataset consists of 185,445 claims generated by altering sentences extracted from Wikipedia, and the task is to classify"
S19-2200,N18-1202,0,0.0244044,"extracted from Wikipedia, and the task is to classify claims as SUPPORTED, REFUTED or NOTENOUGHINFO. However, the verification of the claims is limited to a particular database (namely Wikipedia) unlike Subtask B. Also, the claims are inherently less noisy as compared to answers in Community Question Answering forums. Pre-trained language models have been recently used to achieve state-of-the-art results on a wide range of NLP tasks (e.g., sequence labeling and sentence classification). Some of the recent works that have employed pre-trained language models include (Howard and Ruder, 2018), (Peters et al., 2018), (Yang et al., 2018), and (Radford et al., 2018). In this paper, we show the effectiveness of the Universal Language Model Fine-tunig (ULMFiT) method (Howard and Ruder, 2018) for both question classification and answer fact checking. 3 FALSE 135 Labeled Data Subtask A has a total of 1,118 questions divided into three types. Table 1 show the class distribution. Subtask B has a total of 495 answers divided QUESTIONS 189,941 ANSWERS 1,894,456 Table 3: External unannoted questions and answers. into three types. Table 2 shows the class distribution. 3.2 Unlabeled Data The task allows the use of ex"
S19-2200,N18-1074,0,0.0607499,"Missing"
W01-0719,W95-0107,0,0.0212699,"ses, verb phrases, and prepositional phrases, we assume that noun phrases (NPs) carry the most contentful information about the document, a well-supported hypothesis (Smeaton, 1999; Wacholder, 1998). As considered by Wacholder (1998), the simple NPs are the maximal NPs that contain premodifiers but not post-nominal constituents such as prepositions or clauses. We chose simple NPs for content representation because they are semantically and syntactically coherent and they are less ambiguous than complex NPs. For extracting simple noun phrases we first used Ramshaw and Marcus’s base NP chunker (Ramshaw and Marcus, 1995). The base NP is either a simple NP or a coordination of simple NPs. We used heuristics based on POS tags to automatically split the coordinate NPs into simple ones, properly assigning the premodifiers. Table 1 presents some coordinate NPs (CNP) encountered in our data collection and the results of our algorithm which split them into simple NPs (SNP1 and SNP2). 2.2 Features used for Classification The choice of features used to represent the candidate phrases has a strong impact on the accuracy of the classifiers (e.g. the number of examples needed to obtain a given accuracy on the test data,"
W01-0719,W98-0610,0,0.292856,"es/NNS scientific/JJ articles/NNS technical/JJ articles/NNS scientific/JJ thesauri/NNS and databases/NNS scientific/JJ thesauri/NNS scientific/JJ databases/NNS physics/NN and/CC biology/NN skilled/JJ researchers/NNS physics/NN skilled/JJ researchers/NNS biology/NN skilled/JJ researchers/NNS Table 1: Resolving Coordination of NPs 2.1 Candidate Phrases Of the major syntactic constituents of a sentence, e.g. noun phrases, verb phrases, and prepositional phrases, we assume that noun phrases (NPs) carry the most contentful information about the document, a well-supported hypothesis (Smeaton, 1999; Wacholder, 1998). As considered by Wacholder (1998), the simple NPs are the maximal NPs that contain premodifiers but not post-nominal constituents such as prepositions or clauses. We chose simple NPs for content representation because they are semantically and syntactically coherent and they are less ambiguous than complex NPs. For extracting simple noun phrases we first used Ramshaw and Marcus’s base NP chunker (Ramshaw and Marcus, 1995). The base NP is either a simple NP or a coordination of simple NPs. We used heuristics based on POS tags to automatically split the coordinate NPs into simple ones, properl"
W01-1011,H92-1022,0,0.0194125,"he message content. 2.2 Candidate Simple Noun Phrase Extraction and Filtering Unit This module performs shallow text processing for extraction and filtering of simple NP candidates, consisting of a pipeline of three modules: text tokenization, NP extraction, and NP filtering. Since the tool was created to preprocess email for speech output, some of the text tokenization suitable for speech is not accurate for text processing and some modifications needed to be implemented (e.g. email preprocessor splits acronyms like DLI2 into DLI 2). The noun phrase extraction module uses Brill&apos;s POS tagger [Brill (1992)]and a base NP chunker [Ramshaw and Marcus (1995)]. After analyzing some of these errors, we augmented the tagger lexicon from our training data and we added lexical and contextual rules to deal mainly with incorrect tagging of gerund endings. In order to improve the accuracy of classifiers we perform linguistic filtering, as discussed in detail in Section 3.1.2. 2.3 Machine Learning Unit The first component of the ML unit is the feature selection module to compute NP vectors. In the training phase, a model for identifying salient simple NPs is created. The training data consist of a list of f"
W01-1011,klavans-etal-2000-evaluation,1,0.794805,"tten et al (1999), Turney (1999)]. These approaches select a set of candidate phrases (sequence of one, two or three consecutive stemmed, non-stop words) and then apply machine learning techniques to classify them as key phrases or not. But dealing only with n-grams does not always provide good output in terms of a summary (see discussion in Section 5.4). Wacholder (1998) proposes a linguisticallymotivated method for the representation of the document aboutness: ‘head clustering’. A list of simple noun phrases is first extracted, clustered by head and then ranked by the frequency of the head. Klavans et al (2000) report on the evaluation of ‘usefulness’ of head clustering in the context of browsing applications, in terms of quality and coverage. Other researchers have used noun-phrases quite successfully for information retrieval task [Strzalkowski et al (1999), Sparck-Jones (1999)]. Strzalkowski et al (1999) uses head + HPDLO PHVVDJH modifier pairs as part of a larger system which constitutes the “stream model” that is used for information retrieval. They treat the head-modifier relationship as an ”ordered relation between otherwise equal elements”, emphasizing that for some tasks, the syntactic head"
W01-1011,P98-1112,1,0.841177,"zed speech. The focus of this paper is on extracting content with GISTIT, although presentation is a topic for future research. 3 Combining Linguistic Knowledge Machine Learning for Email Gisting and We combine symbolic machine learning and linguistic processing in order to extract the salient phrases of a document. Out of the large syntactic constituents of a sentence, e.g. noun phrases, verb phrases, and prepositional phrases, we assume that noun phrases (NPs) carry the most contentful information about the document, even if sometimes the verbs are important too, as reported in the work by [Klavans and Kan (1998)]. The problem is that no matter the size of a document, the number of informative noun phrases is very small comparing with the number of all noun phrases, making selection a necessity. Indeed, in the context of gisting, generating and presenting the list of all noun phrases, even with adequate linguistic filtering, may be overwhelming. Thus, we define the extraction of important noun phrases as a classification task, applying machine learning techniques to determine which features associated with the candidate NPs classify them as salient vs. non-salient. We represent the document -- in this"
W01-1011,W01-0719,1,0.824032,"Missing"
W01-1011,W95-0107,0,0.0587534,"mple Noun Phrase Extraction and Filtering Unit This module performs shallow text processing for extraction and filtering of simple NP candidates, consisting of a pipeline of three modules: text tokenization, NP extraction, and NP filtering. Since the tool was created to preprocess email for speech output, some of the text tokenization suitable for speech is not accurate for text processing and some modifications needed to be implemented (e.g. email preprocessor splits acronyms like DLI2 into DLI 2). The noun phrase extraction module uses Brill&apos;s POS tagger [Brill (1992)]and a base NP chunker [Ramshaw and Marcus (1995)]. After analyzing some of these errors, we augmented the tagger lexicon from our training data and we added lexical and contextual rules to deal mainly with incorrect tagging of gerund endings. In order to improve the accuracy of classifiers we perform linguistic filtering, as discussed in detail in Section 3.1.2. 2.3 Machine Learning Unit The first component of the ML unit is the feature selection module to compute NP vectors. In the training phase, a model for identifying salient simple NPs is created. The training data consist of a list of feature vectors already classified as salient/nons"
W01-1011,W98-0610,0,0.279332,"uraev and Kennedy (1999)], the meaning of ‘summary’ should be adjusted depending on the information management task for which it is used. Key phrases, for example, can be seen as semantic metadata that summarize and characterize documents [Witten et al (1999), Turney (1999)]. These approaches select a set of candidate phrases (sequence of one, two or three consecutive stemmed, non-stop words) and then apply machine learning techniques to classify them as key phrases or not. But dealing only with n-grams does not always provide good output in terms of a summary (see discussion in Section 5.4). Wacholder (1998) proposes a linguisticallymotivated method for the representation of the document aboutness: ‘head clustering’. A list of simple noun phrases is first extracted, clustered by head and then ranked by the frequency of the head. Klavans et al (2000) report on the evaluation of ‘usefulness’ of head clustering in the context of browsing applications, in terms of quality and coverage. Other researchers have used noun-phrases quite successfully for information retrieval task [Strzalkowski et al (1999), Sparck-Jones (1999)]. Strzalkowski et al (1999) uses head + HPDLO PHVVDJH modifier pairs as part of"
W08-2002,N06-2015,0,0.0389707,"Missing"
W08-2002,W04-2510,0,0.0602216,"Missing"
W08-2002,P07-1105,1,0.830299,"tic representations. These semantic representations vary from λ-expressions (Bos et al., 2004; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-based semantic representations First, we review our grammar formalism introduced in (Muresan, 2006; Muresan and Rambow, 2007), called Lexicalized Well-Founded Grammars. Second, we present a relational learning algorithm for inducing these grammars from a representative sample of strings annotated with their semantics, along with minimal assumptions about c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 9 Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 9–16 Manchester, August 2008 I. Semantic Molecules 0 2 1 0 2 1 3 3 a. (m"
W08-2002,W04-0906,0,0.0732908,"Missing"
W08-2002,C04-1180,0,0.0281248,"nderstanding” a text is the ability to correctly answer, at the conceptual level, all the questions asked w.r.t to that text, and thus Meaning = Text + all Questions/Answers w.r.t that Text. Under this assumption, obtaining the meaning of a text is reduced to a question answering process, which in our framework is a DAG matching problem. Recent work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; He and Young, 2006) has developed learning algorithms for the problem of mapping sentences to their underlying semantic representations. These semantic representations vary from λ-expressions (Bos et al., 2004; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-based semantic representations First, we review our grammar formalism introduced in (Muresan, 2006; Muresan and Rambow, 2007), called Lexicalized Well-Founded Grammars. Second, we present a rel"
W08-2002,P07-1121,0,0.0570749,") and extra-ontological meaning, such as voice, tense, aspect, modality. 1 Introduction • Our representation and grammar learning framework allow a direct mapping of text to its meaning, encoded as a direct acyclic graph (DAG). We consider that “understanding” a text is the ability to correctly answer, at the conceptual level, all the questions asked w.r.t to that text, and thus Meaning = Text + all Questions/Answers w.r.t that Text. Under this assumption, obtaining the meaning of a text is reduced to a question answering process, which in our framework is a DAG matching problem. Recent work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; He and Young, 2006) has developed learning algorithms for the problem of mapping sentences to their underlying semantic representations. These semantic representations vary from λ-expressions (Bos et al., 2004; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based"
W08-2002,W05-0602,0,0.11937,"l Questions/Answers w.r.t that Text. Under this assumption, obtaining the meaning of a text is reduced to a question answering process, which in our framework is a DAG matching problem. Recent work (Wong and Mooney, 2007; Zettlemoyer and Collins, 2005; He and Young, 2006) has developed learning algorithms for the problem of mapping sentences to their underlying semantic representations. These semantic representations vary from λ-expressions (Bos et al., 2004; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to DB query languages and command-like languages (RoboCup Coach Language, CLang) (Ge and Mooney, 2005). In this paper we focus on an ontology-based semantic representation which allows us to encode the meaning of a text as a direct acyclic graph. Recently, there is a growing interest on ontology-based NLP, starting from efforts in defining ontology-based semantic representations First, we review our grammar formalism introduced in (Muresan, 2006; Muresan and Rambow, 2007), called Lexicalized Well-Founded Grammars. Second, we present a relational learning algorithm for inducing these grammars from a representative sample of strings annotated with their semantics, along with minimal assumptions"
W08-2002,J03-4003,0,\N,Missing
W12-2501,P07-1107,0,0.0214614,"er state-of-the-art coreference systems such as CherryPicker (Rahman and Ng, 2009). The NLP tools and techniques discussed above can be applied to cross-document coreference resolution as well (see Bagga and Baldwin, 1998, for discussion of a meta document), although training the systems for narratives like ours would involve much more manual annotation and supervision, particularly because different authors usually assign different names to a given character. In order to limit the amount of manual annotation, unsupervised methods for coreference resolution (Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2007) could be used. This, however, would require a larger number of picture books and human-produced narratives. Coreference is far from a simple phenomenon, both for theory and application. Nevertheless, ultimately it would be desirable to improve the automatic coreference resolution systems in ways that reflect corpus-linguistic and psycholinguistic findings – e.g., referential distance effects (Givón, 1992), and the privileged status in memory of discourse entities in the immediately preceding clause (Clark and Sengul, 1979). The goal would be to represent as many of the interacting factors in"
W12-2501,D09-1120,0,0.0139002,"n coreference resolution module, and taskspecific post-processing. In this system, global information about the text is shared across mentions in the same cluster in the form of attributes such as gender and number. This system received the highest scores at a recent CoNLL shared task (Pradhan et al., 2011), which the authors attributed to the initial high-recall component (in mention detection) followed by high-precision classifiers in the coreference resolution sieves. ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). We analyzed in depth the performance of these systems on one of our narratives for Frog Goes to Dinner (Mayer, 1974). We expected automatic coreference resolution systems to show poorer performance when applied to our written narratives than that reported in the literature, because most of these systems have been trained on newswire, blog, or conversation corpora, which – though quite a heterogeneous set in themselves – are not similar to our written narrative data. Some of the most noteworthy particularities of our written narrative collection include (a) fictional content, in which animals"
W12-2501,W11-1902,0,0.261687,"of annotated coreference corpora has led to developments in machine learning approaches to automatic coreference resolution (see Ng, 2010). The task of automatic NP coreference resolution is to determine “which NPs in a text […] refer to the same real-world entity” (Ng, 2010, p. 1396). Successful coreference resolution often requires realworld knowledge of public figures, entity relationships, and aliases, beyond linguistic parameters such as number and gender features. In this paper, we have chosen two coreference resolution systems: Stanford’s Multi-Pass Sieve Coreference Resolution System (Lee et al., 2011) (henceforth, Stanford dcoref) and ARKref (O’Connor and Heilman, 2011). Stanford dcoref consists of an initial mention-detection module, the main coreference resolution module, and taskspecific post-processing. In this system, global information about the text is shared across mentions in the same cluster in the form of attributes such as gender and number. This system received the highest scores at a recent CoNLL shared task (Pradhan et al., 2011), which the authors attributed to the initial high-recall component (in mention detection) followed by high-precision classifiers in the coreference"
W12-2501,P10-1142,0,0.0202271,"icture book regardless of writer or narrative length: 8, 5, and 7 for One Frog Too Many (Mayer and Mayer, 1975); 13, 12, and 11 for Frog, Where Are You? (Mayer, 1969); and 23, 21, and 26 for Frog Goes to Dinner (Mayer, 1974). Table 2 also shows that the longest chain contains 60 mentions, and the average chain has about 8 mentions. 4 Performance of Coreference Resolution Systems on Narratives of Picture Books In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic coreference resolution (see Ng, 2010). The task of automatic NP coreference resolution is to determine “which NPs in a text […] refer to the same real-world entity” (Ng, 2010, p. 1396). Successful coreference resolution often requires realworld knowledge of public figures, entity relationships, and aliases, beyond linguistic parameters such as number and gender features. In this paper, we have chosen two coreference resolution systems: Stanford’s Multi-Pass Sieve Coreference Resolution System (Lee et al., 2011) (henceforth, Stanford dcoref) and ARKref (O’Connor and Heilman, 2011). Stanford dcoref consists of an initial mention-de"
W12-2501,W11-1901,0,0.0128761,"umber and gender features. In this paper, we have chosen two coreference resolution systems: Stanford’s Multi-Pass Sieve Coreference Resolution System (Lee et al., 2011) (henceforth, Stanford dcoref) and ARKref (O’Connor and Heilman, 2011). Stanford dcoref consists of an initial mention-detection module, the main coreference resolution module, and taskspecific post-processing. In this system, global information about the text is shared across mentions in the same cluster in the form of attributes such as gender and number. This system received the highest scores at a recent CoNLL shared task (Pradhan et al., 2011), which the authors attributed to the initial high-recall component (in mention detection) followed by high-precision classifiers in the coreference resolution sieves. ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). We analyzed in depth the performance of these systems on one of our narratives for Frog Goes to Dinner (Mayer, 1974). We expected automatic coreference resolution systems to show poorer performance when applied to our written narratives than that reported in the literature, becaus"
W12-2501,D09-1101,0,0.0125934,"her insight into the impact of time change in content on referential choice in naturally occurring discourse can thus lead to a predictive model of referring expressions as well. In the future, we plan to use ‘semantic_class’ attributes and features such as ANIMACY in the According to Lee et al. (2011), Stanford dcoref correctly recognizes coreference in appositive constructions with an indefinite NP after the first mention. HTC schema as our task-specific filters for selecting just story characters. Moreover, we plan to explore other state-of-the-art coreference systems such as CherryPicker (Rahman and Ng, 2009). The NLP tools and techniques discussed above can be applied to cross-document coreference resolution as well (see Bagga and Baldwin, 1998, for discussion of a meta document), although training the systems for narratives like ours would involve much more manual annotation and supervision, particularly because different authors usually assign different names to a given character. In order to limit the amount of manual annotation, unsupervised methods for coreference resolution (Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2007) could be used. This, however, would require a larger num"
W12-2501,M95-1005,0,0.0911091,"ext input box for each new scene in a picture book. The first point, in particular, may limit the utility of named entity recognition (NER) and WordNet relations among nominals in the preprocessing steps prior to coreference resolution. As we discuss below, preprocessing errors in parsing and NER did in fact contribute to coreference precision errors. Our written narratives had a lot of singleton mentions for secondary characters and plural combinations of characters. We thus evaluated the performance based on the B3 measure proposed by Bagga and Baldwin (1998), rather than the linkbased MUC (Vilain et al., 1995). We computed the B3 with equal weighting for all mentions. Stanford dcoref achieved B3 scores of 0.78 Precision, 0.43 Recall and 0.55 F1, while ARKref scores were 0.67 for precision, 0.45 for recall, and 0.54 for F1. Stanford dcoref includes a post-processing module in which singletons are removed, which partially contributes to the low recall score for the system. 4.1 Qualitative analysis of coreference output In this section, we discuss the errors from both ARKref and Stanford dcoref in depth. The coreference outputs from both ARKref and Stanford dcoref demonstrate that preprocessing errors"
W12-2501,D08-1067,0,\N,Missing
W12-2501,D08-1068,0,\N,Missing
W12-2801,D11-1039,0,0.0175406,"l agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior. From this data, a mapping from instructions to tasks is learned, enabling the agent to carry out new instructions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been applied to various tasks such as following navigational instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Tellex et al., 2011), software control (Branavan et al., 2009; Branavan et"
W12-2801,P09-1010,0,0.151325,"County + Computer Science Department, Rutgers University ++ School of Communication and Information, Rutgers University Abstract This paper addresses the problem of training an artificial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior. From this data, a mapping from instructions to tasks is learned, enabling the agent to carry out new instructions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been ap"
W12-2801,P10-1129,0,0.0228388,"ce Department, Rutgers University ++ School of Communication and Information, Rutgers University Abstract This paper addresses the problem of training an artificial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior. From this data, a mapping from instructions to tasks is learned, enabling the agent to carry out new instructions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been applied to various tasks"
W12-2801,P11-1028,0,0.0562335,"University ++ School of Communication and Information, Rutgers University Abstract This paper addresses the problem of training an artificial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior. From this data, a mapping from instructions to tasks is learned, enabling the agent to carry out new instructions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been applied to various tasks such as following navig"
W12-2801,W10-2903,0,0.0137246,"Communication and Information, Rutgers University Abstract This paper addresses the problem of training an artificial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior. From this data, a mapping from instructions to tasks is learned, enabling the agent to carry out new instructions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been applied to various tasks such as following navigational instructions"
W12-2801,P06-1115,0,0.0374573,"nstructions to tasks is learned, enabling the agent to carry out new instructions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been applied to various tasks such as following navigational instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Tellex et al., 2011), software control (Branavan et al., 2009; Branavan et al., 2010), semantic parsing (Clarke et al., 2010; Liang et al., 2011) and learning to play games based on text (Branavan et al., 2011; Goldwasser and Roth, 2011). 1 In this paper,"
W12-2801,P11-1060,0,0.0184651,"raining an artificial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior. From this data, a mapping from instructions to tasks is learned, enabling the agent to carry out new instructions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been applied to various tasks such as following navigational instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Tellex et al., 2011), software control (Branav"
W12-2801,D08-1082,0,0.0226155,"ions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been applied to various tasks such as following navigational instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Tellex et al., 2011), software control (Branavan et al., 2009; Branavan et al., 2010), semantic parsing (Clarke et al., 2010; Liang et al., 2011) and learning to play games based on text (Branavan et al., 2011; Goldwasser and Roth, 2011). 1 In this paper, we present an approach to interpreting language instructions that descr"
W12-2801,P07-1105,1,0.821122,"maximizes the probability of these trajectories. In our system, these state features consist of one of the sets of propositional functions provided by the TA component. For a given task and a set of sets of state features, MLIRL evaluates the feature sets and returns to the TA component its assessment of the probabilities of the various sets. Semantic Parsing. To address the problem of mapping instructions to semantic parses, we use a constraint-based grammar formalism, Lexicalized Well-Founded Grammar (LWFG), which has been shown to balance expressiveness with practical learnability results (Muresan and Rambow, 2007; Muresan, 2011). In LWFG, each string is associated with a syntactic-semantic representation, and the grammar rules have two types of constraints: one for semantic composition (Φc ) and one for semantic interpretation (Φi ). The semantic interpretation constraints, Φi , provide access to a semantic model (domain knowledge) during parsing. In the absence of a semantic model, however, the LWFG learnability result still holds. This fact is important if our agent is assumed to start with no knowledge of the task and domain. LWFG uses an ontology-based semantic representation, which is a logical f"
W12-2801,P10-1083,0,0.0695124,"ersity Abstract This paper addresses the problem of training an artificial agent to follow verbal instructions representing high-level tasks using a set of instructions paired with demonstration traces of appropriate behavior. From this data, a mapping from instructions to tasks is learned, enabling the agent to carry out new instructions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been applied to various tasks such as following navigational instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011;"
W12-2801,P07-1121,0,0.031296,"learned, enabling the agent to carry out new instructions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been applied to various tasks such as following navigational instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Tellex et al., 2011), software control (Branavan et al., 2009; Branavan et al., 2010), semantic parsing (Clarke et al., 2010; Liang et al., 2011) and learning to play games based on text (Branavan et al., 2011; Goldwasser and Roth, 2011). 1 In this paper, we present an approach"
W12-2801,P09-1110,0,0.0241258,"agent to carry out new instructions in novel environments. 1 Introduction Learning to interpret language from a situated context has become a topic of much interest in recent years (Branavan et al., 2009; Branavan et al., 2010; Branavan et al., 2011; Clarke et al., 2010; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Goldwasser and Roth, 2011; Liang et al., 2011; Atrzi and Zettlemoyer, 2011; Tellex et al., 2011). Instead of using annotated training data consisting of sentences and their corresponding logical forms (Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Lu et al., 2008), most of these approaches leverage non-linguistic information from a situated context as their primary source of supervision. These approaches have been applied to various tasks such as following navigational instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Tellex et al., 2011), software control (Branavan et al., 2009; Branavan et al., 2010), semantic parsing (Clarke et al., 2010; Liang et al., 2011) and learning to play games based on text (Branavan et al., 2011; Goldwasser and Roth, 2011). 1 In this paper, we present an approach to interpreting language instr"
W12-2801,C10-2102,1,\N,Missing
W12-4620,J07-4004,0,0.0349869,"int-based grammar formalism for deep linguistic processing, Lexicalized Well-Founded Grammars (LWFGs) (Muresan, 2006; Muresan and Rambow, 2007; Muresan, 2011). Most formalisms used for deep linguistic processing, such as Tree Adjoining Grammars (Joshi and Schabes, 1997) and Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) are not known to be accompanied by a formal guarantee of polynomial learnability. While stochastic grammar learning for statistical parsing for some of these grammars has been achieved using large annotated treebanks (e.g., (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Shen, 2006)), LWFG is suited to learning in resource-poor settings. LWFG’s learning is a relational learning framework which characterizes the importance of substructures in the model not simply by frequency, as in most previous work, but rather linguistically, by defining a notion of representative examples that drives the acquisition process. LWFGs can be seen as a type of Definite Clause Grammars (Pereira and Warren, 1980) where: 1) the Context-Free Grammar backbone is extended by introducing a partial ordering relation among delexicalized nonterminals (wellfounded), 2) nonterminals are a"
W12-4620,P02-1043,0,0.0509015,"of a recently introduced constraint-based grammar formalism for deep linguistic processing, Lexicalized Well-Founded Grammars (LWFGs) (Muresan, 2006; Muresan and Rambow, 2007; Muresan, 2011). Most formalisms used for deep linguistic processing, such as Tree Adjoining Grammars (Joshi and Schabes, 1997) and Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) are not known to be accompanied by a formal guarantee of polynomial learnability. While stochastic grammar learning for statistical parsing for some of these grammars has been achieved using large annotated treebanks (e.g., (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Shen, 2006)), LWFG is suited to learning in resource-poor settings. LWFG’s learning is a relational learning framework which characterizes the importance of substructures in the model not simply by frequency, as in most previous work, but rather linguistically, by defining a notion of representative examples that drives the acquisition process. LWFGs can be seen as a type of Definite Clause Grammars (Pereira and Warren, 1980) where: 1) the Context-Free Grammar backbone is extended by introducing a partial ordering relation among delexicalized nonterminals (wellfounded"
W12-4620,P07-1105,1,0.8542,"examples. Given several assumptions, we define the search space as a complete grammar lattice. In order to prove a learnability theorem, we give a general algorithm through which the top and bottom elements of the complete grammar lattice can be built. 1 Introduction There has been significant interest in grammar induction on the part of both formal languages and natural language processing communities. In this paper, we discuss the learnability of a recently introduced constraint-based grammar formalism for deep linguistic processing, Lexicalized Well-Founded Grammars (LWFGs) (Muresan, 2006; Muresan and Rambow, 2007; Muresan, 2011). Most formalisms used for deep linguistic processing, such as Tree Adjoining Grammars (Joshi and Schabes, 1997) and Head-driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) are not known to be accompanied by a formal guarantee of polynomial learnability. While stochastic grammar learning for statistical parsing for some of these grammars has been achieved using large annotated treebanks (e.g., (Hockenmaier and Steedman, 2002; Clark and Curran, 2007; Shen, 2006)), LWFG is suited to learning in resource-poor settings. LWFG’s learning is a relational learning framework"
W12-4620,P84-1027,0,0.729872,"generated by a rule A → β of a grammar G is ∗G Lw (A → β) = {w|w ∈ Σ+ , (A → β) ⇒ w}, ∗G where (A → β) ⇒ w denotes the ground deriva∗G tion A ⇒ w obtained using the rule A → β in the last derivation step. For a WFG G, we call a set of substrings Ew ⊆ Lw (G) a sublanguage of G. Operational Semantics of WFGs. It has been shown that the operational semantics of a CFG corresponds to the language of the grammar (Wintner, 1999). Analogously, the operational semantics of a WFG, G, is the set of all strings generated by the grammar, Lw (G). Denotational Semantics of WFGs. As discussed in literature (Pereira and Shieber, 1984; Wintner, 1999), the denotational semantics of a grammar is defined through a fixpoint of a transformational operator associated with the grammar. Let I ⊆ Lw (G) be a subset of all strings generated by a grammar G. We define the immediate derivation operator TG : 2Lw (G) → 2Lw (G) , s.t.: TG (I) = ∗G {w ∈ Lw (G) |if (A → B1 . . . Bn ) ∈ PG ∧ Bi ⇒ ∗G wi ∧ wi ∈ I then A ⇒ w}. If we denote TG ↑ 0 = ∅ and TG ↑ (i + 1) = TG (TG ↑ i), then we have that for i = 1, TG ↑ 1 = TG (∅) = ∗G {w ∈ Lw (G)|A ∈ pre(NG ), A ⇒ w}.This corresponds to the strings derived from preterminals, i.e., w ∈ Σ. TG is analo"
W12-4620,P79-1022,0,0.405399,"mmar G. We define the immediate derivation operator TG : 2Lw (G) → 2Lw (G) , s.t.: TG (I) = ∗G {w ∈ Lw (G) |if (A → B1 . . . Bn ) ∈ PG ∧ Bi ⇒ ∗G wi ∧ wi ∈ I then A ⇒ w}. If we denote TG ↑ 0 = ∅ and TG ↑ (i + 1) = TG (TG ↑ i), then we have that for i = 1, TG ↑ 1 = TG (∅) = ∗G {w ∈ Lw (G)|A ∈ pre(NG ), A ⇒ w}.This corresponds to the strings derived from preterminals, i.e., w ∈ Σ. TG is analogous with the immediate consequence operator of definite logic programs (i.e., no negation) (van Emden and Kowalski, 1976; Denecker et al., 2001). TG is monotonous and hence the least fixpoint always exists (Tarski, 1955). This least fixpoint is unique, as for definite logic programs (van Emden and Kowalski, 1976). We have lf p(TG ) = TG ↑ ω, where ω is the minimum limit ordinal. Thus, the denotational semantics of a grammar G can be seen as the least fixpoint of the immediate derivation operator. An assumption for learning WFGs is that the rules corresponding to grammar preterminals, A → w ∈ PΣ , are given, i.e., TG (∅) is given. As in the case of definite logic programs, the denotational semantics is equivalent with the operational one, i.e., Lw (G) = lf p(TG ) . Based on TG we can define the ground derivati"
W12-4620,P99-1013,0,0.157261,"{w|w ∈ Σ+ , ∃A ∈ NG , A ⇒ w}. Extending the 172 notation, the set of strings generated by a nonterminal A of a grammar G is Lw (A) = {w|w ∈ ∗G Σ+ , A ∈ NG , A ⇒ w}, and the set of strings generated by a rule A → β of a grammar G is ∗G Lw (A → β) = {w|w ∈ Σ+ , (A → β) ⇒ w}, ∗G where (A → β) ⇒ w denotes the ground deriva∗G tion A ⇒ w obtained using the rule A → β in the last derivation step. For a WFG G, we call a set of substrings Ew ⊆ Lw (G) a sublanguage of G. Operational Semantics of WFGs. It has been shown that the operational semantics of a CFG corresponds to the language of the grammar (Wintner, 1999). Analogously, the operational semantics of a WFG, G, is the set of all strings generated by the grammar, Lw (G). Denotational Semantics of WFGs. As discussed in literature (Pereira and Shieber, 1984; Wintner, 1999), the denotational semantics of a grammar is defined through a fixpoint of a transformational operator associated with the grammar. Let I ⊆ Lw (G) be a subset of all strings generated by a grammar G. We define the immediate derivation operator TG : 2Lw (G) → 2Lw (G) , s.t.: TG (I) = ∗G {w ∈ Lw (G) |if (A → B1 . . . Bn ) ∈ PG ∧ Bi ⇒ ∗G wi ∧ wi ∈ I then A ⇒ w}. If we denote TG ↑ 0 = ∅"
W14-1807,N01-1021,0,0.440166,"ed by the boy, are known to be harder to process than semantically equivalent active sentences, such as the boy pushed the girl (Slobin, 1966; Forster and Olbrei, 1972; Davison and Lutz, 1985; Kharkwal and Stromswold, 2013). Thus, it is likely that the overall processing complexity of the sentence structures used in an essay could influence its perceived quality. One reason why sentence processing complexity has not yet been fully utilized is the lack of a suitable way of quantifying it. This paper proposes the use of a psycholinguistic model of sentence comprehension called surprisal theory (Hale, 2001; Levy, 2008) to quantify sentence processing complexity. The rest of the paper is organized as follows. Section 2 describes the surprisal theory, and discusses its applicability in modeling sentence processing complexity. Section 3 details our investigation on whether essays’ average surprisal values decrease following English as a Foreign Language training. Section 4 presents a study where we investigated whether surprisal can be effective as a predictor of essay quality. Lastly, SecModern automated essay scoring systems rely on identifying linguistically-relevant features to estimate essay"
W14-1807,W98-0303,0,0.174697,"Missing"
W14-1807,J93-2004,0,0.0451169,"our analysis shows that essays in the second term have an overall mean surprisal values less than than essays in the first term, these differences were not statistically significant. There are a number of factors that could have influenced these results. We made an assumption that only a single term of EFL training could significantly improve 3.2 Computing Surprisal We computed the surprisal value of each word in an essay by using a broad-coverage top-down parser developed by Roark et al. (2009). The parser was trained on sections 02-24 of the Wall Street Journal corpus of the Penn Treebank (Marcus et al., 1993). Essentially, the parser computes a word’s surprisal value as the negative logprobability of the word given the preceding words using prefix probabilities. Thus, the surprisal 1 It is important to note here that these means and standard deviations are computed on mean surprisal values per essays and not surprisal values at individual words. 56 Score Low Medium High Mean 6.22 6.10 6.09 Total SD 0.39 0.34 0.28 Syntactic Mean SD 2.46 0.22 2.35 0.17 2.27 0.14 Mean 3.76 3.75 3.82 Lexical SD 0.29 0.26 0.24 Table 3: Means and standard deviations of total surprisal, syntactic surprisal, and lexical s"
W14-1807,D09-1034,0,0.191727,"R (Burstein et al., Ajay et al., 1973), ETS’s e-rater 54 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 54–60, c Baltimore, Maryland USA, June 26, 2014. 2014 Association for Computational Linguistics The judge who angered the criminal slammed the gavel Mean 5.64 6.94 6.93 11.60 2.32 9.19 16.92 1.94 4.68 7.35 The judge who the criminal angered slammed the gavel Mean 5.64 6.94 6.93 4.20 9.21 13.73 16.65 2.21 4.69 7.80 Table 1: Surprisal values of two example relative-clause sentences. The values were computed using a top-down parser by Roark et al. (2009) trained on the Wall Street Journal corpus. each word position of the two example sentences are shown in Table 1. tion 5 concludes the paper. 2 Surprisal Theory As we can see from Table 1, the mean surprisal value is greater for the object-extracted relative clause sentence. Hence, the surprisal theory correctly predicts greater processing cost for that sentence. Furthermore, it allows for a finer-grained analysis of where the processing cost might occur, specifically at the onset of the relative clause (the) Complexity(wi ) ∝ − log P (wi |w1...i−1 , CONTEXT) and the end (angered). Other diffe"
W14-2106,W11-0702,0,0.802983,"determining which portions of texts are argumentative and what is the nature of argumentation. We propose a two-tiered approach to achieve this goal and report on several initial studies to assess its potential. 1 Introduction Figure 1: Argumentative annotation of an Online Thread An increasing portion of information and opinion exchange occurs in online interactions such as discussion forums, blogs, and webpage comments. This type of user-generated conversational data provides a wealth of naturally occurring arguments. Argument mining of online interactions, however, is still in its infancy (Abbott et al., 2011; Biran and Rambow, 2011; Yin et al., 2012; Andreas et al., 2012; Misra and Walker, 2013). One reason is the lack of annotated corpora in this genre. To make progress, we need to develop a principled and scalable way of determining which portions of texts are argumentative and what is the nature of argumentation. We propose a multi-step coding approach grounded in findings from argumentation research on managing the difficulties of coding arguments (Meyers and Brashers, 2010). In the first step, trained expert annotators identify basic argumentative features (coarse-grained analysis) in full-l"
W14-2106,andreas-etal-2012-annotating,0,0.124212,"is the nature of argumentation. We propose a two-tiered approach to achieve this goal and report on several initial studies to assess its potential. 1 Introduction Figure 1: Argumentative annotation of an Online Thread An increasing portion of information and opinion exchange occurs in online interactions such as discussion forums, blogs, and webpage comments. This type of user-generated conversational data provides a wealth of naturally occurring arguments. Argument mining of online interactions, however, is still in its infancy (Abbott et al., 2011; Biran and Rambow, 2011; Yin et al., 2012; Andreas et al., 2012; Misra and Walker, 2013). One reason is the lack of annotated corpora in this genre. To make progress, we need to develop a principled and scalable way of determining which portions of texts are argumentative and what is the nature of argumentation. We propose a multi-step coding approach grounded in findings from argumentation research on managing the difficulties of coding arguments (Meyers and Brashers, 2010). In the first step, trained expert annotators identify basic argumentative features (coarse-grained analysis) in full-length threads. In the second step, we explore the feasibility of"
W14-2106,J08-4004,0,0.0851004,"ng technique that makes it possible to assess how difficult it is to identify individual text segments as Callouts. Table 1: Number of Callouts by threads and EA Thread Android iPad Layoffs Twitter F1 EM 54.4 51.2 51.9 53.8 F1 OM 87.8 86.0 87.5 88.5 α 0.64 0.73 0.87 0.82 Table 2: IAA for 5 EA: F1 and alpha values per thread Units (ADUs) including their boundaries), Segment classification (label the roles of the ADUs, in this case Callout and Target) and relation identification (indicate the link between a Callout and the most recent Target to which is a response). The segmentation task, which Artstein and Poesio (2008) refer to as the unitization problem, is particularly challenging. Table 1 shows extensive variation in the number of ADUs (Callout in this case) identified by the EAs for each of the four threads. Annotator A1 identified the fewest Callouts (72) while A4 and A5 identified the most (112.3 and 116, respectively). Although these differences could be due to the issues with training, we interpret the consistent variation among coders as an indication that judges can be characterized as “lumpers” or “splitters”. What lumpers considered a single long unit was treated as two (or more) shorter units b"
W14-2106,N03-2012,0,0.0482673,"o annotate. Then we showed that crowdsourcing is a feasible approach to obtain annotations based on a finer grained argumentation scheme, especially on text segments that were easier for the Expert Annotators to label as being argumentative. While more qualitative analysis of these results is still needed, these results are an example of the potential benefits of our multi-step coding approach. Another line of research that is correlated with ours is recognition of agreement/disagreement (Misra and Walker, 2013; Yin et al., 2012; Abbott et al., 2011; Andreas et al., 2012; Galley et al., 2004; Hillard et al., 2003) and classification of stances (Walker et al., 2012; Somasundaran and Wiebe, 2010) in online forums. For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010; Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy. Avenues for future research include but are not limited to: 1) analyzing the differences between the stance and rationale annotations among the novice annotators; 2) improving the classification accura"
W14-2106,W11-0707,0,0.021897,"(see Table 3). While this assumption is logical, we plan to fully investigate it 42 Relation label in future work by running an MTurk experiment on all the Callout ADUs and their corresponding Targets. Agree Disagree Other We utilized Fleiss’ kappa (Fleiss, 1971) to compute IAA between the Turkers (every HIT was completed by five Turkers). Kappa is between 0.45-0.55 for each thread showing moderate agreement between the Turkers (Landis et al., 1977). These agreement results are in line with the agreement noticed in previous studies on agreement/disagreement annotations in online interactions (Bender et al., 2011; Abbott et al., 2011). To select a gold standard for the relation type, we used majority voting. That is, if three or more Turkers agreed on a label, we selected that label as the gold standard. In cases where there was no majority, we assigned the label Other. The total number of Callouts that are in agreement and in disagreement with Targets are 143 and 153, respectively. 5 39.36 56.91 3.72 # of EA ADUs per cluster 4 3 2 1 43.33 42.50 35.48 48.39 31.67 32.50 25.81 19.35 25.00 25.00 38.71 32.26 Table 5: Percentage of Relation labels per EA cluster type either the novice annotators could not"
W14-2106,P11-1020,0,0.0177056,". iPhone is a truly great design. –Same as above–Same as aboveMany of these iPhone . . . griping about issues that will only affect them once in a blue moon Many of these iPhone. . . Except for games?? iPhone is clearly dominant there. Table 3: Examples of Callouts lusters and their corresponding Targets Thread # of Clusters Android Ipad Layoffs Twitter 91 88 86 84 # of EA ADUs per cluster 5 4 3 2 1 52 16 11 7 5 41 17 7 13 10 41 18 11 6 10 44 17 14 4 5 nism to collect annotations and other type of data for natural language processing research (Wang and Callison-Burch, 2010; Snow et al., 2008; Chen and Dolan, 2011; Post et al., 2012). Crowdsourcing platforms such as Amazon Mechanical Turk (MTurk) provide a flexible framework to submit various types of NLP tasks where novice annotators (Turkers) can generate content (e.g., translations, paraphrases) or annotations (labeling) in an inexpensive way and with limited training. MTurk also provides researchers with the ability to control the quality of the Turkers, based on their past performances. Section 3.1 and 3.2 describe our two crowdsourcing studies for fine grain argumentation annotation. Table 4: Number of clusters for each cluster type Table 4 shows"
W14-2106,J87-1002,0,0.526089,"Missing"
W14-2106,W10-0214,0,0.0768784,"tain annotations based on a finer grained argumentation scheme, especially on text segments that were easier for the Expert Annotators to label as being argumentative. While more qualitative analysis of these results is still needed, these results are an example of the potential benefits of our multi-step coding approach. Another line of research that is correlated with ours is recognition of agreement/disagreement (Misra and Walker, 2013; Yin et al., 2012; Abbott et al., 2011; Andreas et al., 2012; Galley et al., 2004; Hillard et al., 2003) and classification of stances (Walker et al., 2012; Somasundaran and Wiebe, 2010) in online forums. For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010; Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy. Avenues for future research include but are not limited to: 1) analyzing the differences between the stance and rationale annotations among the novice annotators; 2) improving the classification accuracies of the Agree/Disagree classifier using more training data; 3) using syntax an"
W14-2106,W13-4006,0,0.051105,"entation. We propose a two-tiered approach to achieve this goal and report on several initial studies to assess its potential. 1 Introduction Figure 1: Argumentative annotation of an Online Thread An increasing portion of information and opinion exchange occurs in online interactions such as discussion forums, blogs, and webpage comments. This type of user-generated conversational data provides a wealth of naturally occurring arguments. Argument mining of online interactions, however, is still in its infancy (Abbott et al., 2011; Biran and Rambow, 2011; Yin et al., 2012; Andreas et al., 2012; Misra and Walker, 2013). One reason is the lack of annotated corpora in this genre. To make progress, we need to develop a principled and scalable way of determining which portions of texts are argumentative and what is the nature of argumentation. We propose a multi-step coding approach grounded in findings from argumentation research on managing the difficulties of coding arguments (Meyers and Brashers, 2010). In the first step, trained expert annotators identify basic argumentative features (coarse-grained analysis) in full-length threads. In the second step, we explore the feasibility of using crowdsourcing and"
W14-2106,W08-0122,0,0.124496,"Missing"
W14-2106,P13-1066,0,0.0153889,"alysis of these results is still needed, these results are an example of the potential benefits of our multi-step coding approach. Another line of research that is correlated with ours is recognition of agreement/disagreement (Misra and Walker, 2013; Yin et al., 2012; Abbott et al., 2011; Andreas et al., 2012; Galley et al., 2004; Hillard et al., 2003) and classification of stances (Walker et al., 2012; Somasundaran and Wiebe, 2010) in online forums. For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010; Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy. Avenues for future research include but are not limited to: 1) analyzing the differences between the stance and rationale annotations among the novice annotators; 2) improving the classification accuracies of the Agree/Disagree classifier using more training data; 3) using syntax and semantics inspired textual features and thread structure; and 4) developing computational models to detect Stance and Rationale. Recently, Cabrio and Villata (2013) proposed a new direct"
W14-2106,D09-1155,0,0.0238886,"Missing"
W14-2106,C10-2100,0,0.0504874,"esults are an example of the potential benefits of our multi-step coding approach. Another line of research that is correlated with ours is recognition of agreement/disagreement (Misra and Walker, 2013; Yin et al., 2012; Abbott et al., 2011; Andreas et al., 2012; Galley et al., 2004; Hillard et al., 2003) and classification of stances (Walker et al., 2012; Somasundaran and Wiebe, 2010) in online forums. For future work, we can utilize textual features (contextual, dependency, discourse markers), relevant multiword expressions and topic modeling (Mukherjee and Liu, 2013), and thread structure (Murakami and Raymond, 2010; Agrawal et al., 2003) to improve the Agree/Disagree classification accuracy. Avenues for future research include but are not limited to: 1) analyzing the differences between the stance and rationale annotations among the novice annotators; 2) improving the classification accuracies of the Agree/Disagree classifier using more training data; 3) using syntax and semantics inspired textual features and thread structure; and 4) developing computational models to detect Stance and Rationale. Recently, Cabrio and Villata (2013) proposed a new direction of argumentative analysis where the authors sh"
W14-2106,W10-0725,0,0.0208624,"tried the iPhone it felt natural immediately . . . iPhone is a truly great design. –Same as above–Same as aboveMany of these iPhone . . . griping about issues that will only affect them once in a blue moon Many of these iPhone. . . Except for games?? iPhone is clearly dominant there. Table 3: Examples of Callouts lusters and their corresponding Targets Thread # of Clusters Android Ipad Layoffs Twitter 91 88 86 84 # of EA ADUs per cluster 5 4 3 2 1 52 16 11 7 5 41 17 7 13 10 41 18 11 6 10 44 17 14 4 5 nism to collect annotations and other type of data for natural language processing research (Wang and Callison-Burch, 2010; Snow et al., 2008; Chen and Dolan, 2011; Post et al., 2012). Crowdsourcing platforms such as Amazon Mechanical Turk (MTurk) provide a flexible framework to submit various types of NLP tasks where novice annotators (Turkers) can generate content (e.g., translations, paraphrases) or annotations (labeling) in an inexpensive way and with limited training. MTurk also provides researchers with the ability to control the quality of the Turkers, based on their past performances. Section 3.1 and 3.2 describe our two crowdsourcing studies for fine grain argumentation annotation. Table 4: Number of clu"
W14-2106,W13-2324,0,0.34599,"Eemeren et al., 1993; Hutchby, 2013; Maynard, 1985). PAT states that an argument can arise at any point when two or more actors engage in calling out and making problematic some aspect of another actor’s prior contribution for what it (could have) said or meant (Van Eemeren et al., 1993). The argumentative relationships among contributions to a discussion are indicated through links between what is targeted and how it is called-out. Figure 1 shows an example of two Callouts that refer back to the same Target. The annotation task performed by the trained annotators includes three subtasks that Peldszus and Stede (2013a) identify as part of the argument mining problem: 1) Segmentation, 2) Segment classification, and 3) Relationship identification. In the language of Peldszus and Stede (2013a), Callouts and Targets are the basic Argument Discourse Units (ADUs) that are segmented, classified, and linked. There are two key advantages of our coarse-grained annotation scheme: 1) It does not initially prescribe what constitutes an argumentative text; 2) It makes it possible for Expert Annotators (EAs) to find ADUs in long 39 Proceedings of the First Workshop on Argumentation Mining, pages 39–48, c Baltimore, Mary"
W14-2106,W12-3152,0,0.00979125,"eat design. –Same as above–Same as aboveMany of these iPhone . . . griping about issues that will only affect them once in a blue moon Many of these iPhone. . . Except for games?? iPhone is clearly dominant there. Table 3: Examples of Callouts lusters and their corresponding Targets Thread # of Clusters Android Ipad Layoffs Twitter 91 88 86 84 # of EA ADUs per cluster 5 4 3 2 1 52 16 11 7 5 41 17 7 13 10 41 18 11 6 10 44 17 14 4 5 nism to collect annotations and other type of data for natural language processing research (Wang and Callison-Burch, 2010; Snow et al., 2008; Chen and Dolan, 2011; Post et al., 2012). Crowdsourcing platforms such as Amazon Mechanical Turk (MTurk) provide a flexible framework to submit various types of NLP tasks where novice annotators (Turkers) can generate content (e.g., translations, paraphrases) or annotations (labeling) in an inexpensive way and with limited training. MTurk also provides researchers with the ability to control the quality of the Turkers, based on their past performances. Section 3.1 and 3.2 describe our two crowdsourcing studies for fine grain argumentation annotation. Table 4: Number of clusters for each cluster type Table 4 shows the number of Callo"
W14-2106,reed-etal-2008-language,0,0.0081964,"ack button is key Navigation is. . . android Moderate — Just because the iPhone has a huge amount of apps, doesn’t mean they’re all worth having. — Difficult — Too difficult to code Table 10: Examples of Callout/Target pairs with difficulty level (majority voting) expresses an ‘attack’) on a dataset of arguments collected from online debates (e.g., Debatepedia). (Canon Camera) from Amazon e-commerce site. Relatively little attention has so far been devoted to the issue of building argumentative corpora from naturally occurring texts (Peldszus and Stede, 2013a; Feng and Hirst, 2011). However, (Reed et al., 2008; Reed and Rowe, 2004) have developed the Araucaria project that maintains an online repository of arguments (AraucariaDB), which recently has been used as research corpus for several automatic argumentation analyses (Palau and Moens, 2009; Wyner et al., 2010; Feng and Hirst, 2011). Our work contributes a new principled method for building annotated corpora for online interactions. The corpus and guidelines will also be shared with the research community. 5 Conclusion and Future Work To make progress in argument mining for online interactions, we need to develop a principled and scalable way t"
W14-2106,W12-3710,0,0.0768598,"entative and what is the nature of argumentation. We propose a two-tiered approach to achieve this goal and report on several initial studies to assess its potential. 1 Introduction Figure 1: Argumentative annotation of an Online Thread An increasing portion of information and opinion exchange occurs in online interactions such as discussion forums, blogs, and webpage comments. This type of user-generated conversational data provides a wealth of naturally occurring arguments. Argument mining of online interactions, however, is still in its infancy (Abbott et al., 2011; Biran and Rambow, 2011; Yin et al., 2012; Andreas et al., 2012; Misra and Walker, 2013). One reason is the lack of annotated corpora in this genre. To make progress, we need to develop a principled and scalable way of determining which portions of texts are argumentative and what is the nature of argumentation. We propose a multi-step coding approach grounded in findings from argumentation research on managing the difficulties of coding arguments (Meyers and Brashers, 2010). In the first step, trained expert annotators identify basic argumentative features (coarse-grained analysis) in full-length threads. In the second step, we expl"
W14-2106,D08-1027,0,0.0698989,"Missing"
W14-2106,P04-1085,0,\N,Missing
W14-2106,P11-1099,0,\N,Missing
W14-4918,J08-4004,0,0.259273,"ally identify discourse units. 1 Introduction Annotation of discourse typically involves three subtasks: segmentation (identification of discourse units, including their boundaries), segment classification (labeling the role of discourse units) and relation identification (indicating the link between the discourse units) (Peldszus and Stede, 2013a). The difficulty of achieving an Inter-Annotator Agreement (IAA) of .80, which is generally accepted as good agreement, is compounded in studies of discourse annotations since annotators must unitize, i.e. identify the boundaries of discourse units (Artstein and Poesio, 2008). The inconsistent assignment of boundaries in annotation of discourse has been noted at least since Grosz and Sidner (1986) who observed that although annotators tended to identify essentially the same units, the boundaries differed slightly. The need for annotators to identify the boundaries of text segments makes measurement of IAA more difficult because standard coefficients such as κ assume that the units to be coded have been identified before the coding begins (Artstein and Poesio, 2008). A second challenge for measuring IAA for discourse annotation is associated with larger numbers of"
W14-4918,J11-4004,0,0.0231622,"(Artstein and Poesio, 2008). A second challenge for measuring IAA for discourse annotation is associated with larger numbers of annotators. Because of the many ways that ideas are expressed in human language, using multiple annotators to study discourse phenomena is important. Such an approach capitalizes on the aggregated intuitions of multiple coders to overcome the potential biases of any one coder and helps identify limitations in the coding scheme, thus adding to the reliability and validity of the annotation study. The more annotators, however, the harder it is to achieve an IAA of .80 (Bayerl and Paul, 2011). What to annotate also depends, among other characteristics, on the phenomenon of interest, the text being annotated, the quality of the annotation scheme and the effectiveness of training. But even if these are excellent, there is natural variability in human judgment for a task that involves subtle distinctions about which competent coders disagree. An accurate computational model should reflect this variability (Aakhus et al., 2013). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence"
W14-4918,W14-2106,1,0.801205,"Table 9; the penalty would be decreased for not identifying ADUs on which fewer annotators agree. Qualitative analysis may help discover the reason for the variation in strength of clusters, thereby supporting our ability to interpret IAA and to create accurate computational models of human judgments about discourse units. As a related research, PAT and the clustering technique discussed in this paper allow the development of a finer-grained annotation scheme to analyze the type of links between Target-Callout (e.g., Agree/Disagree/Other), and the nature of Callouts (e.g., Stance/Rationale) (Ghosh et al., 2014). 5 Conclusion and Future Work Reliability of annotation studies is important both as part of the demonstration of the validity of the phenomena being studied and also to support accurate computational modeling of discourse phenomena. The nature of ADUs, with their fuzzy boundaries, makes it hard to achieve IAA of .80 or higher. Furthermore, the use of a single figure for IAA is a little like relying on an average to convey the range of variation of a set of numbers. The contributions of this paper are i) to provide concrete examples of the difficulties of using state of the art metrics like P"
W14-4918,J86-3001,0,0.735277,"cation of discourse units, including their boundaries), segment classification (labeling the role of discourse units) and relation identification (indicating the link between the discourse units) (Peldszus and Stede, 2013a). The difficulty of achieving an Inter-Annotator Agreement (IAA) of .80, which is generally accepted as good agreement, is compounded in studies of discourse annotations since annotators must unitize, i.e. identify the boundaries of discourse units (Artstein and Poesio, 2008). The inconsistent assignment of boundaries in annotation of discourse has been noted at least since Grosz and Sidner (1986) who observed that although annotators tended to identify essentially the same units, the boundaries differed slightly. The need for annotators to identify the boundaries of text segments makes measurement of IAA more difficult because standard coefficients such as κ assume that the units to be coded have been identified before the coding begins (Artstein and Poesio, 2008). A second challenge for measuring IAA for discourse annotation is associated with larger numbers of annotators. Because of the many ways that ideas are expressed in human language, using multiple annotators to study discours"
W14-4918,N06-4006,0,0.0337807,"ch blog as our corpus, along with the original posting. We refer to each blog and the associated comments as a thread. The complexity of the phenomenon required the perspective of multiple independent annotators, despite the known difficulty in achieving reliable IAA with more than two annotators. For our initial study, in which our goal was to obtain naturally occurring examples of Callouts and Targets and assess the challenges of reliably identifying them, we engaged five graduate students with a strong humanities background. The coding was performed with the open-source Knowtator software (Ogren, 2006). All five judges annotated all 100 comments in all five threads. While the annotation process was under way, annotators were instructed not to communicate with each other about the study. The annotators’ task was to find each instance of a Callout, determine the boundaries, link the Callout to the most recent Target and determine the boundaries of the Target. We prepared and tested a set of guidelines with definitions and examples of key concepts. The following is an adapted excerpt from the guidelines: • Callout: A Callout is (a part of) a subsequent action that selects (a part of) a prior a"
W14-4918,W13-2324,0,0.42418,"re annotators who identify a text segment, the easier we assume that the text segment is to annotate. The clusters make it possible to quantify the extent of agreement judges show about text segments; this information can be used to assess the output of systems that automatically identify discourse units. 1 Introduction Annotation of discourse typically involves three subtasks: segmentation (identification of discourse units, including their boundaries), segment classification (labeling the role of discourse units) and relation identification (indicating the link between the discourse units) (Peldszus and Stede, 2013a). The difficulty of achieving an Inter-Annotator Agreement (IAA) of .80, which is generally accepted as good agreement, is compounded in studies of discourse annotations since annotators must unitize, i.e. identify the boundaries of discourse units (Artstein and Poesio, 2008). The inconsistent assignment of boundaries in annotation of discourse has been noted at least since Grosz and Sidner (1986) who observed that although annotators tended to identify essentially the same units, the boundaries differed slightly. The need for annotators to identify the boundaries of text segments makes meas"
W14-4918,W08-0122,0,0.0292361,"iability highlights an important challenge for calculating IAA with multiple coders and fuzzy unit boundaries. 3 Some Problems of Unitization Reliability with Existing IAA Metrics In this section we discuss two state-of-the-art metrics frequently used for measuring IAA for discourse annotation and we show that these methods offer limited informativeness when text boundaries are fuzzy and there are multiple judges. These methods are the information retrieval inspired precision-recall (P/R/F1) metrics used in Wiebe and her collaborators’ important work on sentiment analysis (Wiebe et al., 2005; Somasundaran et al., 2008) and Krippendorff’s α, a variant of the α family of IAA coefficients specifically designed to handle fuzzy boundaries and multiple annotators (Krippendorff, 1995; Krippendorff, 2004b). Krippendorff’s α determines IAA based on observed disagreement relative to expected agreement and calculates differences in annotators’ judgments. Although it is possible to use number of words or even clauses to measure IAA, we use length in characters both for consistency with Wiebe’s approach and because Krippendorff (2004b, pp.790-791) recommends using “. . . the smallest distinguishable length, for example"
W16-2810,P12-2041,0,0.0242896,"ons and reaching consensus (Van Eemeren et al., 2013). The automatic identification and evaluation of arguments require three main stages: 1) the identification, segmentation and classification of argumentative discourse units (ADUs), 2) the identification and classification of the relations between ADUs (Peldszus and Stede, 2013a), and 3) the identification of argument schemes, namely the implicit and explicit inferential relations within and across ADUs (Macagno, 2014). Although considerable steps have been taken towards the first two stages (Teufel and Moens, 2002; Stab and Gurevych, 2014; Cabrio and Villata, 2012; Ghosh et al., 2014; Aharoni et al., 2014; Rosenthal and McKeown, 2012; Biran and Rambow, 2011; Llewellyn et al., 2014), the third stage still constitutes a major challenge because large corpora systematically annotated with argument schemes are lacking. As noticed by Palau and Moens (2009), this is due to the proliferation in Argumentation Theory of different taxonomies of argument schemes based on weak distinctive criteria, which makes it difficult to develop intersubjective guidelines for annotation. In the Araucaria dataset (Reed and Rowe, 2004), for example, two argument scheme sets othe"
W16-2810,P11-1099,0,0.192854,"ent taxonomies of argument schemes based on weak distinctive criteria, which makes it difficult to develop intersubjective guidelines for annotation. In the Araucaria dataset (Reed and Rowe, 2004), for example, two argument scheme sets other than Walton’s are used as annotation protocols (Katzav and Reed, 2004; Pollock, 1995). To overcome this problem, the most successfully applied strategy has been to pre-select from existing larger typologies, such as that of Walton et al. (2008), a subset of argument schemes which is most frequent in a particular text genre, domain or context (Green, 2015; Feng and Hirst, 2011; Song et al., 2014; Schneider et al., 2013) and provide annotators with critical questions as a means to identify the appropriate scheme. Such a bottom up approach allows one to improve the identiThe annotation of argument schemes represents an important step for argumentation mining. General guidelines for the annotation of argument schemes, applicable to any topic, are still missing due to the lack of a suitable taxonomy in Argumentation Theory and the need for highly trained expert annotators. We present a set of guidelines for the annotation of argument schemes, taking as a framework the"
W16-2810,llewellyn-etal-2014-using,0,\N,Missing
W16-2810,W13-2324,0,\N,Missing
W16-2810,W14-2110,0,\N,Missing
W16-2810,D14-1006,0,\N,Missing
W16-2810,W15-0502,0,\N,Missing
W16-2810,W14-2109,0,\N,Missing
W16-2810,W14-2106,1,\N,Missing
W16-2810,J02-4002,0,\N,Missing
W16-2810,L16-1167,0,\N,Missing
W16-3213,N12-1094,0,0.0172876,"anguage (VDL). They show that humans are able to reliably annotate text segments as containing ‘visually descriptive’ language or not, providing evidence that standalone text can be classified by 90 the visualness of its contents. In our work, motivated by the end task of fine-grained classification, we restrict the definition to ‘visually relevant’. As Gaizauskas et al. (2015) do, we show that humans can reliably annotate text as visually relevant or not. Unlike Gaizauskas et al. (2015), we propose a method to automatically detect visually relevant sentences from full-text documents. Second, Dodge et al. (2012) propose a method to separate visual text from non-visual text in image captions. However, their method focuses just on noun-phrases, while our approach finds visually relevant sentences in full-length documents. While our end result is a set of visually relevant text descriptions, our approach is complementary to the rich body of work on generating text descriptions from images (see (Bernardi et al., 2016) for a survey), since our method extracts such descriptions from existing text. 6 Conclusion Our work shows that it is possible to take domain-specific full-length documents—such as Wikipedi"
W16-3213,W15-2805,0,0.0754475,"descriptions, removing the necessity of manually creating aligned image-text datasets. In this work, we focus on bird species, as this is one of the most well-studied and challenging finegrained classification domains, using Wikipedia articles as our text (Section 2). To build our computational models, we must first define the notion of ‘visually relevant’ sentences. We use the defini86 Proceedings of the 5th Workshop on Vision and Language, pages 86–91, c Berlin, Germany, August 12 2016. 2016 Association for Computational Linguistics tion of Visually Descriptive Language (VDL) introduced by Gaizauskas et al. (2015), with some restrictions. Like VDL, we aim to identify ‘visually confirmed’ rather than ‘visually concrete’ segments of text as our descriptions correspond to a class (the bird species) rather than a particular image. For example, a sentence describing a bird’s feet can be a ‘visually relevant’ sentence for a bird, though it would not be ‘visually concrete’ for an image of the bird flying with its feet hidden. Unlike VDL, for the scope of this paper we are interested only in the sentences which are visually descriptive with respect to the object (i.e., bird species). We define such sentences a"
W16-3213,P14-1062,0,0.0318322,"said 0 and ‘I don’t know’. This test set, which we call 200HumV RL , contains 1248 sentences of class 1 (VRL) and 5094 sentences of class 0 (non-VRL). 3 Detecting Visually Relevant Sentences As mentioned earlier, our task can be framed as a binary sentence classification problem, where each sentence is labeled either as VRL or nonVRL. Deep learning methods, and in particular convolutional neural networks (CNNs), have become some of the top performing methods on various NLP tasks that can be modeled as sentence classification (e.g, sentiment analysis, question type classification) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015). We use the non-linear, non-consecutive convolution neural network architecture proposed by Lei et al. (2015), which we refer to as CNNLei . This CNN uses tensor products to combine nonconsecutive n-grams of each sentence to create an embedding per sentence. The non-consecutive aspect of the n-gram allows it to capture cooccurrence of words spread across sentences: “yellow crown, rump and flank patch” will generate representations of the relevant noun-adjective pairs “yellow crown”, “yellow rump”, and “yellow flank patch”. The tensor product is used as a “generalized approa"
W16-3213,D14-1181,0,0.00674986,"e other two said 0 and ‘I don’t know’. This test set, which we call 200HumV RL , contains 1248 sentences of class 1 (VRL) and 5094 sentences of class 0 (non-VRL). 3 Detecting Visually Relevant Sentences As mentioned earlier, our task can be framed as a binary sentence classification problem, where each sentence is labeled either as VRL or nonVRL. Deep learning methods, and in particular convolutional neural networks (CNNs), have become some of the top performing methods on various NLP tasks that can be modeled as sentence classification (e.g, sentiment analysis, question type classification) (Kim, 2014; Kalchbrenner et al., 2014; Lei et al., 2015). We use the non-linear, non-consecutive convolution neural network architecture proposed by Lei et al. (2015), which we refer to as CNNLei . This CNN uses tensor products to combine nonconsecutive n-grams of each sentence to create an embedding per sentence. The non-consecutive aspect of the n-gram allows it to capture cooccurrence of words spread across sentences: “yellow crown, rump and flank patch” will generate representations of the relevant noun-adjective pairs “yellow crown”, “yellow rump”, and “yellow flank patch”. The tensor product is us"
W16-3213,D15-1180,0,0.0230436,"Missing"
W16-3213,W15-2812,0,0.048208,"Missing"
W17-5102,J17-1004,0,0.0849223,"n: logos, pathos, ethos. At the conceptual level, the distinction between different modes of persuasion dates back to Aristotle’s Rhetorics. Aristotle considered that a good argument consists of the contextually appropriate combination of pathos, ethos, and logos. Duthie et al. (2016) have developed a methodology to retrieve ethos in political debates. Higgins and Walker (2012) traced back ethos, pathos and logos as strategies of persuasion in social and environmental reports. Their definition of logos applies both to premises and claims, while we consider logos as referred to arguments only. Habernal and Gurevych (2017) have also included logos and pathos, but not ethos, among the labels for an argumentatively annotated corpus of 990 user generated comments. They obtained moderate agreement for the annotation of logos, while low agreement for pathos. Our study shows moderate agreement on all types of persuasion modes. On the computational side, the Internet Argument Corpus (IAC) (Walker et al., 2012) —- data from the online discussion sites 4forums.com and CreateDebate — includes the distinction between fact and emotion based arguments. Das et al. (2016) looked at the diffusion of information through social"
W17-5102,W16-2803,0,0.244423,"uished propositions expressing agreement and disagreement because they present an anaphoric function inherent to the dialogic nature of the corpus. An example is given in Figure 1.1 Related Work There are three areas relevant to the work presented in this paper, which we address in turn. Persuasion detection and prediction. Recent studies in argument mining and computational social science have focused on persuasion detection and prediction. A bulk of them have focused on the identification of structural and lexical features that happen to be associated with persuasive arguments. Ghosh et al. (2016) have shown that the number of supported/unsupported claims and the structure of arguments directly affect persuasion. Habernal and Gurevych (2016) have experimented with SVM and bidirectional LSTM to predict arguments scored by annotators as convincing mainly using lexical linguistic features (e.g., modal verbs, verb tenses, sentiment scores). Taking advantage of the Change My View dataset, (Tan et al., 2016), have investigated whether lexical features and interaction patterns affect persuasion, finding that lexical diversity plays a major role. In a similar vein, other studies have ranked ar"
W17-5102,W12-2105,0,0.029992,"pragmatic information. However, distinct taxonomies to account for semantic differences characterizing claims vs. premises and their degrees of persuasiveness has so far not been investigated. Our study contributes to previous work in proposing a novel and reliable annotation scheme, which combines semantic types for both claims and premises at the propositional level, allowing to observe relevant combinations in persuasive messages. who have credibility in the group, persist in attempting to convince others, and introduce ideas that others pick up on or support (Rosenthal and Mckeown, 2017; Biran et al., 2012). In particular, Rosenthal and Mckeown (2017) draw their approach from Cialdini’s (Cialdini, 2005) idea of “weapons of influence,” which include reciprocation (sentiment and agreement components), commitment (claims and agreement), social proof (dialog patterns), liking (sentiment and credibility), authority (credibility), and scarcity (author traits). Our approach zooms into the detection of commitment analyzing not only the presence of claims/arguments, but also their conceptual type. We, moreover, treat credibility as an argument type. Modes of persuasion: logos, pathos, ethos. At the conce"
W17-5102,budzynska-etal-2014-model,0,0.0590892,"/chridey/change-my-view-modes 12 Semantics of argument components. Recently, new interest has arisen in analyzing the semantics of argument components. Becker et al. (2016) have investigated correlations between situation entity types and claims/premises.Park et al. (2015) have proposed a classification of claims in relation to the subjectivity/objectivity of the premises in their support. On a different note, a scalable and empirically validated annotation scheme has been proposed for the analysis of illocutionary structures in argumentative dialogues drawing from Inference Anchoring Theory (Budzynska et al., 2014; Budzynska and Reed, 2011), relying on different types of pragmatic information. However, distinct taxonomies to account for semantic differences characterizing claims vs. premises and their degrees of persuasiveness has so far not been investigated. Our study contributes to previous work in proposing a novel and reliable annotation scheme, which combines semantic types for both claims and premises at the propositional level, allowing to observe relevant combinations in persuasive messages. who have credibility in the group, persist in attempting to convince others, and introduce ideas that o"
W17-5102,E17-1070,0,0.255093,"os. Our study shows moderate agreement on all types of persuasion modes. On the computational side, the Internet Argument Corpus (IAC) (Walker et al., 2012) —- data from the online discussion sites 4forums.com and CreateDebate — includes the distinction between fact and emotion based arguments. Das et al. (2016) looked at the diffusion of information through social media and how author intent affects message propagation. They found that persuasive messages were more likely to be received positively if the emotional or logical components of a message were selected according to the given topic. Lukin et al. (2017) examined how personality traits and emotional or logical arguments affect persuasiveness. 3 3.1 Annotation Process Source data Change My View is a discussion forum on the site reddit.com. The initiator of the discussion will create a title for their post (which contains the major claim of the argument) and then describe the reasons for their belief. Other posters will respond and attempt to change the original poster’s view. If they are successful, the original poster will indicate that their view was changed by providing a ∆ point. We use the same dataset from the Change My View forum create"
W17-5102,L16-1650,0,0.205922,"uished propositions expressing agreement and disagreement because they present an anaphoric function inherent to the dialogic nature of the corpus. An example is given in Figure 1.1 Related Work There are three areas relevant to the work presented in this paper, which we address in turn. Persuasion detection and prediction. Recent studies in argument mining and computational social science have focused on persuasion detection and prediction. A bulk of them have focused on the identification of structural and lexical features that happen to be associated with persuasive arguments. Ghosh et al. (2016) have shown that the number of supported/unsupported claims and the structure of arguments directly affect persuasion. Habernal and Gurevych (2016) have experimented with SVM and bidirectional LSTM to predict arguments scored by annotators as convincing mainly using lexical linguistic features (e.g., modal verbs, verb tenses, sentiment scores). Taking advantage of the Change My View dataset, (Tan et al., 2016), have investigated whether lexical features and interaction patterns affect persuasion, finding that lexical diversity plays a major role. In a similar vein, other studies have ranked ar"
W17-5102,P16-2089,1,0.906908,"iveness of a message lies at the interface between discourse form (i.e., use of hedges, connectives, rhetorical questions) and conceptual form such as the artful use of ethos (credibility and trustworthiness of the speaker), pathos (appeal to audience feelings), and logos (appeal to the rationality of the audience through logical reasoning). Recent work in argumentation mining and detection of persuasion has so far mainly explored the persuasive role played by features related to discourse form (Stab and Gurevych, 2014a; Peldszus and Stede, 2016; Habernal and Gurevych, 2016; Tan et al., 2016; Ghosh et al., 2016). However, due to the lack of suitable training data, the detection of conceptual features is still nascent. Argumentative text has been analyzed both theoretically and computationally in terms of argumentative structure that consists of argument components (e.g., claims, premises) and their argumentative relations (e.g., support, attack). Less emphasis has been placed on analyzing the semantic types of argument components. We propose a two-tiered annotation scheme to label claims and premises and their semantic types in an online persuasive forum, Change My View, with the long-term goal of un"
W17-5102,P16-1150,0,0.427989,"sity kathy@cs.columbia.edu Abstract the persuasiveness of a message lies at the interface between discourse form (i.e., use of hedges, connectives, rhetorical questions) and conceptual form such as the artful use of ethos (credibility and trustworthiness of the speaker), pathos (appeal to audience feelings), and logos (appeal to the rationality of the audience through logical reasoning). Recent work in argumentation mining and detection of persuasion has so far mainly explored the persuasive role played by features related to discourse form (Stab and Gurevych, 2014a; Peldszus and Stede, 2016; Habernal and Gurevych, 2016; Tan et al., 2016; Ghosh et al., 2016). However, due to the lack of suitable training data, the detection of conceptual features is still nascent. Argumentative text has been analyzed both theoretically and computationally in terms of argumentative structure that consists of argument components (e.g., claims, premises) and their argumentative relations (e.g., support, attack). Less emphasis has been placed on analyzing the semantic types of argument components. We propose a two-tiered annotation scheme to label claims and premises and their semantic types in an online persuasive forum, Change"
W17-5102,C14-1142,0,0.162759,"McKeown Computer Science Department Columbia University kathy@cs.columbia.edu Abstract the persuasiveness of a message lies at the interface between discourse form (i.e., use of hedges, connectives, rhetorical questions) and conceptual form such as the artful use of ethos (credibility and trustworthiness of the speaker), pathos (appeal to audience feelings), and logos (appeal to the rationality of the audience through logical reasoning). Recent work in argumentation mining and detection of persuasion has so far mainly explored the persuasive role played by features related to discourse form (Stab and Gurevych, 2014a; Peldszus and Stede, 2016; Habernal and Gurevych, 2016; Tan et al., 2016; Ghosh et al., 2016). However, due to the lack of suitable training data, the detection of conceptual features is still nascent. Argumentative text has been analyzed both theoretically and computationally in terms of argumentative structure that consists of argument components (e.g., claims, premises) and their argumentative relations (e.g., support, attack). Less emphasis has been placed on analyzing the semantic types of argument components. We propose a two-tiered annotation scheme to label claims and premises and th"
W17-5102,D14-1006,0,0.0459201,"McKeown Computer Science Department Columbia University kathy@cs.columbia.edu Abstract the persuasiveness of a message lies at the interface between discourse form (i.e., use of hedges, connectives, rhetorical questions) and conceptual form such as the artful use of ethos (credibility and trustworthiness of the speaker), pathos (appeal to audience feelings), and logos (appeal to the rationality of the audience through logical reasoning). Recent work in argumentation mining and detection of persuasion has so far mainly explored the persuasive role played by features related to discourse form (Stab and Gurevych, 2014a; Peldszus and Stede, 2016; Habernal and Gurevych, 2016; Tan et al., 2016; Ghosh et al., 2016). However, due to the lack of suitable training data, the detection of conceptual features is still nascent. Argumentative text has been analyzed both theoretically and computationally in terms of argumentative structure that consists of argument components (e.g., claims, premises) and their argumentative relations (e.g., support, attack). Less emphasis has been placed on analyzing the semantic types of argument components. We propose a two-tiered annotation scheme to label claims and premises and th"
W17-5102,walker-etal-2012-corpus,0,0.0405065,"os, pathos and logos as strategies of persuasion in social and environmental reports. Their definition of logos applies both to premises and claims, while we consider logos as referred to arguments only. Habernal and Gurevych (2017) have also included logos and pathos, but not ethos, among the labels for an argumentatively annotated corpus of 990 user generated comments. They obtained moderate agreement for the annotation of logos, while low agreement for pathos. Our study shows moderate agreement on all types of persuasion modes. On the computational side, the Internet Argument Corpus (IAC) (Walker et al., 2012) —- data from the online discussion sites 4forums.com and CreateDebate — includes the distinction between fact and emotion based arguments. Das et al. (2016) looked at the diffusion of information through social media and how author intent affects message propagation. They found that persuasive messages were more likely to be received positively if the emotional or logical components of a message were selected according to the given topic. Lukin et al. (2017) examined how personality traits and emotional or logical arguments affect persuasiveness. 3 3.1 Annotation Process Source data Change My"
W17-5102,P16-2032,0,0.063047,"unsupported claims and the structure of arguments directly affect persuasion. Habernal and Gurevych (2016) have experimented with SVM and bidirectional LSTM to predict arguments scored by annotators as convincing mainly using lexical linguistic features (e.g., modal verbs, verb tenses, sentiment scores). Taking advantage of the Change My View dataset, (Tan et al., 2016), have investigated whether lexical features and interaction patterns affect persuasion, finding that lexical diversity plays a major role. In a similar vein, other studies have ranked arguments according to their karma scores (Wei et al., 2016), showing that aspects of argumentative language and social interaction are persuasive features. In this paper, we focus on the conceptual aspects of a persuasive message by analyzing the semantic types of claims and premises. A closely related area of research is the detection of situational influencers — participants in a discussion We aim to answer three questions: 1) can humans reliably annotate claim and premises and their semantic types? (Section 4) 2) are types of premises/claims positioned in recurrent orders? and 3) are certain types of claims and/or premises more likely to appear in"
W17-5523,D15-1075,0,0.210443,"onsiders both the reply and its context (S c+r vs. N S c+r task). We experiment with two types of computational models: Support Vector Machines (SVM) with linguistically-motivated discrete features (used as baseline; SVMbl ), and approaches using distributed representations. For the latter we use the Long short-term Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997) that have been shown to be successful in various NLP tasks, such as constituency parsing (Vinyals et al., 2015), language modeling (Zaremba et al., 2014), machine translation (Sutskever et al., 2014) and textual entailment (Bowman et al., 2015; Rockt¨aschel et al., 2015; Parikh et al., 2016). We present these models in the next subsections. 3.1 • Sarcasm Indicators: Burgers et al. (2012) introduce a set of sarcasm indicators that explicitly signal if an utterance is sarcastic. We use morpho-syntactic features such as interjections (e.g., “uh”, “oh”, “yeah”), tag questions (e.g., “is not it?”, “don’t they”), exclamation marks (e.g., “!”, “?”); typographic features such as capitalization of words, quotation marks, emoticons; tropes such as superlative and intensifiers words (e.g., “greatest”, “best”, “really”) that often occur in sar"
W17-5523,W10-2914,0,0.328542,"LSTM network (Rockt¨aschel et al., 2015) and LSTM networks with sentence level attention on context and response outperform the LSTM model that reads only the response. To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task. 1 Table 1: Sample Context/Reply pairs from two social media platforms statement, the sarcastic intent of UserB’s response might not be detected. Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016). In many instances, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014). In this paper, we investigate the role of conversation context in detecting sarcasm in social media discussions (Twitter conversations and discussion forums). Table 1 shows some examples of sarcastic replies taken from two media platforms (userB Introduction I"
W17-5523,maynard-greenwood-2014-cares,0,0.529114,"and response outperform the LSTM model that reads only the response. To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task. 1 Table 1: Sample Context/Reply pairs from two social media platforms statement, the sarcastic intent of UserB’s response might not be detected. Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016). In many instances, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014). In this paper, we investigate the role of conversation context in detecting sarcasm in social media discussions (Twitter conversations and discussion forums). Table 1 shows some examples of sarcastic replies taken from two media platforms (userB Introduction It has been argued that sarcasm, or verbal irony, is a type of interactional phenomenon with specific perlocu"
W17-5523,W16-0425,0,0.384841,"d issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task. 1 Table 1: Sample Context/Reply pairs from two social media platforms statement, the sarcastic intent of UserB’s response might not be detected. Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016). In many instances, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014). In this paper, we investigate the role of conversation context in detecting sarcasm in social media discussions (Twitter conversations and discussion forums). Table 1 shows some examples of sarcastic replies taken from two media platforms (userB Introduction It has been argued that sarcasm, or verbal irony, is a type of interactional phenomenon with specific perlocutionary effects on the hearer (Haverkate, 1990), such as to break their pattern of e"
W17-5523,D15-1116,1,0.915495,"only the response. To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task. 1 Table 1: Sample Context/Reply pairs from two social media platforms statement, the sarcastic intent of UserB’s response might not be detected. Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016). In many instances, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014). In this paper, we investigate the role of conversation context in detecting sarcasm in social media discussions (Twitter conversations and discussion forums). Table 1 shows some examples of sarcastic replies taken from two media platforms (userB Introduction It has been argued that sarcasm, or verbal irony, is a type of interactional phenomenon with specific perlocutionary effects on the hearer (Haverkate"
W17-5523,P11-2008,0,0.0557849,"Missing"
W17-5523,W16-3604,0,0.133922,"Missing"
W17-5523,P11-2102,1,0.954956,"5) and LSTM networks with sentence level attention on context and response outperform the LSTM model that reads only the response. To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task. 1 Table 1: Sample Context/Reply pairs from two social media platforms statement, the sarcastic intent of UserB’s response might not be detected. Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016). In many instances, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014). In this paper, we investigate the role of conversation context in detecting sarcasm in social media discussions (Twitter conversations and discussion forums). Table 1 shows some examples of sarcastic replies taken from two media platforms (userB Introduction It has been argued that sarcasm, or"
W17-5523,D16-1244,0,0.102712,"Missing"
W17-5523,D13-1066,0,0.507155,"Missing"
W17-5523,P15-2124,0,0.76875,"STM model that reads only the response. To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task. 1 Table 1: Sample Context/Reply pairs from two social media platforms statement, the sarcastic intent of UserB’s response might not be detected. Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016). In many instances, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014). In this paper, we investigate the role of conversation context in detecting sarcasm in social media discussions (Twitter conversations and discussion forums). Table 1 shows some examples of sarcastic replies taken from two media platforms (userB Introduction It has been argued that sarcasm, or verbal irony, is a type of interactional phenomenon with specific perlocutionary effects on t"
W17-5523,D16-1104,0,0.706386,"To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task. 1 Table 1: Sample Context/Reply pairs from two social media platforms statement, the sarcastic intent of UserB’s response might not be detected. Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016). In many instances, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014). In this paper, we investigate the role of conversation context in detecting sarcasm in social media discussions (Twitter conversations and discussion forums). Table 1 shows some examples of sarcastic replies taken from two media platforms (userB Introduction It has been argued that sarcasm, or verbal irony, is a type of interactional phenomenon with specific perlocutionary effects on the hearer (Haverkate, 1990), such as to"
W17-5523,W15-2905,0,0.122993,"ans have difficulty sometimes in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014). Thus, recent work on sarcasm and irony detection have started to exploit contextual information. In par193 congruity). In future, we plan to study larger context, such as the full thread in a discussion forum that consider also the responses to the sarcastic comment, when available. We are also interested in analyzing sarcastic replies that do not contain sarcasm markers or explicit incongruence (i.e., opposing sentiment between the context and the reply). ticular, (Khattri et al., 2015) analyzed authors’ prior sentiment towards certain entities and if a new tweet deviates from the author’s estimated sentiment the tweet is predicted to be sarcastic. Similar to this approach, several models have been introduced; some relied on extensive feature engineering to capture contextual information about authors, topics or conversation context whereas the rest are using deep learning techniques to embed authors’ information (Rajadesingan et al., 2015). The two studies that have considered conversation context among other contextual information have shown minimal improvement when modeli"
W17-5523,W13-1605,0,0.165953,"Missing"
W17-5523,P14-2084,0,0.517142,"man performance on the task. 1 Table 1: Sample Context/Reply pairs from two social media platforms statement, the sarcastic intent of UserB’s response might not be detected. Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016). In many instances, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014). In this paper, we investigate the role of conversation context in detecting sarcasm in social media discussions (Twitter conversations and discussion forums). Table 1 shows some examples of sarcastic replies taken from two media platforms (userB Introduction It has been argued that sarcasm, or verbal irony, is a type of interactional phenomenon with specific perlocutionary effects on the hearer (Haverkate, 1990), such as to break their pattern of expectation. Thus, to be able to detect speakers’ sarcastic intent it is necessary (even if maybe not sufficient) to consider their utterances in t"
W17-5523,H05-1044,0,0.134225,"rcastic vs. non-sarcastic replies was done using crowdsourcing, where annotators were asked to label a reply as sarcastic if any part of the reply contained sarcasm (thus annotation is done at the reply/comment level and not sentence level). The final gold sarcastic label was assigned only if a majority of the annotators labeled the reply as sarcastic. Although the dataset described by Oraby et al. (2016) consists of 9,400 post, only 2 3 This reduction in the training size will have obvious effects in the classification performance. https://github.com/debanjanghosh/sarcasm context 187 “MPQA” (Wilson et al., 2005) and “Opinion Lexicon” (Hu and Liu, 2004). To capture sentiment, we count the number of positive and negative sentiment tokens, negations, and use a boolean feature that represents whether a reply contains both positive and negative sentiment tokens. For the S c+r vs. N S c+r classification task, we check whether the reply r has a different sentiment than the context c (similar to Joshi et al. (2015)). Given that sarcastic utterances often contain a positive sentiment towards a negative situation, we hypothesize that this feature will capture this type of sentiment incongruity. is given direct"
W17-5523,N16-1174,0,0.495554,"ry is treated as a separate feature and we define a Boolean feature that indicates if a context or a reply contains a LIWC category. Two sentiment lexicons are also used to model the utterance sentiment: 188 Parikh et al., 2016)). Since our goal is to explore the role of contextual information (our first input) for recognizing whether the reply (our second input) is sarcastic or not, we argue that using LSTM networks that read the context and reply are a natural modeling choice. Attention-based LSTM Networks: Attentive neural networks have been shown to perform well on a variety of NLP tasks (Yang et al., 2016; Yin et al., 2015; Xu et al., 2015). Using attentionbased LSTM will accomplish two goals: (1) test whether they achieve higher performance than simple LSTM models and (2) use the attention weights produced by the LSTM models to perform a qualitative analysis to determine which portions of context triggers the sarcastic reply. Although Yang et al. (2016) have included two levels of attention mechanisms – one at the word level and another at the sentence level – we primarily focus on sentence level attention for two specific reasons. First, sentence level attentions can show the exact sentence"
W18-5513,N18-2004,0,0.0324742,"Missing"
W18-5513,D17-1070,0,0.0309615,"r (Kingma and Ba, 2014) and trained the model for 10 epochs. For the SJ and S+ MJ conditions we experiment with two architectures: in the first one we just concatenate the justification to the statement and pass it to a single BiLSTM, and in the second one we use a dual/parallel architecture where one BiLSTM reads the statement and another one reads the justification (architecture denoted as P-BiLSTM). The outputs of these BiLSTMs are concatenated and passed to a softmax layer. This latter architecture has been proven to be effective for tasks that model two inputs such as textual entailment (Conneau et al., 2017) or sarcasm detection based on conversation context (Ghosh et al., 2017; Ghosh and Veale, 2017). 87 ID 1 Statement We have the highest tax rate anywhere in the world. 2 “Says Rick Scott cut education to pay for even more tax breaks for big, powerful, well-connected corporations.” 3 Says Donald Trump has given more money to Democratic candidates than Republican candidates. 4 Says out-of-state abortion clinics have marketed their services to minors in states with parental consent laws. 5 Obamacare provision will allow forced home inspections by government agents. In the month of January, Canada"
W18-5513,D17-1050,0,0.0141597,"e experiment with two architectures: in the first one we just concatenate the justification to the statement and pass it to a single BiLSTM, and in the second one we use a dual/parallel architecture where one BiLSTM reads the statement and another one reads the justification (architecture denoted as P-BiLSTM). The outputs of these BiLSTMs are concatenated and passed to a softmax layer. This latter architecture has been proven to be effective for tasks that model two inputs such as textual entailment (Conneau et al., 2017) or sarcasm detection based on conversation context (Ghosh et al., 2017; Ghosh and Veale, 2017). 87 ID 1 Statement We have the highest tax rate anywhere in the world. 2 “Says Rick Scott cut education to pay for even more tax breaks for big, powerful, well-connected corporations.” 3 Says Donald Trump has given more money to Democratic candidates than Republican candidates. 4 Says out-of-state abortion clinics have marketed their services to minors in states with parental consent laws. 5 Obamacare provision will allow forced home inspections by government agents. In the month of January, Canada created more new jobs than we did. 6 7 There has been $5 trillion in debt added over the last f"
W18-5513,W17-5523,1,0.838859,"d S+ MJ conditions we experiment with two architectures: in the first one we just concatenate the justification to the statement and pass it to a single BiLSTM, and in the second one we use a dual/parallel architecture where one BiLSTM reads the statement and another one reads the justification (architecture denoted as P-BiLSTM). The outputs of these BiLSTMs are concatenated and passed to a softmax layer. This latter architecture has been proven to be effective for tasks that model two inputs such as textual entailment (Conneau et al., 2017) or sarcasm detection based on conversation context (Ghosh et al., 2017; Ghosh and Veale, 2017). 87 ID 1 Statement We have the highest tax rate anywhere in the world. 2 “Says Rick Scott cut education to pay for even more tax breaks for big, powerful, well-connected corporations.” 3 Says Donald Trump has given more money to Democratic candidates than Republican candidates. 4 Says out-of-state abortion clinics have marketed their services to minors in states with parental consent laws. 5 Obamacare provision will allow forced home inspections by government agents. In the month of January, Canada created more new jobs than we did. 6 7 There has been $5 trillion in de"
W18-5513,I17-2043,0,0.177663,"umbia.edu sdp2137@columbia.edu Abstract a survey of current datasets and models for factchecking (e.g., (Wang, 2017; Rashkin et al., 2017; Vlachos and Riedel, 2014; Thorne et al., 2018; Long et al., 2017; Potthast et al., 2018; Wang et al., 2018)). Wang (2017) has introduced a large dataset (LIAR) of claims from P OLITI FACT, the associated metadata for each claim and the verdict (6 class labels). Most work on the LIAR dataset has focused on modeling the content of the claim (including hedging, sentiment and emotion analysis) and the speaker-related metadata (Wang, 2017; Rashkin et al., 2017; Long et al., 2017). However, these approaches do not use the evidence and the justification provided by humans to predict the label. Extracting evidence from (trusted) sources for fact-checking or for argument mining is a difficult task (Rinott et al., 2015; Thorne et al., 2018; Baly et al., 2018). For the purpose of our paper, we rely on the fact-checking article associated with the claim. We extend the original LIAR dataset by automatically extracting the justification given by humans for labeling the claim, from the fact-checking article (Section 2). We release the extended LIAR dataset (LIARPLUS) to the com"
W18-5513,W10-0204,0,0.045945,"and Support Vector Machines (SVM) with linear kernel. For the basic representation of the claim/statement (S condition) we experimented with unigram features, tf-idf weighted unigram features and Glove word embeddings (Pennington et al., 2014). The best representation proved to be unigrams. For the enhanced statement representation (S+ ) we modeled: sentiment strength using SentiStrength, which measures the negativity and positivity of a statement on a scale of 1-to-5 (Thelwall et al., 2010); emotion using the NRC Emotion Lexicon (EmoLex), which associates each word with eight basic emotions (Mohammad and Turney, 2010), and the Linguistic Inquiry and Word Count (LIWC) lexicon (Pennebaker et al., 2001). In addition, we include metadata information such as the number of claims each speaker makes for every truth-label (history) (Wang, 2017; Long et al., 2017). Finally for representing the justification in the SJ and S+ MJ conditions, we just use unigram features. Methods The main goal of our paper is to show that modeling the human-provided justification — which can be seen as a summary evidence — improves the assessment of a claim’s truth when compared to modeling the claim (and metadata) alone, regardless of"
W18-5513,D14-1162,0,0.0809693,"such as hedging, sentiment strength and emotion (Rashkin et al., 2017) as well as metadata information (S+ M condition), basic claim/statement and the associated extracted justification (SJ condition) and finally enhanced claim/statement representation, metadata and justification (S+ MJ condition). Feature-based Machine Learning. We experiment with both Logistic Regression (LR) and Support Vector Machines (SVM) with linear kernel. For the basic representation of the claim/statement (S condition) we experimented with unigram features, tf-idf weighted unigram features and Glove word embeddings (Pennington et al., 2014). The best representation proved to be unigrams. For the enhanced statement representation (S+ ) we modeled: sentiment strength using SentiStrength, which measures the negativity and positivity of a statement on a scale of 1-to-5 (Thelwall et al., 2010); emotion using the NRC Emotion Lexicon (EmoLex), which associates each word with eight basic emotions (Mohammad and Turney, 2010), and the Linguistic Inquiry and Word Count (LIWC) lexicon (Pennebaker et al., 2001). In addition, we include metadata information such as the number of claims each speaker makes for every truth-label (history) (Wang,"
W18-5513,P18-1022,0,0.0378186,"Missing"
W18-5513,D17-1317,0,0.180354,"niversity tariq@cs.columbia.edu sdp2137@columbia.edu Abstract a survey of current datasets and models for factchecking (e.g., (Wang, 2017; Rashkin et al., 2017; Vlachos and Riedel, 2014; Thorne et al., 2018; Long et al., 2017; Potthast et al., 2018; Wang et al., 2018)). Wang (2017) has introduced a large dataset (LIAR) of claims from P OLITI FACT, the associated metadata for each claim and the verdict (6 class labels). Most work on the LIAR dataset has focused on modeling the content of the claim (including hedging, sentiment and emotion analysis) and the speaker-related metadata (Wang, 2017; Rashkin et al., 2017; Long et al., 2017). However, these approaches do not use the evidence and the justification provided by humans to predict the label. Extracting evidence from (trusted) sources for fact-checking or for argument mining is a difficult task (Rinott et al., 2015; Thorne et al., 2018; Baly et al., 2018). For the purpose of our paper, we rely on the fact-checking article associated with the claim. We extend the original LIAR dataset by automatically extracting the justification given by humans for labeling the claim, from the fact-checking article (Section 2). We release the extended LIAR dataset ("
W18-5513,D15-1050,0,0.0297105,"., 2018)). Wang (2017) has introduced a large dataset (LIAR) of claims from P OLITI FACT, the associated metadata for each claim and the verdict (6 class labels). Most work on the LIAR dataset has focused on modeling the content of the claim (including hedging, sentiment and emotion analysis) and the speaker-related metadata (Wang, 2017; Rashkin et al., 2017; Long et al., 2017). However, these approaches do not use the evidence and the justification provided by humans to predict the label. Extracting evidence from (trusted) sources for fact-checking or for argument mining is a difficult task (Rinott et al., 2015; Thorne et al., 2018; Baly et al., 2018). For the purpose of our paper, we rely on the fact-checking article associated with the claim. We extend the original LIAR dataset by automatically extracting the justification given by humans for labeling the claim, from the fact-checking article (Section 2). We release the extended LIAR dataset (LIARPLUS) to the community1 . The main contribution of this paper is to show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based"
W18-5513,C18-1283,0,0.0589408,"he veracity of claims. It requires identifying evidence from trusted sources, understanding the context, and reasoning about what can be inferred from the evidence. Several organizations such as FACT C HECK.org, P OLITI FACT.com and F ULL FACT.org are devoted to such activities, and the final verdict can reflect varying degrees of truth (e.g., POLITIFACT labels claims as true, mostly true, half true, mostly false, false and pants on fire). Until recently, the bottleneck for developing automatic methods for fact-checking has been the lack of large datasets for building machine learning models. Thorne and Vlachos (2018) provide 1 https://github.com/Tariq60/LIAR-PLUS 85 Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 85–90 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Statement:“Says Rick Scott cut education to pay for even more tax breaks for big, powerful, well-connected corporations.” Speaker: Florida Democratic Party Context: TV Ad Label: half-true Extracted Justification: A TV ad by the Florida Democratic Party says Scott ”cut education to pay for even more tax breaks for big, powerful, well-connected corporations.” However, the"
W18-5513,N18-1074,0,0.123749,") has introduced a large dataset (LIAR) of claims from P OLITI FACT, the associated metadata for each claim and the verdict (6 class labels). Most work on the LIAR dataset has focused on modeling the content of the claim (including hedging, sentiment and emotion analysis) and the speaker-related metadata (Wang, 2017; Rashkin et al., 2017; Long et al., 2017). However, these approaches do not use the evidence and the justification provided by humans to predict the label. Extracting evidence from (trusted) sources for fact-checking or for argument mining is a difficult task (Rinott et al., 2015; Thorne et al., 2018; Baly et al., 2018). For the purpose of our paper, we rely on the fact-checking article associated with the claim. We extend the original LIAR dataset by automatically extracting the justification given by humans for labeling the claim, from the fact-checking article (Section 2). We release the extended LIAR dataset (LIARPLUS) to the community1 . The main contribution of this paper is to show that modeling the extracted justification in conjunction with the claim (and metadata) provides a significant improvement regardless of the machine learning model used (feature-based or deep learning) bo"
W18-5513,W14-2508,0,0.16194,"Missing"
W18-5513,P17-2067,0,0.374299,"e Columbia University tariq@cs.columbia.edu sdp2137@columbia.edu Abstract a survey of current datasets and models for factchecking (e.g., (Wang, 2017; Rashkin et al., 2017; Vlachos and Riedel, 2014; Thorne et al., 2018; Long et al., 2017; Potthast et al., 2018; Wang et al., 2018)). Wang (2017) has introduced a large dataset (LIAR) of claims from P OLITI FACT, the associated metadata for each claim and the verdict (6 class labels). Most work on the LIAR dataset has focused on modeling the content of the claim (including hedging, sentiment and emotion analysis) and the speaker-related metadata (Wang, 2017; Rashkin et al., 2017; Long et al., 2017). However, these approaches do not use the evidence and the justification provided by humans to predict the label. Extracting evidence from (trusted) sources for fact-checking or for argument mining is a difficult task (Rinott et al., 2015; Thorne et al., 2018; Baly et al., 2018). For the purpose of our paper, we rely on the fact-checking article associated with the claim. We extend the original LIAR dataset by automatically extracting the justification given by humans for labeling the claim, from the fact-checking article (Section 2). We release the e"
W18-5521,D17-1070,0,0.0485872,"Missing"
W18-5521,D16-1244,0,0.0446197,"m and issue a query and collect the top 2 results. • Named Entity Recognition: Second, we use the AllenNLP (Gardner et al., 2017) pretrained bidirectional language model (Peters et al., 2017) for named entity recognition 2 . After finding the named entities in the claim, we use Wikipedia python API 3 to collect the top wikipedia document returned by the API for each named entity. • Textual Entailment: For the entailment task, training is done using labeled claims paired with evidence (labels are SUPPORTS, REFUTES , NOT ENOUGH INFO ). Thorne et al. (2018) used the decomposable attention model (Parikh et al., 2016) for this task. For the case where multiple sentences are required as evidence, the strings were concatenated. • Dependency Parse: Third, to increase the chance of detecting relevant entities in the claim, we find the first lower case verb phrase (VP) in the dependency parse tree and query the Wikipedia API with all the tokens before the VP. The reason for emphasizing lower case verb phrase is to avoid missing entities in claims such as “Finding Dory was directed by X”, where the relevant entity is “Finding Dory”. Our system implements changes in all three modules (Section 2), which leads to s"
W18-5521,P17-1161,0,0.014456,"tify the candidate evidence sentences. Thorne et al. (2018) used a modified document retrieval component of 127 Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 127–131 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning. wikipedia to the claim and issue a query and collect the top 2 results. • Named Entity Recognition: Second, we use the AllenNLP (Gardner et al., 2017) pretrained bidirectional language model (Peters et al., 2017) for named entity recognition 2 . After finding the named entities in the claim, we use Wikipedia python API 3 to collect the top wikipedia document returned by the API for each named entity. • Textual Entailment: For the entailment task, training is done using labeled claims paired with evidence (labels are SUPPORTS, REFUTES , NOT ENOUGH INFO ). Thorne et al. (2018) used the decomposable attention model (Parikh et al., 2016) for this task. For the case where multiple sentences are required as evidence, the strings were concatenated. • Dependency Parse: Third, to increase the chance of detecti"
W18-5521,N18-1074,0,0.383962,"hether the claim is true, false, or mixed. Several organizations such as FactCheck.org and PolitiFact are devoted to such activities. The FEVER Shared task aims to evaluate the ability of a system to verify information using evidence from Wikipedia. Given a claim involving one or more entities (mapping to Wikipedia pages), the system must extract textual evidence (sets of sentences from Wikipedia pages) that supports or refutes the claim and then using this evidence, it must label the claim as Supported, Refuted or NotEnoughInfo. The dataset for the shared task was introduced by Thorne et al. (2018) and consists of 185,445 claims. Table 1 shows three instances from the data set with the claim, the evidence and the verdict. Table 1: Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset (Thorne et al., 2018) The baseline system described by Thorne et al. (2018) uses 3 major components: • Document Retrieval: Given a claim, identify relevant documents from Wikipedia which contain the evidence to verify the claim. Thorne et al. (2018) used the document retrieval component from the DrQA system (Chen et al., 2017), which returns the k nearest do"
W18-5521,Q17-1010,0,0.00698535,". Formally, for a sequence of T words wt“1,..,T , the BiLSTM layer generates a sequence of ht vectors, where ht is the concatenation of a forward and a backward LSTM output. The hidden vectors ht are then converted into a single vector using max-pooling, which chooses the maximum value over each dimension of the hidden units. Overall, the text encoder can be treated as an operator Text Ñ Rd that provides d dimensional encoding for a given text. Out of vocabulary issues in pre-trained word embeddings are a major bottleneck for sentence representations. To solve this we use fastText embeddings (Bojanowski et al., 2017) which rely on subword information. Also, these embeddings were trained on Wikipedia corpus making them an ideal choice for this task. As shown in Figure 1, the shared sentence encoder outputs a representation for the claim u and the evidence v. Once the sentence vectors are generated, the following three methods are applied to extract relations between the claim and the evidence: (i) concatenation of the two representations (u, v); (ii) element-wise product u*v and (iii) absolute element-wise difference |u ´ v|. The resulting vector, which captures information from both the claim and the evid"
W18-5521,P17-1171,0,0.0411196,"shared task was introduced by Thorne et al. (2018) and consists of 185,445 claims. Table 1 shows three instances from the data set with the claim, the evidence and the verdict. Table 1: Examples of claims, the extracted evidence from Wikipedia and the verdicts from the shared task dataset (Thorne et al., 2018) The baseline system described by Thorne et al. (2018) uses 3 major components: • Document Retrieval: Given a claim, identify relevant documents from Wikipedia which contain the evidence to verify the claim. Thorne et al. (2018) used the document retrieval component from the DrQA system (Chen et al., 2017), which returns the k nearest documents for a query using cosine similarity between binned unigram and bigram TF-IDF vectors. • Sentence Selection: Given the set of retrieved document, identify the candidate evidence sentences. Thorne et al. (2018) used a modified document retrieval component of 127 Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 127–131 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics DrQA (Chen et al., 2017) to select the top most similar sentences w.r.t the claim, using bigram TF-IDF with binning. wik"
W18-5808,C10-1092,0,0.0347838,"he six languages. We propose two language-independent classifiers that enable the selection of the optimal or nearly-optimal configuration for the morphological segmentation of unseen languages. 1 Introduction As natural language processing becomes more interested in many languages, including lowresource languages, unsupervised morphological segmentation remains an important area of study. For most of the languages of the world, we do not have morphologically annotated resources. However, many human language technologies profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). In this paper, we build on previous work on unsupervised morphological segmentation using Adaptor Grammars (AGs) (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016), a type of nonparametric Bayesian models that generalize probabilistic context-free grammars (PCFGs) (Johnson et al., 2007), where the PCFG is typically a morphological grammar that spec2 Problem Definition and Dataset Adaptor Grammars (AGs) have been used successfully for unsupervised morphological seg78 Proceedings of the 15th SIGMORPHON Worksh"
W18-5808,N09-1024,0,0.182091,"PrStSu+SM configuration is 81 Grammar Morph+SM Simple Simple+SM PrStSu PrStSu+SM PrStSu+Co+SM PrStSu2a+SM PrStSu2b+SM PrStSu2b+Co+SM Standard 0.647 0.651 0.680 0.642 0.701 0.648 0.676 0.682 0.532 Cascaded 0.642 0.593 0.631 0.646 0.692 0.628 0.682 0.688 0.532 proach to select the best language-independent model for each language. In addition to the use of AGs, several models have been successfully used for unsupervised morphological segmentation such as generative probabilistic models (utilized by Morfessor (Creutz and Lagus, 2007)), and log-linear models using contextual and global features (Poon et al., 2009). Narasimhan et al. (2015) use a discriminative model for unsupervised morphological segmentation that integrates orthographic and semantic properties of words. The model learns morphological chains, where a chain extends a base form available in the lexicon. Another recent notable work is introduced by Wang et al. (2016), who use neural networks for unsupervised segmentation, where they build LSTM (Hochreiter and Schmidhuber, 1997) architectures to learn word structures in order to predict morphological boundaries. Another variation of the this approach is presented by Yang et al. (2017), whe"
W18-5808,C16-1086,1,0.71873,"he research proposed by Eskander et al. (2016), who investigate a large space of parameters when using Adaptor Grammars related to (i) the underlying context-free grammar and (ii) the use of a “Cascaded” system in which one grammar chooses affixes to be seeded into another in order to simulate the situation where scholar-knowledge is available. Their results on a development set of 6 languages (English, German, Finish, Turkish, Estonian and Zulu) show that the best performing AG-based configuration (grammar and learning setup) differ from language to language. For processing unseen languages, Eskander et al. (2016) proposed the Language-Independent Morphological Segmenter (LIMS) based on the best-on-average performing configuration when running leaveone-out cross validation on the development languages. However, while LIMS works best on average and has been shown to outperform other stateof-the-art unsupervised morphological segmentation systems (Eskander et al., 2016), it is not the optimal configuration for any of the development languages except Zulu. Thus, in this paper we propose an approach to automatically select the optimal or nearly-optimal languageindependent configuration for the morphologica"
W18-5808,Q13-1021,0,0.67021,"language processing becomes more interested in many languages, including lowresource languages, unsupervised morphological segmentation remains an important area of study. For most of the languages of the world, we do not have morphologically annotated resources. However, many human language technologies profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). In this paper, we build on previous work on unsupervised morphological segmentation using Adaptor Grammars (AGs) (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016), a type of nonparametric Bayesian models that generalize probabilistic context-free grammars (PCFGs) (Johnson et al., 2007), where the PCFG is typically a morphological grammar that spec2 Problem Definition and Dataset Adaptor Grammars (AGs) have been used successfully for unsupervised morphological seg78 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 78–83 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https://doi.org/10.18653/v1/P17"
W18-5808,W08-0704,0,0.698623,"ion As natural language processing becomes more interested in many languages, including lowresource languages, unsupervised morphological segmentation remains an important area of study. For most of the languages of the world, we do not have morphologically annotated resources. However, many human language technologies profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). In this paper, we build on previous work on unsupervised morphological segmentation using Adaptor Grammars (AGs) (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016), a type of nonparametric Bayesian models that generalize probabilistic context-free grammars (PCFGs) (Johnson et al., 2007), where the PCFG is typically a morphological grammar that spec2 Problem Definition and Dataset Adaptor Grammars (AGs) have been used successfully for unsupervised morphological seg78 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 78–83 c Brussels, Belgium, October 31, 2018. 2018 The Special Interest Group on Computational Morphology and Phonology https"
W18-5808,C10-1116,0,0.0629627,"Missing"
W18-5808,C10-1115,0,0.0807782,"Missing"
W18-5808,Q15-1012,0,0.051137,"u+SM vs. PrStSu2a+SM) classification tasks. We conduct the two classification tasks separately, and then we combine the outcome to obtain the best configuration. In the training phase, we perform leave-oneout cross validation on the six development languages. In each of the six folds of the cross validation, we choose one language in turn as the test language. We use the training and deComparison with existing unsupervised approaches. Table 7 compares the performance of the selected configurations of our system (Table 5) to three other systems; Morfessor (Creutz and Lagus, 2007), MorphoChain (Narasimhan et al., 2015) and LIMS (Eskander et al., 2016) (where the cascaded PrStSu+SM configuration is 81 Grammar Morph+SM Simple Simple+SM PrStSu PrStSu+SM PrStSu+Co+SM PrStSu2a+SM PrStSu2b+SM PrStSu2b+Co+SM Standard 0.647 0.651 0.680 0.642 0.701 0.648 0.676 0.682 0.532 Cascaded 0.642 0.593 0.631 0.646 0.692 0.628 0.682 0.688 0.532 proach to select the best language-independent model for each language. In addition to the use of AGs, several models have been successfully used for unsupervised morphological segmentation such as generative probabilistic models (utilized by Morfessor (Creutz and Lagus, 2007)), and log"
W18-5808,D14-1095,0,0.0470118,"that enable the selection of the optimal or nearly-optimal configuration for the morphological segmentation of unseen languages. 1 Introduction As natural language processing becomes more interested in many languages, including lowresource languages, unsupervised morphological segmentation remains an important area of study. For most of the languages of the world, we do not have morphologically annotated resources. However, many human language technologies profit from morphological segmentation, for example machine translation (Nguyen et al., 2010; Ataman et al., 2017) and speech recognition (Narasimhan et al., 2014). In this paper, we build on previous work on unsupervised morphological segmentation using Adaptor Grammars (AGs) (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016), a type of nonparametric Bayesian models that generalize probabilistic context-free grammars (PCFGs) (Johnson et al., 2007), where the PCFG is typically a morphological grammar that spec2 Problem Definition and Dataset Adaptor Grammars (AGs) have been used successfully for unsupervised morphological seg78 Proceedings of the 15th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, p"
W18-5808,P17-1078,0,0.0264509,"res (Poon et al., 2009). Narasimhan et al. (2015) use a discriminative model for unsupervised morphological segmentation that integrates orthographic and semantic properties of words. The model learns morphological chains, where a chain extends a base form available in the lexicon. Another recent notable work is introduced by Wang et al. (2016), who use neural networks for unsupervised segmentation, where they build LSTM (Hochreiter and Schmidhuber, 1997) architectures to learn word structures in order to predict morphological boundaries. Another variation of the this approach is presented by Yang et al. (2017), where they use partial-word information as character bigram embeddings and evaluate their work on Chinese. Table 6: Adaptor-grammar results (Emma F-scores) for the Standard and Cascaded setups for Arabic. Boldface indicates the best configuration and the choice of our system. Grammar Morfessor MorphoChain LIMS Ours Best English 0.805 0.746 0.809 0.821 0.826 German 0.740 0.625 0.777 0.790 0.790 Finnish 0.675 0.621 0.727 0.733 0.733 Turkish 0.551 0.551 0.591 0.647 0.647 Zulu 0.414 0.390 0.611 0.611 0.611 Estonian 0.779 0.679 0.805 0.828 0.847 Arabic 0.779 0.751 0.682 0.701 0.701 Avg. 0.678 0.6"
W19-3508,W17-3006,0,0.475517,"fortunately, we often see that users resort to verbal abuse to win an argument or overshadow someone’s opinion. Natural Language Processing (NLP) could aid in the process of detecting and flagging abusive language and thus signaling abusive behaviour online. This is a particularly challenging task due to the noisiness of user-generated text and the diverse types of abusive language ranging from racism, sexism, and hate speech to harassment and personal attacks (Zeerak et al., 2017; Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Djuric et al., 2015; Badjatiya et al., 2017; Park and Fung, 2017; Pavlopoulos et al., 2017). Zeerak et al. (2017) point out that different types of abusive language can be reduced to two primary factors: • Conduct an empirical study to deepen our understanding of current datasets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language and personal attacks). Show that our stacked Bidirectional Long Short Term Memory architecture with contextual attention is comparable to or out70 Proceedings of the Third Workshop on Abusive Language Online, pages 70–79 c Florence, Italy, August 1, 2"
W19-3508,S17-2126,0,0.0339524,"The attention mechanism assigns a weight to each word annotation that is obtained from the BiLSTM layer. We compute the fixed representation v of the whole message as a weighted sum of all the word annotations, which is then fed to a final 73 fully-connected Softmax layer to obtain the class probabilities. 4.1 D1 D2 D3 D4 Implementation Details We pre-process the text using Ekphrasis 1 — a text processing tool built specially for social media platforms such as Twitter. For the Twitter datasets we experimented with word vectors that are initialized with pre-trained Twitter-specific embeddings (Baziotis et al., 2017), as well as ELMo embeddings (Peters et al., 2018), which are deep contextualized word representations modeling both complex characteristics of word use (e.g., syntax and semantics), and usage across various linguistic contexts. For the Wikipedia Attacks dataset we relied on both fastText embeddings (Bojanowski et al., 2017) and ELMo embeddings. Out of vocabulary issues in pre-trained word embeddings are a major limitation for sentence representations. To solve this, we use fastText embeddings (Bojanowski et al., 2017), which rely on subword information. Also, these embeddings were trained on"
W19-3508,D17-1117,0,0.623892,"see that users resort to verbal abuse to win an argument or overshadow someone’s opinion. Natural Language Processing (NLP) could aid in the process of detecting and flagging abusive language and thus signaling abusive behaviour online. This is a particularly challenging task due to the noisiness of user-generated text and the diverse types of abusive language ranging from racism, sexism, and hate speech to harassment and personal attacks (Zeerak et al., 2017; Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Djuric et al., 2015; Badjatiya et al., 2017; Park and Fung, 2017; Pavlopoulos et al., 2017). Zeerak et al. (2017) point out that different types of abusive language can be reduced to two primary factors: • Conduct an empirical study to deepen our understanding of current datasets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language and personal attacks). Show that our stacked Bidirectional Long Short Term Memory architecture with contextual attention is comparable to or out70 Proceedings of the Third Workshop on Abusive Language Online, pages 70–79 c Florence, Italy, August 1, 2019. 2019 Association for C"
W19-3508,N18-1202,0,0.324996,"d annotation that is obtained from the BiLSTM layer. We compute the fixed representation v of the whole message as a weighted sum of all the word annotations, which is then fed to a final 73 fully-connected Softmax layer to obtain the class probabilities. 4.1 D1 D2 D3 D4 Implementation Details We pre-process the text using Ekphrasis 1 — a text processing tool built specially for social media platforms such as Twitter. For the Twitter datasets we experimented with word vectors that are initialized with pre-trained Twitter-specific embeddings (Baziotis et al., 2017), as well as ELMo embeddings (Peters et al., 2018), which are deep contextualized word representations modeling both complex characteristics of word use (e.g., syntax and semantics), and usage across various linguistic contexts. For the Wikipedia Attacks dataset we relied on both fastText embeddings (Bojanowski et al., 2017) and ELMo embeddings. Out of vocabulary issues in pre-trained word embeddings are a major limitation for sentence representations. To solve this, we use fastText embeddings (Bojanowski et al., 2017), which rely on subword information. Also, these embeddings were trained on Wikipedia. The embedding dimension of the words in"
W19-3508,Q17-1010,0,0.0293641,"Implementation Details We pre-process the text using Ekphrasis 1 — a text processing tool built specially for social media platforms such as Twitter. For the Twitter datasets we experimented with word vectors that are initialized with pre-trained Twitter-specific embeddings (Baziotis et al., 2017), as well as ELMo embeddings (Peters et al., 2018), which are deep contextualized word representations modeling both complex characteristics of word use (e.g., syntax and semantics), and usage across various linguistic contexts. For the Wikipedia Attacks dataset we relied on both fastText embeddings (Bojanowski et al., 2017) and ELMo embeddings. Out of vocabulary issues in pre-trained word embeddings are a major limitation for sentence representations. To solve this, we use fastText embeddings (Bojanowski et al., 2017), which rely on subword information. Also, these embeddings were trained on Wikipedia. The embedding dimension of the words in our model for pre-trained Twitter embeddings and fastText embedding is set to 300, while for ELMo its set to 1024. We use a dropout rate of 0.25 and train the network using a learning rate of 0.001 for 10 epochs. The results are reported by averaging over 10fold cross-valida"
W19-3508,N16-2013,0,0.133446,"to investigate what type of attention mechanism (contextual vs. self-attention) is better for abusive language detection using deep learning architectures; and 3) to investigate whether stacked architectures provide an advantage over simple architectures for this task. 2. Obama is kinder to islam than any other future western leader is likely to be you can not even imagine how i think because i cannot imagine how anyone would take such a vile religion as islam Table 1: Tweets where the word “islam” is used in two separate contexts: the top tweet is labeled as None while the bottom as Racism (Waseem and Hovy, 2016). • Is the language directed towards a specific individual or entity or is it directed towards a generalized group? • Is the abusive content explicit or implicit? Table 1 shows two examples of tweets from the first large-scale Twitter abusive language detection dataset, where the second tweet expresses racism, while the first one does not (Waseem and Hovy, 2016). The usage of words in a particular context is important in determining the author’s intended meaning. For example, the contexts of the word “islam” in the two tweets in Table 1 are different (a non-racist vs. a racist use of the word,"
W19-3508,N16-1174,0,0.28007,"Fung (2017) use a Hybrid Convolution Neural Network (CNN) with the intuition that character level input would counter the purposely or mistakenly misspelled words and made-up vocabularies. Finally, Pavlopoulos et al. (2017) exploit deep learning methods with attention for abuse detection, where they use a self-attention model to detect abuse in news portals and Wikipedia. In this paper, we present an empirical study that investigates what type of attention mechanism (contextual vs. self-attention) is better for this task and whether stacked architectures are better than simple architectures. Yang et al. (2016) introduced a hierarchical contextual attention in a GRU architecture for document classification. The attention in this hierarchical model is both at the word and sentence level. For our study we use contextual attention only at word level because our Twitter datasets contains mostly single sentence tweets. Unlike Yang et al. (2016), we use a stacked Bidirectional Long-Short Term Memory (Bi-LSTM) network, and show that it is superior to using a single BiLSTM network. performs state of the art approaches on all the existing datasets. • Investigate what type of attention mechanism in deep learn"
W19-3508,W17-5523,1,0.889518,"Missing"
W19-3508,W17-3012,0,0.0364781,"ther in online forums, comment sections or micro-blogging platforms such as Twitter often involves an exchange of ideas or beliefs. Unfortunately, we often see that users resort to verbal abuse to win an argument or overshadow someone’s opinion. Natural Language Processing (NLP) could aid in the process of detecting and flagging abusive language and thus signaling abusive behaviour online. This is a particularly challenging task due to the noisiness of user-generated text and the diverse types of abusive language ranging from racism, sexism, and hate speech to harassment and personal attacks (Zeerak et al., 2017; Waseem and Hovy, 2016; Golbeck et al., 2017; Davidson et al., 2017; Djuric et al., 2015; Badjatiya et al., 2017; Park and Fung, 2017; Pavlopoulos et al., 2017). Zeerak et al. (2017) point out that different types of abusive language can be reduced to two primary factors: • Conduct an empirical study to deepen our understanding of current datasets that focus on different types of abusive language, which are sometimes overlapping (racism, sexism, hate speech, offensive language and personal attacks). Show that our stacked Bidirectional Long Short Term Memory architecture with contextual attent"
W19-4222,W18-4801,1,0.740057,"-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages. 1 Smaranda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approac"
W19-4222,W18-4803,0,0.0390868,"to supervised methods, outperforming them on two of the four languages. 1 Smaranda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson 2 Languages and Da"
W19-4222,C18-1006,0,0.229421,"anguages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages. 1 Smaranda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsu"
W19-4222,W18-4808,0,0.136534,"tric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016, 2018). Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by Kann et al. (2018) in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016); 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from Kann et al. (2018); 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods — M orf essor (Creutz and Lagus, 2007) and M orphoChain (Narasimhan et"
W19-4222,R11-1079,0,0.026377,"nt with four languages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages. 1 Smaranda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages"
W19-4222,W08-0704,0,0.440282,"source Polysynthetic Languages Ramy Eskander Columbia University Dept. of Computer Science rnd2110@columbia.edu Judith L. Klavans University of Maryland UMIACS jklavans@umd.edu Abstract et al., 2007). We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (Kann et al., 2018). Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016, 2018). Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by Kann et al. (2018) in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016); 2) optimize for language using just the small training vocabulary (unsegmented) and d"
W19-4222,W17-0114,0,0.0209778,"tional morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson 2 Languages and Datasets Typically, polysynthetic languages demonstrate holophrasis, i.e. the ability of an entire sentence to be expressed as what is considered by native speakers to be jus"
W19-4222,W18-4802,0,0.0212637,"anda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson 2 Languages and Datasets Typically, polysynthetic languages demonstrate holophrasis, i.e. the ability of"
W19-4222,N18-1005,0,0.0817705,"Missing"
W19-4222,Q15-1012,0,0.298007,"et al. (2018) in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016); 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) from Kann et al. (2018); 3) approximate the effect of having some linguistic knowledge; 4) learn from all languages at once and 5) add additional unsupervised data for NH and WX (Section 3). We show that the AG-based approaches outperform other unsupervised methods — M orf essor (Creutz and Lagus, 2007) and M orphoChain (Narasimhan et al., 2015)) —, and that for two of the languages (NH and YN), the best AG-based approaches outperform the best supervised methods (Section 4). Polysynthetic languages pose a challenge for morphological analysis due to the rootmorpheme complexity and to the word class “squish”. In addition, many of these polysynthetic languages are low-resource. We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) (Eskander et al., 2016). We experiment with four languages from the Uto-Aztecan family. Our AG-based approaches outperform oth"
W19-4222,W17-0102,0,0.0306569,"ethods, outperforming them on two of the four languages. 1 Smaranda Muresan Columbia University Data Science Institute smara@columbia.edu Introduction Computational morphology of polysynthetic languages is an emerging field of research. Polysynthetic languages pose unique challenges for computational approaches, including machine translation and morphological analysis, due to the rootmorpheme complexity and to word class gradations (Homola, 2011; Mager et al., 2018d; Klavans, 2018a). Previous approaches include rule-based methods based on finite state transducers (Farley, 2009; Littell, 2018; Kazeminejad et al., 2017), hybrid models (Mager et al., 2018b; Moeller et al., 2018), and supervised machine learning, particularly deep learning approaches (Micher, 2017; Kann et al., 2018). While each rule-based method is developed for a specific language (Inuktitut (Farley, 2009), or Arapaho (Littell, 2018; Moeller et al., 2018)), machine learning, including deep learning approaches, might be more rapidly scalable to many additional languages. We propose an unsupervised approach for morphological segmentation of polysynthetic languages based on Adaptor Grammars (Johnson 2 Languages and Datasets Typically, polysynth"
W19-4222,Q13-1021,0,0.122423,"hetic Languages Ramy Eskander Columbia University Dept. of Computer Science rnd2110@columbia.edu Judith L. Klavans University of Maryland UMIACS jklavans@umd.edu Abstract et al., 2007). We experiment with four UtoAztecan languages: Mexicanero (MX), Nahuatl (NH), Wixarika (WX) and Yorem Nokki (YN) (Kann et al., 2018). Adaptor Grammars (AGs) are nonparametric Bayesian models that generalize probabilistic context free grammars (PCFG), and have proven to be successful for unsupervised morphological segmentation, where a PCFG is a morphological grammar that specifies word structure (Johnson, 2008; Sirts and Goldwater, 2013; Eskander et al., 2016, 2018). Our main goal is to examine the success of Adaptor Grammars for unsupervised morphological segmentation when applied to polysynthetic languages, where the morphology is synthetically complex (not simply agglutinative), and where resources are minimal. We use the datasets introduced by Kann et al. (2018) in an unsupervised fashion (unsegmented words). We design several AG learning setups: 1) use the best-on-average AG setup from Eskander et al. (2016); 2) optimize for language using just the small training vocabulary (unsegmented) and dev vocabulary (segmented) f"
W19-4452,W15-0608,0,0.043466,"s (4 vs. 5) we noticed an interesting aspect: for the essays scored with 5 the ratio of argumentative sentences w.r.t total number of sentence was higher than for the essays with a 4 score, while the essays with a 4 scores tend to be longer. In general our correlations scores were much lower than the ones reported by Ghosh et al. (2016). There are several tive features in predicting the quality of argument scores (0-5) we use Logistic Regression (LR) learners and evaluate the learners using quadraticweighted kappa (QWK) against the human scores, a methodology generally used for essay scoring (Farra et al., 2015; Ghosh et al., 2016). QWK corrects for chance agreement between the system prediction and the human prediction, and it takes into account the extent of the disagreement between labels. Since the number of essays is very small we did a five-fold cross validation. Table 7 reports the performance for the three feature groups as well as their combination. The baseline feature (bl) is the number of sentences in the essay, since essay length has been shown to be generally highly correlated with essay scores (Chodorow and Burstein, 2004). As seen in Table 7 out of the individual features groups the"
W19-4452,P16-2089,1,0.774315,"answers to open-ended questions, and proves to be reliable, with a Pearson correlation between human graders and GRubric of 0.72. Recent work investigates fine-grained writing assessment, especially on content quality evaluation by diving into linguistic phenomena and structures, combined with various NLP techniques. Klebanov et al. (2014) investigated the correlations between essays scores with a content importance model. Another line of research has studied the role of argumentative features in predicting the overall essay quality (Ong et al., 2014; Song et al., 2014; Klebanov et al., 2016; Ghosh et al., 2016; Persing and Ng, 2015). For example, Klebanov et al. (2016) and Ghosh et al. (2016) examine the relations between argumentation structure features and the holistic essay quality (low, medium and high) applied to TOEFL essays. In this paper, we use the argumentative features introduced by Ghosh et al. (2016), but correlate them with the rubric related to the quality of the argument on a scale of 0-5. 3 1. Autonomous Vehicles: will these change how we travel today? 2. Cybercrime: will education and investment provide the solution? 3. Cryptocurrencies: are they the currencies of the future? For"
W19-4452,W16-2808,0,0.0764335,"s grade the short text answers to open-ended questions, and proves to be reliable, with a Pearson correlation between human graders and GRubric of 0.72. Recent work investigates fine-grained writing assessment, especially on content quality evaluation by diving into linguistic phenomena and structures, combined with various NLP techniques. Klebanov et al. (2014) investigated the correlations between essays scores with a content importance model. Another line of research has studied the role of argumentative features in predicting the overall essay quality (Ong et al., 2014; Song et al., 2014; Klebanov et al., 2016; Ghosh et al., 2016; Persing and Ng, 2015). For example, Klebanov et al. (2016) and Ghosh et al. (2016) examine the relations between argumentation structure features and the holistic essay quality (low, medium and high) applied to TOEFL essays. In this paper, we use the argumentative features introduced by Ghosh et al. (2016), but correlate them with the rubric related to the quality of the argument on a scale of 0-5. 3 1. Autonomous Vehicles: will these change how we travel today? 2. Cybercrime: will education and investment provide the solution? 3. Cryptocurrencies: are they the currencies"
W19-4452,D16-1035,0,0.0242251,"to DUCView. The annotator creates SCUs and exports the pyramid XML file (*.pyr). A pyramid file and a student summary are then the input for the annotator to match phrases in the student summary to pyramid SCUs, which is also exported as XML (*.pan). sive reliability measures in past work on pyramid annotation, we did not re-assess its reliability here. To annotate the content of the argument portion of the essay, we identify all distinct Elementary Discourse Units (EDU). Identifying (segmenting) EDUs from text and representing their meanings play a key role in discourse parsing (Marcu, 2000; Li et al., 2016; Braud et al., 2017). Definitions of EDUs vary, thus Prasad et al. (2008) consider the full range of clause types, including verb arguments and non-finite clauses. To simplify the annotation, we restrict EDUs to propositions derived from tensed clauses that are not verb arguments (such as that complements of verbs of belief). In (Gao et al., 2019), we report the iterative development of reliable annotation guidelines for untrained annotators.2 Annotators first identify the start and end of tensed clauses, omitting discourse connectives from the EDU spans, which can be discontinuous. Annotator"
W19-4452,W14-2104,0,0.053287,"Missing"
W19-4452,W17-5102,1,0.86098,"es has 2.07. Cryptocurrencies has shorter length of EDUs compared to Autonomous Vehicle, as 13.62 and 14.00. For the normalized SCU by total number of EDUs and number of matched EDUs, Autonomous Vehicle shows more with 0.08 and 0.84, while Cryptocurrencies has Topic CrypCurr AutoV Total SCUs 34 41 w=5 0 0 w=4 3 6 w=3 5 2 8 Annotation of Argument Structure To annotate the argumentative part of the essays, we used the coarse-grained argumentative structure proposed by Stab and Gurevych (2014): argument components (major claim, claim, premises) and argument relations (support/attack). Similar to Hidey et al. (2017), we took as annotation unit the proposition instead of the clause, given that premises are frequently propositions that conflate multiple clauses. For this pilot annotation task we labeled the 37 Cryptocurrency essays and used two expert annotators with background in linguistics and/or argumentation. We used Brat as annotation tool.4 The set contains 36 main claims, 559 claims, 277 premises, 560 support relations and 101 attack relations. Ghosh et al. (2016) proposed a set of argumentative features and showed that they correlate well with the holistic essay scores (low, medium and high) when"
W19-4452,P15-1053,0,0.019633,"d questions, and proves to be reliable, with a Pearson correlation between human graders and GRubric of 0.72. Recent work investigates fine-grained writing assessment, especially on content quality evaluation by diving into linguistic phenomena and structures, combined with various NLP techniques. Klebanov et al. (2014) investigated the correlations between essays scores with a content importance model. Another line of research has studied the role of argumentative features in predicting the overall essay quality (Ong et al., 2014; Song et al., 2014; Klebanov et al., 2016; Ghosh et al., 2016; Persing and Ng, 2015). For example, Klebanov et al. (2016) and Ghosh et al. (2016) examine the relations between argumentation structure features and the holistic essay quality (low, medium and high) applied to TOEFL essays. In this paper, we use the argumentative features introduced by Ghosh et al. (2016), but correlate them with the rubric related to the quality of the argument on a scale of 0-5. 3 1. Autonomous Vehicles: will these change how we travel today? 2. Cybercrime: will education and investment provide the solution? 3. Cryptocurrencies: are they the currencies of the future? For the second assignment ("
W19-4452,prasad-etal-2008-penn,0,0.0960809,"le (*.pyr). A pyramid file and a student summary are then the input for the annotator to match phrases in the student summary to pyramid SCUs, which is also exported as XML (*.pan). sive reliability measures in past work on pyramid annotation, we did not re-assess its reliability here. To annotate the content of the argument portion of the essay, we identify all distinct Elementary Discourse Units (EDU). Identifying (segmenting) EDUs from text and representing their meanings play a key role in discourse parsing (Marcu, 2000; Li et al., 2016; Braud et al., 2017). Definitions of EDUs vary, thus Prasad et al. (2008) consider the full range of clause types, including verb arguments and non-finite clauses. To simplify the annotation, we restrict EDUs to propositions derived from tensed clauses that are not verb arguments (such as that complements of verbs of belief). In (Gao et al., 2019), we report the iterative development of reliable annotation guidelines for untrained annotators.2 Annotators first identify the start and end of tensed clauses, omitting discourse connectives from the EDU spans, which can be discontinuous. Annotators then provide a paraphrase of the EDU span as an independent simple sente"
W19-4452,P14-2041,0,0.0311314,"ngful classroom interacˇ tions. Agejev and Snajder (2017) uses ROUGE and BLEU in assessing summary writing from college L2 students. Santamar´ıa Lancho et al. (2018) show that using G-Rubric, an LSA-based tool applying rubric assessment, helps the instructors grade the short text answers to open-ended questions, and proves to be reliable, with a Pearson correlation between human graders and GRubric of 0.72. Recent work investigates fine-grained writing assessment, especially on content quality evaluation by diving into linguistic phenomena and structures, combined with various NLP techniques. Klebanov et al. (2014) investigated the correlations between essays scores with a content importance model. Another line of research has studied the role of argumentative features in predicting the overall essay quality (Ong et al., 2014; Song et al., 2014; Klebanov et al., 2016; Ghosh et al., 2016; Persing and Ng, 2015). For example, Klebanov et al. (2016) and Ghosh et al. (2016) examine the relations between argumentation structure features and the holistic essay quality (low, medium and high) applied to TOEFL essays. In this paper, we use the argumentative features introduced by Ghosh et al. (2016), but correlat"
W19-4452,W14-2110,0,0.0271382,"elps the instructors grade the short text answers to open-ended questions, and proves to be reliable, with a Pearson correlation between human graders and GRubric of 0.72. Recent work investigates fine-grained writing assessment, especially on content quality evaluation by diving into linguistic phenomena and structures, combined with various NLP techniques. Klebanov et al. (2014) investigated the correlations between essays scores with a content importance model. Another line of research has studied the role of argumentative features in predicting the overall essay quality (Ong et al., 2014; Song et al., 2014; Klebanov et al., 2016; Ghosh et al., 2016; Persing and Ng, 2015). For example, Klebanov et al. (2016) and Ghosh et al. (2016) examine the relations between argumentation structure features and the holistic essay quality (low, medium and high) applied to TOEFL essays. In this paper, we use the argumentative features introduced by Ghosh et al. (2016), but correlate them with the rubric related to the quality of the argument on a scale of 0-5. 3 1. Autonomous Vehicles: will these change how we travel today? 2. Cybercrime: will education and investment provide the solution? 3. Cryptocurrencies:"
W19-4452,C14-1142,0,0.0172333,"0 for Autonomous Vehicle and 36.76 for Cryptocurrencies. Autonomous Vehicle has 2.75 as average weight of SCUs and Cryptocurrencies has 2.07. Cryptocurrencies has shorter length of EDUs compared to Autonomous Vehicle, as 13.62 and 14.00. For the normalized SCU by total number of EDUs and number of matched EDUs, Autonomous Vehicle shows more with 0.08 and 0.84, while Cryptocurrencies has Topic CrypCurr AutoV Total SCUs 34 41 w=5 0 0 w=4 3 6 w=3 5 2 8 Annotation of Argument Structure To annotate the argumentative part of the essays, we used the coarse-grained argumentative structure proposed by Stab and Gurevych (2014): argument components (major claim, claim, premises) and argument relations (support/attack). Similar to Hidey et al. (2017), we took as annotation unit the proposition instead of the clause, given that premises are frequently propositions that conflate multiple clauses. For this pilot annotation task we labeled the 37 Cryptocurrency essays and used two expert annotators with background in linguistics and/or argumentation. We used Brat as annotation tool.4 The set contains 36 main claims, 559 claims, 277 premises, 560 support relations and 101 attack relations. Ghosh et al. (2016) proposed a s"
Y12-1013,W11-0705,0,0.23689,"Negative 5691 2323 381 Table 2: Statistics of Data Sets 4 Linguistic-based Approach In this section, we present our baseline approach using only linguistic features. 4.1 Pre-processing We have applied the tool developed by Han and Baldwin (2011) together with the following additional steps to perform normalization for informal documents (tweets and forum posts). • Replace URLs with “@URL”. • Replace @username with “@USERNAME”. • Replace negation words with “NOT” based on the list derived from the LIWCLexicon (Pennebaker et al., 2001). • Normalize slang words (e.g. “LOL” to “laugh out loud”) (Agarwal et al., 2011). • Spelling correction using WordNet (Fellbaum, 2005) (e.g. “cooooool” to “cool”) In addition, each document has been tokenized and annotated with Part-of-speech tags (Toutanova et al., 2003). 129 4.2 Target and Issue Detection After pre-processing, the first step is to detect documents which include popular targets and issues. A popular target is an entity that users frequently discuss, such as a product (e.g. “Iphone4”), a person (e.g. “Ron Paul”) or an organization (e.g. “Red Cross”). A popular issue is a related aspect associated with a target, such as “display function” or “economic issu"
Y12-1013,baccianella-etal-2010-sentiwordnet,0,0.0354325,"pectively. 4.3 Sentiment Detection We have developed a supervised learning model based on Support Vector Machines to classify sentiment labels for each document (a post, a tweet message or a movie review document), incorporating several features such as N-grams, POS, various lexicons, punctuation, capitalization (see Table 3). Feature N-grams Part-of-Speech Gazetteer Word Cluster Punctuation Capitalization Description All unique unigrams, bigrams and trigrams Part-Of-Speech tags generated by Stanford Parser (Toutanova et al., 2003) Lexical matching based on (Joshi et al., 2011), SentiWordNet (Baccianella et al., 2010), Subjectivity Lexicon (Wiebe et al., 2004), Inquirer (Stone et al., 1966), Taboada (Taboada and Grieve, 2004), UICLexicon (Hu and Liu, 2004), LIWCLexicon (Pennebaker et al., 2001) Use synset information provided by Wordnet to expand the entries of each gazettteer; Lexical matching based on the expanded gazetteers Whether the document includes any exclamation mark or question mark Unique words which include all capitalized letters Table 3: Linguistic Features Used in the Baseline System The classification results are normalized to probability based confidence values via a sigmoid kernel functi"
Y12-1013,W10-2914,0,0.0388359,"low confidence values. 6 Remaining Challenges Although the proposed approach based on social cognitive theories has significantly enhanced the we would need to identify agreement/disagreement relations among posts. 0.75 Baseline +Hypothesis 1 +Hypothesis 1+2 +Hypothesis 1+2+3 Accuracy 0.70 0.65 0.60 0.55 0.5 0.6 0.7 0.8 Threshold Figure 4: Impact of Parameters performance of sentiment analysis, some challenges remain. We analyze the major sources of the remaining errors as follows. Sarcasm Detection. For both tweets and forum posts, some remaining errors require accurate detection of sarcasm (Davidov et al., 2010; GonzalezIbanez et al., 2011). For example, “LOL..remember Obama chastising business’s for going to Vegas. Vegas would have cost a third of what these locations costs. But hey, no big deal... ” contains sarcasm, which leads our system to misclassify this post. Domain-specific Latent Sentiments. The same word or phrase might indicate completely different sentiments in various domains. For example, “big” usually indicates positive sentiment, but it indicates negative sentiment in the following sentence: “tell me how the big government, big bank backing, war mongering Obama differs from Bush?”."
Y12-1013,P11-2102,1,0.839681,"Missing"
Y12-1013,P11-1038,0,0.0255817,"problem. We also used a more traditional set for sentiment analysis — the movie review polarity data set shared by (Pang et al., 2002) — to highlight the challenges of more informal texts. Table 2 summarizes the statistics of data sets used for each genre. All experiments in this paper are based on three-fold cross-validation. Genre Review Tweet Forum Positive 5691 2323 381 Negative 5691 2323 381 Table 2: Statistics of Data Sets 4 Linguistic-based Approach In this section, we present our baseline approach using only linguistic features. 4.1 Pre-processing We have applied the tool developed by Han and Baldwin (2011) together with the following additional steps to perform normalization for informal documents (tweets and forum posts). • Replace URLs with “@URL”. • Replace @username with “@USERNAME”. • Replace negation words with “NOT” based on the list derived from the LIWCLexicon (Pennebaker et al., 2001). • Normalize slang words (e.g. “LOL” to “laugh out loud”) (Agarwal et al., 2011). • Spelling correction using WordNet (Fellbaum, 2005) (e.g. “cooooool” to “cool”) In addition, each document has been tokenized and annotated with Part-of-speech tags (Toutanova et al., 2003). 129 4.2 Target and Issue Detect"
Y12-1013,D10-1121,0,0.0463535,"Missing"
Y12-1013,H05-1003,1,0.872191,"Missing"
Y12-1013,P11-1016,0,0.242892,"Missing"
Y12-1013,P11-4022,0,0.0663571,"d one another in an online discussion forum using a signed network representation of participant interaction. In contrast, we are interested in discovering the opinions of participants toward a public figure in light of their stance on various political issues. Sentiment Analysis can be categorized into targetindependent and target-dependent. The targetindependent work mainly focused on exploring various local linguistic features and incorporating them into supervised learning based systems (Pang and Lee, 2004; Zhao et al., 2008; Narayanan et al., 2009) or unsupervised learning based systems (Joshi et al., 2011). Recent target-dependent work has focused on automatically extracting sentiment expressions for a given target (Godbole et al., 2007; Chen et al., 2012), or incorporating target-dependent features into sentiment analysis (Liu et al., 2005; Jiang et al., 2011). In this paper we focus on the task of jointly extracting sentiment, target and issue in order to provide richer and more concrete evidence to describe and predict the attitudes of online users. This bares similarity to the idea of modeling aspect rating in product reviews (Titov and McDonald, 2008; Wang et al., 2011). When sentiment ana"
Y12-1013,D09-1019,0,0.0223555,"Missing"
Y12-1013,pak-paroubek-2010-twitter,0,0.45822,"Missing"
Y12-1013,P04-1035,0,0.0429906,"Missing"
Y12-1013,W02-1011,0,0.011769,"l!!! #GOPFAIL I also find it interesting that so many people ridicule Ron Paul's foreign policy yet the people that are directly affected by it, our troops, support Ron Paul more than any other GOP candidate combined and more than Obama. Obama screwed up by not fixing the economy first and leaving health care reform for a second term. Table 1: Sentiment Examples of Different Genres negative sentiments as opposed to neutral, therefore we only focus on the polarity classification problem. We also used a more traditional set for sentiment analysis — the movie review polarity data set shared by (Pang et al., 2002) — to highlight the challenges of more informal texts. Table 2 summarizes the statistics of data sets used for each genre. All experiments in this paper are based on three-fold cross-validation. Genre Review Tweet Forum Positive 5691 2323 381 Negative 5691 2323 381 Table 2: Statistics of Data Sets 4 Linguistic-based Approach In this section, we present our baseline approach using only linguistic features. 4.1 Pre-processing We have applied the tool developed by Han and Baldwin (2011) together with the following additional steps to perform normalization for informal documents (tweets and forum"
Y12-1013,W11-2207,0,0.17712,"Missing"
Y12-1013,P08-1036,0,0.0926009,"Missing"
Y12-1013,N03-1033,0,0.0293076,"ave applied the tool developed by Han and Baldwin (2011) together with the following additional steps to perform normalization for informal documents (tweets and forum posts). • Replace URLs with “@URL”. • Replace @username with “@USERNAME”. • Replace negation words with “NOT” based on the list derived from the LIWCLexicon (Pennebaker et al., 2001). • Normalize slang words (e.g. “LOL” to “laugh out loud”) (Agarwal et al., 2011). • Spelling correction using WordNet (Fellbaum, 2005) (e.g. “cooooool” to “cool”) In addition, each document has been tokenized and annotated with Part-of-speech tags (Toutanova et al., 2003). 129 4.2 Target and Issue Detection After pre-processing, the first step is to detect documents which include popular targets and issues. A popular target is an entity that users frequently discuss, such as a product (e.g. “Iphone4”), a person (e.g. “Ron Paul”) or an organization (e.g. “Red Cross”). A popular issue is a related aspect associated with a target, such as “display function” or “economic issue”. We have applied a state-of-the-art English entity extraction system (Li et al., 2012; Ji et al., 2005) that includes name tagging and coreference resolution to detect name variants from ea"
Y12-1013,J04-3002,0,0.0429006,"ped a supervised learning model based on Support Vector Machines to classify sentiment labels for each document (a post, a tweet message or a movie review document), incorporating several features such as N-grams, POS, various lexicons, punctuation, capitalization (see Table 3). Feature N-grams Part-of-Speech Gazetteer Word Cluster Punctuation Capitalization Description All unique unigrams, bigrams and trigrams Part-Of-Speech tags generated by Stanford Parser (Toutanova et al., 2003) Lexical matching based on (Joshi et al., 2011), SentiWordNet (Baccianella et al., 2010), Subjectivity Lexicon (Wiebe et al., 2004), Inquirer (Stone et al., 1966), Taboada (Taboada and Grieve, 2004), UICLexicon (Hu and Liu, 2004), LIWCLexicon (Pennebaker et al., 2001) Use synset information provided by Wordnet to expand the entries of each gazettteer; Lexical matching based on the expanded gazetteers Whether the document includes any exclamation mark or question mark Unique words which include all capitalized letters Table 3: Linguistic Features Used in the Baseline System The classification results are normalized to probability based confidence values via a sigmoid kernel function (Wu et al., 2004). 4.4 Results and Analy"
Y12-1013,W03-1017,0,0.220023,"Missing"
Y12-1013,D08-1013,0,0.0324631,"Missing"
