2020.lrec-1.178,Q17-1010,0,0.0146337,"Missing"
2020.lrec-1.178,E14-1030,0,0.0980854,"ade necessary the development of new approaches for unmasking deceit not relying on human judgement. Recently, there has been growing interest in the automatic detection of deception focused on language analysis (Hauch et al., 2015; Fitzpatrick et al., 2015). Among the others, some recent studies in this area have shown different promising applications by analysing the language of fake news (Conroy et al., 2015; Pérez-Rosas et al., 2017), court cases transcriptions (Fornaciari and Poesio, 2013; Yancheva and Rudzicz, 2013; Pérez-Rosas et al., 2015), deceptive product reviews (Ott et al., 2011; Fornaciari and Poesio, 2014; Kleinberg et al., 2018), cyber-crimes (Abozinadah et al., 2015; Mbaziira and Jones, 2016), autobiographical information (Levitan et al., 2018) and deceptive intentions regarding the future (Kleinberg et al., 2017). Nonetheless, although the encouraging results achieved so far, there are at least two main unsolved issues in the automatic detection of deception. A deceiving content can be created on a virtually unlimited range of subjects and thus the first issue refers to the generalization of the classification performances when dealing with data composed by different domains. Practically, p"
2020.lrec-1.178,L18-1550,0,0.0191478,"le cross-language comparison unfair. It is easy to see that word embeddings are affected by the same issue. However, after a preliminary investigation, we argue that this gap is acceptable. • FastText showed excellent performance on several tasks compared with other methods, such as word-2vec. • This part of the work aim at providing a baseline for most of the tasks that can be analyzed with the DecOp corpus. Further models, extensions, ensembles, or specific techniques are outside the scope of this paper. • Recently, pre-trained FastText models have been released for 157 different languages (Grave et al., 2018). Most important, the same pre-training procedure has 1427 within topic cross topic author-based language EN IT EN IT EN IT Abo 0.656±0.060 0.656±0.026 0.720±0.014 0.818±0.012 0.873±0.005 0.901±0.016 CL 0.630±0.055 0.688±0.041 0.726±0.043 0.818±0.012 0.767±0.019 0.873±0.027 Eut 0.676±0.014 0.684±0.030 0.692±0.012 0.788±0.042 0.782±0.085 0.891±0.019 GM 0.620±0.087 0.664±0.089 0.710±0.023 0.816±0.026 0.883±0.015 0.848±0.020 PoM 0.676±0.067 0.732±0.077 0.758±0.015 0.772±0.024 0.896±0.014 0.877±0.010 Table 5: Average test accuracy scores and standard deviation computed on within-topic, cross-topic"
2020.lrec-1.178,N18-1176,0,0.0114211,"automatic detection of deception focused on language analysis (Hauch et al., 2015; Fitzpatrick et al., 2015). Among the others, some recent studies in this area have shown different promising applications by analysing the language of fake news (Conroy et al., 2015; Pérez-Rosas et al., 2017), court cases transcriptions (Fornaciari and Poesio, 2013; Yancheva and Rudzicz, 2013; Pérez-Rosas et al., 2015), deceptive product reviews (Ott et al., 2011; Fornaciari and Poesio, 2014; Kleinberg et al., 2018), cyber-crimes (Abozinadah et al., 2015; Mbaziira and Jones, 2016), autobiographical information (Levitan et al., 2018) and deceptive intentions regarding the future (Kleinberg et al., 2017). Nonetheless, although the encouraging results achieved so far, there are at least two main unsolved issues in the automatic detection of deception. A deceiving content can be created on a virtually unlimited range of subjects and thus the first issue refers to the generalization of the classification performances when dealing with data composed by different domains. Practically, previous findings showed a general decrease in classifiers’ performance when training and test data belong to different domains and are significa"
2020.lrec-1.178,P09-2078,1,0.677021,"to the generalization of the classification performances when dealing with data composed by different domains. Practically, previous findings showed a general decrease in classifiers’ performance when training and test data belong to different domains and are significantly different in content (Krüger et al., 2017). In particular, previous studies about the automatic deception detection displayed that the effect of domain change determines a significant drop in the model performances in cross-domain classification experiments (Hernández-Castañeda et al., 2017; Pérez-Rosas and Mihalcea, 2014; Mihalcea and Strapparava, 2009). However, large labelled datasets which allow cross-domain comparisons are few in the scientific literature (Yao et al., 2017; Pérez-Rosas et al., 2017; Pérez-Rosas and Mihalcea, 2014; Mihalcea and Strapparava, 2009), making harder facing this issue. The other unsolved argument refers to the stability of linguistic clues of deception across different languages. In fact, previous research findings highlighted contrasting results in support of both the stability (Matsumoto et al., 2015b; Matsumoto et al., 2015a; Matsumoto and Hwang, 2015) and inconsistencies (Leal et al., 2018; Rungruangthum an"
2020.lrec-1.178,P11-1032,0,0.601637,"tioned phenomena made necessary the development of new approaches for unmasking deceit not relying on human judgement. Recently, there has been growing interest in the automatic detection of deception focused on language analysis (Hauch et al., 2015; Fitzpatrick et al., 2015). Among the others, some recent studies in this area have shown different promising applications by analysing the language of fake news (Conroy et al., 2015; Pérez-Rosas et al., 2017), court cases transcriptions (Fornaciari and Poesio, 2013; Yancheva and Rudzicz, 2013; Pérez-Rosas et al., 2015), deceptive product reviews (Ott et al., 2011; Fornaciari and Poesio, 2014; Kleinberg et al., 2018), cyber-crimes (Abozinadah et al., 2015; Mbaziira and Jones, 2016), autobiographical information (Levitan et al., 2018) and deceptive intentions regarding the future (Kleinberg et al., 2017). Nonetheless, although the encouraging results achieved so far, there are at least two main unsolved issues in the automatic detection of deception. A deceiving content can be created on a virtually unlimited range of subjects and thus the first issue refers to the generalization of the classification performances when dealing with data composed by diff"
2020.lrec-1.178,N13-1053,0,0.0240983,"anguage classification tasks. 2. Related work Recently, the scientific community showed a rising interest in the automatic detection of deception focused on the analysis of linguistic clues of deceit in typed text (Fitzpatrick et al., 2015; Hauch et al., 2015; Nunamaker et al., 2012). One of the resulting contributions is the introduction of several datasets for studying different aspects and scenarios in which deceit can lead to serious consequences. For example, previous works have presented labelled corpora composed of both truthful and deceptive reviews regarding hotels (Ott et al., 2011; Ott et al., 2013), books (Fornaciari and Poesio, 2014) and restaurants (Li et al., 2015), as well as collections of trustworthy news and fact-checked fake news (Wang, 2017; Pérez-Rosas et al., 2017). Nonetheless, the just mentioned works tried to face deception-related practical issues, focusing on single and specific domains. However, as stated in the previous section, since the difference in content between training and test data leads to a decrease in classification performances, the need for multi-domain datasets is primary for assessing the models’ generalization abilities. Multi-domains datasets for auto"
2020.lrec-1.178,P14-2072,0,0.847931,"and thus the first issue refers to the generalization of the classification performances when dealing with data composed by different domains. Practically, previous findings showed a general decrease in classifiers’ performance when training and test data belong to different domains and are significantly different in content (Krüger et al., 2017). In particular, previous studies about the automatic deception detection displayed that the effect of domain change determines a significant drop in the model performances in cross-domain classification experiments (Hernández-Castañeda et al., 2017; Pérez-Rosas and Mihalcea, 2014; Mihalcea and Strapparava, 2009). However, large labelled datasets which allow cross-domain comparisons are few in the scientific literature (Yao et al., 2017; Pérez-Rosas et al., 2017; Pérez-Rosas and Mihalcea, 2014; Mihalcea and Strapparava, 2009), making harder facing this issue. The other unsolved argument refers to the stability of linguistic clues of deception across different languages. In fact, previous research findings highlighted contrasting results in support of both the stability (Matsumoto et al., 2015b; Matsumoto et al., 2015a; Matsumoto and Hwang, 2015) and inconsistencies (Le"
2020.lrec-1.178,P17-2067,0,0.0562284,"analysis of linguistic clues of deceit in typed text (Fitzpatrick et al., 2015; Hauch et al., 2015; Nunamaker et al., 2012). One of the resulting contributions is the introduction of several datasets for studying different aspects and scenarios in which deceit can lead to serious consequences. For example, previous works have presented labelled corpora composed of both truthful and deceptive reviews regarding hotels (Ott et al., 2011; Ott et al., 2013), books (Fornaciari and Poesio, 2014) and restaurants (Li et al., 2015), as well as collections of trustworthy news and fact-checked fake news (Wang, 2017; Pérez-Rosas et al., 2017). Nonetheless, the just mentioned works tried to face deception-related practical issues, focusing on single and specific domains. However, as stated in the previous section, since the difference in content between training and test data leads to a decrease in classification performances, the need for multi-domain datasets is primary for assessing the models’ generalization abilities. Multi-domains datasets for automatic deceit detection has been recently introduced concerning fake products reviews (Yao et al., 2017) and fake news (Pérez-Rosas et al., 2017). After co"
2020.lrec-1.178,P13-1093,0,0.01844,"amodt and Custer, 2006). As a result, the possible dangers associated with the mentioned phenomena made necessary the development of new approaches for unmasking deceit not relying on human judgement. Recently, there has been growing interest in the automatic detection of deception focused on language analysis (Hauch et al., 2015; Fitzpatrick et al., 2015). Among the others, some recent studies in this area have shown different promising applications by analysing the language of fake news (Conroy et al., 2015; Pérez-Rosas et al., 2017), court cases transcriptions (Fornaciari and Poesio, 2013; Yancheva and Rudzicz, 2013; Pérez-Rosas et al., 2015), deceptive product reviews (Ott et al., 2011; Fornaciari and Poesio, 2014; Kleinberg et al., 2018), cyber-crimes (Abozinadah et al., 2015; Mbaziira and Jones, 2016), autobiographical information (Levitan et al., 2018) and deceptive intentions regarding the future (Kleinberg et al., 2017). Nonetheless, although the encouraging results achieved so far, there are at least two main unsolved issues in the automatic detection of deception. A deceiving content can be created on a virtually unlimited range of subjects and thus the first issue refers to the generalization of"
2020.lrec-1.178,yao-etal-2017-online,0,0.113568,"s showed a general decrease in classifiers’ performance when training and test data belong to different domains and are significantly different in content (Krüger et al., 2017). In particular, previous studies about the automatic deception detection displayed that the effect of domain change determines a significant drop in the model performances in cross-domain classification experiments (Hernández-Castañeda et al., 2017; Pérez-Rosas and Mihalcea, 2014; Mihalcea and Strapparava, 2009). However, large labelled datasets which allow cross-domain comparisons are few in the scientific literature (Yao et al., 2017; Pérez-Rosas et al., 2017; Pérez-Rosas and Mihalcea, 2014; Mihalcea and Strapparava, 2009), making harder facing this issue. The other unsolved argument refers to the stability of linguistic clues of deception across different languages. In fact, previous research findings highlighted contrasting results in support of both the stability (Matsumoto et al., 2015b; Matsumoto et al., 2015a; Matsumoto and Hwang, 2015) and inconsistencies (Leal et al., 2018; Rungruangthum and Todd, 2017; DeCicco and Schafer, 2015) across different languages of verbal clues of deception. Moreover, so far most studie"
2020.lrec-1.186,S19-2005,0,0.0223473,"hey reacted to them. The valence and arousal Facebook posts is a dataset of 2,895 Social Media posts rated by two psychologically trained annotators on two separate ordinal ninepoint scales. These scales represent valence and arousal. The Affective Text (Strapparava and Mihalcea, 2008) developed for the shared task of affective computing in SemEval 2017 consists of news headlines taken from major newspapers. The annotation was performed manually by six annotators, and the set of labels includes six emotions: anger, disgust, fear, joy, sadness, and surprise. In SemEval-2019 Task 3: EmoContext (Chatterjee et al., 2019), the organizers provided a dataset of textual dialogues annotated for four classes: happy, sad, anger and others. TEC (Mohammad, 2012) is a large dataset of more than 20,000 emotion-labeled tweets automatically label using hashtags. The set of labels includes six basic emotions: anger, disgust, fear, joy, sadness, and surprise. EmoTweet28 (Liew et al., 2016) is a corpus developed using four different sampling strategies based on random sampling by topic and user. The corpus contains tweets annotated with 28 emotions categories and captures the language used to express an emotion explicitly an"
2020.lrec-1.186,L16-1183,0,0.0449609,"Missing"
2020.lrec-1.186,S18-1001,0,0.0304931,"basic emotions: anger, disgust, fear, joy, sadness, and surprise. EmoTweet28 (Liew et al., 2016) is a corpus developed using four different sampling strategies based on random sampling by topic and user. The corpus contains tweets annotated with 28 emotions categories and captures the language used to express an emotion explicitly and implicitly. However, the availability of datasets created specifically for languages other than English is very limited. In SemEval-2018 Task 1: Affect in Tweets, the organizers provided the Affect in Tweets (AIT) dataset for English, Arabic and Spanish tweets (Mohammad et al., 2018). It is composed of a set of tweets annotated for four basic emotions: anger, fear, joy, and sadness. A blog emotion corpus was constructed for Chinese emotional expression analysis (Quan and Ren, 2009). This corpus contains manual annotations of eight emotional categories: expectation, joy, love, surprise, anxiety, sorrow, anger and hate. In particular, we found only a few resources annotated with emotions in Spanish and even most of the English emotion datasets have not been fully annotated manually. For this reason, it is important to focus efforts on creating datasets that are manually lab"
2020.lrec-1.186,S12-1033,0,0.040183,"tors on two separate ordinal ninepoint scales. These scales represent valence and arousal. The Affective Text (Strapparava and Mihalcea, 2008) developed for the shared task of affective computing in SemEval 2017 consists of news headlines taken from major newspapers. The annotation was performed manually by six annotators, and the set of labels includes six emotions: anger, disgust, fear, joy, sadness, and surprise. In SemEval-2019 Task 3: EmoContext (Chatterjee et al., 2019), the organizers provided a dataset of textual dialogues annotated for four classes: happy, sad, anger and others. TEC (Mohammad, 2012) is a large dataset of more than 20,000 emotion-labeled tweets automatically label using hashtags. The set of labels includes six basic emotions: anger, disgust, fear, joy, sadness, and surprise. EmoTweet28 (Liew et al., 2016) is a corpus developed using four different sampling strategies based on random sampling by topic and user. The corpus contains tweets annotated with 28 emotions categories and captures the language used to express an emotion explicitly and implicitly. However, the availability of datasets created specifically for languages other than English is very limited. In SemEval-2"
2020.lrec-1.186,D09-1150,0,0.0363313,"The corpus contains tweets annotated with 28 emotions categories and captures the language used to express an emotion explicitly and implicitly. However, the availability of datasets created specifically for languages other than English is very limited. In SemEval-2018 Task 1: Affect in Tweets, the organizers provided the Affect in Tweets (AIT) dataset for English, Arabic and Spanish tweets (Mohammad et al., 2018). It is composed of a set of tweets annotated for four basic emotions: anger, fear, joy, and sadness. A blog emotion corpus was constructed for Chinese emotional expression analysis (Quan and Ren, 2009). This corpus contains manual annotations of eight emotional categories: expectation, joy, love, surprise, anxiety, sorrow, anger and hate. In particular, we found only a few resources annotated with emotions in Spanish and even most of the English emotion datasets have not been fully annotated manually. For this reason, it is important to focus efforts on creating datasets that are manually labeled and not only in English. 3. Creating the Multilingual Emotion Corpus Our goal in collecting emotions tweets is to explore great relevant events in a specific time frame on Twitter. In order to acco"
2020.lrec-1.742,W07-2314,0,0.0605593,"hence potentially contribute to better memorisation of them in the mind. One of the initial hypothesis of this study was that abstract language is more strongly associated with higher levels of language production. The BNC (BNC, 2007) proves that 6057 the top 500 most frequent verbs used in academic texts are, by and large, abstract. This revelation, tied with the literature previously mentioned, leads us to understand that abstract verbs dominate in advanced genres of writing such as scientific, argumentative, critical and evaluative texts, in which “argument predominates over description"" (Power, 2007). Verbs used in such genres of writing are referred to as discourse verbs (Danlos, 2006), and they include verbs like: express, suggest and involve. The current study uses a combination of graphic symbols, animation and sign language to contribute towards tackling the following question: how can we represent abstract verbs visually? The study of sign language was given priority due to its excellent mapping from meaning to form, using a wide vocabulary of signs that enables the interpretation of any possible universal concept. This mapping is referred to as a formational link (Brennan and Brien"
2020.lrec-1.742,P14-1024,0,0.0140384,"rm their communicative meaning through naïve human judgements. The methodology employed was motivated by Richardson et al. (2001), a study in which four images depicting the four directions (up, down, left and right) were created in association with concrete and abstract words, proving the hypothesis that “language is spatial"". 3.1. Data preparation Two methods were attempted in the data preparation stage of this study. First, a list of 150,114 words of mixed classes rated by abstractness and imageability were extracted from the MRC psycholinguistic database (Coltheart, 1981) and organised by Tsvetkov et al. (2014) with the intention of applying a top-down approach to narrow this down to our target verbs. Elimination was done in order to obtain a list of words that were only abstract verbs, by tagging with WordNet. Then, weighting was given to each verb to prioritise on high synonymy count, high polysemy count, high abstractness rating, inclusion in the Academic Word List (Coxhead, 2000), and inclusion in the top 500 verbs used in academic texts on the British National Corpus. After being weighted and sorted, the top 500 verbs were passed through clustering techniques like Latent Dirichlet Allocation (B"
A92-1003,P90-1029,0,0.0312712,"Missing"
A92-1003,J87-1005,0,0.0801603,"Missing"
A92-1003,J89-1001,0,0.0407361,"Missing"
A92-1003,P89-1024,0,0.0335843,"Missing"
A92-1003,P88-1003,0,\N,Missing
aleksandrov-strapparava-2012-ngramquery,J06-1003,0,\N,Missing
aleksandrov-strapparava-2012-ngramquery,P11-1029,0,\N,Missing
C12-2117,N04-4038,0,0.0134344,"focus In the experiment of exploring similarity, we exploited three corpora in the respective languages. Arabic: Arabic Gigaword Third Edition is a comprehensive archive of newswire text data acquired from Arabic news sources. The six distinct sources of Arabic newswire are: Agence France Presse, Assabah, Al Hayat, An Nahar, Ummah Press, and Xinhua News Agency. The total number of documents is about 1.500.000 in a span time from 1995 until 2007. The preprocessing on this corpus consisted of a conversion from Arabic to Buckwalter ascii encoding and of a postagging process with the AMIRA tool (Diab et al., 2004). English: We collected about 400.000 Google-News in the years 2008/2009. The documents have been pos-tagged with the TextPro tool (Pianta et al., 2008). Hebrew: We used a collection of news documents from three newspapers in the span time 1990 - 2002: Arutz7, The Marker, and HaAretz.The corpus includes 11.474 documents and it has been preprocessed with a pos-tagger (Itai and Wintner, 2008). In building the datasets from the documents of the three corpora, we considered as parts of speech nouns, verbs, adjectives and adverbs. In order to select a suitable set of terms of conflict and emotion t"
C12-2117,pianta-etal-2008-textpro,0,0.0145612,"omprehensive archive of newswire text data acquired from Arabic news sources. The six distinct sources of Arabic newswire are: Agence France Presse, Assabah, Al Hayat, An Nahar, Ummah Press, and Xinhua News Agency. The total number of documents is about 1.500.000 in a span time from 1995 until 2007. The preprocessing on this corpus consisted of a conversion from Arabic to Buckwalter ascii encoding and of a postagging process with the AMIRA tool (Diab et al., 2004). English: We collected about 400.000 Google-News in the years 2008/2009. The documents have been pos-tagged with the TextPro tool (Pianta et al., 2008). Hebrew: We used a collection of news documents from three newspapers in the span time 1990 - 2002: Arutz7, The Marker, and HaAretz.The corpus includes 11.474 documents and it has been preprocessed with a pos-tagger (Itai and Wintner, 2008). In building the datasets from the documents of the three corpora, we considered as parts of speech nouns, verbs, adjectives and adverbs. In order to select a suitable set of terms of conflict and emotion terms, questionnaires were distributed among native speakers of Arabic and Hebrew respectively, i.e. students of universities (Tel Aviv, Haifa), colleges"
C18-1293,C12-1025,0,0.289788,"even when a high proficiency level in a non-native language is achieved. 1 Introduction Native Language Identification (NLI) – identifying the native language (L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007)"
C18-1293,P17-2086,1,0.931542,"ntification (NLI) – identifying the native language (L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a linguistic point of view, punctuation has been disputed as following prosodic"
C18-1293,W17-5049,0,0.660463,"Missing"
C18-1293,D14-1142,0,0.770328,"or, which do not diminish in influence even when a high proficiency level in a non-native language is achieved. 1 Introduction Native Language Identification (NLI) – identifying the native language (L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribut"
C18-1293,W13-1732,0,0.0787801,"ency level in a non-native language is achieved. 1 Introduction Native Language Identification (NLI) – identifying the native language (L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a lin"
C18-1293,W17-5007,0,0.388824,"Missing"
C18-1293,W17-5042,1,0.778619,"ical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a linguistic point of view, punctuation has been disputed as following prosodic principles or as a clarifier of grammatical structure (Baron, 2001; Bruthiaux, 1993). Moore (2016) finds a common ground for these two views by observing that prosody and punctuation realize the same function – revealing/emphasizing the information structure of an utterance – in the spoken and respectively written modes of language. Since grammar and prosodic structure are language specific, indicators that reveal them would be language specific as well. As with other aspects"
C18-1293,P13-1112,0,0.0182134,"s achieved. 1 Introduction Native Language Identification (NLI) – identifying the native language (L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a linguistic point of view, punctuation has"
C18-1293,D17-1286,1,0.829035,"the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a linguistic point of view, punctuation has been disputed as following prosodic principles or as a clarifier of grammatical structure (Baron, 2001; Bruthiaux, 1993). Moore (2016) finds a co"
C18-1293,W13-1718,0,0.16246,"(L1) of a person based on his/her writing in the second language (L2) – is useful for a variety of purposes, including security, marketing, and educational applications. The effect of native language phenomena seeping into texts produced in a different language is known as language transfer (Odlin, 1989). Numerous aspects of the language have been explored for NLI – character-level language models (Ionescu et al., 2014), lexical choice (Brooke and Hirst, 2012; Lahiri and Mihalcea, 2013), grammar (Nagata and Whittaker, 2013), spelling errors (Chen et al., 2017; Koppel et al., 2005), cognates (Nicolai et al., 2013), and general etymology (Nastase and Strapparava, 2017). While punctuation has been included in some of these studies (e.g., in character-level models), its impact has not been studied. It is however an important, and often revealing, aspect of written language. For example, punctuation is a strong indicator of authorship, and has been used successfully in stylometric analysis for authorship attribution (Markov et al., 2017b; Grieve, 2007). More generally, from a linguistic point of view, punctuation has been disputed as following prosodic principles or as a clarifier of grammatical structure"
C18-1293,C12-1158,0,0.361292,"ays were written in response to eight different writing prompts/topics (P0–P7), all of which appear in all 11 L1 groups. The dataset also contains information regarding the proficiency level (low, medium, high) of the essay authors. Dataset statistics in terms of proficiency levels and writing prompts are presented in Tables 1 and 2, respectively. ICLEv2 (Granger et al., 2009): the ICLEv2 dataset (henceforth, ICLE) consists of essays written by highly-proficient non-native college-level students of English. We used a 7-language subset of the corpus normalized for topic and character encoding (Tetreault et al., 2012; Ionescu et al., 2014)2 . This subset contains 110 essays (with an average of 747 tokens per essay after tokenization and removal of metadata) for each of the 7 languages: Bulgarian, Chinese, Czech, French, Japanese, Russian, and Spanish. 2 The authors express their gratitude to A. Cahill for providing the list of documents used in their paper. 3458 3.2 Features The suite of experiments we report were designed to investigate the impact of punctuation-based features on native language identification. The hypothesis we are testing is whether patterns of punctuation usage – possibly motivated by"
C18-1293,W13-1706,0,0.376823,"Missing"
C18-1293,W07-0602,0,0.0397926,"Missing"
C18-1293,U09-1008,0,0.0674874,"Missing"
caselli-etal-2014-enriching,baroni-etal-2004-introducing,0,\N,Missing
caselli-etal-2014-enriching,alvez-etal-2008-complete,0,\N,Missing
caselli-etal-2014-enriching,W11-0122,0,\N,Missing
caselli-etal-2014-enriching,N09-2066,0,\N,Missing
caselli-etal-2014-enriching,W03-1022,0,\N,Missing
caselli-etal-2014-enriching,E09-1005,0,\N,Missing
caselli-etal-2014-enriching,P06-1014,0,\N,Missing
caselli-etal-2014-enriching,Q13-1013,0,\N,Missing
caselli-etal-2014-enriching,caselli-etal-2012-customizable,1,\N,Missing
caselli-etal-2014-enriching,lenci-etal-2012-lexit,0,\N,Missing
caselli-etal-2014-enriching,I11-1099,0,\N,Missing
caselli-etal-2014-enriching,W13-3816,1,\N,Missing
caselli-etal-2014-enriching,jezek-quochi-2010-capturing,0,\N,Missing
D12-1054,H05-1073,0,0.129543,"g; and the duration of the note. Table 1 shows statistics on the corpus. An example from the corpus, consisting of the first two lines from the Beatles’ song A hard day’s night, is illustrated in Figure 3. S ONGS S ONGS IN “ MAJOR ” KEY S ONGS IN “ MINOR ” KEY L INES A LIGNED SYLLABLES / NOTES 100 59 41 4,976 34,045 Table 1: Some statistics of the corpus Emotion Annotations with Mechanical Turk. In order to explore the classification of emotions in songs, we needed a gold standard consisting of manual emotion annotations of the songs. Following 592 previous work on emotion annotation of text (Alm et al., 2005; Strapparava and Mihalcea, 2007), to annotate the emotions in songs we use the six basic emotions proposed by (Ekman, 1993): ANGER, DISGUST , FEAR , JOY , SADNESS , SURPRISE . To collect the annotations, we use the Amazon Mechanical Turk service, which was previously found to produce reliable annotations with a quality comparable to those generated by experts (Snow et al., 2008). The annotations are collected at line level, with a separate annotation for each of the six emotions. We collect numerical annotations using a scale between 0 and 10, with 0 corresponding to the absence of an emotion"
D12-1054,S07-1094,0,0.0200567,"Missing"
D12-1054,S07-1067,0,0.0532147,"Missing"
D12-1054,S07-1072,0,0.0171951,"Missing"
D12-1054,D08-1027,0,0.141238,"Missing"
D12-1054,S07-1013,1,0.53812,"n of the note. Table 1 shows statistics on the corpus. An example from the corpus, consisting of the first two lines from the Beatles’ song A hard day’s night, is illustrated in Figure 3. S ONGS S ONGS IN “ MAJOR ” KEY S ONGS IN “ MINOR ” KEY L INES A LIGNED SYLLABLES / NOTES 100 59 41 4,976 34,045 Table 1: Some statistics of the corpus Emotion Annotations with Mechanical Turk. In order to explore the classification of emotions in songs, we needed a gold standard consisting of manual emotion annotations of the songs. Following 592 previous work on emotion annotation of text (Alm et al., 2005; Strapparava and Mihalcea, 2007), to annotate the emotions in songs we use the six basic emotions proposed by (Ekman, 1993): ANGER, DISGUST , FEAR , JOY , SADNESS , SURPRISE . To collect the annotations, we use the Amazon Mechanical Turk service, which was previously found to produce reliable annotations with a quality comparable to those generated by experts (Snow et al., 2008). The annotations are collected at line level, with a separate annotation for each of the six emotions. We collect numerical annotations using a scale between 0 and 10, with 0 corresponding to the absence of an emotion, and 10 corresponding to the hig"
D12-1054,strapparava-valitutti-2004-wordnet,1,0.490929,"e equal to the average of the scores on the training data, and measured the correlation between these default scores and the gold standard, consistently led to correlations close to 0 (0.0081-0.0221). 3 594 guistic Inquiry and Word Count (LIWC) and WordNet Affect (WA) to derive coarse textual features. LIWC was developed as a resource for psycholinguistic analysis (Pennebaker and Francis, 1999; Pennebaker and King, 1999). The 2001 version of LIWC includes about 2,200 words and word stems grouped into about 70 broad categories relevant to psychological processes (e.g., emotion, cognition). WA (Strapparava and Valitutti, 2004) is a resource that was created starting with WordNet, by annotating synsets with several emotions. It uses several resources for affective information, including the emotion classification of Ortony (Ortony et al., 1987). From WA, we extract the words corresponding to the six basic emotions used in our experiments. For each semantic class, we infer a feature indicating the number of words in a line belonging to that class. Table 3 shows the Pearson correlations obtained for each of the six emotions, when using only unigrams, only semantic classes, or both. Emotion ANGER DISGUST FEAR JOY SADNE"
D12-1054,P08-2034,0,0.0883511,"nnotated corpus is found in (O’Hara, 2011), who presented preliminary research that checks whether the expressive meaning of a particular harmony or harmonic sequence could be deduced from the lyrics it accompanies, by using harmonically annotated chords from the Usenet group alt.guitar.tab. Finally, in natural language processing, there are a few studies that mainly exploited the lyrics component of the songs, while generally ignoring the musical component. For instance, (Mahedero et al., 2005) dealt with language identification, structure extraction, and thematic categorization for lyrics. (Xia et al., 2008) addressed the task of sentiment classification in lyrics, recognizing positive and negative moods in a large dataset of Chinese pop songs, while (Yang and Lee, 2009) approached the problem of emotion identification in lyrics, classifying songs from allmusic.com using a set of 23 emotions. 3 A Corpus of Music and Lyrics Annotated for Emotions To enable our exploration of emotions in songs, we compiled a corpus of 100 popular songs (e.g., Dancing Queen by ABBA, Hotel California by Eagles, Let it Be by The Beatles). Popular songs exert a lot of power on people, both at an individual level as wel"
D14-1046,agirre-etal-2010-exploring,0,0.0898934,"een obtained through two strategies: • lemma label: we extract all normalized domain labels associated to each sense of each lemma in the sense description from MWN. The value of the feature GENERIC corresponds to the sum of the FACTOTUM labels. The value of the feature SPECIFIC corresponds to the sum of all other specific domain labels (e.g. MEDICINE, SPORT etc.) after they have been collapsed into a single value (i.e. NOT-FACTOTUM). • word sense label: for each sense description, we have first performed Word Sense Disambiguation by means of an adapted version to Italian of the UKB package2 (Agirre et al., 2010; Agirre et al., 2014)3 . Only the highest ranked synset, and associated WN Domain(s), was retained as good. Similarly to the lemma label strategy, the sum of the domain label FACTOTUM is assigned to the feature GENERIC, while the sum of all other domain labels collapsed into the single value NOTFACTOTUM is assigned to the feature SPECIFIC. Classifier and feature selection We have developed a training set by manually aligning noun senses between the two lexica. The sense alignment allows us to associate all the information of a synset to a corresponding entry in the SCDM lexicon, including the"
D14-1046,W04-2214,0,0.0428249,"8 38123 Povo, Italy t.caselli@gmail.com Carlo Strapparava FBK / Via Sommarive, 18 38123 Povo, Italy strappa@fbk.eu Abstract The purpose of this work is two folded: first, we experiment on the automatic assignment of domain labels to sense descriptions, and then, evaluate the impact of this information for improving an existing sense aligned dataset for nouns. Previous works has demonstrated that domain labels are a good feature for obtaining high quality alignments of entries (Navigli, 2006; Toral et al., 2009; Navigli and Ponzetto, 2012). The WordNet (WN) Domains (Magnini and Cavaglia, 2000; Bentivogli et al., 2004) have been selected as reference domain labels. We will use as candidate lexico-semantic resources to be aligned two Italian lexica, namely, MultiWordNet (MWN) and the Senso Comune De Mauro Lexicon (SCDM) (Vetere et al., 2011). The two resources differ in terms of modelization: the former, MWN, is an Italian version of WN obtained through the “expand model” (Vossen, 1996) and perfectly aligned to Princeton WN 1.6, while the latter, SCDM, is a machine readable dictionary obtained from a paper-based reference lexicographic dictionary, De Mauro GRADIT. Major issues for WSA of the lexica concern t"
D14-1046,W14-0140,1,0.773603,"od. Similarly to the lemma label strategy, the sum of the domain label FACTOTUM is assigned to the feature GENERIC, while the sum of all other domain labels collapsed into the single value NOTFACTOTUM is assigned to the feature SPECIFIC. Classifier and feature selection We have developed a training set by manually aligning noun senses between the two lexica. The sense alignment allows us to associate all the information of a synset to a corresponding entry in the SCDM lexicon, including the WN Domain label. Concerning the test set, we have used an existing dataset of aligned noun pairs as in (Caselli et al., 2014). We report in Table 1 the figures for the training and test sets. Multiple alignments with the same domain label have been excluded from the training set. Characteristics # lemmas # of aligned pairs # of SCDM senses # of MWN synsets # SCDM with WN Domain label Training Set 131 369 747 675 Test Set 46 166 216 229 350 118 We experimented with two classifiers: Naive Bayes and Maximum Entropy as implemented in the MALLET package (McCallum, 2002). We illustrate the results in Table 2. The classifiers have been evaluated with respect to standard Precision (P), Recall (R) and F1 against the test set"
D14-1046,magnini-cavaglia-2000-integrating,0,0.0945283,"rentoRISE / Via Sommarive, 18 38123 Povo, Italy t.caselli@gmail.com Carlo Strapparava FBK / Via Sommarive, 18 38123 Povo, Italy strappa@fbk.eu Abstract The purpose of this work is two folded: first, we experiment on the automatic assignment of domain labels to sense descriptions, and then, evaluate the impact of this information for improving an existing sense aligned dataset for nouns. Previous works has demonstrated that domain labels are a good feature for obtaining high quality alignments of entries (Navigli, 2006; Toral et al., 2009; Navigli and Ponzetto, 2012). The WordNet (WN) Domains (Magnini and Cavaglia, 2000; Bentivogli et al., 2004) have been selected as reference domain labels. We will use as candidate lexico-semantic resources to be aligned two Italian lexica, namely, MultiWordNet (MWN) and the Senso Comune De Mauro Lexicon (SCDM) (Vetere et al., 2011). The two resources differ in terms of modelization: the former, MWN, is an Italian version of WN obtained through the “expand model” (Vossen, 1996) and perfectly aligned to Princeton WN 1.6, while the latter, SCDM, is a machine readable dictionary obtained from a paper-based reference lexicographic dictionary, De Mauro GRADIT. Major issues for W"
D14-1046,S01-1027,1,0.592062,"P 0.77 0.70 0.77 0.74 R 0.58 0.49 0.58 0.54 F1 0.66 0.58 0.66 0.62 10-Fold F1 0.66 0.63 0.69 0.67 Table 2: Results for the Naive Bayes and Maximum Entropy binary classifiers. synset1 and express a subject field label (e.g. SPORT, MEDICINE). A special label, FACTOTUM, has been used for those synsets which can appear in almost all subject fields. The identification of a domain label to the nominal entries in the SCDM Lexicon is based the “One Domain per Discourse” (ODD) hypothesis applied to the sense descriptions. We have used a reduced set of domains labels (45 normalized domains) following (Magnini et al., 2001). To assign the WN domain label to the SCDM entries, we have developed a hybrid method: first a binary classifier is applied to the SCDM sense descriptions to discriminate between two domain values, FACTOTUM and OTHER, where the OTHER value includes all remaining 44 normalized domains. After this, all entries classified with the OTHER value are analyzed by a rule based system and associated with a specific domain label (i.e. SPORT, MEDICINE, FOOD . . . ). 2.1 GENERIC:val SPECIFIC:val). Feature values have been obtained through two strategies: • lemma label: we extract all normalized domain lab"
D14-1046,R09-1080,0,0.0609121,"Missing"
D14-1046,Q13-1013,0,0.0192327,".g. WordNet (Fellbaum, 1998)). The process of creation of lexical resources is costly both in terms of money and time. To overcome these limits, semi-automatic approaches have been developed (e.g. MultiWordNet (Pianta et al., 2002)) with different levels of success. Furthermore, important information is scattered in different resources and difficult to use. Semantic interoperability between resources could represent a viable solution to allow reusability and develop more robust and powerful resources. Word sense alignment (WSA) qualifies as the preliminary requirement for achieving this goal (Matuschek and Gurevych, 2013). WSA aims at creating lists of pairs of senses from two, or more, (lexical-semantic) resources which denote the same meaning. Different approaches to WSA have been proposed and they all share some common elements, namely: i.) the extensive use of sense descriptions of the words (e.g. WordNet glosses); and ii.) the extension of the basic sense descriptions with additional information such as hypernyms, synonyms and domain or category labels. • SCMD has no structure of word senses (i.e. no taxonomy, no synonymy relations, no distinction between core senses and subsenses for polysemous entries)"
D14-1046,N07-1025,0,0.102901,"Missing"
D14-1046,P06-1014,0,0.0571605,"Missing"
D14-1046,W11-0122,0,0.0379761,"Missing"
D14-1046,P07-2041,0,0.0781805,"Missing"
D14-1046,J14-1003,0,\N,Missing
D14-1160,J90-1003,0,0.328286,"manner, we analyze the co-occurrence of each unique lemma-POS pair in the corpus with the sense seeds. We eliminate the candidates which have less than 5 co-occurrences with the sense categories. 3.3.2 Normalized Pointwise Mutual Information For the co-occurrence analysis of the candidate words and seeds, we use pointwise mutual information (PMI), which is simply a measure of 5 http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2011T07 association between the probability of the cooccurrence of two events and their individual probabilities when they are assumed to be independent (Church and Hanks, 1990). PMI can be exploited as a semantic similarity measure (Han et al., 2013) and it is calculated as: [ ] p(x, y) P M I(x, y) = log (1) p(x)p(y) To calculate the PMI value of a candidate word and a specific sense, we consider p(x) as the probability of the candidate word to occur in the corpus. Therefore, p(x) is calculated as p(x) = c(x)/N , where c(x) is the total count of the occurrences of the candidate word x in the corpus and N is the total co-occurrence count of all words in the corpus. Similarly, we calculate p(y) as the total occurrence count of all the seeds for the sense considered (y"
D14-1160,esuli-sebastiani-2006-sentiwordnet,0,0.0194075,"ey pro1512 2 http://www.crowdflower.com/ pose that larger lexicons with less accurate generation method perform better than the smaller human annotated lexicons. While a major drawback of manually generated lexicons is that they require a great deal of human labor, crowdsourcing services provide an easier procedure for manual annotations. Mohammad and Turney (2010) generate an emotion lexicon by using the crowdsourcing service provided by Amazon Mechanical Turk3 and it covers 14,200 term-emotion associations. Regarding the sentiment orientations and subjectivity levels of words, Sentiwordnet (Esuli and Sebastiani, 2006) is constructed as an extension to WordNet and it provides sentiments in synset level. Positive, negative and neutral values are assigned to synsets by using ternary classifiers and synset glosses. Another study that has been inspirational for the design of our approach is Banea et al. (2008). The authors generate a subjectivity lexicon starting with a set of seed words and then using a similarity measure among the seeds and the candidate words. Another cognitive feature relevant to sensorial load of the words is the association between colors and words. Mohammad (2011) builds a colorword asso"
D14-1160,P98-1013,0,0.442412,"Missing"
D14-1160,banea-etal-2008-bootstrapping,0,0.0219989,"asier procedure for manual annotations. Mohammad and Turney (2010) generate an emotion lexicon by using the crowdsourcing service provided by Amazon Mechanical Turk3 and it covers 14,200 term-emotion associations. Regarding the sentiment orientations and subjectivity levels of words, Sentiwordnet (Esuli and Sebastiani, 2006) is constructed as an extension to WordNet and it provides sentiments in synset level. Positive, negative and neutral values are assigned to synsets by using ternary classifiers and synset glosses. Another study that has been inspirational for the design of our approach is Banea et al. (2008). The authors generate a subjectivity lexicon starting with a set of seed words and then using a similarity measure among the seeds and the candidate words. Another cognitive feature relevant to sensorial load of the words is the association between colors and words. Mohammad (2011) builds a colorword association lexicon by organizing a crowdsourcing task on Amazon Mechanical Turk. Instead, Özbal et al. (2011) aim to automate this process and propose three computational methods based on image analysis, language models and latent semantic analysis (LSA) (Landauer and Dumais, 1997). The authors"
D14-1160,P03-1054,0,0.00700126,"ly, we treat each POS role of the same lemmas as a distinct seed and extract 4287 lemma-POS pairs from 2572 synsets. In this section, we explain the steps to construct our final sensorial lexicon in detail. 3.3.1 Corpus and Candidate Words As a corpus, we use a subset of English GigaWord 5th Edition released by Linguistic Data Consortium (LDC)5 . This resource is a collection of almost 10 million English newswire documents collected in recent years, whose content sums up to nearly 5 billion words. The richly annotated GigaWord data comprises automatic parses obtained with the Stanford parser (Klein and Manning, 2003) so that we easily have access to the lemma and POS information of each word in the resource. For the scope of this study, we work on a randomly chosen subset that contains 79800 sentences and we define a co-occurrence event as the co-existence of a candidate word and a seed word within a window of 9 words(the candidate word, 4 words to its left and 4 words to its right). In this manner, we analyze the co-occurrence of each unique lemma-POS pair in the corpus with the sense seeds. We eliminate the candidates which have less than 5 co-occurrences with the sense categories. 3.3.2 Normalized Poin"
D14-1160,W09-1127,0,0.0196435,"cy of the new seed list for sense classification by means of cross-validation against WordNet glosses. For each sense, we continue iterating until the cross-validation accuracy becomes stable or starts to decrease. The following sections explain the whole process in detail. 3.2.1 Extending the Seed List with WordNet While the initial sensory seed list obtained from FrameNet contains only 277 lemma-POS pairs, we extend this list by utilizing the semantic relations provided by WordNet. To achieve that, we first map each lemma-POS pair in the seed list to WordNet synsets with the help of MapNet (Tonelli and Pighin, 2009), which is a resource providing direct mapping between WordNet synsets and FrameNet lexical units. Then, we add to the list the synsets that have WordNet relations direct antonymy, similarity, derived-from, derivationally-related, pertains-to, attribute and also-see with the already existing seeds. For instance, we add the synset containing the verb laugh for the synset of the verb cry with the relation direct antonymy, or the synset containing the adjective chilly for the synset of the adjective cold with the relation similarity. We prefer to use these relations as they might allow us to pres"
D14-1160,D11-1063,0,0.201734,"om such a resource especially by using synaesthesia1 , as it strengthens creative thinking and it is commonly exploited as an imagination boosting tool in advertisement slogans (Pricken, 2008). As an example, we can consider the slogans “The taste of a paradise” where the sense of sight is combined with the sense of taste or “Hear the big picture” where sight and hearing are merged. Various studies have been conducted both in computational linguistics and cognitive science that build resources associating words with several cognitive features such as abstractnessconcreteness (Coltheart, 1981; Turney et al., 2011), emotions (Strapparava and Valitutti, 2004; Mohammad and Turney, 2010), colors (Özbal et al., 2011; Mohammad, 2011) and imageability (Coltheart, 1981). However, to the best of our knowledge, there is no attempt in the literature to build a resource that associates words with senses. In this paper, we propose a computational method to automatically generate a sensorial lexicon that associates words in English with senses. Our method consists of two main steps. First, we gen1 American Heritage Dictionary (http:// ahdictionary.com/) defines synaesthesia in linguistics as the description of one k"
D14-1160,S07-1009,0,0.206375,"ery high as expected, they have no association with the sense of smell. This kind of analysis could be useful for copywriters to decide which sensory modalities to invoke while creating a slogan for a specific product category. 4.2 Evaluation Measures Based on the annotation results of our crowdsourcing task, we propose an evaluation technique considering that a lemma-POS or a sentence might be associated with more than one sensory modalities. Similar to the evaluation framework defined by Özbal et al. (2011), we adapt the evaluation measures of SemEval-2007 English Lexical Substitution Task (McCarthy and Navigli, 2007), where Category Si He Ta Sm To personal care travel fashion beauty computing food beverages communications electronics education transport 49.36 58.18 43.47 84.56 32.25 0.00 22.68 25.00 45.94 28.57 61.81 10.75 0.00 0.00 0.00 59.13 5.46 0.00 67.50 54.05 42.85 38.18 0.00 29.09 0.00 0.00 0.00 94.53 59.79 0.00 0.00 0.00 0.00 13.29 0.00 26.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 26.58 12.72 30.43 15.43 8.60 0.00 17.52 0.075 0.00 28.57 0.00 story 58.37 20.81 0.00 7.23 13.57 Table 3: The categories of the annotated data and their sense association percentages. a system generates one or more possi"
D14-1160,P02-1053,0,0.010518,". For instance, Sensicon associates the noun wine with [smell, taste, sight]. In this experiment, best scoring considers the associated senses as the best answer, smell, taste, sight according to the previous example, and calculates a score with respect to the best answer in the gold standard and the number of the senses in this answer. Instead, oot scoring takes the first two answers, smell and taste according to the previous example, and assigns the score accordingly. To determine the senses associated with a sentence for the second experiment, we use a method similar to the one proposed by Turney (2002). For each sense, we simply calculate the average score of the lemma-POS pairs in a sentence. We set a threshold value of 0 to decide whether a sentence is associated with a given sense. In this manner, we obtain a sorted list of average sensory scores for each sentence according to the three methods. For instance, the classifier based on Sensicon associates the sentence Smash it to pieces, love it to bits. with [touch, taste]. For the best score, only touch would be considered, whereas oot would consider both touch and taste. 4.4 Evaluation Results In Table 4, we list the F1 values that we ob"
D14-1160,H93-1061,0,0.52768,"572 synsets yielding the highest performance when used to learn a sensorial model. 3.3 Sensorial Lexicon Construction Using Corpus Statistics After generating the seed lists consisting of synsets for each sense category with the help of a set of WordNet relations and a bootstrapping process, we use corpus statistics to create our final sensorial lexicon. More specifically, we exploit a probabilistic approach based on the co-occurrence of the seeds and the candidate lexical entries. Since working on the synset level would raise the data sparsity problem in synset tagged corpora such as SemCor (Miller et al., 1993) and we need a corpus that provides sufficient statistical information, we migrate from synset level to lexical level. Accordingly, we treat each POS role of the same lemmas as a distinct seed and extract 4287 lemma-POS pairs from 2572 synsets. In this section, we explain the steps to construct our final sensorial lexicon in detail. 3.3.1 Corpus and Candidate Words As a corpus, we use a subset of English GigaWord 5th Edition released by Linguistic Data Consortium (LDC)5 . This resource is a collection of almost 10 million English newswire documents collected in recent years, whose content sums"
D14-1160,W10-0204,0,0.198218,"ngthens creative thinking and it is commonly exploited as an imagination boosting tool in advertisement slogans (Pricken, 2008). As an example, we can consider the slogans “The taste of a paradise” where the sense of sight is combined with the sense of taste or “Hear the big picture” where sight and hearing are merged. Various studies have been conducted both in computational linguistics and cognitive science that build resources associating words with several cognitive features such as abstractnessconcreteness (Coltheart, 1981; Turney et al., 2011), emotions (Strapparava and Valitutti, 2004; Mohammad and Turney, 2010), colors (Özbal et al., 2011; Mohammad, 2011) and imageability (Coltheart, 1981). However, to the best of our knowledge, there is no attempt in the literature to build a resource that associates words with senses. In this paper, we propose a computational method to automatically generate a sensorial lexicon that associates words in English with senses. Our method consists of two main steps. First, we gen1 American Heritage Dictionary (http:// ahdictionary.com/) defines synaesthesia in linguistics as the description of one kind of sense impression by using words that normally describe another."
D14-1160,W11-0611,0,0.0924468,"s an imagination boosting tool in advertisement slogans (Pricken, 2008). As an example, we can consider the slogans “The taste of a paradise” where the sense of sight is combined with the sense of taste or “Hear the big picture” where sight and hearing are merged. Various studies have been conducted both in computational linguistics and cognitive science that build resources associating words with several cognitive features such as abstractnessconcreteness (Coltheart, 1981; Turney et al., 2011), emotions (Strapparava and Valitutti, 2004; Mohammad and Turney, 2010), colors (Özbal et al., 2011; Mohammad, 2011) and imageability (Coltheart, 1981). However, to the best of our knowledge, there is no attempt in the literature to build a resource that associates words with senses. In this paper, we propose a computational method to automatically generate a sensorial lexicon that associates words in English with senses. Our method consists of two main steps. First, we gen1 American Heritage Dictionary (http:// ahdictionary.com/) defines synaesthesia in linguistics as the description of one kind of sense impression by using words that normally describe another. erate a set of seed words for each sense cate"
D14-1160,P07-2034,0,0.0246249,"dge there is no attempt in the literature to automatically associate words with human senses, in this section we will summarize the most relevant studies that focused on linking words with various other cognitive features. There are several studies focusing on wordemotion associations. WordNet Affect Lexicon (Strapparava and Valitutti, 2004) maps WordNet (Fellbaum, 1998) synsets to various cognitive features (e.g., emotion, mood, behaviour). This resource is created by using a small set of synsets as seeds and expanding them with the help of semantic and lexical relations among these synsets. Yang et al. (2007) propose a collocation model with emoticons instead of seed words while creating an emotion lexicon from a corpus. Perrie et al. (2013) build a word-emotion association lexicon by using subsets of a human-annotated lexicon as seed sets. The authors use frequencies, counts, or unique seed words extracted from an n-gram corpus to create lexicons in different sizes. They pro1512 2 http://www.crowdflower.com/ pose that larger lexicons with less accurate generation method perform better than the smaller human annotated lexicons. While a major drawback of manually generated lexicons is that they req"
D14-1160,strapparava-valitutti-2004-wordnet,1,0.8317,"using synaesthesia1 , as it strengthens creative thinking and it is commonly exploited as an imagination boosting tool in advertisement slogans (Pricken, 2008). As an example, we can consider the slogans “The taste of a paradise” where the sense of sight is combined with the sense of taste or “Hear the big picture” where sight and hearing are merged. Various studies have been conducted both in computational linguistics and cognitive science that build resources associating words with several cognitive features such as abstractnessconcreteness (Coltheart, 1981; Turney et al., 2011), emotions (Strapparava and Valitutti, 2004; Mohammad and Turney, 2010), colors (Özbal et al., 2011; Mohammad, 2011) and imageability (Coltheart, 1981). However, to the best of our knowledge, there is no attempt in the literature to build a resource that associates words with senses. In this paper, we propose a computational method to automatically generate a sensorial lexicon that associates words in English with senses. Our method consists of two main steps. First, we gen1 American Heritage Dictionary (http:// ahdictionary.com/) defines synaesthesia in linguistics as the description of one kind of sense impression by using words that"
D14-1160,C98-1013,0,\N,Missing
D16-1220,W04-2214,0,0.00951562,"domains (ds ) and normalized domains (dn ): These features reflect our intuition that there is a strong prior for some domains to be used as a source for metaphors. This notion is backed by ¨ the analysis of P RO M ETHEUS carried out by Ozbal et al. (2016). We also expect that words which are clearly out of context with respect to the rest of the sentence are more likely to be used as metaphors. The correlation between word and sentence domains described below aims to model such phenomenon. For each lemma-POS pair, we collect the domain information from WordNet Domains4 (Magnini et al., 2002, Bentivogli et al., 2004) for the standard 3 Counts are estimated on training folds. To reduce overfitting, lemmas are randomly sampled with a probability of 2/3. 4 We always select the first sense of the lemma-POS. 2062 Feature sets B# N∗ B ∪ N∗ N N N N N N N  i∗  c∗  m∗#  d∗s  d∗n  (ds ∪ dn )#  s∗ (N  (ds ∪ dn )) ∪ t# B C P R F 0.9 0.6 0.6 0.666 0.785 0.798 0.832 0.884 0.875 0.738 0.833 0.834 0.6 0.6 0.6 1.0 1.0 1.0 1.0 0.788 0.782 0.780 0.787 0.789 0.746 0.776 0.886 0.888 0.824 0.842 0.884 0.704 0.909 0.833 0.831 0.799 0.815 0.832 0.724 0.836 0.6 0.751 0.705 0.724 Table 1: Cross-validation performance on th"
D16-1220,W14-2302,0,0.220024,"f our knowledge, this is the first attempt to design a word-level metaphor recognizer specifically tailored to such metaphorically rich data. Even though some of the resources that we use (e.g., imageability and concreteness) have been used for this task before, we propose new ways of encoding this information, especially with respect to the density of the feature space and the way that the context of each word is modeled. On the proverb data, the novel features result in compact models that significantly outperform existing features designed for word-level metaphor detection in other genres (Klebanov et al., 2014), such as news and essays. By also testing the new features on these other genres, we show that their generalization power is not limited to proverbs. 2 Background In this section we provide a brief overview of the efforts of the NLP community to build metaphor datasets and utilize them to develop computational techniques for metaphor processing. Steen et al. (2010) construct the Amsterdam Metaphor Corpus (VUAMC) by annotating a subset of BNC Baby1 . Linguistic metaphors in VUAMC are annotated by utilizing the Metaphor Annotation Procedure (MIP) proposed by Group (2007). VUAMC contains 200,000"
D16-1220,W15-1402,0,0.0692625,"evel metaphors given an unrestricted text, and they create a corpus containing sentences where one target token for each sentence is annotated as metaphorical or literal. They use SVM and CRF models with dependency tree-kernels to capture the anomalies in semantic patterns. Klebanov et al. (2014) propose a supervised approach to predict the metaphoricity of all content words in a running text. Their model combines unigram, topic model, POS and concreteness features and it is evaluated on VUAMC and a set of essays written for a large-scale assessment of college graduates. Following this study, Klebanov et al. (2015) improve their model by re-weighting the training examples and redesigning the concreteness features. The experiments in this paper are carried out on ¨ P RO M ETHEUS (Ozbal et al., 2016), a dataset consisting of 1,054 English proverbs and their equivalents in Italian. Proverbs are annotated with wordlevel metaphors, overall metaphoricity, meaning and century of first appearance. For our experiments, we only use the word-level annotations on the English data. 3 Word-level metaphor detection Similarly to Klebanov et al. (2014), we classify each content word (i.e., adjective, noun, verb or adver"
D16-1220,L16-1600,1,0.796173,"Missing"
D16-1220,shutova-teufel-2010-metaphor,0,0.0172899,"tekiroglu}@fbk.eu, biondo@google.com Abstract In this paper, we experiment with a resource consisting of metaphorically annotated proverbs on the task of word-level metaphor recognition. We observe that existing feature sets do not perform well on this data. We design a novel set of features to better capture the peculiar nature of proverbs and we demonstrate that these new features are significantly more effective on the metaphorically dense proverb data. 1 Introduction Recent years have seen a growing attention towards attempts to understand figurative language in text (Steen et al., 2010, Shutova and Teufel, 2010, Turney et al., 2011, Neuman et al., 2013, Klebanov ¨ et al., 2015). Recently, Ozbal et al. (2016) published a resource consisting of 1,054 proverbs annotated with metaphors at the word and sentence level, making it possible for the first time to test existing models for metaphor detection on such data. More than in other genres, such as news, fiction and essays, in proverbs metaphors can resolve a significant amount of the figurative meaning (Faycel, 2012). The richness of proverbs in terms of metaphors is very fascinating from a linguistic and cultural point of view. Due to this richness, p"
D16-1220,P14-1024,0,0.074112,"nary feature that encodes whether the lemma exists in the LDA model. 3.2 Novel features (N ) We introduce five feature sets that capture other aspects of the data which we consider to be meaningful for the peculiar characteristics of proverbs. 2 Klebanov et al. (2014) consider the Penn Treebank tagset generated by Stanford POS tagger. Imageability (i) and Concreteness (c): Imageability and concreteness of the metaphor constituents were found to be highly effective in metaphor identification by several studies in the literature (Turney et al., 2011, Broadwell et al., 2013, Neuman et al., 2013, Tsvetkov et al., 2014). We obtain the imageability and concreteness scores of each lemma from the resource constructed by Tsvetkov et al. (2014), as it accounts for both dimensions. The imageability (concreteness) feature set contains the following four features: • Has score: A binary feature that indicates whether the lemma exists in the relevant resource. • Score value: The imageability (concreteness) score of the lemma. • Average sentence score: The average imageability (concreteness) score of the other lemmas in the sentence. • Score difference: The difference between Average sentence score and Score value. The"
D16-1220,D11-1063,0,0.483736,"@google.com Abstract In this paper, we experiment with a resource consisting of metaphorically annotated proverbs on the task of word-level metaphor recognition. We observe that existing feature sets do not perform well on this data. We design a novel set of features to better capture the peculiar nature of proverbs and we demonstrate that these new features are significantly more effective on the metaphorically dense proverb data. 1 Introduction Recent years have seen a growing attention towards attempts to understand figurative language in text (Steen et al., 2010, Shutova and Teufel, 2010, Turney et al., 2011, Neuman et al., 2013, Klebanov ¨ et al., 2015). Recently, Ozbal et al. (2016) published a resource consisting of 1,054 proverbs annotated with metaphors at the word and sentence level, making it possible for the first time to test existing models for metaphor detection on such data. More than in other genres, such as news, fiction and essays, in proverbs metaphors can resolve a significant amount of the figurative meaning (Faycel, 2012). The richness of proverbs in terms of metaphors is very fascinating from a linguistic and cultural point of view. Due to this richness, proverbs constitute a"
D16-1220,N10-1147,0,\N,Missing
D16-1220,W13-0907,0,\N,Missing
D17-1286,C12-1025,0,0.0586323,"rote “All men are equal: but some are more equal than others”. How true is this today? In the words of the old song “Money is the root of all evil”. Europe. In the 19th century, Victor Hugo said: ”How sad it is to think that nature is calling out but humanity refuses to pay heed. ”Do you think it is still true nowadays ? Some people say that in our modern world, dominated by science technology and industrialization, there is no longer a place for dreaming and imagination. What is your opinion ? Table 2: Topics in the ICLE dataset. The suitability of the dataset above for NLI was questioned by Brooke and Hirst (2012). They have shown that the fact that the corpus consists of sets of essays on a number of topics causes an overes2704 Language Bulgarian Czech Dutch French German Italian Norwegian Polish Rusian Spanish Swedish Accuracy timation of the results of NLI when random splitting, particularly for groups of contributors that were presented with very different topics – e.g. students from Asia vs. students from Europe. We have analyzed the distribution of essays into topics using the essay titles, and observed that contributions from Europe (which are our focus) have similar distributions across the fea"
D17-1286,D11-1010,0,0.0203646,"rors caused by “etymological interference”; (iii) together with other interference phenomena, for the automatic corrections of language errors. 2 Related Work English is a widespread common language for communication in a variety of fields – science, news, entertainment, politics, etc. A consequence is that numerous people learn English as a second (or indeed nth language). The study of native language interference with the learning of English can be used in multiple ways, including devising methods to make the learning easier and correcting language errors (Leacock et al., 2014; Gamon, 2010; Dahlmeier and Ng, 2011). 2702 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2702–2707 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Massung and Zhai (2016) present an overview of approaches to the task of natural language identification (NLI). Various surface indicators hold clues about a speaker’s native language, that make their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts."
D17-1286,N10-1019,0,0.0244574,"cal choice errors caused by “etymological interference”; (iii) together with other interference phenomena, for the automatic corrections of language errors. 2 Related Work English is a widespread common language for communication in a variety of fields – science, news, entertainment, politics, etc. A consequence is that numerous people learn English as a second (or indeed nth language). The study of native language interference with the learning of English can be used in multiple ways, including devising methods to make the learning easier and correcting language errors (Leacock et al., 2014; Gamon, 2010; Dahlmeier and Ng, 2011). 2702 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2702–2707 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Massung and Zhai (2016) present an overview of approaches to the task of natural language identification (NLI). Various surface indicators hold clues about a speaker’s native language, that make their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the prod"
D17-1286,D14-1142,0,0.162594,"their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts. Tsur and Rappoport (2007) verify the hypothesis that lexical choice of non-native speakers is influenced by the phonology of their native language, and Wong and Dras (2009) propose the idea that (grammatical) errors are also influenced by the native language. One could draw the inference that character n-grams then could be indicative of the native language, and this was shown to be the case by Ionescu et al. (2014). The natural language identification task (Tetreault et al., 2013) attracted 29 participating teams, which used a variety of features to accomplish the NLI task as a classification exercise: n-grams of lexical tokens (words and POS tags), skip-grams, grammatical information (dependency parses, parse tree rules, preference for particular grammatical forms, e.g. active or passive voice), spelling errors. Apart from morphological, lexical, grammatical features, words also have an etymological dimension. The language family tree itself is drawn based on the analysis of the evolution of languages."
D17-1286,W13-1732,0,0.465352,"Missing"
D17-1286,W15-0606,0,0.0141736,"s, grammatical information (dependency parses, parse tree rules, preference for particular grammatical forms, e.g. active or passive voice), spelling errors. Apart from morphological, lexical, grammatical features, words also have an etymological dimension. The language family tree itself is drawn based on the analysis of the evolution of languages. Language evolution includes, or starts with, word etymologies. Word etymologies have been under-used for tasks related to NLI. They have been used implicitly in work that investigates cognate interference (Nicolai et al., 2013), and explicitly by (Malmasi and Cahill, 2015) who use words with Old English and Latin etymologies as unigram features in building classifiers for the T OEFL 11 dataset. Etymological information is obtained from the Etymological WordNet (de Melo and Weikum, 2010). We also investigate here the impact of etymological information, but unlike previous work, we do not extract unigram/n-gram features for classification, but we look at the collective evidence captured by the etymological “fingerprint” for each document and set of essays. 3 Etymological fingerprints To investigate the influence of etymological ancestor languages, we represent ea"
D17-1286,C14-1183,0,0.0205578,"ke the learning easier and correcting language errors (Leacock et al., 2014; Gamon, 2010; Dahlmeier and Ng, 2011). 2702 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2702–2707 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Massung and Zhai (2016) present an overview of approaches to the task of natural language identification (NLI). Various surface indicators hold clues about a speaker’s native language, that make their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts. Tsur and Rappoport (2007) verify the hypothesis that lexical choice of non-native speakers is influenced by the phonology of their native language, and Wong and Dras (2009) propose the idea that (grammatical) errors are also influenced by the native language. One could draw the inference that character n-grams then could be indicative of the native language, and this was shown to be the case by Ionescu et al. (2014). The natural language identification task (Tetreault et al., 2013) attracted 29"
D17-1286,P13-1112,0,0.166073,"uding devising methods to make the learning easier and correcting language errors (Leacock et al., 2014; Gamon, 2010; Dahlmeier and Ng, 2011). 2702 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2702–2707 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Massung and Zhai (2016) present an overview of approaches to the task of natural language identification (NLI). Various surface indicators hold clues about a speaker’s native language, that make their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts. Tsur and Rappoport (2007) verify the hypothesis that lexical choice of non-native speakers is influenced by the phonology of their native language, and Wong and Dras (2009) propose the idea that (grammatical) errors are also influenced by the native language. One could draw the inference that character n-grams then could be indicative of the native language, and this was shown to be the case by Ionescu et al. (2014). The natural language identification task (Tetreault et al., 2013)"
D17-1286,W13-1718,0,0.730005,"the native language such as pronunciation, vocabulary and grammar are well-attested, and the phenomenon is called native language interference (Odlin, 1989). At the lexical level, the choice as well as the spelling can be indicative of the native language, through the choice of cognates, true or false friends – e.g. a writer with native language German may choose bloom cognate with blume, while a French one may choose flower, cognate with fleur. Misspellings – cuestion instead of question are also indicative, as the writer will tend to spell words close to the form from her original language (Nicolai et al., 2013). In this paper we also look at native language interference starting from the lexical level, but abstract away from the actual word forms, and focus instead on the language of the etymological ancestors. The hypothesis we investigate is that the collective evidence of etymological ancestor languages are indicative of the language of the native speaker, and that this effect is sufficiently strong to allow us to rebuild an Indo-European language family tree. We use a corpus of essays written by English language learners, whose native language cover the languages from the Indo-European family. E"
D17-1286,W07-0602,0,0.102212,". 2702 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2702–2707 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Massung and Zhai (2016) present an overview of approaches to the task of natural language identification (NLI). Various surface indicators hold clues about a speaker’s native language, that make their way into language production in a non-native language. Nagata and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts. Tsur and Rappoport (2007) verify the hypothesis that lexical choice of non-native speakers is influenced by the phonology of their native language, and Wong and Dras (2009) propose the idea that (grammatical) errors are also influenced by the native language. One could draw the inference that character n-grams then could be indicative of the native language, and this was shown to be the case by Ionescu et al. (2014). The natural language identification task (Tetreault et al., 2013) attracted 29 participating teams, which used a variety of features to accomplish the NLI task as a classification exercise: n-grams of lex"
D17-1286,W13-1706,0,0.284714,"ta and Whittaker (2013),Nagata (2014) find that grammatical patterns from the native language seep into the production of English texts. Tsur and Rappoport (2007) verify the hypothesis that lexical choice of non-native speakers is influenced by the phonology of their native language, and Wong and Dras (2009) propose the idea that (grammatical) errors are also influenced by the native language. One could draw the inference that character n-grams then could be indicative of the native language, and this was shown to be the case by Ionescu et al. (2014). The natural language identification task (Tetreault et al., 2013) attracted 29 participating teams, which used a variety of features to accomplish the NLI task as a classification exercise: n-grams of lexical tokens (words and POS tags), skip-grams, grammatical information (dependency parses, parse tree rules, preference for particular grammatical forms, e.g. active or passive voice), spelling errors. Apart from morphological, lexical, grammatical features, words also have an etymological dimension. The language family tree itself is drawn based on the analysis of the evolution of languages. Language evolution includes, or starts with, word etymologies. Wor"
D18-1367,J08-4004,0,0.156854,"Missing"
D18-1367,D14-1162,0,0.08169,"ms frequently occur, as well as contrasts and similarities among their meanings. In fact, according to the Distributional Hypothesis (Harris, 1954), lexical items appearing in similar contexts tend to have similar meanings. Our expectation is that the words of a figurative sentence carry less similar meanings compared to those of a literal instance, and hence, that their vectorial representations turn out to be more distant. For every sentence in the dataset, we map its words onto both the pre-trained vectors of Mikolov et al. (2013), obtained from the Skipgram model, and the GloVe vectors by Pennington et al. (2014). Then, we consider the cosine distance between all possible word pairs to score their semantic similarity. The unexpectedness feature of a sentence is found in two ways: as the average similarity among all of its word pairs, and as the lowest of those pair similarities. Both measures are separately computed with Skip-Gram and GloVe vectors, resulting in 4 scores. Polarity corresponds to the sentiment of a statement. It is extracted through both TextBlob (Loria, 2014), a system that directly scores sentences, and SentiWords (Gatti et al., 2016), which lists polarity values for 155k POS-tagged"
D18-1367,P14-1024,0,0.0664735,"or the quantity group, the notion that hyperboles say more of X when X is the case is decomposed into two features, i.e. imageability and unexpectedness, while the qualitative marker of hyperboles, or the view that they shape a speaker’s perspective about X, is rendered by the polarity, the subjectivity and the emotional strength of sentences. Imageability is the degree to which a word can evoke a mental image. Speakers hyperbolize to convey meanings with strength, and we assumed that such a goal might be backed by a highly picturable vocabulary. This feature is extracted from the resource of Tsvetkov et al. (2014), who propagated the imageability ratings of the MRC psycholinguistic database to 150.114 terms. For each sentence, we averaged the imageability values of all its words. Unexpectedness refers to the fact that hyperboles are less predictable expressions than literals. Basically, minimal hyperbolic units modify the real characteristics of X, and in this sense, they are incoherent with the rest of discourse about X: they are out of context, and come unexpected to the hearers of overstatements. We conjecture that word vectors may capture if an expression is being used “unexpectedly” because they e"
E17-2022,N13-1121,0,0.0436872,"Missing"
E17-2022,P11-1137,0,0.0143765,"differentiated by drugs. Our research is also related to the broad theme of latent user attribute prediction, which is an emerging task within the natural language processing community, having recently been employed in fields such as public health (Coppersmith et al., 2015) and politics (Conover et al., 2011; Cohen and Ruths, 2013). Some of the attributes targeted for extraction focus on demographic related information, such as gender/age (Koppel et al., 2002; Mukherjee and Liu, 2010; Burger et al., 2011; Van Durme, 2012; Volkova et al., 2015), race/ethnicity (Pennacchiotti and Popescu, 2011; Eisenstein et al., 2011; Rao et al., 2011; Volkova et al., 2015), location (Bamman et al., 2014), yet other aspects are mined as well, among them emotion and sentiment (Volkova et al., 2015), personality types (Schwartz et al., 2013; Volkova et al., 2015), user political affiliation (Cohen and Ruths, 2013; Volkova and Durme, 2015), mental health diagnosis (Coppersmith et al., 2015) and even lifestyle choices such as coffee preference (Pennacchiotti and Popescu, 2011). The task is typically approached from a machine learning perspective, with data originating from a variety of user generated content, most often micro"
E17-2022,D12-1135,0,0.0192008,"n et al., 2014), social posts (originating from sites such as Facebook, MySpace, Google+) (Gong et al., 2012), or discussion forums on particular topics (Gottipati et al., 2014). Classification labels are then assigned either based on manual annotations (Volkova et al., 2015), self identified user attributes (Pennacchiotti and Popescu, 2011), affiliation with a given discussion forum type, or online surveys set up to link a social media user identification to the responses provided (Schwartz et al., 2013). Learning has typically employed bagof-words lexical features (ngrams) (Van Durme, 2012; Filippova, 2012; Nguyen et al., 2013), with some works focusing on deriving additional signals from the underlying social network structure (Pennacchiotti and Popescu, 2011; Yang et al., 2011; Gong et al., 2012; Volkova and Durme, 2015), syntactic and stylistic features (Bergsma et al., 2012), or the intrinsic social media generation dynamic (Volkova and Durme, 2015). We should note that some works have also explored unsupervised approaches for demographic dimensions extraction, among them large-scale clustering (Bergsma et al., 2013) and probabilistic graphical models (Eisenstein et al., 2010). of research"
E17-2022,D11-1120,0,0.0366011,"classifier over 1,000 random-collected reports of the website www.erowid.org they identified subsets of words differentiated by drugs. Our research is also related to the broad theme of latent user attribute prediction, which is an emerging task within the natural language processing community, having recently been employed in fields such as public health (Coppersmith et al., 2015) and politics (Conover et al., 2011; Cohen and Ruths, 2013). Some of the attributes targeted for extraction focus on demographic related information, such as gender/age (Koppel et al., 2002; Mukherjee and Liu, 2010; Burger et al., 2011; Van Durme, 2012; Volkova et al., 2015), race/ethnicity (Pennacchiotti and Popescu, 2011; Eisenstein et al., 2011; Rao et al., 2011; Volkova et al., 2015), location (Bamman et al., 2014), yet other aspects are mined as well, among them emotion and sentiment (Volkova et al., 2015), personality types (Schwartz et al., 2013; Volkova et al., 2015), user political affiliation (Cohen and Ruths, 2013; Volkova and Durme, 2015), mental health diagnosis (Coppersmith et al., 2015) and even lifestyle choices such as coffee preference (Pennacchiotti and Popescu, 2011). The task is typically approached fro"
E17-2022,P15-2100,0,0.0306003,"e should note that some works have also explored unsupervised approaches for demographic dimensions extraction, among them large-scale clustering (Bergsma et al., 2013) and probabilistic graphical models (Eisenstein et al., 2010). of research on the state of consciousness are focused on alcoholic intoxication and mostly performed on the Alcohol Language Corpus (Schiel et al., 2012), only available in German: for example, speech analysis (Wang et al., 2013; Bone et al., 2014) and a text based system (Jauch et al., 2013) were used to analyse this data. Regarding alcohol intoxication detection, (Joshi et al., 2015) developed a system for automatic detection of drunk people by using their posts on Twitter. (Bedi et al., 2014) performed their analysis on transcriptions from a free speech task, in which the participants were volunteers previously administered with a dose of MDMA (3,4methylenedioxy-methamphetamine). Even if this is an ideal case study for analyzing cognitively the intoxication state, it is difficult to replicate on a large scale. Finally, as far as we know, the only attempt to classify and characterize experiences over different kinds of drugs was the project of (Coyle et al., 2012). Using"
E17-2022,W15-1201,0,0.0244675,"ication state, it is difficult to replicate on a large scale. Finally, as far as we know, the only attempt to classify and characterize experiences over different kinds of drugs was the project of (Coyle et al., 2012). Using a random-forest classifier over 1,000 random-collected reports of the website www.erowid.org they identified subsets of words differentiated by drugs. Our research is also related to the broad theme of latent user attribute prediction, which is an emerging task within the natural language processing community, having recently been employed in fields such as public health (Coppersmith et al., 2015) and politics (Conover et al., 2011; Cohen and Ruths, 2013). Some of the attributes targeted for extraction focus on demographic related information, such as gender/age (Koppel et al., 2002; Mukherjee and Liu, 2010; Burger et al., 2011; Van Durme, 2012; Volkova et al., 2015), race/ethnicity (Pennacchiotti and Popescu, 2011; Eisenstein et al., 2011; Rao et al., 2011; Volkova et al., 2015), location (Bamman et al., 2014), yet other aspects are mined as well, among them emotion and sentiment (Volkova et al., 2015), personality types (Schwartz et al., 2013; Volkova et al., 2015), user political af"
E17-2022,P09-2078,1,0.775812,"ycholinguistic word classes according to the Linguistic Inquiry and Word Count (LIWC) lexicon – a resource developed by Pennebaker and colleagues (Pennebaker and Francis, 1999). The 2015 version of LIWC includes 19,000 words and word stems grouped into 73 broad categories relevant to psychological processes. The LIWC lexicon has been validated by showing significant correlation between human ratings of a large number of written texts and the rating obtained through LIWC-based analyses of the same texts. For each drug type T , we calculate the dominance score associated with each LIWC class C (Mihalcea and Strapparava, 2009). This score is calculated as the ratio between the percentage of words that appear in T and belong to C, and the percentage of words that appear in any other drug type but T and belong to C. A score significantly higher than 1 indicates a LIWC class that is dominant for the drug type T , and thus likely to be a characteristic of the experiences reported by users of this drug. Table 5 shows the top five dominant psycholinguistic word classes associated with each drug type. Interestingly, descriptions of experiences reported by users of empathogens are centered around people (e.g., Affiliation"
E17-2022,D10-1124,0,0.117264,"Missing"
E17-2022,strapparava-valitutti-2004-wordnet,1,0.31404,"ening I do a line whenever I feel like it. At bedtime I tell myself over and over that it’s time to go to sleep. Sometimes I sleep but if I can’t I know I have my friend to help me through the next day. Table 2: Sample entries in the drug dataset. EMP HAL SED STI micro-average Prec. 0.84 0.93 0.86 0.73 Rec. 0.71 0.92 0.86 0.85 F1 0.77 0.92 0.86 0.78 0.88 methodology similar to the one described above, and calculate the dominance score for each of six emotion word classes: anger, disgust, fear, joy, sadness, and surprise (Ortony et al., 1987; Ekman, 1993). As a resource, we use WordNet Affect (Strapparava and Valitutti, 2004), in which words from WordNet are annotated with several emotions. As before, the dominance scores are calculated for the experiences reported for each drug type when compared to the other drug types. Table 7 shows the scores for the four drug types and the six emotions. A score significantly higher than 1 indicates a class that is dominant in that category. Clearly, interesting differences emerge from this table: the use of emphathogens leads to experiences that are high on joy and surprise, whereas the dominant emotion in the use of hallucinogens as compared to the other drugs is fear. Sedat"
E17-2022,D12-1005,0,0.04849,"Missing"
E17-2022,N13-1017,0,0.0713576,"Missing"
E17-2022,W14-2701,0,0.0120002,"n directions 1 www.erowid.org: 95000 unique visitor per day; www.drugs-forum.com: 210000 members with 3.6 million unique visitor per month; www.psychonaut.com: 46000 members. 2 http://medicine.wright.edu/citar/edrugtrends http://medicine.wright.edu/citar/nida-national-earlywarning-system-network-in3-an-innovative-approach 3 136 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 136–142, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ments to news stories or op-ed pieces (Riordan et al., 2014), social posts (originating from sites such as Facebook, MySpace, Google+) (Gong et al., 2012), or discussion forums on particular topics (Gottipati et al., 2014). Classification labels are then assigned either based on manual annotations (Volkova et al., 2015), self identified user attributes (Pennacchiotti and Popescu, 2011), affiliation with a given discussion forum type, or online surveys set up to link a social media user identification to the responses provided (Schwartz et al., 2013). Learning has typically employed bagof-words lexical features (ngrams) (Van Durme, 2012; Filippova, 2012"
E17-2022,P14-2134,0,\N,Missing
E17-2048,P14-5010,0,0.00261821,"al the “memorable” part of a song (i.e., the part that most people are supposed to quickly recognize). We did this annotation manually for each entry in the corpus, but this step could also be automatized, in case new songs need to be added (Eronen, 2007). 4 1) Key concepts from the news The process starts by downloading the news of the day from important news providers, such as the BBC and the New York Times. Each news article is composed of a headline and a short summary describing its content. Both the headline and the summary are lemmatized and PoS-tagged using the Stanford CoreNLP suite (Manning et al., 2014), which also identifies any named entity present in the text. The system then discards all the irrelevant tokens and lemmas by removing stop words and keeping only the words that are more characteristic of the specific text, appearing less frequently in a news corpus (Parker et al., 2011). All the named entities are considered relevant, and thus are never removed. As an example, let us take the headline “Mom protects 2-year-old daughter by biting off dog’s ear”, where the system will identify the nouns “mom”, “dog” and “ear” and the verb “to bite” as characterizing words. Algorithm The parody"
E17-2048,D12-1054,1,0.761476,"014), which focuses on creating a graphic rendition of each parodied ad. Lexical substitution has also been commonly used by various studies focusing on humor generation. Stock and Strapparava (2006) generate acronyms based on lexical substitution via semantic field opposition, rhyme, rhythm and semantic relations provided by WordNet. The proposed model is limited to the generation of noun phrases. Valitutti et al. (2009) present an interactive system which generates humorous puns obtained by modifying familiar ex3 Corpus For this work we used the corpus developed by Strapparava and Mihalcea (Mihalcea and Strapparava, 2012). The corpus contains 100 popular songs (e.g., Dancing Queen by ABBA, Hotel California by the Eagles, Alejandro by Lady Gaga), where the notes of the melody are strictly aligned with the corresponding syllables in the lyrics. The genres of the songs fall mainly into pop, rock and evergreen. The corpus was built by aligning the melody contained within the MIDI tracks4 of a song with its lyrics. In the corpus, several features are present for each song. In the first place, the key of the song (e.g., G major, C minor). At the note level: the time code of the note with respect to the beginning of"
E17-2048,esuli-sebastiani-2006-sentiwordnet,0,0.0208537,"in the sentences. B RAIN S UP makes heavy use of syntactic information to enforce well-formed sentences and to constraint the search for a solution, and provides an extensible framework in which various forms of linguistic creativity can easily be incorporated. The authors evaluate the proposed model on automatic slogan generation. As a study focusing on the modification of linguistic expressions, the system called Valentino (Guerini et al., 2011) slants existing textual expressions to obtain more positively or negatively valenced versions by using WordNet semantic relations and SentiWordNet (Esuli and Sebastiani, 2006). The slanting is carried out by modifying, adding or deleting single words from existing sentences. Insertion and deletion of words is performed by utilizing Google Web 1T 5-Grams Corpus to extract information about the modifiers of terms based on their part-of-speech. Valentino has also been used to spoof existing ads by exaggerating them, as described in (Gatti et al., 2014), which focuses on creating a graphic rendition of each parodied ad. Lexical substitution has also been commonly used by various studies focusing on humor generation. Stock and Strapparava (2006) generate acronyms based"
E17-2048,P13-1142,1,0.82537,"g to a userprovided desired rhyming and stress scheme. The model is applied to translation, making it possible to generate translations that conform to the desired meter. Toivanen et al. (2012) propose to generate novel poems by replacing words in existing poetry with morphologically compatible words that are semantically related to a target domain. Content control and the inclusion of phonetic features are left as future work and syntactic information is not taken into account. Recently, some attempt has been made to generate creative sentences for educational and adver¨ tising applications. Ozbal et al. (2013) propose an extensible framework called B RAIN S UP for the generation of creative sentences in which users are able to force several words to appear in the sentences. B RAIN S UP makes heavy use of syntactic information to enforce well-formed sentences and to constraint the search for a solution, and provides an extensible framework in which various forms of linguistic creativity can easily be incorporated. The authors evaluate the proposed model on automatic slogan generation. As a study focusing on the modification of linguistic expressions, the system called Valentino (Guerini et al., 2011"
E17-2048,D10-1051,0,0.0357417,"l2 for the company GoldieBlox (that produces toys for girls). This parody modifies the lyrics of the song to promote less “gender-stereotypical” toys. As it often happens in these cases, the video quickly went viral (Fell, 2013). The same song 1 2 2 Related Works Much of lyric writing is technical and it certainly falls under the area of creative writing. Computational linguistics has recently advanced into the field of computational creativity. Poetry generation systems face similar challenges to ours as they struggle to combine semantic, lexical and phonetic features in a unified framework. Greene et al. (2010) describe a model for poetry generation in which users can control http://youtu.be/0e8j3-TuzCs http://youtu.be/M0NoOtaFrEs 3 http://youtu.be/bRqW4PxipG4 298 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 298–304, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics pressions with word substitution. The modification takes place considering the phonetic distance between the replaced and candidate words, and semantic constraints such as semantic similarity, domain opposition a"
E17-2048,P13-2044,0,0.0286464,"ation in which users can control http://youtu.be/0e8j3-TuzCs http://youtu.be/M0NoOtaFrEs 3 http://youtu.be/bRqW4PxipG4 298 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 298–304, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics pressions with word substitution. The modification takes place considering the phonetic distance between the replaced and candidate words, and semantic constraints such as semantic similarity, domain opposition and affective polarity difference. Valitutti et al. (2013) propose an approach based on lexical substitution to introduce adult humor in SMS texts. A “taboo” word is injected in an existing sentence to make it humorous. As another application of Optimal Innovation Hypotesis, (Gatti et al., 2015) present a system that produces catchy news headlines. The methodology takes existing well-known expressions and innovates them by inserting a novel concept coming from evolving news. Finally, regarding our specific task of generating song parodies, we notice that in advertising, music is a widely used element to improve the recall of the advertised product, a"
gella-etal-2014-mapping,perez-rosas-etal-2012-learning,0,\N,Missing
gella-etal-2014-mapping,W12-4210,0,\N,Missing
gella-etal-2014-mapping,fernando-stevenson-2012-mapping,0,\N,Missing
gella-etal-2014-mapping,magnini-cavaglia-2000-integrating,0,\N,Missing
guerini-etal-2008-resources,W98-0718,0,\N,Missing
guerini-etal-2008-resources,guerini-etal-2008-valentino,1,\N,Missing
guerini-etal-2008-resources,W00-1408,0,\N,Missing
guerini-etal-2008-valentino,guerini-etal-2008-resources,1,\N,Missing
guerini-etal-2008-valentino,P98-1013,0,\N,Missing
guerini-etal-2008-valentino,C98-1013,0,\N,Missing
guerini-etal-2008-valentino,W06-2915,0,\N,Missing
guerini-etal-2008-valentino,P02-1053,0,\N,Missing
guerini-etal-2008-valentino,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
guerini-etal-2010-evaluation,guerini-etal-2008-valentino,1,\N,Missing
guerini-etal-2010-evaluation,D08-1027,0,\N,Missing
H05-1017,W99-0613,0,0.250205,"ct the required amounts of hand labeled data. Unlabeled text collections, on the other hand, are in general easily available. An alternative approach is to provide the necessary supervision by means of sets of “seeds” of intuitively relevant features. Adopting terminology The IL approach reflects on classical rule-based classification methods, where the user is expected to specify exact classification rules that operate in the feature space. Within the machine learning paradigm, IL has been incorporated as a technique for bootstrapping an extensional learning algorithm, as in (Yarowsky, 1995; Collins and Singer, 1999; Liu et al., 2004). This way the user does not need to specify exact classification rules (and feature weights), but rather perform a somewhat simpler task of specifying few typical seed features for the category. Given the list of seed features, the bootstrapping scheme consists of (i) preliminary unsupervised categorization of the unlabeled data set based on the seed features, and (ii) training an (extensional) supervised classifier using the automatic classification labels of step (i) as the training data (the second step is possibly reiterated, such as by an Expectation-Maximization schem"
H05-1017,C00-1066,0,0.0228791,"ng for Text Categorization The TC task is to assign category labels to documents. In the IL setting, a category Ci is described by providing a set of relevant features, termed an intensional description (ID), idci ⊆ V , where V is the vocabulary. In addition a training corpus T = {t1 , t2 , . . . tn } of unlabeled texts is provided. Evaluation is performed on a separate test corpus of labeled documents, to which standard evaluation metrics can be applied. The approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (McCallum and Nigam, 1999; Ko and Seo, 2000; Liu et al., 2004; Ko and Seo, 2004). Several names have been proposed for it – such as TC by bootstrapping with keywords, unsupervised TC, TC by labelling words – where the proposed methods 130 fall (mostly) within the IL settings described here1 . It is possible to recognize a common structure of these works, based on a typical bootstrap schema (Yarowsky, 1995; Collins and Singer, 1999): Step 1: Initial unsupervised categorization. This step was approached by applying some similarity criterion between the initial category seed and each unlabeled document. Similarity may be determined as a b"
H05-1017,P04-1033,0,0.365294,"sk is to assign category labels to documents. In the IL setting, a category Ci is described by providing a set of relevant features, termed an intensional description (ID), idci ⊆ V , where V is the vocabulary. In addition a training corpus T = {t1 , t2 , . . . tn } of unlabeled texts is provided. Evaluation is performed on a separate test corpus of labeled documents, to which standard evaluation metrics can be applied. The approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (McCallum and Nigam, 1999; Ko and Seo, 2000; Liu et al., 2004; Ko and Seo, 2004). Several names have been proposed for it – such as TC by bootstrapping with keywords, unsupervised TC, TC by labelling words – where the proposed methods 130 fall (mostly) within the IL settings described here1 . It is possible to recognize a common structure of these works, based on a typical bootstrap schema (Yarowsky, 1995; Collins and Singer, 1999): Step 1: Initial unsupervised categorization. This step was approached by applying some similarity criterion between the initial category seed and each unlabeled document. Similarity may be determined as a binary criterion, considering each see"
H05-1017,W99-0908,0,0.708007,"re research. 2 Bootstrapping for Text Categorization The TC task is to assign category labels to documents. In the IL setting, a category Ci is described by providing a set of relevant features, termed an intensional description (ID), idci ⊆ V , where V is the vocabulary. In addition a training corpus T = {t1 , t2 , . . . tn } of unlabeled texts is provided. Evaluation is performed on a separate test corpus of labeled documents, to which standard evaluation metrics can be applied. The approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (McCallum and Nigam, 1999; Ko and Seo, 2000; Liu et al., 2004; Ko and Seo, 2004). Several names have been proposed for it – such as TC by bootstrapping with keywords, unsupervised TC, TC by labelling words – where the proposed methods 130 fall (mostly) within the IL settings described here1 . It is possible to recognize a common structure of these works, based on a typical bootstrap schema (Yarowsky, 1995; Collins and Singer, 1999): Step 1: Initial unsupervised categorization. This step was approached by applying some similarity criterion between the initial category seed and each unlabeled document. Similarity may be"
H05-1017,P95-1026,0,0.231154,"fficult to collect the required amounts of hand labeled data. Unlabeled text collections, on the other hand, are in general easily available. An alternative approach is to provide the necessary supervision by means of sets of “seeds” of intuitively relevant features. Adopting terminology The IL approach reflects on classical rule-based classification methods, where the user is expected to specify exact classification rules that operate in the feature space. Within the machine learning paradigm, IL has been incorporated as a technique for bootstrapping an extensional learning algorithm, as in (Yarowsky, 1995; Collins and Singer, 1999; Liu et al., 2004). This way the user does not need to specify exact classification rules (and feature weights), but rather perform a somewhat simpler task of specifying few typical seed features for the category. Given the list of seed features, the bootstrapping scheme consists of (i) preliminary unsupervised categorization of the unlabeled data set based on the seed features, and (ii) training an (extensional) supervised classifier using the automatic classification labels of step (i) as the training data (the second step is possibly reiterated, such as by an Expe"
H05-1017,W05-0608,1,\N,Missing
I13-1040,P12-2051,0,0.128012,"Missing"
I13-1040,W10-0204,0,0.0183233,"agoya, Japan, 14-18 October 2013. compiled for the political and sociological domain publicly available1 . The frequency of these terms and their covariance is analyzed over the years and non-random changes are found according to the methodology presented in Section 3. The methodology itself is purely statistical and it does not depend in any way on what the list contains. We could have equally chosen terms from art or sport domain, obtaining epoch boundaries specific to each domain. The emotion words used in epoch characterization come primarily from the NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2010) to which the list of emotion words extracted from WordNet-Affect (Strapparava and Valitutti, 2004), distributed in the Semeval 2007 Affective Text task (Strapparava and Mihalcea, 2007), has been added. The lexicon is made up of English words to which eight possible tags are attached: anger, anticipation, disgust, fear, joy, sadness, surprise and trust. All in all there are 14,000 words for which at least one affective tag is given. The paper is organized as follows. In Section 2 we review the relevant literature. Section 3 presents the statistical apparatus employed in epoch determination and"
I13-1040,S07-1013,1,0.921977,"ars and non-random changes are found according to the methodology presented in Section 3. The methodology itself is purely statistical and it does not depend in any way on what the list contains. We could have equally chosen terms from art or sport domain, obtaining epoch boundaries specific to each domain. The emotion words used in epoch characterization come primarily from the NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2010) to which the list of emotion words extracted from WordNet-Affect (Strapparava and Valitutti, 2004), distributed in the Semeval 2007 Affective Text task (Strapparava and Mihalcea, 2007), has been added. The lexicon is made up of English words to which eight possible tags are attached: anger, anticipation, disgust, fear, joy, sadness, surprise and trust. All in all there are 14,000 words for which at least one affective tag is given. The paper is organized as follows. In Section 2 we review the relevant literature. Section 3 presents the statistical apparatus employed in epoch determination and epoch characterization. In Section 4 we present the experiments and the results we have obtained. In the last section we highlight the contribution of this paper and make an overview o"
I13-1040,strapparava-valitutti-2004-wordnet,1,0.330976,"available1 . The frequency of these terms and their covariance is analyzed over the years and non-random changes are found according to the methodology presented in Section 3. The methodology itself is purely statistical and it does not depend in any way on what the list contains. We could have equally chosen terms from art or sport domain, obtaining epoch boundaries specific to each domain. The emotion words used in epoch characterization come primarily from the NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2010) to which the list of emotion words extracted from WordNet-Affect (Strapparava and Valitutti, 2004), distributed in the Semeval 2007 Affective Text task (Strapparava and Mihalcea, 2007), has been added. The lexicon is made up of English words to which eight possible tags are attached: anger, anticipation, disgust, fear, joy, sadness, surprise and trust. All in all there are 14,000 words for which at least one affective tag is given. The paper is organized as follows. In Section 2 we review the relevant literature. Section 3 presents the statistical apparatus employed in epoch determination and epoch characterization. In Section 4 we present the experiments and the results we have obtained."
J09-4007,W04-0827,0,0.0446019,"Missing"
J09-4007,W06-2608,1,0.867902,"Missing"
J09-4007,E06-1051,1,0.830508,"Missing"
J09-4007,P05-1050,1,0.785313,"Missing"
J09-4007,magnini-cavaglia-2000-integrating,0,0.0627202,"Missing"
J09-4007,P04-1043,0,0.0343892,"typically used as features in WSD. 3.3 Composite Kernel Having deﬁned all the individual kernels representing syntagmatic and domain aspects of sense distinction, we can deﬁne the composite kernel to combine and extend the individual kernels. The closure properties of the kernel functions allows us to deﬁne the composite kernel as n  KC (xi , xj ) = l =1 Kl (xi , xj )  Kl (xj , xj )Kl (xi , xi ) (19) where Kl is a valid individual kernel. The individual kernels are normalized—this plays an important role in allowing us to integrate information from heterogeneous feature spaces. Recent work (Moschitti 2004; Gliozzo, Giuliano, and Strapparava 2005; Zhao and Grishman 2005; Giuliano, Lavelli, and Romano 2006) has empirically shown the effectiveness of combining kernels in this way: The composite kernel consistently improves the performance of the individual ones. In addition, this formulation allows us to evaluate the individual contribution of each information source. In order to show the effectiveness of the proposed domain model in supervised  . They are completely speciﬁed by learning, we deﬁned two WSD kernels, Kwsd and Kwsd the n individual kernels that compose them in Equation (19). Kwsd i"
J09-4007,S07-1016,0,0.0252856,"Missing"
J09-4007,W04-0811,0,0.0503622,"Missing"
J09-4007,W04-0856,1,0.901318,"Missing"
J09-4007,P94-1013,0,0.0994997,"t can be applied to the strictly supervised settings, in which external knowledge is not available. To summarize, the domain kernel allows us to plug external knowledge into the supervised learning process; it will be compared and combined with the standard bagof-words approach in Section 4. In the following section, we shall see that domain models are also useful for deﬁning soft-matching collocation kernels. 3.2 Syntagmatic Kernels Collocations (such as bigrams and trigrams) extracted from the local context of the word to be disambiguated are typically used to capture syntagmatic relations (Yarowsky 1994). However, traditional approaches to WSD fail to represent non-contiguous or shifted collocations, and fail to consider lexical variability. For example, suppose we have to disambiguate the verb to score in the sentence Ronaldo scored the ﬁrst goal, given the labeled example The football player scored two goals in the second half as training. A traditional approach has no clues to return the right answer because the two sentences have no features in common. The use of kernels on strings allows us to overcome the aforementioned problems by representing (non-contiguous) collocations and exploiti"
J09-4007,W04-0864,0,0.0268411,"Missing"
J09-4007,P05-1052,0,0.0237351,"nto account second order relations among terms. For example, the similarity of the two sentences He is affected by AIDS and HIV is a virus is very high, because the terms AIDS, HIV, and virus are strongly associated with the medicine domain. A DM can be estimated from manually constructed lexical resources, such as WordNet Domains (Magnini and Cavagli`a 2000), or by performing a term-clustering process on a (large) corpus. However, the second approach is more attractive because it allows us to automatically acquire DMs for different languages and domains. In Gliozzo, Giuliano, and Strapparava (2005), we use singular valued decomposition (SVD) to acquire DMs from a corpus represented by its term-by-document matrix T, in a unsupervised way.4 SVD decomposes the term-by-document matrix T into three matrixes T  VΣk UT , where V and U are orthogonal matrices (i.e., VT V = I and UT U = I) whose columns are the eigenvectors of TTT and TT T, respectively, and Σk is the diagonal k × k matrix containing the highest k  k eigenvalues of T, and all the remaining elements set to 0. The parameter k is the dimensionality of the domain VSM and can be ﬁxed in advance. Under this setting, we deﬁne the"
L16-1600,W04-2214,0,0.234334,"71 32.08 31.72 11.16 12.97 9.32 9.60 Table 2: Distribution of the annotated tokens based on parts-of-speech. Language Noun Verb Adjective Adverb English Italian 70.16 65.70 21.37 22.56 7.08 8.23 1.25 3.21 Table 3: Distribution of the metaphorical tokens based on parts-of-speech. Language Noun Verb Adjective Adverb English Italian 42.91 44.01 19.33 21.68 18.41 19.35 3.89 10.18 Table 4: Percentage of the metaphorical words per part-ofspeech. We also analyze the distribution of the domains that the metaphorical words belong to. To this end, we consider the domains extracted from WordNet Domains (Bentivogli et al., 2004) for the first sense of each metaphorical lemma and POS pair. First, for each lemma that is used metaphorically at least once, we compute an estimate of the likelihood of the lemma being used metaphorically. Let N be the number of distinct lemmas used metaphorically, wi be a word lemma, mi the number of times wi is used metaphorically, ci the total count of wi in the dataset. The likelihood of wi being used metaphorically is computed as: P mi + N1 j mj P , L(wi ) = ci + N1 j cj where the sum terms, like in a Bayesian average, are used to mitigate the effect of infrequent words which are used a"
L16-1600,W14-2302,0,0.433979,"Missing"
L16-1600,W07-0103,0,0.0677601,"Missing"
L16-1600,P14-5010,0,0.00303711,"whole for both en and it, iii) state whether they agreed with the ideas or statements presented by en, iv) state whether en include old-fashioned concepts or ideas. Five annotators with a linguistic background were involved in this phase. Word-level metaphors: As a preprocessing step, we tokenized and POS-tagged the 5 We preferred to carry out the annotations in the literal translations rather than the original proverbs to make them easily comparable and comprehensible for the target of this dataset. English proverbs and the English translations of the Italian proverbs using Stanford CoreNLP (Manning et al., 2014). It is worth noting that for this part of the annotation task, we excluded 261 Italian proverbs having a similarity degree of 3 with the English proverbs since they were literal translations of the ones in English and they had the same metaphorical words. Instead, our motivation with this annotation is to determine the differences between English and Italian in the metaphorical word choices to convey the same message. The annotators were required to identify the metaphors among the nouns, verbs, adjectives and adverbs (in total 4978 tokens for English and 4098 tokens for Italian) appearing in"
L16-1600,shutova-teufel-2010-metaphor,0,0.06565,"Missing"
L16-1600,J15-4002,0,0.0196861,"y (Lakoff and Johnson, 1980), the mapping across domains stands at the core of the metaphorical connection and this kind of connection is very commonly established in proverbs as linguistic metaphors. Therefore, metaphors can resolve a significant amount of the figurative meaning in many proverbial utterances (Faycel, 2012). This richness of proverbs in terms of metaphors makes them an extremely beneficial linguistic resource as they can be utilized as a gold standard for various metaphor related linguistic tasks such as metaphor identification or interpretation. For instance, as suggested by Shutova (2015), Machine Translation (MT) task should especially tackle metaphor identification and interpretation since metaphorical expressions are usually specific to each culture and language. In the same manner, proverbs would also be compelling for MT as they are culture-specific and metaphorically rich. In addition, a collection of proverbs from various languages annotated with metaphors would also be essential for social scientists to investigate the cultural differences between different language groups. In this paper, we introduce P RO M ETHEUS, a dataset of English proverbs together with their equ"
L16-1600,D11-1063,0,0.267654,"Missing"
N09-3015,W02-0213,0,0.0540247,"Missing"
N09-3015,pianta-etal-2008-textpro,0,0.0499253,". In particular the Switchboard corpus employs a revision (Jurafsky et al., 1997).2 Table 2 shows the set of labels employed with their definitions, examples and distributions in the two data sets. The categories maintain the DAMSL main characteristic of being domain-independent and can be easily mapped back into SWBD-DAMSL ones, and maintain their original semantics. Thus, the original SWBD-DAMSL annotation had been automatically converted into the categories included in our markup language.3 4 Data preprocessing To reduce the data sparseness, we used a POS-tagger and morphological analyzer (Pianta et al., 2008) for preprocessing both corpora. So we considered lemmata instead of tokens in the format lemma#POS. In addition, we augment the features of each sentence with a set of linguistic markers, defined according to 2 The SWBD-DAMSL modifies the original DAMSL framework by further specifying some categories or by adding extra features (mainly prosodic) which were not originally included in the scheme. 3 Also we did not consider the utterances formed only by non-verbal material (e.g. laughter). 86 the semantic of the DA categories. We hypothesize, in fact, these features could play an important role"
N09-3015,J00-3003,0,0.238268,"Missing"
N09-3015,W05-0608,1,\N,Missing
N15-1172,P12-1094,0,0.244846,"other “broadcasting” actions such as sharing on Twitter. 1484 Another line of research investigates the impact of various textual features on audience reactions. The work by Guerini et al. (2011) correlates several viral phenomena with the wording of a post, while Guerini et al. (2012) show that features such as the readability level of an abstract influence the number of downloads, bookmarking and citations. A particular approach to content virality is presented by Simmons et al. (2011), who explore the impact of different types of modification on memes spreading from one person to another. Danescu-Niculescu-Mizil et al. (2012) measure a different ingredient of persuasion by analyzing the features of a movie quote that make it “memorable”. They compile a corpus consisting of memorable and non-memorable movie quote pairs and conduct a detailed analysis to investigate the lexical and syntactic differences between these pairs. Louis and Nenkova (2013) focus on influential science articles in newspapers by considering characteristics such as readability, description vividness, use of unusual words and affective content. High quality articles (NYT articles appearing in “The Best American Science Writing” anthology) are c"
N15-1172,Q13-1028,0,0.150798,"an abstract influence the number of downloads, bookmarking and citations. A particular approach to content virality is presented by Simmons et al. (2011), who explore the impact of different types of modification on memes spreading from one person to another. Danescu-Niculescu-Mizil et al. (2012) measure a different ingredient of persuasion by analyzing the features of a movie quote that make it “memorable”. They compile a corpus consisting of memorable and non-memorable movie quote pairs and conduct a detailed analysis to investigate the lexical and syntactic differences between these pairs. Louis and Nenkova (2013) focus on influential science articles in newspapers by considering characteristics such as readability, description vividness, use of unusual words and affective content. High quality articles (NYT articles appearing in “The Best American Science Writing” anthology) are compared against typical NYT articles. Borghol et al. (2012) investigate how differences in textual description affect the spread of contentcontrolled videos. Lakkaraju et al. (2013) focus on the act of resubmissions (i.e., content that is submitted multiple times with multiple titles to multiple different communities) to unde"
N15-1172,P14-5010,0,0.00484244,"xperiments, we used the four datasets described in Section 4 (i.e., CORPS, Twitter, Slogan and Movie), all of which consist of a persuasive sentence P and its non-persuasive counterpart (¬P ) labeled as either left or right. To make the positions of the sentences in a pair irrelevant (i.e. to provide symmetry), for each instance occurring in the original datasets (e.g., (s1 , s2 ) with label left), we added another instance including the same sentence pair in reverse order (i.e., (s2 , s1 ) with label right). As a preprocessing step, all the sentences were tokenized by using Stanford CoreNLP (Manning et al., 2014). 6.2 filtering for bigrams and trigrams to capture longerrange usage patterns such as propositional phrases. The third feature set is simply the union of both phonetic and n-gram features. To find the best configuration for each dataset and feature set, we conducted a grid search over the degree of the polynomial kernel (1 or 2) and the number of features to be used (in the range between 1,000 and 20,000). Due to the low dimensionality of the phonetic feature set, feature selection was performed only for the feature sets including n-grams. The selection was performed based on the information"
N15-1172,ozbal-etal-2012-brand,1,0.842491,"es such as rhyme and alliteration are systematically exploited by advertisers to achieve memorability. Similarly, Wales (2001) underlines the effectiveness of alliteration and rhyme on emphasis and memorability of an expression. The relation between the usage of plosives (i.e., consonants in which the vocal tract is blocked so that all airflow ceases, such as “p”, “t” or “k”) and memorability has also been investigated. According to the study carried out by Bergh et al. (1984) brand names starting with plosive sounds are recalled and recognized more than the ones starting with other ¨ sounds. Ozbal et al. (2012) carry out an analysis of brand names and discover that plosives are very commonly used. Danescu-Niculescu-Mizil et al. (2012), whom we previously mentioned, carry out an auxiliary analysis and observe the differences in letter and sound distribution (e.g. usage of labials or front vowels, back sounds, coordinating conjunctions) of memorable and non-memorable quotes. ¨ Ozbal et al. (2013) propose a phonetic scorer for creative sentence generation such that generated sentences can contain various phonetic features including alliteration, rhyme and plosive sounds. The authors evaluate the propos"
N15-1172,P13-1142,1,0.837921,"lity has also been investigated. According to the study carried out by Bergh et al. (1984) brand names starting with plosive sounds are recalled and recognized more than the ones starting with other ¨ sounds. Ozbal et al. (2012) carry out an analysis of brand names and discover that plosives are very commonly used. Danescu-Niculescu-Mizil et al. (2012), whom we previously mentioned, carry out an auxiliary analysis and observe the differences in letter and sound distribution (e.g. usage of labials or front vowels, back sounds, coordinating conjunctions) of memorable and non-memorable quotes. ¨ Ozbal et al. (2013) propose a phonetic scorer for creative sentence generation such that generated sentences can contain various phonetic features including alliteration, rhyme and plosive sounds. The authors evaluate the proposed model on automatic ¨ slogan generation. In a more recent work (Ozbal et al., 2014), they enforce the existence of these features in the sentences that are automatically generated for second language learning to introduce hooks to echoic memory. 3 Phonetic Scorer For the design of the phonetic features, we were ¨ mostly inspired by the work of Ozbal et al. (2013), who built and used thr"
N15-1172,P14-2058,1,0.833886,"re very commonly used. Danescu-Niculescu-Mizil et al. (2012), whom we previously mentioned, carry out an auxiliary analysis and observe the differences in letter and sound distribution (e.g. usage of labials or front vowels, back sounds, coordinating conjunctions) of memorable and non-memorable quotes. ¨ Ozbal et al. (2013) propose a phonetic scorer for creative sentence generation such that generated sentences can contain various phonetic features including alliteration, rhyme and plosive sounds. The authors evaluate the proposed model on automatic ¨ slogan generation. In a more recent work (Ozbal et al., 2014), they enforce the existence of these features in the sentences that are automatically generated for second language learning to introduce hooks to echoic memory. 3 Phonetic Scorer For the design of the phonetic features, we were ¨ mostly inspired by the work of Ozbal et al. (2013), who built and used three phonetic scorers for creative sentence generation. Similarly to this work, all the phonetic features that we used are based on the phonetic representation of English words of the Carnegie Mellon University pronouncing dictionary1 . We selected four classes of phonetic devices, 1 The CMU pro"
N15-1172,P14-1017,0,0.285024,"people think or behave but they are also so powerful that they can – allegedly – change reality. Spells are often very euphonic (and meaningless) sentences, e.g. “Hocus Pocus”. Various psycholinguistic studies addressed the effects of phonetics on the audience in different aspects such as memorability (Wales, 2001; Benczes, 2013) or more specifically advertisement (Leech, 1966; Bergh et al., 1984). There are also computational studies that address the problem of recognizing persuasive sentences according to various syntactic, lexical and semantic features (DanescuNiculescu-Mizil et al., 2012; Tan et al., 2014). However, to the best of our knowledge, the direct impact of phonetic elements on persuasiveness has not been explored in computational settings yet. In this paper, we fill in this gap by conducting a series of analyses and prediction experiments on four datasets representing different aspects of persuasive language to evaluate the importance of a set of phonetic devices (i.e. rhyme, alliteration, homogeneity and plosives) on various forms of persuasiveness. Our experiments show that phonetic features play an important role in the detection of persuasiveness and encode a notion of “melodious"
novielli-strapparava-2010-studying,N09-3015,1,\N,Missing
novielli-strapparava-2010-studying,S07-1013,1,\N,Missing
novielli-strapparava-2010-studying,J00-3003,0,\N,Missing
novielli-strapparava-2010-studying,P98-2188,0,\N,Missing
novielli-strapparava-2010-studying,C98-2183,0,\N,Missing
novielli-strapparava-2010-studying,strapparava-valitutti-2004-wordnet,1,\N,Missing
novielli-strapparava-2010-studying,pianta-etal-2008-textpro,0,\N,Missing
ozbal-etal-2012-brand,guerini-etal-2008-resources,1,\N,Missing
P05-1050,W04-0827,0,0.0156161,"Missing"
P05-1050,magnini-cavaglia-2000-integrating,0,0.0207238,"Missing"
P05-1050,W04-0856,1,0.684687,"a, with a large set of unlabeled data. However, at our knowledge, none of the participants exploited this unlabeled material. Exploring this direction is the main focus of this paper. In particular we acquire a Domain Model (DM) for the lexicon (i.e. a lexical resource representing domain associations among terms), and we exploit this information inside our supervised WSD algorithm. DMs can be automatically induced from unlabeled corpora, allowing the portability of the methodology among languages. We identified kernel methods as a viable framework in which to implement the assumptions above (Strapparava et al., 2004). 404 Exploiting the properties of kernels, we have defined independently a set of domain and syntagmatic kernels and we combined them in order to define a complete kernel for WSD. The domain kernels estimate the (domain) similarity (Magnini et al., 2002) among contexts, while the syntagmatic kernels evaluate the similarity among collocations. We will demonstrate that using DMs induced from unlabeled corpora is a feasible strategy to increase the generalization capability of the WSD algorithm. Our system far outperforms the state-ofthe-art systems in all the tasks in which it has been tested."
P06-1057,C04-1177,0,0.0111673,"ubstitution lexicons in practical applications, since they would mostly introduce noise to the system. To avoid this problem the list of WordNet synonyms for each target word was filtered by a lexicographer, who excluded manually obscure synonyms that seemed worthless in practice. The source synonym for each target word was then picked randomly from the filtered list. Table 1 shows the 25 source-target pairs created for our experiments. In future work it may be possible to apply automatic methods for filtering infrequent sense correspondences in the dataset, by adopting algorithms such as in (McCarthy et al., 2004). Problem Setting and Dataset To investigate the direct sense matching problem it is necessary to obtain an appropriate dataset of examples for this binary classification task, along with gold standard annotation. While there is no such standard (application independent) dataset available it is possible to derive it automatically from existing WSD evaluation datasets, as described below. This methodology also allows comparing direct approaches for sense matching with classical indirect approaches, which apply an intermediate step of identifying the most likely WordNet sense. We derived our dat"
P06-1057,W02-0816,0,0.492519,"Missing"
P06-1057,J98-1004,0,0.0536338,"Missing"
P06-1057,W98-0705,0,0.0286631,"Missing"
P06-1057,P94-1013,0,0.0448618,"Missing"
P06-1057,P98-2127,0,0.0820922,"htein1 , Carlo Strapparava2 1 Department of Computer Science, Bar Ilan University, Ramat Gan, 52900, Israel 2 ITC-Irst, via Sommarive, I-38050, Trento, Italy Abstract ference between a pair of texts in a generalized application independent setting (Dagan et al., 2005). To perform lexical substitution NLP applications typically utilize a knowledge source of synonymous word pairs. The most commonly used resource for lexical substitution is the manually constructed WordNet (Fellbaum, 1998). Another option is to use statistical word similarities, such as in the database constructed by Dekang Lin (Lin, 1998). We generically refer to such resources as substitution lexicons. When using a substitution lexicon it is assumed that there are some contexts in which the given synonymous words share the same meaning. Yet, due to polysemy, it is needed to verify that the senses of the two words do indeed match in a given context. For example, there are contexts in which the source word ‘weapon’ may be substituted by the target word ‘arm’; however one should recognize that ‘arm’ has a different sense than ‘weapon’ in sentences such as “repetitive movements could cause injuries to hands, wrists and arms.” A c"
P06-1057,W97-0322,0,\N,Missing
P06-1057,W07-1401,1,\N,Missing
P06-1057,C98-2122,0,\N,Missing
P06-1070,W02-0902,0,0.0107383,"even if in principle it could exist and return 1 for a strict subset of document pairs. The texts inside comparable corpora, being about the same topics, should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevant entities and words that are not yet lexicalized in the language, are expressed by using their original terms. As a consequence the same entities will be denoted with the same words in different languages, allowing us to automatically detect couples of translation pairs just by looking at the word shape (Koehn and Knight, 2002). Our hypothesis is that comparable corpora contain a large amount of such words, just because texts, referring to the same topics in different languages, will often adopt the same terms to denote the same entities1 . 1 554 According to our assumption, a possible additional crirelies on the availability of a multilingual lexical resource. For languages with scarce resources a bilingual dictionary could be not easily available. Secondly, an important requirement of such a resource is its coverage (i.e. the amount of possible translation pairs that are actually contained in it). Finally, another"
P06-1070,W04-0856,1,0.233652,"Section 5, this model is rather poor because of its sparseness. In the next section, we will show how to use such words as seeds to induce a Multilingual Domain VSM, in which second order relations among terms and documents in different languages are considered to improve the similarity estimation. 3.1 Multilingual Domain Model A MDM is a multilingual extension of the concept of Domain Model. In the literature, Domain Models have been introduced to represent ambiguity and variability (Gliozzo et al., 2004) and successfully exploited in many NLP applications, such as Word Sense Disambiguation (Strapparava et al., 2004), Text Categorization and Term Categorization. A Domain Model is composed of soft clusters of terms. Each cluster represents a semantic domain, i.e. a set of terms that often co-occur in texts having similar topics. Such clusters identify groups of words belonging to the same semantic field, and thus highly paradigmatically related. MDMs are Domain Models containing terms in more than one language. A MDM is represented by a matrix D, containing the degree of association among terms in all the languages and domains, as illustrated in Table 1. For example the term virus is associated to both 3 E"
P06-1070,P04-1023,0,0.0138086,"Such limitation is mainly due to the problem of tuning large scale multilingual lexical resources (e.g. MultiWordNet, EuroWordNet) for the specific application task (e.g. discarding irrelevant senses, extending the lexicon with domain specific terms and their translations). On the other hand, empirical approaches are in general more accurate, because they can be trained from domain specific collections of parallel text to represent the application needs. There exist many interesting works about using parallel corpora for multilingual applications (Melamed, 2001), such as Machine Translation (Callison-Burch et al., 2004), Cross Lingual Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language (e.g. English) while the system is trained using labeled documents in a source language (e.g. Italian). In this work we present many solutions according to the availability of bilingual resources, and we show that it is possible to deal with the problem even when no such resources are accessible. The core technique relies on the automatic acquisition of Multilingual Domain Models from comparable corpora. Experiments show the effectiveness of our approach, provi"
P06-1070,P04-1067,0,0.0121436,"ing labeled examples in a source language (e.g. English), and it classifies documents in a different target language (e.g. Italian). 553 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 553–560, c Sydney, July 2006. 2006 Association for Computational Linguistics Information Retrieval (Littman et al., 1998), and so on. However it is not always easy to find or build parallel corpora. This is the main reason why the “weaker” notion of comparable corpora is a matter of recent interest in the field of Computational Linguistics (Gaussier et al., 2004). In fact, comparable corpora are easier to collect for most languages (e.g. collections of international news agencies), providing a low cost knowledge source for multilingual applications. The main problem of adopting comparable corpora for multilingual knowledge acquisition is that only weaker statistical evidence can be captured. In fact, while parallel corpora provide stronger (text-based) statistical evidence to detect translation pairs by analyzing term co-occurrences in translated documents, comparable corpora provides weaker (term-based) evidence, because text alignments are not avail"
P06-1070,W05-0802,1,0.614459,"Abstract The applicative interest for the CLTC is immediately clear in the globalized Web scenario. For example, in the community based trade (e.g. eBay) it is often necessary to archive texts in different languages by adopting common merceological categories, very often defined by collections of documents in a source language (e.g. English). Another application along this direction is Cross Lingual Question Answering, in which it would be very useful to filter out the candidate answers according to their topics. In the literature, this task has been proposed quite recently (Bel et al., 2003; Gliozzo and Strapparava, 2005). In those works, authors exploited comparable corpora showing promising results. A more recent work (Rigutini et al., 2005) proposed the use of Machine Translation techniques to approach the same task. Classical approaches for multilingual problems have been conceived by following two main directions: (i) knowledge based approaches, mostly implemented by rule based systems and (ii) empirical approaches, in general relying on statistical learning from parallel corpora. Knowledge based approaches are often affected by low accuracy. Such limitation is mainly due to the problem of tuning large sc"
P09-2078,D08-1027,0,0.0303132,"Missing"
P12-1074,esuli-sebastiani-2006-sentiwordnet,0,0.0109215,"Missing"
P12-1074,ozbal-etal-2012-brand,1,0.433283,"in questionnaires to learn about their markets, competitors, and expectations. In the end, they present a list of name candidates to be chosen from. Although the resulting names can be successful and satisfactory, these services are very expensive and the processing time is rather long. 3 Dataset and Annotation In order to create a gold standard for linguistic creativity in naming, collect the common creativity devices used in the naming process and determine the suitable ones for automation, we conducted an annotation task on a dataset of 1000 brand and com¨ pany names from various domains (Ozbal et al., 2012). These names were compiled from a book dedicated to brand naming strategies (Botton and Cegarra, 1990) and various web resources related to creative naming such as adslogans.co.uk and brandsandtags.com. Our list contains names which were invented via various creativity methods. While the creativity in some of these names is independent of the context and the names themselves are sufficient to realize the methods used (e.g. alliteration in Peak Performance, modification of one letter in Vimeo), for some of them the context information such as the description of the product or the area of the c"
P12-1074,P11-1029,0,0.0781544,"we can decide what relations to explore, with the result of a more precise process with possibly less recall. 4.4 Retrieving metaphors A metaphor is a figure of speech in which an implied comparison is made to indicate how two things that are not alike in most ways are similar in one important way. Metaphors are common devices for evocation, which has been found to be a very important technique used in naming according to the analysis of our dataset. In order to generate metaphors, we start with the set of properties determined by the user and adopt a similar technique to the one proposed by (Veale, 2011). In this work, to metaphorically ascribe a property to a term, stereotypes for which the property is culturally salient are intersected with stereotypes to which the term is pragmatically comparable. The stereotypes for a property are found by querying on the web with the simile pattern “as hpropertyi as *”. Unlike the proposed approach, we do not apply any intersection with comparable stereotypes since the naming task should favor further terms to the category word in order to exaggerate, to evoke and thereby to be more effective. The first constituent of our approach uses the pattern “as hp"
P12-1104,W08-0123,0,0.0473218,"Missing"
P12-1104,H89-2010,0,0.0655132,"Missing"
P12-1104,guerini-etal-2010-evaluation,1,0.901828,"ons can be page views or signups. By assigning a value to a conversion the resulting conversions represents a return on investment, or ROI. • Google Analytics Tool: Google Analytics is a web analytics tool that gives insights into website traffic, like number of visited pages, time spent on the site, location of visitors, etc. Note that similar ecological approaches are beginning to be investigated: for example in (Aral and Walker, 2010) an approach to assessing the social effects of content features on an on-line community is presented. A previous approach that uses AdWords was presented in (Guerini et al., 2010), but it crowdsourced only the running of the experiment, not data manipulation and analysis, and was not totally controlled for subject randomness. 3 AdWords Features Google AdWords is Google’s advertising program. The central idea is to let advertisers display their messages only to relevant audiences. This is done by means of keyword-based contextualization on the Google network, divided into: • Search network: includes Google search pages, search sites and properties that display search results pages (SERPs), such as Froogle and Earthlink. • Display network: includes news pages, topicspeci"
P12-1104,D08-1027,0,0.0481037,"Missing"
P12-1104,W10-0211,0,0.0125876,"s of variations and with polarized conditions, neutral vs. biased. In addition we wanted to test how quickly experiments could be performed (two days versus the two week suggestion of Google). Adjectives were chosen according to MAX bigram frequencies with the modified noun, using the Web 1T 5-gram corpus (Brants and Franz, 2006). Deciding whether this is the best metric for choosing adjectives to modify a noun or not (e.g. also pointwise mutual-information score can be used with a different rationale) is out of the scope of the present paper, but previous work has already used this approach (Whitehead and Cavedon, 2010). Top ranked adjectives were then manually ordered - according to affective weight - to choose the best one (we used a standard procedure using 3 annotators and a reconciliation phase for the final decision). 6.1 First Experiment The first experiment lasted 48 hour with a total of 38 thousand subjects and a cost of 30 euros (see Table 1 for the complete description of the experimental setup). It was meant to test broadly how affective variations in the body of the ads performed. The two variations contained a fragment of a commentary of the museum guide; the control condition contained “gentle"
P12-1104,guerini-etal-2008-valentino,1,\N,Missing
P12-1104,W09-0625,0,\N,Missing
P12-1104,D11-1062,0,\N,Missing
P13-1064,Y09-2026,0,0.161141,"rted through micro-averaged precision, recall and F1-score for the targeted class, as well as overall accuracy. The high results, on a par with text categorization experiments in the field, validates our experimental set-up. For the cross language categorization experiments described in this paper, we use the data described above, and train on one language (English/Italian), and test on the other, using the same Word etymologies are a novel source of linguistic information in NLP, possibly because resources that capture this information in a machine readable format are also novel. Fang et al. (2009) used limited etymological information extracted from the Collins English Dictionary (CED) for text categorization on the British National Corpus (BNC): information on the provenance of words (ranges of 654 Categories quality of life made in Italy tourism culture and school Total Training 5759 5711 5731 3665 20866 English Test 1989 1864 1857 1245 6955 Total 7748 7575 7588 4910 27821 Training 5781 6111 6090 6284 24266 Italian Test 1901 2068 2015 2104 8088 Total 7682 8179 8105 8388 32354 Table 1: Dataset statistics monolingual BoW categorization Prec Rec F1 Train EN / Test EN 0.92 0.92 0.92 Trai"
P13-1064,W05-0802,1,0.63796,"corpora through latent semantic analysis (LSA). Most CLTC methods rely heavily on machine translation (MT). MT has been used: to cast the cross-language text 653 probability distribution of etymologies in different versions of Latin – New Latin, Late Latin, Medieval Latin) was used in a “home-made” range classifier. The experiments presented in this paper use the bag-of-word document representation with absolute frequency values. To this basic representation we add word etymological ancestors and run classification experiments. We then use LSA – previously shown by (Dumais et al., 1997) and (Gliozzo and Strapparava, 2005) to be useful for this task – to induce the latent semantic dimensions of documents and words respectively, hypothesizing that word etymological ancestors will lead to semantic dimensions that transcend language boundaries. The vectors obtained through LSA (on the training data only) for words that are shared by the English training data and the Italian test data (names, and most importantly, etymological ancestors of words in the original documents) are then used for rerepresenting the training and test data. The same process is applied for Italian training and English test data. Classificati"
P13-1064,P06-1070,1,0.879523,"Missing"
P13-1064,D10-1103,0,0.0521905,"Missing"
P13-1064,P10-1114,0,0.0421059,"Missing"
P13-1064,I08-1022,0,0.0394112,"Missing"
P13-1142,W10-0204,0,0.0220553,"nnotation. The chromatic connotation of a sentence sP = [s0 , . . . , sn ]Pis computed as f(s, U ) = si (sim(si , c) − cj 6=c sim(si , cj )), where c is the user selected target color and sim(si , cj ) is the degree of association between the word si and the color cj as calculated by Mohammad (2011). All the words in the sentence which have an association with the target color c give a positive contribution, while those that are associated with a color ci 6= c contribute negatively. Emotional connotation works exactly in the same way, but in this case word-emotion associations are taken from (Mohammad and Turney, 2010). Domain relatedness. This feature function uses an LSA (Deerwester et al., 1990) vector space 1450 model to measure the similarity between the words in the sentence and the target domain d specifiedP by the Puser. It is calculated as: f(s, U ) = k P v(di )· s v(si ) Pi v(di )k·k s v(si )k di di where v(·) returns the repi resentation of a word in the vector space. Semantic cohesion. This feature behaves exactly like domain relatedness, with the only difference that it measures the similarity between the words in the sentence and the target words t. Target-words scorer. This feature function s"
P13-1142,P11-2064,0,0.0774484,"wk ] the associated vector of weights in U . TheP overall score of s is calculated as score(s, U ) = ki=0 wi fi (s, U ) . Solutions that do not contain all the required target words are discarded and not shown to the user. Currently, the model employs the following 12 feature functions: Chromatic and emotional connotation. The chromatic connotation of a sentence sP = [s0 , . . . , sn ]Pis computed as f(s, U ) = si (sim(si , c) − cj 6=c sim(si , cj )), where c is the user selected target color and sim(si , cj ) is the degree of association between the word si and the color cj as calculated by Mohammad (2011). All the words in the sentence which have an association with the target color c give a positive contribution, while those that are associated with a color ci 6= c contribute negatively. Emotional connotation works exactly in the same way, but in this case word-emotion associations are taken from (Mohammad and Turney, 2010). Domain relatedness. This feature function uses an LSA (Deerwester et al., 1990) vector space 1450 model to measure the similarity between the words in the sentence and the target domain d specifiedP by the Puser. It is calculated as: f(s, U ) = k P v(di )· s v(si ) Pi v(d"
P13-1142,esuli-sebastiani-2006-sentiwordnet,0,0.00345774,"e, 1997; McKay, 2002; Manurung et al., 2008). Valitutti et al. (2009) present an interactive system which generates humorous puns obtained through variation of familiar expressions with word substitution. The variation takes place considering the phonetic distance and semantic constraints such as semantic similarity, semantic domain opposition and affective polarity difference. Possibly closer to slogan generation, Guerini et al. (2011) slant existing textual expressions to obtain more positively or negatively valenced versions using WordNet (Miller, 1995) semantic relations and SentiWordNet (Esuli and Sebastiani, 2006) annotations. Stock and Strapparava (2006) generate acronyms based on lexical substitution via semantic field opposition, rhyme, rythm and semantic relations. The model is limited to the generation of noun phrases. Poetry generation systems face similar challenges to B RAIN S UP as they struggle to combine semantic, lexical and phonetic features in a unified framework. Greene et al. (2010) describe a model for poetry generation in which users can control meter and rhyme scheme. Generation is modeled as a cascade of weighted Finite State Transducers that only accept strings conforming to the de"
P13-1142,D10-1051,0,0.390335,"teaching the Italian word “tenda”, which means “curtain” in English, the learners are asked to imagine “rubbing a tender part of their leg with a curtain”. These words should co-occur in the same sentence, but constructing such sentences by hand can be a dif¨ ficult and very time-consuming process. Ozbal and Strapparava (2011), who attempted to automate the process, conclude that the inability to retrieve from the web a good sentence for all cases is a major bottleneck. Although state of the art computational models of creativity often produce remarkable results, e.g., Manurung et al. (2008), Greene et al. (2010), Guerini et al. (2011), Colton et al. (2012) just to name a few, to our best knowledge there is no attempt to develop an unified framework for the generation of creative sentences in which users can control all the variables involved in the creative process to achieve the desired effect. In this paper, we advocate the use of syntactic information to generate creative utterances by describing a methodology that accounts for lexical and phonetic constraints and multiple semantic dimensions at the same time. We present B RAIN S UP, an extensible framework for creative sentence generation in whic"
P13-1142,P12-1074,1,0.90015,"Missing"
P13-1142,P03-1054,0,0.00469605,"Missing"
P14-2058,P13-1142,1,0.847734,"e keyword method by retrieving sentences from the Web. However, we did not provide any evaluation to demonstrate the effectiveness of our approach in a real life scenario. In addition, we observed that retrieval poses severe limitations in terms of recall and sentence quality, and it might incur copyright violations. In this paper, we overcome these limitations by introducing a semi-automatic system implementing the keyword method that builds upon the key¨ word selection mechanism of Ozbal and Strapparava (2011) and combines it with a state-of-the-art ¨ creative sentence generation framework (Ozbal et al., 2013). We set up an experiment to simulate the situation in which a teacher needs to prepare material for a vocabulary teaching resource. According to our scenario, the teacher relies on automatic techniques to generate relatively few, high quality mnemonics in English to teach Italian vocabulary. She only applies a very light supervision in the last step of the process, in which the most suitable among the generated sentences are selected before being presented to the learners. In this stage, the teacher may want to consider factors which are not yet in reach of automatic linguistic processors, su"
P14-2058,W10-0721,0,0.0223888,"Missing"
P14-2058,W10-0707,0,\N,Missing
P17-2086,W13-1732,0,0.391403,"Missing"
P17-2086,C12-1025,0,0.181793,"Missing"
P17-2086,W13-1733,0,0.0322676,"Missing"
P17-2086,W13-1718,0,0.167874,"). For each target L1, the number of essays is equal (900 in the train set, 100 in the development set and 100 in the test set). The distribution of the number of essays per topic is not perfectly balanced across different L1s, but they are rather close: the average number of essays per topic is 1,513 and the standard deviation is 229. 543 Type of Feature (1) word ngrams (2) lemma ngrams (3) word error toefl (4) word error icle (5) char ngrams (6) char error icle (7) char error toefl (1) + (2) (1) + (2) + (3) (1) + (2) + (4) (1) + (2) + (5) (1) + (2) + (6) (1) + (2) + (7) Jarvis et al. (2013) Nicolai et al. (2013) Italian, German, Turkish, Chinese and Japanese). They are referred to here as Spelling error ICLE and Spelling error TOEFL in the later part of this paper. Spelling errors are binary features. Spelling errors as character n-grams Every misspelled word in a text will be represented as character n-grams, where n = 1..3. Special characters marking the start and end of a word will be part of the n-grams. The value of these features is their relative frequencies, as for character n-grams. 3.2 Classifiers Following the proven effectiveness of Support Vector Machine (SVM) by numerous experiments on"
P17-2086,W13-1729,0,0.0647926,"Missing"
P17-2086,W13-1714,0,0.150597,"relative frequency with respect to the set of n-grams (unigram, bigram, trigram) that they belong to: Using complete words to represent spelling errors would not capture regularities that go beyond a single misspelled instance – like the preference of using i instead of e by Italian writers. We investigate the representation of spelling errors through character n-grams with size up to 3. We assess the effectiveness of using such feature representation for NLI and its contribution when combined with word and lemma n-grams, whose effectiveness has already been established (Gyawali et al., 2013; Jarvis et al., 2013). We report high classification results when using only spelling errors, and an improvement of 1.2 percentage points in accuracy, compared to the best results obtained in NLI shared task, when using spelling errors in combination with word and lemma features. 2 Methods Data The experiments are performed on the TOEFL11 corpus (Blanchard et al., 2013) of English essays written by non-native English learners as part of the Test of English as a Foreign Language (TOEFL). We also use the ICLEv2 corpus (Granger et al., 2009) for extracting additional spelling errors. The TOEFL11 corpus is not the mos"
S01-1027,magnini-cavaglia-2000-integrating,1,0.739707,"stical techniques, for the three tasks we participated in, i.e. English &apos;all words&apos;, English &apos;lexical sample&apos; and Italian &apos;lexical sample&apos;. The main lexical resource for domains is &quot;WordNet Domains&quot;, an extension of English Wordnet 1.6 (Fellbaum, 1998) developed at ITC-irst, where synsets have been annotated with domain information. 2 WordN et Domains The basic lexical resource we used in SENSEVAL2 is &quot;WordNet Domains&quot;, an extension of WoRDNET 1.6 where each synset has been annotated with at least one domain label, selected from a set of about two hundred labels hierarchically organized (see (Magnini and Cavaglia, 2000) for the annotation methodology and for the evaluation of the resource). The information from the domains that we added is complementary to what is already in WoRDNET. First of all a domain may include synsets of different syntactic categories: for instance MEDICINE groups together senses from Nouns, such as doctor#i and hospi tal#i, and from Verbs such as operate#7. Second, a domain may include senses from different WoRDNET sub-hierarchies (i.e. deriving from different &quot;unique beginners&quot; or from different &quot;lexicographer files&quot;). For example, SPORT contains senses such as athlete#i, deriving f"
S01-1027,W00-0804,1,0.924982,"focuses on the role of domain information. The hypothesis is that domain labels (such as MEDICINE, ARCHITECTURE and SPORT) provide a natural and powerful way to establish semantic relations among word senses, which can be profitably used during the disambiguation process. In particular, domains constitute a fundamental feature of text coherence, such that word senses occurring in a coherent portion of text tend to maximize domain similarity. The importance of domain information in WSD has been remarked in several works, including (Gonzalo et al., 1998) and (Buitelaar and Sacaleanu, 2001). In (Magnini and Strapparava, 2000) we introduced &quot;Word Domain Disambiguation&quot; (WDD) as a variant of WSD where for each word in a text a domain label (among those allowed by the word) has to be chosen instead of a sense label. We also argued that WDD can be applied to disambiguation tasks that do not require fine grained sense distinctions, such as information retrieval and content-based user modeling. For SENSEVAL111 2 the goal was to evaluate the role of domain information in WSD: no other syntactic or semantic information has been used (e.g. semantic relations in WoRDNET) except domain labels. Three systems have been impleme"
S07-1013,esuli-sebastiani-2006-sentiwordnet,0,0.258438,"d to identify what is being said about the main subject by exploiting the dependency graph obtained from the parser. Each word was first rated separately for each emotion (the six emotions plus Compassion) and for valence. Next, the main subject rating was boosted. Contrasts and accentuations between “good” or “bad” were detected, making it possible to identify surprising good or bad news. The system also takes into account: human will (as opposed to illness or natural disasters); negation and modals; high-tech context; celebrities. The lexical resource used was a combination of SentiWordNet (Esuli and Sebastiani, 2006) and WordNetAffect (Strapparava and Valitutti, 2004), which were semi-automatically enriched on the basis of the original trial data. SICS: The SICS team used a very simple approach for valence annotation based on a word-space model and a set of seed words. The idea was to create two points in a high-dimensional word space one representing positive valence, the other representing negative valence - and then projecting each headline into this space, choosing the valence whose point was closer to the headline. The word space was produced from a lemmatized and stop list filtered version of the LA"
S07-1013,P97-1023,0,0.0190084,"a knowledgebased domain-independent unsupervised approach to headline valence detection and scoring. The system uses three main kinds of knowledge: a list of sentiment-bearing words, a list of valence shifters and a set of rules that define the scope and the result of the combination of sentiment-bearing words and valence shifters. The unigrams used for sentence/headline classification were learned from WordNet dictionary entries. In order to take advantage of the special properties of WordNet glosses and relations, we developed a system that used the list of human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and learned additional unigrams from WordNet synsets and glosses. The list was then expanded by adding to it all the words annotated with Positive or Negative tags in the General Inquirer. Each unigram in the resulting list had the degree of membership in the category of positive or negative sentiment assigned to it using the fuzzy Net Overlap Score method described in the team’s earlier work (Andreevskaia and Bergler, 2006). Only words with fuzzy membership score not equal to zero were retained in the list. The resulting list contained 10,809 sentimentbearing words of differen"
S07-1013,P04-1035,0,0.0278626,"e developed a Webbased annotation interface that displayed one headline at a time, together with six slide bars for emotions and one slide bar for valence. The interval for the emotion annotations was set to [0, 100], where 0 means the emotion is missing from the given headline, and 100 represents maximum emotional load. The interval for the valence annotations was set to [−100, 100], where 0 represents a neutral headline, −100 represents a highly negative headline, and 100 corresponds to a highly positive headline. Unlike previous annotations of sentiment or subjectivity (Wiebe et al., 2005; Pang and Lee, 2004), which typically relied on binary 0/1 annotations, we decided to use a finer-grained scale, hence allowing the annotators to select different degrees of emotional load. The test data set was independently labeled by six annotators. The annotators were instructed to select the appropriate emotions for each headline based on the presence of words or phrases with emotional content, as well as the overall feeling invoked by the headline. Annotation examples were also provided, including examples of headlines bearing two or more emotions to illustrate the case where several emotions were jointly a"
S07-1013,strapparava-valitutti-2004-wordnet,1,0.728742,"am was able to participate in one or both tasks. The task was carried out in an unsupervised setting, and consequently no training was provided. The reason behind this decision is that we wanted to emphasize the study of emotion lexical semantics, and avoid biasing the participants toward simple “text categorization” approaches. Nonetheless supervised systems were not precluded from participation, and in such cases the teams were allowed to create their own supervised training sets. Participants were free to use any resources they wanted. We provided a set words extracted from WordNet Affect (Strapparava and Valitutti, 2004), relevant to the six emotions of interest. However, the use of this list was entirely optional. 71 2.1 Data Set The data set consisted of news headlines drawn from major newspapers such as New York Times, CNN, and BBC News, as well as from the Google News search engine. We decided to focus our attention on headlines for two main reasons. First, news have typically a high load of emotional content, as they describe major national or worldwide events, and are written in a style meant to attract the attention of the readers. Second, the structure of headlines was appropriate for our goal of cond"
S07-1013,strapparava-etal-2006-affective,1,0.465856,"ches to emotion recognition. The task is not easy. Indeed, as (Ortony et al., 1987) indicates, besides words directly referring to emotional states (e.g., “fear”, “cheerful”) and for which an appropriate lexicon would help, there are words that act only as an indirect reference to 70 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 70–74, c Prague, June 2007. 2007 Association for Computational Linguistics emotions depending on the context (e.g. “monster”, “ghost”). We can call the former direct affective words and the latter indirect affective words (Strapparava et al., 2006). 2 Task Definition We proposed to focus on the emotion classification of news headlines extracted from news web sites. Headlines typically consist of a few words and are often written by creative people with the intention to “provoke” emotions, and consequently to attract the readers’ attention. These characteristics make this type of text particularly suitable for use in an automatic emotion recognition setting, as the affective/emotional features (if present) are guaranteed to appear in these short sentences. The structure of the task was as follows: Corpus: News titles, extracted from news"
S07-1029,P06-1057,1,0.540703,"which is formally defined as a relationship between a coherent text T and a language expression, the hypothesis H. T is said to entail H, denoted by T → H, if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005; Dagan and Glickman., 2004). Even though this notion has been only recently proposed in the computational linguistics literature, it attracts more and more attention due to the high generality of its settings and to the usefulness of its (potential) applications. 1 In the literature, slight variations of this problem have been also referred to as sense matching (Dagan et al., 2006). With respect to lexical entailment, the lexical substitution task has a more restrictive criterion. In fact, two words can be substituted when meaning is preserved, while the criterion for lexical entailment is that the meaning of the thesis is implied by the meaning of the hypothesis. The latter condition is in general ensured by substituting either hyperonyms or synonyms, while the former is more rigid because only synonyms are in principle accepted. Formally, in a lexical entailment task a system is asked to decide whether the substitution of a particular term w with the term e in a coher"
S07-1029,W06-1621,0,0.0200022,"Abstract This paper summarizes FBK-irst participation at the lexical substitution task of the S EMEVAL competition. We submitted two different systems, both exploiting synonym lists extracted from dictionaries. For each word to be substituted, the systems rank the associated synonym list according to a similarity metric based on Latent Semantic Analysis and to the occurrences in the Web 1T 5-gram corpus, respectively. In particular, the latter system achieves the state-of-the-art performance, largely surpassing the baseline proposed by the organizers. 1 Introduction The lexical substitution (Glickman et al., 2006a) can be regarded as a subtask of the lexical entailment, in which for a given word in context the system is asked to select an alternative word that can be replaced in that context preserving the meaning. Lexical Entailment, and in particular lexical reference (Glickman et al., 2006b)1 , is in turn a subtask of textual entailment, which is formally defined as a relationship between a coherent text T and a language expression, the hypothesis H. T is said to entail H, denoted by T → H, if the meaning of H can be inferred from the meaning of T (Dagan et al., 2005; Dagan and Glickman., 2004). Ev"
S07-1029,W06-2907,0,\N,Missing
S15-2077,W10-4001,0,0.0369826,"Missing"
S15-2077,W11-1707,0,0.0696797,"Missing"
S15-2077,P13-1174,0,0.090042,"Missing"
S15-2077,N09-1057,0,0.0259327,"store polarity values. For bag-of-words approaches the polarity of a text depends on the presence/absence of a set of lexical items. This methodology is successful to detect opinions about entities (such as reviews) but it shows mixed results when complex opinions about events - involving perspectives and points of view - are expressed. In terms of parts of speech involved, SA approaches tend to focus on lexical items that explicitly convey opinions - mainly adjectives, adverbs and several nouns - leaving verbs on the foreground. Improvements have been proposed by taking into account syntax (Greene and Resnik 2009) and by investigating the connotative polarity of words (Cambria et al., 2009; Akkaya et al., 2009, Balhaur et Carlo Strapparava Fondazione Bruno Kessler Via Sommarive, 18 38123 Povo (TN) strappa@fbk.eu al., 2011; Russo et al. 2011; Cambria et al., 2012, Deng et al., 2013 among others). One of the key aspects of sentiment analysis, which has been only marginally tackled so far, is the identification of implicit polarity. By implicit polarity we refer to the recognition of subjective textual units where no polarity markers are present but still people are able to state whether the text portion"
S15-2077,W12-3018,0,0.0498221,"Missing"
S15-2077,S14-2009,0,0.0998214,"Missing"
S15-2077,D09-1020,0,\N,Missing
S15-2077,P13-2022,0,\N,Missing
S15-2077,S14-2004,0,\N,Missing
S15-2147,E14-4004,0,0.264093,"Section 5 we discuss the main properties of the submitted systems and their results. The paper ends with a substantial section on conclusion and main future research direction in DTE. 2 Related Work The availability of large time annotated corpora like Google N-gram open the perspective of a new field of the research which focuses on the distribution of the linguistics elements in certain periods. (Popescu and Strapparava, 2014; Popescu and Strapparava, 2013) showed how such corpora can be used to infer transition periods between epoch with specific characteristics. A ground breaking paper, (Niculae et al., 2014) focuses on historical documents in three languages, English, Portuguese and Romanian. The paper shows how statistical method can be used to predict the date when the documents have been created. The similarity of the ideas in the present task and their paper, although developed in completely autonomy, prove that there is indeed a major interest in building diachronic systems and that the time is high for this task. We believe that there is a lot to do in this emergent field. 3 Task Description In this section we present the main motivations for a diachronic task and in particular, we focus on"
S15-2147,I13-1040,1,0.305261,"f specific linguistics variability in a certain epoch, location, social class etc. The statistical methods are able to discover correlations and linguistic provable evidence of language change at all levels: morphological, syntactical, semantic and discourse. It would be physically impossible for a human, or a team of humans for what it matters, to analyze and corroborate the data from hundreds of gigabytes of data and find all the relevant differences. Looking at the distribution of words across timeline, salient periods, with statistically non-random behavior, can be automatically inferred (Popescu and Strapparava, 2013). The structure of such periods, or epochs, are by far more complex than what it could be manually performed. From a practical point of view, diachronic systems have a wide range of applications from emergent fields such as computational forensics, computational journalism to more traditional tasks, such as discourse similarity, sense shifting, readability and narrative frameworks, etc. The paper is organized as follow: in the next section we review the relevant literature. In Section 3 we present the main motivation for the DTE task and the three subtasks with their specific corpora. In Secti"
strapparava-etal-2006-affective,magnini-cavaglia-2000-integrating,0,\N,Missing
strapparava-etal-2006-affective,strapparava-valitutti-2004-wordnet,1,\N,Missing
strapparava-etal-2006-affective,W05-0608,1,\N,Missing
strapparava-etal-2010-predicting,P06-1134,0,\N,Missing
strapparava-etal-2012-parallel,D08-1027,0,\N,Missing
strapparava-etal-2014-creative,D13-1125,1,\N,Missing
strapparava-etal-2014-creative,W98-0720,0,\N,Missing
strapparava-etal-2014-creative,J91-1003,0,\N,Missing
strapparava-etal-2014-creative,S07-1009,0,\N,Missing
strapparava-etal-2014-creative,J06-1003,0,\N,Missing
strapparava-etal-2014-creative,P11-1029,0,\N,Missing
strapparava-etal-2014-creative,aleksandrov-strapparava-2012-ngramquery,1,\N,Missing
strapparava-etal-2014-creative,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
strapparava-valitutti-2004-wordnet,magnini-cavaglia-2000-integrating,0,\N,Missing
W00-0804,W00-0103,0,0.0293694,"Missing"
W00-0804,magnini-cavaglia-2000-integrating,1,0.684897,"Missing"
W00-0804,P99-1020,0,0.0528559,"Missing"
W00-0804,kilgarriff-yallop-2000-whats,0,\N,Missing
W04-0856,W04-0800,0,0.235941,"Missing"
W04-0856,magnini-cavaglia-2000-integrating,0,0.334557,"Missing"
W04-0856,H93-1052,0,0.298182,"Missing"
W04-0861,W04-0828,1,0.681474,"Missing"
W04-0861,W04-0837,1,0.823382,"Missing"
W04-0861,P94-1013,0,0.0414439,"Catalonia. The integration was carried out by the TALP group.   Naive Bayes (NB) is the well–known Bayesian algorithm that classifies an example by choosing the class that maximizes the product, over all features, of the conditional probability of the class given the feature. The provider of this module is IXA. Conditional probabilities were smoothed by Laplace correction.  Decision List (DL) are lists of weighted classification rules involving the evaluation of one single feature. At classification time, the algorithm applies the rule with the highest weight that matches the test example (Yarowsky, 1994). The provider is IXA and they also applied smoothing to generate more robust decision lists.  In the Vector Space Model method (cosVSM), each example is treated as a binary-valued feature vector. For each sense, one centroid vector is obtained from training. Centroids are compared with the vectors representing test examples, using the cosine similarity function, and the closest centroid is used to classify the example. No smoothing is required for this method provided by IXA. 2 The WSD Modules Support Vector Machines (SVM) find the hyperplane (in a high dimensional feature space) that separa"
W04-3249,C00-1066,0,0.0320323,", domain detection allows a number of useful simplifications in text processing applications, such as, for instance, in Word Sense Disambiguation (WSD). In this paper we introduce Domain Relevance Estimation (DRE) a fully unsupervised technique for domain detection. Roughly speaking, DRE can be viewed as a text categorization (TC) problem (Sebastiani, 2002), even if we do not approach the problem in the standard supervised setting requiring category labeled training data. In fact, recently, unsupervised approaches to TC have received more and more attention in the literature (see for example (Ko and Seo, 2000). We assume a pre-defined set of categories, each defined by means of a list of related terms. We call such categories domains and we consider them as a set of general topics (e.g. S PORT, M EDICINE, P OLITICS) that cover the main disciplines and areas of human activity. For each domain, the list of related words is extracted from W ORD N ET D O MAINS (Magnini and Cavagli`a, 2000), an extension of W ORD N ET in which synsets are annotated with domain labels. We have identified about 40 domains (out of 200 present in W ORD N ET D OMAINS) and we will use them for experiments throughout the paper"
W04-3249,magnini-cavaglia-2000-integrating,1,0.883007,"Missing"
W04-3249,S01-1005,0,0.012662,"- Table 2: W ORD N ET senses and domains for the word “bank”. from act#2, and playing field#1 from location#1. Domains may group senses of the same word into thematic clusters, which has the important sideeffect of reducing the level of ambiguity when we are disambiguating to a domain. Table 2 shows an example. The word “bank” has ten different senses in W ORD N ET 1.6: three of them (i.e. bank#1, bank#3 and bank#6) can be grouped under the E CONOMY domain, while bank#2 and bank#7 both belong to G EOGRAPHY and G EOL OGY. Grouping related senses is an emerging topic in WSD (see, for instance (Palmer et al., 2001)). Finally, there are W ORD N ET synsets that do not belong to a specific domain, but rather appear in texts associated with any domain. For this reason, a FACTOTUM label has been created that basically includes generic synsets, which appear frequently in different contexts. Thus the FACTOTUM domain can be thought of as a “placeholder” for all other domains. 3 Domain Relevance Estimation for Texts The basic idea of domain relevance estimation for texts is to exploit lexical coherence inside texts. From the domain point of view lexical coherence is equivalent to domain coherence, i.e. the fact"
W05-0608,W04-0856,1,0.479824,"ers of terms. Each cluster represents a semantic domain (Gliozzo et al., 2004), i.e. a set of terms that often co-occur in texts having similar topics. A Domain Model is represented by a k × k 0 rectangular matrix D, containing the degree of association among terms and domains, as illustrated in Table 1. HIV AIDS virus laptop M EDICINE 1 1 0.5 0 C OMPUTER S CIENCE 0 0 0.5 1 Table 1: Example of Domain Matrix 1 The idea of exploiting a Domain Kernel to help a supervised classification framework, has been profitably used also in other NLP tasks such as word sense disambiguation (see for example (Strapparava et al., 2004)). 57 Domain Models can be used to describe lexical ambiguity and variability. Lexical ambiguity is represented by associating one term to more than one domain, while variability is represented by associating different terms to the same domain. For example the term virus is associated to both the domain C OMPUTER S CIENCE and the domain M EDICINE (ambiguity) while the domain M EDICINE is associated to both the terms AIDS and HIV (variability). More formally, let D = {D1 , D2 , ..., Dk0 } be a set of domains, such that k 0  k. A Domain Model is fully defined by a k × k 0 domain matrix D repres"
W05-0802,P04-1067,0,0.0222367,"fferent news agencies), and it is not known if a func10 tion ψ exists, even if in principle it could exist and return 1 for a strict subset of document pairs. There exist many interesting works about using parallel corpora for multilingual applications (Melamed, 2001), such as Machine Translation, Cross language Information Retrieval (Littman et al., 1998), lexical acquisition, and so on. However it is not always easy to find or build parallel corpora. This is the main reason because the weaker notion of comparable corpora is a matter recent interest in the field of Computational Linguistics (Gaussier et al., 2004). The texts inside comparable corpora, being about the same topics (i.e. about the same semantic domains), should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevant entities and words that are not yet lexicalized in the language, are expressed by using their original terms. As a consequence the same entities will be denoted with the same words in different languages, allowing to automatically detect couples of translation pairs just by looking at the word shape (Koehn and Knight, 2002). Our hypothesis is that co"
W05-0802,W02-0902,0,0.011052,"Computational Linguistics (Gaussier et al., 2004). The texts inside comparable corpora, being about the same topics (i.e. about the same semantic domains), should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevant entities and words that are not yet lexicalized in the language, are expressed by using their original terms. As a consequence the same entities will be denoted with the same words in different languages, allowing to automatically detect couples of translation pairs just by looking at the word shape (Koehn and Knight, 2002). Our hypothesis is that comparable corpora contain a large amount of such words, just because texts, referring to the same topics in different languages, will often adopt the same terms to denote the same entities1 . However, the simple presence of these shared words is not enough to get significant results in TC tasks. As we will see, we need to exploit these common words to induce a second-order similarity for the other words in the lexicons. 3 The Multilingual Vector Space Model Let T = {t1 , t2 , . . . , tn } be a corpus, and V = {w1 , w2 , . . . , wk } be its vocabulary. In the monolingu"
W05-0802,magnini-cavaglia-2000-integrating,0,0.0339774,"Missing"
W05-0802,W04-0856,1,0.625109,"Section 6, this model is rather poor because of its sparseness. In the next section, we will show how to use such words as seeds to induce a Multilingual Domain VSM, in which second order relations among terms and documents in different languages are considered to improve the similarity estimation. 4 Multilingual Domain Models A MDM is a multilingual extension of the concept of Domain Model. In the literature, Domain Models have been introduced to represent ambiguity and variability (Gliozzo et al., 2004) and successfully exploited in many NLP applications, such us Word Sense Disambiguation (Strapparava et al., 2004), Text Categorization and Term Categorization. 11 A Domain Model is composed by soft clusters of terms. Each cluster represents a semantic domain, i.e. a set of terms that often co-occur in texts having similar topics. Such clusters identifies groups of words belonging to the same semantic field, and thus highly paradigmatically related. MDMs are Domain Models containing terms in more than one language. A MDM is represented by a matrix D, containing the degree of association among terms in all the languages and domains, as illustrated in Table 1. e/i HIV AIDS e/i viruse/i hospitale laptope M i"
W06-2608,P05-1050,1,0.535293,"k from the Vector Space Model (Salton and McGill, 1983) into the Domain Space (see Section 4), defined by the following mapping: C OMPUTER S CIENCE 0 0 0.5 1 Table 1: Example of Domain Model. ing a lexical coherence assumption (Gliozzo, 2005). To this aim, Term Clustering algorithms can be used: a different domain is defined for each cluster, and the degree of association between terms and clusters, estimated by the unsupervised learning algorithm, provides a domain relevance function. As a clustering technique we exploit Latent Semantic Analysis (LSA), following the methodology described in (Gliozzo et al., 2005b). This operation is done offline, and can be efficiently performed on large corpora. LSA is performed by means of SVD of the termby-document matrix T representing the corpus. The SVD algorithm can be exploited to acquire a domain matrix D from a large corpus in a totally unsupervised way. SVD decomposes the term-by-document matrix T into three matrices T = VΣ k UT where Σk is the diagonal k × k matrix containing the k singular values of T. D = VΣk0 where k 0  k. Once a DM has been defined by the matrix D, the Domain Space is a k 0 dimensional space, in which both texts and terms are represe"
W06-2608,P94-1013,0,\N,Missing
W10-3405,S07-1013,1,0.82428,"ional response and a positive/negative orientation related with this color. More than 150 subjects participated in this study, roughly equally partitioned in gender. There are two main reasons why we preferred to use this study for our evaluation procedure. Firstly, the presentation and organization of the results provide a good reference for our own experiments. In addition, it focusses on advertisement, which is one of the applicative fields we want to address in future work. 3 Methodology Sensing emotions from text is an appealing task of natural language processing (Pang and Lee, 29 2008; Strapparava and Mihalcea, 2007): the automatic recognition of affective states is becoming a fundamental issue in several domains such as human-computer interaction or sentiment analysis for opinion mining. Indeed, a large amount of textual material has become available form the Web (e.g. blogs, forums, social networks), raising the attractiveness of empirical methods analysis on this field. For representing the emotions, we exploit the methodology described in (Strapparava and Mihalcea, 2008). The idea underlying the method is the distinction between direct and indirect affective words. For direct affective words (i.e. wor"
W10-3405,strapparava-valitutti-2004-wordnet,1,0.614925,"sue in several domains such as human-computer interaction or sentiment analysis for opinion mining. Indeed, a large amount of textual material has become available form the Web (e.g. blogs, forums, social networks), raising the attractiveness of empirical methods analysis on this field. For representing the emotions, we exploit the methodology described in (Strapparava and Mihalcea, 2008). The idea underlying the method is the distinction between direct and indirect affective words. For direct affective words (i.e. words that directly denote emotions), authors refer to the W ORD N ET A FFECT (Strapparava and Valitutti, 2004) lexicon, a freely available extension of the W ORD N ET database which employs some basic emotion labels (e.g. anger, disgust, fear, joy, sadness) to annotate W ORD N ET synsets. For indirect affective words, a crucial aspect is building a mechanism to represent an emotion starting from affective lexical concepts and to introduce a semantic similarity among generic terms (and hence also words denoting colors) and these emotion representations. Latent Semantic Analysis is used to acquire, in an unsupervised setting, a vector space from the British National Corpus1 . In LSA, term cooccurrences"
W13-3816,E09-1005,0,0.020113,"uncts, and the semantic type of the complement filler(s). An example of an SFS is reported in example 1. The alignment of senses is based on the notion of lexical overlap. We used the Text::Similarity v.0.09 module2 to obtain the overlap value between two bags of words. Text similarity is based on counting the number of overlapping tokens between the two strings, normalized by the length of the strings. 4.2 Semantic Match: Exploiting Shallow Frames Structures Sense Similarity In the second approach, Sense Similarity, the basis for sense alignment is the Personalized Page Rank (PPR) algorithm (Eneko and Soroa, 2009) relying on a lexical-semantic knowledge base model as a graph G = (V, E) as available in the UKB tool suite3 . As knowledge base we have used WN 3.0 extended with the “Princeton Annotated Gloss Corpus”. Each vertex v of the graph is a 1. Marco ha comprato un libro. [Marco bought a book.] Verb: comprare [to buy] SFS: SUBJ[person] OBJ[artifact] To obtain the SFSs, two different strategies have been used. For the SCL, we have extracted all 1 http://wordnet.princeton.edu/glosstag.shtml 2 http://www.d.umn.edu/∼tpederse/text-similarity.html 3 http://ixa2.si.ehu.es/ukb/ 4 35 We use Google Translate"
W13-3816,jezek-quochi-2010-capturing,0,0.293585,"Missing"
W13-3816,lenci-etal-2012-lexit,0,0.217714,"Missing"
W13-3816,Q13-1013,0,0.198285,"ssing tasks, such as Word Sense Disambiguation, Information Extraction, and Question-Answering, among others. The creation of lexical-sematic resources is costly in terms of manual efforts and time, and often important information is scattered in different lexica and difficult to use. Semantic interoperability between resources could represent the viable solution to allow reusability and develop more robust and powerful resources. Word sense alignment (WSA), a research area which has seen an increasing interest in recent years, qualifies as the preliminary requirement for achieving this goal (Matuschek and Gurevych, 2013). The purpose of this work is to merge two Italian lexical-semantic resources, namely MultiWordNet (Pianta et al., 2002) (MWN) and Senso Comune Lexicon (SCL) (Oltramari et al., 2013), by automatically linking their entries. The final result will be two-folded. On the MWN side, this will provide Italian with a more complete and robust version of this lexicon. On the SCL side, the linking with MWN entries will introduce lexical-semantic relations, thus facilitating its use 2 Task and Resources Following (Matuschek and Gurevych, 2013), WSA can be defined as the identification of pairs of senses f"
W13-3816,N07-1025,0,0.0375074,"o et al., 2002). Ontological classification of verb entries will start in the near future. With respect to MWN, word senses are not hierarchically structured and no semantic relation has been encoded yet. Senses of polysemous entries have a flat representation, one following the other. 3 Related Works Previous works in WSA can be divided into two main groups: a.) approaches and frameworks which aim at linking entries to WN from lexica based on different models (Rigau and Agirre (1995); Navigli (2006); Roventini et al. (2007)) or language resources, such as Wikipedia (RuizCasado et al. (2005); Mihalcea (2007); Niemann and Gurevych (2011)), and b.) approaches towards the merging of different language resources (Navigli and Ponzetto (2012)). Our work clearly fits into the first group. While different methods are employed (similarity-based approaches vs. graphbased approaches), common elements of these works are: i.) the extensive use of the lexical knowledge of the sense descriptions; e.g. the WN glosses or an article first paragraph as in the case of Wikipedia; and ii.) the extension of the basic sense descriptions with additional information 4 Methodology The automatic alignment of senses has been"
W13-3816,N09-2066,0,0.0981197,"Missing"
W13-3816,P06-1014,0,0.0816085,"op level ontology is inspired by DOLCE (Descriptive Ontology for Linguistic and Cognitive Engineering) (Masolo et al., 2002). Ontological classification of verb entries will start in the near future. With respect to MWN, word senses are not hierarchically structured and no semantic relation has been encoded yet. Senses of polysemous entries have a flat representation, one following the other. 3 Related Works Previous works in WSA can be divided into two main groups: a.) approaches and frameworks which aim at linking entries to WN from lexica based on different models (Rigau and Agirre (1995); Navigli (2006); Roventini et al. (2007)) or language resources, such as Wikipedia (RuizCasado et al. (2005); Mihalcea (2007); Niemann and Gurevych (2011)), and b.) approaches towards the merging of different language resources (Navigli and Ponzetto (2012)). Our work clearly fits into the first group. While different methods are employed (similarity-based approaches vs. graphbased approaches), common elements of these works are: i.) the extensive use of the lexical knowledge of the sense descriptions; e.g. the WN glosses or an article first paragraph as in the case of Wikipedia; and ii.) the extension of the"
W13-3816,W11-0122,0,0.405855,"Ontological classification of verb entries will start in the near future. With respect to MWN, word senses are not hierarchically structured and no semantic relation has been encoded yet. Senses of polysemous entries have a flat representation, one following the other. 3 Related Works Previous works in WSA can be divided into two main groups: a.) approaches and frameworks which aim at linking entries to WN from lexica based on different models (Rigau and Agirre (1995); Navigli (2006); Roventini et al. (2007)) or language resources, such as Wikipedia (RuizCasado et al. (2005); Mihalcea (2007); Niemann and Gurevych (2011)), and b.) approaches towards the merging of different language resources (Navigli and Ponzetto (2012)). Our work clearly fits into the first group. While different methods are employed (similarity-based approaches vs. graphbased approaches), common elements of these works are: i.) the extensive use of the lexical knowledge of the sense descriptions; e.g. the WN glosses or an article first paragraph as in the case of Wikipedia; and ii.) the extension of the basic sense descriptions with additional information 4 Methodology The automatic alignment of senses has been conducted by applying three"
W13-3816,W03-1022,0,0.171912,"Missing"
W13-3816,pianta-etal-2008-textpro,0,0.0119654,"een conducted by applying three approaches Lexical Match, Sense Similarity and Semantic Match. 4.1 Lexical Match In the first approach, Lexical Match, for each word w and for each sense s in the given resources R ∈ {MWN, SCL} we constructed a sense description 34 dR (s) as a bag of words in Italian. Provided the different characteristics of the two resources, two different types of bag of words have been built. As for the SCL, the bag of words is represented by the lexical items in the textual definition of sw , automatically lemmatized and part-of-speech analyzed with the TextPro tool suite (Pianta et al., 2008) with standard stopword removal. On the other hand, for each synset, S, the sense description of each MWN synset was built by optionally exploiting: synset, and the edges represent semantic relations between synsets (e.g. hyperonymy, hyponymy, etc.). The PPR algorithm ranks the vertices in a graph according to their importance within the set and assigns stronger initial probabilities to certain kinds of vertices in the graph. The result of the PPR algorithm is a vector whose elements denote the probability for the corresponding vertex that a jumper ends on that vertex if randomly following the"
W13-3816,P07-2041,0,0.0199492,"y is inspired by DOLCE (Descriptive Ontology for Linguistic and Cognitive Engineering) (Masolo et al., 2002). Ontological classification of verb entries will start in the near future. With respect to MWN, word senses are not hierarchically structured and no semantic relation has been encoded yet. Senses of polysemous entries have a flat representation, one following the other. 3 Related Works Previous works in WSA can be divided into two main groups: a.) approaches and frameworks which aim at linking entries to WN from lexica based on different models (Rigau and Agirre (1995); Navigli (2006); Roventini et al. (2007)) or language resources, such as Wikipedia (RuizCasado et al. (2005); Mihalcea (2007); Niemann and Gurevych (2011)), and b.) approaches towards the merging of different language resources (Navigli and Ponzetto (2012)). Our work clearly fits into the first group. While different methods are employed (similarity-based approaches vs. graphbased approaches), common elements of these works are: i.) the extensive use of the lexical knowledge of the sense descriptions; e.g. the WN glosses or an article first paragraph as in the case of Wikipedia; and ii.) the extension of the basic sense descriptions"
W13-5502,baccianella-etal-2010-sentiwordnet,0,0.0652912,"Missing"
W13-5502,strapparava-valitutti-2004-wordnet,1,\N,Missing
W14-0140,baroni-etal-2004-introducing,0,0.0434723,"Missing"
W14-0140,E09-1005,0,0.0118341,"English). In BabelNet English WN 3.0 synsets have been aligned to their corresponding Wikipedia pages and then extended to other languages, including Italian, by exploiting Wikipedia language links and WN mappings. As for our task, we have retained only those BabelNet entries which have a corresponding synset word in MWN. In this way, we have extended the bag of words representation of nominal entries for MWN synsets by adding the Italian Wikipedia glosses from BabelNet. 4.2 In the second approach, Sense Similarity, the basis for sense alignment is the Personalized Page Rank (PPR) algorithm (Eneko and Soroa, 2009) relying on a lexical-semantic knowledge base model as a graph G = (V, E) as available in the UKB tool suite4 . As knowledge base we have used WN 3.0 extended with the “Princeton Annotated Gloss Corpus”. Each vertex v of the graph is a synset, and the edges represent semantic relations between synsets (e.g. hyperonymy, hyponymy, etc.). The PPR algorithm ranks the vertices in a graph according to their importance within the set and assigns stronger initial probabilities to certain kinds of vertices in the graph. The result of the PPR algorithm is a vector whose elements denotes the probability"
W14-0140,E12-1059,0,0.0264903,"Missing"
W14-0140,jezek-quochi-2010-capturing,0,0.0331949,"Missing"
W14-0140,Q13-1013,0,0.534544,"ty, for word sense alignment between MultiWordNet and a lexicographic dictionary, Senso Comune De Mauro, when having few sense descriptions (MultiWordNet) and no structure over senses (Senso Comune De Mauro). The results obtained from the merging of the two approaches are satisfying, with F1 values of 0.47 for verbs and 0.64 for nouns. 1 Introduction This work is situated in the field of word sense alignment, a research area which has seen an increasing interest in recent years and which is a key requirement for achieving semantic interoperability between different lexical-semantic resources (Matuschek and Gurevych, 2013). Our goal is to automatically import high-quality glosses in Italian in MultiWordNet (Pianta et al., 2002) (MWN) by aligning its synsets to the entries of a lexicographic dictionary, namely the Senso Comune De Mauro (SCDM), thus providing Italian with a more complete and robust version of MWN. For SCDM, the linking of the entries with MWN plays a double role. On the one hand, it will introduce lexical-semantic relations, thus facilitating its use for NLP tasks in Italian, and, on the other hand, it will make SCDM a structurally and semantically interoperable resource for Italian, to which oth"
W14-0140,I11-1099,0,0.0271442,"Missing"
W14-0140,N07-1025,0,0.251904,"Missing"
W14-0140,P06-1014,0,0.0964714,"ering) (Masolo et al., 2002). All nominal entries have been manually classified according to the ontological concepts and an ontological classification of verb entries will start in the near future. With respect to MWN, word senses are not hierarchically structured and no semantic relation is encoded. Senses of polysemous entries have a flat representation, one following the other. 3 Related Works Previous works in word sense alignment can be divided into two main groups: a.) approaches and frameworks which aim at linking lexica based on different models to WN synsets (Rigau and Eneko (1995); Navigli (2006); Roventini et al. (2007)) or language resources, such as Wikipedia (RuizCasado et al. (2005); Mihalcea (2007); Niemann and Gurevych (2011)), and b.) approaches towards the merging of different language resources (Gurevych et al. (2012); Navigli and Ponzetto (2012)). Our work clearly fits into the first group. While different methods are employed (similaritybased approaches vs. graph-based approaches), common elements of these works are: i.) the extensive use of lexical knowledge based on the sense descriptions such as the WN glosses or an article first paragraph as in the case of Wikipedia; a"
W14-0140,W11-0122,0,0.276737,"Missing"
W14-0140,pianta-etal-2008-textpro,0,0.019392,"ying two approaches for constructing the sense representations of the resources and evaluation. 4.1 Lexical Match In the first approach, Lexical Match, for each word w and for each sense s in the given resources R ∈ {MWN, SCDM} we constructed a sense descriptions dR (s) as a bag of words in Italian. Provided the different characteristics of the two resources, two different types of bag of words have been built. As for the SCDM, the bag of words is represented by the lexical items in the textual definition of sw , automatically lemmatized and partof-speech analyzed with the TextPro tool suite (Pianta et al., 2008) with standard stopword removal. On the other hand, for each synset, S, and for each part of speech in analysis, the sense description of each MWN synset was built by optionally exploiting: • the set of synset words in a synset excluding w; • the set of direct hypernyms of s in the taxonomy hierarchy in MWN; • the set of synset words in MWN standing in the relation of nearest synonyms with s; • the set of synset words in MWN composing the manually disambiguated glosses of s from the “Princeton Annotated Gloss Corpus”2 . To extract the corresponding Italian synset(s), we have ported MWN to WN 3"
W14-0140,P07-2041,0,0.365143,"Missing"
W14-4716,banea-etal-2008-bootstrapping,0,0.0227342,"ne kind of sense impression by using words that normally describe another. 2 The sensorial lexicon is publicly available, upon request to the authors. 3 http://www.crowdflower.com/ 4 http://www.mturk.com/mturk 115 Regarding the sentiment orientations and subjectivity levels of words, Sentiwordnet (Esuli and Sebastiani, 2006) is constructed as an extension to WordNet and it provides sentiments in synset level. Positive, negative and neutral values are assigned to synsets by using ternary classifiers and synset glosses. Another study that has been inspirational for the design of our approach is Banea et al. (2008). The authors generate a subjectivity lexicon starting with a set of seed words and then using a similarity measure among the seeds and the candidate words. Concerning the association between colors and words, Mohammad (2011) builds a color-word association lexicon by organizing a crowdsourcing task on Amazon Mechanical Turk. Instead, Özbal et al. (2011) aim to automate this process and propose three computational methods based on image analysis, language models and latent semantic analysis (LSA) (Landauer and Dumais, 1997). The authors compare these methods against a gold standard obtained by"
W14-4716,J90-1003,0,0.417686,"9 words (the candidate word, 4 words to its left and 4 words to its right). In this manner, we analyze the cooccurrence of each unique lemma-POS pair in the corpus with the sense seeds. We eliminate the candidates which have less than 5 cooccurences with the sense categories. Normalized Pointwise Mutual Information For the cooccurrence analysis of the candidate words and seeds, we use pointwise mutual information (PMI), which is simply a measure of association between the probability of the co-occurence of two events and their individual probabilities when they are assumed to be independent (Church and Hanks, 1990) and it is calculated as: [ ] p(x, y) P M I(x, y) = log (1) p(x)p(y) To calculate the PMI value of a candidate word and a specific sense, we consider p(x) as the probability of the candidate word to occur in the corpus. Therefore, p(x) is calculated as p(x) = c(x)/N , where c(x) is the total count of the occurences of the candidate word x in the corpus and N is the total cooccurrence count of all words in the corpus. Similarly, we calculate p(y) as the total occurrence count of all the 6 http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2011T07 118 majority class word sentence 3 4"
W14-4716,esuli-sebastiani-2006-sentiwordnet,0,0.00999963,"otations. Mohammad and Turney (2010) generate an emotion lexicon by using the crowdsourcing service provided by Amazon Mechanical Turk4 and it covers 14,200 term-emotion associations. 1 American Heritage Dictionary (http://ahdictionary.com/) defines synaesthesia in linguistics as the description of one kind of sense impression by using words that normally describe another. 2 The sensorial lexicon is publicly available, upon request to the authors. 3 http://www.crowdflower.com/ 4 http://www.mturk.com/mturk 115 Regarding the sentiment orientations and subjectivity levels of words, Sentiwordnet (Esuli and Sebastiani, 2006) is constructed as an extension to WordNet and it provides sentiments in synset level. Positive, negative and neutral values are assigned to synsets by using ternary classifiers and synset glosses. Another study that has been inspirational for the design of our approach is Banea et al. (2008). The authors generate a subjectivity lexicon starting with a set of seed words and then using a similarity measure among the seeds and the candidate words. Concerning the association between colors and words, Mohammad (2011) builds a color-word association lexicon by organizing a crowdsourcing task on Ama"
W14-4716,P03-1054,0,0.00643123,"ordingly, we treat each POS role of the same lemmas as a distinct seed and extract 4287 lemma-POS pairs from 2572 synsets. In this section, we explain the steps to construct our final sensorial lexicon in detail. Corpus and Candidate Words As a corpus, we use a subset of English GigaWord 5th Edition released by Linguistic Data Consortium (LDC)6 . This resource is a collection of almost 10 million English newswire documents collected in recent years, whose content sums up to nearly 5 billion words. The richly annotated GigaWord data comprises automatic parses obtained with the Stanford parser (Klein and Manning, 2003) so that we easily have access to the lemma and POS information of each word in the resource. For the scope of this study, we work on a randomly chosen subset that contains 79800 sentences and we define a co-occurrence event as the co-existence of a candidate word and a seed word within a window of 9 words (the candidate word, 4 words to its left and 4 words to its right). In this manner, we analyze the cooccurrence of each unique lemma-POS pair in the corpus with the sense seeds. We eliminate the candidates which have less than 5 cooccurences with the sense categories. Normalized Pointwise Mu"
W14-4716,S07-1009,0,0.0214301,"for copywriters to decide which sensory modalities to invoke while creating a slogan for a specific product category. 7 If the annotators gave additional answers to the expected ones, we considered their answers as correct. 120 4.2 Evaluation Measures Based on the annotation results of our crowdsourcing task, we propose an evaluation technique considering that a lemma-POS or a sentence might be associated with more than one sensory modalities. Similar to the evaluation framework defined by Özbal et al. (2011), we adapt the evaluation measures of SemEval2007 English Lexical Substitution Task (McCarthy and Navigli, 2007), where a system generates one or more possible substitutions for a target word in a sentence preserving its meaning. For a given lemma-POS or a sentence, which we will name as item in the rest of the section, we allow our system to provide as many sensorial associations as it determines using a specific lexicon. While evaluating a sense-item association of a method, a best and an oot score are calculated by considering the number of the annotators who associate that sense with the given item, the number of the annotators who associate any sense with the given item and the number of the senses"
W14-4716,H93-1061,0,0.343293,"2572 synsets yielding the highest performance when used to learn a sensorial model. 3.3 Sensorial Lexicon Construction Using Corpus Statistics After generating the seed lists consisting of synsets for each sense category with the help of a set of WordNet relations and a bootstrapping process, we use corpus statistics to create our final sensorial lexicon. More specifically, we exploit a probabilistic approach based on the co-occurence of the seeds and the candidate lexical entries. Since working on the synset level would raise the data sparsity problem in synset tagged corpora such as SemCor (Miller et al., 1993) and we need a corpus that provides sufficient statistical information, we migrate from synset level to lexical level. Accordingly, we treat each POS role of the same lemmas as a distinct seed and extract 4287 lemma-POS pairs from 2572 synsets. In this section, we explain the steps to construct our final sensorial lexicon in detail. Corpus and Candidate Words As a corpus, we use a subset of English GigaWord 5th Edition released by Linguistic Data Consortium (LDC)6 . This resource is a collection of almost 10 million English newswire documents collected in recent years, whose content sums up to"
W14-4716,W10-0204,0,0.231596,"ia1 , as it reinforces creative thinking and it is commonly exploited as an imagination boosting tool in advertisement slogans (Pricken, 2008). As an example, we can consider the slogans “Taste the rainbow” where the sense of sight is combined with the sense of taste or “Hear the big picture” where sight and hearing are merged. There are various studies both in computational linguistics and cognitive science that build resources associating words with several cognitive features such as abstractness-concreteness (Coltheart, 1981; Turney et al., 2011), emotions (Strapparava and Valitutti, 2004; Mohammad and Turney, 2010), colors (Özbal et al., 2011; Mohammad, 2011) and imageability (Coltheart, 1981). However, to the best of our knowledge, there is no attempt in the literature to build a resource that associates words with senses. In this paper, we propose a computational method to automatically generate a sensorial lexicon2 that associates words in English with senses. Our method consists of two main steps. First, we generate the initial seed words for each sense category with the help of a bootstrapping approach. Then, we exploit a corpus based probabilistic technique to create the final lexicon. We evaluate"
W14-4716,W11-0611,0,0.10184,"nly exploited as an imagination boosting tool in advertisement slogans (Pricken, 2008). As an example, we can consider the slogans “Taste the rainbow” where the sense of sight is combined with the sense of taste or “Hear the big picture” where sight and hearing are merged. There are various studies both in computational linguistics and cognitive science that build resources associating words with several cognitive features such as abstractness-concreteness (Coltheart, 1981; Turney et al., 2011), emotions (Strapparava and Valitutti, 2004; Mohammad and Turney, 2010), colors (Özbal et al., 2011; Mohammad, 2011) and imageability (Coltheart, 1981). However, to the best of our knowledge, there is no attempt in the literature to build a resource that associates words with senses. In this paper, we propose a computational method to automatically generate a sensorial lexicon2 that associates words in English with senses. Our method consists of two main steps. First, we generate the initial seed words for each sense category with the help of a bootstrapping approach. Then, we exploit a corpus based probabilistic technique to create the final lexicon. We evaluate this resource with the help of a gold standa"
W14-4716,strapparava-valitutti-2004-wordnet,1,0.746476,"nt especially by using synaesthesia1 , as it reinforces creative thinking and it is commonly exploited as an imagination boosting tool in advertisement slogans (Pricken, 2008). As an example, we can consider the slogans “Taste the rainbow” where the sense of sight is combined with the sense of taste or “Hear the big picture” where sight and hearing are merged. There are various studies both in computational linguistics and cognitive science that build resources associating words with several cognitive features such as abstractness-concreteness (Coltheart, 1981; Turney et al., 2011), emotions (Strapparava and Valitutti, 2004; Mohammad and Turney, 2010), colors (Özbal et al., 2011; Mohammad, 2011) and imageability (Coltheart, 1981). However, to the best of our knowledge, there is no attempt in the literature to build a resource that associates words with senses. In this paper, we propose a computational method to automatically generate a sensorial lexicon2 that associates words in English with senses. Our method consists of two main steps. First, we generate the initial seed words for each sense category with the help of a bootstrapping approach. Then, we exploit a corpus based probabilistic technique to create th"
W14-4716,W09-1127,0,0.0192714,"tion accuracy becomes stable or starts to decrease. The following sections explain the whole process in detail. 116 FrameNet Sense Seed Synsets MapNet Synset Set Expansion WordNet SVM Crossvalidation Break-Point Detection Figure 1: Bootstrapping procedure to expand the seed list. Extending the Seed List with WordNet While the initial sensory seed list obtained from FrameNet contains only 277 lemma-POS pairs, we extend this list by utilizing the semantic relations provided by WordNet. To achieve that, we first map each lemma-POS pair in the seed list to WordNet synsets with the help of MapNet (Tonelli and Pighin, 2009), which is a resource providing direct mapping between WordNet synsets and FrameNet lexical units. Then, we add to the list the synsets that are in WordNet relations direct antonymy, similarity, derived-from, derivationally-related, pertains-to, attribute and also-see with the already existing seeds. For instance, we add the synset containing the verb laugh for the synset of the verb cry with the relation direct antonymy, or the synset containing the adjective chilly for the synset of the adjective cold with the relation similarity. We prefer to use these relations as they might allow us to pr"
W14-4716,D11-1063,0,0.134643,"m such a resource is advertisement especially by using synaesthesia1 , as it reinforces creative thinking and it is commonly exploited as an imagination boosting tool in advertisement slogans (Pricken, 2008). As an example, we can consider the slogans “Taste the rainbow” where the sense of sight is combined with the sense of taste or “Hear the big picture” where sight and hearing are merged. There are various studies both in computational linguistics and cognitive science that build resources associating words with several cognitive features such as abstractness-concreteness (Coltheart, 1981; Turney et al., 2011), emotions (Strapparava and Valitutti, 2004; Mohammad and Turney, 2010), colors (Özbal et al., 2011; Mohammad, 2011) and imageability (Coltheart, 1981). However, to the best of our knowledge, there is no attempt in the literature to build a resource that associates words with senses. In this paper, we propose a computational method to automatically generate a sensorial lexicon2 that associates words in English with senses. Our method consists of two main steps. First, we generate the initial seed words for each sense category with the help of a bootstrapping approach. Then, we exploit a corpus"
W14-4716,P02-1053,0,0.00757497,"e, the sensorial lexicon associates the noun wine with [smell, taste, sight]. In this experiment, best scoring considers the associated senses as the best answer, smell, taste, sight according to the previous example, and calculates a score with respect to the best answer in the gold standard and the number of the senses in this answer. Instead, oot scoring takes the first two answers, smell and taste according to the previous example, and assigns the score accordingly. To determine the senses associated with a sentence for the second experiment, we use a method similar to the one proposed by Turney (2002). For each sense, we simply calculate the average score of the lemma-POS pairs in a sentence. We set a threshold value of 0 to decide whether a sentence is associated with a given sense. In this manner, we obtain a sorted list of average sensory scores for each sentence according to the three methods. For instance, the classifier based on the sensorial lexicon associates the sentence Smash it to pieces, love it to bits. with [touch, taste]. For the best score, only touch would be considered, whereas oot would consider both touch and taste. 4.4 Evaluation Results In Table 4, we list the F1 valu"
W14-4716,P07-2034,0,0.0189532,"e there is no attempt in the literature to automatically associate words with human senses, in this section we will summarize the most relevant studies that focused on linking words with various other cognitive features. There are several studies dealing with word-emotion associations. WordNet Affect Lexicon (Strapparava and Valitutti, 2004) maps WordNet (Fellbaum, 1998) synsets to various cognitive features (e.g., emotion, mood, behaviour). This resource is created by using a small set of synsets as seeds and expanding them with the help of semantic and lexical relations among these synsets. Yang et al. (2007) propose a collocation model with emoticons instead of seed words while creating an emotion lexicon from a corpus. Perrie et al. (2013) build a word-emotion association lexicon by using subsets of a human-annotated lexicon as seed sets. The authors use frequencies, counts, or unique seed words extracted from an ngram corpus to create lexicons in different sizes. They propose that larger lexicons with less accurate generation method perform better than the smaller human annotated lexicons. While a major drawback of manually generated lexicons is that they require a great deal of human labor, cr"
W14-4716,P98-1013,0,\N,Missing
W14-4716,C98-1013,0,\N,Missing
W15-1404,P98-1013,0,0.248634,"ce a novel hypothesis that metaphors are likely to also use sensorial words. To extract the sensorial associations of words, we use the following two resources. 3.1 Sensicon This resource (Tekiroglu et al., 2014) is a large sensorial lexicon that associates 22,684 English words with human senses. It is constructed by employing a two phased computational approach. In the first phase, a bootstrapping strategy is performed to generate a relatively large set of sensory seed words from a small set of manually selected seed words. Following an annotation task to select the seed words from FrameNet (Baker et al., 1998), WordNet relations are exploited to expand the sensory seed synsets that are acquired by mapping the seed words to WordNet synsets. At each bootstrapping cycle, a five-class sensorial classifier model is constructed over the seed synsets defined by their WordNet glosses. The expansion continues until the prediction performance of the model steadily drops. In the second phase, a corpus based method is utilized to estimate the association scores in the final lexicon. Each entry in the lexicon consists of a lemma and part-of-speech (POS) tag pair and their associations to the five human senses ("
W15-1404,W14-2302,0,0.499227,"ey are things, events, and properties that can be perceivable by human senses. Neuman et al. (2013) extend the abstractness/concreteness model of Turney et al. (2011) with a selectional preference approach in order to detect metaphors consisting of concrete concepts. They focus on three types of metaphors including i) a subject noun and an object noun associated by the verb to be (e.g., “God is a king”), ii) the metaphorical verb representing the act of a subject noun on an object noun (e.g., “The war absorbed his energy”), iii) metaphorical adjective-noun phrases (e.g., “sweet kid”). Beigman Klebanov et al. (2014) propose a supervised approach to predict the metaphoricity of all content words with any part-of-speech in a running text. The authors propose a model combining unigram, topic models, POS, and concreteness features. While unigram features contribute the most, concreteness features are found to be effective only for some of the sets. Based on the hypothesis that on the conceptual level, metaphors are shared across languages, rather than being lexical or language specific, Tsvetkov et al. (2014a) propose a metaphor detection system 33 with cross-lingual model transfer for English that exploits"
W15-1404,E06-1042,0,0.0721691,"is more complex among the sensory modalities. Williams (1976) constitutes a generalized mapping for the synaesthetic metaphorical transfer by means of the diachronic semantic change of sensorial adjectives. Having regard to the citation dates of adjective meanings from Oxford English Dictionary2 and Middle English Dictionary3 , the regular transfer rules among the sensorial modalities are introduced. Several techniques for metaphor identification have been explored, including selectional preference violations (Fass, 1991; Neuman et al., 2013) or verb and noun clustering (Shutova et al., 2010; Birke and Sarkar, 2006; Shutova and Sun, 2013), supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2014a). As well as the identification techniques, different cognitive properties such as imageability (Broadwell et al., 2013; Tsvetkov et al., 2014a) and concreteness of the metaphor constituents (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2014a), or lexical semantic properties such as supersenses (Hovy et al., 2013; Tsvetkov et al., 2014a) have been exploited. While detecting and interpreting metaphors, imageability and concreteness features are generally utilized"
W15-1404,J91-1003,0,0.219046,"976; Shen, 1997) propose that the mapping in the synaesthetic metaphorical transfer is more complex among the sensory modalities. Williams (1976) constitutes a generalized mapping for the synaesthetic metaphorical transfer by means of the diachronic semantic change of sensorial adjectives. Having regard to the citation dates of adjective meanings from Oxford English Dictionary2 and Middle English Dictionary3 , the regular transfer rules among the sensorial modalities are introduced. Several techniques for metaphor identification have been explored, including selectional preference violations (Fass, 1991; Neuman et al., 2013) or verb and noun clustering (Shutova et al., 2010; Birke and Sarkar, 2006; Shutova and Sun, 2013), supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2014a). As well as the identification techniques, different cognitive properties such as imageability (Broadwell et al., 2013; Tsvetkov et al., 2014a) and concreteness of the metaphor constituents (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2014a), or lexical semantic properties such as supersenses (Hovy et al., 2013; Tsvetkov et al., 2014a) have been exploited. While det"
W15-1404,W06-3506,0,0.330383,"generalized mapping for the synaesthetic metaphorical transfer by means of the diachronic semantic change of sensorial adjectives. Having regard to the citation dates of adjective meanings from Oxford English Dictionary2 and Middle English Dictionary3 , the regular transfer rules among the sensorial modalities are introduced. Several techniques for metaphor identification have been explored, including selectional preference violations (Fass, 1991; Neuman et al., 2013) or verb and noun clustering (Shutova et al., 2010; Birke and Sarkar, 2006; Shutova and Sun, 2013), supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2014a). As well as the identification techniques, different cognitive properties such as imageability (Broadwell et al., 2013; Tsvetkov et al., 2014a) and concreteness of the metaphor constituents (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2014a), or lexical semantic properties such as supersenses (Hovy et al., 2013; Tsvetkov et al., 2014a) have been exploited. While detecting and interpreting metaphors, imageability and concreteness features are generally utilized to identify the metaphorical transfer from a more concrete to a less conc"
W15-1404,W13-0907,0,0.132722,"ve been explored, including selectional preference violations (Fass, 1991; Neuman et al., 2013) or verb and noun clustering (Shutova et al., 2010; Birke and Sarkar, 2006; Shutova and Sun, 2013), supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2014a). As well as the identification techniques, different cognitive properties such as imageability (Broadwell et al., 2013; Tsvetkov et al., 2014a) and concreteness of the metaphor constituents (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2014a), or lexical semantic properties such as supersenses (Hovy et al., 2013; Tsvetkov et al., 2014a) have been exploited. While detecting and interpreting metaphors, imageability and concreteness features are generally utilized to identify the metaphorical transfer from a more concrete to a less concrete or from a more imageable to a less imageable word. However, in synaesthetic metaphors, the imageability or concreteness levels of both tenor and vehicle (or tar2 3 http://www.oed.com/ http://quod.lib.umich.edu/m/med/ get and source) words can be similar. For instance, according to the MRC Psycholinguistic Database (MRCPD) (Coltheart, 1981) the concreteness (C) and im"
W15-1404,W13-0904,0,0.241665,"r the synaesthetic metaphorical transfer by means of the diachronic semantic change of sensorial adjectives. Having regard to the citation dates of adjective meanings from Oxford English Dictionary2 and Middle English Dictionary3 , the regular transfer rules among the sensorial modalities are introduced. Several techniques for metaphor identification have been explored, including selectional preference violations (Fass, 1991; Neuman et al., 2013) or verb and noun clustering (Shutova et al., 2010; Birke and Sarkar, 2006; Shutova and Sun, 2013), supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2014a). As well as the identification techniques, different cognitive properties such as imageability (Broadwell et al., 2013; Tsvetkov et al., 2014a) and concreteness of the metaphor constituents (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2014a), or lexical semantic properties such as supersenses (Hovy et al., 2013; Tsvetkov et al., 2014a) have been exploited. While detecting and interpreting metaphors, imageability and concreteness features are generally utilized to identify the metaphorical transfer from a more concrete to a less concrete or from a more i"
W15-1404,P14-2058,1,0.806606,"adjective dark is related to sight as the literal sense association, Sensicon assigns very high association values to both sight and taste. While this tendency would be helpful as a hint for identifying synaesthetic words, metaphor identification task would need a complementary wordsense association resource that could highlight the literal sense association of a word. 3.2 Dependency-parsed corpus (DPC) As an alternative to Sensicon for building wordsense associations, we extract this information from a corpus of dependency-parsed sentences. To ¨ achieve that, we follow a similar approach to Ozbal et al. (2014) and use a database that stores, for each relation in the dependency treebank of LDC GigaWord 5th Edition corpus5 ), its occurrences with specific “governors” (heads) and “dependents” (modifiers). To determine the sensorial load of a noun n, we first count how many times n occurs with the verb lemmas ‘see’, ‘smell’, ‘hear’, ‘touch’ and ‘taste’ in a direct object (dobj) syntactic relation in the database. Then, we divide each count by the number of times n appears in a direct object syntactic relation independently of the head that it is connected to. More specifically, the probability that n i"
W15-1404,W14-2303,0,0.0116024,"in Section 5, we draw our conclusions and outline possible future directions. 2 Related Work Mohler et al. (2013) exploit a supervised classification approach to detect linguistic metaphors. In this work, they first produce a domain-specific semantic signature which can be found to be encoded in the semantic network (linked senses) of WordNet, Wikipedia4 links and corpus collocation statistics. A set of binary classifiers are actuated to detect metaphoricity within a text by comparing its seman4 32 http://www.wikipedia.org/ tic signature to the semantic signatures of a set of known metaphors. Schulder and Hovy (2014) consider the term relevance as an indicator of being non-literal and propose that novel metaphorical words are less prone to occur in the typical vocabulary of a text. The performance of this approach is evaluated both as a standalone metaphor classifier and as a component of a classifier using lexical properties of the words such as part-of-speech roles. The authors state that term relevance could improve the random baselines for both tasks and it could especially be useful in case of a sparse dataset. Rather than an anomaly in the language or a simple word sense disambiguation problem, a co"
W15-1404,N13-1118,0,0.0145174,"e sensory modalities. Williams (1976) constitutes a generalized mapping for the synaesthetic metaphorical transfer by means of the diachronic semantic change of sensorial adjectives. Having regard to the citation dates of adjective meanings from Oxford English Dictionary2 and Middle English Dictionary3 , the regular transfer rules among the sensorial modalities are introduced. Several techniques for metaphor identification have been explored, including selectional preference violations (Fass, 1991; Neuman et al., 2013) or verb and noun clustering (Shutova et al., 2010; Birke and Sarkar, 2006; Shutova and Sun, 2013), supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2014a). As well as the identification techniques, different cognitive properties such as imageability (Broadwell et al., 2013; Tsvetkov et al., 2014a) and concreteness of the metaphor constituents (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2014a), or lexical semantic properties such as supersenses (Hovy et al., 2013; Tsvetkov et al., 2014a) have been exploited. While detecting and interpreting metaphors, imageability and concreteness features are generally utilized to identify the metapho"
W15-1404,C10-1113,0,0.255439,"Missing"
W15-1404,D14-1160,1,0.8412,"e using a bilingual dictionary. To the best of our knowledge, this system is the current state of the art for metaphor detection in English and constitutes the baseline for our experiments. 3 Word-Sense Associations Following the hypothesis of Broadwell et al. (2013) that “Metaphors are likely to use highly imageable words, and words that are generally more imageable than the surrounding context”, we introduce a novel hypothesis that metaphors are likely to also use sensorial words. To extract the sensorial associations of words, we use the following two resources. 3.1 Sensicon This resource (Tekiroglu et al., 2014) is a large sensorial lexicon that associates 22,684 English words with human senses. It is constructed by employing a two phased computational approach. In the first phase, a bootstrapping strategy is performed to generate a relatively large set of sensory seed words from a small set of manually selected seed words. Following an annotation task to select the seed words from FrameNet (Baker et al., 1998), WordNet relations are exploited to expand the sensory seed synsets that are acquired by mapping the seed words to WordNet synsets. At each bootstrapping cycle, a five-class sensorial classifi"
W15-1404,P14-1024,0,0.519749,"taphorical transfer by means of the diachronic semantic change of sensorial adjectives. Having regard to the citation dates of adjective meanings from Oxford English Dictionary2 and Middle English Dictionary3 , the regular transfer rules among the sensorial modalities are introduced. Several techniques for metaphor identification have been explored, including selectional preference violations (Fass, 1991; Neuman et al., 2013) or verb and noun clustering (Shutova et al., 2010; Birke and Sarkar, 2006; Shutova and Sun, 2013), supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2014a). As well as the identification techniques, different cognitive properties such as imageability (Broadwell et al., 2013; Tsvetkov et al., 2014a) and concreteness of the metaphor constituents (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2014a), or lexical semantic properties such as supersenses (Hovy et al., 2013; Tsvetkov et al., 2014a) have been exploited. While detecting and interpreting metaphors, imageability and concreteness features are generally utilized to identify the metaphorical transfer from a more concrete to a less concrete or from a more imageable to a less imag"
W15-1404,tsvetkov-etal-2014-augmenting-english,0,0.150645,"taphorical transfer by means of the diachronic semantic change of sensorial adjectives. Having regard to the citation dates of adjective meanings from Oxford English Dictionary2 and Middle English Dictionary3 , the regular transfer rules among the sensorial modalities are introduced. Several techniques for metaphor identification have been explored, including selectional preference violations (Fass, 1991; Neuman et al., 2013) or verb and noun clustering (Shutova et al., 2010; Birke and Sarkar, 2006; Shutova and Sun, 2013), supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2014a). As well as the identification techniques, different cognitive properties such as imageability (Broadwell et al., 2013; Tsvetkov et al., 2014a) and concreteness of the metaphor constituents (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2014a), or lexical semantic properties such as supersenses (Hovy et al., 2013; Tsvetkov et al., 2014a) have been exploited. While detecting and interpreting metaphors, imageability and concreteness features are generally utilized to identify the metaphorical transfer from a more concrete to a less concrete or from a more imageable to a less imag"
W15-1404,D11-1063,0,0.592604,"s among the sensorial modalities are introduced. Several techniques for metaphor identification have been explored, including selectional preference violations (Fass, 1991; Neuman et al., 2013) or verb and noun clustering (Shutova et al., 2010; Birke and Sarkar, 2006; Shutova and Sun, 2013), supervised classification (Gedigian et al., 2006; Mohler et al., 2013; Tsvetkov et al., 2014a). As well as the identification techniques, different cognitive properties such as imageability (Broadwell et al., 2013; Tsvetkov et al., 2014a) and concreteness of the metaphor constituents (Neuman et al., 2013; Turney et al., 2011; Tsvetkov et al., 2014a), or lexical semantic properties such as supersenses (Hovy et al., 2013; Tsvetkov et al., 2014a) have been exploited. While detecting and interpreting metaphors, imageability and concreteness features are generally utilized to identify the metaphorical transfer from a more concrete to a less concrete or from a more imageable to a less imageable word. However, in synaesthetic metaphors, the imageability or concreteness levels of both tenor and vehicle (or tar2 3 http://www.oed.com/ http://quod.lib.umich.edu/m/med/ get and source) words can be similar. For instance, acco"
W16-0430,S15-2077,1,0.836503,"-educated to the nuances of emotions and benefit from them, and NLP should help about this issue. Event based emotions. Emotions are elicited by significant and specific events, and events are significant when they touch on one or more of the concerns of the people. Even considering the traditional sentiment analysis, it is a good idea to investigate a more holistic approach, combining the detection of implicit polarity with the expression of opinions on events. For example we proposed CLIPEval, a task based on a dataset of events annotated as instantiations of pleasant and unpleasant events (Russo et al., 2015). Research efforts along this direction will be fruitful. This holds even more when we consider emotion classification. I think that psychological research and cognitive science can help substantially. Cultural differences from corpora. Even an excellent human translator has problems in carrying over the target language all the culture-related aspects that go with words. If the focus is on emotionrelated aspects, the matter is even subtler. The relation of a word to emotion concepts may depend on ideology and in general on cultural aspects that can be inferred from extensive word usage rather"
W16-0430,strapparava-valitutti-2004-wordnet,1,0.590434,"Missing"
W16-4310,H05-1073,0,0.386463,"emotions are the most basic. Nevertheless, most of the work in automatic detection of emotions in text has focused on the limited set of proposed basic emotions, since this allows reducing the cost in terms of time and money. Even though there also are approaches based on non-basic emotions. Most of the emotional resources developed so far have been annotated manually, since, in this way, machine learning systems learn from human annotations that are generally more accurate. Among these resources, we can find corpora labelled with the six basic emotions categories proposed by Ekman such as: (Alm et al., 2005) annotated a sentence-level corpus of approximately 185 children stories with emotion categories; (Aman and Szpakowicz, 2007) annotated blog posts collected directly from Web with emotion categories and intensity; or (Strapparava and Mihalcea, 2007) annotated news headlines with emotion categories and valence. As mentioned previously, there are corpora labelled with other small set of emotions by manually annotation like: (Neviarouskaya et al., 2009) corpus extracted 1,000 sentences from various stories; Emotiblog-corpus that consists of a collection of blog posts manually extracted from the W"
W16-4310,E14-1058,0,0.0138054,"the Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 91–100, Osaka, Japan, December 12 2016. ment manual process where human annotators will determine which is the predominant emotion between the emotional categories selected in phase 1. By means of proposing innovation in terms of annotation methodology, our aim is to reduce the complexity of emotion annotation task through reducing the number of emotional categories automatically, since the influence of the number of coding categories on reliability estimation is really important. As Antoine et al. (2014) concluded, the agreement values increase significantly when the number of classes decreases. Hence, our hypothesis is that the decrease of complexity of emotion annotation task through the reduction of the number of emotional categories will allow us to improve the reliability on the task. This methodology will allow us annotating large amount of emotional data in any genre efficiently and with guarantee of high standards of reliability. Our proposal in this paper is to show and evaluate the pre-annotation process to analyse the feasibility and the benefits by the methodology proposed. The re"
W16-4310,W10-0205,0,0.0185675,"set was used: anger-disgust, fear, joy, sadness, and surprise. Aman corpus. This dataset contains sentence-level annotation of 4,000 sentences from blogs posts collected directly from Web. This resource was annotated manually with the six emotion categories proposed by Ekman and the emotion intensity (high, medium, or low). These corpora are selected because of several reasons: (i) both corpora are manually annotated allowing us to compare automatic annotation to manual annotation; (ii) they are relevant to emotion detection task since they have been employed in many works to detect emotions (Keshtkar and Inkpen, 2010; Chaffar and Inkpen, 2011; Mohammad, 2012b); and (iii) these corpora allow us to test our approach about corpora with different sources of information: tales and blogs from Web. Thus, the usability and effectiveness of our approach can be checked. 93 3.2 Selecting Emotional Seed Words In this section, the process of creation the emotional seed words employing an emotional resource is presented. This approach employs NRC Word-Emotion Association Lexicon (Emolex) (Version 0.92) (Mohammad and Turney, 2013) as emotional lexicon, although the process can be adapted to another resource annotated wi"
W16-4310,L16-1183,0,0.0230311,"(Strapparava and Mihalcea, 2007) annotated news headlines with emotion categories and valence. As mentioned previously, there are corpora labelled with other small set of emotions by manually annotation like: (Neviarouskaya et al., 2009) corpus extracted 1,000 sentences from various stories; Emotiblog-corpus that consists of a collection of blog posts manually extracted from the Web and annotated with three annotation levels: document, sentence and element (Boldrini and Mart´ınez-Barco, 2012); or EmoTweet-28 corpus that consists of a collection of tweets annotated with 28 emotion categories (Liew et al., 2016). The common feature of these emotional corpora is that have been annotated manually, a hard and time-consuming task where the obtaining an agreement between annotations is a challenge, due to the subjectivity of the task and the need to invest in many resources to annotate large scale emotional corpora. Consequently and with the aim of overcoming the cost and time consuming shortcoming of manual annotation, several emotional resources have recently been developed employing emotion word hashtags to create automatic emotional corpus on Twitter. (Mohammad, 2012a) describe how they created a corp"
W16-4310,P14-5010,0,0.00719864,"is based on the estimation the similarity among them. For this reason, in this paper we test a model based on global matrix factorization methods: GloVe (Pennington et al., 2014). This model is run with the default settings, 300 dimensions and on the lemmas of the British National Corpus (BNC)3 that can be considered as a balanced resource since it includes texts from different genres and domains The process of the association consists of: • Step 1 - Emotional distributional vector: each sentence is pre-processed (tokenization, lemmatization and Part-Of-Speech Tagger) using Stanford Core NLP (Manning et al., 2014) and then is represented by a distributional vector adding up the vectors of their words (noum, verbs, adjectives and adverbs). Figure 3 shows an example for the sentence ’The bear in great fury ran after the carriage’. • Step 2 - Emotions-Sentences Association: the process measure the similarity between the vector of the sentence and the vectors of each emotional category and associates the three emotions whose semantic similarity is higher. Figure 3 shows the pre-annotated emotions for the example sentence, among which is the emotion of the gold standard of Alm corpus: ANGER-DISGUST. Figure"
W16-4310,S12-1033,0,0.108625,"ed with 28 emotion categories (Liew et al., 2016). The common feature of these emotional corpora is that have been annotated manually, a hard and time-consuming task where the obtaining an agreement between annotations is a challenge, due to the subjectivity of the task and the need to invest in many resources to annotate large scale emotional corpora. Consequently and with the aim of overcoming the cost and time consuming shortcoming of manual annotation, several emotional resources have recently been developed employing emotion word hashtags to create automatic emotional corpus on Twitter. (Mohammad, 2012a) describe how they created a corpus from Twitter post (Twitter Emotional Corpus - TEC) using this technique. In literature, several works can be found with the use emotion word hashtags to create emotional corpora from Twitter (Choudhury et al., 2012; Wang et al., 2012). Thus, in Sentiment Analysis research community, the interest of developing amounts of emotional This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ 92 Licence details: http:// corpora has increased because that would allow us to obtain better supervised m"
W16-4310,N12-1071,0,0.0884996,"ed with 28 emotion categories (Liew et al., 2016). The common feature of these emotional corpora is that have been annotated manually, a hard and time-consuming task where the obtaining an agreement between annotations is a challenge, due to the subjectivity of the task and the need to invest in many resources to annotate large scale emotional corpora. Consequently and with the aim of overcoming the cost and time consuming shortcoming of manual annotation, several emotional resources have recently been developed employing emotion word hashtags to create automatic emotional corpus on Twitter. (Mohammad, 2012a) describe how they created a corpus from Twitter post (Twitter Emotional Corpus - TEC) using this technique. In literature, several works can be found with the use emotion word hashtags to create emotional corpora from Twitter (Choudhury et al., 2012; Wang et al., 2012). Thus, in Sentiment Analysis research community, the interest of developing amounts of emotional This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ 92 Licence details: http:// corpora has increased because that would allow us to obtain better supervised m"
W16-4310,D14-1162,0,0.0838282,"ory is represented by a bag of words. Figure 2 shows an example for ANGER, DISGUST and SADNESS emotions. • Step 2 - Corpus adaptation: These bags of words are adapted to each corpus removing those words that not appear in the corpus. In this manner, the seed contains only the emotional words employed in the corpus to annotate. Figure 2 shows an example of the adaptation process for Alm corpus. • Step 3 - Emotional distributional vector: Each seed is transformed into a distributional vector adding up the distributional vectors of each word contained in the seed. To achieve that, a GloVe model (Pennington et al., 2014) built from the lemmas and POS of the British National Corpus (BNC)1 is employed. This model is explained in detail in Section 3.3. Once the process is completed, each emotion is represented by a distributional vector, a real-valued vector that stores its semantic features. Moreover, the process also creates a vector for a NEUTRAL category with the Emolex words not associated with the Ekman’s basic emotions. Figure 2: Creation of the emotional seed words for ANGER, DISGUST and SADNESS emotions (sample). 1 http://www.natcorp.ox.ac.uk/ 94 3.3 Associating Emotions with Sentences After having the"
W16-4310,S07-1013,1,0.69063,"ugh there also are approaches based on non-basic emotions. Most of the emotional resources developed so far have been annotated manually, since, in this way, machine learning systems learn from human annotations that are generally more accurate. Among these resources, we can find corpora labelled with the six basic emotions categories proposed by Ekman such as: (Alm et al., 2005) annotated a sentence-level corpus of approximately 185 children stories with emotion categories; (Aman and Szpakowicz, 2007) annotated blog posts collected directly from Web with emotion categories and intensity; or (Strapparava and Mihalcea, 2007) annotated news headlines with emotion categories and valence. As mentioned previously, there are corpora labelled with other small set of emotions by manually annotation like: (Neviarouskaya et al., 2009) corpus extracted 1,000 sentences from various stories; Emotiblog-corpus that consists of a collection of blog posts manually extracted from the Web and annotated with three annotation levels: document, sentence and element (Boldrini and Mart´ınez-Barco, 2012); or EmoTweet-28 corpus that consists of a collection of tweets annotated with 28 emotion categories (Liew et al., 2016). The common fe"
W16-4310,J08-4004,0,\N,Missing
W17-5042,de-marneffe-etal-2006-generating,0,0.0512094,"Missing"
W17-5042,W17-1217,1,0.888704,"Missing"
W17-5042,guthrie-etal-2006-closer,0,0.0256693,"masi et al. (2017). 3 3.1.2 Function words are the most common words in a language (e.g., articles, determiners, conjunctions). They are considered one of the most important stylometric features (Kestemont, 2014). Function words can be seen as indicators of the grammatical relations between other words. We use a set of 318 English function words from the scikit-learn package (Pedregosa et al., 2011). Other examined function word lists obtained from the Natural Language Toolkit1 (127 function words) and the Onix Text Retrieval Toolkit2 (429 function words), as well as function word skip-grams (Guthrie et al., 2006) did not lead to an improvement in accuracy. Methodology 3.1.3 Our system incorporates a wide range of features, i.e., word, lemma, and POS n-grams, spelling error character n-grams, typed character n-grams, and syntactic n-grams. We used the tokenized version of essays provided by the organizers. For the evaluation of our approach, we merged the training and development sets, and conducted experiments under 10-fold cross-validation. System performance was measured in terms of both classification accuracy and F1 (macro) score. The former was used as evaluation metric in the majority of previou"
W17-5042,D14-1142,0,0.461087,"utational Paralinguistics Challenge (Schuller et al., 2016) and the VarDial workshop (Malmasi et al., 2016), this year’s competition covers an NLI task based on the spoken response. Overall, this year’s task consists of three tracks: NLI on the essay only, NLI on the spoken response only, and NLI on both essay and spoken response. In this paper, we describe the CIC-FBK approach to the essay-only track. Previous works on identifying the native language from texts explored a large variety of features, including lexical and part-of-speech (POS) features (Koppel et al., 2005a), character n-grams (Ionescu et al., 2014), spelling errors (Koppel et al., 2005b), and syntactic features (Wong and Dras, 2011). Following previous research on the NLI task, we incorporate commonly used word n-grams, lemma n-grams, POS n-grams, and function words. In order to capture the L1 influences at the character level, we use recently introduced character n-grams from misspelled words (Chen et al., 2017), as well as 10 categories of character n-gram features proposed by Sapkota et al. (2015). We also include syntactic features by extracting syntactic dependencybased n-grams of words and of syntactic relation tags (Sidorov et al"
W17-5042,P17-2086,1,0.400902,"roach to the essay-only track. Previous works on identifying the native language from texts explored a large variety of features, including lexical and part-of-speech (POS) features (Koppel et al., 2005a), character n-grams (Ionescu et al., 2014), spelling errors (Koppel et al., 2005b), and syntactic features (Wong and Dras, 2011). Following previous research on the NLI task, we incorporate commonly used word n-grams, lemma n-grams, POS n-grams, and function words. In order to capture the L1 influences at the character level, we use recently introduced character n-grams from misspelled words (Chen et al., 2017), as well as 10 categories of character n-gram features proposed by Sapkota et al. (2015). We also include syntactic features by extracting syntactic dependencybased n-grams of words and of syntactic relation tags (Sidorov et al., 2014) using the algorithm designed by Posadas-Dur´an et al. (2014, 2017). We describe the features used by the CIC-FBK system in more detail in subsection 3.1. Our system achieved 0.8808 macro-averaged F1-score and 0.8809 accuracy in the essay-only We present the CIC-FBK system, which took part in the Native Language Identification (NLI) Shared Task 2017. Our approac"
W17-5042,W14-0908,0,0.107759,"provided for each essay. The training, development, and test sets are balanced in terms of the number of essays per L1 group. The 11 L1s covered by the corpus are: Arabic (ARA), Chinese (CHI), French (FRE), German (GER), Hindi (HIN), Italian (ITA), Japanese (JAP), Korean (KOR), Spanish (SPA), Telugu (TEL), and Turkish (TUR). The detailed description of the corpus and its statistics can be found in Malmasi et al. (2017). 3 3.1.2 Function words are the most common words in a language (e.g., articles, determiners, conjunctions). They are considered one of the most important stylometric features (Kestemont, 2014). Function words can be seen as indicators of the grammatical relations between other words. We use a set of 318 English function words from the scikit-learn package (Pedregosa et al., 2011). Other examined function word lists obtained from the Natural Language Toolkit1 (127 function words) and the Onix Text Retrieval Toolkit2 (429 function words), as well as function word skip-grams (Guthrie et al., 2006) did not lead to an improvement in accuracy. Methodology 3.1.3 Our system incorporates a wide range of features, i.e., word, lemma, and POS n-grams, spelling error character n-grams, typed ch"
W17-5042,N15-1160,0,0.0578646,") of a language learner based on his/her writing in the second language (L2). Identifying the native language is based on the hypothesis that the L1 of a learner impacts his/her L2 writing due to the language transfer effect. NLI can be used for a variety of purposes, including marketing, security, and educational applications. From the machinelearning perspective, the NLI task is viewed as a multi-class, single-label classification problem, in which automatic methods have to assign class labels (L1s) to objects (texts). Recent trends in NLI include cross-genre and cross-corpus NLI scenarios (Malmasi and Dras, 2015a), as well as identifying the L1 based on writings in other non-English L2s and cross374 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 374–381 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics track and shared the 1st rank in the NLI Shared Task 2017 scoring, obtaining the 2nd absolute score with the difference of 0.0010 F1-score and 0.0009 accuracy with the 1st place. 2 digit by the same symbol (e.g., 12,345 → 00,000), as proposed in Markov et al. (2017), to capture the format (e.g., 00.000 vs. 00"
W17-5042,N15-1010,0,0.0354815,"Missing"
W17-5042,W17-5007,0,0.135597,"oses. The corpus consists of 13,200 essays (1,000 essays per L1 for training, 100 for development, and 100 for testing). The essays are sampled from 8 prompts, and score levels (low/medium/high) are provided for each essay. The training, development, and test sets are balanced in terms of the number of essays per L1 group. The 11 L1s covered by the corpus are: Arabic (ARA), Chinese (CHI), French (FRE), German (GER), Hindi (HIN), Italian (ITA), Japanese (JAP), Korean (KOR), Spanish (SPA), Telugu (TEL), and Turkish (TUR). The detailed description of the corpus and its statistics can be found in Malmasi et al. (2017). 3 3.1.2 Function words are the most common words in a language (e.g., articles, determiners, conjunctions). They are considered one of the most important stylometric features (Kestemont, 2014). Function words can be seen as indicators of the grammatical relations between other words. We use a set of 318 English function words from the scikit-learn package (Pedregosa et al., 2011). Other examined function word lists obtained from the Natural Language Toolkit1 (127 function words) and the Onix Text Retrieval Toolkit2 (429 function words), as well as function word skip-grams (Guthrie et al., 20"
W17-5042,W16-4801,0,0.0936439,"Missing"
W17-5042,P12-2038,0,0.0699804,"r n-grams (n = 4) outperformed traditional character n-grams of the same size in most system configurations. In addition, we compared the performance of typed and traditional character n-grams on the 7-way ICLEv2 corpus (Granger et al., 2009), following the corpus splitting as described in Ionescu et al. (2014). In this experiment, typed character n-grams proved to be more indicative than traditional character n-grams when used in combination with features described in this paper. 3.1.5 Syntactic features, including production rules (Wong and Dras, 2011) and Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), have been previously explored for NLI. Tetreault et al. (2012) experimented with the Stanford parser (de Marneffe et al., 2006) dependency features and concluded that they are strong indicators of structural differences in L2 writing. We exploit the Stanford dependencies to build syntactic n-gram features by using the algorithm designed and made available by Posadas-Dur´an et al. (2014, 2017).3 Consider the following sample sentence: (1) Lisa said, “John should repair it tomorrow.” The character n-grams (n = 4) for the sample sentence (1) for each of the categories proposed by Sapkota et al."
W17-5042,W13-1706,0,0.39194,"r, since these modifications showed only marginal accuracy variations and did not improve system performance on the test set, the results for these runs are omitted in this paper. The confusion matrix for our best run is shown in Figure 1. The highest level of confusion is between Hindi and Telugu classes. Korean and Japanese is another problematic language pair, in which Korean native speakers are often classified as Japanese. The highest accuracy of 0.9800 was achieved for German native speakers. These results are in line with the ones reported in the previous edition of the NLI share task (Tetreault et al., 2013), where the teams achieved low levels of accuracy for the Hindi/Telugu (none of the systems was able to reach 0.8000 accuracy for Hidni) and the Korean/Japanese pairs. In future work, we intend to tackle these two language pairs in isolation in order to improve the overall system performance. Table 2: 10-fold cross-validation accuracy of each feature type individually on the merged training and development sets. In line with the previous works on the NLI task (Tetreault et al., 2013; Jarvis et al., 2013; Chen et al., 2017), in our configurations word and lemma n-grams are the most predictive f"
W17-5042,C12-1158,0,0.841104,"same size in most system configurations. In addition, we compared the performance of typed and traditional character n-grams on the 7-way ICLEv2 corpus (Granger et al., 2009), following the corpus splitting as described in Ionescu et al. (2014). In this experiment, typed character n-grams proved to be more indicative than traditional character n-grams when used in combination with features described in this paper. 3.1.5 Syntactic features, including production rules (Wong and Dras, 2011) and Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), have been previously explored for NLI. Tetreault et al. (2012) experimented with the Stanford parser (de Marneffe et al., 2006) dependency features and concluded that they are strong indicators of structural differences in L2 writing. We exploit the Stanford dependencies to build syntactic n-gram features by using the algorithm designed and made available by Posadas-Dur´an et al. (2014, 2017).3 Consider the following sample sentence: (1) Lisa said, “John should repair it tomorrow.” The character n-grams (n = 4) for the sample sentence (1) for each of the categories proposed by Sapkota et al. (2015) are shown in Table 1. For clarity, spaces are represente"
W17-5042,W07-0602,0,0.2258,"text and able to cap1 2 375 http://www.nltk.org http://www.lextek.com/manuals/onix/functionwords1.html ture lexical and syntactic information, punctuation and capitalization information related with the authors’ style (Stamatatos, 2013). The effectiveness of character n-gram features for representing the stylistic properties of a text has been demonstrated in previous NLI studies (Ionescu et al., 2014; Chen et al., 2017). Their effectiveness in NLI is hypothesized to be a result of phoneme transfer from the learner’s L1, and by their ability to capture orthographic conventions of a language (Tsur and Rappoport, 2007). Sapkota et al. (2015) defined 10 different character n-gram categories based on affixes, words, and punctuation. In this approach, instances of the same n-gram may refer to different typed n-gram features. For example, in the phrase less carelessness, the two instances of the 4-gram less are assigned to different character n-gram categories. As an example, consider the following sample sentence: character n-grams (n = 4) outperformed traditional character n-grams of the same size in most system configurations. In addition, we compared the performance of typed and traditional character n-gram"
W17-5042,D11-1148,0,0.263246,"almasi et al., 2016), this year’s competition covers an NLI task based on the spoken response. Overall, this year’s task consists of three tracks: NLI on the essay only, NLI on the spoken response only, and NLI on both essay and spoken response. In this paper, we describe the CIC-FBK approach to the essay-only track. Previous works on identifying the native language from texts explored a large variety of features, including lexical and part-of-speech (POS) features (Koppel et al., 2005a), character n-grams (Ionescu et al., 2014), spelling errors (Koppel et al., 2005b), and syntactic features (Wong and Dras, 2011). Following previous research on the NLI task, we incorporate commonly used word n-grams, lemma n-grams, POS n-grams, and function words. In order to capture the L1 influences at the character level, we use recently introduced character n-grams from misspelled words (Chen et al., 2017), as well as 10 categories of character n-gram features proposed by Sapkota et al. (2015). We also include syntactic features by extracting syntactic dependencybased n-grams of words and of syntactic relation tags (Sidorov et al., 2014) using the algorithm designed by Posadas-Dur´an et al. (2014, 2017). We descri"
W18-6218,W17-5042,1,0.735025,"culture specific (Wierzbicka, 1994, 1999), and thus could be indicative of the native language of a speaker. 3.1 Experiment setup 3.3 Features 3.3.1 Part-of-speech tags and function words POS tag n-grams and function words (FWs) are considered core features in NLI research (Malmasi and Dras, 2015), not susceptible to topic bias, unlike word and character n-grams (Brooke and Hirst, 2011). POS n-grams, n=1..3 POS features capture the morpho-syntactic patterns in a text, and are indicative of the L1, especially when used in combination with other types of features (Cimino and Dell’Orletta, 2017; Markov et al., 2017). POS tags were obtained with TreeTagger (Schmid, 1999), which uses the Penn Treebank tagset (36 tags). Datasets We conduct experiments on two datasets commonly used in NLI research: TOEFL11 (Blanchard et al., 2013): the ETS Corpus of Non-Native Written English (TOEFL 11) contains 1,100 essays in English (avg. 348 tokens/essay) for each of the 11 L1s: Arabic (ARA), Chinese (CHI), French (FRE), German (GER), Hindi (HIN), Italian (ITA), Japanese (JPN), Korean (KOR), Spanish (SPA), Telugu (TEL), and Function words (FWs) n-grams, n=1..3 Function words clarify the relationships between the content-"
W18-6218,C18-1293,1,0.779736,"Missing"
W18-6218,W17-5049,0,0.719417,"Missing"
W18-6218,D14-1142,0,0.218691,"his by evaluating the impact of emotion-based features on classifying the L1 of the authors of essays written in L2. 3 Turkish (TUR). The essays were written in response to eight different writing prompts, all of which appear in all 11 L1 groups. The dataset contains information regarding the proficiency level (low, medium, high) of the authors. ICLE (Granger et al., 2009): the ICLEv2 dataset consists of essays written by highlyproficient non-native college-level students of English. We used a 7-language subset of the corpus normalized for topic and character encoding (Tetreault et al., 2012; Ionescu et al., 2014) to which we refer as ICLE. This subset contains 110 essays (avg. 747 tokens/essay after tokenization and removal of metadata) for each of the 7 languages: Bulgarian (BUL), Chinese (CHI), Czech (CZE), French (FRE), Japanese (JPN), Russian (RUS), and Spanish (SPA). 3.2 We used the (pre-)tokenized version of TOEFL 11 and tokenized ICLE with the Natural Language Toolkit (NLTK)1 tokenizer. ICLE metadata was removed in pre-processing. Each essay was represented through the sets of features described below, using term frequency (tf) and the liblinear scikit-learn (Pedregosa et al., 2011) implementat"
W18-6218,W13-1714,0,0.0640457,"h (SPA). 3.2 We used the (pre-)tokenized version of TOEFL 11 and tokenized ICLE with the Natural Language Toolkit (NLTK)1 tokenizer. ICLE metadata was removed in pre-processing. Each essay was represented through the sets of features described below, using term frequency (tf) and the liblinear scikit-learn (Pedregosa et al., 2011) implementation of Support Vector Machines (SVM) with OvR (one vs. the rest) multi-class strategy. We report classification accuracy on 10-fold cross-validation experiments. Emotion features for NLI The best performing features for NLI are word and character n-grams (Jarvis et al., 2013). They cover – and obscure – a wide range of phenomena, because language usage has multiple dimensions that can reveal information such as age, gender, cultural influences. In this study, we investigate the impact of words that have an emotion signal, since studies have shown that emotion is culture specific (Wierzbicka, 1994, 1999), and thus could be indicative of the native language of a speaker. 3.1 Experiment setup 3.3 Features 3.3.1 Part-of-speech tags and function words POS tag n-grams and function words (FWs) are considered core features in NLI research (Malmasi and Dras, 2015), not sus"
W18-6218,W14-0908,0,0.0296986,"Missing"
W18-6218,C12-1158,0,0.0958251,"zbicka, 1999). We test this by evaluating the impact of emotion-based features on classifying the L1 of the authors of essays written in L2. 3 Turkish (TUR). The essays were written in response to eight different writing prompts, all of which appear in all 11 L1 groups. The dataset contains information regarding the proficiency level (low, medium, high) of the authors. ICLE (Granger et al., 2009): the ICLEv2 dataset consists of essays written by highlyproficient non-native college-level students of English. We used a 7-language subset of the corpus normalized for topic and character encoding (Tetreault et al., 2012; Ionescu et al., 2014) to which we refer as ICLE. This subset contains 110 essays (avg. 747 tokens/essay after tokenization and removal of metadata) for each of the 7 languages: Bulgarian (BUL), Chinese (CHI), Czech (CZE), French (FRE), Japanese (JPN), Russian (RUS), and Spanish (SPA). 3.2 We used the (pre-)tokenized version of TOEFL 11 and tokenized ICLE with the Natural Language Toolkit (NLTK)1 tokenizer. ICLE metadata was removed in pre-processing. Each essay was represented through the sets of features described below, using term frequency (tf) and the liblinear scikit-learn (Pedregosa et"
W19-3411,P13-1035,0,0.0382921,"Missing"
W19-3411,P14-1035,0,0.018183,"y traits recognition, the used datasets are often collected from modern communication means, e.g. messages posted in social media. Indeed there is interest in using modern NLP tools in literary texts, for example Grayson et al. (2016) use word embeddings for analyzing literature, Boyd (2017) describes the current status and tool for psychological text analysis, Flekova and Gurevych (2015) profile fictional characters, Liu et al. (2018) conduct a traits analysis of two fictional characters in a Chinese novel. The use of the Five Factor Model for literature is explained in McCrae et al. (2012). Bamman et al. (2014) consider the problem of automatically inferring latent character types in a collection of English novels. Bamman et al. (2013) present a new dataset for the text-driven analysis of film. Then they present some latent variable models for learning character types in movies. Vala et al. (2015) propose a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets. Introduction The availability of texts produced by people using the modern communication means can give an important insight in personality profiling. And computational linguist"
W19-3411,pianta-etal-2008-textpro,0,0.017598,"comparable to the ones reported by Celli et al. (2013). The results shown in Table 2 report the best results. We started working on our model using the Scikitlearn toolkit to do Machine Learning in Python (Pedregosa et al., 2011). The initial task was to get reasonable performance on the “essay” dataset. The problem falls in the class of multi-output labels. For simplicity each label (corresponding to a personal trait) can be treated as independent, partitioning the problem in 5 classification problems. Starting from a simple bag of words model, we added to the features the output of TextPro (Pianta et al., 2008) for the columns: pos, chunk, entity, and tokentype4 . The possible values of those columns are categorical variables that can be counted for each character in order to build a feature for the model. Following the suggestion from (Celli et al., 2013, 2014), our model was built as a pipeline incorporating both the bag of word model and the output of TextPro. We acknowledge that a lot of tweaking is possible for improving the performance of a model (such as building a different model for each trait, or use different features or classifier). However that was not our primary scope. 2.4 Table 2: Mo"
W19-3411,D15-1088,0,0.0209135,"escribes the current status and tool for psychological text analysis, Flekova and Gurevych (2015) profile fictional characters, Liu et al. (2018) conduct a traits analysis of two fictional characters in a Chinese novel. The use of the Five Factor Model for literature is explained in McCrae et al. (2012). Bamman et al. (2014) consider the problem of automatically inferring latent character types in a collection of English novels. Bamman et al. (2013) present a new dataset for the text-driven analysis of film. Then they present some latent variable models for learning character types in movies. Vala et al. (2015) propose a novel technique for character detection, achieving significant improvements over state of the art on multiple datasets. Introduction The availability of texts produced by people using the modern communication means can give an important insight in personality profiling. And computational linguistic community has been quite active in this topic. In this paper we want to explore the use of the techniques and tools nowadays used for user generated content, for the analysis of literary characters in books and plays. In particular we will focus on the analysis of speech utterances in the"
W19-3411,D15-1208,0,0.0174593,"dge, there is little ongoing research on personality traits recognition in literary texts. Most of the works in literary text is focused on other aspects such as author attribution, stylometry, plagiarism detection. Regarding personality traits recognition, the used datasets are often collected from modern communication means, e.g. messages posted in social media. Indeed there is interest in using modern NLP tools in literary texts, for example Grayson et al. (2016) use word embeddings for analyzing literature, Boyd (2017) describes the current status and tool for psychological text analysis, Flekova and Gurevych (2015) profile fictional characters, Liu et al. (2018) conduct a traits analysis of two fictional characters in a Chinese novel. The use of the Five Factor Model for literature is explained in McCrae et al. (2012). Bamman et al. (2014) consider the problem of automatically inferring latent character types in a collection of English novels. Bamman et al. (2013) present a new dataset for the text-driven analysis of film. Then they present some latent variable models for learning character types in movies. Vala et al. (2015) propose a novel technique for character detection, achieving significant impro"
W19-4429,W15-0620,0,0.448395,"Missing"
W19-4429,P07-1083,0,0.0438934,"of expressions and the extracted vocabularies is provided in Table 2. We apply the following algorithm: 3.3.2 Misspelled cognates, L2-ed words and other misspellings We build features that gather information from misspelled words in the essays in the data. The information about which L1 a cognate or L2-ed word hints to is used as an attribute of the word. 1. For each misspelled English word wm identify its closest word in some L1: 2. For wf in each L1: Misspelled cognates. Several studies applied discriminative string similarity to the task of cognate identification (Mann and Yarowsky, 2001; Bergsma and Kondrak, 2007; Nicolai et al., 2013). Following the work by Nicolai et al. (2013), we detect cognates by identifying the cases where the closest correctly spelled L2 word we to the misspelled word wm has a translation in an L1 wf to which it is close in form, and wm is closer to wf than to we . Formally: (a) Replace diacritics in wf with the corresponding Latin equivalent (e.g., “´e” → “e”). (b) Compute the Levenshtein distance D(wm , wf ). (c) Identify the L1 with the smallest D(wm , wf ) value, and if D(wm , wf ) < 5 then take wm to be an L2-ed version 4 We use Python’s translation tool: https://pypi.org"
W19-4429,N01-1020,0,0.0668213,"ge in terms of the number of expressions and the extracted vocabularies is provided in Table 2. We apply the following algorithm: 3.3.2 Misspelled cognates, L2-ed words and other misspellings We build features that gather information from misspelled words in the essays in the data. The information about which L1 a cognate or L2-ed word hints to is used as an attribute of the word. 1. For each misspelled English word wm identify its closest word in some L1: 2. For wf in each L1: Misspelled cognates. Several studies applied discriminative string similarity to the task of cognate identification (Mann and Yarowsky, 2001; Bergsma and Kondrak, 2007; Nicolai et al., 2013). Following the work by Nicolai et al. (2013), we detect cognates by identifying the cases where the closest correctly spelled L2 word we to the misspelled word wm has a translation in an L1 wf to which it is close in form, and wm is closer to wf than to we . Formally: (a) Replace diacritics in wf with the corresponding Latin equivalent (e.g., “´e” → “e”). (b) Compute the Levenshtein distance D(wm , wf ). (c) Identify the L1 with the smallest D(wm , wf ) value, and if D(wm , wf ) < 5 then take wm to be an L2-ed version 4 We use Python’s transla"
W19-4429,W17-5042,1,0.646129,"he intended word we in L1.4 (b) Replace diacritics in wf with the corresponding Latin equivalent (e.g., “´e” → “e”). (c) Compute the Levenshtein distance D between we and wf . (d) If D(we , wf ) < 3 then wf is assumed to be a cognate of we .5 (e) If wf is a cognate and D(wm , wf ) < D(we , wf ) then consider the L1 as a clue of the native language of the author.6 3.3.1 Part-of-speech tags and function words POS features capture the morpho-syntactic patterns in a text, and are indicative of the L1, especially when used in combination with other types of features (Cimino and Dell’Orletta, 2017; Markov et al., 2017). POS tags were obtained with TreeTagger (Schmid, 1999), which uses the Penn Treebank tagset (36 tags). FWs clarify the relationships between the content-carrying elements of a sentence, and introduce syntactic structures like verbal complements, relative clauses, and questions (Smith and Witten, 1993). The FW feature set consists of 318 English FWs from the scikit-learn package (Pedregosa et al., 2011). L2-ed words. To identify the L2-ed, in our case anglicized, words we take a misspelled word and look for forms close to it in the L1 vocabularies. The idea is that a misspelled word may be an"
W19-4429,C18-1293,1,0.823977,"osa et al., 2011) implementation of Support Vector Machines (SVM) with OvR (one vs. the rest) multi-class strategy. We report classification accuracy on 10-fold cross-validation experiments. Methodology To investigate the impact of L2-ed words and cognates, we use the native language identification task: we perform multi-class classification of essays written in L2 (English in our case) by people with different native languages (L1s) – with L1 as the class labels – using a representation of these essays through features that capture these 3.3 Features Following previous studies on NLI, e.g., (Markov et al., 2018a,b), we evaluate the impact of L2-ed words and cognates in combination with the part2 277 http://www.nltk.org of-speech (POS) tag and function word (FW) representations. POS tags and function words (FWs) are considered core features in NLI research (Malmasi and Dras, 2015), not susceptible to topic bias, unlike word and character n-grams (Brooke and Hirst, 2011). An essay will be represented through various combinations of the feature sets we consider: POS & FW n-grams; n-grams from POS & FW sequences including word-level L1 information; character n-grams that represent misspelled words. 2. F"
W19-4429,P17-2086,1,0.880127,"Missing"
W19-4429,W18-6218,1,0.684474,"osa et al., 2011) implementation of Support Vector Machines (SVM) with OvR (one vs. the rest) multi-class strategy. We report classification accuracy on 10-fold cross-validation experiments. Methodology To investigate the impact of L2-ed words and cognates, we use the native language identification task: we perform multi-class classification of essays written in L2 (English in our case) by people with different native languages (L1s) – with L1 as the class labels – using a representation of these essays through features that capture these 3.3 Features Following previous studies on NLI, e.g., (Markov et al., 2018a,b), we evaluate the impact of L2-ed words and cognates in combination with the part2 277 http://www.nltk.org of-speech (POS) tag and function word (FW) representations. POS tags and function words (FWs) are considered core features in NLI research (Malmasi and Dras, 2015), not susceptible to topic bias, unlike word and character n-grams (Brooke and Hirst, 2011). An essay will be represented through various combinations of the feature sets we consider: POS & FW n-grams; n-grams from POS & FW sequences including word-level L1 information; character n-grams that represent misspelled words. 2. F"
W19-4429,W17-5049,0,0.0234891,"Missing"
W19-4429,Q18-1024,0,0.0183207,"rm with the vocabulary of the native language L1. Examples of this process are cognates, which are words that have the same ancestors or were derived from the same sources, that we often approximate in computational approaches as words having similar forms and similar meaning in L1 and L2, for example, S PA . religi´on and E NG . religion. Research in psycholinguistics and native language identification have shown that using cognates when producing L2 is common and shared across native speakers of the same L1 to the degree that a quite accurate phylogenetic language tree can be reconstructed (Rabinovich et al., 2018). 275 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 275–284 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics native language has been explored in various ways through the task of native language identification.1 Nicolai et al. (2013) add cognate-based features to frequently used ones (e.g., character and word n-grams, syntax production rules, misspelling features) for the NLI shared task 2013 (Tetreault et al., 2013). Cognates were detected by identifying misspelled words whose form is closer to an"
W19-4429,D17-1286,1,0.643296,"Missing"
W19-4429,W13-1718,0,0.017321,"racted vocabularies is provided in Table 2. We apply the following algorithm: 3.3.2 Misspelled cognates, L2-ed words and other misspellings We build features that gather information from misspelled words in the essays in the data. The information about which L1 a cognate or L2-ed word hints to is used as an attribute of the word. 1. For each misspelled English word wm identify its closest word in some L1: 2. For wf in each L1: Misspelled cognates. Several studies applied discriminative string similarity to the task of cognate identification (Mann and Yarowsky, 2001; Bergsma and Kondrak, 2007; Nicolai et al., 2013). Following the work by Nicolai et al. (2013), we detect cognates by identifying the cases where the closest correctly spelled L2 word we to the misspelled word wm has a translation in an L1 wf to which it is close in form, and wm is closer to wf than to we . Formally: (a) Replace diacritics in wf with the corresponding Latin equivalent (e.g., “´e” → “e”). (b) Compute the Levenshtein distance D(wm , wf ). (c) Identify the L1 with the smallest D(wm , wf ) value, and if D(wm , wf ) < 5 then take wm to be an L2-ed version 4 We use Python’s translation tool: https://pypi.org/project/translate/ 5 F"
W19-4429,W14-3907,0,0.0699155,"Missing"
W19-4429,W13-1706,0,0.232758,"Missing"
W97-0805,J96-4006,0,0.0245649,"Missing"
