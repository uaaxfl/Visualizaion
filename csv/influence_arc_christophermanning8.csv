1997.iwpt-1.18,H91-1060,0,0.0774122,"Missing"
1997.iwpt-1.18,J93-1002,0,0.100482,"Missing"
1997.iwpt-1.18,W96-0209,0,0.0565044,"Missing"
1997.iwpt-1.18,P96-1025,0,0.0715431,"Missing"
1997.iwpt-1.18,J93-1005,0,0.095593,"Missing"
1997.iwpt-1.18,H91-1046,0,0.0700894,"Missing"
1997.iwpt-1.18,P95-1037,0,0.120299,"Missing"
1997.iwpt-1.18,E93-1040,0,0.0494001,"Missing"
2009.mtsummit-caasl.4,P08-1087,0,0.0282424,"Missing"
2009.mtsummit-caasl.4,A00-2031,0,0.0392798,"Missing"
2009.mtsummit-caasl.4,C02-1013,0,0.0366783,"Missing"
2009.mtsummit-caasl.4,E06-1047,0,0.0439479,"is decision is motivated by experience with integrating other syntactic MT features into phrase-based decoders. As a general rule, if a classification decision cannot be made with high confidence, then it is best to abstain from influencing decoding. 4 4.1 Evaluation Subject Detection We implement the CRF classifier using the publicly available package of Finkel et al. (2005) and modify the Arabic parsers of both Klein and Manning (2002) and Bikel (2004) to train on trees marked with subject-inside-VP constituents. We divide the ATB into training, development, and test sets using the split of Chiang et al. (2006).5 To make the comparison fair, we use this split along with common orthographic normalization rules for both the classifier and parser experiments. We pre-tag the test set 5 The original split contained the duplicate document ANN20021115.0092, which has been removed from the ATB. for Bikel (2004) using the POS tagger of Toutanova et al. (2003), a practice that slightly enhances performance.6 The classifier training set is linearized and labeled according to the conventions described previously. We run MADA and the POS tagger of Toutanova et al. (2003) on the classifier test set instead of inc"
2009.mtsummit-caasl.4,W07-0812,0,0.0399574,"Missing"
2009.mtsummit-caasl.4,P05-1045,1,0.0302541,"oun attachment for PPs near pseudoverbs Adapted from Gildea and Jurafsky (2002) 01&23&quot; iDafa POS patterns Observation has a verb stem Subject classifier features and descripWe choose features that bias the classifier toward high precision. This decision is motivated by experience with integrating other syntactic MT features into phrase-based decoders. As a general rule, if a classification decision cannot be made with high confidence, then it is best to abstain from influencing decoding. 4 4.1 Evaluation Subject Detection We implement the CRF classifier using the publicly available package of Finkel et al. (2005) and modify the Arabic parsers of both Klein and Manning (2002) and Bikel (2004) to train on trees marked with subject-inside-VP constituents. We divide the ATB into training, development, and test sets using the split of Chiang et al. (2006).5 To make the comparison fair, we use this split along with common orthographic normalization rules for both the classifier and parser experiments. We pre-tag the test set 5 The original split contained the duplicate document ANN20021115.0092, which has been removed from the ATB. for Bikel (2004) using the POS tagger of Toutanova et al. (2003), a practice"
2009.mtsummit-caasl.4,J02-3001,0,0.0110132,"Missing"
2009.mtsummit-caasl.4,P05-1071,0,0.102454,"gical analyses to shortened part-of-speech (POS) tags. We augment shortened tags for definite nouns with “DT”, which improves performance. We then linearize the parse trees and label each word according to the classes in Table 3. A reserved symbol separates sentences. Finally, delimiters are attached to the beginning and end of each word so that initial, medial, and final character n-grams may be identified. 3.2 Morphological Information In addition to the POS tags and words, we add morphological data to the classifier input. We run MADA 2.12, a morphological pipeline, on each input sentence (Habash and Rambow, 2005). MADA first uses a morphological analyzer to generate an nbest list of analyses for each word. It then re-ranks the n-best lists using a weighted set of support vector machine (SVM) classifiers. We use the stock classifier set—which includes number and person—plus the gender classifier, which we give a 0.02 weight. For verbs, we also retrieve the verb stem. Two sets of data result from this procedure: the output of the classifiers, and the top-ranked morphological analyses. We take the verb stem from the morphological analyses and all other features from the SVM output. Including the words an"
2009.mtsummit-caasl.4,J99-4005,0,0.0483545,"Missing"
2009.mtsummit-caasl.4,P07-2045,0,0.0164973,"Missing"
2009.mtsummit-caasl.4,levy-andrew-2006-tregex,0,0.0274992,"e NP-SBJ constituents in training, but we must carefully design features to handle the pro-drop and inverted cases. To prepare the classifier training data, we use Function Tag NP-SBJ S-NOM-SBJ SBAR-NOM-SBJ SBAR-SBJ S-SBJ NP-TPC Description Subject inside VP, including null subjects Clausal subjects Clausal subjects Clausal subjects Co-extensive with some quotations (rare) Topicalized subjects (SVO ordering) Table 2: Of the subject-related functional tags in the ATB, we only include NP-SBJ dominated by VP in training. Tregex expressions to identify NP-SBJ constituents in verb-initial clauses (Levy and Andrew, 2006). Using the Bies mappings provided with the ATB, we convert the pre-terminal morphological analyses to shortened part-of-speech (POS) tags. We augment shortened tags for definite nouns with “DT”, which improves performance. We then linearize the parse trees and label each word according to the classes in Table 3. A reserved symbol separates sentences. Finally, delimiters are attached to the beginning and end of each word so that initial, medial, and final character n-grams may be identified. 3.2 Morphological Information In addition to the POS tags and words, we add morphological data to the c"
2009.mtsummit-caasl.4,N06-1014,0,0.0796557,"Missing"
2009.mtsummit-caasl.4,P03-1021,0,0.00648016,"TER (Snover et al., 2006) metrics. times, respectively, in the training data. The output of the CRF classifier is incorporated into the decoder using a simple model. We positively weight phrase pairs that fully overlap with the subject and penalize partial overlaps with a score inversely proportional to the size of the overlap. The feature therefore prefers hypotheses in which the subject is translated as a contiguous block. Hypotheses that split the subject—by inserting a verb inside the subject, for example—receive a negative score. A single feature weight for this model is set during MERT (Och, 2003). Table 6 shows results using this feature design. 5 Discussion We have shown a considerable improvement in subject detection performance and, for completeness, have presented preliminary MT results. In this section, we analyze current sources of error and identify areas for improvement. Subject/Object Boundary and PP Attachment We have specified a set of strong features that indicate the beginning of a subject, but have yet to discover a robust way to find the last word in a subject. “Catastrophic” chains of false positives can thus occur. In these scenarios, the classifier properly detects t"
2009.mtsummit-caasl.4,2001.mtsummit-papers.68,0,0.0343879,"Missing"
2009.mtsummit-caasl.4,P01-1060,0,0.0352759,"Missing"
2009.mtsummit-caasl.4,2006.amta-papers.25,0,0.0150395,"04) (Klein and Manning, 2002) CRF BASELINE CRF B EST Features 1,2 2,7-19 P 50.9 55.2 57.4 65.9 R 59.7 57.5 49.1 57.3 Fβ=1 55.0 56.3 52.9 61.3 Table 5: Test set performance of two CRF models v. statistical parser baselines. The feature indices correspond to Table 4. BLEU MT04 (dev) MT03 BASELINE 48.69 52.63 BASELINE +SUBJ 48.66 (−0.03) 52.61 (−0.02) Translation Error Rate (TER) MT04 (dev) MT03 BASELINE 42.02 40.30 BASELINE +SUBJ 42.05 (+0.03) 40.51 (+0.21) MT05 53.57 53.43 (−0.14) MT05 39.48 39.56 (+0.08) Table 6: MT experimental results evaluated with the BLEU (Papineni et al., 2001) and TER (Snover et al., 2006) metrics. times, respectively, in the training data. The output of the CRF classifier is incorporated into the decoder using a simple model. We positively weight phrase pairs that fully overlap with the subject and penalize partial overlaps with a score inversely proportional to the size of the overlap. The feature therefore prefers hypotheses in which the subject is translated as a contiguous block. Hypotheses that split the subject—by inserting a verb inside the subject, for example—receive a negative score. A single feature weight for this model is set during MERT (Och, 2003). Table 6 shows"
2009.mtsummit-caasl.4,N03-1033,1,0.0503984,"Missing"
2020.acl-demos.14,N19-4010,0,0.0697116,"Missing"
2020.acl-demos.14,C18-1139,0,0.0284468,"pendency parser (Dozat and Manning, 2017). We further augment this model with two linguistically motivated features: one that predicts the linearization order of two words in a given language, and the other that predicts the typical distance in linear order between them. We have previously shown that these features significantly improve parsing accuracy (Qi et al., 2018). Named Entity Recognition. For each input sentence, Sta n z a also recognizes named entities in it (e.g., person names, organizations, etc.). For NER we adopt the contextualized string representationbased sequence tagger from Akbik et al. (2018). We first train a forward and a backward characterlevel LSTM language model, and at tagging time we concatenate the representations at the end of each word position from both language models with word embeddings, and feed the result into a standard one-layer Bi-LSTM sequence tagger with a conditional random field (CRF)-based decoder. 2.2 existing server interface in CoreNLP, and implement a robust client as its Python interface. When the CoreNLP client is instantiated, Sta n z a will automatically start the CoreNLP server as a local process. The client then communicates with the server throug"
2020.acl-demos.14,W19-5301,0,0.0659423,"Missing"
2020.acl-demos.14,benikova-etal-2014-nosta,0,0.0461707,"Missing"
2020.acl-demos.14,Q17-1010,0,0.19505,"Missing"
2020.acl-demos.14,P14-5010,1,0.0127466,"Python Objects TOKEN LEMMA POS HEAD DEPREL ... WORD Fully Neural: Language-agnostic SENTENCE PROCESSORS DOCUMENT Figure 1: Overview of Sta n z a ’s neural NLP pipeline. Sta n z a takes multilingual text as input, and produces annotations accessible as native Python objects. Besides this neural pipeline, Sta n z a also features a Python client interface to the Java CoreNLP software. The growing availability of open-source natural language processing (NLP) toolkits has made it easier for users to build tools with sophisticated linguistic processing. While existing NLP toolkits such as CoreNLP (Manning et al., 2014), F LAIR (Akbik et al., 2019), spaCy1 , and UDPipe (Straka, 2018) have had wide usage, they also suffer from several limitations. First, existing toolkits often support only a few major languages. This has significantly limited the community’s ability to process multilingual text. Second, widely used tools are sometimes under-optimized for accuracy either due to a focus on efficiency (e.g., spaCy) or use of less powerful models (e.g., CoreNLP), potentially mislead1 Multilingual: 66 Languages LEMMA POS & Morphological Tagging Introduction ∗ Lemmatization RU नमसकार! HI ing downstream applicatio"
2020.acl-demos.14,E12-1017,0,0.0728053,"Missing"
2020.acl-demos.14,2020.lrec-1.497,1,0.82991,"Missing"
2020.acl-demos.14,K18-2016,1,0.941297,"Sta n z a ’s neural pipeline consists of models that range from tokenizing raw text to performing syntactic analysis on entire sentences (see Figure 1). All components are designed with processing many human languages in mind, with high-level design choices capturing common phenomena in many languages and data-driven models that learn the difference between these languages from data. Moreover, the implementation of Sta n z a components is highly modular, and reuses basic model architectures when possible for compactness. We highlight the important design choices here, and refer the reader to Qi et al. (2018) for modeling details. POS and Morphological Feature Tagging. For each word in a sentence, Sta n z a assigns it a partof-speech (POS), and analyzes its universal morphological features (UFeats, e.g., singular/plural, 1st /2nd /3rd person, etc.). To predict POS and UFeats, we adopt a bidirectional long short-term memory network (Bi-LSTM) as the basic architecture. For consistency among universal POS (UPOS), 3 Following Universal Dependencies (Nivre et al., 2020), we make a distinction between tokens (contiguous spans of characters in the input text) and syntactic words. These are interchangeabl"
2020.acl-demos.14,taule-etal-2008-ancora,0,0.167638,"Missing"
2020.acl-demos.14,W02-2024,0,0.53937,"Missing"
2020.acl-demos.14,K18-2001,0,0.0845404,"Missing"
2020.acl-demos.14,W03-0419,0,\N,Missing
2020.acl-demos.14,K18-2020,0,\N,Missing
2020.acl-main.458,P19-1483,0,0.0549464,"Missing"
2020.acl-main.458,D18-1409,0,0.0322222,"Missing"
2020.acl-main.458,P19-1213,0,0.072017,"cifically, Paulus et al. (2018) found that directly optimizing an abstractive summarization model on the ROUGE metric via RL can improve the summary ROUGE scores. Our work extends the rewards used in existing work with a factual correctness reward to further improve the correctness of the generated summaries. Factual Correctness in Summarization. Our work is closely related to recent work that studies factual correctness in summarization. Cao et al. (2017) proposed to improve summarization models by attending to fact triples extracted using open information extraction systems. Goodrich et al. (2019) compared different information extraction systems to evaluate the factual accuracy of generated text. Falke et al. (2019) explored using natural language inference systems to evaluate the correctness of generated summaries, and found models trained on existing datasets to be inadequate. Kry´sci´nski et al. (2019b) proposed to evaluate factual consistencies in the generated summaries using a weakly-supervised fact verification model. Despite these efforts, none of this work has shown success in directly optimizing a summarization system for factual correctness, and to our knowledge our work re"
2020.acl-main.458,D19-1051,0,0.059033,"Missing"
2020.acl-main.458,P14-5010,1,0.0130919,"Missing"
2020.acl-main.458,K16-1028,0,0.153683,"Missing"
2020.acl-main.458,D14-1162,1,0.107611,"Missing"
2020.acl-main.458,P17-1099,1,0.950844,"othorax is observed. bilateral pleural effusions continue. Summary B (ROUGE-L = 0.44): pneumothorax is observed on radiograph. bilateral pleural effusions continue to be seen. Figure 1: A (truncated) radiology report and summaries with their ROUGE-L scores. Compared to the human summary, Summary A has high textual overlap (i.e., ROUGE-L) but makes a factual error; Summary B has a lower ROUGE-L score but is factually correct. Introduction Neural abstractive summarization systems aim at generating sentences which compress a document while preserving the key facts in it (Nallapati et al., 2016b; See et al., 2017; Chen and Bansal, 2018). These systems are potentially useful in many realworld applications. For example, Zhang et al. (2018) have shown that customized neural abstractive summarization models are able to generate radiology summary statements with high quality by summarizing textual findings written by radiologists. This task has significant clinical value because of its potential to accelerate the radiology workflow, reduce repetitive human labor, and improve clinical communications (Kahn Jr et al., 2009). However, while existing abstractive summarization models are optimized to generate su"
2020.acl-main.458,W18-5623,1,0.871039,"Missing"
2020.acl-main.493,N19-1112,0,0.159631,"that even without explicit supervision, multilingual masked language models learn certain linguistic universals. 1 Figure 1: t-SNE visualization of head-dependent dependency pairs belonging to selected dependencies in English and French, projected into a syntactic subspace of Multilingual BERT, as learned on English syntax trees. Colors correspond to gold UD dependency type labels. Although neither mBERT nor our probe was ever trained on UD dependency labels, English and French dependencies exhibit cross-lingual clustering that largely agrees with UD dependency labels. Introduction Past work (Liu et al., 2019; Tenney et al., 2019a,b) has found that masked language models such as BERT (Devlin et al., 2019) learn a surprising amount of linguistic structure, despite a lack of direct linguistic supervision. Recently, large multilingual masked language models such as Multilingual BERT (mBERT) and XLM (Conneau and Lample, 2019; Conneau et al., 2019) have shown strong cross-lingual performance on tasks like XNLI (Lample and Conneau, 2019; Williams et al., 2018) and dependency parsing (Wu and Dredze, 2019). Much previous analysis has been motivated by a desire to explain why BERT-like models perform so we"
2020.acl-main.493,J93-2004,0,0.069507,"nstruction accuracy (UUAS) for selected languages at layer 7 when the linear transformation is constrained to varying maximum dimensionality. Figure 4: Parse distance tree reconstruction accuracy (UUAS) on layers 1–12 for selected languages, with probe maximum rank 128. 4 4.1 no further gains, implying that the syntactic subspace is a small part of the overall mBERT representation, which has dimension 768 (Figure 3). These results closely correspond to the results found by Hewitt and Manning (2019) for an equivalently sized monolingual English model trained and evaluated on the Penn Treebank (Marcus et al., 1993), suggesting that mBERT behaves similarly to monolingual BERT in representing syntax. Cross-Lingual Probing Transfer Experiments We now evaluate the extent to which Multilingual BERT’s syntactic subspaces are similar across languages. To do this, we evaluate the performance of a structural probe when evaluated on a language unseen at training time. If a probe trained to predict syntax from representations in language i also predicts syntax in language j, this is evidence that mBERT’s syntactic subspace for language i also encodes syntax in language j, and thus that syntax 5567 is encoded simil"
2020.acl-main.493,2020.acl-main.659,0,0.0120482,"y recreate parse distances, (that is, dB (h`i , h`j ) ≈ d`T (wi` , wj` )) a perfect UUAS score under the minimum spanning tree construction can be achieved by ensuring that dB (h`i , h`j ) is small if there is an edge between wi` and wj` , and large otherwise, instead of accurately recreating distances between words connected by longer paths. By evaluating Spearman correlation between all pairs of words, one directly evaluates the extent to which the ordering of words j by distance to each word i is correctly predicted, a key notion of the geometric interpretation of the structural probe. See Maudslay et al. (2020) for further discussion. Limitations Our methods are unable to tease apart, for all pairs of languages, whether transfer performance is caused by subword overlap (Singh et al., 2019) or by a more fundamental sharing of parameters, though we do note that language pairs with minimal subword overlap do exhibit nonzero transfer, both in our experiments and in others (K et al., 2020). Moreover, while we quantitatively evaluate cross-lingual transfer in recovering dependency distances, we only conduct a qualitative study in the unsupervised emergence of dependency labels via t-SNE. Future work could"
2020.acl-main.493,2020.emnlp-main.552,0,0.0347873,"Missing"
2020.acl-main.493,2020.lrec-1.497,1,0.86646,"Missing"
2020.acl-main.493,P19-1493,0,0.0307239,"(2020) demonstrate that strong cross-lingual transfer is possible without any word piece overlap at all. Analysis with the structural probe In a monolingual study, Reif et al. (2019) also use the structural probe of Hewitt and Manning (2019) as a tool for understanding the syntax of BERT. They plot the words of individual sentences in a 2dimensional PCA projection of the structural probe distances, for a geometric visualization of individual syntax trees. Further, they find that distances in the mBERT space separate clusters of word senses for the same word type. Understanding representations Pires et al. (2019) find that cross-lingual BERT representations share a common subspace representing useful linguistic information. Libovick`y et al. (2019) find that mBERT representations are composed of a language-specific component and a languageneutral component. Both Libovick`y et al. (2019) and Kudugunta et al. (2019) perform SVCCA on LM representations extracted from mBERT and a massively multilingual transformer-based NMT model, finding language family-like clusters. 5571 Li and Eisner (2019) present a study in syntactically motivated dimensionality reduction; they find that after being passed through a"
2020.acl-main.493,N18-1101,0,0.0201698,"endency labels, English and French dependencies exhibit cross-lingual clustering that largely agrees with UD dependency labels. Introduction Past work (Liu et al., 2019; Tenney et al., 2019a,b) has found that masked language models such as BERT (Devlin et al., 2019) learn a surprising amount of linguistic structure, despite a lack of direct linguistic supervision. Recently, large multilingual masked language models such as Multilingual BERT (mBERT) and XLM (Conneau and Lample, 2019; Conneau et al., 2019) have shown strong cross-lingual performance on tasks like XNLI (Lample and Conneau, 2019; Williams et al., 2018) and dependency parsing (Wu and Dredze, 2019). Much previous analysis has been motivated by a desire to explain why BERT-like models perform so well on downstream applications in the monolingual setting, which begs the question: what properties of these models make them so crosslingually effective? In this paper, we examine the extent to which Multilingual BERT learns a cross-lingual representation of syntactic structure. We extend probing methodology, in which a simple supervised model is used to predict linguistic properties from a model’s representations. In a key departure from past work,"
2020.acl-main.493,D19-1077,0,0.038987,"exhibit cross-lingual clustering that largely agrees with UD dependency labels. Introduction Past work (Liu et al., 2019; Tenney et al., 2019a,b) has found that masked language models such as BERT (Devlin et al., 2019) learn a surprising amount of linguistic structure, despite a lack of direct linguistic supervision. Recently, large multilingual masked language models such as Multilingual BERT (mBERT) and XLM (Conneau and Lample, 2019; Conneau et al., 2019) have shown strong cross-lingual performance on tasks like XNLI (Lample and Conneau, 2019; Williams et al., 2018) and dependency parsing (Wu and Dredze, 2019). Much previous analysis has been motivated by a desire to explain why BERT-like models perform so well on downstream applications in the monolingual setting, which begs the question: what properties of these models make them so crosslingually effective? In this paper, we examine the extent to which Multilingual BERT learns a cross-lingual representation of syntactic structure. We extend probing methodology, in which a simple supervised model is used to predict linguistic properties from a model’s representations. In a key departure from past work, we not only evaluate a probe’s performance (o"
2020.acl-main.493,K18-2001,0,0.0708086,"Missing"
2020.acl-main.493,N19-1419,1,\N,Missing
2020.acl-main.493,P19-1452,0,\N,Missing
2020.acl-main.493,N19-1423,0,\N,Missing
2020.acl-main.493,D19-1167,0,\N,Missing
2020.acl-main.493,D19-1575,0,\N,Missing
2020.acl-main.493,D19-1275,1,\N,Missing
2020.acl-main.493,D19-1276,0,\N,Missing
2020.acl-main.493,D19-6106,0,\N,Missing
2020.acl-main.69,W13-2114,0,0.0229574,"tecture by embedding the passage via a novel gated bi-directional graph neural network and generating the question via a recurrent neural network. To estimate the positions of copied words, Liu et al. (2019) used a graph convolution network and convolved over the nodes of the dependency parse of the passage. Li et al. (2019) jointly modeled OpenIE relations along with the passage using a gated-attention mechanism and a dual copy mechanism. Traditionally, question generation has been tackled by numerous rule-based approaches (Heilman and Smith, 2009; Mostow and Chen, 2009; Yao and Zhang, 2010; Lindberg et al., 2013; Labutov et al., 2015). Heilman and Smith (2009, 2010) introduced an overgenerate-and-rank approach that generated multiple questions via rule-based tree transformations of the constituency parse of a declarative sentence and then ranked them using a logistic-regression ranker with manually designed features. Yao and Zhang (2010) described transformations of Minimal Recursion Semantics representations guaranteeing grammaticality. Other transformations have been in the past defined in terms of templates (Mazidi and Nielsen, 2014, 2015; Mazidi and Tarau, 2016; Flor and Riordan, 2018), or explic"
2020.acl-main.69,P15-1086,0,0.0443402,"e passage via a novel gated bi-directional graph neural network and generating the question via a recurrent neural network. To estimate the positions of copied words, Liu et al. (2019) used a graph convolution network and convolved over the nodes of the dependency parse of the passage. Li et al. (2019) jointly modeled OpenIE relations along with the passage using a gated-attention mechanism and a dual copy mechanism. Traditionally, question generation has been tackled by numerous rule-based approaches (Heilman and Smith, 2009; Mostow and Chen, 2009; Yao and Zhang, 2010; Lindberg et al., 2013; Labutov et al., 2015). Heilman and Smith (2009, 2010) introduced an overgenerate-and-rank approach that generated multiple questions via rule-based tree transformations of the constituency parse of a declarative sentence and then ranked them using a logistic-regression ranker with manually designed features. Yao and Zhang (2010) described transformations of Minimal Recursion Semantics representations guaranteeing grammaticality. Other transformations have been in the past defined in terms of templates (Mazidi and Nielsen, 2014, 2015; Mazidi and Tarau, 2016; Flor and Riordan, 2018), or explicitly performed (Heilman"
2020.acl-main.69,N10-1086,0,0.25962,"Missing"
2020.acl-main.69,N19-1237,0,0.0490112,"dvent of large-scale QA datasets (Rajpurkar et al., 2016; Nguyen et al., 2016), recent work in QG (Du et al., 2017; Zhou et al., 2017) has primarily focused on training sequence-tosequence and attention-based architectures. Dong et al. (2019) fine-tuned the question generation task by taking advantage of a large pre-trained language model. Success in reinforcement learning has inspired teacher-student frameworks (Wang et al., 2017; Tang et al., 2017) treating QA and QG as complementary tasks and performing joint training by using results from QA as rewards for the QG task. Yuan et al. (2017); Hosking and Riedel (2019); Zhang and Bansal (2019) used evaluation metrics like BLEU, sentence perplexity, and QA probability as rewards for dealing with exposure bias. Vis-`a-vis current neural question generators, rule-based architectures are highly transparent, easily extensible, and generate well-formed questions since they perform clearly defined syntactic transformations like subject-auxiliary inversion and WHmovement over parse structures whilst leveraging fundamental NLP annotations like named entities, co-reference, temporal entities, etc. However, most of the existing rule-based systems have lacked diversity"
2020.acl-main.69,S12-1020,0,0.0356477,"er polarities would be hierarchically entailed from their parent clauses in cases like “Picard did not fail to X” where the entailed polarity of “X” is, in fact, positive. Moreover, in one-way implications like “Bojack hesitated to X”, it would be best not to generate a question for unsure cases since it is open-ended if Bojack did or did not X. A similar example is displayed in Figure 5. For each verb representing a subordinate clause, we compute its entailed truth or falsity from its parent clause using the set of one-way and two-way implicative verbs, and verb-noun collocations provided by Karttunen (2012). For example, the two-way implicative construction “forget to X” entails that “X” did not happen, so it would be wrong to ask questions about “X”. Karttunen (2012) provides simple implications in the form of 92 verbs and phrasal implications in the form of 9 sets of verbs and 8 sets of nouns making 1002 verb-noun collocations. The entailed polarity of a 755 This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.69. Question Template Sample Fact Generated Question Locative (LOC) Where mainAux nsubj otherAux verb obj modifiers ? Americans eat about 100 acres"
2020.acl-main.69,de-marneffe-etal-2014-universal,1,0.833648,"Missing"
2020.acl-main.69,P14-2053,0,0.0187022,"Heilman and Smith, 2009; Mostow and Chen, 2009; Yao and Zhang, 2010; Lindberg et al., 2013; Labutov et al., 2015). Heilman and Smith (2009, 2010) introduced an overgenerate-and-rank approach that generated multiple questions via rule-based tree transformations of the constituency parse of a declarative sentence and then ranked them using a logistic-regression ranker with manually designed features. Yao and Zhang (2010) described transformations of Minimal Recursion Semantics representations guaranteeing grammaticality. Other transformations have been in the past defined in terms of templates (Mazidi and Nielsen, 2014, 2015; Mazidi and Tarau, 2016; Flor and Riordan, 2018), or explicitly performed (Heilman and Smith, 2009) by searching tree patterns via Tregex, followed by their manipulation using Tsurgeon (Levy and Andrew, 2006). Kurdi et al. (2020) provide a comprehensive summary of QG, analysing and comparing approaches before and after 2014. D pragmatic constraints. We approach these by making use of semantic role labelers (SRL), previously unexploited linguistic semantic resources like VerbNet’s predicates (Figure 2) and PropBank’s rolesets and custom rules like implications, allowing us to generate a"
2020.acl-main.69,W16-6609,0,0.56402,"and Chen, 2009; Yao and Zhang, 2010; Lindberg et al., 2013; Labutov et al., 2015). Heilman and Smith (2009, 2010) introduced an overgenerate-and-rank approach that generated multiple questions via rule-based tree transformations of the constituency parse of a declarative sentence and then ranked them using a logistic-regression ranker with manually designed features. Yao and Zhang (2010) described transformations of Minimal Recursion Semantics representations guaranteeing grammaticality. Other transformations have been in the past defined in terms of templates (Mazidi and Nielsen, 2014, 2015; Mazidi and Tarau, 2016; Flor and Riordan, 2018), or explicitly performed (Heilman and Smith, 2009) by searching tree patterns via Tregex, followed by their manipulation using Tsurgeon (Levy and Andrew, 2006). Kurdi et al. (2020) provide a comprehensive summary of QG, analysing and comparing approaches before and after 2014. D pragmatic constraints. We approach these by making use of semantic role labelers (SRL), previously unexploited linguistic semantic resources like VerbNet’s predicates (Figure 2) and PropBank’s rolesets and custom rules like implications, allowing us to generate a broader range of questions of"
2020.acl-main.69,D19-1326,0,0.108392,"Missing"
2020.acl-main.69,D17-1090,0,0.0841648,"elevant as compared to their original counterparts. The Java code is publicly available at https://bitbucket.org/kaustubhdhole/syn-qg/. 2 Related Work RE With the advent of large-scale QA datasets (Rajpurkar et al., 2016; Nguyen et al., 2016), recent work in QG (Du et al., 2017; Zhou et al., 2017) has primarily focused on training sequence-tosequence and attention-based architectures. Dong et al. (2019) fine-tuned the question generation task by taking advantage of a large pre-trained language model. Success in reinforcement learning has inspired teacher-student frameworks (Wang et al., 2017; Tang et al., 2017) treating QA and QG as complementary tasks and performing joint training by using results from QA as rewards for the QG task. Yuan et al. (2017); Hosking and Riedel (2019); Zhang and Bansal (2019) used evaluation metrics like BLEU, sentence perplexity, and QA probability as rewards for dealing with exposure bias. Vis-`a-vis current neural question generators, rule-based architectures are highly transparent, easily extensible, and generate well-formed questions since they perform clearly defined syntactic transformations like subject-auxiliary inversion and WHmovement over parse structures whil"
2020.acl-main.69,N19-4009,0,0.0392526,"Missing"
2020.acl-main.69,N18-1057,0,0.0690579,"Missing"
2020.acl-main.69,P02-1040,0,0.107514,"(Figure 5). We use two state-of-the-art (SOTA) pre-trained transformer models transformer.wmt19.en-de and transformer.wmt19.de-en from Ott et al. (2019) trained on the English-German and GermanEnglish translation tasks of WMT 2019. Figure 6 in the Appendix shows the output of all the five sets of templates applied together over one 758 sentence (along-with implicature). 4 Evaluation and Results Most of the prior QG studies have evaluated the performance of the generated questions using automatic evaluation metrics used in the machine translation literature. We use the traditional BLEU scores (Papineni et al., 2002) and compare the performance of Syn-QG on the SQuAD (Rajpurkar et al., 2016) test split created by Zhou et al. (2017). BLEU measures the average n-gram precision on a set of reference sentences. A question lexically and syntactically similar to a human question would score high on such n-gram metrics. Despite not utilizing any training data, Syn-QG performs better than the previous SOTA on two evaluation metrics BLEU-3 and BLEU-4 and close to SOTA on BLEU-1 and BLEU-2 (Table 4) at the time of submission. The high scores obtained without conducting any training arguably shed a little light on t"
2020.acl-main.69,D16-1264,0,0.45361,"escribed in Figure 3. We evaluate our QG framework, Syn-QG against three QG systems on a mixture of Wikipedia and commercial text sentences outperforming existing approaches in grammaticality and relevance in a crowd-sourced human evaluation while simultaneously generating more types of questions. We also notice that back-translated questions are grammatically superior but are sometimes slightly irrelevant as compared to their original counterparts. The Java code is publicly available at https://bitbucket.org/kaustubhdhole/syn-qg/. 2 Related Work RE With the advent of large-scale QA datasets (Rajpurkar et al., 2016; Nguyen et al., 2016), recent work in QG (Du et al., 2017; Zhou et al., 2017) has primarily focused on training sequence-tosequence and attention-based architectures. Dong et al. (2019) fine-tuned the question generation task by taking advantage of a large pre-trained language model. Success in reinforcement learning has inspired teacher-student frameworks (Wang et al., 2017; Tang et al., 2017) treating QA and QG as complementary tasks and performing joint training by using results from QA as rewards for the QG task. Yuan et al. (2017); Hosking and Riedel (2019); Zhang and Bansal (2019) used"
2020.acl-main.69,D18-1427,0,0.147608,"Missing"
2020.acl-main.69,W17-2603,0,0.107415,"d Work RE With the advent of large-scale QA datasets (Rajpurkar et al., 2016; Nguyen et al., 2016), recent work in QG (Du et al., 2017; Zhou et al., 2017) has primarily focused on training sequence-tosequence and attention-based architectures. Dong et al. (2019) fine-tuned the question generation task by taking advantage of a large pre-trained language model. Success in reinforcement learning has inspired teacher-student frameworks (Wang et al., 2017; Tang et al., 2017) treating QA and QG as complementary tasks and performing joint training by using results from QA as rewards for the QG task. Yuan et al. (2017); Hosking and Riedel (2019); Zhang and Bansal (2019) used evaluation metrics like BLEU, sentence perplexity, and QA probability as rewards for dealing with exposure bias. Vis-`a-vis current neural question generators, rule-based architectures are highly transparent, easily extensible, and generate well-formed questions since they perform clearly defined syntactic transformations like subject-auxiliary inversion and WHmovement over parse structures whilst leveraging fundamental NLP annotations like named entities, co-reference, temporal entities, etc. However, most of the existing rule-based sy"
2020.acl-main.69,D19-1253,0,0.486183,"asets (Rajpurkar et al., 2016; Nguyen et al., 2016), recent work in QG (Du et al., 2017; Zhou et al., 2017) has primarily focused on training sequence-tosequence and attention-based architectures. Dong et al. (2019) fine-tuned the question generation task by taking advantage of a large pre-trained language model. Success in reinforcement learning has inspired teacher-student frameworks (Wang et al., 2017; Tang et al., 2017) treating QA and QG as complementary tasks and performing joint training by using results from QA as rewards for the QG task. Yuan et al. (2017); Hosking and Riedel (2019); Zhang and Bansal (2019) used evaluation metrics like BLEU, sentence perplexity, and QA probability as rewards for dealing with exposure bias. Vis-`a-vis current neural question generators, rule-based architectures are highly transparent, easily extensible, and generate well-formed questions since they perform clearly defined syntactic transformations like subject-auxiliary inversion and WHmovement over parse structures whilst leveraging fundamental NLP annotations like named entities, co-reference, temporal entities, etc. However, most of the existing rule-based systems have lacked diversity, being mostly focused on"
2020.acl-main.69,D18-1424,0,0.180164,"Missing"
2020.acl-main.69,levy-andrew-2006-tregex,0,\N,Missing
2020.acl-main.69,P12-3029,0,\N,Missing
2020.acl-main.69,W18-2501,0,\N,Missing
2020.acl-main.69,W18-0530,0,\N,Missing
2020.acl-main.69,W19-3318,0,\N,Missing
2020.blackboxnlp-1.26,D16-1032,0,0.0299536,"to terminate. We explore training probe to find this linear separator, and are able to predict where sequences end with high accuracy (Table 3). In our other experiments, we did not see such a simple possible solution, but can speculate as to what it would require. In particular, the length manifold and length attractor behaviors seem to indicate that length extrapolation fails because the conditions for stopping are tracked in these models more or less in terms of absolute linear position. As such, it is possible that a successful parameterization may make use of an implicit checklist model (Kiddon et al., 2016), that checks off which parts of the input have been accounted for in the output. Conclusion In this work, we studied a decision often overlooked in NLP: modeling the probability of ending the generative process through a special token in a neural decoder’s output vocabulary. We trained neural models to predict this special EOS token and studied how this objective affected their behavior and representations across three diverse tasks. Our quantitative evaluations took place in an oracle setting in which we forced all models to generate until the optimal sequence length at test time. Under this"
2020.blackboxnlp-1.26,P17-4012,0,0.0659463,"Missing"
2020.emnlp-main.120,D19-1060,0,0.153821,"Missing"
2020.emnlp-main.120,N19-1423,0,0.229409,"ncourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering. Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins. 1 Figure 1: An example of a shuffled conversation and potential features that can aid in reconstructing the original sentence ordering, C → D → B → A. Introduction Recent representation learning methods in NLP such as BERT (Devlin et al., 2019) have focused on learning two types of representations: the bottomlevel – a contextual representation centered at a single word, trained by recovering randomly masked tokens or predicting previous and next words, and the top-level text, implicitly represented as a single [CLS] symbol and trained by predicting a relation between input segments, which usually consist of multiple sentences. However, natural language text has, in contrast, a very dominant hierarchical structure, with words grouped together into intermediate semantic units such as phrases and then sentences to convey the full meani"
2020.emnlp-main.120,D17-1254,0,0.0180738,"ned by predicting a relation between input segments, which usually consist of multiple sentences. However, natural language text has, in contrast, a very dominant hierarchical structure, with words grouped together into intermediate semantic units such as phrases and then sentences to convey the full meaning of a given text. Neither the transformer architecture nor the pre-training task leverages this hierarchy, treating the language as a flat sequence of tokens instead. Inspired by prior works about sentence-based representations for recurrent networks (Kiros et al., 2015; Hill et al., 2016; Gan et al., 2017; Jernite et al., 2017; Logeswaran et al., 2018; Gong et al., 2016; Chen et al., 2016), we seek to incorporate a more explicit hierarchy into the transformer by extending it with the capacity to learn contextualized sentence-level representations. Equipping the model with such a capacity allows it to learn languages at multiple levels of granularity, ranging from fine-grain connections across words to high-level discourse relations between sentences and paragraphs. Predicting an original sequence of sentences requires deep understanding of natural language, including a variety of phenomena suc"
2020.emnlp-main.120,N16-1162,0,0.112235,"Missing"
2020.emnlp-main.120,E14-3011,0,0.142048,"Missing"
2020.emnlp-main.120,2020.acl-main.703,0,0.132478,"19) tasks, we freeze the encoder and only fine-tune the output layer following Chen et al. (2019). Although the recent state-of-the-art models such as ALBERT (Lan et al., 2019) or T5 use extremely large model and training data, we follow BERTBASE ’s model size because of practical limitations of computation resources. We believe that the BASE settings allow for a robust comparison to the current leading approaches. 4 Results We compare our SLM model with the original BERT, XLNet (Yang et al., 2019), ELECTRA (Clark et al., 2019), ERNIE 2.0 (Sun et al., 2020), CONPONO (Iter et al., 2020), BART (Lewis et al., 2020) and T5 which are the current state-of-theart for the benchmark datasets we use. We mainly compare with models using BERTBASE hyperparameters. 4.1 GLUE results Table 1 shows the performance of our method on the GLUE dataset for all tasks except WNLI. Our method significantly improves the downstream NLP tasks in the GLUE dataset. While most tasks are improved from the original BERT, improvement of RTE by 12.1 points is the most significant. We assume this is because our model learns the relationship of sentences thanks to our objective and it is the most effective on the entailment task with re"
2020.emnlp-main.120,2021.ccl-1.108,0,0.114899,"Missing"
2020.emnlp-main.120,W02-0109,0,0.166512,".5 Avg. 83.6 85.0 86.4 86.1 86.8 81.7 84.0 84.7 84.1 87.4 87.9 87.9 85.0 Table 1: GLUE results. BERT dev scores are produced by same hyperparameter searches using the official pre-trained models. Bolded are the highest scores among Base-size models. XLNet and ELECTRA’s scores are averages except MNLI-mm that are not reported in the papers. 3.2 Experimental Setup For the experiments, we follow BERTBASE ’s hyperparameters and corpus. Our model is trained with 512 length-256 batch size using Wikipedia dumps and BookCorpus (Zhu et al., 2015). We split inputs into sentences using the NLTK toolkit (Loper and Bird, 2002), which are then re-shuffled for every epoch. In terms of data preprocessing, when each input is fed into the model, we set the maximum count M of sentences to process as 20 in our experiments. If exceeded, we randomly merge pairs of adjacent sentences and treat them as a single sentence until the required sentence count is reached. We perform this preprocessing step in order to deal with an uncommonly large number of sentences. For the masked language model objective, we randomly mask up to three continuous word tokens using a geometric distribution, Geo(p = 0.2) following the findings of T5"
2020.emnlp-main.120,N18-1202,0,0.0538564,"ng objects of natural history. The country is pleasant for exercise. 5) My maid is a treasure. My dressmaker is charming. Table 6: Nearest neighbors of contextualized sentence representations. relationships between the sentences. 6 (Wang et al., 2019), detecting incorrectly replaced tokens (Clark et al., 2019), combining multiple tasks (Sun et al., 2020), a decoder-based masked word prediction (Song et al., 2019), and so on. Related Work Contextualized Representation learning for NLP: Self-supervised representation learning became popular after contextualization methods were introduced. ELMo (Peters et al., 2018) dynamically contextualizes representations of adjacent words using bi-directional recurrent encoders. BERT (Devlin et al., 2019) adopted a deep transformer encoder and has proposed the masked language modeling objective incorporating a bi-directional context. After the success of BERT, researchers started exploring various new pre-training objectives such as span boundary representations (Joshi et al., 2020), reordering of local permutations Another line of works tried to scale up the model with more parameters, training data, and computation resources. RoBERTa (Liu et al., 2019) trains BERTL"
2020.emnlp-main.120,2020.acl-main.439,0,0.178847,"DiscoEval(Chen et al., 2019) tasks, we freeze the encoder and only fine-tune the output layer following Chen et al. (2019). Although the recent state-of-the-art models such as ALBERT (Lan et al., 2019) or T5 use extremely large model and training data, we follow BERTBASE ’s model size because of practical limitations of computation resources. We believe that the BASE settings allow for a robust comparison to the current leading approaches. 4 Results We compare our SLM model with the original BERT, XLNet (Yang et al., 2019), ELECTRA (Clark et al., 2019), ERNIE 2.0 (Sun et al., 2020), CONPONO (Iter et al., 2020), BART (Lewis et al., 2020) and T5 which are the current state-of-theart for the benchmark datasets we use. We mainly compare with models using BERTBASE hyperparameters. 4.1 GLUE results Table 1 shows the performance of our method on the GLUE dataset for all tasks except WNLI. Our method significantly improves the downstream NLP tasks in the GLUE dataset. While most tasks are improved from the original BERT, improvement of RTE by 12.1 points is the most significant. We assume this is because our model learns the relationship of sentences thanks to our objective and it is the most effective on"
2020.emnlp-main.120,P18-2124,0,0.0601115,"erbased neural module that is specialized to predict the sentence order, called the Sequence Reconstructor (SR). In the SR, each sentence is represented by a sentential token that is inserted in front of it. A pointer network layer stacked on the transformer decoder is trained to point at the next contextualized sentence representation based on the previous sentence representations. 2 We show that our method achieves robust improvements over standard BERT’s performance on the following downstream NLP tasks: GLUE for Natural Language Inference (Wang et al., 2018), SQuAD for Question Answering (Rajpurkar et al., 2018), and DiscoEval (Chen et al., 2019) for discourse aware sentence representations. We match the score of Text-To-Text Transfer Transformer Base (T5BASE ) (Raffel et al., 2020), a state-of-theart model that uses BERTBASE hyperparameters, while using only half the parameters, shorter training (3/8 tokens overall), and a fraction (1/37) of the data compared to T5. Moreover, we investigate the effect of the proposed objective through a qualitative analysis of the neighbor sentences of sentences that have similar sentential representations. We show that the results support our aim of enriching the t"
2020.emnlp-main.120,2020.tacl-1.5,0,0.235842,"ry epoch. In terms of data preprocessing, when each input is fed into the model, we set the maximum count M of sentences to process as 20 in our experiments. If exceeded, we randomly merge pairs of adjacent sentences and treat them as a single sentence until the required sentence count is reached. We perform this preprocessing step in order to deal with an uncommonly large number of sentences. For the masked language model objective, we randomly mask up to three continuous word tokens using a geometric distribution, Geo(p = 0.2) following the findings of T5 (Raffel et al., 2020) and SpanBERT (Joshi et al., 2020), but sentence tokens are not masked. The models are pre-trained for 1M and 3M steps and optimized by Adam with linear weight decaying using a learning rate 1.5e-4. Any other configuration not mentioned here is the same as the original BERT model. Fine-tuning is done by a hyperparameter search of learning rate: {1e-5, 3e-5, 5e-5, 1e-4} and epoch between 2 to 15 depends on the tasks. We select the run with the best development set score among five runs for each parameter combination for more stable results. For GLUE test set results, predictions from models with the best development score are s"
2020.emnlp-main.120,Q19-1016,1,0.783729,"with the capacity to learn contextualized sentence-level representations. Equipping the model with such a capacity allows it to learn languages at multiple levels of granularity, ranging from fine-grain connections across words to high-level discourse relations between sentences and paragraphs. Predicting an original sequence of sentences requires deep understanding of natural language, including a variety of phenomena such as discourse relations, coreference, temporal dependencies, entailments, narratives, and so on. Figure 1 shows a set of statements from a conversation in the CoQA dataset (Reddy et al., 2019), and clues that can potentially be used to reconstruct their original ordering. To figure out the correct order, in this case 1551 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1551–1562, c November 16–20, 2020. 2020 Association for Computational Linguistics C-D-B-A, a model has to understand that D and A are answers to questions C and B respectively and that ‘she’ in B refers to D, and it may need to know that B is a follow-up question on C. Not only are the semantics of separate sentences important but the relationships between them are crucia"
2020.emnlp-main.120,W18-5446,0,0.0202805,"the new unshuffling objective, we propose a pointerbased neural module that is specialized to predict the sentence order, called the Sequence Reconstructor (SR). In the SR, each sentence is represented by a sentential token that is inserted in front of it. A pointer network layer stacked on the transformer decoder is trained to point at the next contextualized sentence representation based on the previous sentence representations. 2 We show that our method achieves robust improvements over standard BERT’s performance on the following downstream NLP tasks: GLUE for Natural Language Inference (Wang et al., 2018), SQuAD for Question Answering (Rajpurkar et al., 2018), and DiscoEval (Chen et al., 2019) for discourse aware sentence representations. We match the score of Text-To-Text Transfer Transformer Base (T5BASE ) (Raffel et al., 2020), a state-of-theart model that uses BERTBASE hyperparameters, while using only half the parameters, shorter training (3/8 tokens overall), and a fraction (1/37) of the data compared to T5. Moreover, we investigate the effect of the proposed objective through a qualitative analysis of the neighbor sentences of sentences that have similar sentential representations. We s"
2020.emnlp-main.120,P19-1499,0,0.0184728,"auto-regressive decoder, which reconstructs the whole sentences by word prediction. However, their approach does not provide any representation of each sentence. Moreover, while SLM shows strong task improvements, that is not the case for these models. We believe that, similar to words, sentence representations should also be properly contextualized in order to embed richer meaning including relationships to other sentences. HLSTM (Chang et al., 2019) considered contextualization of sentence representations by incorporating previous step’s sentence representation for word prediction. HIBERT (Zhang et al., 2019) proposed a hierarchical transformer encoder trained by recovering masked sentences, focusing in particular on summarization. However, both of these models are trained by performing prediction at the word level, wheres our sentence unshuffling approach demands fine understanding of the relations among the sentences at the more global discourse level. 7 Learning from Sentence Ordering: There are prior works about learning contextualized sentence representations by recovering the original sequence of sentences. Gong et al. (2016) and Logeswaran et al. (2018) proposed RNN-based pointer networks f"
2020.emnlp-main.20,D19-1539,0,0.208188,"context, but BERT produces a full output distribution over tokens only for masked positions while Electric produces unnormalized probabilities (but no full distribution) for all input tokens. expensive because the partition function Zθ (x	 ) requires running the transformer |V |times; unlike most EBMs, the intractability of Zθ (x	 ) is due to the expensive scoring function rather than having a large sample space. suggest that EBMs are a promising alternative to the standard generative models currently used for language representation learning. 2 Method BERT and related pre-training methods (Baevski et al., 2019; Liu et al., 2019; Lan et al., 2020) train a large neural network to perform the cloze task. These models learn the probability pdata (xt |x	 ) of a token xt occurring in the surrounding context x	 = [x1 , ..., xt−1 , xt+1 , ..., xn ]. Typically the context is represented as the input sequence with xt replaced by a special [MASK]placeholder token. This masked sequence is encoded into vector representations by a transformer network (Vaswani et al., 2017). Then the representation at position t is passed into a softmax layer to produce a distribution over tokens pθ (xt |x	 ) for the position."
2020.emnlp-main.20,S17-2001,0,0.0867064,"Missing"
2020.emnlp-main.20,P19-1595,1,0.883064,"Missing"
2020.emnlp-main.20,N18-1202,0,0.0399545,"ng worse than BERT, Electric is much faster to run. It also slightly outperforms ELECTRA-TT, which is consistent with the finding from Labeau and Allauzen (2018) that NCE outperforms negative sampling for training language models. Furthermore, Electric is simpler and faster than ELETRA-TT in that it does not require running the generator to produce PLL scores. TwoTower scores lower than Electric, presumably because it is not a “deeply” bidirectional model and instead just concatenates forward and backward hidden states. 4 Related Work Language modeling (Dai and Le, 2015; Radford et al., 2018; Peters et al., 2018) and cloze modeling (Devlin et al., 2019; Baevski et al., 2019; Liu et al., 2019) have proven to be effective pre-training tasks for NLP. Unlike Electric, these methods follow the standard recipe of estimating token probabilities with an output softmax and using maximumlikelihood training. Energy-based models have been widely explored in machine learning (Dayan et al., 1995; LeCun 5 With ELECTRA’s original masked LM generator, it would be impossible to score all tokens in a single pass. Model Pre-train Data Clean Other Runtime WER WER None – 7.26 20.37 0 BERT Electric WikiBooks 5.41 WikiBooks"
2020.emnlp-main.20,2020.acl-main.240,0,0.0936899,"QuAD, average score3 over 2 The original GPT-2 dataset is not public, so we use a public re-implementation. 3 Matthews correlation coefficient for CoLA, Spearman correlation for STS, accuracy for the other tasks. t=1 PLLs can be used to re-rank the outputs of an NMT or ASR system. While historically log-likelihoods from language models have been used for such reranking, recent work has demonstrated that PLLs from masked language models perform better (Shin et al., 2019). However, computing PLLs from a masked language model requires n passes of the transformer: once with each token masked out. Salazar et al. (2020) suggest distilling BERT into a model that uses no masking to avoid this cost, but this model considerably under-performed regular LMs in their experiments. Electric can produce PLLs for all input tokens in a single pass like a LM while being bidirectional like a masked LM. We use the PLLs from Electric for re-ranking the 100-best hypotheses of a 5-layer BLSTMP model from ESPnet (Watanabe et al., 2018) on the 960-hour LibriSpeech corpus (Panayotov et al., 2015) following the same experimental setup and using the same n-best lists as Salazar 4 We exclude WNLI, for which models do not outperform"
2020.emnlp-main.20,P16-1162,0,0.0115692,"tive data point is the same except xt , the token at position t, is replaced with a noise token x ˆt sampled from q. • Define a binary classifier D that estimates the probability of a data point being positive as n · pˆθ (xt |x	 ) n · pˆθ (xt |x	 ) + k · q(xt |x	 ) • The binary classifier is trained to distinguish positive vs negative data points, with k negatives sampled for every n positive data points. exp (−E(x)t ) =P exp (−E( REPLACE(x, t, x0 ))t ) x0 ∈V where REPLACE(x, t, x0 ) denotes replacing the token at position t with x0 and V is the vocabulary, in practice usually word pieces (Sennrich et al., 2016). Unlike with BERT, which produces the probabilities for all possible tokens x0 using a softmax layer, a candidate x0 is passed in as input to the transformer. As a result, computing pθ is prohibitively 286 Formally, the NCE loss L(θ) is   n · pˆθ (xt |x	 ) n · E − log + n · pˆθ (xt |x	 ) + k · q(xt |x	 ) x,t   k · q(ˆ xt |x	 ) k · E − log n · pˆθ (ˆ xt |x	 ) + k · q(ˆ xt |x	 ) x,t x ˆt ∼q This loss is minimized when pˆθ matches the data distribution pdata (Gutmann and Hyv¨arinen, 2010). A consequence of this property is that the model learns to be self-normalized such that Zθ (x	 )"
2020.emnlp-main.20,N18-2074,0,0.0588723,"Missing"
2020.findings-emnlp.3,P19-1000,0,0.25792,"Missing"
2020.findings-emnlp.3,N19-1349,0,0.0385417,"Missing"
2020.findings-emnlp.3,D14-1162,1,0.106685,"Missing"
2020.findings-emnlp.3,D16-1127,0,0.0386184,"he other hand, since we sample frequent questions in the training set as negative examples for the classifier, it also discourages the model from generating overly generic questions. Previous work has attacked the problem of genericness in conversational natural language generation by proposing auxiliary training objectives, e.g., ones that maximize the utility of the generated utterance estimated with adversarial networks (Rao and Daum´e III, 2019), specificity estimates that are estimated from data (Ko et al., 2019a,b), or the mutual information between the generated turn and previous ones (Li et al., 2016). Our proposed method can be viewed as a generalization of these approaches, where the objective to be optimized at the time of generation is implicitly specified via a parameterized model by choosing negative examples for contrast. 3.5 ` = λ2 `R + (1 − λ2 )`NLL . (7) P PMi (i) Here, `R = N1p N i=1 j=1 `R (qj ). We choose λ1 = 0.5 and λ2 = 0.98 in our experiments, which were chosen by tuning the model on the dev set. 4 Experiments Data. For our experiments, we use the QuAC dataset presented by Choi et al. (2018). Although other similar datasets share some common characteristics, some crucial d"
2020.findings-emnlp.3,E17-2025,0,0.0218064,"Missing"
2020.findings-emnlp.3,W04-1013,0,0.0521058,"cificity of questions, which are essential for efficient iterative system development; • We show that optimizing the proposed metrics via reinforcement learning leads to a system that behaves pragmatically and has improved communication efficiency, as also verified by human evaluation. This represents a practical 1 We release our code and models at https://github. com/qipeng/stay-hungry-stay-focused. 26 Evaluating System-generated Questions. Automatic evaluation of system-generated text has long been an important topic in NLP. Traditional ngram overlap-based approaches (Papineni et al., 2002; Lin, 2004) are computationally efficient, but have been shown to correlate poorly with human judgement of quality (Novikova et al., 2017). More recently, Zhang et al. (2020) leverage large of discussion T (Background and Topic in the figure), as well as a common goal for the student to acquire some knowledge K on this topic that only the teacher has direct access to (Private Knowledge in the figure). We consider the scenario where the agents can only communicate to each other by engaging in a conversation, where the conversation history H is shared between the agents. We further constrain the conversati"
2020.findings-emnlp.3,D18-1432,0,0.0499425,"Missing"
2020.findings-emnlp.3,N10-1086,0,\N,Missing
2020.findings-emnlp.3,W10-4234,0,\N,Missing
2020.findings-emnlp.3,P02-1040,0,\N,Missing
2020.findings-emnlp.3,Q17-1023,0,\N,Missing
2020.findings-emnlp.3,D17-1090,0,\N,Missing
2020.findings-emnlp.3,D18-1241,0,\N,Missing
2020.findings-emnlp.3,P19-1480,0,\N,Missing
2020.findings-emnlp.3,P19-1203,0,\N,Missing
2020.findings-emnlp.3,P18-1078,0,\N,Missing
2020.findings-emnlp.3,N19-1423,0,\N,Missing
2020.findings-emnlp.3,D17-1238,0,\N,Missing
2020.findings-emnlp.3,Q19-1016,1,\N,Missing
2020.lrec-1.497,de-marneffe-etal-2006-generating,1,\N,Missing
2020.lrec-1.497,zeman-2008-reusable,1,\N,Missing
2020.lrec-1.497,de-marneffe-etal-2014-universal,1,\N,Missing
2020.lrec-1.497,W08-1301,1,\N,Missing
2020.lrec-1.497,petrov-etal-2012-universal,0,\N,Missing
2020.lrec-1.497,P13-1051,1,\N,Missing
2020.lrec-1.497,P15-2111,0,\N,Missing
2020.lrec-1.497,L16-1376,1,\N,Missing
2020.lrec-1.497,L16-1262,1,\N,Missing
2020.lrec-1.497,W18-6012,1,\N,Missing
2021.acl-long.564,D15-1075,1,0.841597,"Missing"
2021.acl-long.564,W02-2015,0,0.170369,"-specific, we sample 4 datasets and 5 representative models, each utilizing unique visual and linguistic features and employing different inductive biases. Interpreting and Analyzing Datasets. Given the prevalence of large datasets in modern machine learning, it is critical to assess dataset properties to remove redundancies (Gururangan et al., 2018; Li and Vasconcelos, 2019) or biases (Torralba and Efros, 2011; Khosla et al., 2012; Bolukbasi et al., 2016), both of which negatively impact sample efficiency. Prior work has used training dynamics to find examples which are frequently forgotten (Krymolowski, 2002; Toneva et al., 2019) versus those that are easy to learn (Bras et al., 2020). This work suggests using two model-specific measures – confidence and prediction variance – as indicators of a training example’s “learnability” (Chang et al., 2017; Swayamdipta et al., 2020). Dataset Maps (Swayamdipta et al., 2020), a recently introduced framework uses these two measures to profile datasets to find learnable examples. Unlike prior datasets analyzed by Dataset Maps that have a small number of global outliers as hard examples, we discover that VQA datasets contain copious amounts of collective outli"
2021.acl-long.564,Q19-1026,0,0.0125427,"nal knowledge to answer (e.g., “What is the symbol on the hood often associated with?”) or that ask the model to read text in the images (e.g., “What is the word on the wall?”). Similarly, GQA (Hudson and Manning, 2019) asks underspecified questions (e.g., “what is the person wearing?” which can have multiple correct answers). Collective outliers are not specific to VQA, but can similarly be found in many open-ended tasks, including visual navigation (Anderson et al., 2018b) (e.g., “Go to the grandfather clock” requires identifying rare grandfather clocks), and open-domain question answering (Kwiatkowski et al., 2019), amongst others. Using Dataset Maps, we profile active learning methods and show that they prefer acquiring collective outliers that models are unable to learn, explaining their poor improvements in sample efficiency relative to random sampling. Building on this, we use these maps to perform ablations where we identify and remove outliers iteratively from the active learning pool, observing correlated improvements in sample efficiency. This allows us to conclude that collective outliers are, indeed, responsible for the ineffectiveness of active learning for VQA. We end with prescriptive sugge"
2021.acl-long.564,N18-1101,0,0.0337772,"Missing"
2021.emnlp-main.122,2021.acl-long.145,0,0.0243822,"Ontonotes v5 corpus (Weischedel et al., 2013), recreating the splits used in the CoNLL 2012 shared task, as verified against the split statistics provided by Strubell et al. (2017).78 Since Ontonotes is annotated with constituency parses, not Universal Dependencies, we use the converter provided in CoreNLP (Schuster and Manning, In this work, we intentionally avoid claims as to the “correct” functional family V to be used in conditional probing. Some work has argued for simple probe families (Hewitt and Liang, 2019; Alain and Bengio, 2016), others for complex families (Pimentel et al., 2020b; Hou and Sachan, 2021). 7 In order to provide word vectors for each token in the Pimentel et al. (2020a) argues for choosing mulcorpus, we heuristically align the subword tokenizations of tiple points along an axis of expressivity, while RoBERTa with the corpus-specified tokens through characterlevel alignments, following Tenney et al. (2019). Cao et al. (2021) define the family through the 8 Ontonotes uses the destructive Penn Treebank tokenizaweights of the neural network. Other work pertion (like replacing brackets { with -LCB- (Marcus et al., forms structural analysis of representations without 1993)). We perfo"
2021.emnlp-main.122,P14-5010,1,0.00829756,"ned Conditional 0.15 Bits 0.21 Bits Baselined Conditional 0.65 1 2 3 4 5 6 7 8 9 10 11 12 0.23 upos xpos dep rel ner sst2 0.70 0.20 0.19 Baselined Conditional φ1 φ2 φ1 φ2 0.20 0.20 0.99 0.24 0.18 0.16 0.16 0.81 0.23 0.13 0.22 0.21 1.00 0.25 0.17 0.20 0.20 0.87 0.24 0.13 Table 1: Results on ELMo, reported in bits of Vinformation; higher is better. φi refers to layer i. 0.10 0.05 0.18 Baselined Conditional 1 2 3 4 5 6 7 8 9 10 11 12 Model layer 0.00 1 2 3 4 5 6 7 8 9 10 11 12 Model layer Figure 1: Probing results on RoBERTa. Results are reported in bits of V-information; higher is better. 2016; Manning et al., 2014). For the sentiment annotation, we use the binary GLUE version (Wang et al., 2019) of the the Stanford Sentiment Treebank corpus (Socher et al., 2013). All results are reported on the development sets. Models. We evaluate the popular RoBERTa model (Liu et al., 2019), as provided by the HuggingFace Transformers package (Wolf et al., 2020), as well as the ELMo model (Peters et al., 2018a), as provided by the AllenNLP package (Gardner et al., 2017). When multiple RoBERTa subwords are aligned to a single corpus token, we average the subword vector representations. Probe families. For all of our ex"
2021.emnlp-main.122,J93-2004,0,0.0794903,"Missing"
2021.emnlp-main.122,2020.lrec-1.497,1,0.870655,"Missing"
2021.emnlp-main.122,N18-1202,0,0.716085,"the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1626–1639 c November 7–11, 2021. 2021 Association for Computational Linguistics provides an estimate of conditional V-information IV (repr → property |baseline). In a case study, we answer an open question posed by Hewitt and Liang (2019): how are the aspects of linguistic properties that aren’t explainable by the input layer accessible across the rest of the layers of the network? We find that the partof-speech information not attributable to the input layer remains accessible much deeper into the layers of ELMo (Peters et al., 2018a) and RoBERTa (Liu et al., 2019) than the overall property, a fact previously obscured by the gradual loss across layers of the aspects attributable to the input layer. For the other properties, conditioning on the input layer does not change the trends across layers. Conditional V-information Probing 2 In this section, we describe probing methods and introduce conditional probing. We then review Vinformation and use it to ground probing. 2.1 Probing setup We start with some notation. Let X ∈ X be a random variable taking the value of a sequence of tokens. Let φ(X) be a representation resulti"
2021.emnlp-main.122,D18-1179,0,0.251491,"the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1626–1639 c November 7–11, 2021. 2021 Association for Computational Linguistics provides an estimate of conditional V-information IV (repr → property |baseline). In a case study, we answer an open question posed by Hewitt and Liang (2019): how are the aspects of linguistic properties that aren’t explainable by the input layer accessible across the rest of the layers of the network? We find that the partof-speech information not attributable to the input layer remains accessible much deeper into the layers of ELMo (Peters et al., 2018a) and RoBERTa (Liu et al., 2019) than the overall property, a fact previously obscured by the gradual loss across layers of the aspects attributable to the input layer. For the other properties, conditioning on the input layer does not change the trends across layers. Conditional V-information Probing 2 In this section, we describe probing methods and introduce conditional probing. We then review Vinformation and use it to ground probing. 2.1 Probing setup We start with some notation. Let X ∈ X be a random variable taking the value of a sequence of tokens. Let φ(X) be a representation resulti"
2021.emnlp-main.122,2020.emnlp-main.254,0,0.49658,"ins two probes: (1) on just the baseline, and (2) on the concatenation of the baseline and the representation. The performance of probe (1) is then subtracted from that of probe (2). We call this process conditional probing. Intuitively, the representation is not penalized for lacking aspects of the property accessible in the baseline. We then theoretically ground our probing methodology in V-information, a theory of usable information introduced by Xu et al. (2020) that we additionally extend to multiple predictive variables. We use V-information instead of mutual information (Shannon, 1948; Pimentel et al., 2020b) because any injective deterministic transformation of the input has the same mutual information as the input. For example, a representation that maps each unique sentence to a unique integer must have the same mutual information with any property as does BERT’s representation of that sentence, yet the latter is more useful. In contrast, V-information is defined with respect to a family of functions V that map one random variable to (a probability distribution over) another. V-information can be constructed by deterministic transformations that make a property more accessible to the function"
2021.emnlp-main.122,2020.acl-main.420,0,0.281148,"ins two probes: (1) on just the baseline, and (2) on the concatenation of the baseline and the representation. The performance of probe (1) is then subtracted from that of probe (2). We call this process conditional probing. Intuitively, the representation is not penalized for lacking aspects of the property accessible in the baseline. We then theoretically ground our probing methodology in V-information, a theory of usable information introduced by Xu et al. (2020) that we additionally extend to multiple predictive variables. We use V-information instead of mutual information (Shannon, 1948; Pimentel et al., 2020b) because any injective deterministic transformation of the input has the same mutual information as the input. For example, a representation that maps each unique sentence to a unique integer must have the same mutual information with any property as does BERT’s representation of that sentence, yet the latter is more useful. In contrast, V-information is defined with respect to a family of functions V that map one random variable to (a probability distribution over) another. V-information can be constructed by deterministic transformations that make a property more accessible to the function"
2021.emnlp-main.122,2020.acl-demos.14,1,0.883433,"Missing"
2021.emnlp-main.122,2020.tacl-1.54,0,0.0222857,"the latter is more useful. In contrast, V-information is defined with respect to a family of functions V that map one random variable to (a probability distribution over) another. V-information can be constructed by deterministic transformations that make a property more accessible to the functions in the family. We show that conditional probing Neural language models have become the foundation for modern NLP systems (Devlin et al., 2019; Radford et al., 2018), but what they understand about language, and how they represent that knowledge, is still poorly understood (Belinkov and Glass, 2019; Rogers et al., 2020). The probing methodology grapples with these questions by relating neural representations to well-understood properties. Probing analyzes a representation by using it as input into a supervised classifier, which is trained to predict a property, such as part-of-speech (Shi et al., 2016; Ettinger et al., 2016; Alain and Bengio, 2016; Adi et al., 2017; Belinkov, 2021). 1 One suggests that a representation encodes a Our code is available at https://github.com/ property of interest if probing that representation john-hewitt/conditional-probing. 1626 Proceedings of the 2021 Conference on Empirical"
2021.emnlp-main.292,P17-1171,0,0.335868,"ich allows it to easily adapt to arbitrary collections of text without requiring welltuned neural retrieval systems or extra metadata. This further allows users to understand and control IRRR, if necessary, to facilitate trust. Moreover, IRRR iteratively retrieves more context to answer the question, which allows it to easily accommodate questions of different number of reasoning steps. To evaluate the performance of open-domain QA systems in a more realistic setting, we construct a new benchmark called BeerQA1 by combining the questions from the single-hop SQuAD Open (Rajpurkar et al., 2016; Chen et al., 2017) and the two-hop HotpotQA (Yang et al., 2018) with a new collection of 530 human-annotated questions that require information from at least three Wikipedia pages to answer. We map all questions to a unified version of the English Wikipedia to reduce stylistic differences that might provide statistical shortcuts to models. As a result, BeerQA provides a more realistic evaluation of open-ended question answering systems in their ability to answer questions without knowledge of the number of reasoning steps required ahead of time. We show that IRRR not 1 https://beerqa.github.io/ only achieves co"
2021.emnlp-main.292,2020.findings-emnlp.91,0,0.0334409,"Missing"
2021.emnlp-main.292,P18-1078,0,0.0227554,"from Wikipedia, researchers have also used news articles (Trischler et al., 2016) and search results from the web (Dunn et al., 2017; Talmor and Berant, 2018) as the corpus for open-domain QA. Inspired by the TREC QA challenge,8 Chen et al. (2017) were the first to combine information retrieval systems with accurate neural network-based reading comprehension models for open-domain QA. Recent work has improved open-domain QA performance by enhancing various components in this retrieve-and-read approach. While much research focused on improving the reading comprehension model (Seo et al., 2017; Clark and Gardner, 2018), especially with pretrained langauge models like BERT (Devlin et al., 2019), researchers have The availability of large-scale question answering (QA) datasets has greatly contributed to the research progress on open-domain QA. SQuAD (Rajpurkar 3606 8 https://trec.nist.gov/data/qamain.html also demonstrated that neural network-based information retrieval systems achieve competitive, if not better, performance compared to traditional IR engines (Lee et al., 2019; Khattab et al., 2020; Guu et al., 2020; Xiong et al., 2021). Aside from the reading comprehension and retrieval components, researche"
2021.emnlp-main.292,N19-1423,0,0.529385,"(the question and all retrieved paragraphs so far) as input, and one set of task-specific parameters for each task of retrieval, reranking, and reading comprehension (see Figure 2). The retriever generates natural language search queries by selecting words from the reasoning path, the reader extracts answers from the reasoning path and abstains if its confidence is not high enough, and the reranker assigns a scalar score for each retrieved paragraph as a potential continuation of the current reasoning path. The input to our Transformer encoder is formatted similarly to that of the BERT model (Devlin et al., 2019). For a reasoning path ? that consists of the question and ? retrieved paragraphs, the input is formatted as “[CLS] question [SEP] title1 [CONT] para1 [SEP] . . . title? [CONT] para? [SEP]”, where [CLS], [SEP], and [CONT] are special tokens to separate different components of the input. The [CONT] embedding is randomly initialized with a truncated normal distribution with a standard deviation of 0.02, and finetuned with other model parameters during training. We will detail each of the task-specific components in the following subsections. 3601 4 For simplicity, we assume that there is a singl"
2021.emnlp-main.292,P19-1222,0,0.0740918,"ted ponents simultaneously in an multi-task learning to the HotpotQA leaderboard. On the 3+ hop chal6 In this work, we used the English Wikipedia dump from lenge set, we similarly notice a large performance August 1st, 2020. margin between IRRR and GRR, although neither 7 We refer the reader to Appendix A for further details is trained with questions requiring three or more about these Wikipedia corpora and how we process and map between them. hops, demonstrating that IRRR generalizes well to 3604 DrQA (Chen et al., 2017) DensePR (Karpukhin et al., 2020) BERTserini (Yang et al., 2019) MUPPET (Feldman and El-Yaniv, 2019) RE3 (Hu et al., 2019) Knowledge-aided (Zhou et al., 2020) Multi-passage BERT (Wang et al., 2019) GRR (Asai et al., 2020) FiD (Izacard and Grave, 2020) SPARTA (Zhao et al., 2020b) 27.1 38.1 38.6 39.3 41.9 43.6 53.0 56.5 56.7 59.3 — — 46.1 46.2 50.2 53.4 60.9 63.8 — 66.5 IRRR (SQuAD) IRRR (SQuAD+HotpotQA) 56.8 61.8 63.2 68.9 Table 2: End-to-end question answering performance on SQuAD Open, evaluated on the same set of documents as Chen et al. (2017). System HotpotQA EM F1 3+ hop EM F1 GRR (Asai et al., 2020) Step-by-step ⊗ DDRQA (Zhang et al., 2021) MDR (Xiong et al., 2021) EBS-SH ⊗ TPRR ⊗ HopR"
2021.emnlp-main.292,N19-4013,0,0.0365239,"Missing"
2021.emnlp-main.292,D18-1259,1,0.913569,"llections of text without requiring welltuned neural retrieval systems or extra metadata. This further allows users to understand and control IRRR, if necessary, to facilitate trust. Moreover, IRRR iteratively retrieves more context to answer the question, which allows it to easily accommodate questions of different number of reasoning steps. To evaluate the performance of open-domain QA systems in a more realistic setting, we construct a new benchmark called BeerQA1 by combining the questions from the single-hop SQuAD Open (Rajpurkar et al., 2016; Chen et al., 2017) and the two-hop HotpotQA (Yang et al., 2018) with a new collection of 530 human-annotated questions that require information from at least three Wikipedia pages to answer. We map all questions to a unified version of the English Wikipedia to reduce stylistic differences that might provide statistical shortcuts to models. As a result, BeerQA provides a more realistic evaluation of open-ended question answering systems in their ability to answer questions without knowledge of the number of reasoning steps required ahead of time. We show that IRRR not 1 https://beerqa.github.io/ only achieves competitive performance with stateof-the-art mo"
2021.emnlp-main.292,2020.acl-tutorials.8,0,0.054275,"Missing"
2021.emnlp-main.292,D19-1599,0,0.0267422,"Missing"
2021.findings-emnlp.164,N19-1423,0,0.0214681,"ounding tokens n, max. context length l Output: List of overlapping contexts contexts = [] ; start = 0 ; while len(B) &gt; 0 do for bi in B where bi − start &lt;= l do B.remove(bi−1 ) ; end = bi−1 ; end contexts.append(T [start : (start + l)]) ; start = end − n ; end return contexts ; Transformer-based models have become a dominant approach for many NLP tasks. Previous works 1 2 implemented span identification on the Trans3 former architecture by predicting start and end to4 kens, scaling it to a document by splitting the doc5 6 ument into multiple contexts with a static window 7 and a stride size (Devlin et al., 2019; Hendrycks 8 et al., 2021). The start/end token prediction makes 9 10 the problem unnecessarily difficult because the 11 model has to solve span boundary detection and Algorithm 1: Dynamic context segmentation evidence identification concurrently, whereas the definition of spans is usually fixed for many applications. Splitting a document can be problematic marked span and repeat this until all the spans are when a span is split into multiple contexts or when a span does not receive enough surrounding con- marked. We mark variables associated with m-th context with a left superscript m where"
2021.findings-emnlp.164,2020.findings-emnlp.309,0,0.099173,"Missing"
2021.findings-emnlp.164,2020.acl-demos.14,1,0.830447,"Missing"
2021.findings-emnlp.164,N18-1074,0,0.0543515,"Missing"
2021.naacl-main.61,2020.sigdial-1.29,0,0.0568936,"Missing"
2021.naacl-main.61,N16-1014,0,0.284946,"of the above types in machine-learned models, we consider a simplified task of conversational rephrasing (Figure 1), in which the factual content to be added is not left latent but is provided as a text input to the model (as in Dinan et al. (2019)), along with conversational history. Just as humans do not recite a fact verbatim in a conversation, we expect the model to rephrase the factual content by taking conversational context into account. We derive the data for this task using the Topical Chat dataset (Gopalakrishnan et al., 2019) and fine-tune a large pre-trained language model on it. Li et al. (2016); Zhang et al. (2020) use maximum pointwise mutual information (Max-PMI) to filter out bad and unspecific responses sampled from a generative language model. However, we observe that Max-PMI responses lack in acknowledgement, an essential human trait. This is because a generated response that simply copies over the new factual content while largely ignoring the conversational history can have high mutual information (MI) with the overall input. Our second contribution is a method to select responses that exhibit human-like acknowledgement. To quantify the amount of information drawn from the t"
2021.naacl-main.61,D18-1073,0,0.0643413,"Missing"
2021.naacl-main.61,J00-3003,0,0.705366,"Missing"
2021.naacl-main.61,2020.emnlp-demos.6,0,0.0466335,"Missing"
2021.naacl-main.61,P18-1205,0,0.0671386,"Missing"
2021.naacl-main.61,2020.acl-demos.30,0,0.236058,"s in machine-learned models, we consider a simplified task of conversational rephrasing (Figure 1), in which the factual content to be added is not left latent but is provided as a text input to the model (as in Dinan et al. (2019)), along with conversational history. Just as humans do not recite a fact verbatim in a conversation, we expect the model to rephrase the factual content by taking conversational context into account. We derive the data for this task using the Topical Chat dataset (Gopalakrishnan et al., 2019) and fine-tune a large pre-trained language model on it. Li et al. (2016); Zhang et al. (2020) use maximum pointwise mutual information (Max-PMI) to filter out bad and unspecific responses sampled from a generative language model. However, we observe that Max-PMI responses lack in acknowledgement, an essential human trait. This is because a generated response that simply copies over the new factual content while largely ignoring the conversational history can have high mutual information (MI) with the overall input. Our second contribution is a method to select responses that exhibit human-like acknowledgement. To quantify the amount of information drawn from the two contexts of new fa"
2021.naacl-main.88,2020.emnlp-main.38,0,0.299452,"onal unlabeled data. 2 Related Work Few-shot learning in NLP. The goal of learning from few examples has been studied for various NLP applications. Common settings include few-shot adaptation to new relations (Han et al., 2018), words (Holla et al., 2020), domains (Bao et al., 2020; Yu et al., 2018; Geng et al., 2019), and language pairs (Gu et al., 2018). Since these applications come with well-defined task distributions, they do not have the same overfitting challenges. On the other hand, many works deal with few-shot adaptation in settings with no clear task distribution (Dou et al., 2019; Bansal et al., 2020a) but do not address meta-overfitting, and thus are complementary to our work. first studied in Rajendran et al. (2020) in the context of few-shot label adaptation. Hsu et al. (2019) propose CACTUs, a clustering-based approach for unsupervised meta-learning in the context of few-shot label adaptation for images. While also based on clustering, CACTUs creates meta-learning tasks where the goal is to predict cluster membership of images, whereas our work is focused on using clusters to subdivide pre-existing tasks for mitigating meta-overfitting in NLP. Most closely related to our work is the S"
2021.naacl-main.88,D15-1075,1,0.766491,"our work is the SMLMT method from Bansal et al. (2020b). SMLMT creates new self-supervised tasks that improve meta-overfitting but this does not directly address the dataset-as-tasks problem we identify. In contrast, we focus on using clustering as a way to subdivide and fix tasks that already exist. This approach allows us to mitigate meta-overfitting without additional unlabeled data. In Section 6, we compare our model against SMLMT, and demonstrate comparable or better performance. 3 3.1 Setting NLI We consider the problem of Natural Language Inference or NLI (MacCartney and Manning, 2008; Bowman et al., 2015), also known as Recognising Textual Entailment (RTE) (Dagan et al., 2005). Given a sentence pair x = (p, h) where p is referred to as the premise sentence, and h is the hypothesis sentence, the goal is to output a binary label1 yˆ ∈ {0, 1} indicating whether the hypothesis h is entailed by the premise p or not. For instance, the sentence pair (The dog barked, The animal barked) is classified as entailed, whereas the sentence pair (The dog barked, The labrador barked) would be classified as not entailed. As shown in Table 1, NLI datasets typically encompass a broad range of linguistic phenomena"
2021.naacl-main.88,W07-1401,0,0.165546,"2020) for training. These training datasets cover a broad range of NLI phenomena. MultiNLI consists of crowdsourced examples, DNC consists of various semantic annotations from NLP datasets re-cast into NLI and Semantic fragments is a synthetic NLI dataset covering logical and monotonicity reasoning. Our objective is to train a single meta-learner that can then be used to make predictions on diverse NLP problems recast as NLI. To this end, we evaluate models trained on C OMBINED NLI on 2 datasets. In C OMBINED NLIRTE, we evaluate on the RTE datasets (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) as provided in GLUE (Wang et al., 2019). The RTE datasets consist of various IE and QA datasets recast as NLI. Second, we consider the QANLI dataset (Demszky et al., 2018) which recasts question answering into NLI. In particular, we consider RACE (Lai et al., 2017) and use gold annotations provided in Demszky et al. (2018) to transform it into an NLI dataset. GLUE-SciTail where we train on all NLI datasets from GLUE (Wang et al., 2019) and evaluate on SciTail (Khot et al., 2018). This setting is comparable to Bansal et al. (2020b) with the difference that we only met"
2021.naacl-main.88,D18-1398,0,0.0835072,"we consider the problem of natural language inference (NLI). We show that meta-learners augmented with DR E C A improve over baselines by 1.5–4 accuracy points across four separate NLI few-shot problems without requiring domain-specific engineering or additional unlabeled data. 2 Related Work Few-shot learning in NLP. The goal of learning from few examples has been studied for various NLP applications. Common settings include few-shot adaptation to new relations (Han et al., 2018), words (Holla et al., 2020), domains (Bao et al., 2020; Yu et al., 2018; Geng et al., 2019), and language pairs (Gu et al., 2018). Since these applications come with well-defined task distributions, they do not have the same overfitting challenges. On the other hand, many works deal with few-shot adaptation in settings with no clear task distribution (Dou et al., 2019; Bansal et al., 2020a) but do not address meta-overfitting, and thus are complementary to our work. first studied in Rajendran et al. (2020) in the context of few-shot label adaptation. Hsu et al. (2019) propose CACTUs, a clustering-based approach for unsupervised meta-learning in the context of few-shot label adaptation for images. While also based on clu"
2021.naacl-main.88,D18-1514,0,0.0794499,"cedures fail to adapt. However, a model that meta-learns over the underlying reasoning types shows a substantial improvement. Then, we consider the problem of natural language inference (NLI). We show that meta-learners augmented with DR E C A improve over baselines by 1.5–4 accuracy points across four separate NLI few-shot problems without requiring domain-specific engineering or additional unlabeled data. 2 Related Work Few-shot learning in NLP. The goal of learning from few examples has been studied for various NLP applications. Common settings include few-shot adaptation to new relations (Han et al., 2018), words (Holla et al., 2020), domains (Bao et al., 2020; Yu et al., 2018; Geng et al., 2019), and language pairs (Gu et al., 2018). Since these applications come with well-defined task distributions, they do not have the same overfitting challenges. On the other hand, many works deal with few-shot adaptation in settings with no clear task distribution (Dou et al., 2019; Bansal et al., 2020a) but do not address meta-overfitting, and thus are complementary to our work. first studied in Rajendran et al. (2020) in the context of few-shot label adaptation. Hsu et al. (2019) propose CACTUs, a cluste"
2021.naacl-main.88,2020.findings-emnlp.405,0,0.0205945,"ever, a model that meta-learns over the underlying reasoning types shows a substantial improvement. Then, we consider the problem of natural language inference (NLI). We show that meta-learners augmented with DR E C A improve over baselines by 1.5–4 accuracy points across four separate NLI few-shot problems without requiring domain-specific engineering or additional unlabeled data. 2 Related Work Few-shot learning in NLP. The goal of learning from few examples has been studied for various NLP applications. Common settings include few-shot adaptation to new relations (Han et al., 2018), words (Holla et al., 2020), domains (Bao et al., 2020; Yu et al., 2018; Geng et al., 2019), and language pairs (Gu et al., 2018). Since these applications come with well-defined task distributions, they do not have the same overfitting challenges. On the other hand, many works deal with few-shot adaptation in settings with no clear task distribution (Dou et al., 2019; Bansal et al., 2020a) but do not address meta-overfitting, and thus are complementary to our work. first studied in Rajendran et al. (2020) in the context of few-shot label adaptation. Hsu et al. (2019) propose CACTUs, a clustering-based approach for unsu"
2021.naacl-main.88,2020.conll-1.4,0,0.0323541,"ion, and train on the remaining 10. While this is a simple setting, it allows us to compare DR E C A against an “oracle"" with access to the underlying reasoning categories. In this section, we introduce our approach for extracting reasoning categories for NLI. The key observation here is that high quality sentence pair representations, such as those obtained from a finetuned BERT model, can bring out the microstructure of NLI datasets. Indeed, the fact that pretrained transformers can be used to create meaningful clusters has been shown in other recent works (c.f. Aharoni and Goldberg (2020); Joshi et al. (2020)). At a high level, the goal of DR E C A is to take a 2 Note that we do not instantiate the K N tasks. Instead, we heterogeneous task (such as a dataset) and produce simply sample an episode from random chosen clusters from a decomposed set of tasks. In doing so, we hope to each label group. 1117 C OMBINED NLI consists of a combination of 3 NLI datasets—MultiNLI (Williams et al., 2018), Diverse Natural Language Inference Collection (DNC; Poliak et al. (2018)) and Semantic Fragments (Richardson et al., 2020) for training. These training datasets cover a broad range of NLI phenomena. MultiNLI co"
2021.naacl-main.88,D17-1082,0,0.0330176,"g. Our objective is to train a single meta-learner that can then be used to make predictions on diverse NLP problems recast as NLI. To this end, we evaluate models trained on C OMBINED NLI on 2 datasets. In C OMBINED NLIRTE, we evaluate on the RTE datasets (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) as provided in GLUE (Wang et al., 2019). The RTE datasets consist of various IE and QA datasets recast as NLI. Second, we consider the QANLI dataset (Demszky et al., 2018) which recasts question answering into NLI. In particular, we consider RACE (Lai et al., 2017) and use gold annotations provided in Demszky et al. (2018) to transform it into an NLI dataset. GLUE-SciTail where we train on all NLI datasets from GLUE (Wang et al., 2019) and evaluate on SciTail (Khot et al., 2018). This setting is comparable to Bansal et al. (2020b) with the difference that we only meta-train on the NLI subset of GLUE, whereas they meta-train on all GLUE tasks. We follow the same evaluation protocol as Bansal et al. (2020b) and report 2-way 4-shot accuracy. 6.2 Models Non-Episodic Baselines. All non-episodic baselines train hθ on the union of all examples from each Titr ."
2021.naacl-main.88,C08-1066,1,0.476428,"NLP. Most closely related to our work is the SMLMT method from Bansal et al. (2020b). SMLMT creates new self-supervised tasks that improve meta-overfitting but this does not directly address the dataset-as-tasks problem we identify. In contrast, we focus on using clustering as a way to subdivide and fix tasks that already exist. This approach allows us to mitigate meta-overfitting without additional unlabeled data. In Section 6, we compare our model against SMLMT, and demonstrate comparable or better performance. 3 3.1 Setting NLI We consider the problem of Natural Language Inference or NLI (MacCartney and Manning, 2008; Bowman et al., 2015), also known as Recognising Textual Entailment (RTE) (Dagan et al., 2005). Given a sentence pair x = (p, h) where p is referred to as the premise sentence, and h is the hypothesis sentence, the goal is to output a binary label1 yˆ ∈ {0, 1} indicating whether the hypothesis h is entailed by the premise p or not. For instance, the sentence pair (The dog barked, The animal barked) is classified as entailed, whereas the sentence pair (The dog barked, The labrador barked) would be classified as not entailed. As shown in Table 1, NLI datasets typically encompass a broad range o"
2021.naacl-main.88,J93-2004,0,0.074053,"rm this and hypothesis. We then use t-SNE (Maaten and analysis, we focus on MultiNLI annotation tags Hinton, 2008) to project these representations onto from Williams et al. (2018). A subset of exam2 dimensions. Each point in Fig. 4 is colored with ples in MultiNLI are assigned tags based on the its corresponding reasoning category, and we can presence of certain keywords, e.g., time words like observe a clear clustering of examples according days of the week; quantifiers like every, each, some; 1119 negation words like no, not, never. Additionally, certain tags are assigned based on the PTB (Marcus et al., 1993) parses of examples, e.g., presence or absence of adjectives/adverbs etc. For each annotation tag, we compute the fraction of examples labeled with that tag in each cluster. We visualize this for 10 annotation tags and indicate statistically significant deviations from the averages in Fig. 5. Statistical significance is measured with binomial testing with a Bonferroni correction to account for multiple testing. For every annotation tag, we shade all clusters that contain a statistically significant deviation from the mean. For instance, there is a positive cluster with 2.5 fold enrichment in N"
2021.naacl-main.88,P19-1334,0,0.0171502,"ulting clusters to roughly correspond to distinct reasoning categories. Indeed, when the true reasoning categories are known, we show in Section 7.2 that DR E C A yields clusters that recover these reasoning categories almost exactly. 6 6.1 NLI Experiments Datasets We evaluate DR E C A on 4 NLI few-shot learning problems which we describe below (more details in Appendix A.2.1). The first problem is based on synthetic data, while the other 3 problems are on real datasets and hence a good demonstration of the utility of our proposal. HANS- FEWSHOT is a few-shot classification problem over HANS (McCoy et al., 2019), a synthetic diagnostic dataset for NLI. Each example in HANS comes from a hand-designed syntactic template which is associated with a fixed label (entailment or not_entailment). The entire dataset consists of 30 such templates which we use to define 15 reasoning categories. We then hold out 5 of these for evaluation, and train on the remaining 10. While this is a simple setting, it allows us to compare DR E C A against an “oracle"" with access to the underlying reasoning categories. In this section, we introduce our approach for extracting reasoning categories for NLI. The key observation her"
2021.naacl-main.88,W18-5441,0,0.0563086,"Missing"
2021.naacl-main.88,N18-1101,0,0.0276929,"inetune BERT (Devlin et al., 2019) for 5000 ran- 7.3 Distribution of linguistic phenomena across clusters domly chosen examples from HANS. To obtain a vector representation for each example x = (p, h), We seek to understand how different linguistic phewe concatenate the vector at the [CLS] token, along nomena present in the overall population are diswith a mean pooled representation of the premise tributed among various clusters. To perform this and hypothesis. We then use t-SNE (Maaten and analysis, we focus on MultiNLI annotation tags Hinton, 2008) to project these representations onto from Williams et al. (2018). A subset of exam2 dimensions. Each point in Fig. 4 is colored with ples in MultiNLI are assigned tags based on the its corresponding reasoning category, and we can presence of certain keywords, e.g., time words like observe a clear clustering of examples according days of the week; quantifiers like every, each, some; 1119 negation words like no, not, never. Additionally, certain tags are assigned based on the PTB (Marcus et al., 1993) parses of examples, e.g., presence or absence of adjectives/adverbs etc. For each annotation tag, we compute the fraction of examples labeled with that tag in"
2021.naacl-main.88,2020.emnlp-demos.6,0,0.0572305,"Missing"
2021.naacl-main.88,N18-1109,0,0.0795353,"ng reasoning types shows a substantial improvement. Then, we consider the problem of natural language inference (NLI). We show that meta-learners augmented with DR E C A improve over baselines by 1.5–4 accuracy points across four separate NLI few-shot problems without requiring domain-specific engineering or additional unlabeled data. 2 Related Work Few-shot learning in NLP. The goal of learning from few examples has been studied for various NLP applications. Common settings include few-shot adaptation to new relations (Han et al., 2018), words (Holla et al., 2020), domains (Bao et al., 2020; Yu et al., 2018; Geng et al., 2019), and language pairs (Gu et al., 2018). Since these applications come with well-defined task distributions, they do not have the same overfitting challenges. On the other hand, many works deal with few-shot adaptation in settings with no clear task distribution (Dou et al., 2019; Bansal et al., 2020a) but do not address meta-overfitting, and thus are complementary to our work. first studied in Rajendran et al. (2020) in the context of few-shot label adaptation. Hsu et al. (2019) propose CACTUs, a clustering-based approach for unsupervised meta-learning in the context of few"
2021.nllp-1.15,L18-1298,0,0.0255021,", which partially represent the baselines, 7 Related Works were ranked high in many cases. At the same time, other features such as “all capital (T10)” and “punc- As discussed in Section 1, previous works mainly focused on word segmentation and layout analysis, tuated (T2)” were also contributing significantly to the accuracy, which made our system much supe- whereas fine-grained logical structure analysis of VSDs is less addressed. Nevertheless, there exist rior to the baselines. The feature importance revealed that the seman- some studies that focus on similar goals. Abreu et al. (2019) and Ferrés et al. (2018) have tic cue (S1) was no more important than other tried to deal with logical structure analysis by idencues. We suspect that the feature (which compares tifying specific structures in VSDs such as subheadwhether adjacent or non-adjacent block is more ings. However, these studies are too coarse-grained likely given a context) had fallen back to mere and cannot handle paragraph-level logical structure, language model with the context being ignored in some cases, possibly due to GPT-2 not being fine- thus they are unable to satisfy the need we have discussed in Section 1. FinSBD-3 shared task ("
2021.nllp-1.15,N09-2061,0,0.0147805,"s, some examples. Unlike for Contract pdf , we attribute this en of their ideas could be incorporated to our work as problem to lack of training data, as those should additional features. We leave use of more advanced have been classified correctly with other features semantic cues for a future work. (such as T4 and T8) if the system had seen similar patterns in the training data. While the goal is different, our textual features Interestingly, we observed that the system tends have some similarity to those used in sentence to do better in documents that are hierarchically boundary detection (Gillick, 2009). Since our goal more complex. This may be because hierarchically is to predict structures as well as boundaries, we complex documents tend to incorporate more cues employ richer textual and visual features that they 151 do not utilize. LayoutLM (Xu et al., 2020, 2021) incorporates multimodal self-supervised learning to utilize deep learning for form understanding. While it may alleviate the need for a large training dataset, it is not trivial to adopt the same method for logical structure analysis as text blocks would not fit onto the LayoutLM’s context. Furthermore, it is easier to diagnose"
2021.nllp-1.15,W19-6410,0,0.0120003,"umbering hierarchy (T1)”, which partially represent the baselines, 7 Related Works were ranked high in many cases. At the same time, other features such as “all capital (T10)” and “punc- As discussed in Section 1, previous works mainly focused on word segmentation and layout analysis, tuated (T2)” were also contributing significantly to the accuracy, which made our system much supe- whereas fine-grained logical structure analysis of VSDs is less addressed. Nevertheless, there exist rior to the baselines. The feature importance revealed that the seman- some studies that focus on similar goals. Abreu et al. (2019) and Ferrés et al. (2018) have tic cue (S1) was no more important than other tried to deal with logical structure analysis by idencues. We suspect that the feature (which compares tifying specific structures in VSDs such as subheadwhether adjacent or non-adjacent block is more ings. However, these studies are too coarse-grained likely given a context) had fallen back to mere and cannot handle paragraph-level logical structure, language model with the context being ignored in some cases, possibly due to GPT-2 not being fine- thus they are unable to satisfy the need we have discussed in Section"
2021.nllp-1.15,D19-1348,0,0.0613106,"Missing"
2021.nllp-1.15,W04-3210,0,0.0336772,"s not consider hierarchies in non-list without any indentation (predicted continuous instead of down). We believe incorporating ty- paragraphs. Hatsutori et al. (2017) proposed a rule-based syspographic features would improve our system as implied by the success of the “all capital (T10)” tem that purely relies on numberings. We compared our system against it in Section 6 and showed that feature. txt our system, which also incorporates textual and For Contract en , we found that blocks that are all semantic cues, is superior to their method. capitals or are all underbars were misclassified as Sporleder and Lapata (2004) proposed a paraomitted. All capital words and underbars are fregraph boundary detection method for plain texts quently used to denote headers and footers, but they that purely relies on textual and semantic cues. were used as section titles and input fields in these While their method is not intended for VSDs, some examples. Unlike for Contract pdf , we attribute this en of their ideas could be incorporated to our work as problem to lack of training data, as those should additional features. We leave use of more advanced have been classified correctly with other features semantic cues for a f"
2021.nllp-1.15,2021.acl-long.201,0,0.0898943,"Missing"
2021.nllp-1.15,P19-1009,0,0.0541099,"Missing"
2021.nlp4posimpact-1.8,W16-3612,0,0.0272498,"n extraction, question answering allows for a greater range of tasks, represented by the diversity of question formulations (Rajpurkar et al., 2016) and is an alternative approach to the task of creating parole hearing annotations. For both information extraction and question answering, current top-performing models are pretrained large language models (Devlin et al., 2019; Radford et al., 2019) that have been fine-tuned on specific tasks, such as question answering. Applications to dialogue focus on entity-based tasks like argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), relation extraction (Yu et al., 2020), and task-based extraction (Fang et al., 2018; Finch et al., 2020; Liang et al., 2020). Dialogue-like settings are relatively new for question answering. CoQA (Reddy et al., 2019) aims to answer questions over a written text in an abstractive way, but it is only conversational in that multiple questions can be asked of the same source text sequentially. FriendsQA (Yang and Choi, 2019) answers extractive questions about a multiparty dialogue. The questions are considered to be asked of the dialogue, by a third pa"
2021.nlp4posimpact-1.8,N16-1097,0,0.0219218,"Missing"
2021.nlp4posimpact-1.8,N19-1300,0,0.01499,"thousand words long. The score can be mentioned at the very beginning or very end, but often it is tucked away somewhere in the middle. Can neural question answering successfully address parole hearings? Neural question answering systems have the flexibility of handling a large range of question formulations and feature types. Compared to other models, this flexibility improves the performance on date features, but surprisingly, on only one additional task, risk assess. Boolean questions remain an outstanding challenge. Reading comprehension datasets like CoQA (Reddy et al., 2019) and BoolQ (Clark et al., 2019) include such questions but leave a substantial performance gap for future work. The reliance on manual conversion of some answers to binary or 76 8 Conclusion on risk-related words in the sub-topics, rather than returning to the higher level question of the risk scores. Common sense knowledge will also play a role in solving this challenge. In one section of a hearing, the commissioner says, “And, uh, I note that you – you have both a high school diploma and GED, is that correct?” Over the course of the next eight thousand words, the parole candidate describes his life, from playing sports in"
2021.nlp4posimpact-1.8,L18-1707,0,0.0222301,"r a greater range of tasks, represented by the diversity of question formulations (Rajpurkar et al., 2016) and is an alternative approach to the task of creating parole hearing annotations. For both information extraction and question answering, current top-performing models are pretrained large language models (Devlin et al., 2019; Radford et al., 2019) that have been fine-tuned on specific tasks, such as question answering. Applications to dialogue focus on entity-based tasks like argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), relation extraction (Yu et al., 2020), and task-based extraction (Fang et al., 2018; Finch et al., 2020; Liang et al., 2020). Dialogue-like settings are relatively new for question answering. CoQA (Reddy et al., 2019) aims to answer questions over a written text in an abstractive way, but it is only conversational in that multiple questions can be asked of the same source text sequentially. FriendsQA (Yang and Choi, 2019) answers extractive questions about a multiparty dialogue. The questions are considered to be asked of the dialogue, by a third party outside the dialogue. Like FriendsQA, D"
2021.nlp4posimpact-1.8,chang-manning-2012-sutime,1,0.684789,"models are extractive question answering models, i.e. the answers returned are taken from the text of the hearing. In some cases, the text needs additional processing to be transformed into a label. The transformation may be human intervention, such as in the case of edu level, where the extractive answer “ninth grade” and needs to be translated into a categorical answer “no high school or GED.” In other cases, such as with dates, the transformation can be partially or fully automated, such as by parsing answers like “March the 6th, 2019” into the MEPD year, 2019, using tools such as SUTime (Chang and Manning, 2012). Overall, WSLF does well on most classification tasks, though it is beaten by QA2 on risk assess and by the more powerful classifier Task-FT on off mur1. QA2 is strongest on dates and generally outperforms QA1. Task-FT performs best on a variety of tasks, but surprisingly, it does not always improve over WSLF and Snorkel, even though its training process uses the very labels produced by the data programming methods, but augmented with even more information, the underlying text itself. Pre-Trained Language Models Data programming generalizes the knowledge of domain experts; pre-trained languag"
2021.nlp4posimpact-1.8,N19-1423,0,0.0661768,"level (Nguyen and Grishman, 2015; Adel et al., 2016; Levy et al., 2017; Karita et al., 2019; Luo et al., 2019), but techniques have emerged for cross-sentence or even document-level relation extraction (Yao et al., 2019). Compared to information extraction, question answering allows for a greater range of tasks, represented by the diversity of question formulations (Rajpurkar et al., 2016) and is an alternative approach to the task of creating parole hearing annotations. For both information extraction and question answering, current top-performing models are pretrained large language models (Devlin et al., 2019; Radford et al., 2019) that have been fine-tuned on specific tasks, such as question answering. Applications to dialogue focus on entity-based tasks like argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), relation extraction (Yu et al., 2020), and task-based extraction (Fang et al., 2018; Finch et al., 2020; Liang et al., 2020). Dialogue-like settings are relatively new for question answering. CoQA (Reddy et al., 2019) aims to answer questions over a written text in an abstractive way, but it is only conversati"
2021.nlp4posimpact-1.8,D19-1667,0,0.0278403,"Missing"
2021.nlp4posimpact-1.8,2020.acl-main.261,0,0.0602206,"Missing"
2021.nlp4posimpact-1.8,N18-5020,0,0.0165242,"rkar et al., 2016) and is an alternative approach to the task of creating parole hearing annotations. For both information extraction and question answering, current top-performing models are pretrained large language models (Devlin et al., 2019; Radford et al., 2019) that have been fine-tuned on specific tasks, such as question answering. Applications to dialogue focus on entity-based tasks like argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), relation extraction (Yu et al., 2020), and task-based extraction (Fang et al., 2018; Finch et al., 2020; Liang et al., 2020). Dialogue-like settings are relatively new for question answering. CoQA (Reddy et al., 2019) aims to answer questions over a written text in an abstractive way, but it is only conversational in that multiple questions can be asked of the same source text sequentially. FriendsQA (Yang and Choi, 2019) answers extractive questions about a multiparty dialogue. The questions are considered to be asked of the dialogue, by a third party outside the dialogue. Like FriendsQA, DREAM (Sun et al., 2019) also uses a dialogue as its source text, but its answers are"
2021.nlp4posimpact-1.8,K17-1034,0,0.0116633,"nuous dialogue in a single sitting between a decision-maker and a parole candidate, with brief statements from the candidate’s attorney. In comparison, criminal trials are much longer, present many forms of exhibits which are often not digitally available, and contain many additional complexities. 2.1 Related Work Information Extraction and Question Answering Information extraction spans a number of tasks, but neural approaches have concentrated on binary relation extraction. Many relation extraction tasks are performed on only the sentence level (Nguyen and Grishman, 2015; Adel et al., 2016; Levy et al., 2017; Karita et al., 2019; Luo et al., 2019), but techniques have emerged for cross-sentence or even document-level relation extraction (Yao et al., 2019). Compared to information extraction, question answering allows for a greater range of tasks, represented by the diversity of question formulations (Rajpurkar et al., 2016) and is an alternative approach to the task of creating parole hearing annotations. For both information extraction and question answering, current top-performing models are pretrained large language models (Devlin et al., 2019; Radford et al., 2019) that have been fine-tuned o"
2021.nlp4posimpact-1.8,W19-1505,0,0.0151896,"en a decision-maker and a parole candidate, with brief statements from the candidate’s attorney. In comparison, criminal trials are much longer, present many forms of exhibits which are often not digitally available, and contain many additional complexities. 2.1 Related Work Information Extraction and Question Answering Information extraction spans a number of tasks, but neural approaches have concentrated on binary relation extraction. Many relation extraction tasks are performed on only the sentence level (Nguyen and Grishman, 2015; Adel et al., 2016; Levy et al., 2017; Karita et al., 2019; Luo et al., 2019), but techniques have emerged for cross-sentence or even document-level relation extraction (Yao et al., 2019). Compared to information extraction, question answering allows for a greater range of tasks, represented by the diversity of question formulations (Rajpurkar et al., 2016) and is an alternative approach to the task of creating parole hearing annotations. For both information extraction and question answering, current top-performing models are pretrained large language models (Devlin et al., 2019; Radford et al., 2019) that have been fine-tuned on specific tasks, such as question answe"
2021.nlp4posimpact-1.8,C18-1041,0,0.028388,"Missing"
2021.nlp4posimpact-1.8,D16-1264,0,0.158878,"Information Extraction and Question Answering Information extraction spans a number of tasks, but neural approaches have concentrated on binary relation extraction. Many relation extraction tasks are performed on only the sentence level (Nguyen and Grishman, 2015; Adel et al., 2016; Levy et al., 2017; Karita et al., 2019; Luo et al., 2019), but techniques have emerged for cross-sentence or even document-level relation extraction (Yao et al., 2019). Compared to information extraction, question answering allows for a greater range of tasks, represented by the diversity of question formulations (Rajpurkar et al., 2016) and is an alternative approach to the task of creating parole hearing annotations. For both information extraction and question answering, current top-performing models are pretrained large language models (Devlin et al., 2019; Radford et al., 2019) that have been fine-tuned on specific tasks, such as question answering. Applications to dialogue focus on entity-based tasks like argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), relation extraction (Yu et al., 2020), and task-based extraction (Fang et al., 2018;"
2021.nlp4posimpact-1.8,2020.lrec-1.667,0,0.0172601,"the very labels produced by the data programming methods, but augmented with even more information, the underlying text itself. Pre-Trained Language Models Data programming generalizes the knowledge of domain experts; pre-trained language models generalize the knowledge of a large English corpus. We first use models fine-tuned for question answering, which allows us to use a single model for a wide range of features. We study two question answering models: DistilBERT (Sanh et al., 2019) fine-tuned on SQuAD (Rajpurkar et al., 2016) and Longformer (Beltagy et al., 2020) fine-tuned on SQuAD 2.0 (Lee et al., 2020). We call these two models QA1 and QA2, respectively. Through QA1, we hope to understand the overall performance gain, if any, from pre-training. Through QA2, we hope to understand any advantages of using a model with a longer context window (4,096 tokens) that can handle unanswerable questions, which are common in this corpus. Our second approach is to model each task as a classification task and to fine-tune a language model for each task. We first fine-tune the base BERT model (Devlin et al., 2019) on all parole hearing text, including unlabeled documents. We then train a classifier layer o"
2021.nlp4posimpact-1.8,Q19-1014,0,0.0215464,"tion extraction (Yu et al., 2020), and task-based extraction (Fang et al., 2018; Finch et al., 2020; Liang et al., 2020). Dialogue-like settings are relatively new for question answering. CoQA (Reddy et al., 2019) aims to answer questions over a written text in an abstractive way, but it is only conversational in that multiple questions can be asked of the same source text sequentially. FriendsQA (Yang and Choi, 2019) answers extractive questions about a multiparty dialogue. The questions are considered to be asked of the dialogue, by a third party outside the dialogue. Like FriendsQA, DREAM (Sun et al., 2019) also uses a dialogue as its source text, but its answers are multiple-choice. We have identified 11 case factors representative of the types of features (binary, multi-class, date, and numerical) that are relevant to the parole decision-making system and illustrate a range of challenges in information extraction. We evaluate three families of models on this task: (1) an unsupervised data programming paradigm (Ratner et al., 2016) extended to weak supervision, (2) pretrained question answering models based on DistilBERT (Sanh et al., 2019) and Longformer (Beltagy et al., 2020), and (3) classif"
2021.nlp4posimpact-1.8,W15-4631,0,0.0294168,"action (Yao et al., 2019). Compared to information extraction, question answering allows for a greater range of tasks, represented by the diversity of question formulations (Rajpurkar et al., 2016) and is an alternative approach to the task of creating parole hearing annotations. For both information extraction and question answering, current top-performing models are pretrained large language models (Devlin et al., 2019; Radford et al., 2019) that have been fine-tuned on specific tasks, such as question answering. Applications to dialogue focus on entity-based tasks like argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), relation extraction (Yu et al., 2020), and task-based extraction (Fang et al., 2018; Finch et al., 2020; Liang et al., 2020). Dialogue-like settings are relatively new for question answering. CoQA (Reddy et al., 2019) aims to answer questions over a written text in an abstractive way, but it is only conversational in that multiple questions can be asked of the same source text sequentially. FriendsQA (Yang and Choi, 2019) answers extractive questions about a multiparty dialogue. The questions are consid"
2021.nlp4posimpact-1.8,W19-5923,0,0.0245268,"g. Applications to dialogue focus on entity-based tasks like argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), relation extraction (Yu et al., 2020), and task-based extraction (Fang et al., 2018; Finch et al., 2020; Liang et al., 2020). Dialogue-like settings are relatively new for question answering. CoQA (Reddy et al., 2019) aims to answer questions over a written text in an abstractive way, but it is only conversational in that multiple questions can be asked of the same source text sequentially. FriendsQA (Yang and Choi, 2019) answers extractive questions about a multiparty dialogue. The questions are considered to be asked of the dialogue, by a third party outside the dialogue. Like FriendsQA, DREAM (Sun et al., 2019) also uses a dialogue as its source text, but its answers are multiple-choice. We have identified 11 case factors representative of the types of features (binary, multi-class, date, and numerical) that are relevant to the parole decision-making system and illustrate a range of challenges in information extraction. We evaluate three families of models on this task: (1) an unsupervised data programming"
2021.nlp4posimpact-1.8,P19-1074,0,0.0159927,", criminal trials are much longer, present many forms of exhibits which are often not digitally available, and contain many additional complexities. 2.1 Related Work Information Extraction and Question Answering Information extraction spans a number of tasks, but neural approaches have concentrated on binary relation extraction. Many relation extraction tasks are performed on only the sentence level (Nguyen and Grishman, 2015; Adel et al., 2016; Levy et al., 2017; Karita et al., 2019; Luo et al., 2019), but techniques have emerged for cross-sentence or even document-level relation extraction (Yao et al., 2019). Compared to information extraction, question answering allows for a greater range of tasks, represented by the diversity of question formulations (Rajpurkar et al., 2016) and is an alternative approach to the task of creating parole hearing annotations. For both information extraction and question answering, current top-performing models are pretrained large language models (Devlin et al., 2019; Radford et al., 2019) that have been fine-tuned on specific tasks, such as question answering. Applications to dialogue focus on entity-based tasks like argument extraction (Swanson et al., 2015), na"
2021.nlp4posimpact-1.8,2020.acl-main.444,0,0.0158383,"the diversity of question formulations (Rajpurkar et al., 2016) and is an alternative approach to the task of creating parole hearing annotations. For both information extraction and question answering, current top-performing models are pretrained large language models (Devlin et al., 2019; Radford et al., 2019) that have been fine-tuned on specific tasks, such as question answering. Applications to dialogue focus on entity-based tasks like argument extraction (Swanson et al., 2015), named entity recognition (Chen and Choi, 2016; Choi and Chen, 2018; Bowden et al., 2018), relation extraction (Yu et al., 2020), and task-based extraction (Fang et al., 2018; Finch et al., 2020; Liang et al., 2020). Dialogue-like settings are relatively new for question answering. CoQA (Reddy et al., 2019) aims to answer questions over a written text in an abstractive way, but it is only conversational in that multiple questions can be asked of the same source text sequentially. FriendsQA (Yang and Choi, 2019) answers extractive questions about a multiparty dialogue. The questions are considered to be asked of the dialogue, by a third party outside the dialogue. Like FriendsQA, DREAM (Sun et al., 2019) also uses a dia"
2021.sigdial-1.1,2020.emnlp-main.656,0,0.094892,"Missing"
2021.sigdial-1.1,2020.emnlp-main.28,0,0.042882,"sfaction labels and the PkNN 11 timestep t of the input, and apply a linear layer (W ∈ IR1280 ) and sigmoid activation: 10 These questions are often used repetitively, if the user’s answer to the first asking is unclear/negative (see Appendix A). p &lt;1e-5, Fisher transformation test (null hypothesis ρ=0) 7 8 Did you have fun?, Did you enjoy it? tend to lead to less. Figure 5 (right) shows that the predictor learns these patterns quite closely. 7 Previous work has used a variety of user signals to improve dialogue agents. When learning from a variable-quality human-human dataset such as Reddit, Gao et al. (2020) showed that engagement measures like upvotes and replies are more effective than perplexity to train a ranking model. For oneon-one empathetic conversations like ours, Shin et al. (2019) trained a neural generative model with reinforcement learning to improve next-turn user sentiment (as simulated by a user response model, rather than human responses). Though we considered taking a sentiment-based approach in C HIRPY, we found that user sentiment doesn’t always align with good user experience: first, expressing negative emotions is sometimes unavoidable, and second, sentiment classifiers tend"
2021.sigdial-1.1,P19-1358,0,0.028368,"simulated by a user response model, rather than human responses). Though we considered taking a sentiment-based approach in C HIRPY, we found that user sentiment doesn’t always align with good user experience: first, expressing negative emotions is sometimes unavoidable, and second, sentiment classifiers tend not to distinguish between sentiment about the conversation and sentiment about other issues. We find next-turn user dissatisfaction to be a comparatively more precise, well-aligned learning signal. Dialogue systems that learn from their own interactions with humans are relatively rare. Hancock et al. (2019) also use user satisfaction to identify high-quality bot utterances; these become additional training examples for the neural generative model. However, this work uses paid crowdworkers; research involving intrinsically-motivated, unpaid users is rarer still. In symmetric settings such as the role-playing game LIGHT (Shuster et al., 2020), the user utterances themselves can be used to retrain the dialogue agent. In the asymmetric Alexa Prize setting, Shalyminov et al. (2018) show that conversation-level metrics like rating and length can also be used to train an effective ranker. Ranking neura"
2021.sigdial-1.1,2020.acl-demos.30,0,0.381975,"behaviors that are then replicated by a model trained on them. We use our chatbot’s real-life conversations as a source Introduction Neural generative dialogue agents have become sufficiently mature to make contact with real users through programs such as the Alexa Prize (Gabriel et al., 2020). Though these models have known problems with factual correctness (Mielke et al., 2020), using dialogue history (Sankar et al., 2019), and bias (Dinan et al., 2020), they have nevertheless produced good written conversations when evaluated by crowdworkers or volunteers in carefullycontrolled scenarios (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020). 1 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 1–12 July 29–31, 2021. ©2021 Association for Computational Linguistics of natural in-domain data. In particular, we train a model that can predict authentic user dissatisfaction before it occurs, thus helping us to avoid it. eration. An open-source version of C HIRPY is available, including the code and pretrained model for the Neural Chat module.2 Our Contributions. Through a detailed casestudy of a neural generative model speaking with intri"
2021.sigdial-1.1,P19-1534,0,0.0269999,"utterances. 2.1 2 The Neural Chat module has seven discussion areas, all relating to personal experiences and emotions: Current and Recent Activities, Future Activities, General Activities, Emotions, Family Members, Living Situation, and Food. A Neural Chat discussion begins by asking the user a handwritten starter question from one of the discussion areas; these are designed to be easy-to-answer and applicable to most users. See Appendix D for more details. For subsequent turns of the discussion, we use a GPT-2-medium (Radford et al., 2019) model finetuned on the EmpatheticDialogues dataset (Rashkin et al., 2019).3 Though larger GPT-2 models are now available, their latency and cost is prohibitively high for inclusion in C HIRPY. On each turn, we provide the current Neural Chat discussion history as context to the GPT-2 model, and generate 20 possible responses using top-p sampling with p = 0.9 and temperature 0.7. Repetitive responses (containing previously-used trigrams) are removed. Except when transitioning out of the Neural Chat discussion (see below), we always choose a neural response containing a question.4 Of the responses satisfying these criteria, we choose the longest response, as it tends"
2021.sigdial-1.1,P19-1004,0,0.0255392,"ms can be difficult to build due to a lack of sufficient publiclyavailable data in the appropriate domain; meanwhile synthetic crowdsourced dialogue datasets can contain unnatural patterns or behaviors that are then replicated by a model trained on them. We use our chatbot’s real-life conversations as a source Introduction Neural generative dialogue agents have become sufficiently mature to make contact with real users through programs such as the Alexa Prize (Gabriel et al., 2020). Though these models have known problems with factual correctness (Mielke et al., 2020), using dialogue history (Sankar et al., 2019), and bias (Dinan et al., 2020), they have nevertheless produced good written conversations when evaluated by crowdworkers or volunteers in carefullycontrolled scenarios (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020). 1 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 1–12 July 29–31, 2021. ©2021 Association for Computational Linguistics of natural in-domain data. In particular, we train a model that can predict authentic user dissatisfaction before it occurs, thus helping us to avoid it. eration. An open-source version o"
2021.sigdial-1.1,W18-5701,0,0.0479071,"Missing"
2021.sigdial-1.11,P90-1010,0,0.660211,"ly enjoyable. They asked us if we think it’s possible to make laws that eliminate all corruption. I don’t think we can make laws to eliminate anything totally. You can make laws against corruption Yeah Uh-huh I don’t think making laws will stop it. Chatbots serve in a broad range of roles, from taskbased assistants to social companions. However, the experience of dialogue agents differs starkly from human interaction. One key difference is how initiative, or conversational control, is shared. In human-human dialogue, participants take turns leading the conversation and introducing new topics (Walker and Whittaker, 1990), but in most humanbot dialogue the bot either leads unilaterally (social chatbots) or responds passively (task-oriented assistants). Socially, it is unpleasant and unengaging to be responsible for all topic selection and equally undesirable to have no agency. However, these one-sided experiences persist as a natural consequence of rule-based bot design, which only works well when user intents are narrowly confined. With advances in neurally generated dialogue, agents Me too! What about cats, do you like cats? Talk to me about movies. We don’t have a theater in the cloud, but I love movies. Ha"
2021.sigdial-1.11,E03-1072,0,0.223156,"Missing"
2021.sigdial-1.11,P88-1015,0,0.656575,"ee” or “Mm-hmm”. Discourse research suggests that back-channeling signals the other speaker to continue directing the conversation (Duncan, 1974). Second, using open-ended statements as prompts, because repeatedly forcing the user to respond to questions limits their agency. Third, selfdisclosure by the conversational agent, which has been shown to have a reciprocal effect on users (Lee et al., 2020), since sharing unprompted information indicates higher initiative (Cohen et al., 1999). Rethinking Initiative 2.1 Human-Human Conversation Control rules based on dialogue acts have been proposed (Whittaker and Stenton, 1988; Walker and Whittaker, 1990); however they do not account for varying degrees of initiative which are common in social conversations. Addressing this, Cohen et al. (1999) defines initiative on a spectrum. For example, a command (“Let’s talk about cats”) is stronger than a suggestion (“Maybe we should talk about cats”). We extend this idea and account for the effect 100 2.2 High Medium Low None generally prefer interrogatives over imperatives when making requests (Ervin-Tripp, 1976). Abrupt of conversational context on the degree of initiative in an utterance. For instance, the answer “I love"
2021.sigdial-1.11,2021.eacl-main.94,0,0.0342321,"Sutton, 1997; Horvitz, 1999; Allen et al., 1999; Harms et al., 2019). In this setting, initiative frameworks are based on “collaboration” around a goal, which is accomplished through a series of sub-goals. Although collaborative, social conversation has no clearly-defined objective. The closest analogue is topic, since just as task-oriented conversation breaks down into units of sub-goals, social conversation breaks down into units of topics. We therefore consider the degree of contribution to topical direction as initiative. Defining a dialogue act schema for human-bot social conversations, Yu and Yu (2021) highlight key differences from human-human dialogue acts, most notably the prevalence of user commands as a means of directing conversation. This brings to the fore the asymmetry of the human-bot social setting. Current implementations of social chatbots railroad the user and are less perceptive to implicit cues. This forces the user to use explicit commands to take initiative, which is uncommon in human-human conversations, since humans Do you like dogs? 2.3 Defining Initiative for Social Chatbots We are now ready to define initiative in the social chatbot domain. Drawing from work on human-"
2021.sigdial-1.58,W18-0802,0,0.0494959,"Missing"
2021.sigdial-1.58,W19-5942,0,0.072849,"he Alexa Prize is a competition organized by Amazon Science to advance Conversational Artificial Intelligence, allowing university teams to develop conversational bots and get feedback from real users. Related Work There’s a large body of research on physical agent abuse (Bartneck et al., 2005, 2007), particularly by children (Brˇscˇ i´c et al., 2015; Nomura et al., 2016; Tan et al., 2018; Gallego P´erez et al., 2019; Yamada et al., 2020). There has also been much work on understanding the reason behind bot abuse (Angeli and Carpenter, 2005; Angeli, 2006; Brahnam, 2006). More recently, Cercas Curry and Rieser (2019) found that “polite refusal” responses are the most appropriate compared to many other responses by commercial bots. Similarly, Chin and Yi (2019); Chin et al. (2020) further evaluated the effectiveness of empathetic and counter-attacking response strategies by measuring their impact on cultivating emotions that are known to reduce ag556 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 556–561 July 29–31, 2021. ©2021 Association for Computational Linguistics Strategy Description Example Script AVOIDANCE The bot politely avoids talking about"
2021.sigdial-1.58,2020.gebnlp-1.7,0,0.0194657,"ld also be useful to gather metadata about our participants such as age and gender (while maintaining anonymity). However, this is not allowed under Alexa Prize competition rules. 8 Ethical Concerns Despite the empirical effectiveness of the AVOIDANCE + REDIRECT strategy as detailed in this work, we would like to remind researchers of the societal dangers of adopting similar strategies. Alexa has a default female voice and the majority of offensive responses we receive are sexual in nature as stated before. As pointed out by prior work (Cercas Curry and Rieser, 2019; West et al., 2019; Cercas Curry et al., 2020), inappropriate responses further gender stereotypes and set unreasonable expectations of how women would react to verbal abuse. Without pointing out the inappropriateness of user offenses, these response strategies could cause users to believe their offenses will go unnoticed in the real world as well. Thus, we urge researchers to consider the greater impact of deploying such strategies in voice-based dialogue agents beyond the proposed effectiveness metrics. 9 re-offense ratio, length of the conversation after bot response, and number of turns until the next offensive utterance. We believe t"
2021.sigdial-1.58,W19-5935,0,0.0632157,"Missing"
C02-2025,J97-4005,0,0.0188522,"d information about the words. Well-understood statistical part-of-speech tagging technology is sufficient for this approach. In order to use more information about the parse, we might examine the entire derivation of the string. Most probabilistic parsing research – including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these pr"
C02-2025,W97-1502,0,0.122641,"nks, there is no need to define a (new) form of grammatical representation specific to the treebank. Instead, the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity. The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar. Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar). The tree selection tool presents users, who need little expert knowledge of the underlying grammar, with a range of basic properties that distinguish competing analyses and that are relatively easy to judge. All disambiguating decisions made by annotators are recorded in the [incr tsdb()] database and thus become available for (i) later dynamic extraction from the annotated profile or (ii) dynamic prop"
C02-2025,P97-1003,0,0.0128863,"t the simplest end, we might look only at the lexical type sequence assigned to the words by each parse and rank the parse based on the likelihood of that sequence. These lexical types – the preterminals in the derivation – are essentially part-of-speech tags, but encode considerably finer-grained information about the words. Well-understood statistical part-of-speech tagging technology is sufficient for this approach. In order to use more information about the parse, we might examine the entire derivation of the string. Most probabilistic parsing research – including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). The"
C02-2025,P01-1019,1,0.256862,"with an enhanced version of the grammar in an automated fashion, viz. by re-applying the disambiguating decisions on the corpus with an updated version of the grammar. Depth of Representation and Transformation of Information Internally, the [incr tsdb()] database records analyses in three different formats, viz. (i) as a derivation tree composed of identifiers of lexical items and constructions used to build the analysis, (ii) as a traditional phrase structure tree labeled with an inventory of some fifty atomic labels (of the type ‘S’, ‘NP’, ‘VP’ et al.), and (iii) as an underspecified MRS (Copestake, Lascarides, & Flickinger, 2001) meaning representation. While representation (ii) will in many cases be similar to the representation found in the Penn Treebank, representation (iii) subsumes the functor – argument (or tectogrammatical) structure advocated in the Prague Dependency Treebank or the German TiGer corpus. Most importantly, however, representation (i) provides all the information required to replay the full HPSG analysis (using the original grammar and one of the open-source HPSG processing environments, e.g., the LKB or PET, which already have been interfaced to [incr tsdb()]). Using the latter approach, users"
C02-2025,W00-1908,0,0.0559015,"Missing"
C02-2025,P99-1069,0,0.0373806,"including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Manning, 2002)). The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them. We report parse selection perfo"
C02-2025,2000.iwpt-1.19,1,0.696136,"SG framework and a generally-available broad-coverage grammar of English, the LinGO English Resource Grammar (Flickinger, 2000) as implemented with the LKB grammar development environment (Copestake, 2002). Unlike existing treebanks, there is no need to define a (new) form of grammatical representation specific to the treebank. Instead, the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity. The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar. Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar). The tree selection tool presents users, who need little expert knowledge of the underlying grammar, with a range of basic properties that distinguish competing analyses and that are relat"
C02-2025,W02-2030,1,0.780762,"ermining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Manning, 2002)). The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them. We report parse selection performance as percentage of test sentences for which the correct parse was highest ranked by the model. (We restrict attention in the test corpus to sentences that are ambiguous according to the grammar, that is, for which the parse selection task is nontrivial.) We examine four models: an HMM tagging model, a simple PCFG, a PCFG with grandparent annotation, and a hybrid model that combines predictions from the PCFG and the tagger. Thes"
C08-1066,W06-0705,0,0.0165641,"Missing"
C08-1066,W07-1423,0,0.121864,"Missing"
C08-1066,O97-1002,0,0.0225797,"ssifier to predict an entailment relation for each atomic edit based solely on features of the lexical items involved, independent of context. (For example, this model should assign the entailment relation A to the edit SUB(move, dance), regardless of whether the effective projectivity at the locus of the edit is upward monotone, downward monotone, or something else.) In the case of a SUB edit, the features include: • WordNet-derived measures of synonymy, hyponymy, and antonymy between sub524 stituends; • other features indicating semantic relatedness: the WordNet-based Jiang-Conrath measure (Jiang and Conrath, 1997) and a feature based on NomBank (Meyers et al., 2004); • string similarity features based on Levenshtein string-edit distance between lemmas; • lexical category features, indicating whether the substituends are prepositions, possessives, articles, auxiliaries, pronouns, proper nouns, operator adjectives, punctuation, etc.; • quantifier category features, which identify classes of quantifiers with similar properties; • a feature for unequal numeric expressions For DEL edits, we use only the lexical category features and a feature based on a custombuilt resource which maps implicatives and facti"
C08-1066,P03-1054,1,0.00687348,"ic analysis, (2) alignment, (3) lexical entailment classification, (4) entailment projection, and (5) entailment composition. We’ll use the following inference as a running example: (2) Jimmy Dean refused to move without blue jeans. James Dean didn’t dance without pants. The example is admittedly contrived, but it compactly exhibits containment, exclusion, and implicativity. How the NatLog system handles this example is depicted in table 1. Linguistic analysis. Relative to other NLI systems, the NatLog system does comparatively little linguistic pre-processing. We rely on the Stanford parser (Klein and Manning, 2003), a Penn Treebank-trained statistical parser, for tokenization, lemmatization, part-of-speech tagging, and phrase-structure parsing. By far the most important analysis performed at this stage, however, is projectivity marking, in which we compute the effective projectivity for each token span in each input sentence. In the premise of (2), for example, we want to determine that the effective projectivity is upward monotone 3 Factives, however, do not fit as neatly as implicatives: For example, deleting signature +/+ generates @ (Jim forgot that dancing is fun @ dancing is fun); yet under negati"
C08-1066,levy-andrew-2006-tregex,0,0.00730846,"nitions. for Jimmy Dean and refused to, downward monotone for move and without, and upward monotone for blue and jeans. Our choice of a Treebanktrained parser (driven by the goal of broad coverage) complicates this effort, because the nesting of constituents in phrase-structure parses does not always correspond to the structure of idealized semantic composition trees. Our solution is imperfect but effective. We define a list of operator types affecting projectivity (e.g., implicatives like refuse to, prepositions like without), and for each type we specify its arity and a Tregex tree pattern (Levy and Andrew, 2006) which permits us to identify its occurrences in our Treebank parses. We also specify, for each argument position of each type, both the projectivity class and another Tregex pattern which helps us to determine the sentence span over which the operator’s effect is projected. (Figure 1 shows some example definitions.) The marking process computes these projections, performs projectivity composition where needed, and marks each token span with its final effective projectivity. Alignment. Next, we establish an alignment between the premise P and hypothesis H, represented by a sequence of atomic e"
C08-1066,W07-1431,1,0.793695,"Missing"
C08-1066,N06-1006,1,0.91998,"Missing"
C08-1066,W05-1201,0,0.0298616,"m didn’t attempt to dance @ Jim didn’t dance).3 We can also account for monotonicity effects of implicative and factive operators by describing the projectivity properties of each implication signature: signatures +/–, +/◦, and ◦/– are upward monotone (attempt to tango @ attempt to dance); signatures –/+, –/◦, and ◦/+ are downward monotone (refuse to dance @ refuse to tango); and signatures +/+, –/–, and ◦/◦ are non-monotone (think dancing is fun # think tangoing is fun). 3 The NatLog system Our implementation of natural logic, the NatLog system, uses a multi-stage architecture like those of (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic analysis, (2) alignment, (3) lexical entailment classification, (4) entailment projection, and (5) entailment composition. We’ll use the following inference as a running example: (2) Jimmy Dean refused to move without blue jeans. James Dean didn’t dance without pants. The example is admittedly contrived, but it compactly exhibits containment, exclusion, and implicativity. How the NatLog system handles this example is depicted in table 1. Linguistic analysis. Relative to other NLI systems, the NatLog system does comparatively little linguist"
C08-1066,W06-3907,0,0.173094,"n a formal form by van Benthem (1986) and S´anchez Valencia (1991), who proposed a natural logic based on categorial grammar to handle inferences involving containment relations and upward and downward monotonicity, such as (1). Their monotonicity calculus explains inferences involving even nested inversions of monotonicity, but because it lacks any representation of exclusion (as opposed to containment), it cannot explain simple inferences such as (38) and (205) in table 2, below. Another model which arguably follows the natural logic tradition (though not presented as such) was developed by Nairn et al. (2006) to explain inversions and nestings of implicative (and factive) predicates, as in Ed did not forget to force Dave to leave |= Dave left. Their implication projection algorithm bears some resemblance to the monotonicity calculus, but does not incorporate containment relations or explain interactions between implicatives and monotonicity, and thus fails to license John refused to dance |= John didn’t tango. We propose a new model of natural logic which generalizes the monotonicity calculus to cover inferences involving exclusion, and (partly) unifies it with Nairn et al.’s model of implicatives"
C08-1066,E06-1052,0,0.0182071,"Missing"
C10-1045,P05-1038,0,0.0308508,"Missing"
C10-1045,E06-1047,0,0.164869,"that dominate a word with the feminine ending + taa marbuuTa (markFeminine). To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC). The intuition here is that the role of a discourse marker can usually be deStandard Parsing Experiments We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley (Petrov et al., 2006) and Bikel (Bikel, 2004) parsers. All experiments use ATB parts 1–3 divided according to the canonical split suggested by Chiang et al. (2006). Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text. At the phrasal level, we remove all function tags and traces. We also collapse unary chains with identical basic categories like NP → NP. The preterminal morphological analyses are mapped to the shortened “Bies” tags provided with the treebank. Finally, we add “DT” to the tags for definite nouns and adjectives (Kulick et al., 2006). The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics,"
C10-1045,W05-1102,0,0.0456587,"Missing"
C10-1045,D07-1022,0,0.0923751,"ntil now, all evaluations of Arabic parsing—including the experiments in the previous section—have assumed gold segmentation. But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I ∈ L, O ∈ / L, each word automaton accepts the language I ∗ (O + I)I ∗ . Aside from adding a simple rule to correct alif deletion caused by the preposition 0, no other language-specific proces"
C10-1045,P99-1065,0,0.27361,"Missing"
C10-1045,J03-4003,0,0.0744189,"Missing"
C10-1045,H05-1100,0,0.0169971,"Missing"
C10-1045,P03-1013,0,0.0092687,"Missing"
C10-1045,N09-1046,0,0.0109571,"tructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I ∈ L, O ∈ / L, each word automaton accepts the language I ∗ (O + I)I ∗ . Aside from adding a simple rule to correct alif deletion caused by the preposition 0, no other language-specific processing is performed. Our evaluation includes both weighted and unweighted lattices. We weight edges using a unigram language model estimated with GoodTuring smoothing. Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005). MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer. For each 13 Of course, this weighting makes the PCFG an improper distribution. However, in practice, unknown word models also make the distribution improper. 400 Label ADJP SBAR FRAG VP S PP NP ADVP WHNP # gold 1216 2918 254 5507 6579 7516 34025 1093 787 F1 59.45 69.81 72.87 78.83 78.91 80.93 84.95 90.64 96.00 (a"
C10-1045,P08-2053,0,0.091255,"Missing"
C10-1045,P08-1043,0,0.266799,"Missing"
C10-1045,2009.mtsummit-caasl.4,1,0.706508,"egories that are difficult to identify without semantic clues. Two common cases are the attributive adjective and the process nominal   maSdar, which can have a verbal reading.4 Attributive adjectives are hard because they are orthographically identical to nominals; they are inflected for gender, number, case, and definiteness. Moreover, they are used as substantives much 2 Unlike machine translation, constituency parsing is not significantly affected by variable word order. However, when grammatical relations like subject and object are evaluated, parsing performance drops considerably (Green et al., 2009). In particular, the decision to represent arguments in verbinitial clauses as VP internal makes VSO and VOS configurations difficult to distinguish. Topicalization of NP subjects in SVO configurations causes confusion with VO (pro-drop). 3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007). However, the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB, so vocalizing and then parsing may well not help performance. 4 Traditional Arabic linguistic theory treats both of these"
C10-1045,P05-1071,0,0.289073,"Missing"
C10-1045,N07-2014,0,0.012858,"over, they are used as substantives much 2 Unlike machine translation, constituency parsing is not significantly affected by variable word order. However, when grammatical relations like subject and object are evaluated, parsing performance drops considerably (Green et al., 2009). In particular, the decision to represent arguments in verbinitial clauses as VP internal makes VSO and VOS configurations difficult to distinguish. Topicalization of NP subjects in SVO configurations causes confusion with VO (pro-drop). 3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007). However, the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB, so vocalizing and then parsing may well not help performance. 4 Traditional Arabic linguistic theory treats both of these types as subcategories of noun . 1 2 3 4 Word Head Of Complement POS  inna “Indeed, truly”  anna “That”  in “If”  an “to” VP SBAR SBAR SBAR Noun Noun Verb Verb VBP IN IN IN Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form  an. The distinctions in the ATB ar"
C10-1045,P09-2056,0,0.156291,"(Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima’an, 2008), and the effect of variable word order (Collins et al., 1999). Certainly these linguistic factors increase the difficulty of syntactic disambiguation. Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; K¨ubler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajiˇc and Zem´anek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages. But Arabic contains a variety of linguistic phenomena unseen in English. Crucially, the conventional orthographic form"
C10-1045,N06-2013,0,0.025556,"haracters to their Latin equivalents. We retain segmentation markers—which are consistent only in the vocalized section of the treebank—to differentiate between e.g. . “they” and .+ “their.” Because we use the vocalized section, we must remove null pronoun markers. In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 9 Both the corpus split and pre-processing code are available at http://nlp.stanford.edu/projects/arabic.shtml. 10 Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. 11 taTweel (/) is an elongation character used in Arabic script to justify text. It has no syntactic function. Variants of alif are inconsistently used in Arabic texts. For alif with hamza, normalization can be seen as another level of devocalization. 12 For English, our Evalb implementation is identical to the most recent reference (EVALB20080701). For Arabic we 398 Model System Baseline Stanford (v1.6.3) GoldPOS Baseline (Self-tag) Bikel (v1.2) Berkeley (Sep. 09) Baseline"
C10-1045,D09-1087,0,0.0378054,"Missing"
C10-1045,P03-1054,1,0.0424837,"Missing"
C10-1045,P03-1056,1,0.795387,"that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Sima’an, 2008), and the effect of variable word order (Collins et al., 1999). Certainly these linguistic factors increase the difficulty of syntactic disambiguation. Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; K¨ubler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajiˇc and Zem´anek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, M"
C10-1045,W04-1602,0,0.0100223,"3; K¨ubler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajiˇc and Zem´anek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages. But Arabic contains a variety of linguistic phenomena unseen in English. Crucially, the conventional orthographic form of MSA text is unvocalized, a property that results in a deficient graphical representation. For humans, this characteristic can impede the acquisition of literacy. How do additional ambiguities caused by devocalization affect statistical learning? How should the absence of vowels and syntactic markers influence annotation choices and grammar development? Motivated by these questions, we significantly rais"
C10-1045,maamouri-etal-2008-enhancing,0,0.0914675,"Missing"
C10-1045,J93-2004,0,0.0556047,"l design (Levy and Manning, 2003; K¨ubler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (Hajiˇc and Zem´anek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply “Arabic”) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages. But Arabic contains a variety of linguistic phenomena unseen in English. Crucially, the conventional orthographic form of MSA text is unvocalized, a property that results in a deficient graphical representation. For humans, this characteristic can impede the acquisition of literacy. How do additional ambiguities caused by devocalization affect statistical learning? How should the absence of vowels and syntactic markers influence annotation choices and grammar development? Motivated by the"
C10-1045,P06-1055,0,0.217762,"For parsing, this is a mistake, especially in the case of interrogatives. splitPUNC restores the convention of the WSJ. We also mark all tags that dominate a word with the feminine ending + taa marbuuTa (markFeminine). To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC). The intuition here is that the role of a discourse marker can usually be deStandard Parsing Experiments We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley (Petrov et al., 2006) and Bikel (Bikel, 2004) parsers. All experiments use ATB parts 1–3 divided according to the canonical split suggested by Chiang et al. (2006). Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text. At the phrasal level, we remove all function tags and traces. We also collapse unary chains with identical basic categories like NP → NP. The preterminal morphological analyses are mapped to the shortened “Bies” tags provided with the treebank. Finally, we add “DT” to the tags for definite no"
C10-1045,D07-1066,0,0.0111071,"Missing"
C10-1045,roark-etal-2006-sparseval,0,0.0166028,"Missing"
C10-1045,N03-1033,1,0.0170849,"Missing"
C10-1045,C08-1112,0,0.126223,"Missing"
C10-1045,P06-3009,0,0.176282,"alternatives. Until now, all evaluations of Arabic parsing—including the experiments in the previous section—have assumed gold segmentation. But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I ∈ L, O ∈ / L, each word automaton accepts the language I ∗ (O + I)I ∗ . Aside from adding a simple rule to correct alif deletion caused by the preposition 0, no other"
C10-1045,P06-1073,0,0.0316533,"and definiteness. Moreover, they are used as substantives much 2 Unlike machine translation, constituency parsing is not significantly affected by variable word order. However, when grammatical relations like subject and object are evaluated, parsing performance drops considerably (Green et al., 2009). In particular, the decision to represent arguments in verbinitial clauses as VP internal makes VSO and VOS configurations difficult to distinguish. Topicalization of NP subjects in SVO configurations causes confusion with VO (pro-drop). 3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007). However, the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB, so vocalizing and then parsing may well not help performance. 4 Traditional Arabic linguistic theory treats both of these types as subcategories of noun . 1 2 3 4 Word Head Of Complement POS  inna “Indeed, truly”  anna “That”  in “If”  an “to” VP SBAR SBAR SBAR Noun Noun Verb Verb VBP IN IN IN Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form  an. The"
C10-1045,J04-4004,0,\N,Missing
C10-1045,2006.amta-papers.7,0,\N,Missing
C10-1131,P08-1024,0,0.0948626,"e parse tree of the other sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden structures for simple tasks such as restaurant name matching. Our work is also closely related to other recent work on learning probabilistic models involving structural latent variables (Clark and Curran, 2004; Petrov et al., 2007; Blunsom et al., 2008; Chang et al., 2010). The Tree-edit CRF model we present here is a new addition to this family of interesting models for discriminative learning with structural latent variables. 11 Conclusion We described a Tree-edit CRF model for predicting semantic relatedness of pairs of sentences. Our approach generalizes TED in a principled probabilistic model that embeds alignments as structured latent variables. We demonstrate a wide-range of lexical-semantic and syntactic features can be easily incorporated into the model. Discriminatively trained, the Tree-edit CRF led to competitive performance on"
C10-1131,N10-1066,0,0.0333508,"her sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden structures for simple tasks such as restaurant name matching. Our work is also closely related to other recent work on learning probabilistic models involving structural latent variables (Clark and Curran, 2004; Petrov et al., 2007; Blunsom et al., 2008; Chang et al., 2010). The Tree-edit CRF model we present here is a new addition to this family of interesting models for discriminative learning with structural latent variables. 11 Conclusion We described a Tree-edit CRF model for predicting semantic relatedness of pairs of sentences. Our approach generalizes TED in a principled probabilistic model that embeds alignments as structured latent variables. We demonstrate a wide-range of lexical-semantic and syntactic features can be easily incorporated into the model. Discriminatively trained, the Tree-edit CRF led to competitive performance on the task of Recognizi"
C10-1131,P04-1014,0,0.0415761,"ee of one sentence, loosely conditioned on the parse tree of the other sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden structures for simple tasks such as restaurant name matching. Our work is also closely related to other recent work on learning probabilistic models involving structural latent variables (Clark and Curran, 2004; Petrov et al., 2007; Blunsom et al., 2008; Chang et al., 2010). The Tree-edit CRF model we present here is a new addition to this family of interesting models for discriminative learning with structural latent variables. 11 Conclusion We described a Tree-edit CRF model for predicting semantic relatedness of pairs of sentences. Our approach generalizes TED in a principled probabilistic model that embeds alignments as structured latent variables. We demonstrate a wide-range of lexical-semantic and syntactic features can be easily incorporated into the model. Discriminatively trained, the Tree-"
C10-1131,P09-1053,0,0.1081,"the semantic relation between words and phrases. Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al., 2006). Studies have also shown that certain prominent syntactic features are often found beneficial (Snow et al., 2006). More recent studies gained further leverage from systematic exploration of the syntactic feature space through analysis of parse trees (Wang et al., 1164 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1164–1172, Beijing, August 2010 2007; Das and Smith, 2009). There are two key challenges imposed by these tasks. The first challenge has to do with the hidden alignment structures embedded in the sentence pairs. It is straightforward to see that in order to extract word-matching and/or syntax-matching features, inevitably one has to consider the alignment between words and/or syntactic parts. These alignments are not given as inputs, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008"
C10-1131,P05-1045,1,0.0330707,"Missing"
C10-1131,W06-1673,1,0.282736,"hen to activate what feature is desired. Traditional approaches employ a two-stage or multi-stage model where tasks are broken down into alignment finding, feature extraction, and feature learning subtasks (Haghighi et al., 2005; MacCartney et al., 2008). The alignment finding task is typically done by committing to a one best alignment, and subsequent features are extracted only according to this alignment. A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not benefit from the potential for joint learning (Finkel et al., 2006). In this paper, we present a novel undirected graphical model to address these challenges. A promising approach to these challenges is modeling the alignment as an edit operation sequence over parse tree representation, an approach pioneered by (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). We improve upon this earlier work by showing how alignment structures can be inherently learned as structured latent variables in our model. Tree edits are represented internally as state transitions in a Finite-State Machine (FSM), and our model is parameterized as"
C10-1131,H05-1049,1,0.509719,"pages 1164–1172, Beijing, August 2010 2007; Das and Smith, 2009). There are two key challenges imposed by these tasks. The first challenge has to do with the hidden alignment structures embedded in the sentence pairs. It is straightforward to see that in order to extract word-matching and/or syntax-matching features, inevitably one has to consider the alignment between words and/or syntactic parts. These alignments are not given as inputs, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008; Das and Smith, 2009, inter alia). Although alignment is a commonly used approach, it is not the only one. Other studies have successfully applied theorem proving and logical induction techniques, translating both sentences to knowledge representations and then doing inference on these representations (Moldovan et al., 2003; Raina et al., 2005; de Salvo Braz et al., 2005; MacCartney and Manning, 2007, inter alia). A second challenge arises when a system needs to combine various sources of evidence (i.e., surface text features, semantic features, and"
C10-1131,W07-1423,0,0.347059,"ne best alignment, and subsequent features are extracted only according to this alignment. A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not benefit from the potential for joint learning (Finkel et al., 2006). In this paper, we present a novel undirected graphical model to address these challenges. A promising approach to these challenges is modeling the alignment as an edit operation sequence over parse tree representation, an approach pioneered by (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). We improve upon this earlier work by showing how alignment structures can be inherently learned as structured latent variables in our model. Tree edits are represented internally as state transitions in a Finite-State Machine (FSM), and our model is parameterized as a Conditional Random Field (CRF) (Lafferty et al., 2001), which allows us to incorporate a diverse set of arbitrarily overlapping features. In comparison to previous work that exploits various ad-hoc or heuristic ways of incorporating tree-edit operations, our model provides an elegant and much more principled way"
C10-1131,N10-1145,0,0.364401,"itive if the answer candidate sentence correctly answers the question and provides sufficient con1169 System Punyakanok et al., 2004 Cui et al., 2005 Wang et al., 2007 H&S, 2010 Tree-edit CRF MAP 0.4189 0.4350 0.6029 0.6091 0.5951 MRR 0.4939 0.5569 0.6852 0.6917 0.6951 9 Table 3: Results on QA task reported in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). textual support (i.e., does not merely contain the answer key, for example, ”Ingemar Johansson was a world heavyweight champion” would not be a correct answer). We followed the same experimental setup as Wang et al. (2007) and Heilman and Smith (2010). The training portion of the dataset consists of 5919 manually judged Q/A pairs from previous QA tracks at Text REtrieval Conference (TREC 8–12). There are also 1374 Q/A pairs for development and 1866 Q/A pairs for testing, both from the TREC 3 evaluation. The task is framed as a sentence retrieval task, and thus Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported for the ranked list of most probable answer candidates. We compare out model with four other systems. Wang et al. (2007) proposed a Quasi-synchronous Grammar formulation of the problem which also models alignmen"
C10-1131,W07-1431,1,0.709657,"judged positive if the answer candidate sentence correctly answers the question and provides sufficient con1169 System Punyakanok et al., 2004 Cui et al., 2005 Wang et al., 2007 H&S, 2010 Tree-edit CRF MAP 0.4189 0.4350 0.6029 0.6091 0.5951 MRR 0.4939 0.5569 0.6852 0.6917 0.6951 9 Table 3: Results on QA task reported in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). textual support (i.e., does not merely contain the answer key, for example, ”Ingemar Johansson was a world heavyweight champion” would not be a correct answer). We followed the same experimental setup as Wang et al. (2007) and Heilman and Smith (2010). The training portion of the dataset consists of 5919 manually judged Q/A pairs from previous QA tracks at Text REtrieval Conference (TREC 8–12). There are also 1374 Q/A pairs for development and 1866 Q/A pairs for testing, both from the TREC 3 evaluation. The task is framed as a sentence retrieval task, and thus Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) are reported for the ranked list of most probable answer candidates. We compare out model with four other systems. Wang et al. (2007) proposed a Quasi-synchronous Grammar formulation of the probl"
C10-1131,C08-1066,1,0.797134,"Missing"
C10-1131,N06-1006,1,0.429403,"Missing"
C10-1131,D08-1084,1,0.828911,"7; Das and Smith, 2009). There are two key challenges imposed by these tasks. The first challenge has to do with the hidden alignment structures embedded in the sentence pairs. It is straightforward to see that in order to extract word-matching and/or syntax-matching features, inevitably one has to consider the alignment between words and/or syntactic parts. These alignments are not given as inputs, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008; Das and Smith, 2009, inter alia). Although alignment is a commonly used approach, it is not the only one. Other studies have successfully applied theorem proving and logical induction techniques, translating both sentences to knowledge representations and then doing inference on these representations (Moldovan et al., 2003; Raina et al., 2005; de Salvo Braz et al., 2005; MacCartney and Manning, 2007, inter alia). A second challenge arises when a system needs to combine various sources of evidence (i.e., surface text features, semantic features, and syntactic features) to make a global classi"
C10-1131,W07-1414,0,0.0312169,"Missing"
C10-1131,P05-1012,0,0.00992581,"labels also match 4. (2.) and labels mismatch 5. (4.) and detailing the mismatching labels 6. parent+label match, child mismatch 7. child and label match, parents are {hyper/syno/anto}nym 8. looking for specific SUB/OBJ/PRD construct as in Snow et al. (2006). 6 Preprocessing In all of our experiments, each input pair of text and hypothesis sentence is preprocessed as following: Sentences were first tokenized by the standard Penn TreeBank tokenization script, and then we used MXPOST tagger (Ratnaparkhi, 1996) for part-of-speech (POS) tagging. POS tagged sentences were then parsed by MSTParser (McDonald et al., 2005) to produce labeled dependency parse trees. The parser was trained 1168 RTE2 Vanderwende et al., 2006 K&M, 2006 Nielsen et al., 2006 Zanzotto et al., 2006 Tree-edit CRF RTE3 Marsi et al., 2007 Harmeling, 2007 de Marneffe et al., 2006 Tree-edit CRF on the entire Penn TreeBank. The last step in the pipeline is named-entity tagging using Stanford NER Tagger (Finkel et al., 2005). 7 RTE Experiments Given an input text sentence and a hypothesis sentence, the task of RTE is to make predictions about whether or not the hypothesis can be entailed from the text sentence. We use standard evaluation data"
C10-1131,P09-2073,0,0.108031,", and subsequent features are extracted only according to this alignment. A large body of literature in joint learning has demonstrated that such an approach can suffer from cascaded errors at testing, and does not benefit from the potential for joint learning (Finkel et al., 2006). In this paper, we present a novel undirected graphical model to address these challenges. A promising approach to these challenges is modeling the alignment as an edit operation sequence over parse tree representation, an approach pioneered by (Punyakanok et al., 2004; Kouylekov and Magnini, 2006; Harmeling, 2007; Mehdad, 2009). We improve upon this earlier work by showing how alignment structures can be inherently learned as structured latent variables in our model. Tree edits are represented internally as state transitions in a Finite-State Machine (FSM), and our model is parameterized as a Conditional Random Field (CRF) (Lafferty et al., 2001), which allows us to incorporate a diverse set of arbitrarily overlapping features. In comparison to previous work that exploits various ad-hoc or heuristic ways of incorporating tree-edit operations, our model provides an elegant and much more principled way of describing t"
C10-1131,N03-1022,0,0.0228925,"en words and/or syntactic parts. These alignments are not given as inputs, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008; Das and Smith, 2009, inter alia). Although alignment is a commonly used approach, it is not the only one. Other studies have successfully applied theorem proving and logical induction techniques, translating both sentences to knowledge representations and then doing inference on these representations (Moldovan et al., 2003; Raina et al., 2005; de Salvo Braz et al., 2005; MacCartney and Manning, 2007, inter alia). A second challenge arises when a system needs to combine various sources of evidence (i.e., surface text features, semantic features, and syntactic features) to make a global classification decision. Quite often these features are heavily overlapping and sometimes contradicting, and thus a robust learning scheme that knows when to activate what feature is desired. Traditional approaches employ a two-stage or multi-stage model where tasks are broken down into alignment finding, feature extraction, and f"
C10-1131,P02-1040,0,0.0890963,"guage Processing (NLP) applications can be broken down to a subtask of evaluating the semantic relationship of pairs of sentences (e.g., in Question Answering, answer selection involve comparing each answer candidate against the question). This means that research aiming at analyzing pairs of semantically related natural language sentences is promising because of its reusability: it is not tied to a particular internal representation of meanings, Earlier studies in these domains have concluded that simple word overlap measures (e.g., bag of words, n-grams) have a surprising degree of utility (Papineni et al., 2002; Jijkoun and de Rijke, 2005b), but are nevertheless not sufficient for these tasks (Jijkoun and de Rijke, 2005a). A common problem identified in these earlier systems is the lack of understanding of the semantic relation between words and phrases. Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al., 2006). Studies have also shown that certain prominent syntactic features are often found beneficial (Snow et al., 2006). More recent studies gained further leverage from systematic exploration of the syntactic fe"
C10-1131,W06-3104,0,0.0151407,"but unfortunately experiments in the paper were limited to digit recognition and tasks on small artificial datasets. Many different approaches to modeling sentence alignment have been proposed before (Haghighi et al., 2005; MacCartney et al., 2008). Haghighi et al. (2005) treated alignment finding in RTE as a graph matching problem 1170 between sentence parse trees. MacCartney et al. (2008) described a phrase-based alignment model for MT, trained by the Perceptron learning algorithm. A line of work that offers similar treatment of alignment to our model is the Quasi-synchronous Grammar (QG) (Smith and Eisner, 2006; Wang et al., 2007; Das and Smith, 2009). QG models alignments between two parse trees as structured latent variables. The generative story of QG describes one that builds the parse tree of one sentence, loosely conditioned on the parse tree of the other sentence. This formalism prefers but is not confined to tree isomorphism, therefore possesses more model flexibility than synchronous grammars. The work of McCallum et al. (2005) inspired the discriminative training framework that we used in our experiments. They presented a String Edit Distance model that also learns alignments as hidden str"
C10-1131,N06-1005,0,0.0452768,"word overlap measures (e.g., bag of words, n-grams) have a surprising degree of utility (Papineni et al., 2002; Jijkoun and de Rijke, 2005b), but are nevertheless not sufficient for these tasks (Jijkoun and de Rijke, 2005a). A common problem identified in these earlier systems is the lack of understanding of the semantic relation between words and phrases. Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al., 2006). Studies have also shown that certain prominent syntactic features are often found beneficial (Snow et al., 2006). More recent studies gained further leverage from systematic exploration of the syntactic feature space through analysis of parse trees (Wang et al., 1164 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1164–1172, Beijing, August 2010 2007; Das and Smith, 2009). There are two key challenges imposed by these tasks. The first challenge has to do with the hidden alignment structures embedded in the sentence pairs. It is straightforward to see that in order to extract word-matching and/or syntax-matching features, inevitably one has to consider t"
C10-1131,D07-1003,1,0.722641,"ng, August 2010 2007; Das and Smith, 2009). There are two key challenges imposed by these tasks. The first challenge has to do with the hidden alignment structures embedded in the sentence pairs. It is straightforward to see that in order to extract word-matching and/or syntax-matching features, inevitably one has to consider the alignment between words and/or syntactic parts. These alignments are not given as inputs, and it is a non-trivial task to decide what the correct alignment is. Alignment-based approach have been proven effective by many RTE, QA and MTE systems (Haghighi et al., 2005; Wang et al., 2007; MacCartney et al., 2008; Das and Smith, 2009, inter alia). Although alignment is a commonly used approach, it is not the only one. Other studies have successfully applied theorem proving and logical induction techniques, translating both sentences to knowledge representations and then doing inference on these representations (Moldovan et al., 2003; Raina et al., 2005; de Salvo Braz et al., 2005; MacCartney and Manning, 2007, inter alia). A second challenge arises when a system needs to combine various sources of evidence (i.e., surface text features, semantic features, and syntactic features"
C10-1131,W96-0213,0,\N,Missing
C10-1131,W07-1401,0,\N,Missing
chang-manning-2012-sutime,C08-3012,0,\N,Missing
chang-manning-2012-sutime,S10-1071,0,\N,Missing
chang-manning-2012-sutime,P00-1010,0,\N,Missing
chang-manning-2012-sutime,S10-1010,0,\N,Missing
chang-manning-2012-sutime,P05-1045,1,\N,Missing
D08-1038,A00-2035,0,0.0330014,"Missing"
D08-1038,W93-0231,0,0.289836,"Missing"
D08-1084,W06-3123,0,0.0103284,"heuristics using wordaligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14 For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. 810 lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebased systems that jointly align and weight phrases, though these systems have not come close to the state of the art when evaluated in terms of MT performance. We would argue that previous work in MT phrase alignment is orthogonal to our work. In MANLI, the need for phrases arises when word-based representations are not appropriate for alignment (e.g., between close down and terminate), though longer phrases are not needed to achieve good alignment quality. In MT phrase alignment, it is beneficial to account for arbitrarily large phr"
D08-1084,J93-2003,0,0.00979229,"ods in Natural Language Processing, pages 802–811, c Honolulu, October 2008. 2008 Association for Computational Linguistics 3 Data Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment anno803 o ar me e n po o re rly pr in ese nt ed pa rli a . m en t W ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clau"
D08-1084,W07-1427,1,0.825638,"Missing"
D08-1084,P08-2007,0,0.0210802,"daligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14 For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. 810 lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebased systems that jointly align and weight phrases, though these systems have not come close to the state of the art when evaluated in terms of MT performance. We would argue that previous work in MT phrase alignment is orthogonal to our work. In MANLI, the need for phrases arises when word-based representations are not appropriate for alignment (e.g., between close down and terminate), though longer phrases are not needed to achieve good alignment quality. In MT phrase alignment, it is beneficial to account for arbitrarily large phrases, since the larger co"
D08-1084,W06-3105,0,0.0259711,"October 2008. 2008 Association for Computational Linguistics 3 Data Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment anno803 o ar me e n po o re rly pr in ese nt ed pa rli a . m en t W ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no counterpart in H. 3. Indeed, one cannot assume"
D08-1084,J07-3002,0,0.0176829,"Missing"
D08-1084,W07-1428,0,0.0289183,"m et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness. In such approaches, the correspondence between the words of P and H is implicit in the steps of the proof. Increasingly, however, the most successful RTE systems have made the alignment problem explicit. Marsi and Krahmer (2005) and MacCartney et al. (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al. (2006) and Hickl and Bensley (2007). However, each of these systems has pursued alignment in idiosyncratic and poorly-documented ways, often using proprietary data, making comparisons and further development difficult. In this paper we undertake the first systematic study of alignment for NLI. We propose a new NLI alignment system which uses a phrase-based representation of alignment, exploits external resources for knowledge of semantic relatedness, and capitalizes on the recent appearance of new supervised training data for NLI alignment. In addition, we examine the relation between NLI alignment and MT alignment, and investi"
D08-1084,O97-1002,0,0.00458023,"involved in the edit, and whether these phrases are non-constituents (in syntactic parses of the sentences involved). Lexical similarity feature. For SUB edits, a very important feature represents the lexical similarity of the substituends, as a real value in [0, 1]. This similarity score is computed as a max over a number of component scoring functions, some based on external lexical resources, including: • various string similarity functions, of which most are applied to word lemmas • measures of synonymy, hypernymy, antonymy, and semantic relatedness, including a widelyused measure due to Jiang and Conrath (1997), based on manually constructed lexical resources such as WordNet and NomBank • a function based on the well-known distributional similarity metric of Lin (1998), which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources— both manually and automatically constructed—is critical to the success of MANLI. Contextual features. Even when the lexical similarity for a SUB edit is high, it may not be a good match. If P or H contains multiple occurrences of the same word—which happens freq"
D08-1084,P07-2045,0,0.00293131,"performance of the LCC system, all achieve respectable results, and the Stanford and MANLI aligners outperform the average RTE2 entry. Thus, even if alignment quality does not determine inferential validity, many NLI systems could be improved by harnessing a well-designed NLI aligner. 7 Related work Given the extensive literature on phrase-based MT, it may be helpful further to situate our phrase-based alignment model in relation to past work. The standard approach to training a phrase-based MT system is to apply phrase extraction heuristics using wordaligned training sets (Och and Ney, 2003; Koehn et al., 2007). Unfortunately, word alignment models assume that source words are individually trans14 For good results, it may be necessary to normalize the alignment score. Scores from MANLI were normalized by the number of tokens in the problem. The Stanford aligner performs a similar normalization internally. 810 lated into target words, which stands at odds with the key assumption in phrase-based systems that many translations are non-compositional. More recently, several works (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006; DeNero and Klein, 2008) have presented more unified phrasebase"
D08-1084,N06-1014,0,0.11006,"Missing"
D08-1084,P98-2127,0,0.0185433,"ant feature represents the lexical similarity of the substituends, as a real value in [0, 1]. This similarity score is computed as a max over a number of component scoring functions, some based on external lexical resources, including: • various string similarity functions, of which most are applied to word lemmas • measures of synonymy, hypernymy, antonymy, and semantic relatedness, including a widelyused measure due to Jiang and Conrath (1997), based on manually constructed lexical resources such as WordNet and NomBank • a function based on the well-known distributional similarity metric of Lin (1998), which automatically infers similarity of words and phrases from their distributions in a very large corpus of English text The ability to leverage external lexical resources— both manually and automatically constructed—is critical to the success of MANLI. Contextual features. Even when the lexical similarity for a SUB edit is high, it may not be a good match. If P or H contains multiple occurrences of the same word—which happens frequently with function words, and occasionally with content words—lexical similarity may not suffice to determine the right match. To remedy this, we introduce con"
D08-1084,N06-1006,1,0.811831,"Missing"
D08-1084,W02-1018,0,0.041262,"s 802–811, c Honolulu, October 2008. 2008 Association for Computational Linguistics 3 Data Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment anno803 o ar me e n po o re rly pr in ese nt ed pa rli a . m en t W ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no counterpart in H. 3. Ind"
D08-1084,W05-1201,0,0.0773166,"n bags of words. While ignoring structure, such methods depend on matching each word in H to the word in P with which it is most similar—in effect, an alignment. At the other extreme, Tatu and Moldovan (2007) and Bar-Haim et al. (2007) have formulated the inference problem as analogous to proof search, using inferential rules which encode (among other things) knowledge of lexical relatedness. In such approaches, the correspondence between the words of P and H is implicit in the steps of the proof. Increasingly, however, the most successful RTE systems have made the alignment problem explicit. Marsi and Krahmer (2005) and MacCartney et al. (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al. (2006) and Hickl and Bensley (2007). However, each of these systems has pursued alignment in idiosyncratic and poorly-documented ways, often using proprietary data, making comparisons and further development difficult. In this paper we undertake the first systematic study of alignment for NLI. We propose a new NLI alignment system which uses a phrase-based representation of alignment, exploits external resources"
D08-1084,J03-1002,0,0.0704504,"tly, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment anno803 o ar me e n po o re rly pr in ese nt ed pa rli a . m en t W ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no counterpart in H. 3. Indeed, one cannot assume even approximate semantic equivalence—usually a given in MT. Because NLI prob"
D08-1084,C96-2141,0,0.253093,"age Processing, pages 802–811, c Honolulu, October 2008. 2008 Association for Computational Linguistics 3 Data Until recently, research on alignment for NLI has been hampered by a paucity of high-quality, publicly available data from which to learn. Happily, that has begun to change, with the release by Microsoft Research (MSR) of human-generated alignment anno803 o ar me e n po o re rly pr in ese nt ed pa rli a . m en t W ing a correspondence between she and elle, and between came and est venue. The MT community has developed not only an extensive literature on alignment (Brown et al., 1993; Vogel et al., 1996; Marcu and Wong, 2002; DeNero et al., 2006), but also standard, proven alignment tools such as GIZA++ (Och and Ney, 2003). Can off-the-shelf MT aligners be applied to NLI? There is reason to be doubtful. Alignment for NLI differs from alignment for MT in several important respects, including: 1. Most obviously, it is monolingual rather than cross-lingual, opening the door to utilizing abundant (monolingual) sources of information on semantic relatedness, such as WordNet. 2. It is intrinsically asymmetric: P is often much longer than H, and commonly contains phrases or clauses which have no co"
D08-1084,C04-1051,0,\N,Missing
D08-1084,W02-1001,0,\N,Missing
D08-1084,W07-1404,0,\N,Missing
D08-1084,2006.amta-papers.2,0,\N,Missing
D08-1084,C98-2122,0,\N,Missing
D08-1089,koen-2004-pharaoh,0,\N,Missing
D08-1089,N04-4026,0,\N,Missing
D08-1089,P06-1067,0,\N,Missing
D08-1089,J09-4009,0,\N,Missing
D08-1089,P06-1066,0,\N,Missing
D08-1089,J04-4002,0,\N,Missing
D08-1089,P07-2045,0,\N,Missing
D08-1089,P06-1098,0,\N,Missing
D08-1089,P05-1033,0,\N,Missing
D08-1089,N03-1017,0,\N,Missing
D08-1089,W06-3108,0,\N,Missing
D08-1089,J97-3002,0,\N,Missing
D08-1089,W05-0908,0,\N,Missing
D08-1089,W04-3250,0,\N,Missing
D08-1089,N04-1021,0,\N,Missing
D08-1089,N03-1021,0,\N,Missing
D08-1089,D08-1076,0,\N,Missing
D09-1015,P08-1109,1,0.86679,"N NN VBD DT DNA PROT NN NNS NN IN DT . DNA NN PROT NN NN PEBP2 alpha A1 , alpha B1 , and alpha B2 proteins bound the PEBP2 site within the mouse GM-CSF promoter . Figure 1: An example of our tree representation over nested named entities. The sentence is from the GENIA corpus. PROT is short for PROTEIN. but they tend to be much flatter. This model allows us to include parts of speech in the tree, and therefore to jointly model the named entities and the part of speech tags. Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkel et al., 2008). We found that on top-level entities, our model does just as well as more conventional methods. When evaluating on all entities our model does well, with F-scores ranging from slightly worse than performance on top-level only, to substantially better than top-level only. entities, etc. For outside-in layering the first CRF would identify outermost entities, and then successive CRFs would identify increasingly nested entities. They also tried a cascaded approach, with separate CRFs for each entity type. The CRFs would be applied in a specified order, and then each CRF could utilize features de"
D09-1015,W02-0301,0,0.00939186,"nique has the problem that it cannot identify nested entities of the same type; this happens frequently in the data, such as the nested proteins at the beginning of the sentence in Figure 1. They also tried a joint labeling approach, where they trained a single CRF, but the label set was significantly expanded so that a single label would include all of the entities for a particular word. Their best results where from the cascaded approach. 2 Related Work There is a large body of work on named entity recognition, but very little of it addresses nested entities. Early work on the GENIA corpus (Kazama et al., 2002; Tsuruoka and Tsujii, 2003) only worked on the innermost entities. This was soon followed by several attempts at nested NER in GENIA (Shen et al., 2003; Zhang et al., 2004; Zhou et al., 2004) which built hidden Markov models over the innermost named entities, and then used a rule-based post-processing step to identify the named entities containing the innermost entities. Zhou (2006) used a more elaborate model for the innermost entities, but then used the same rule-based post-processing method on the output to identify non-innermost entities. Gu (2006) focused only on proteins and DNA, by bui"
D09-1015,J93-2004,0,0.0337966,"del nested entities. In this paper we present a novel solution to the problem of nested named entity recognition. Our model explicitly represents the nested structure, allowing entities to be influenced not just by the labels of the words surrounding them, as in a CRF, but also by the entities contained in them, and in which they are contained. We represent each sentence as a parse tree, with the words as leaves, and with phrases corresponding to each entity (and a node which joins the entire sentence). Our trees look just like syntactic constituency trees, such as those in the Penn TreeBank (Marcus et al., 1993), 141 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 141–150, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP ROOT PROT PROT PROT NN NN NN , PROT NN NN , CC PROT NN NN VBD DT DNA PROT NN NNS NN IN DT . DNA NN PROT NN NN PEBP2 alpha A1 , alpha B1 , and alpha B2 proteins bound the PEBP2 site within the mouse GM-CSF promoter . Figure 1: An example of our tree representation over nested named entities. The sentence is from the GENIA corpus. PROT is short for PROTEIN. but they tend to be much flatter. This model allows us to include parts of speech in"
D09-1015,S07-1095,0,0.0329219,"Missing"
D09-1015,W07-1009,0,0.595182,"then used the same rule-based post-processing method on the output to identify non-innermost entities. Gu (2006) focused only on proteins and DNA, by building separate binary SVM classifiers for innermost and outermost entities for those two classes. Several techniques for nested NER in GENIA where presented in (Alex et al., 2007). Their first approach was to layer CRFs, using the output of one as the input to the next. For inside-out layering, the first CRF would identify the innermost entities, the next layer would be over the words and the innermost entities to identify second-level Byrne (2007) took a different approach, on historical archive text. She modified the data by concatenating adjacent tokens (up to length six) into potential entities, and then labeled each concatenated string using the C&C tagger (Curran and Clark, 1999). When labeling a string, the “previous” string was the one-token-shorter string containing all but the last token of the current string. For single tokens the “previous” token was the longest concatenation starting one token earlier. SemEval 2007 Task 9 (M´arquez et al., 2007b) included a nested NER component, as well as noun sense disambiguation and sema"
D09-1015,W06-1655,0,0.00758836,"re 2: An example of a subtree after it has been annotated and binarized. Features are computed over this representation. An @ indicates a chart parser active state (incomplete constituent). parse tree which, based on their labels, could potentially be entities. Finally, McDonald et al. (2005) presented a technique for labeling potentially overlapping segments of text, based on a large margin, multilabel classification algorithm. Their method could be used for nested named entity recognition, but the experiments they performed were on joint (flat) NER and noun phrase chunking. and Cohen, 2004; Andrew, 2006), but with no length restriction on entities. Like a semi-CRF, we are able to define features over entire entities of arbitrary length, instead of just over a small, fixed window of words like a regular linear chain CRF. We model part of speech tags jointly with the named entities, though the model also works without them. We determine the possible part of speech tags based on distributional similarity clusters. We used Alexander Clarke’s software, 1 based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other"
D09-1015,S07-1008,0,0.0369487,"Missing"
D09-1015,H05-1124,0,0.00921591,"´arquez et al., 2007a) worked on the named entity component. They used a multilabel AdaBoost.MH algorithm, over phrases in the 142 DNAparent=ROOT @DNAparent=ROOT,prev=NN,first=PROT NNparent=DNA,grandparent=ROOT PROTparent=DNA,grandparent=ROOT NNparent=DNA,grandparent=ROOT NNparent=PROT,grandparent=DNA mouse GM-CSF promoter Figure 2: An example of a subtree after it has been annotated and binarized. Features are computed over this representation. An @ indicates a chart parser active state (incomplete constituent). parse tree which, based on their labels, could potentially be entities. Finally, McDonald et al. (2005) presented a technique for labeling potentially overlapping segments of text, based on a large margin, multilabel classification algorithm. Their method could be used for nested named entity recognition, but the experiments they performed were on joint (flat) NER and noun phrase chunking. and Cohen, 2004; Andrew, 2006), but with no length restriction on entities. Like a semi-CRF, we are able to define features over entire entities of arbitrary length, instead of just over a small, fixed window of words like a regular linear chain CRF. We model part of speech tags jointly with the named entitie"
D09-1015,E03-1009,0,0.00935171,"ntity recognition, but the experiments they performed were on joint (flat) NER and noun phrase chunking. and Cohen, 2004; Andrew, 2006), but with no length restriction on entities. Like a semi-CRF, we are able to define features over entire entities of arbitrary length, instead of just over a small, fixed window of words like a regular linear chain CRF. We model part of speech tags jointly with the named entities, though the model also works without them. We determine the possible part of speech tags based on distributional similarity clusters. We used Alexander Clarke’s software, 1 based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. Because the parts of speech are annotated with the parent (and grandparent) labels, they determine what, if any, entity types a word can be labeled with. Many words, such as verbs, cannot be labeled with any entities. We also limit our grammar based on the rules observed in the data. The rules whose children include part of speech tags restrict the possible pairs of adjacent tags. Interestingly, the restrictions imposed by this joint modeling (bot"
D09-1015,W03-0419,0,0.156031,"Missing"
D09-1015,W03-1307,0,0.0551114,"ginning of the sentence in Figure 1. They also tried a joint labeling approach, where they trained a single CRF, but the label set was significantly expanded so that a single label would include all of the entities for a particular word. Their best results where from the cascaded approach. 2 Related Work There is a large body of work on named entity recognition, but very little of it addresses nested entities. Early work on the GENIA corpus (Kazama et al., 2002; Tsuruoka and Tsujii, 2003) only worked on the innermost entities. This was soon followed by several attempts at nested NER in GENIA (Shen et al., 2003; Zhang et al., 2004; Zhou et al., 2004) which built hidden Markov models over the innermost named entities, and then used a rule-based post-processing step to identify the named entities containing the innermost entities. Zhou (2006) used a more elaborate model for the innermost entities, but then used the same rule-based post-processing method on the output to identify non-innermost entities. Gu (2006) focused only on proteins and DNA, by building separate binary SVM classifiers for innermost and outermost entities for those two classes. Several techniques for nested NER in GENIA where prese"
D09-1015,W03-1306,0,0.00710505,"that it cannot identify nested entities of the same type; this happens frequently in the data, such as the nested proteins at the beginning of the sentence in Figure 1. They also tried a joint labeling approach, where they trained a single CRF, but the label set was significantly expanded so that a single label would include all of the entities for a particular word. Their best results where from the cascaded approach. 2 Related Work There is a large body of work on named entity recognition, but very little of it addresses nested entities. Early work on the GENIA corpus (Kazama et al., 2002; Tsuruoka and Tsujii, 2003) only worked on the innermost entities. This was soon followed by several attempts at nested NER in GENIA (Shen et al., 2003; Zhang et al., 2004; Zhou et al., 2004) which built hidden Markov models over the innermost named entities, and then used a rule-based post-processing step to identify the named entities containing the innermost entities. Zhou (2006) used a more elaborate model for the innermost entities, but then used the same rule-based post-processing method on the output to identify non-innermost entities. Gu (2006) focused only on proteins and DNA, by building separate binary SVM cl"
D09-1015,W04-1219,0,0.030191,"l # Test Entities 4944 1030 115 487 1858 8434 Nested NER Model (train on all entities) Precision Recall F1 66.98 74.58 70.57 62.96 66.50 64.68 63.06 60.87 61.95 49.92 60.78 54.81 75.12 65.34 69.89 66.78 70.57 68.62 Semi-CRF Model (train on top-level entities) Precision Recall F1 68.15 62.68 65.30 65.45 52.23 58.10 64.55 61.74 63.11 49.61 52.16 50.85 73.29 55.81 63.37 67.50 59.27 63.12 Zhou & Su (2004) Precision 69.01 66.84 64.66 53.85 78.06 69.42 Recall 79.24 73.11 63.56 65.80 72.41 75.99 F1 73.77 69.83 64.10 59.23 75.13 72.55 Table 4: Named entity results on the JNLPBA 2004 shared task data. Zhou and Su (2004) was the best system at the shared task, and is still state-of-the-art on the dataset. ROOT SP AQ NC FC ORGANIZATION VS DA AQ FE FC VM DA ORGANIZATION PERSON NP FP PERSON NP PERSON FC NC SP ORGANIZATION NP A doble partido , el Barc¸a es el favorito ” , afirma Makaay , delantero del Deportivo . At double match , the Barc¸a is the favorite ” , states Makaay , Deportivo . attacker of Figure 3: An example sentence from the AnCora corpus, along with its English translation. ence with no improvement in performance. on both top-level entities and all entities. While not our main focus, we also evalua"
D09-1015,W03-0424,0,\N,Missing
D09-1015,W06-3318,0,\N,Missing
D10-1048,D08-1031,0,0.846856,"s. 1 The second attack occurred after some rocket firings aimed, apparently, toward [the israelis], apparently in retaliation. [we]’re checking our facts on that one. ... the president, quoted by ari fleischer, his spokesman, is saying he’s concerned the strike will undermine efforts by palestinian authorities to bring an end to terrorist attacks and does not contribute to the security of [israel]. Introduction Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010). When such a rich representation Most state-of-the-art models will incorrectly link we to the israelis because of their proximity and compatibility of attributes (both we and the israelis are plural). In contrast, a more cautious approach is to first cluster the israelis with israel because the demonymy relation is highly precise. This initial clustering step will assign the correct animacy attribute (inanimate) to the corresponding geo-political entity, which will prevent the incorrect merging with the mention we (animate) in later steps."
D10-1048,P06-1005,0,0.370589,"ttributes. These are crucial factors for pronominal coreference. Like previous work, we implement pronominal coreference resolution by enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). Gender – we assign gender attributes from static lexicons from (Bergsma and Lin, 2006; Ji and Lin, 2009). Person – we assign person attributes only to pronouns. However, we do not enforce this constraint when linking two pronouns if one appears within quotes. This is a simple heuristic for speaker detection, e.g., I and she point to the same person in “[I] voted my conscience,” [she] said. Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web (Ji and Lin, 2009)."
D10-1048,J93-2003,0,0.0155393,"ls entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inference models proposed in these works. From a high level perspective, this work falls under the theory of shaping, defined as a “method of successive approximations” for learning (Skinner, 1938). This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, etc. To the best of our knowledge, we are the first to apply this theory to coreference resolution. 3 Description of the Task Intra-document coreference resolution clusters together textual mentions within a single document based on the underlying referent entity. Mentions are usually noun phrases (NPs) headed by nominal or pronominal terminals. To f"
D10-1048,W99-0613,0,0.0599343,"Missing"
D10-1048,N07-1011,0,0.106871,"in both supervised and unsupervised learning setups (Bengston and Roth, 2008; Haghighi and Klein, 2009). Our work reinforces this observation, and extends it by proposing a novel architecture that: (a) allows easy deployment of such features, and (b) infuses global information that can be readily exploited by these features or constraints. Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (Bengston and Roth, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Stoyanov, 2010). Two recent works that diverge from this pattern are Culotta et al. (2007) and Poon and 493 Domingos (2008). They perform coreference resolution jointly for all mentions in a document, using first-order probabilistic models in either supervised or unsupervised settings. Haghighi and Klein (2010) propose a generative approach that models entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inf"
D10-1048,P10-2007,0,0.0457833,"Missing"
D10-1048,P05-1045,1,0.02431,"onference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005"
D10-1048,P08-2012,1,0.850737,"rk This work builds upon the recent observation that strong features outweigh complex models for coreference resolution, in both supervised and unsupervised learning setups (Bengston and Roth, 2008; Haghighi and Klein, 2009). Our work reinforces this observation, and extends it by proposing a novel architecture that: (a) allows easy deployment of such features, and (b) infuses global information that can be readily exploited by these features or constraints. Most coreference resolution approaches perform the task by aggregating local decisions about pairs of mentions (Bengston and Roth, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Stoyanov, 2010). Two recent works that diverge from this pattern are Culotta et al. (2007) and Poon and 493 Domingos (2008). They perform coreference resolution jointly for all mentions in a document, using first-order probabilistic models in either supervised or unsupervised settings. Haghighi and Klein (2010) propose a generative approach that models entity clusters explicitly using a mostly-unsupervised generative model. As previously mentioned, our work is not constrained by first-order or Bayesian formalisms in how it uses cluster information. Additionally, the"
D10-1048,D09-1120,0,0.142423,"curred after some rocket firings aimed, apparently, toward [the israelis], apparently in retaliation. [we]’re checking our facts on that one. ... the president, quoted by ari fleischer, his spokesman, is saying he’s concerned the strike will undermine efforts by palestinian authorities to bring an end to terrorist attacks and does not contribute to the security of [israel]. Introduction Recent work on coreference resolution has shown that a rich feature space that models lexical, syntactic, semantic, and discourse phenomena is crucial to successfully address the task (Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010). When such a rich representation Most state-of-the-art models will incorrectly link we to the israelis because of their proximity and compatibility of attributes (both we and the israelis are plural). In contrast, a more cautious approach is to first cluster the israelis with israel because the demonymy relation is highly precise. This initial clustering step will assign the correct animacy attribute (inanimate) to the corresponding geo-political entity, which will prevent the incorrect merging with the mention we (animate) in later steps. We propose an unsupervised"
D10-1048,N10-1061,0,0.468981,"Missing"
D10-1048,Y09-1024,0,0.0248102,"reference resolution by enforcing agreement constraints between the coreferent mentions. We use the following attributes for these constraints: Number – we assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). Gender – we assign gender attributes from static lexicons from (Bergsma and Lin, 2006; Ji and Lin, 2009). Person – we assign person attributes only to pronouns. However, we do not enforce this constraint when linking two pronouns if one appears within quotes. This is a simple heuristic for speaker detection, e.g., I and she point to the same person in “[I] voted my conscience,” [she] said. Animacy – we set animacy attributes using: (a) a static list for pronouns; (b) NER labels, e.g., PERSON is animate whereas LOCATION is not; and (c) a dictionary boostrapped from the web (Ji and Lin, 2009). NER label – from the Stanford NER. If we cannot detect a value, we set attributes to unknown and treat th"
D10-1048,P03-1054,1,0.0100249,"a et al., 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009). It consists of 107 documents and 5,469 mentions. • ACE2004-NWIRE – the newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. It contains 128 documents and 11,413 mentions. • MUC6-TEST – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many pr"
D10-1048,H05-1004,0,0.787257,"l., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005; Finkel and Manning, 2008) for an analysis of these metrics. 4 Description of the Multi-Pass Sieve Our sieve framework is implemented as a succession of independent coreference models. We first describe how each model selects candidate mentions, and then describe the models themselves. 494 4.1 Mention Processing Given a mention mi , each model may either decline to propose a solution (in the hope that one of the subsequent models will solve it) or deterministically select a single best antecedent from a list of previous mentions m1 , . . . , mi−1 . We sort candidate antecedents using syntacti"
D10-1048,D08-1068,0,0.227799,"• ACE2004-ROTH-DEV2 – development split of Bengston and Roth (2008), from the corpus used in the 2004 Automatic Content Extraction (ACE) evaluation. It contains 68 documents and 4,536 mentions. 2 We use the same corpus names as (Haghighi and Klein, 2009) to facilitate comparison with previous work. • ACE2004-CULOTTA-TEST – partition of ACE 2004 corpus reserved for testing by several previous works (Culotta et al., 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009). It consists of 107 documents and 5,469 mentions. • ACE2004-NWIRE – the newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) for testing. It contains 128 documents and 11,413 mentions. • MUC6-TEST – test corpus from the sixth Message Understanding Conference (MUC-6) evaluation. It contains 30 documents and 2,068 mentions. We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with pr"
D10-1048,N10-1116,0,0.0131313,"er information. Additionally, the deterministic models in our tiered model are significantly simpler, yet perform generally better than the complex inference models proposed in these works. From a high level perspective, this work falls under the theory of shaping, defined as a “method of successive approximations” for learning (Skinner, 1938). This theory is known by different names in many NLP applications: Brown et al. (1993) used simple models as “stepping stones” for more complex word alignment models; Collins (1999) used “cautious” decision list learning for named entity classification; Spitkovsky et al. (2010) used “baby steps” for unsupervised dependency parsing, etc. To the best of our knowledge, we are the first to apply this theory to coreference resolution. 3 Description of the Task Intra-document coreference resolution clusters together textual mentions within a single document based on the underlying referent entity. Mentions are usually noun phrases (NPs) headed by nominal or pronominal terminals. To facilitate comparison with most of the recent previous work, we report results using gold mention boundaries. However, our approach does not make any assumptions about the underlying mentions,"
D10-1048,M95-1005,0,0.961704,"ing the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al., 2005). 3.2 Evaluation Metrics We use three evaluation metrics widely used in the literature: (a) pairwise F1 (Ghosh, 2003) – computed over mention pairs in the same entity cluster; (b) MUC (Vilain et al., 1995) – which measures how many predicted clusters need to be merged to cover the gold clusters; and (c) B3 (Amit and Baldwin, 1998) – which uses the intersection between predicted and gold clusters for a given mention to mark correct mentions and the sizes of the the predicted and gold clusters as denominators for precision and recall, respectively. We refer the interested reader to (X. Luo, 2005; Finkel and Manning, 2008) for an analysis of these metrics. 4 Description of the Multi-Pass Sieve Our sieve framework is implemented as a succession of independent coreference models. We first describe h"
D10-1048,P09-1074,0,\N,Missing
D11-1014,D08-1083,0,0.0210461,"ing one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as “prevent cancer”, where prevent reverses the negative polarity of cancer. Like our approach they capture compositional semantics. However, our model does so without manually constructing any rules or lexica. Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as “once in a life time”. While our method can also learn multiword phrases it do"
D11-1014,P07-1054,0,0.0495924,"features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica"
D11-1014,I08-1039,0,0.0131334,"ument level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and"
D11-1014,D07-1113,0,0.0138366,"e many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually"
D11-1014,N10-1120,0,0.768432,"vergence, predicting simply the average distribution in the training data give 0.83. Fig. 4 shows that our RAE-based model outperforms the other baselines. Table 2 shows EP example entries with predicted and gold distributions, as well as numbers of votes. 4.4 Binary Polarity Classification In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews4 (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al., 2005).We give statistical information on these and the EP corpus in Table 1. We compare to the state-of-the-art system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables. We use the same training and testing regimen (10-fold cross validation) as well as their baselines: majority phrase voting using sentiment and reversal lexica; rule-based reversal using a dependency tree; Bag-of-Features and their full Tree-CRF model. As shown in Table 4, our algorithm outperforms their approach on both datasets. For the movie review (MR) data set, we do not use any handdesigned lexica. An error analysis on the MPQA dataset showed several cases of single words which never occurred in the trai"
D11-1014,P04-1035,0,0.140477,"ysis include (Maas et al., 2011). 5.2 Sentiment Analysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection i"
D11-1014,P05-1015,0,0.817704,"semantic vector space (blue). Then they are recursively merged by the same autoencoder network into a fixed length sentence representation. The vectors at each node are used as features to predict a distribution over sentiment labels. 2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules). This limits the applicability of these methods to a broader range of tasks and languages. Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings. Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al., 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007). Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments. In this work, we seek to address three issues. (i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment. (ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, Procee"
D11-1014,W02-1011,0,0.0414298,"onal reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines. 1 Introduction The ability to identify sentiments about personal experiences, products, movies etc. is crucial to understand user generated content in social networks, blogs or product reviews. Detecting sentiment in these data is a challenging task which has recently spawned a lot of interest (Pang and Lee, 2008). Current baseline methods often use bag-of-words representations which cannot properly capture more complex linguistic phenomena in sentiment analysis (Pang et al., 2002). For instance, while the two phrases “white blood cells destroying an infection” and “an infection destroying white blood cells” have the same bag-of-words representation, the former is a positive reaction while the later is very negative. More advanced methods such as (Nakagawa et al., 151 ang@cs.stanford.edu Recursive Autoencoder Sorry, Hugs You Rock Teehee I Understand Wow, Just Wow Predicted Sentiment Distribution Semantic Representations i walked into a parked car Indices Words Figure 1: Illustration of our recursive autoencoder architecture which learns semantic vector representations o"
D11-1014,N07-1038,0,0.0263354,"representation. The vectors at each node are used as features to predict a distribution over sentiment labels. 2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules). This limits the applicability of these methods to a broader range of tasks and languages. Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings. Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al., 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007). Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments. In this work, we seek to address three issues. (i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment. (ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151–161, c Edinburgh, Scotland, UK, July 27–"
D11-1014,P10-1040,0,0.528937,"ce project (EP) dataset, results of standard classification tasks on this dataset and how to predict its sentiment label distributions. We then show results on other commonly used datasets and conclude with an analysis of the important parameters of the model. In all experiments involving our model, we represent words using 100-dimensional word vectors. We explore the two settings mentioned in Sec. 2.1. We compare performance on standard datasets when using randomly initialized word vectors (random word init.) or word vectors trained by the model of Collobert and Weston (2008) and provided by Turian et al. (2010).2 These vectors were trained on an unlabeled corpus of the English Wikipedia. Note that alternatives such as Brown clusters are not suitable since they do not capture sentiment information (good and bad are usually in the same cluster) and cannot be modified via backpropagation. 2 http://metaoptimize.com/projects/ wordreprs/ Corpus MPQA MR EP EP≥ 4 K 2 2 5 5 Instances 10,624 10,662 31,675 6,129 Distr.(+/-) 0.31/0.69 0.5/0.5 .2/.2/.1/.4/.1 .2/.2/.1/.4/.1 Avg|W | 3 22 113 129 Table 1: Statistics on the different datasets. K is the number of classes. Distr. is the distribution of the different c"
D11-1014,P02-1053,0,0.0452827,". Other recent deep learning methods for sentiment analysis include (Maas et al., 2011). 5.2 Sentiment Analysis Pang et al. (2002) were one of the first to experiment with sentiment classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predicti"
D11-1014,N10-1119,0,0.0168405,"oth single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment. The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as “prevent cancer”, where prevent reverses the negative polarity of cancer. Like our approach they capture compositional semantics. However, our model does so without manually constructing any rules or lexica. Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as “once in a life time”. While our method can also learn multiword phrases it does not require a seed set or a large web graph. (Nakagawa et al., 2010) introduced an approach based on CRFs with hidden variables with very good performance. We compare to their stateof-the-art system. We outperform them on the standard corpora that we tested on without requiring external systems such as POS taggers, dependency parsers and sentiment lexica. Our a"
D11-1014,H05-1044,0,0.215154,"nt classification. They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification. Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements. Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004). For further references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around"
D11-1014,W03-1017,0,0.0245035,"rther references, see (Pang and Lee, 2008). Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees. They also show improvements by first distinguishing between neutral and polar sentences. Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions. Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008). Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002). In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words. In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5). (Mao and Lebanon, 2007) propose isotonic conditional random fields and"
D11-1014,P06-1055,0,\N,Missing
D11-1067,E89-1001,0,0.645467,"e FTB does not mark it as one. There are multiple examples were the DP-TSG found the MWE whereas Stanford (its base distribution) did not, such as in Figure 4. Note that the “N P N” structure is quite frequent for MWNs, but the TSG correctly identifies the MWADV in emplois à domicile [jobs at home] ‘homeworking’. 7 Related Work There is a voluminous literature on MWE identification. Here we review closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an inemplois NP MWADV P N à domicile (c) DP-TSG N PP NP emplois P N à domicile (d) Stanford Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To our knowledge, Wehrli (2000) w"
D11-1067,C88-1002,0,0.433647,"hould be an MWE. The FTB does not mark it as one. There are multiple examples were the DP-TSG found the MWE whereas Stanford (its base distribution) did not, such as in Figure 4. Note that the “N P N” structure is quite frequent for MWNs, but the TSG correctly identifies the MWADV in emplois à domicile [jobs at home] ‘homeworking’. 7 Related Work There is a voluminous literature on MWE identification. Here we review closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an inemplois NP MWADV P N à domicile (c) DP-TSG N PP NP emplois P N à domicile (d) Stanford Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To"
D11-1067,P05-1038,0,0.343069,"s used in the WSJ—did not imof MWEs occurring in the language. For exam- prove accuracy. Enriched tag sets like that of Crabbé and Canple, Gross (1986) shows that dictionaries contain dito (2008) could also be investigated and compared to our reabout 1,500 single-word adverbs but that French con- sults since Evalb is insensitive to POS tags. 726 labels shown in Table 3, resulting in 24 total phrasal categories. correction of annotation errors. Like Arun-Cont, MFT contains concatenated MWEs. Corrections Historically, the FTB suffered from annotation errors such as missing POS and phrasal tags (Arun and Keller, 2005). We found that this problem has been largely resolved in the current release. However, 1,949 tokens and 36 MWE spans still lacked tags. We restored the labels by first assigning each token its most frequent POS tag elsewhere in the treebank, and then assigning the most frequent MWE phrasal category for the resulting POS sequence.2 FTB-UC (Candito and Crabbé, 2009): An instantiation of the functionally annotated section that makes a distinction between MWEs that are “syntactically regular” and those that are not. Syntactically regular MWEs were given internal structure, while all other MWEs we"
D11-1067,P10-1112,0,0.0175887,"se the collapsed, block Gibbs sampler of Liang et al. (2010). We sample binary variables bs associated with each non-terminal node/site in the treebank. The key idea is to select a block of exchangeable sites S of the same type that do not conflict (Figure 1). Since the sites in S are exchangeable, we can set bS randomly so long as we know m, the number of sites with bs = 1. Because this algorithm is a not a contribution of this paper, we refer the reader to Liang et al. (2010). 6 The Stanford parser is a product model, so the results in §5.1 include the contribution of a dependency parser. 7 Bansal and Klein (2010) also experimented with symbol refinement in an all-fragments (parametric) TSG for English. After each Gibbs iteration, we sample each sc directly using binomial-Beta conjugacy. We re-sample the DP concentration parameters αc with the auxiliary variable procedure of West (1995). Decoding We compute the rule score of each tree fragment from a single grammar sample as follows: θc,e = nc,e (z) + αc P0 (e|c) nc,· (z) + αc (5) To make the grammar more robust, we also include all CFG rules in P0 with zero counts in n. Scores for these rules follow from (5) with nc,e (z) = 0. For decoding, we note th"
D11-1067,C92-3126,0,0.0465504,"atic usage. For example, consider the two utterances: (5) a. b. He [ MWV kicked the bucket] . He [ VP kicked [ NP the pail]] . The examples in (5) may be equally probable and receive the same analysis under a PCFG; words are generated independently. However, recall that in our representation, (5a) should receive a flat analysis as MWV, whereas (5b) should have a conventional analysis of the verb kicked and its two arguments. An alternate view of parsing is one in which new utterances are built from previously observed fragments. This is the original motivation for data oriented parsing (DOP) (Bod, 1992), in which “idiomaticity is the rule rather than the exception” (Scha, 1990). If we have seen the collocation kicked the bucket several times before, we should store that whole fragment for later use. We consider a variant of the non-parametric PTSG model of Cohn et al. (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a DOP model with Bayesian parameter estimation. A PTSG is a 5-tuple hV, Σ, R, ♦, θi where c ∈ V are non-terminals; 4.1.1 Grammar Development Table 4 lists the symbol refinements used in our grammar. Most of the features ar"
D11-1067,W09-3821,0,0.017284,"Table 3, resulting in 24 total phrasal categories. correction of annotation errors. Like Arun-Cont, MFT contains concatenated MWEs. Corrections Historically, the FTB suffered from annotation errors such as missing POS and phrasal tags (Arun and Keller, 2005). We found that this problem has been largely resolved in the current release. However, 1,949 tokens and 36 MWE spans still lacked tags. We restored the labels by first assigning each token its most frequent POS tag elsewhere in the treebank, and then assigning the most frequent MWE phrasal category for the resulting POS sequence.2 FTB-UC (Candito and Crabbé, 2009): An instantiation of the functionally annotated section that makes a distinction between MWEs that are “syntactically regular” and those that are not. Syntactically regular MWEs were given internal structure, while all other MWEs were concatenated into single tokens. For example, nouns followed by adjectives, such as loi agraire ‘land law’ or Union monétaire et économique ‘monetary and economic Union’ were considered syntactically regular. They are MWEs because the choice of adjective is arbitrary (loi agraire and not * loi agricole, similarly to ‘coal black’ but not * ‘crow black’ for exampl"
D11-1067,candito-etal-2010-statistical,0,0.0177664,"Missing"
D11-1067,N10-1029,0,0.208887,"Missing"
D11-1067,N09-1062,0,0.0216909,"t in our representation, (5a) should receive a flat analysis as MWV, whereas (5b) should have a conventional analysis of the verb kicked and its two arguments. An alternate view of parsing is one in which new utterances are built from previously observed fragments. This is the original motivation for data oriented parsing (DOP) (Bod, 1992), in which “idiomaticity is the rule rather than the exception” (Scha, 1990). If we have seen the collocation kicked the bucket several times before, we should store that whole fragment for later use. We consider a variant of the non-parametric PTSG model of Cohn et al. (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a DOP model with Bayesian parameter estimation. A PTSG is a 5-tuple hV, Σ, R, ♦, θi where c ∈ V are non-terminals; 4.1.1 Grammar Development Table 4 lists the symbol refinements used in our grammar. Most of the features are POS splits as many phrasal tag splits did not lead to any improve4 ment. Parent annotation of POS tags (tagPA) capSimilar models were developed independently by tures information about the external context. mark- O’Donnell et al. (2009) and Post and Gildea (2009). 729 αc P0"
D11-1067,P10-4002,0,0.00556612,"nals and non-terminals. We encode this fragment as an SCFG rule of the form [X → γ , X → i, Y1 , . . . , Yn ] (6) where Y1 , . . . , Yn is the sequence of non-terminal nodes in γ.8 During decoding, the input is rewritten as a sequence of tree fragment (rule) indices {i, j, k, . . . }. Because the TSG substitution operator always applies to the leftmost frontier node, we can deterministically recover the monolingual parse with top-down re-writes of ♦. The SCFG formulation has a practical benefit: we can take advantage of the heavily-optimized SCFG decoders for machine translation. We use cdec (Dyer et al., 2010) to recover the Viterbi derivation under a DP-TSG grammar sample. 5 Experiments 5.1 Standard Parsing Experiments We evaluate parsing accuracy of the Stanford and DP-TSG models (Table 6). For comparison, we also include the Berkeley parser (Petrov et al., 2006).9 For the DP-TSG, we initialized all bs with fair coin tosses and ran for 400 iterations, after which likelihood stopped improving. 8 This formulation is due to Chris Dyer. Training settings: right binarization, no parent annotation, six split-merge cycles, and random initialization. 9 731 Leaf Ancestor Corpus Sent PA-PCFG DP-TSG Stanfor"
D11-1067,C86-1001,0,0.577847,"in à l’insu de ‘to the ignorance of’. Labels We augmented the basic FTB label set— which contains 14 POS tags and 19 phrasal tags—in two ways. First, we added 16 finer-grained POS tags for punctuation.1 Second, we added the 11 MWE The corpus used in our experiments is the French Treebank (Abeillé et al. (2003), version from June 2010, hereafter FTB). In French, there is a linguistic tradition of lexicography which compiles lists 1 Punctuation tag clusters—as used in the WSJ—did not imof MWEs occurring in the language. For exam- prove accuracy. Enriched tag sets like that of Crabbé and Canple, Gross (1986) shows that dictionaries contain dito (2008) could also be investigated and compared to our reabout 1,500 single-word adverbs but that French con- sults since Evalb is insensitive to POS tags. 726 labels shown in Table 3, resulting in 24 total phrasal categories. correction of annotation errors. Like Arun-Cont, MFT contains concatenated MWEs. Corrections Historically, the FTB suffered from annotation errors such as missing POS and phrasal tags (Arun and Keller, 2005). We found that this problem has been largely resolved in the current release. However, 1,949 tokens and 36 MWE spans still lacke"
D11-1067,D07-1028,0,0.0808741,"Missing"
D11-1067,P03-1054,1,0.0130339,"Missing"
D11-1067,levy-andrew-2006-tregex,0,0.0166486,"ey mistakenly removed half of the corpus, believing that the multi-terminal (per POS tag) annotations of MWEs were XML errors (Schluter and Genabith, 2007). MFT (Schluter and Genabith, 2007): Manual revision to 3,800 sentences. Major changes included coordination raising, an expanded POS tag set, and the 2 73 of the unlabeled word types did not appear elsewhere in the treebank. All but 11 of these were nouns. We manually assigned the correct tags, but we would not expect a negative effect by deterministically labeling all of them as nouns. 3 We automate tree manipulation with Tregex/Tsurgeon (Levy and Andrew, 2006). Our pre-processing package is available at http://nlp.stanford.edu/software/lex-parser.shtml. 727 Almost all work on the FTB has followed ArunCont and used gold MWE pre-grouping. As a result, most results for French parsing are analogous to early results for Chinese, which used gold word segmentation, and Arabic, which used gold clitic segmentation. Candito et al. (2010) were the first to acknowledge and address this issue, but they still used FTBUC (with some pre-grouped MWEs). Since the syntax and definition of MWEs is a contentious issue, we take a more agnostic view—which is consistent w"
D11-1067,N10-1082,0,0.0870239,"xternal context. mark- O’Donnell et al. (2009) and Post and Gildea (2009). 729 αc P0 (e|c) x S S b = {bs }s∈S z m n = {nc,e } ∆nS:m DP concentration parameter for each c ∈ V CFG base distribution Set of non-terminal nodes in the treebank Set of sampling sites (one for each x ∈ x) A block of sampling sites, where S ⊆ S Binary variables to be sampled (bs = 1 → frontier node) Latent state of the segmented treebank Number of sites s ∈ S s.t. bS = 1 Sufficient statistics of z Change in counts by setting m sites in S Table 5: DP-TSG model notation. For consistency, we largely follow the notation of Liang et al. (2010). Note that z = (b, x), and as such z = hc, ei. t ∈ Σ are terminals; e ∈ R are elementary trees;5 ♦ ∈ V is a unique start symbol; and θc,e ∈ θ are parameters for each tree fragment. A PTSG derivation is created by successively applying the substitution operator to the leftmost frontier node (denoted by c+ ). All other nodes are internal (denoted by c− ). In the supervised setting, DP-TSG grammar extraction reduces to a segmentation problem. We have a treebank T that we segment into the set R, a process that we model with Bayes’ rule: p(R |T ) ∝ p(T |R) p(R) (1) Since the tree fragments complet"
D11-1067,P99-1041,0,0.0626924,"Missing"
D11-1067,P06-1055,0,0.0129047,"icting sites of the same def type. Define the type of a site t(z, s) = (∆ns:0 , ∆ns:1 ). Sites (1) and (2) above have the same type since t(z, s1 ) = t(z, s2 ). However, the two sites conflict since the probabilities of setting bs1 and bs2 both depend on counts for the tree fragment rooted at NP. Consequently, sites (1) and (2) are not exchangeable: the probabilities of their assignments depend on the order in which they are sampled. ford parser.6,7 After applying the manual state splits, we perform simple right binarization, collapse unary rules, and replace rare words with their signatures (Petrov et al., 2006). For each non-terminal type c, we learn a stop probability sc ∼ Beta(1, 1). Under P0 , the probability of generating a rule A+ → B − C + composed of nonterminals is P0 (A+ → B − C + ) = pMLE (A → B C)sB (1−sC ) (3) For lexical insertion rules, we add a penalty proportional to the frequency of the lexical item: P0 (c → t) = pMLE (c → t)p(t) (4) where p(t) is equal to the MLE unigram probability of t in the treebank. Lexicalizing a rule makes it very specific, so we generally want to avoid lexicalization with rare words. Empirically, we found that this penalty reduces overfitting. Type-based In"
D11-1067,P09-2012,0,0.0401802,"c PTSG model of Cohn et al. (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a DOP model with Bayesian parameter estimation. A PTSG is a 5-tuple hV, Σ, R, ♦, θi where c ∈ V are non-terminals; 4.1.1 Grammar Development Table 4 lists the symbol refinements used in our grammar. Most of the features are POS splits as many phrasal tag splits did not lead to any improve4 ment. Parent annotation of POS tags (tagPA) capSimilar models were developed independently by tures information about the external context. mark- O’Donnell et al. (2009) and Post and Gildea (2009). 729 αc P0 (e|c) x S S b = {bs }s∈S z m n = {nc,e } ∆nS:m DP concentration parameter for each c ∈ V CFG base distribution Set of non-terminal nodes in the treebank Set of sampling sites (one for each x ∈ x) A block of sampling sites, where S ⊆ S Binary variables to be sampled (bs = 1 → frontier node) Latent state of the segmented treebank Number of sites s ∈ S s.t. bS = 1 Sufficient statistics of z Change in counts by setting m sites in S Table 5: DP-TSG model notation. For consistency, we largely follow the notation of Liang et al. (2010). Note that z = (b, x), and as such z = hc, ei. t ∈ Σ"
D11-1067,ramisch-etal-2010-mwetoolkit,0,0.0869479,"Missing"
D11-1067,seddah-2010-exploring,0,0.0838383,"Missing"
D11-1067,J93-1007,0,0.431093,"the TSG correctly identifies the MWADV in emplois à domicile [jobs at home] ‘homeworking’. 7 Related Work There is a voluminous literature on MWE identification. Here we review closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an inemplois NP MWADV P N à domicile (c) DP-TSG N PP NP emplois P N à domicile (d) Stanford Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To our knowledge, Wehrli (2000) was the first to use syntactically annotated corpora to improve a parser for MWE identification. He proposed to rank analyses of a symbolic parser based on the presence of collocations, although details of the r"
D11-1067,E93-1045,0,0.0586717,"fragments (parametric) TSG for English. After each Gibbs iteration, we sample each sc directly using binomial-Beta conjugacy. We re-sample the DP concentration parameters αc with the auxiliary variable procedure of West (1995). Decoding We compute the rule score of each tree fragment from a single grammar sample as follows: θc,e = nc,e (z) + αc P0 (e|c) nc,· (z) + αc (5) To make the grammar more robust, we also include all CFG rules in P0 with zero counts in n. Scores for these rules follow from (5) with nc,e (z) = 0. For decoding, we note that the derivations of a TSG are a CFG parse forest (Vijay-Shanker and Weir, 1993). As such, we can use a Synchronous Context Free Grammar (SCFG) to translate the 1-best parse to its derivation. Consider a unique tree fragment ei rooted at X with frontier γ, which is a sequence of terminals and non-terminals. We encode this fragment as an SCFG rule of the form [X → γ , X → i, Y1 , . . . , Yn ] (6) where Y1 , . . . , Yn is the sequence of non-terminal nodes in γ.8 During decoding, the input is rewritten as a sequence of tree fragment (rule) indices {i, j, k, . . . }. Because the TSG substitution operator always applies to the leftmost frontier node, we can deterministically"
D12-1042,P07-1073,0,0.721989,"ormation (e.g., events, binary relations, etc.) from free text, has received renewed interest in the “big data” era, when petabytes of natural-language text containing thousands of different structure types are readily available. However, traditional supervised methods are unlikely to scale in this context, as training data is either limited or nonexistent for most of these structures. One of the most promising approaches to IE that addresses this limitation is distant supervision, which generates training data automatically by aligning a database of facts with text (Craven and Kumlien, 1999; Bunescu and Mooney, 2007). In this paper we focus on distant supervision for relation extraction (RE), a subproblem of IE that addresses the extraction of labeled relations between two named entities. Figure 1 shows a simple example for a RE domain with two labels. Distant supervision introduces two modeling challenges, which we highlight in the table. The first challenge is that some training examples obtained through this heuristic are not valid, e.g., the last sentence in Figure 1 is not a correct example for any of the known labels for the tuple. The percentage of such false positives can be quite high. For exampl"
D12-1042,P05-1045,1,0.0789188,"Missing"
D12-1042,P11-1055,0,0.705827,"ns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu 456 et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multip"
D12-1042,P09-1113,0,0.84884,"ween labels). For example, our model learns that certain labels tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu 456 et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instan"
D12-1042,P11-2048,0,0.0116906,"tend to be generated jointly while others cannot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu 456 et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple mentions for the same tup"
D12-1042,W11-0902,1,0.586201,"ot be jointly assigned to the same tuple. (b) We show that MIML-RE performs competitively on two difficult domains. 2 Related Work Distant supervision for IE was introduced by Craven and Kumlien (1999), who focused on the extraction of binary relations between proteins and cells/tissues/diseases/drugs using the Yeast Protein Database as a source of distant supervision. Since then, the approach grew in popularity (Bunescu and Mooney, 2007; Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu et al., 2011a). However, most of these approaches make one or more approximations in learning. For example, most proposals heuristically transform distant supervision to traditional supervised learning (i.e., singleinstance single-label) (Bellare and McCallum, 2007; Wu and Weld, 2007; Mintz et al., 2009; Nguyen and Moschitti, 2011; Sun et al., 2011; Surdeanu 456 et al., 2011a). Bunescu and Mooney (2007) and Riedel et al. (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple mentions for the same tuple but disallows more than one label per"
D12-1080,P11-1062,0,0.0126492,"elated work that focuses on ordering events or classifying temporal relations between them (Ling and Weld, 2010; Yoshikawa et al., 2009; Chambers and Jurafsky, 2008; Mani et al., 2006, inter alia). Much of this work uses the Allen interval relations (Allen, 1983) which richly describe partial orderings of fluents. We use several of these as fluent-level question templates. Joint inference has been applied successfully 10 Percentages for “unknown” are omitted here. 880 to other NLP problems (Roth and Yih, 2004; Toutanova et al., 2008; Martins et al., 2009; Chang et al., 2010; Koo et al., 2010; Berant et al., 2011). Two recent examples in information extraction include using Markov Logic for temporal ordering (Ling and Weld, 2010) and using dualdecomposition for event extraction (Riedel and McCallum, 2011). Our work is closest to Temporal KBP slot filling systems. The CUNY and UNED systems (Artiles et al., 2011; Garrido et al., 2011) for this task used classifiers to determine the relation between temporal expressions and fluents. These systems use the hard decisions from the classifier and combine the decisions by finding a span that includes all temporal expressions. In contrast, our system uses the c"
D12-1080,D08-1073,0,0.450141,"Missing"
D12-1080,chang-manning-2012-sutime,1,0.880562,"Missing"
D12-1080,N10-1066,0,0.0220732,"elated work There is a large body of related work that focuses on ordering events or classifying temporal relations between them (Ling and Weld, 2010; Yoshikawa et al., 2009; Chambers and Jurafsky, 2008; Mani et al., 2006, inter alia). Much of this work uses the Allen interval relations (Allen, 1983) which richly describe partial orderings of fluents. We use several of these as fluent-level question templates. Joint inference has been applied successfully 10 Percentages for “unknown” are omitted here. 880 to other NLP problems (Roth and Yih, 2004; Toutanova et al., 2008; Martins et al., 2009; Chang et al., 2010; Koo et al., 2010; Berant et al., 2011). Two recent examples in information extraction include using Markov Logic for temporal ordering (Ling and Weld, 2010) and using dualdecomposition for event extraction (Riedel and McCallum, 2011). Our work is closest to Temporal KBP slot filling systems. The CUNY and UNED systems (Artiles et al., 2011; Garrido et al., 2011) for this task used classifiers to determine the relation between temporal expressions and fluents. These systems use the hard decisions from the classifier and combine the decisions by finding a span that includes all temporal express"
D12-1080,P05-1045,1,0.0453045,"ntly). The inference process is described in §3.4. 3.1 Temporal expression retrieval Given a fluent, we search for all textual mentions of the fluent and collect nearby temporal expression mentions. These temporal expressions are used as candidate boundaries for the fluent in later steps. The search process assumes that if a fluent’s entity and slot value co-occur in a sentence,5 that sentence is typically a positive example of the fluent.6 This is sometimes known as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009). We use the Stanford Core NLP suite (Toutanova et al., 2003; Finkel et al., 2005; Klein and Manning, 2003; Lee et al., 2011) to annotate each document with POS and NER tags, parse trees, and coreference chains. On top of this, we apply a rule-based temporal expression extractor (Chang and Manning, 2012). Since we have coreference links, we also search documents for anything coreferent with the fluent’s entity. The temporal expression extractor handles most standard date and time formats. For each document, one can provide an optional reference time. For underspecified dates, the reference time is used to 5 While we limit our scope to sentences in this work, it is trivial"
D12-1080,P03-1054,1,0.0189108,"process is described in §3.4. 3.1 Temporal expression retrieval Given a fluent, we search for all textual mentions of the fluent and collect nearby temporal expression mentions. These temporal expressions are used as candidate boundaries for the fluent in later steps. The search process assumes that if a fluent’s entity and slot value co-occur in a sentence,5 that sentence is typically a positive example of the fluent.6 This is sometimes known as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009). We use the Stanford Core NLP suite (Toutanova et al., 2003; Finkel et al., 2005; Klein and Manning, 2003; Lee et al., 2011) to annotate each document with POS and NER tags, parse trees, and coreference chains. On top of this, we apply a rule-based temporal expression extractor (Chang and Manning, 2012). Since we have coreference links, we also search documents for anything coreferent with the fluent’s entity. The temporal expression extractor handles most standard date and time formats. For each document, one can provide an optional reference time. For underspecified dates, the reference time is used to 5 While we limit our scope to sentences in this work, it is trivial to extend this to larger"
D12-1080,D10-1125,0,0.0143955,"a large body of related work that focuses on ordering events or classifying temporal relations between them (Ling and Weld, 2010; Yoshikawa et al., 2009; Chambers and Jurafsky, 2008; Mani et al., 2006, inter alia). Much of this work uses the Allen interval relations (Allen, 1983) which richly describe partial orderings of fluents. We use several of these as fluent-level question templates. Joint inference has been applied successfully 10 Percentages for “unknown” are omitted here. 880 to other NLP problems (Roth and Yih, 2004; Toutanova et al., 2008; Martins et al., 2009; Chang et al., 2010; Koo et al., 2010; Berant et al., 2011). Two recent examples in information extraction include using Markov Logic for temporal ordering (Ling and Weld, 2010) and using dualdecomposition for event extraction (Riedel and McCallum, 2011). Our work is closest to Temporal KBP slot filling systems. The CUNY and UNED systems (Artiles et al., 2011; Garrido et al., 2011) for this task used classifiers to determine the relation between temporal expressions and fluents. These systems use the hard decisions from the classifier and combine the decisions by finding a span that includes all temporal expressions. In contrast,"
D12-1080,P06-1095,0,0.060991,"Missing"
D12-1080,P09-1039,0,0.0679482,"raints are useful. 6 Related work There is a large body of related work that focuses on ordering events or classifying temporal relations between them (Ling and Weld, 2010; Yoshikawa et al., 2009; Chambers and Jurafsky, 2008; Mani et al., 2006, inter alia). Much of this work uses the Allen interval relations (Allen, 1983) which richly describe partial orderings of fluents. We use several of these as fluent-level question templates. Joint inference has been applied successfully 10 Percentages for “unknown” are omitted here. 880 to other NLP problems (Roth and Yih, 2004; Toutanova et al., 2008; Martins et al., 2009; Chang et al., 2010; Koo et al., 2010; Berant et al., 2011). Two recent examples in information extraction include using Markov Logic for temporal ordering (Ling and Weld, 2010) and using dualdecomposition for event extraction (Riedel and McCallum, 2011). Our work is closest to Temporal KBP slot filling systems. The CUNY and UNED systems (Artiles et al., 2011; Garrido et al., 2011) for this task used classifiers to determine the relation between temporal expressions and fluents. These systems use the hard decisions from the classifier and combine the decisions by finding a span that includes"
D12-1080,P09-1113,0,0.011579,"fluent is a unary relation. joint inference (note that they are trained independently). The inference process is described in §3.4. 3.1 Temporal expression retrieval Given a fluent, we search for all textual mentions of the fluent and collect nearby temporal expression mentions. These temporal expressions are used as candidate boundaries for the fluent in later steps. The search process assumes that if a fluent’s entity and slot value co-occur in a sentence,5 that sentence is typically a positive example of the fluent.6 This is sometimes known as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009). We use the Stanford Core NLP suite (Toutanova et al., 2003; Finkel et al., 2005; Klein and Manning, 2003; Lee et al., 2011) to annotate each document with POS and NER tags, parse trees, and coreference chains. On top of this, we apply a rule-based temporal expression extractor (Chang and Manning, 2012). Since we have coreference links, we also search documents for anything coreferent with the fluent’s entity. The temporal expression extractor handles most standard date and time formats. For each document, one can provide an optional reference time. For underspecified dates, the reference tim"
D12-1080,D11-1001,0,0.0934638,"r alia). Much of this work uses the Allen interval relations (Allen, 1983) which richly describe partial orderings of fluents. We use several of these as fluent-level question templates. Joint inference has been applied successfully 10 Percentages for “unknown” are omitted here. 880 to other NLP problems (Roth and Yih, 2004; Toutanova et al., 2008; Martins et al., 2009; Chang et al., 2010; Koo et al., 2010; Berant et al., 2011). Two recent examples in information extraction include using Markov Logic for temporal ordering (Ling and Weld, 2010) and using dualdecomposition for event extraction (Riedel and McCallum, 2011). Our work is closest to Temporal KBP slot filling systems. The CUNY and UNED systems (Artiles et al., 2011; Garrido et al., 2011) for this task used classifiers to determine the relation between temporal expressions and fluents. These systems use the hard decisions from the classifier and combine the decisions by finding a span that includes all temporal expressions. In contrast, our system uses the classifier’s marginal probabilities along with the consistency component to incorporate global consistency constraints. Other participants used rule-based and pattern matching approaches (Byrne an"
D12-1080,W04-2401,0,0.0756157,"e noise in the data, it is clear these constraints are useful. 6 Related work There is a large body of related work that focuses on ordering events or classifying temporal relations between them (Ling and Weld, 2010; Yoshikawa et al., 2009; Chambers and Jurafsky, 2008; Mani et al., 2006, inter alia). Much of this work uses the Allen interval relations (Allen, 1983) which richly describe partial orderings of fluents. We use several of these as fluent-level question templates. Joint inference has been applied successfully 10 Percentages for “unknown” are omitted here. 880 to other NLP problems (Roth and Yih, 2004; Toutanova et al., 2008; Martins et al., 2009; Chang et al., 2010; Koo et al., 2010; Berant et al., 2011). Two recent examples in information extraction include using Markov Logic for temporal ordering (Ling and Weld, 2010) and using dualdecomposition for event extraction (Riedel and McCallum, 2011). Our work is closest to Temporal KBP slot filling systems. The CUNY and UNED systems (Artiles et al., 2011; Garrido et al., 2011) for this task used classifiers to determine the relation between temporal expressions and fluents. These systems use the hard decisions from the classifier and combine"
D12-1080,N03-1033,1,0.0261697,"ey are trained independently). The inference process is described in §3.4. 3.1 Temporal expression retrieval Given a fluent, we search for all textual mentions of the fluent and collect nearby temporal expression mentions. These temporal expressions are used as candidate boundaries for the fluent in later steps. The search process assumes that if a fluent’s entity and slot value co-occur in a sentence,5 that sentence is typically a positive example of the fluent.6 This is sometimes known as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009). We use the Stanford Core NLP suite (Toutanova et al., 2003; Finkel et al., 2005; Klein and Manning, 2003; Lee et al., 2011) to annotate each document with POS and NER tags, parse trees, and coreference chains. On top of this, we apply a rule-based temporal expression extractor (Chang and Manning, 2012). Since we have coreference links, we also search documents for anything coreferent with the fluent’s entity. The temporal expression extractor handles most standard date and time formats. For each document, one can provide an optional reference time. For underspecified dates, the reference time is used to 5 While we limit our scope to sentences in this"
D12-1080,J08-2002,1,0.761267,"S the span of another one, whether a fluent is COMPLETELY BEFORE another fluent, and whether two fluents TOUCH (the start of one fluent is the same as the end of another). Since all of these questions involve ordering but ignore the actual differences in time, we create one more set of questions asking whether two boundaries are WITHIN a certain number of years: While distant supervision can be used to create implicit negative examples for the classifier component |boundary 1 (fluent 1 ) − boundary 2 (fluent 2 ) |≤ K dency paths and their collapsed Stanford Dependencies forms (de Marneffe and Manning, 2008). We include the lengths of each path and, if the path is shorter than four edges, the grammatical relations, words, POS tags, and NER labels along the path. We extract the same sorts of features from surface paths (i.e., the words and tags between the entity and the temporal expression) if the path is five tokens or shorter. For temporal expressions, we include their century and decade as features. These features act as a crude prior over when valid temporal expressions occur. There are also features for the precision of the temporal expression (year only, has month, and has day). Lastly, we"
D12-1080,P09-1046,0,0.291108,"Missing"
D12-1080,W08-1301,1,\N,Missing
D12-1080,W11-1902,0,\N,Missing
D12-1090,P07-1111,0,0.0189342,"iu et al., 2011). Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al., 1990), and paraphrasing (Snover et al., 2009; Denkowski and Lavie, 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Kulesza and Shieber, 2004; Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al., 2008; Pado et al., 2009). The drawback, however, is they rely on time-consuming preprocessing modules to extract linguistic features (e.g., a full end-toend textual entailment system was needed in Pado et al. (2009)), which severely limits their practical use. Furthermore, these models employ a large number of features (on the order of hundreds), and consequently make the model predictions opaque and hard to analyze. In this paper, we propose a simple yet powerful probabilistic Finite State Machine (pFSM) for the task of MT evaluation. It is built on the"
D12-1090,P07-1038,0,0.0209185,"iu et al., 2011). Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al., 1990), and paraphrasing (Snover et al., 2009; Denkowski and Lavie, 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Kulesza and Shieber, 2004; Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al., 2008; Pado et al., 2009). The drawback, however, is they rely on time-consuming preprocessing modules to extract linguistic features (e.g., a full end-toend textual entailment system was needed in Pado et al. (2009)), which severely limits their practical use. Furthermore, these models employ a large number of features (on the order of hundreds), and consequently make the model predictions opaque and hard to analyze. In this paper, we propose a simple yet powerful probabilistic Finite State Machine (pFSM) for the task of MT evaluation. It is built on the"
D12-1090,W05-0909,0,0.0291156,"ar-regression model. Both pFSM and pPDA learned to assign a lower negative feature weight for deletion than insertion (i.e., it is bad to insert an unseen word into system trans9 M ETEOR R actually has an unfair advantage in this comparison, since it uses synonym information from WordNet; T ER R on the other hand has a disadvantage because it does not use lemmas. Lemma is added later in the TERplus extension (Snover et al., 2009). lation, but worse if words from reference translation are deleted), which corresponds to the setting in METEOR where recall is given more importance than precision (Banerjee and Lavie, 2005). The RTE R and M T +RTE R linear regression models benefit from the rich linguistic features in the textual entailment system’s output. It has access to all the features in pPDA+f such as paraphrase and dependency parse relations, and many more (e.g., Norm Bank, part-of-speech, negation, antonyms). However, our pPDA+f model rivals the performance of RTE R and M T +RTE R on Arabic (with no statistically significant difference from RTE R), and greatly improve over these two models on Urdu and Chinese. Most noticeably, pPDA+f is 7.7 points better than RTE R on Chinese. Consistent with our earlie"
D12-1090,N10-1083,0,0.0276476,"edit distance models with block movements was also explored in Leusch et al. (2003). However, their model is largely empirical and not in a probabilistic learning setting. The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallum et al., 2005; Bernard et al., 2008; Wang and Manning, 2010; Emms, 2012). In particular, our pFSM model and the log-linear parameterization were inspired by Wang and Manning (2010). Another body of literature that is closely related to this work is FSM models for word alignment (Vogel et al., 1996; Saers et al., 2010; Berg-Kirkpatrick et al., 2010). The stochastic Inversion Transduction Grammar in Saers et al. (2010) for instance, is a pFSM with special constraints. More recently, Saers and Wu (2011) further explored the connection between Linear Transduction Grammars and FSMs. There is a close tie between our pFSM model and the HMM model in Berg-Kirkpatrick et al. (2010). Both models adopted a log-linear parameterization for the state transition distribution, 13 but in their case the HMM model and the pFSM arc weights are normalized locally, and the objective is non-convex. 6 Conclusion We described a probabilistic finite state machine"
D12-1090,P11-1103,0,0.0178972,"and the idea of using paraphrases in MT evaluation was first proposed by Zhou et al. (2006). Several recent studies have introduced metrics over dependency parses (Liu et al., 2005; Owczarzak et al., 2008; He et al., 2010), but their improvements over n-gram models at the sentence level are not always consistent (Liu et al., 2005; Peterson and Przybocki, 2010). Other than string-based methods, recent work has explored more alternative representations for MT evaluation, such as network properties (Amancio et al., 2011), semantic role structures (Lo and Wu, 2011), and the quality of word order (Birch and Osborne, 2011). Modeling MetricsMATR10 and WMT12 Results An earlier version of the pFSM model that was trained on the OpenMT08 data set was submitted to the single reference sentence level track at MetricsMATR10 (Peterson and Przybocki, 2010) NIST evaluation. Even though our system was not in the most ideal state at the time of the evaluation, 12 and was trained on a small amount of data, the pFSM model still performed competitively against other metrics. Noticeably, we achieved second best results for Human-targeted Translation Edit Rate (HTER) assessment, trailing behind TERplus with no statistically sign"
D12-1090,W09-0434,0,0.0267888,"mns 5 through 8 in Table 1 show experimental results validating the contribution of our pPDA extension to the pFSM model (cf. Section 2.2). We can see that the pPDA extension gave modest improvements on the Urdu test set, but at a small decrease in performance on the Arabic data. However, for Chinese, there is a substantial gain, particularly with jump distances of five or longer. This trend is even more pronounced at the long jump distance of 10, consistent with the observation that Chinese-English translations exhibit much more medium and long distance reordering than languages like Arabic (Birch et al., 2009). 4.1.2 Evaluating Linguistic Features Experimental results evaluating the benefits of each linguistic feature set are presented in Table 3. The first row is the pPDA model with jump distance limit 5, without other additional features. The next three rows are the results of adding each of the three feature sets described in Section 3. Overall, we observed that only paraphrase matching features gave a significant boost to performance. Data Set train test A+C U A+U C A C+U MT08 MT06 pFSM 54.6 59.9 61.2 65.2 Our Metrics pPDA pPDA+f 55.3 57.2 63.8 65.8 60.4 59.8 63.4 64.5 B LEU R 49.9 53.9 52.5 57"
D12-1090,W10-1703,0,0.0698688,"Missing"
D12-1090,W11-2103,0,0.0412552,"Missing"
D12-1090,N10-1031,0,0.0694171,"relation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation (CallisonBurch et al. (2009; 2010; 2011), inter alia). Recent studies have also confirmed that tuning MT systems against better MT metrics — using algorithms like MERT (Och, 2003) — leads to better system performance (He and Way, 2009; Liu et al., 2011). Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al., 1990), and paraphrasing (Snover et al., 2009; Denkowski and Lavie, 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Kulesza and Shieber, 2004; Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al., 2008; Pado et al., 2009). The drawback, however, is they rely on time-consuming preprocessing modules to extract linguistic features (e.g., a full end-toend textual ent"
D12-1090,P02-1001,0,0.033743,"pFSMs for MT Regression 0 |e | We start off by framing the problem of machine translation evaluation in terms of weighted edit distances calculated using probabilistic finite state machines (pFSMs). A FSM defines a language by accepting a string of input tokens in the language, and rejecting those that are not. A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM. Commonly used models such as HMMs, n-gram models, Markov Chains and probabilistic finite state transducers all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal et al., 2005). Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations. A string of tokens that our pFSM accepts is an edit sequence that transforms a reference translation (denoted as ref ) into a system translation (sys). Our pFSM has a unique start and stop state, and one state per edit operation (i.e., Insert, Delete, Substitution). The probability of an edit sequence e is generated by the model is the product of the state transition probabilities in the pFSM, formally described as: |e| w(e"
D12-1090,2009.mtsummit-posters.8,0,0.0167931,"(SMT) systems. The early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches (Papineni et al., 2002; Doddington, 2002). Despite their simplicity, these measures have shown good correlation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation (CallisonBurch et al. (2009; 2010; 2011), inter alia). Recent studies have also confirmed that tuning MT systems against better MT metrics — using algorithms like MERT (Och, 2003) — leads to better system performance (He and Way, 2009; Liu et al., 2011). Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al., 1990), and paraphrasing (Snover et al., 2009; Denkowski and Lavie, 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Kulesza and Shieber, 200"
D12-1090,W10-1753,0,0.0352408,"Missing"
D12-1090,knight-al-onaizan-1998-translation,0,0.0573762,"ences, formally expressed as: pFSMs for MT Regression 0 |e | We start off by framing the problem of machine translation evaluation in terms of weighted edit distances calculated using probabilistic finite state machines (pFSMs). A FSM defines a language by accepting a string of input tokens in the language, and rejecting those that are not. A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM. Commonly used models such as HMMs, n-gram models, Markov Chains and probabilistic finite state transducers all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal et al., 2005). Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations. A string of tokens that our pFSM accepts is an edit sequence that transforms a reference translation (denoted as ref ) into a system translation (sys). Our pFSM has a unique start and stop state, and one state per edit operation (i.e., Insert, Delete, Substitution). The probability of an edit sequence e is generated by the model is the product of the state transition probabilities in the pFSM, formally describe"
D12-1090,W04-3250,0,0.0771488,"), as well as rich linguistic features from a textual entailment system (RTE). In all of our experiments, each reference and system translation sentence pair is tokenized using the Penn Treebank (Marcus et al., 1993) tokenization script, and lemmatized by the Porter Stemmer (Porter, 1980). For the overall sentence structure experiment, translations are additionally part-of-speech tagged with MXPOST tagger (Ratnaparkhi, 1996), and parsed with MSTParser (McDonald et al., 2005) 7 labeled dependency parser. Statistical significance tests are performed using the paired bootstrap resampling method (Koehn, 2004). We divide our experiments into two sections, based on two different prediction tasks — predicting absolute scores and predicting pairwise preference. 4.1 Exp. 1: Predicting Absolute Scores The first task is to evaluate a system translation on a seven point Likert scale against a single reference. Higher scores indicate translations that are closer to the meaning intended by the reference. Human ratings in the form of absolute scores are available for standard evaluation data sets such as NIST OpenMT06,08.8 Since our model makes predictions at the granularity of a whole sentence, we focus on"
D12-1090,N03-1019,0,0.0343177,"Regression 0 |e | We start off by framing the problem of machine translation evaluation in terms of weighted edit distances calculated using probabilistic finite state machines (pFSMs). A FSM defines a language by accepting a string of input tokens in the language, and rejecting those that are not. A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM. Commonly used models such as HMMs, n-gram models, Markov Chains and probabilistic finite state transducers all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal et al., 2005). Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations. A string of tokens that our pFSM accepts is an edit sequence that transforms a reference translation (denoted as ref ) into a system translation (sys). Our pFSM has a unique start and stop state, and one state per edit operation (i.e., Insert, Delete, Substitution). The probability of an edit sequence e is generated by the model is the product of the state transition probabilities in the pFSM, formally described as: |e| w(e |s, r) = ∏k=1 exp θ · f"
D12-1090,2003.mtsummit-papers.32,0,0.061095,"performed competitively against other metrics. Noticeably, we achieved second best results for Human-targeted Translation Edit Rate (HTER) assessment, trailing behind TERplus with no statistically significant difference. On average, our system made 5th place among 15 different sites and 7th place among 25 different metrics, averaged across 9 assessment types. 12 Unfortunately the version we submitted in 2010 was plagued with a critical bug. More general enhancements have been made to the model since. 992 The idea of using extended edit distance models with block movements was also explored in Leusch et al. (2003). However, their model is largely empirical and not in a probabilistic learning setting. The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallum et al., 2005; Bernard et al., 2008; Wang and Manning, 2010; Emms, 2012). In particular, our pFSM model and the log-linear parameterization were inspired by Wang and Manning (2010). Another body of literature that is closely related to this work is FSM models for word alignment (Vogel et al., 1996; Saers et al., 2010; Berg-Kirkpatrick et al., 2010). The stochastic Inversion Transduction Grammar in Sa"
D12-1090,W05-0904,0,0.0755097,"Missing"
D12-1090,D11-1035,0,0.0159017,"e early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches (Papineni et al., 2002; Doddington, 2002). Despite their simplicity, these measures have shown good correlation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation (CallisonBurch et al. (2009; 2010; 2011), inter alia). Recent studies have also confirmed that tuning MT systems against better MT metrics — using algorithms like MERT (Och, 2003) — leads to better system performance (He and Way, 2009; Liu et al., 2011). Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al., 1990), and paraphrasing (Snover et al., 2009; Denkowski and Lavie, 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Kulesza and Shieber, 2004; Albrecht and Hwa"
D12-1090,P11-1023,0,0.0181385,"that paraphrasing helps boosting model accuracy, and the idea of using paraphrases in MT evaluation was first proposed by Zhou et al. (2006). Several recent studies have introduced metrics over dependency parses (Liu et al., 2005; Owczarzak et al., 2008; He et al., 2010), but their improvements over n-gram models at the sentence level are not always consistent (Liu et al., 2005; Peterson and Przybocki, 2010). Other than string-based methods, recent work has explored more alternative representations for MT evaluation, such as network properties (Amancio et al., 2011), semantic role structures (Lo and Wu, 2011), and the quality of word order (Birch and Osborne, 2011). Modeling MetricsMATR10 and WMT12 Results An earlier version of the pFSM model that was trained on the OpenMT08 data set was submitted to the single reference sentence level track at MetricsMATR10 (Peterson and Przybocki, 2010) NIST evaluation. Even though our system was not in the most ideal state at the time of the evaluation, 12 and was trained on a small amount of data, the pFSM model still performed competitively against other metrics. Noticeably, we achieved second best results for Human-targeted Translation Edit Rate (HTER) asses"
D12-1090,J93-2004,0,0.0384176,"9) that 6 The baseline metric (e.g., BLEU) computes its raw score by taking the geometric mean of n-gram precision scores (1 ≤ n ≤ 4) scaled by a brevity penalty. The regression model learns to combine these fine-grained scores more intelligently, by optimizing their weights to regress human judgments. See Pado et al. (2009) for more discussion. combine features from multiple MT evaluation metrics (MT), as well as rich linguistic features from a textual entailment system (RTE). In all of our experiments, each reference and system translation sentence pair is tokenized using the Penn Treebank (Marcus et al., 1993) tokenization script, and lemmatized by the Porter Stemmer (Porter, 1980). For the overall sentence structure experiment, translations are additionally part-of-speech tagged with MXPOST tagger (Ratnaparkhi, 1996), and parsed with MSTParser (McDonald et al., 2005) 7 labeled dependency parser. Statistical significance tests are performed using the paired bootstrap resampling method (Koehn, 2004). We divide our experiments into two sections, based on two different prediction tasks — predicting absolute scores and predicting pairwise preference. 4.1 Exp. 1: Predicting Absolute Scores The first tas"
D12-1090,P05-1012,0,0.0244773,"their weights to regress human judgments. See Pado et al. (2009) for more discussion. combine features from multiple MT evaluation metrics (MT), as well as rich linguistic features from a textual entailment system (RTE). In all of our experiments, each reference and system translation sentence pair is tokenized using the Penn Treebank (Marcus et al., 1993) tokenization script, and lemmatized by the Porter Stemmer (Porter, 1980). For the overall sentence structure experiment, translations are additionally part-of-speech tagged with MXPOST tagger (Ratnaparkhi, 1996), and parsed with MSTParser (McDonald et al., 2005) 7 labeled dependency parser. Statistical significance tests are performed using the paired bootstrap resampling method (Koehn, 2004). We divide our experiments into two sections, based on two different prediction tasks — predicting absolute scores and predicting pairwise preference. 4.1 Exp. 1: Predicting Absolute Scores The first task is to evaluate a system translation on a seven point Likert scale against a single reference. Higher scores indicate translations that are closer to the meaning intended by the reference. Human ratings in the form of absolute scores are available for standard e"
D12-1090,P03-1021,0,0.014861,"ecent advances of statistical machine translation (SMT) systems. The early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches (Papineni et al., 2002; Doddington, 2002). Despite their simplicity, these measures have shown good correlation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation (CallisonBurch et al. (2009; 2010; 2011), inter alia). Recent studies have also confirmed that tuning MT systems against better MT metrics — using algorithms like MERT (Och, 2003) — leads to better system performance (He and Way, 2009; Liu et al., 2011). Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al., 1990), and paraphrasing (Snover et al., 2009; Denkowski and Lavie, 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art cor"
D12-1090,P02-1040,0,0.0927379,"orate a rich set of linguistic features, and automatically learn their weights, eliminating the need for ad-hoc parameter tuning. Our methods achieve state-of-the-art correlation with human judgments on two different prediction tasks across a diverse set of standard evaluations (NIST OpenMT06,08; WMT0608). 1 Introduction Research in automatic machine translation (MT) evaluation metrics has been a key driving force behind the recent advances of statistical machine translation (SMT) systems. The early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches (Papineni et al., 2002; Doddington, 2002). Despite their simplicity, these measures have shown good correlation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation (CallisonBurch et al. (2009; 2010; 2011), inter alia). Recent studies have also confirmed that tuning MT systems against better MT metrics — using algorithms like MERT (Och, 2003) — leads to better system performance (He and Way, 2009; Liu et al., 2011). Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources lik"
D12-1090,P00-1061,0,0.078122,"Missing"
D12-1090,R11-1092,0,0.0253454,"ting. The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallum et al., 2005; Bernard et al., 2008; Wang and Manning, 2010; Emms, 2012). In particular, our pFSM model and the log-linear parameterization were inspired by Wang and Manning (2010). Another body of literature that is closely related to this work is FSM models for word alignment (Vogel et al., 1996; Saers et al., 2010; Berg-Kirkpatrick et al., 2010). The stochastic Inversion Transduction Grammar in Saers et al. (2010) for instance, is a pFSM with special constraints. More recently, Saers and Wu (2011) further explored the connection between Linear Transduction Grammars and FSMs. There is a close tie between our pFSM model and the HMM model in Berg-Kirkpatrick et al. (2010). Both models adopted a log-linear parameterization for the state transition distribution, 13 but in their case the HMM model and the pFSM arc weights are normalized locally, and the objective is non-convex. 6 Conclusion We described a probabilistic finite state machine based on string edits and a novel pushdown automaton extension for the task of machine translation evaluation. The models admit a rich set of linguistic f"
D12-1090,N10-1050,0,0.0212737,"a of using extended edit distance models with block movements was also explored in Leusch et al. (2003). However, their model is largely empirical and not in a probabilistic learning setting. The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallum et al., 2005; Bernard et al., 2008; Wang and Manning, 2010; Emms, 2012). In particular, our pFSM model and the log-linear parameterization were inspired by Wang and Manning (2010). Another body of literature that is closely related to this work is FSM models for word alignment (Vogel et al., 1996; Saers et al., 2010; Berg-Kirkpatrick et al., 2010). The stochastic Inversion Transduction Grammar in Saers et al. (2010) for instance, is a pFSM with special constraints. More recently, Saers and Wu (2011) further explored the connection between Linear Transduction Grammars and FSMs. There is a close tie between our pFSM model and the HMM model in Berg-Kirkpatrick et al. (2010). Both models adopted a log-linear parameterization for the state transition distribution, 13 but in their case the HMM model and the pFSM arc weights are normalized locally, and the objective is non-convex. 6 Conclusion We described a pr"
D12-1090,2006.amta-papers.25,0,0.315687,"dependency structure, in order to test the hypothesis that modeling overall sentence structure can lead to more accurate evaluation measures. We conducted extensive experiments on a diverse set of standard evaluation data sets (NIST OpenMT06, 08; WMT06, 07, 08). Our model achieves or surpasses state-of-the-art results on all test sets. 2 and the state transition sequence of the previous state and the current state. The feature weights are then automatically learned by training a global regression model where some translational equivalence judgment score (e.g., human assessment score, or HTER (Snover et al., 2006)) for each sys and ref translation pair is the regression target (y). ˆ We introduce a new regression variable y ∈ R which is the log-sum of the unnormalized weights (Eqn. (1)) of all edit sequences, formally expressed as: pFSMs for MT Regression 0 |e | We start off by framing the problem of machine translation evaluation in terms of weighted edit distances calculated using probabilistic finite state machines (pFSMs). A FSM defines a language by accepting a string of input tokens in the language, and rejecting those that are not. A probabilistic FSM defines the probability that a string is in"
D12-1090,W09-0441,0,0.0553838,"Missing"
D12-1090,P08-3005,0,0.0186914,"-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al., 1990), and paraphrasing (Snover et al., 2009; Denkowski and Lavie, 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Kulesza and Shieber, 2004; Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al., 2008; Pado et al., 2009). The drawback, however, is they rely on time-consuming preprocessing modules to extract linguistic features (e.g., a full end-toend textual entailment system was needed in Pado et al. (2009)), which severely limits their practical use. Furthermore, these models employ a large number of features (on the order of hundreds), and consequently make the model predictions opaque and hard to analyze. In this paper, we propose a simple yet powerful probabilistic Finite State Machine (pFSM) for the task of MT evaluation. It is built on the backbone of weighted edit distance models,"
D12-1090,C96-2141,0,0.165702,"l since. 992 The idea of using extended edit distance models with block movements was also explored in Leusch et al. (2003). However, their model is largely empirical and not in a probabilistic learning setting. The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallum et al., 2005; Bernard et al., 2008; Wang and Manning, 2010; Emms, 2012). In particular, our pFSM model and the log-linear parameterization were inspired by Wang and Manning (2010). Another body of literature that is closely related to this work is FSM models for word alignment (Vogel et al., 1996; Saers et al., 2010; Berg-Kirkpatrick et al., 2010). The stochastic Inversion Transduction Grammar in Saers et al. (2010) for instance, is a pFSM with special constraints. More recently, Saers and Wu (2011) further explored the connection between Linear Transduction Grammars and FSMs. There is a close tie between our pFSM model and the HMM model in Berg-Kirkpatrick et al. (2010). Both models adopted a log-linear parameterization for the state transition distribution, 13 but in their case the HMM model and the pFSM arc weights are normalized locally, and the objective is non-convex. 6 Conclusi"
D12-1090,C10-1131,1,0.844807,"lace among 15 different sites and 7th place among 25 different metrics, averaged across 9 assessment types. 12 Unfortunately the version we submitted in 2010 was plagued with a critical bug. More general enhancements have been made to the model since. 992 The idea of using extended edit distance models with block movements was also explored in Leusch et al. (2003). However, their model is largely empirical and not in a probabilistic learning setting. The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallum et al., 2005; Bernard et al., 2008; Wang and Manning, 2010; Emms, 2012). In particular, our pFSM model and the log-linear parameterization were inspired by Wang and Manning (2010). Another body of literature that is closely related to this work is FSM models for word alignment (Vogel et al., 1996; Saers et al., 2010; Berg-Kirkpatrick et al., 2010). The stochastic Inversion Transduction Grammar in Saers et al. (2010) for instance, is a pFSM with special constraints. More recently, Saers and Wu (2011) further explored the connection between Linear Transduction Grammars and FSMs. There is a close tie between our pFSM model and the HMM model in Berg-Kirk"
D12-1090,W06-1610,0,0.0234615,"the version of the pPDA+f model trained on the WMT07 dataset to the “into English” segment-leval track of the WMT 2012 Shared Evaluation Metrics Task (Callison-Burch et al., 2012). Our model achieved the highest score (measured by Kendall’s tau correlation) on all four language pairs (Fr-En, De-En, Es-En and Cs-En), and tied for the first place with METEOR v1.3 on average correlation. 5 Related Work Features and Representation One of the findings in our experimentation is that paraphrasing helps boosting model accuracy, and the idea of using paraphrases in MT evaluation was first proposed by Zhou et al. (2006). Several recent studies have introduced metrics over dependency parses (Liu et al., 2005; Owczarzak et al., 2008; He et al., 2010), but their improvements over n-gram models at the sentence level are not always consistent (Liu et al., 2005; Peterson and Przybocki, 2010). Other than string-based methods, recent work has explored more alternative representations for MT evaluation, such as network properties (Amancio et al., 2011), semantic role structures (Lo and Wu, 2011), and the quality of word order (Birch and Osborne, 2011). Modeling MetricsMATR10 and WMT12 Results An earlier version of th"
D12-1090,2004.tmi-1.8,0,\N,Missing
D12-1090,W12-3102,0,\N,Missing
D12-1090,W09-0401,0,\N,Missing
D12-1090,P09-1034,1,\N,Missing
D12-1110,D10-1115,0,0.787239,"ith human judgments of word similarity (Griffiths et al., 2007). Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) 1201 Proceedings of the 2012 Joint Conference on Empir"
D12-1110,W06-1670,0,0.0811849,"Missing"
D12-1110,W11-0112,0,0.0537427,"th no relation to each other. There have been many attempts at automatically parsing natural language to a logical form using recursive compositional rules. Conversely, vector space models have the attractive property that they can automatically extract knowledge from large corpora without supervision. Unlike logic-based approaches, these models allow us to make fine-grained statements about the semantic similarity of words which correlate well with human judgments (Griffiths et al., 2007). Logic-based approaches are often seen as orthogonal to distributional vector-based approaches. However, Garrette et al. (2011) recently introduced a combination of a vector space model inside a Markov Logic Network. One open question is whether vector-based models can learn some of the simple logic encountered in language such as negation or conjunctives. To this end, we illustrate in a simple example that our MV-RNN model and its learned word matrices (operators) have the ability to learn propositional logic operators such as ∧, ∨, ¬ (and, or, not). This is a necessary (though not sufficient) condition for the ability to pick up these phenomena in real datasets 1207 and tasks such as sentiment detection which we foc"
D12-1110,D11-1129,0,0.0583693,"Missing"
D12-1110,S10-1006,0,0.0179044,"Missing"
D12-1110,P03-1054,1,0.0608372,"truct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2 www.socher.org Method Tree-CRF (Nakagawa et al., 2010) RAE (Socher et al., 2011c) Linear MVR MV-RNN Acc. 77.3 77.7 77.1 79.0 MV-RNN for Relationship Classification … … Table 1: Accuracy of classification on full length movie review polarity (MR). S. C. √ 1 √ 0 0 √ 0 x 1 x Review sentence The film is bright and flashy in all the right ways. Not always too whimsical for its own good this strange hybrid of crime thriller, quirky character study, third-rate roman"
D12-1110,P98-2127,0,0.058322,"Missing"
D12-1110,N10-1120,0,0.357826,"s a training error of 0. Hence, the output of these 6 examples has exactly one of the truth representations, making it recursively compatible with further combinations of operators. Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2 www.socher.org Method Tree-CRF (Nakagawa et al., 2010) RAE (Socher et al., 2011c) Linear MVR MV-RNN Acc. 77.3 77.7 77.1 79.0 MV-RNN for Relationship Classification … … Table 1: Accuracy of classification on full length movie review po"
D12-1110,P06-1102,0,0.0491241,"Missing"
D12-1110,J07-2002,0,0.0182724,"Missing"
D12-1110,P05-1015,0,0.329341,"imizer quickly yields a training error of 0. Hence, the output of these 6 examples has exactly one of the truth representations, making it recursively compatible with further combinations of operators. Thus, we can combine these operators to construct any propositional logic function of any number of inputs (including xor). Hence, this MV-RNN is complete in terms of propositional logic. 4 Predicting Movie Review Ratings In this section, we analyze the model’s performance on full length sentences. We compare to previous state of the art methods on a standard benchmark dataset of movie reviews (Pang and Lee, 2005; Nakagawa et al., 2010; Socher et al., 2011c). This dataset consists of 10,000 positive and negative single sentences describing movie sentiment. In this and the next experiment we use binarized trees from the Stanford Parser (Klein and Manning, 2003). We use the exact same setup and parameters (regularization, word vector size, etc.) as the published code of Socher et al. (2011c).2 2 www.socher.org Method Tree-CRF (Nakagawa et al., 2010) RAE (Socher et al., 2011c) Linear MVR MV-RNN Acc. 77.3 77.7 77.1 79.0 MV-RNN for Relationship Classification … … Table 1: Accuracy of classification on full"
D12-1110,P11-1138,0,0.246925,"1: A recursive neural network which learns semantic vector representations of phrases in a tree structure. Each word and phrase is represented by a vector and a matrix, e.g., very = (a, A). The matrix is applied to neighboring vectors. The same function is repeated to combine the phrase very good with movie. Introduction Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (Jones et al., 2006), fact extraction for information retrieval (Pas¸ca et al., 2006) and automatic annotation of text with disambiguated Wikipedia links (Ratinov et al., 2011), among many others (Turney and Pantel, 2010). In these models the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words. Such vectors have been shown to correlate well with human judgments of word similarity (Griffiths et al., 2007). Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege,"
D12-1110,S10-1057,0,0.412789,"Missing"
D12-1110,P10-1093,0,0.0142925,"Missing"
D12-1110,J98-1004,0,0.0537195,"Missing"
D12-1110,D11-1014,1,0.651216,"cess, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) 1201 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c La"
D12-1110,D11-1016,0,0.741154,"al., 2007). Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) 1201 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Compu"
D12-1110,C10-1142,0,0.639488,"imilarity (Griffiths et al., 2007). Despite their success, single word vector models are severely limited since they do not capture compositionality, the important quality of natural language that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them (Frege, 1892). This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences. Recently, there has been much progress in capturing compositionality in vector spaces, e.g., (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2011c) (see related work). We extend these approaches with a more general and powerful model of semantic composition. We present a novel recursive neural network model for semantic compositionality. In our context, compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length. Fig. 1 shows an illustration of the model in which each constituent (a word or longer phrase) has a matrix-vector (MV) 1201 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural"
D12-1110,J10-4006,0,\N,Missing
D12-1110,C98-2122,0,\N,Missing
D12-1110,D13-1170,1,\N,Missing
D13-1117,1993.eamt-1.1,0,0.444029,"Missing"
D13-1117,P05-1045,1,0.0174749,"linear-chain CRFs on two sequence tagging tasks: the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and the SANCL 2012 POS tagging task (Petrov and McDonald, 2012) . The standard CoNLL-2003 English shared task benchmark dataset (Tjong Kim Sang and De Meulder, 2003) is a collection of documents from Reuters newswire articles, annotated with four entity types: Person, Location, Organization, and Miscellaneous. We predicted the label sequence Y = {LOC, MISC, ORG, PER, O}T without considering the BIO tags. For training the CRF model, we used a comprehensive set of features from Finkel et al. (2005) that gives state-of-the-art results on this task. A total number of 437906 features were generated on the CoNLL-2003 training dataset. The most important features are: • The word, word shape, and letter n-grams (up to 6gram) at current position • The prediction, word, and word shape of the previous and next position • Previous word shape in conjunction with current word shape • Disjunctive word set of the previous and next 4 positions • Capitalization pattern in a 3 word window • Previous two words in conjunction with the word shape of the previous word • The current word matched against a li"
D13-1117,P06-1027,0,0.0803156,"Missing"
D13-1117,P05-1003,0,0.0148756,"rediction, without actually having to generate noised data. We focus on dropout noise (Hinton et al., 2012), a recently popularized form of artificial feature noise where a random subset of features is omitted independently for each training example. Dropout and its variants have been shown to outperform L2 regularization on various tasks (Hinton et al., 2012; Wang and Manning, 2013; Wan et al., 2013). Dropout is is similar in spirit to feature bagging in the deliberate removal of features, but performs the removal in a preset way rather than randomly (Bryll et al., 2003; Sutton et al., 2005; Smith et al., 2005). 1170 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1170–1179, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Our approach is based on a second-order approximation to feature noising developed among others by Bishop (1995) and Wager et al. (2013), which allows us to convert dropout noise into a form of adaptive regularization. This method is suitable for structured prediction in log-linear models where second derivatives are computable. In particular, it can be used for multiclass classification wi"
D13-1117,W03-0419,0,0.0238872,"Missing"
D13-1141,D10-1005,0,0.03299,"nd Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collober"
D13-1141,N10-1080,1,0.345132,"onolingual, but significantly better than Align-Init (as in Section3.2.1) on the NER task. 4.3 Vector matching alignment Translation equivalence of the bilingual embeddings is evaluated by naive word alignment to match word embeddings by cosine distance.5 The Alignment Error Rates (AER) reported in Table 3 suggest that bilingual training using Equation 5 produces embeddings with better translation equivalence compared to those produced by monolingual training. 4.4 Phrase-based machine translation Our experiments are performed using the Stanford Phrasal phrase-based machine translation system (Cer et al., 2010). In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years. In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results. We use Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoder. In the phrase-based MT system, we add one feature to bilingual phrase-pairs. For each phrase, the word embeddings are averaged to obtain a feature vector. If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; i"
D13-1141,W09-0439,0,0.00837207,"Missing"
D13-1141,P13-1031,1,0.225345,"Missing"
D13-1141,N06-2015,0,0.261157,"dings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset (Hovy et al., 2006) with a neural network model. We apply the bilingual embeddings in an end-toend phrase-based MT system by computing semantic similarities between phrase pairs. On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system. 1393 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Review of prior work Distributed word representations are u"
D13-1141,P12-1092,1,0.809739,"s across languages. The Fr-En phrase-pair {‘un cas de force majeure’, ‘case of absolute necessity’}, Zh-En phrase pair {‘依然故我’,‘persist in a stubborn manner’} are similar in semantics. If cooccurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase. We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity"
D13-1141,S12-1049,0,0.294238,"nsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improvement in Named Entity Recognition on the OntoNotes dataset (Hovy et al., 2006) with a neural network model. We apply the bilingual embeddings in an end-toend phrase-based MT system by computing semantic similarities between phrase pairs. On NIST08 Chinese-English translation task, we obtain an improvement of 0.48 BLEU from a competitive baseline (30.01 BLEU to 30.49 BLEU) with the Stanford Phrasal MT system. 1393 Proceedings of the 2013 Conference o"
D13-1141,N03-1017,0,0.0720823,"Missing"
D13-1141,N12-1005,0,0.00587547,"ements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). Given a context window c in a document d, the optimization minimizes the following Context Objective for a word w in the"
D13-1141,N06-1014,0,0.0184131,"tions, e.g. ‘lake’ and the Chinese word ‘潭’ (deep pond), their semantic proximity could be correctly quantified. We describe in the next sub-sections the methods to intialize and train bilingual embeddings. These methods ensure that bilingual embeddings retain their translational equivalence while their distributional semantics are improved during online training with a monolingual corpus. 3.2.1 Initialization by MT alignments First, we use MT Alignment counts as weighting to initialize Chinese word embeddings. In our experiments, we use MT word alignments extracted with the Berkeley Aligner (Liang et al., 2006) 1 . Specifically, we use the following equation to compute starting word embeddings: Wt-init = S X Cts + 1 s=1 Algorithm and methods JCO = Here f is a function defined by a neural network. wr is a word chosen in a random subset VR of the r vocabulary, and cw is the context window containing word wr . This unsupervised objective function contrasts the score between when the correct word is placed in context with when a random word is placed in the same context. We incorporate the global context information as in Huang et al. (2012), shown to improve performance of word embeddings. r max(0, 1 −"
D13-1141,W13-3512,1,0.24197,"and apply word embeddings using continuous models for language. Collobert et al. (2008) learn embeddings in an unsupervised manner through a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve compe"
D13-1141,P11-1015,0,0.289194,"Missing"
D13-1141,P03-1021,0,0.0270902,"g using Equation 5 produces embeddings with better translation equivalence compared to those produced by monolingual training. 4.4 Phrase-based machine translation Our experiments are performed using the Stanford Phrasal phrase-based machine translation system (Cer et al., 2010). In addition to NIST08 training data, we perform phrase extraction, filtering and phrase table learning with additional data from GALE MT evaluations in the past 5 years. In turn, our baseline is established at 30.01 BLEU and reasonably competitive relative to NIST08 results. We use Minimum Error Rate Training (MERT) (Och, 2003) to tune the decoder. In the phrase-based MT system, we add one feature to bilingual phrase-pairs. For each phrase, the word embeddings are averaged to obtain a feature vector. If a word is not found in the vocabulary, we disregard and assume it is not in the phrase; if no word is found in a phrase, a zero vector is assigned 4 Due to variations caused by online minibatch L-BFGS, we take embeddings from five random points out of last 105 minibatch iterations, and average their semantic similarity results. 1396 5 This is evaluated on 10,000 randomly selected sentence pairs from the MT training s"
D13-1141,P06-1102,0,0.021416,"Missing"
D13-1141,N10-1135,0,0.126436,"Missing"
D13-1141,N10-1013,0,0.320906,"Missing"
D13-1141,D11-1014,1,0.274729,"Missing"
D13-1141,P00-1054,0,0.281177,"a contrastive estimation technique. Mnih and Hinton ( 2008), Morin and Bengio ( 2005) proposed efficient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training wi"
D13-1141,P07-1066,0,0.0208964,"icient hierarchical continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). G"
D13-1141,P10-1040,0,0.914221,"semantic similarities across languages. The Fr-En phrase-pair {‘un cas de force majeure’, ‘case of absolute necessity’}, Zh-En phrase pair {‘依然故我’,‘persist in a stubborn manner’} are similar in semantics. If cooccurrences of exact word combinations are rare in the training parallel text, it can be difficult for classical statistical MT methods to identify this similarity, or produce a reasonable translation given the source phrase. We introduce an unsupervised neural model to learn bilingual semantic embedding for words across two languages. As an extension to their monolingual counter-part (Turian et al., 2010; Huang et al., 2012; Bengio et al., 2003), bilingual embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. This propConsequently, we produce for the research community a first set of Mandarin Chinese word embeddings with 100,000 words trained on the Chinese Gigaword corpus. We evaluate these embedding on Chinese word semantic similarity from SemEval2012 (Jin and Wu, 2012). The embeddings significantly out-perform prior work and pruned tf-idf base-lines. In addition, the learned embeddings give rise to 0.11 F1 improve"
D13-1141,P13-1106,1,0.321038,"trained embeddings4 out-perform pruned tf-idf by 14.1 and 12.6 Spearman Correlation (×100), respectively. Further, they out-perform embeddings initialized from alignment by 7.9 and 6.4. Both our tf-idf implementation and the word embeddings have significantly higher Kendall’s Tau value compared to Prior work (Jin and Wu, 2012). We verified Tau calculations with original submissions provided by the authors. 4.2 Named Entity Recognition We perform NER experiments on OntoNotes (v4.0) (Hovy et al., 2006) to validate the quality of the Chinese word embeddings. Our experimental setup is the same as Wang et al. (2013). With embeddings, we build a naive feed-forward neural network (Collobert et al., 2008) with 2000 hidden neurons and a sliding window of five words. This naive setting, without sequence modeling or sophisticated Embeddings Align-Init Mono-trained Biling-trained Prec. 0.34 0.54 0.48 Rec. 0.52 0.62 0.55 F1 0.41 0.58 0.52 Improve 0.17 0.11 Table 3: Vector Matching Alignment AER (lower is better) Embeddings Mono-trained Biling-trained Prec. 0.27 0.37 Rec. 0.32 0.45 AER 0.71 0.59 join optimization, is not competitive with state-ofthe-art (Wang et al., 2013). Table 2 shows that the bilingual embedd"
D13-1141,P01-1067,0,0.0161864,"Missing"
D13-1141,P06-2124,0,0.00531532,"l continuous-space models. To systematically compare embeddings, Turian et al. (2010) evaluated improvements they bring to state-of-theart NLP benchmarks. Huang et al. (2012) introduced global document context and multiple word prototypes. Recently, morphology is explored to learn better word representations through Recursive Neural Networks (Luong et al., 2013). Bilingual word representations have been explored with hand-designed vector space models (Peirsman and Pad´o , 2010; Sumita, 2000), and with unsupervised algorithms such as LDA and LSA (Boyd-Graber and Resnik, 2010; Tam et al., 2007; Zhao and Xing, 2006). Only recently have continuous space models been applied to machine translation (Le et al., 2012). Despite growing interest in these models, little work has been done along the same lines to train bilingual distributioned word represenations to improve machine translation. In this paper, we learn bilingual word embeddings which achieve competitive performance on semantic word similarity, and apply them in a practical phrase-based MT system. 3 3.1 Unsupervised training with global context Our method starts with embedding learning formulations in Collobert et al. (2008). Given a context window"
D13-1141,C12-1089,0,\N,Missing
D13-1170,J10-4006,0,0.113389,"junction ‘but’ dominates. The complete training and testing code, a live demo and the Stanford Sentiment Treebank dataset are available at http://nlp.stanford.edu/ sentiment. 2 Related Work This work is connected to five different areas of NLP research, each with their own large amount of related work to which we cannot do full justice given space constraints. Semantic Vector Spaces. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks"
D13-1170,D08-1094,0,0.140304,"Missing"
D13-1170,D11-1129,0,0.143203,"ions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector RNNs (Socher et al., 2012) both of which have been applied to bag of words sentiment corpora. Logical Form. A related field that tackles compositionality from a very different angle is that of trying to map sentences to logical form (Zettlemoyer and Coll"
D13-1170,W13-0112,0,0.126107,"data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dat"
D13-1170,P12-1092,1,0.167577,"used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-c"
D13-1170,P03-1054,1,0.146647,"ll and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena. Fig. 1 shows one of the many examples with clear compositional structure. The granularity and size of 1631 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics this dataset will enable the community to train compositional models that a"
D13-1170,N10-1120,0,0.13649,"Missing"
D13-1170,J07-2002,0,0.140631,"/ sentiment. 2 Related Work This work is connected to five different areas of NLP research, each with their own large amount of related work to which we cannot do full justice given space constraints. Semantic Vector Spaces. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Variants of this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus."
D13-1170,P05-1015,0,0.37586,"he meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges. This new dataset allows us to analyze the intricacies of sentiment and to capture complex linguistic phenomena. Fig. 1 shows one of the many examples with clear compositional structure. The granularity and size of 1631 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, c Seattle, Washington, USA, 18-21 O"
D13-1170,rentoumi-etal-2010-united,0,0.131845,"Missing"
D13-1170,P10-1093,0,0.139526,"her et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Giesbrecht, 2010) have not been experimentally validated on larger corpora. Yessenalina and Cardie (2011) compute matrix representations for longer phrases and define composition as matrix multiplication, and also evaluate on sentiment. Grefenstette and Sadrzadeh (2011) analyze subject-verbobject triplets and find a matrix-based categorical model to correlate well with human judgments. We compare to the recent line of work on supervised compositional models. In particular we will describe and experimentally compare our new RNTN model to recursive neural networks (RNN) (Socher et al., 2011b) and matrix-vector R"
D13-1170,N07-1038,0,0.134233,"by Bottou (2011) and Hinton (1990) and first models such as Recursive Auto-associative memories been experimented with by Pollack (1990). The idea to relate inputs through three way interactions, parameterized by a tensor have been proposed for relation classification (Sutskever et al., 2009; Jenatton et al., 2012), extending Restricted Boltzmann machines (Ranzato and Hinton, 2010) and as a special layer for speech recognition (Yu et al., 2012). Sentiment Analysis. Apart from the abovementioned work, most approaches in sentiment analysis use bag of words representations (Pang and Lee, 2008). Snyder and Barzilay (2007) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants, such as food or atmosphere. Several works have explored sentiment compositionality through careful engineering of features or polarity shifting rules on syntactic structures (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Rentoumi et al., 2010; Nakagawa et al., 2010). 3 Stanford Sentiment Treebank Bag of words classifiers can work well in longer documents by relying on a few words with strong sentiment like ‘awesome’ or ‘exhilarating.’ However, sentiment accuracies even for binary posit"
D13-1170,D11-1014,1,0.293141,"this idea use more complex frequencies such as how often a 1632 word appears in a certain syntactic context (Pado and Lapata, 2007; Erk and Pad´o, 2008). However, distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts. One possibility to remedy this is to use neural word vectors (Bengio et al., 2003). These vectors can be trained in an unsupervised fashion to capture distributional similarities (Collobert and Weston, 2008; Huang et al., 2012) but then also be fine-tuned and trained to specific tasks such as sentiment detection (Socher et al., 2011b). The models in this paper can use purely supervised word representations learned entirely on the new corpus. Compositionality in Vector Spaces. Most of the compositionality algorithms and related datasets capture two word compositions. Mitchell and Lapata (2010) use e.g. two-word phrases and analyze similarities computed by vector addition, multiplication and others. Some related models such as holographic reduced representations (Plate, 1995), quantum logic (Widdows, 2008), discrete-continuous models (Clark and Pulman, 2007) and the recent compositional matrix space model (Rudolph and Gies"
D13-1170,D12-1110,1,0.209782,"na presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse trees, each annotated by"
D13-1170,P12-3020,0,0.239464,"Missing"
D13-1170,D11-1016,0,0.210149,"capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique phrases from those parse tree"
D13-1170,C10-1142,0,0.126738,". models to accurately capture the underlying phenomena presented in such data. To address this need, we introduce the Stanford Sentiment Treebank and a powerful Recursive Neural Tensor Network that can accurately predict the compositional semantic effects present in this new corpus. Introduction Semantic vector spaces for single words have been widely used as features (Turney and Pantel, 2010). Because they cannot capture the meaning of longer phrases properly, compositionality in semantic vector spaces has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Zanzotto et al., 2010; Yessenalina and Cardie, 2011; Socher et al., 2012; Grefenstette et al., 2013). However, progress is held back by the current lack of large and labeled compositionality resources and The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser (Klein and Manning, 2003) and includes a total of 215,154 unique"
D13-1170,R09-1048,0,\N,Missing
D13-1177,D08-1073,0,0.65788,"ens # of events # of non-N ONE relations Avg 3.80 89.98 6.20 5.64 Min 1 19 2 1 Max 15 319 15 24 Table 1: Process statistics over 148 process descriptions. N ONE is used to indicate no relation. relation set R. We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible. For instance, we include the S UPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al., 2003) and Allen’s work, but in practice was not considered by many temporal ordering systems (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). Importantly, our relation set also includes the relations C AUSES and E NABLES, which are fundamental to modeling processes and go beyond simple temporal ordering. We also added event coreference (S AME) to R. Do et al. (2012) used event coreference information in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference. In this paper, we simply treat S AME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference"
D13-1177,P11-1098,0,0.14816,"and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are given a paragraph and have a “one-shot” chance to extract the process structure. We showed in this paper that global structural properties lead to significant improvements in extraction accuracy, and ILP is an effective framework CAUSES CAUSES shifts CA"
D13-1177,N13-1104,0,0.0243669,"ons between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one another and their semantic roles are extracted. Recently, Cheung et al. (2013) proposed an unsupervised generative model for inducing such templates. A major difference in our work is that we do not learn typical event relations from a large and redundant corpus, but are given a paragraph and have a “one-shot” chance to extract the process structure. We showed in this paper that global structural properties lead to significant improvements in extraction accuracy, and ILP is an effective framework CAUSES CAUSES shifts CAUSES skipped CAUSES deleted COTEMP secrete PREV CAUSES CAUSES CAUSES used duplicated bind CAUSES PREV ENABLES SAME NONE binds Figure 3: Process graph fra"
D13-1177,de-marneffe-etal-2006-generating,1,0.126092,"Missing"
D13-1177,D12-1062,0,0.42201,"turing, economical developments, and various phenomena in life and social sciences can all be viewed as types of processes. Processes are complicated objects; consider for example the biological process of ATP synthesis described in Figure 1. This process involves 12 entities and 8 events. Additionally, it describes relations between events and entities, and the relationship between events (e.g., the second occurrence of the event ‘enter’, causes the event ‘changing’). ∗ Both authors equally contributed to the paper Timeline construction involves identifying temporal relations between events (Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sentences. However, events in processes are tightly coupled in ways that go beyond simple temporal ordering, and these dependencies are central for the process extraction task. Hence, capturing process structure requires modeling a larger set of relations that includes temporal, causal and co-reference relations. In this paper, we formally define the task of process extraction and present automatic extraction methods. Our approach handles an op"
D13-1177,N13-1112,0,0.0818844,"Missing"
D13-1177,P08-2012,1,0.92408,"fier and the gold standard disagree. We are interested in triads that never occur in training data but are predicted by the classifier, and vice versa. Figure 2 illustrates some of the triads found and Equations 8-12 provide the corresponding ILP formulations. Equations 8-10 were formulated as soft constraints (expanding the set K) and were incorporated by defining a reward αk for each triad type.3 On the other hand, Equations 11-12 were formulated as hard constraints to prevent certain structures. 1. S AME transitivity (Figure 2a, Eqn. 8): Coreference transitivity has been used in past work (Finkel and Manning, 2008) and we incorporate it by a constraint that encourages triads that respect transitivity. 2. C AUSE-C OTEMP (Figure 2b, Eqn. 9): If ti causes both tj and tk , then often tj and tk are co-temporal. E.g, in “genetic drift has led to a loss of genetic variation and an increase in the frequency of . . .”, a single event causes two subsequent events that occur simultaneously. 3. C OTEMP transitivity (Figure 2c, Eqn. 10): If ti is co-temporal with tj and tj is co-temporal with tk , then usually ti and tk are either cotemporal or denote the same event. 4. S AME contradiction (Figure 2d, Eqn. 11): If t"
D13-1177,W11-1801,0,0.226726,"hich exploits these structural properties by performing joint inference over the set of extracted relations. On a novel dataset containing 148 descriptions of biological processes (released with this paper), we show significant improvement comparing to baselines that disregard process structure. 1 Process extraction is related to two recent lines of work in Information Extraction – event extraction and timeline construction. Traditional event extraction focuses on identifying a closed set of events within a single sentence. For example, the BioNLP 2009 and 2011 shared tasks (Kim et al., 2009; Kim et al., 2011) consider nine events types related to proteins. In practice, events are currently almost always extracted from a single sentence. Process extraction, on the other hand, is centered around discovering relations between events that span multiple sentences. The set of possible event types in process extraction is also much larger. Introduction A process is defined as a series of inter-related events that involve multiple entities and lead to an end result. Product manufacturing, economical developments, and various phenomena in life and social sciences can all be viewed as types of processes. Pr"
D13-1177,P03-1054,1,0.0154475,"gy” and marking any contiguous sequence of sentences that describes a process, i.e., a series of events that lead towards some objective. Then, each process description was annotated by a biologist. The annotator was first presented with annotation guidelines and annotated 20 descriptions. The annotations were then discussed with the authors, after which all process descriptions were annotated. After training a second biologist, we measured inter-annotator agreement κ = 0.69, on 30 random process descriptions. Process descriptions were parsed with Stanford constituency and dependency parsers (Klein and Manning, 2003; de Marneffe et al., 2006), and 35 process descriptions were set aside as a test set (number of training set trigger pairs: 1932, number of test set trigger pairs: 906). We performed 10fold cross validation over the training set for feature selection and tuning of constraint parameters. For each constraint type (connectivity, chain-structure, and five triad constraints) we introduced a parameter and tuned the seven parameters by coordinatewise ascent, where for hard constraints a binary parameter controls whether the constraint is used, and for soft constraints we attempted 10 different rewar"
D13-1177,P09-1039,0,0.019495,"for a relation r between the trigger pair (ti , tj ) (e.g., θijr = log pijr ), and yijr be the corresponding indicator variable. Our goal is to find an assignment for the indicators y = {yijr |1 ≤ i &lt; j ≤ n, r ∈ R}. With no global constraints this can be formulated as the following ILP: arg max y X θijr yijr (1) ijr s.t.∀i,j X yijr = 1 r where the constraint ensures exactly one relation between each event pair. We now describe constraints that result in a coherent global process structure. Connectivity Our ILP formulation for enforcing connectivity is a minor variation of the one suggested by Martins et al. (2009) for dependency parsing. In our setup, we want P to be a connected undirected graph, and not a directed tree. However, an undirected graph P is connected iff there exists a directed tree that is a subgraph of P when edge directions are ignored. Thus the resulting formulation is almost identical and is based on flow constraints which ensure that there is a path from a designated root in the graph to all other nodes. ¯ be the set R  N ONE. An edge (ti , tj ) is Let R in E iff there is some non-N P ONE relation between ti and tj , i.e. iff yij := ¯ yijr is equal to 1. r∈R For each variable yij w"
D13-1177,D12-1080,1,0.93945,"l developments, and various phenomena in life and social sciences can all be viewed as types of processes. Processes are complicated objects; consider for example the biological process of ATP synthesis described in Figure 1. This process involves 12 entities and 8 events. Additionally, it describes relations between events and entities, and the relationship between events (e.g., the second occurrence of the event ‘enter’, causes the event ‘changing’). ∗ Both authors equally contributed to the paper Timeline construction involves identifying temporal relations between events (Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and is thus related to process extraction as both focus on event-event relations spanning multiple sentences. However, events in processes are tightly coupled in ways that go beyond simple temporal ordering, and these dependencies are central for the process extraction task. Hence, capturing process structure requires modeling a larger set of relations that includes temporal, causal and co-reference relations. In this paper, we formally define the task of process extraction and present automatic extraction methods. Our approach handles an open set of event types and wo"
D13-1177,P11-1163,1,0.901643,"Missing"
D13-1177,N10-1123,0,0.0211054,"nalized for both the false negative of the correct class (just as before), and also for the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky ("
D13-1177,N12-1008,0,0.0193823,"the predictions of Global and Local are identical. Original text, Left: “... the template shifts . . . , and a part of the template strand is either skipped by the replication machinery or used twice as a template. As a result, a segment of DNA is deleted or duplicated.” Right: “Cells of mating type A secrete a signaling molecule, which can bind to specific receptor proteins on nearby cells. At the same time, cells secrete factor, which binds to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biol"
D13-1177,D11-1001,0,0.0317982,"the false positive of the predicted class. Despite that we see that Global improves overall performance by 3.7 F1 points on the test set. 5 Related Work A related line of work is biomedical event extraction in recent BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011). Earlier work employed a pipeline architecture where first events are found, and then their arguments are identified (Miwa et al., 2010; Bj¨orne et al., 2011). Subsequent methods predicted events and arguments jointly using Markov logic (Poon and Vanderwende, 2010) and dependency parsing algorithms (McClosky et al., 2011). Riedel and McCallum (2011) further improved performance by capturing correlations between events and enforcing consistency across arguments. Temporal event-event relations have been extensively studied (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Denis and Muller, 2011; Do et al., 2012; McClosky and Manning, 2012; D’Souza and Ng, 2013), and we leverage such techniques in our work (Section 3.1). However, we extend beyond temporal relations alone, and strongly rely on dependencies between process events. Chambers and Jurafsky (2011) learned event templates (or frames), where events that are related to one anothe"
D13-1177,D12-1131,0,0.0299692,"t the same time, cells secrete factor, which binds to receptors on A cells.” for modeling global constraints. Similar observations and techniques have been proposed in other information extraction tasks. Reichart and Barzilay (2012) tied information from multiple sequence models that describe the same event by using global higher-order potentials. Berant et al. (2011) proposed a global inference algorithm to identify entailment relations. There is an abundance of examples of enforcing global constraints in other NLP tasks, such as in coreference resolution (Finkel and Manning, 2008), parsing (Rush et al., 2012) and named entity recognition (Wang et al., 2013). 6 Conclusion Developing systems that understand process descriptions is an important step towards building applications that require deeper reasoning, such as biological process models from text, intelligent tutoring systems, and non-factoid QA systems. In this paper we have presented the task of process extraction, and developed methods for extracting relations between process events. Processes contain events that are tightly coupled through strong dependencies. We have shown that exploiting these structural dependencies and performing joint"
D13-1177,J11-2003,0,0.0141453,"iversity, Stanford Justin Lewis and Brittany Harding University of Washington, Seattle Peter Clark Allen Institute for Artificial Intelligence, Seattle Abstract Automatically extracting the structure of processes from text is crucial for applications that require reasoning, such as non-factoid QA. For instance, answering a question on ATP synthesis, such as “How do H+ ions contribute to the production of ATP?” requires a structure that links H+ ions (Figure 1, sentence 1) to ATP (Figure 1, sentence 4) through a sequence of intermediate events. Such “How?” questions are common on FAQ websites (Surdeanu et al., 2011), which further supports the importance of process extraction. Biological processes are complex phenomena involving a series of events that are related to one another through various relationships. Systems that can understand and reason over biological processes would dramatically improve the performance of semantic applications involving inference such as question answering (QA) – specifically “How?” and “Why?” questions. In this paper, we present the task of process extraction, in which events within a process and the relations between the events are automatically extracted from text. We rep"
D13-1177,J08-2002,1,0.92479,"e gold standard disagree. We are interested in triads that never occur in training data but are predicted by the classifier, and vice versa. Figure 2 illustrates some of the triads found and Equations 8-12 provide the corresponding ILP formulations. Equations 8-10 were formulated as soft constraints (expanding the set K) and were incorporated by defining a reward αk for each triad type.3 On the other hand, Equations 11-12 were formulated as hard constraints to prevent certain structures. 1. S AME transitivity (Figure 2a, Eqn. 8): Coreference transitivity has been used in past work (Finkel and Manning, 2008) and we incorporate it by a constraint that encourages triads that respect transitivity. 2. C AUSE-C OTEMP (Figure 2b, Eqn. 9): If ti causes both tj and tk , then often tj and tk are co-temporal. E.g, in “genetic drift has led to a loss of genetic variation and an increase in the frequency of . . .”, a single event causes two subsequent events that occur simultaneously. 3. C OTEMP transitivity (Figure 2c, Eqn. 10): If ti is co-temporal with tj and tj is co-temporal with tk , then usually ti and tk are either cotemporal or denote the same event. 4. S AME contradiction (Figure 2d, Eqn. 11): If t"
D13-1177,P09-1046,0,0.281107,"E relations Avg 3.80 89.98 6.20 5.64 Min 1 19 2 1 Max 15 319 15 24 Table 1: Process statistics over 148 process descriptions. N ONE is used to indicate no relation. relation set R. We chose a relation set R that captures the essential aspects of temporal relations between events in a process, while keeping the annotation as simple as possible. For instance, we include the S UPER relation that appears in temporal annotations such as the Timebank corpus (Pustejovsky et al., 2003) and Allen’s work, but in practice was not considered by many temporal ordering systems (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012). Importantly, our relation set also includes the relations C AUSES and E NABLES, which are fundamental to modeling processes and go beyond simple temporal ordering. We also added event coreference (S AME) to R. Do et al. (2012) used event coreference information in a temporal ordering task to modify probabilities provided by pairwise classifiers prior to joint inference. In this paper, we simply treat S AME as another event-event relation, which allows us to easily perform joint inference and employ structural constraints that combine both coreference and temporal relations"
D13-1177,W09-1401,0,\N,Missing
D13-1177,J12-1003,1,\N,Missing
D14-1059,P12-1092,1,0.0959804,"ogously. We define p(w) to be the “probability” of a concept – that is, the normalized frequency of a word w or any of its hyponyms in the Google N-Grams corpus (Brants and Franz, 2006). Intuitively, this ensures that relatively long paths through fine-grained sections of WordNet are not unduly penalized. For instance, the path from cat to animal traverses six intermediate nodes, na¨ıvely yielding a prohibitive search depth of 6. However, many of these transitions have low weight: for instance f↑cat→feline is only 0.37. For nearest neighbors edges, we take Neural Network embeddings learned in Huang et al. (2012) corresponding to each vocabulary entry. We then define fN Nw→w0 to be the arc cosine of the cosine similarity (i.e., the angle) between word vectors associated with lexical items w and w0 :   w · w0 fN Nw→w0 = arccos . kwkkw0 k For instance, fN Ncat→dog = 0.43. In practice, we explore the 100 nearest neighbors of each word. We can express fti as a feature vector by representing it as a vector with value fti at the index corresponding to (t, v, p) – the transition template, the validity of the inference, and the polarity of the mutated word. Note that the size of this vector mirrors the numb"
D14-1059,W13-3515,1,0.664716,"h a low confidence should translate to a probability of 12 , rather than a probability of 0. We therefore define the probability of validity as follows: We take v to be 1 if the query is in the valid state with respect to the premise, and −1 if the query is in the invalid state. For completeness, if no path is given we can set v = 0. The Note that setting both θ↑ and θ↓ to 1 exactly yields Formula (1) for JC distance. This, in addition to the inclusion of nearest neighbors as transitions, allows the search to capture the intuition that similar objects have similar properties (e.g., as used in Angeli and Manning (2013)). 4.4 Confidence Estimation Deletions in Inference Although inserting lexical items in a derivation (deleting words from the reversed derivation) is trivial, the other direction is not. For brevity, we refer to a deletion in the derivation as an insertion, since from the perspective of search we are inserting lexical items. 540 probability of validity becomes: v 1 p(valid) = + . 2 1 + evθ·f § 1 Quantifiers 2 Plurals 3 Anaphora 4 Ellipses 5 Adjectives 6 Comparatives 7 Temporal 8 Verbs 9 Attitudes Applicable (1,5,6) (2) Note that in the case where v = −1, the above expression reduces to 21 − co"
D14-1059,W13-2322,0,0.00454669,"imals is false given a database containing the cat ate a mouse (see Figure 1). These inferences are difficult to capture in a principled way while maintaining high recall, particularly for large scale open-domain tasks. Learned inference rules are difficult to generalize to arbitrary relations, and standard IR methods easily miss small but semantically important lexical differences. Furthermore, many methods require explicitly modeling either the database, the query, or both in a formal meaning representation (e.g., Freebase tuples). Although projects like the Abstract Meaning Representation (Banarescu et al., 2013) have made 534 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 534–545, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics supporting premise. Each transition along the search denotes a (reverse) inference step in Natural Logic, and incurs a cost reflecting the system’s confidence in the validity of that step. This approach offers two contributions over prior work in database completion: (i) it allows for unstructured text as the input database without any assumptions about the schema or domain of the text, a"
D14-1059,P14-1133,0,0.0094359,"Missing"
D14-1059,P11-1062,0,0.0606702,"Missing"
D14-1059,O97-1002,0,0.0416986,"re constructing a reversed derivation, and are therefore “traversing” the FSA backwards. This approach is efficient over a large database of 270 million entries without making use of explicit queries over the database; nor does the approach make use of any sort of approximate matching against the database, beyond lemmatizing individual lexical items. The motivation in prior work for approximate matches – to improve the recall of candidate premises – is captured elegantly by relaxing Natural Logic itself. We show that allowing invalid transitions with appropriate costs generalizes JC distance (Jiang and Conrath, 1997) – a common thesaurus-based similarity metric (Section 4.3). Importantly, however, the entire inference pipeline is done within the framework of weighted lexical transitions in Natural Logic. 4.1 Polarity Mutating operators can change the polarity on a span in the fact. Since we do not have the full parse tree at our disposal at search time, we track a small amount of metadata to guess the scope of the mutated operator. 4.2 We begin by introducing some terminology. A transition template is a broad class of transitions; for instance WordNet hypernymy. A transition (or transition instance) is a"
D14-1059,D13-1161,0,0.0442329,"Missing"
D14-1059,Q13-1015,0,0.08078,"Missing"
D14-1059,W07-1431,1,0.864142,"The logic builds upon traditional rather than first-order logic: to a first approximation, Natural Logic can be seen as an enhanced version of Aristotle’s syllogistic system (van Benthem, 2008). A working understanding of the logic as syllogistic reasoning is sufficient for understanding the later contributions of the paper. While some inferences of first-order logic are not captured by Natural Logic, it nonetheless allows for a wide range of intuitive inferences in a computationally efficient and conceptually clean way. We build upon the variant of the logic introduced by the NatLog system (MacCartney and Manning, 2007; 2008; 2009), based on earlier theoretical work on Natural Logic and Monotonicity Calculus (van Benthem, 1986; Valencia, 1991). Later work formalizes many aspects of the logic (Icard, 2012; Djalali, 2013); we adopt the formal semantics of Icard and Moss (2014), along with much of their notation. At a high level, Natural Logic proofs operate by mutating spans of text to ensure that the mutated sentence follows from the original – each step is much like a syllogistic inference. We construct a complete proof system in three parts: we define MacCartney’s atomic relations between lexical entries ("
D14-1059,C08-1066,1,0.945184,"− confidence; in the case where v = 0 it reduces to simply 12 . Furthermore, note that the probability of truth makes use of the same parameters as the cost in the search. 5 Count 44 24 6 25 15 16 36 8 9 75 Precision N M08 91 95 80 90 100 100 100 100 80 71 90 88 75 86 − 80 − 100 89 89 Recall N M08 100 100 29 64 20 60 5 5 66 83 100 89 53 71 0 66 0 83 94 94 N 95 38 33 28 73 87 52 25 22 89 Accuracy M07 M08 84 97 42 75 50 50 28 24 60 80 69 81 61 58 63 62 55 89 76 90 Table 4: Results on the FraCaS textual entailment suite. N is this work; M07 refers to MacCartney and Manning (2007); M08 refers to MacCartney and Manning (2008). The relevant sections of the corpus intended to be handled by this system are sections 1, 5, and 6 (although not 2 and 9, which are also included in M08). Learning Transition Costs We describe our procedure for learning the transition costs θ. Our training data D consists of query facts q and their associated gold truth values y. Equation (2) gives us a probability that a particular inference is valid; we axiomatically consider a valid inference from a known premise to be justification for the truth of the query. This is at the expense of the (often incorrect) assumption that our database is"
D14-1059,W09-3714,1,0.850055,"Missing"
D14-1059,D12-1048,0,0.0998507,"Missing"
D14-1059,D11-1142,0,0.174874,"Missing"
D14-1059,D13-1080,0,0.0273471,"Missing"
D14-1059,N13-1008,0,0.0666644,"Missing"
D14-1059,P11-1055,0,0.0685405,"Missing"
D14-1059,D10-1106,0,0.0353592,"Missing"
D14-1059,P06-1101,0,0.122241,"Missing"
D14-1059,D12-1042,1,0.381777,"Missing"
D14-1059,E09-1092,0,0.0614545,"Missing"
D14-1059,W12-3022,0,0.0602678,"Missing"
D14-1059,N07-4013,0,0.114106,"Missing"
D14-1059,D07-1071,0,0.0594017,"Missing"
D14-1082,D09-1127,0,0.0303187,", w denotes word, t denotes POS tag. given configuration. Information that can be obtained from one configuration includes: (1) all the words and their corresponding POS tags (e.g., has / VBZ); (2) the head of a word and its label (e.g., nsubj, dobj) if applicable; (3) the position of a word on the stack/buffer or whether it has already been removed from the stack. Conventional approaches extract indicator features such as the conjunction of 1 ∼ 3 elements from the stack/buffer using their words, POS tags or arc labels. Table 1 lists a typical set of feature templates chosen from the ones of (Huang et al., 2009; Zhang and Nivre, 2011).2 These features suffer from the following problems: • LEFT-ARC(l): adds an arc s1 → s2 with label l and removes s2 from the stack. Precondition: |s |≥ 2. • Sparsity. The features, especially lexicalized features are highly sparse, and this is a common problem in many NLP tasks. The situation is severe in dependency parsing, because it depends critically on word-to-word interactions and thus the high-order features. To give a better understanding, we perform a feature analysis using the features in Table 1 on the English Penn Treebank (CoNLL representations). The resul"
D14-1082,W07-2416,0,0.0443966,"Missing"
D14-1082,P08-1068,0,0.0249383,"y parsers has been very appealing. However, these parsers are not perfect. First, from a statistical perspective, these parsers suffer from the use of millions of mainly poorly estimated feature weights. While in aggregate both lexicalized features and higher-order interaction term features are very important in improving the performance of these systems, nevertheless, there is insufficient data to correctly weight most such features. For this reason, techniques for introducing higher-support features such as word class features have also been very successful in improving parsing performance (Koo et al., 2008). Second, almost all existing parsers rely on a manually designed set of feature templates, which require a lot 740 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Single-word features (9) s1 .w; s1 .t; s1 .wt; s2 .w; s2 .t; s2 .wt; b1 .w; b1 .t; b1 .wt Word-pair features (8) s1 .wt ◦ s2 .wt; s1 .wt ◦ s2 .w; s1 .wts2 .t; s1 .w ◦ s2 .wt; s1 .t ◦ s2 .wt; s1 .w ◦ s2 .w s1 .t ◦ s2 .t; s1 .t ◦ b1 .t Three-word feaures (8) s2 .t ◦ s1 .t ◦ b1 .t; s2 .t ◦ s1"
D14-1082,E06-1011,0,0.025396,"Missing"
D14-1082,de-marneffe-etal-2006-generating,1,0.0769947,"Missing"
D14-1082,P14-1129,0,0.0611131,"h step, we extract all the corresponding word, POS and label embeddings from the current configuration c, compute the hidden layer h(c) ∈ Rdh , and pick the transition with the highest score: t = arg maxt is feasible W2 (t, ·)h(c), and then execute c → t(c). Comparing with indicator features, our parser does not need to compute conjunction features and look them up in a huge feature table, and thus greatly reduces feature generation time. Instead, it involves many matrix addition and multiplication operations. To further speed up the parsing time, we apply a pre-computation trick, similar to (Devlin et al., 2014). For each position chosen from S w , we pre-compute matrix multiplications for most top frequent 10, 000 words. Thus, computing the hidden layer only requires looking up the table for these frequent words, and adding the dh -dimensional vector. Similarly, we also precompute matrix computations for all positions and all POS tags and arc labels. We only use this optimization in the neural network parser, but it is only feasible for a parser like the neural network parser which uses a small number of features. In practice, this pre-computation step increases the speed of our parser 8 ∼ 10 times."
D14-1082,nivre-etal-2006-maltparser,0,0.0231344,"Missing"
D14-1082,P13-1045,1,0.200137,"g a simple synchrony network to predict parse decisions in a constituency parser. More recently, (Titov and Henderson, 2007) applied Incremental Sigmoid Belief Networks to constituency parsing and then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There have been a number of recent uses of deep learning for constituency parsing (Collobert, 2011; Socher et al., 2013). (Socher et al., 2014) has also built models over dependency representations but this work has not attempted to learn neural networks for dependency parsing. Most recently, (Stenetorp, 2013) attempted to build recursive neural networks for transitionbased dependency parsing, however the empirical performance of his model is still unsatisfactory. • Different features have varied distributions of the weights. However, most of the discriminative weights come from W1t (the middle zone in Figure 6), and this further justifies the importance of POS tags in dependency parsing. • We carefully examine"
D14-1082,Q14-1017,1,0.094044,"twork to predict parse decisions in a constituency parser. More recently, (Titov and Henderson, 2007) applied Incremental Sigmoid Belief Networks to constituency parsing and then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There have been a number of recent uses of deep learning for constituency parsing (Collobert, 2011; Socher et al., 2013). (Socher et al., 2014) has also built models over dependency representations but this work has not attempted to learn neural networks for dependency parsing. Most recently, (Stenetorp, 2013) attempted to build recursive neural networks for transitionbased dependency parsing, however the empirical performance of his model is still unsatisfactory. • Different features have varied distributions of the weights. However, most of the discriminative weights come from W1t (the middle zone in Figure 6), and this further justifies the importance of POS tags in dependency parsing. • We carefully examine many of the h = 200 fe"
D14-1082,P11-2003,0,0.0440756,"alist one-hot word representations rather than the distributed representations of modern work. (Mayberry III and Miikkulainen, 1999) explored a shift reduce constituency parser with one-hot word representations and did subsequent parsing work in (Mayberry III and Miikkulainen, 2005). (Henderson, 2004) was the first to attempt to use neural networks in a broad-coverage Penn Treebank parser, using a simple synchrony network to predict parse decisions in a constituency parser. More recently, (Titov and Henderson, 2007) applied Incremental Sigmoid Belief Networks to constituency parsing and then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There have been a number of recent uses of deep learning for constituency parsing (Collobert, 2011; Socher et al., 2013). (Socher et al., 2014) has also built models over dependency representations but this work has not attempted to learn neural networks for dependency parsing. Most recently, (Stenetorp, 2013) attempted to bui"
D14-1082,D13-1152,0,0.0227377,"Missing"
D14-1082,D07-1099,0,0.0611729,"verlap but also major differences from our work here. One big difference is that much early work uses localist one-hot word representations rather than the distributed representations of modern work. (Mayberry III and Miikkulainen, 1999) explored a shift reduce constituency parser with one-hot word representations and did subsequent parsing work in (Mayberry III and Miikkulainen, 2005). (Henderson, 2004) was the first to attempt to use neural networks in a broad-coverage Penn Treebank parser, using a simple synchrony network to predict parse decisions in a constituency parser. More recently, (Titov and Henderson, 2007) applied Incremental Sigmoid Belief Networks to constituency parsing and then (Garg and Henderson, 2011) extended this work to transition-based dependency parsers using a Temporal Restricted Boltzman Machine. These are very different neural network architectures, and are much less scalable and in practice a restricted vocabulary was used to make the architecture practical. There have been a number of recent uses of deep learning for constituency parsing (Collobert, 2011; Socher et al., 2013). (Socher et al., 2014) has also built models over dependency representations but this work has not atte"
D14-1082,N03-1033,1,0.0890461,"Missing"
D14-1082,D08-1059,0,0.0559058,"Missing"
D14-1082,P11-2033,0,0.0884169,"denotes POS tag. given configuration. Information that can be obtained from one configuration includes: (1) all the words and their corresponding POS tags (e.g., has / VBZ); (2) the head of a word and its label (e.g., nsubj, dobj) if applicable; (3) the position of a word on the stack/buffer or whether it has already been removed from the stack. Conventional approaches extract indicator features such as the conjunction of 1 ∼ 3 elements from the stack/buffer using their words, POS tags or arc labels. Table 1 lists a typical set of feature templates chosen from the ones of (Huang et al., 2009; Zhang and Nivre, 2011).2 These features suffer from the following problems: • LEFT-ARC(l): adds an arc s1 → s2 with label l and removes s2 from the stack. Precondition: |s |≥ 2. • Sparsity. The features, especially lexicalized features are highly sparse, and this is a common problem in many NLP tasks. The situation is severe in dependency parsing, because it depends critically on word-to-word interactions and thus the high-order features. To give a better understanding, we perform a feature analysis using the features in Table 1 on the English Penn Treebank (CoNLL representations). The results given in Table 2 demo"
D14-1082,P04-1013,0,\N,Missing
D14-1130,2013.mtsummit-wptp.7,0,0.0193322,"lated Work The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work Green et al. (2013a) comparing scratch translation to post-edit. Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature. Incremental MT learning has been investigated several times, usually starting from no data (Barrachina et al., 2009; Ortiz-Martínez et al., 2010), via simulated post-editing (Martínez-Gómez et al., 2012; Denkowski et al., 2014a), or via re-ranking (Wäschle et al., 2013). No previous experiments combined large-scale baselines, full re-tuning of the model weights, and HTER optimization. HTER tuning can be simulated by reparameterizing an existing metric."
D14-1130,J09-1002,0,0.0310919,"th professional bilingual translators follows our previous work Green et al. (2013a) comparing scratch translation to post-edit. Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature. Incremental MT learning has been investigated several times, usually starting from no data (Barrachina et al., 2009; Ortiz-Martínez et al., 2010), via simulated post-editing (Martínez-Gómez et al., 2012; Denkowski et al., 2014a), or via re-ranking (Wäschle et al., 2013). No previous experiments combined large-scale baselines, full re-tuning of the model weights, and HTER optimization. HTER tuning can be simulated by reparameterizing an existing metric. Snover et al. (2009) tuned TERp to correlate with HTER, while Denkowski and Lavie (2010) did the same for METEOR. Zaidan and Callison-Burch (2010) showed how to solicit MT corrections for HTER from Amazon Mechanical Turk. Our learning approach is related to"
D14-1130,E06-1032,0,0.0164244,"3 Ortiz-Martínez et al. (2009) describe a related technique in which all source and target words can align, with scores set by smoothing. systems tuned to other metrics. Conversely, systems tuned to edit-distance-based metrics like TER tend to produce short translations that are heavily penalized by other metrics. When human corrections become available, we switch to HTER, which correlates with human judgment and is an interpretable measure of editing effort. Whereas TER is computed as TER(e, eˆ), HTER is HTER(h, eˆ). HBLEU is an alternative, but since BLEU is invariant to some permutations (Callison-Burch et al., 2006), it is less interpretable. We find that it also does not work as well in practice. We previously proposed a fast, online tuning algorithm (Green et al., 2013b) based on AdaGrad (Duchi et al., 2011). The default loss function is expected error (EE) (Och, 2003; Cherry and Foster, 2012). Expected BLEU is an example of EE, which we found to be unstable when switching metrics. This may result from direct incorporation of the error metric into the gradient computation. To solve this problem, we propose a crossentropy loss which, to our knowledge, is new in ˆ = {ˆ MT. Let E ei }ni=1 be an n-best lis"
D14-1130,2010.tc-1.10,0,0.146806,"on with human subjects necessarily involves many moving parts. Section 2 briefly describes the interface, focusing on NLP components. Section 3 describes changes to the backend MT system. Section 4 explains the user study, and reports human translation time and quality results. Section 5 describes the MT re-tuning experiment. Analysis (section 6) and related work (section 7) round out the paper. 2 New Translator User Interface Figure 1 shows the translator interface, which is designed for expert, bilingual translators. Previous studies have shown that expert translators work and type quickly (Carl, 2010), so the interface is designed to be very responsive, and to be primarily operated by the keyboard. Most aids can be accessed via either typing or four hot keys. The current design focuses on the point of text entry and does not include conventional translator workbench features such as workflow management, spell checking, and text formatting tools. In the trivial post-edit mode, the interactive aids are disabled and a 1-best translation pre-populates the text entry box. We have described the HCI-specific motivations for and contributions of this new interface in Green et al. (2014c). This sec"
D14-1130,N10-1080,1,0.51038,"allowing any source word to align with any unseen or ungeneratable (due to the distortion limit) target word are created.3 These synthetic rules are given rule scores lower than any other rules in the set of queried rules for that source input f . Then candidates are allowed to compete on the beam. Candidates with spurious alignments will likely be pruned in favor of those that only turn to synthetic rules as a last resort. 3.2 Tuning We choose BLEU (Papineni et al., 2002) for baseline tuning to independent references, and HTER for re-tuning to human corrections. Our rationale is as follows: Cer et al. (2010) showed that BLEUtuned systems score well across automatic metrics and also correlate with human judgment better than 2 Och et al. (2003) describe a similar algorithm for word graphs. 3 Ortiz-Martínez et al. (2009) describe a related technique in which all source and target words can align, with scores set by smoothing. systems tuned to other metrics. Conversely, systems tuned to edit-distance-based metrics like TER tend to produce short translations that are heavily penalized by other metrics. When human corrections become available, we switch to HTER, which correlates with human judgment and"
D14-1130,N12-1047,0,0.0169764,"penalized by other metrics. When human corrections become available, we switch to HTER, which correlates with human judgment and is an interpretable measure of editing effort. Whereas TER is computed as TER(e, eˆ), HTER is HTER(h, eˆ). HBLEU is an alternative, but since BLEU is invariant to some permutations (Callison-Burch et al., 2006), it is less interpretable. We find that it also does not work as well in practice. We previously proposed a fast, online tuning algorithm (Green et al., 2013b) based on AdaGrad (Duchi et al., 2011). The default loss function is expected error (EE) (Och, 2003; Cherry and Foster, 2012). Expected BLEU is an example of EE, which we found to be unstable when switching metrics. This may result from direct incorporation of the error metric into the gradient computation. To solve this problem, we propose a crossentropy loss which, to our knowledge, is new in ˆ = {ˆ MT. Let E ei }ni=1 be an n-best list ranked by a gold metric G(e, eˆ) ≥ 0. Assume we have a preference of a higher G (e.g., BLEU or ˆ 1−HTER). Define the model distribution over E > as e|f ) ∝ exp[w φ(ˆ e, f )] normalized so that P q(ˆ q indicates how much the model q(ˆ e |f ) = 1; ˆ eˆ∈E prefers each translation. Simi"
D14-1130,2003.mtsummit-papers.10,0,0.0829333,"ver, the re-tuned 4-gram and 3-gram precisions are signifHTER↓ int 44.05 43.99 42.35 −1.80 System baseline re-tune (pe) – HTER↓ pe 41.05 40.34 – −0.71 Table 7: En-De test results for re-tuning to post-edit (pe) vs. interactive (int). Features cannot be extracted from the post-edit data, so the re-tune+feat system cannot be learned. The Fr-En results are similar but are omitted due to space. icantly higher. The unchanged Fr-En TER value can be explained by the observation that no human translators produced TER scores higher than the baseline MT. This odd result has also been observed for BLEU (Culy and Riehemann, 2003), although here we do observe a slight BLEU improvement. The additional features (854 for Fr-En; 847 for En-De) help significantly and do not slow down decoding. We used the same L1 regularization strength as the baseline, but feature growth could be further constrained by increasing this parameter. Tuning is very fast at about six minutes for the whole dataset, so tuning during a live user session is already practical. Table 7 compares re-tuning to interactive vs. post-edit corrections. Recall that the int-test and pe-test datasets are different and contain different references. The post-edit"
D14-1130,N10-1031,0,0.00993423,"icitly designed according to mixedinitiative principles from the HCI literature. Incremental MT learning has been investigated several times, usually starting from no data (Barrachina et al., 2009; Ortiz-Martínez et al., 2010), via simulated post-editing (Martínez-Gómez et al., 2012; Denkowski et al., 2014a), or via re-ranking (Wäschle et al., 2013). No previous experiments combined large-scale baselines, full re-tuning of the model weights, and HTER optimization. HTER tuning can be simulated by reparameterizing an existing metric. Snover et al. (2009) tuned TERp to correlate with HTER, while Denkowski and Lavie (2010) did the same for METEOR. Zaidan and Callison-Burch (2010) showed how to solicit MT corrections for HTER from Amazon Mechanical Turk. Our learning approach is related to coactive learning (Shivaswamy and Joachims, 2012). Their basic preference perceptron updates toward a correction, whereas we use the correction for metric scoring and feature extraction. 8 Conclusion We presented a new CAT interface that supports post-edit and interactive modes. Evaluation with professional, bilingual translators showed post-edit to be faster, but prior subject familiarity with postedit may have mattered. For"
D14-1130,E14-1042,0,0.267789,"ecoder OOV model generates an identity translation rule. We add features in which the source word is concatenated with the left, right, and left/right contexts in the target, e.g., {<s>-tarceva, tarcevawas, <s>-tarceva-was}. We also add versions with target words mapped to classes. 3.4 Differences from Previous Work Our backend innovations support the UI and enable feature-based learning from human corrections. In contrast, most previous work on incremental MT learning has focused on extracting new translation rules, language model updating, and modifying translation model probabilities (see: Denkowski et al. (2014a)). We regard these features as additive to our own work: certainly extracting new, unseen rules should help translation in a new domain. Moreover, to our knowledge, all previous work on updating the weight vector w has considered simulated post-editing, in which the independent references e are substituted for corrections h. Here we extract features from and re-tune to actual corrections to the baseline MT output. 1229 4 Translation User Study We conducted a human translation experiment with a 2 (translation conditions) × n (source sentences) mixed design, where n depended on the language pa"
D14-1130,W14-0311,0,0.0267055,"Missing"
D14-1130,W14-0313,0,0.0184757,"g is very fast at about six minutes for the whole dataset, so tuning during a live user session is already practical. Table 7 compares re-tuning to interactive vs. post-edit corrections. Recall that the int-test and pe-test datasets are different and contain different references. The post-edit baseline is lower because humans performed less editing in the baseline condition (see Table 1). Features account for the greatest reduction in HTER. Of course, the features are based mostly on word alignments, which could be obtained for the post-edit data by running an online word alignment tool (see: Farajian et al. (2014)). However, the interactive logs contain much richer user state information that we could not exploit due to data sparsity. We also hypothesize that the final interactive corrections might be more useful since suggestions prime translators (Green et al., 2013a), and the MT system was able to refine its suggestions. 6 Re-tuning Analysis Tables 6 and 7 raise two natural questions: what accounts for the reduction in HTER, and why are the TER/BLEU results mixed? Comparison of the BLEU-tuned baseline to the HTER re-tuned systems gives some insight. For both questions, fine1233 grained corrections a"
D14-1130,federmann-2010-appraise,0,0.0167869,"ity with both automatic and manual measures. Table 1 shows that in the interactive mode, TER is lower and HTER is higher: subjects created translations closer to the references (lower TER), but performed more editing (higher HTER). This result suggests better translations in the interactive mode. To confirm that intuition, we elicited judgments from professional human raters. The setup followed the manual quality evaluation of the WMT 2014 shared task (Bojar et al., 2014). We hired six raters— three for each language pair—who were paid between $15–20 per hour. The raters logged into Appraise (Federmann, 2010) and for each source segment, ranked five randomly selected translations. From these 5-way rankings we extracted pairwise judgments π = {<, =}, where u1 < u2 indicates that subject u1 provided a better translation than subject u2 for a given source input (Table 2). 9 See (Green et al., 2014c) for significance levels of the other covariates along with analysis of subject learning rates, subject behavior, and qualitative feedback. #pairwise #ties (=) IAA EW (inter.) Fr-En 14,211 5,528 0.419 (0.357) 0.512 En-De 15,001 2,964 0.407 (0.427) 0.491 Table 2: Pairwise judgments for the manual quality as"
D14-1130,W02-1020,0,0.0688444,"ion of f , h is the correction of eˆ, and u is the log of interaction events during the translation session. Our evaluation corpora also include independently generated references e for each f . 3 Interactive MT Backend Now we describe modifications to Phrasal (Green et al., 2014b), the phrase-based MT system that supports the interface. Phrasal follows the log-linear approach to phrase-based translation (Och and Ney, 2004) in which the decision rule has the familiar linear form eˆ = arg max w> φ(e, f ) e 1 (1) The classic TransType system included a probabilistic prediction length component (Foster et al., 2002), but we find that the simpler projection technique works well in practice. 1227 where w ∈ Rd is the model weight vector and φ(·) ∈ Rd is a feature map. 3.1 Decoding The default Phrasal search algorithm is cube pruning (Huang and Chiang, 2007). In the post-edit condition, search is executed as usual for each source input, and the 1-best output is inserted into the target textbox. However, in interactive mode, the full search algorithm is executed each time the user modifies the partial translation. Machine suggestions eˆ must match user prefix h. Define indicator function pref(ˆ e, h) to retur"
D14-1130,D08-1089,1,0.447304,"am size is 1,200, we were able to reduce the beam size to 800 and run the tuner longer to achieve the same level of translation quality. For example, at the default beam size for French-English, the algorithm converges after 12 iterations, whereas at the lower beam size it achieves that level after 20 iterations. In our experience, batch tuning algorithms seem to be more sensitive to the beam size. 3.3 Feature Templates The baseline system contains 19 dense feature templates: the nine Moses (Koehn et al., 2007) baseline features, the eight-feature hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule in the bitext, and an indicator for unique rules. We found that sparse features, while improving translation quality, came at the cost of slower decoding due to feature extraction and inner products with a higher dimensional feature map φ. During prototyping, we observed that users found the system to be sluggish unless it responded in approximately 300ms or less. This budget restricted us to dense features. When re-tuning to corrections, we extract features from the user logs u and add them to the baseline dense model. For each tuning input f , the MT system pro"
D14-1130,P13-1031,1,0.842567,"s. Conversely, systems tuned to edit-distance-based metrics like TER tend to produce short translations that are heavily penalized by other metrics. When human corrections become available, we switch to HTER, which correlates with human judgment and is an interpretable measure of editing effort. Whereas TER is computed as TER(e, eˆ), HTER is HTER(h, eˆ). HBLEU is an alternative, but since BLEU is invariant to some permutations (Callison-Burch et al., 2006), it is less interpretable. We find that it also does not work as well in practice. We previously proposed a fast, online tuning algorithm (Green et al., 2013b) based on AdaGrad (Duchi et al., 2011). The default loss function is expected error (EE) (Och, 2003; Cherry and Foster, 2012). Expected BLEU is an example of EE, which we found to be unstable when switching metrics. This may result from direct incorporation of the error metric into the gradient computation. To solve this problem, we propose a crossentropy loss which, to our knowledge, is new in ˆ = {ˆ MT. Let E ei }ni=1 be an n-best list ranked by a gold metric G(e, eˆ) ≥ 0. Assume we have a preference of a higher G (e.g., BLEU or ˆ 1−HTER). Define the model distribution over E > as e|f ) ∝"
D14-1130,W14-3360,1,0.847793,"and type quickly (Carl, 2010), so the interface is designed to be very responsive, and to be primarily operated by the keyboard. Most aids can be accessed via either typing or four hot keys. The current design focuses on the point of text entry and does not include conventional translator workbench features such as workflow management, spell checking, and text formatting tools. In the trivial post-edit mode, the interactive aids are disabled and a 1-best translation pre-populates the text entry box. We have described the HCI-specific motivations for and contributions of this new interface in Green et al. (2014c). This section focuses on interface elements built on NLP components. 2.1 UI Overview and Walkthrough We categorized interactions into three groups: source comprehension: word lookups, source coverage highlighting; target gisting: 1-best translation, real-time target completion; target generation: real-time autocomplete, target reordering, insert complete translation. The interaction designs are novel; those in italic have, to our knowledge, never appeared in a translation workbench. Source word lookup When the user hovers over a source word, a menu of up to four ranked translation suggestio"
D14-1130,W14-3311,1,0.795682,"and type quickly (Carl, 2010), so the interface is designed to be very responsive, and to be primarily operated by the keyboard. Most aids can be accessed via either typing or four hot keys. The current design focuses on the point of text entry and does not include conventional translator workbench features such as workflow management, spell checking, and text formatting tools. In the trivial post-edit mode, the interactive aids are disabled and a 1-best translation pre-populates the text entry box. We have described the HCI-specific motivations for and contributions of this new interface in Green et al. (2014c). This section focuses on interface elements built on NLP components. 2.1 UI Overview and Walkthrough We categorized interactions into three groups: source comprehension: word lookups, source coverage highlighting; target gisting: 1-best translation, real-time target completion; target generation: real-time autocomplete, target reordering, insert complete translation. The interaction designs are novel; those in italic have, to our knowledge, never appeared in a translation workbench. Source word lookup When the user hovers over a source word, a menu of up to four ranked translation suggestio"
D14-1130,P13-2121,0,0.0329425,"-tune and pe-tune, respectively. We report heldout results on the two test data sets. All sets are supplied with independent references. 5.1 Datasets Table 4 shows the monolingual and parallel training corpora. Most of the data come from the constrained track of the WMT 2013 shared task (Bojar et al., 2013). We also added 61k parallel segments of TAUS data to the En-De bitext, and 26k TAUS segments to the Fr-En bitext. We aligned the parallel data with the Berkeley Aligner (Liang et al., 2006) and symmetrized the alignments with the grow-diag heuristic. For each target language we used lmplz (Heafield et al., 2013) to estimate unfiltered, 5-gram Kneser-Ney LMs from the concatenation of the target side of the bitext and the monolingual data. For the class-based features, we estimated 512-class source and target mappings with the algorithm of Green et al. (2014a). The upper part of Table 5 shows the baseline tuning and development sets, which also contained 1/3 TAUS medical text, 1/3 TAUS software text, and 1/3 WMT newswire text (see section 4). The lower part of Table 5 shows the organization of the human corrections for re-tuning and testing. Recall that for each unique source input, eight human transla"
D14-1130,P07-1019,0,0.0160969,"fications to Phrasal (Green et al., 2014b), the phrase-based MT system that supports the interface. Phrasal follows the log-linear approach to phrase-based translation (Och and Ney, 2004) in which the decision rule has the familiar linear form eˆ = arg max w> φ(e, f ) e 1 (1) The classic TransType system included a probabilistic prediction length component (Foster et al., 2002), but we find that the simpler projection technique works well in practice. 1227 where w ∈ Rd is the model weight vector and φ(·) ∈ Rd is a feature map. 3.1 Decoding The default Phrasal search algorithm is cube pruning (Huang and Chiang, 2007). In the post-edit condition, search is executed as usual for each source input, and the 1-best output is inserted into the target textbox. However, in interactive mode, the full search algorithm is executed each time the user modifies the partial translation. Machine suggestions eˆ must match user prefix h. Define indicator function pref(ˆ e, h) to return true if eˆ begins with h, and false otherwise. Eq. 1 becomes: eˆ = arg max w> φ(e, f ) e s.t. pref(e,h) (2) Cube pruning can be straightforwardly modified to satisfy this constraint by simple string matching of candidate translations. Also,"
D14-1130,P02-1050,0,0.0151933,"ns a distinct 10-best list for the full source input. The UI builds up a trie from these 10-best lists. Up to four distinct suggestions are then shown at the point of translation. The suggestion length is based on a syntactic parse of the fixed source input. As an offline, pre-processing step, we parse each source input with Stanford CoreNLP (Manning et al., 2014). The UI combines those parses with word alignments from the full translation suggestions to project syntactic constituents to each item on the n-best list. Syntactic projection is a very old idea that underlies many MT systems (see: Hwa et al. (2002)). Here we make novel use of it for suggestion prediction Target Reordering Carl (2010) showed that expert translators tend to adopt local planning: they read a few words ahead and then translate in a roughly online fashion. However, word order differences between languages will necessarily require longer range planning and movement. To that end, the UI supports keyboard-based reordering. Suppose that the user wants to move a span in gray text to the insertion position for editing. Typing the prefix of this string will update the autocomplete dropdown with matching strings from the gray text."
D14-1130,P07-2045,0,0.0140133,"e tuning also permits a trick that speeds up decoding during deployment. Whereas the Phrasal default beam size is 1,200, we were able to reduce the beam size to 800 and run the tuner longer to achieve the same level of translation quality. For example, at the default beam size for French-English, the algorithm converges after 12 iterations, whereas at the lower beam size it achieves that level after 20 iterations. In our experience, batch tuning algorithms seem to be more sensitive to the beam size. 3.3 Feature Templates The baseline system contains 19 dense feature templates: the nine Moses (Koehn et al., 2007) baseline features, the eight-feature hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule in the bitext, and an indicator for unique rules. We found that sparse features, while improving translation quality, came at the cost of slower decoding due to feature extraction and inner products with a higher dimensional feature map φ. During prototyping, we observed that users found the system to be sluggish unless it responded in approximately 300ms or less. This budget restricted us to dense features. When re-tuning to corrections, we extract featur"
D14-1130,P09-4005,0,0.0925355,"an effort, making fine distinctions between 0 (no editing) and 1 (complete rewrite). We designed a new user interface (UI) for the experiment. The interface places demands on the MT backend—not the other way around. The most significant new MT system features are prefix decoding, for translation completion based on a user prefix; and dynamic phrase table augmentation, to handle target out-of-vocabulary (OOV) words. Discriminative re-tuning is accomplished with a novel cross-entropy objective function. We report three main findings: (1) post-editing is faster than interactive MT, corroborating Koehn (2009a); (2) interactive MT yields higher quality translation when baseline MT quality is high; and (3) re-tuning to interactive feedback leads to larger held-out HTER gains relative to post-edit. Together these results show that a human-centered approach to computer aided translation (CAT) may involve tradeoffs between human effort and machine learnability. For example, if speed is the top priority, then a design geared toward post-editing 1225 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1225–1236, c October 25-29, 2014, Doha, Qatar. 2014 A"
D14-1130,J10-4005,0,0.0117586,"uments), and prepositional phrases. Crucially, these units are natural to humans, unlike statistical target phrases. Figure 2: Source word lookup and target autocomplete menus. The menus show different suggestions. The word lookup menu (top) is not dependent on the target context Teachers, whereas the autocomplete dropdown (bottom) is. based on the word alignments between source and target generated by the MT system. We found that the raw alignments are too noisy to show users, so the UI filters them with phrase-level heuristics. 1-best translation The most common use of MT output is gisting (Koehn, 2010, p.21). The gray text below each black source input shows the best MT system output (Figure 1B). Real-time target completion When the user extends the black prefix, the gray text will update to the most probable completion (Figure 1E). This update comes from decoding under the full translation model. All previous systems performed inference in a word lattice. Real-time autocomplete The autocomplete dropdown at the point of text entry is the main translation aid (Figures 1D and 2). Each real-time update actually contains a distinct 10-best list for the full source input. The UI builds up a tri"
D14-1130,N10-1079,0,0.0338006,"translators follows our previous work Green et al. (2013a) comparing scratch translation to post-edit. Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature. Incremental MT learning has been investigated several times, usually starting from no data (Barrachina et al., 2009; Ortiz-Martínez et al., 2010), via simulated post-editing (Martínez-Gómez et al., 2012; Denkowski et al., 2014a), or via re-ranking (Wäschle et al., 2013). No previous experiments combined large-scale baselines, full re-tuning of the model weights, and HTER optimization. HTER tuning can be simulated by reparameterizing an existing metric. Snover et al. (2009) tuned TERp to correlate with HTER, while Denkowski and Lavie (2010) did the same for METEOR. Zaidan and Callison-Burch (2010) showed how to solicit MT corrections for HTER from Amazon Mechanical Turk. Our learning approach is related to coactive learning (Shivaswamy"
D14-1130,W00-0507,0,0.0438496,"ent’, which is closer to both the independent reference ‘je nach datei’ and the human corrections. This change improves TER, BLEU, and HTER. 7 Related Work The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work Green et al. (2013a) comparing scratch translation to post-edit. Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature. Incremental MT learning has been investigated several times, usually starting from no data (Barrachina et al., 2009; Ortiz-Martínez et al., 2010), via simulated post-editing (Martínez-Gómez et al., 2012; Denkowski et al., 2014a), or via re-ranking (Wäschle et al., 2013). No previous experiments combined large-sc"
D14-1130,P02-1040,0,0.106127,"stortion limit. To solve these problems, we perform dynamic phrase table augmentation, adding new synthetic rules specific to each search. Rules allowing any source word to align with any unseen or ungeneratable (due to the distortion limit) target word are created.3 These synthetic rules are given rule scores lower than any other rules in the set of queried rules for that source input f . Then candidates are allowed to compete on the beam. Candidates with spurious alignments will likely be pruned in favor of those that only turn to synthetic rules as a last resort. 3.2 Tuning We choose BLEU (Papineni et al., 2002) for baseline tuning to independent references, and HTER for re-tuning to human corrections. Our rationale is as follows: Cer et al. (2010) showed that BLEUtuned systems score well across automatic metrics and also correlate with human judgment better than 2 Och et al. (2003) describe a similar algorithm for word graphs. 3 Ortiz-Martínez et al. (2009) describe a related technique in which all source and target words can align, with scores set by smoothing. systems tuned to other metrics. Conversely, systems tuned to edit-distance-based metrics like TER tend to produce short translations that a"
D14-1130,N06-1014,0,0.0232367,"ent, and test corpora (#segments). tune and dev were used for baseline system preparation. Re-tuning was performed on int-tune and pe-tune, respectively. We report heldout results on the two test data sets. All sets are supplied with independent references. 5.1 Datasets Table 4 shows the monolingual and parallel training corpora. Most of the data come from the constrained track of the WMT 2013 shared task (Bojar et al., 2013). We also added 61k parallel segments of TAUS data to the En-De bitext, and 26k TAUS segments to the Fr-En bitext. We aligned the parallel data with the Berkeley Aligner (Liang et al., 2006) and symmetrized the alignments with the grow-diag heuristic. For each target language we used lmplz (Heafield et al., 2013) to estimate unfiltered, 5-gram Kneser-Ney LMs from the concatenation of the target side of the bitext and the monolingual data. For the class-based features, we estimated 512-class source and target mappings with the algorithm of Green et al. (2014a). The upper part of Table 5 shows the baseline tuning and development sets, which also contained 1/3 TAUS medical text, 1/3 TAUS software text, and 1/3 WMT newswire text (see section 4). The lower part of Table 5 shows the or"
D14-1130,P14-5010,1,0.00802598,"under the full translation model. All previous systems performed inference in a word lattice. Real-time autocomplete The autocomplete dropdown at the point of text entry is the main translation aid (Figures 1D and 2). Each real-time update actually contains a distinct 10-best list for the full source input. The UI builds up a trie from these 10-best lists. Up to four distinct suggestions are then shown at the point of translation. The suggestion length is based on a syntactic parse of the fixed source input. As an offline, pre-processing step, we parse each source input with Stanford CoreNLP (Manning et al., 2014). The UI combines those parses with word alignments from the full translation suggestions to project syntactic constituents to each item on the n-best list. Syntactic projection is a very old idea that underlies many MT systems (see: Hwa et al. (2002)). Here we make novel use of it for suggestion prediction Target Reordering Carl (2010) showed that expert translators tend to adopt local planning: they read a few words ahead and then translate in a roughly online fashion. However, word order differences between languages will necessarily require longer range planning and movement. To that end,"
D14-1130,W05-0908,0,0.0302884,"33 45.29 39.99 45.73 40.30 45.28 HTER 28.28 26.96 26.40 System baseline re-tune (int) re-tune+feat ∆ (a) En-De int-test results. System baseline re-tune re-tune+feat tune bleu hter hter (b) Fr-En int-test results. Table 6: Main re-tuning results for interactive data. baseline is the BLEU-tuned system used in the translation user study. re-tune is the baseline feature set re-tuned to HTER on int-tune. retune+feat adds the human feature templates described in section 3.3. bold indicates statistical significance relative to the baseline at p < 0.001; italic at p < 0.05 by the permutation test of Riezler and Maxwell (2005). data. There were five source segments per document, and each document was rendered as a single screen during the translation experiment. Segment order was not randomized, so we could split the data as follows: assign the first three segments of each screen to tune, and the last two to test. This is a clean split with no overlap. This tune/test split has two attractive properties. First, if we can quickly re-tune on the first few sentences on a screen and provide better translations for the last few, then presumably the user experience improves. Second, source inputs f are repeated— eight tra"
D14-1130,W14-3301,0,0.0243992,"Missing"
D14-1130,J04-4002,0,0.0155251,"requests to the MT system, and logs user records to a database. Each user record is a tuple of the form (f, eˆ, h, u), where f is the source sequence, eˆ is the latest 1-best machine translation of f , h is the correction of eˆ, and u is the log of interaction events during the translation session. Our evaluation corpora also include independently generated references e for each f . 3 Interactive MT Backend Now we describe modifications to Phrasal (Green et al., 2014b), the phrase-based MT system that supports the interface. Phrasal follows the log-linear approach to phrase-based translation (Och and Ney, 2004) in which the decision rule has the familiar linear form eˆ = arg max w> φ(e, f ) e 1 (1) The classic TransType system included a probabilistic prediction length component (Foster et al., 2002), but we find that the simpler projection technique works well in practice. 1227 where w ∈ Rd is the model weight vector and φ(·) ∈ Rd is a feature map. 3.1 Decoding The default Phrasal search algorithm is cube pruning (Huang and Chiang, 2007). In the post-edit condition, search is executed as usual for each source input, and the 1-best output is inserted into the target textbox. However, in interactive"
D14-1130,E03-1032,0,0.082,"ic rules are given rule scores lower than any other rules in the set of queried rules for that source input f . Then candidates are allowed to compete on the beam. Candidates with spurious alignments will likely be pruned in favor of those that only turn to synthetic rules as a last resort. 3.2 Tuning We choose BLEU (Papineni et al., 2002) for baseline tuning to independent references, and HTER for re-tuning to human corrections. Our rationale is as follows: Cer et al. (2010) showed that BLEUtuned systems score well across automatic metrics and also correlate with human judgment better than 2 Och et al. (2003) describe a similar algorithm for word graphs. 3 Ortiz-Martínez et al. (2009) describe a related technique in which all source and target words can align, with scores set by smoothing. systems tuned to other metrics. Conversely, systems tuned to edit-distance-based metrics like TER tend to produce short translations that are heavily penalized by other metrics. When human corrections become available, we switch to HTER, which correlates with human judgment and is an interpretable measure of editing effort. Whereas TER is computed as TER(e, eˆ), HTER is HTER(h, eˆ). HBLEU is an alternative, but"
D14-1130,P03-1021,0,0.0533103,"re heavily penalized by other metrics. When human corrections become available, we switch to HTER, which correlates with human judgment and is an interpretable measure of editing effort. Whereas TER is computed as TER(e, eˆ), HTER is HTER(h, eˆ). HBLEU is an alternative, but since BLEU is invariant to some permutations (Callison-Burch et al., 2006), it is less interpretable. We find that it also does not work as well in practice. We previously proposed a fast, online tuning algorithm (Green et al., 2013b) based on AdaGrad (Duchi et al., 2011). The default loss function is expected error (EE) (Och, 2003; Cherry and Foster, 2012). Expected BLEU is an example of EE, which we found to be unstable when switching metrics. This may result from direct incorporation of the error metric into the gradient computation. To solve this problem, we propose a crossentropy loss which, to our knowledge, is new in ˆ = {ˆ MT. Let E ei }ni=1 be an n-best list ranked by a gold metric G(e, eˆ) ≥ 0. Assume we have a preference of a higher G (e.g., BLEU or ˆ 1−HTER). Define the model distribution over E > as e|f ) ∝ exp[w φ(ˆ e, f )] normalized so that P q(ˆ q indicates how much the model q(ˆ e |f ) = 1; ˆ eˆ∈E pref"
D14-1130,E14-2012,0,0.0395684,"ce ‘je nach datei’ and the human corrections. This change improves TER, BLEU, and HTER. 7 Related Work The process study most similar to ours is that of Koehn (2009a), who compared scratch, post-edit, and simple interactive modes. However, he used undergraduate, non-professional subjects, and did not consider re-tuning. Our experimental design with professional bilingual translators follows our previous work Green et al. (2013a) comparing scratch translation to post-edit. Many research translation UIs have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature. Incremental MT learning has been investigated several times, usually starting from no data (Barrachina et al., 2009; Ortiz-Martínez et al., 2010), via simulated post-editing (Martínez-Gómez et al., 2012; Denkowski et al., 2014a), or via re-ranking (Wäschle et al., 2013). No previous experiments combined large-scale baselines, full re-tuning of the model weights, and HTER optimiz"
D14-1130,R09-1060,0,0.0212786,"et of queried rules for that source input f . Then candidates are allowed to compete on the beam. Candidates with spurious alignments will likely be pruned in favor of those that only turn to synthetic rules as a last resort. 3.2 Tuning We choose BLEU (Papineni et al., 2002) for baseline tuning to independent references, and HTER for re-tuning to human corrections. Our rationale is as follows: Cer et al. (2010) showed that BLEUtuned systems score well across automatic metrics and also correlate with human judgment better than 2 Och et al. (2003) describe a similar algorithm for word graphs. 3 Ortiz-Martínez et al. (2009) describe a related technique in which all source and target words can align, with scores set by smoothing. systems tuned to other metrics. Conversely, systems tuned to edit-distance-based metrics like TER tend to produce short translations that are heavily penalized by other metrics. When human corrections become available, we switch to HTER, which correlates with human judgment and is an interpretable measure of editing effort. Whereas TER is computed as TER(e, eˆ), HTER is HTER(h, eˆ). HBLEU is an alternative, but since BLEU is invariant to some permutations (Callison-Burch et al., 2006), i"
D14-1130,2006.amta-papers.25,0,0.114284,"ze professional, bilingual translators working in both modes, looking first at user productivity. Does the additional machine assistance available in the interactive mode affect translation time and/or quality? Then we turn to the machine side of the partnership. The user study results in corrections to the baseline MT output. Do these corrections help the MT system, and can it learn from them quickly enough to help the user? We perform a re-tuning experiment in which we directly optimize human Translation Edit Rate (HTER), which correlates highly with human judgments of fluency and adequacy (Snover et al., 2006). It is also an intuitive measure of human effort, making fine distinctions between 0 (no editing) and 1 (complete rewrite). We designed a new user interface (UI) for the experiment. The interface places demands on the MT backend—not the other way around. The most significant new MT system features are prefix decoding, for translation completion based on a user prefix; and dynamic phrase table augmentation, to handle target out-of-vocabulary (OOV) words. Discriminative re-tuning is accomplished with a novel cross-entropy objective function. We report three main findings: (1) post-editing is fa"
D14-1130,W09-0441,0,0.0148145,"However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature. Incremental MT learning has been investigated several times, usually starting from no data (Barrachina et al., 2009; Ortiz-Martínez et al., 2010), via simulated post-editing (Martínez-Gómez et al., 2012; Denkowski et al., 2014a), or via re-ranking (Wäschle et al., 2013). No previous experiments combined large-scale baselines, full re-tuning of the model weights, and HTER optimization. HTER tuning can be simulated by reparameterizing an existing metric. Snover et al. (2009) tuned TERp to correlate with HTER, while Denkowski and Lavie (2010) did the same for METEOR. Zaidan and Callison-Burch (2010) showed how to solicit MT corrections for HTER from Amazon Mechanical Turk. Our learning approach is related to coactive learning (Shivaswamy and Joachims, 2012). Their basic preference perceptron updates toward a correction, whereas we use the correction for metric scoring and feature extraction. 8 Conclusion We presented a new CAT interface that supports post-edit and interactive modes. Evaluation with professional, bilingual translators showed post-edit to be faster,"
D14-1130,2013.mtsummit-papers.2,0,0.189162,"s have been proposed including TransType (Langlais et al., 2000), Caitra (Koehn, 2009b), Thot (Ortiz-Martínez and Casacuberta, 2014), TransCenter (Denkowski et al., 2014b), and CasmaCat (Alabau et al., 2013). However, to our knowledge, none of these interfaces were explicitly designed according to mixedinitiative principles from the HCI literature. Incremental MT learning has been investigated several times, usually starting from no data (Barrachina et al., 2009; Ortiz-Martínez et al., 2010), via simulated post-editing (Martínez-Gómez et al., 2012; Denkowski et al., 2014a), or via re-ranking (Wäschle et al., 2013). No previous experiments combined large-scale baselines, full re-tuning of the model weights, and HTER optimization. HTER tuning can be simulated by reparameterizing an existing metric. Snover et al. (2009) tuned TERp to correlate with HTER, while Denkowski and Lavie (2010) did the same for METEOR. Zaidan and Callison-Burch (2010) showed how to solicit MT corrections for HTER from Amazon Mechanical Turk. Our learning approach is related to coactive learning (Shivaswamy and Joachims, 2012). Their basic preference perceptron updates toward a correction, whereas we use the correction for metric"
D14-1130,N10-1057,0,0.0210879,"les from the HCI literature. Incremental MT learning has been investigated several times, usually starting from no data (Barrachina et al., 2009; Ortiz-Martínez et al., 2010), via simulated post-editing (Martínez-Gómez et al., 2012; Denkowski et al., 2014a), or via re-ranking (Wäschle et al., 2013). No previous experiments combined large-scale baselines, full re-tuning of the model weights, and HTER optimization. HTER tuning can be simulated by reparameterizing an existing metric. Snover et al. (2009) tuned TERp to correlate with HTER, while Denkowski and Lavie (2010) did the same for METEOR. Zaidan and Callison-Burch (2010) showed how to solicit MT corrections for HTER from Amazon Mechanical Turk. Our learning approach is related to coactive learning (Shivaswamy and Joachims, 2012). Their basic preference perceptron updates toward a correction, whereas we use the correction for metric scoring and feature extraction. 8 Conclusion We presented a new CAT interface that supports post-edit and interactive modes. Evaluation with professional, bilingual translators showed post-edit to be faster, but prior subject familiarity with postedit may have mattered. For French-English, the interactive mode enabled higher qualit"
D14-1130,W13-2201,0,\N,Missing
D14-1130,D08-1076,0,\N,Missing
D14-1159,D13-1160,1,0.289643,"contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehensio"
D14-1159,D08-1073,0,0.0214459,"the set of regular expressions that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al.,"
D14-1159,W06-0602,0,0.0205921,"urafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke e"
D14-1159,W10-2903,0,0.0252125,"., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choic"
D14-1159,clarke-etal-2012-nlp,1,0.779928,"Missing"
D14-1159,W02-1001,0,0.0492796,"ossible assignments to the inference variables are not feasible and there are both structural as well as prior knowledge constraints over the output space. Table 2 states the constraints we include, which are expressed as linear inequalities over output variables using standard techniques (e.g., (Roth and Yih, 2004)). 4.4 Learning in the Joint Model We train both the trigger classifier and the argument identifier using L2 -regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions. For event relations, we include the features described in Scaria et al. (2013), as well as context features for both triggers, and the dependency path between them, if one exists. 5 Question Answering via Structures This section describes our question answering system that, given a process struc"
D14-1159,D12-1062,0,0.0400506,"d by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank a"
D14-1159,doddington-etal-2004-automatic,0,0.188353,"Missing"
D14-1159,D11-1142,0,0.015657,"in the paragraph. Then, we map the question to a formal query, which is executed against the predicted structure. We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗ Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we consider processes in"
D14-1159,P13-1158,0,0.00703221,"ons via predicted structures substantially improves accuracy over baselines that use shallower representations. 1 Introduction The goal of machine reading is to develop programs that read text to learn about the world and make decisions based on accumulated knowledge. Work in this field has focused mostly on macro-reading, i.e., processing large text collections and extracting knowledge bases of facts (Etzioni et al., 2006; Carlson et al., 2010; Fader et al., 2011). Such methods rely on redundancy, and are thus suitable for answering common factoid questions which have ample evidence in text (Fader et al., 2013). However, reading a single document (micro-reading) to answer comprehension questions that require deep reasoning is currently beyond the scope of state-of-the-art systems. In this paper, we introduce a task where given a paragraph describing a process, the goal is to ∗ Both authors equally contributed to the paper. answer reading comprehension questions that test understanding of the underlying structure. In particular, we consider processes in biology textbooks such as this excerpt and the question that follows: “. . . Water is split, providing a source of electrons and protons (hydrogen io"
D14-1159,P99-1042,0,0.0251544,"ap natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating machine reading. Hirschman et al. (1999) presented a bag-ofwords approach to retrieving sentences for reading comprehension. Richardson et al. (2013) recently released the MCTest reading comprehension dataset that examines understanding of fictional stories. Their work shares our goal of advancing micro-reading, but they do not focus on process understanding. Developing programs that perform deep reasoning over complex descriptions of processes is an important step on the road to fulfilling the higher goals of machine reading. In this paper, we present an end-to-end system for reading comprehension of paragraphs which describe biolo"
D14-1159,W11-1801,0,0.0399034,"Missing"
D14-1159,Q13-1016,0,0.0240224,"e do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future research. Multiple choice reading comprehension tests are a natural choice for evaluating mach"
D14-1159,P14-1026,0,0.0192904,"he text to the question. We propose a novel method that tackles this challenging problem (see Figure 1). First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process. This is a difficult problem because events have complex interactions that span multiple sentences. Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer. Micro-reading is an important aspect of natural language understanding (Richardson et al., 2013; Kushman et al., 2014). In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions. While we work in the biology “. . . Water is split, providing a source of electrons and protons (hydrogen ions, H+ ) and giving off O2 as a by-product. Light absorbed by chlorophyll drives a transfer of water T HEME Step 1 split absorb E NABLE C AUSE transfer the electrons and hydrogen ions from water T HEME T HEME light ions to an acceptor called NADP+ . . . ” Step 3: Answer = b Q What can the splitting of water lead to? a Light absorption b Transfer of io"
D14-1159,P14-5010,1,0.0110213,"Missing"
D14-1159,P09-1039,0,0.00814841,"iption Every argument candidate and trigger pair has exactly one label. Two arguments of the same trigger cannot overlap. The S AME relation is symmetric. All other relations are anti-symmetric, i.e., for any relation label other than S AME, at most one of (ti , tj ) or (tj , ti ) can take that label and the other is assigned the label NULL - REL. Every trigger can have no more than two arguments with the same label. The same span of text can not be an argument for more than two triggers. The triggers must form a connected graph, framed as flow constraints as in Magnanti and Wolsey (1995) and Martins et al. (2009). If the same span of text is an argument of two triggers, then the triggers must be connected by a relation that is not NULL - REL. This ensures that triggers that share arguments are related. For any trigger, at most one outgoing edge can be labeled S UPER. Table 2: Constraints for joint inference. Formulation Given the two sets of variables, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: max y,z X t,a∈At ,A bt,a,A · yt,a,A + X ct1 ,t2 ,R · zt1 ,t2 ,R t1 ,t2 ,R Here, y and z refer to all the argument and re"
D14-1159,D12-1080,1,0.842646,"m. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow"
D14-1159,N03-1022,0,0.0797767,"igure 1). Our strategy is to treat the process structure as a small knowledge-base. We map each answer along with the question into a structured query that we compare against the structure. The query can prove either the correctness or incorrectness of the answer being considered. That is, either we get a valid match for an answer (proving that the corresponding answer is correct), or we get a refutation in the form of a contradicted causal chain (thus proving that the other answer is correct). This is similar to theorem proving approaches suggested in the past for factoid question answering (Moldovan et al., 2003). The rest of this section is divided into three parts: Section 5.1 defines the queries we use, Section 5.2 describes a rule-based algorithm for converting a question and an answer into a query and finally, 5.3 describes the overall algorithm. 5.1 Queries over Processes We model a query as a directed graph path with regular expressions over edge labels. The bottom right portion of Figure 1 shows examples of queries for our running example. In general, given a question and one of the answer candidates, one end of the path is populated by a trigger/argument found in the question and the other is"
D14-1159,J05-1004,0,0.128022,"tudied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of superv"
D14-1159,J08-2005,0,0.144807,"Missing"
D14-1159,D13-1020,0,0.248644,"ong answer is closer in the text to the question. We propose a novel method that tackles this challenging problem (see Figure 1). First, we train a supervised structure predictor that learns to extract entities, events and their relations describing the biological process. This is a difficult problem because events have complex interactions that span multiple sentences. Then, treating this structure as a small knowledge-base, we map questions to formal queries that are executed against the structure to provide the answer. Micro-reading is an important aspect of natural language understanding (Richardson et al., 2013; Kushman et al., 2014). In this work, we focus specifically on modeling processes, where events and entities relate to one another through complex interactions. While we work in the biology “. . . Water is split, providing a source of electrons and protons (hydrogen ions, H+ ) and giving off O2 as a by-product. Light absorbed by chlorophyll drives a transfer of water T HEME Step 1 split absorb E NABLE C AUSE transfer the electrons and hydrogen ions from water T HEME T HEME light ions to an acceptor called NADP+ . . . ” Step 3: Answer = b Q What can the splitting of water lead to? a Light abso"
D14-1159,D11-1001,0,0.0772427,"Missing"
D14-1159,W04-2401,0,0.0455766,"es, the objective of inference is to find a global assignment that maximizes the score. That is, the objective can be stated as follows: max y,z X t,a∈At ,A bt,a,A · yt,a,A + X ct1 ,t2 ,R · zt1 ,t2 ,R t1 ,t2 ,R Here, y and z refer to all the argument and relation variables respectively. Clearly, all possible assignments to the inference variables are not feasible and there are both structural as well as prior knowledge constraints over the output space. Table 2 states the constraints we include, which are expressed as linear inequalities over output variables using standard techniques (e.g., (Roth and Yih, 2004)). 4.4 Learning in the Joint Model We train both the trigger classifier and the argument identifier using L2 -regularized logistic regression. For the joint model, we use a linear model for the scoring functions, and train jointly using the structured averaged perceptron algorithm (Collins, 2002). Since argument labeling is similar to semantic role labeling (SRL), we extract standard SRL features given the trigger and argument from the syntactic tree for the corresponding sentence. In addition, we add features extracted from an off-theshelf SRL system. We also include all feature conjunctions."
D14-1159,D13-1177,1,0.941392,"and the accompanying dataset. We will use the example in Figure 1 as our running example throughout the paper. Our goal is to tackle a complex reading comprehension setting that centers on understanding the underlying meaning of a process description. We target a multiple-choice setting in which each input consists of a paragraph of text describing a biological process, a question, and two possible answers. The goal is to identify the correct answer using the text (Figure 1, left). We used the 148 paragraphs from the textbook Biology (Campbell and Reece, 2005) that were manually identified by Scaria et al. (2013). We extended this set to 200 paragraphs by including additional paragraphs that describe biological processes. Each paragraph in the collection represents a single biological process and describes a set of events, their participants and their interactions. Because we target understanding of paragraph meaning, we use the following desiderata for building the corpus of questions and answers: 1. The questions should focus on the events and entities participating in the process described in the paragraph, and answering the questions should require reasoning about the relations between those event"
D14-1159,N06-1056,0,0.0533243,"and BioProp (Chou et al., 2006). However, in contrast to PropBank and FrameNet, we do not allow all verbs to trigger events and instead let the annotators decide on biologically important triggers, which are not restricted to verbs (unlike BioProp, where 30 pre-specified verbs were selected for annotation). Like PropBank and BioProp, the argument labels are not trigger specific. Mapping questions to queries is effectively a semantic parsing task. In recent years, several lines of work addressed semantic parsing using various formalisms and levels of supervision (Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Clarke et al., 2010; Berant et al., 2013). In particular, Krishnamurthy and Kollar (2013) learned to map natural language utterances to referents in an image by constructing a KB from the image and then mapping the utterance to a query over the KB. This is analogous to our process of constructing a process structure and performing QA by querying that structure. In our work, we parse questions into graph-based queries, suitable for modeling processes, using a rule-based heuristic. Training a statistical semantic parser that will replace the QA system is an interesting direction for future res"
D14-1159,P09-1046,0,0.0164746,"s that are currently used by our QA system. 7 Discussion This work touches on several strands of work in NLP including information extraction, semantic role labeling, semantic parsing and reading comprehension. Event and relation extraction have been studied via the ACE data (Doddington et al., 2004) and related work. The BioNLP shared tasks (Kim et al., 2009; Kim et al., 2011; Riedel and McCallum, 2011) focused on biomedical data to extract events and their arguments. Event-event relations have been mostly studied from the perspective of temporal ordering; e.g., (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009; Do et al., 2012; McClosky and Manning, 2012). The process structure predicted in this work differs from these lines of work in two important ways: First, we predict events, arguments and their interactions from multiple sentences, while most earlier work focused on one or two of these components. Second, we model processes, and thus target causal relations between events, rather than temporal order only. Our semantic role annotation is similar to existing SRL schemes such as PropBank (Palmer et al., 2005), FrameNet (Ruppenhofer et al., 2006) and BioProp (Chou et al., 2006). However, in contr"
D14-1159,W09-1401,0,\N,Missing
D14-1159,E12-2021,0,\N,Missing
D14-1162,P14-1023,0,0.148116,"hieves better results faster, and also obtains the best results irrespective of speed. 5 Conclusion Recently, considerable attention has been focused on the question of whether distributional word representations are best learned from count-based 9 In contrast, noise-contrastive estimation is an approximation which improves with more negative samples. In Table 1 of (Mnih et al., 2013), accuracy on the analogy task is a non-decreasing function of the number of negative samples. methods or from prediction-based methods. Currently, prediction-based models garner substantial support; for example, Baroni et al. (2014) argue that these models perform better across a range of tasks. In this work we argue that the two classes of methods are not dramatically different at a fundamental level since they both probe the underlying co-occurrence statistics of the corpus, but the efficiency with which the count-based methods capture global statistics can be advantageous. We construct a model that utilizes this main benefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec. The result, GloVe, is a new global log-bil"
D14-1162,E14-1051,0,0.148116,"co-occurrence matrix is first transformed by an entropy- or correlation-based normalization. An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval. A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations. Shallow Window-Based Methods. Another approach is to learn word representations that aid in making predictions within local context windows. For example, Bengio et al. (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word representatio"
D14-1162,W14-1618,0,0.148116,"way for Collobert et al. (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural network structure for learning useful word representations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a simple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embeddings based on a PPMI metric. In the skip-gram and ivLBL models, the objective is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors. Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Ins"
D14-1162,W13-3512,1,0.148116,"hat is depends on whether α > 1, ( O(|C|) if α < 1, |X |= (22) 1/α O(|C |) if α > 1. For the corpora studied in this article, we observe that X i j is well-modeled by Eqn. (17) with α = 1.25. In this case we have that |X |= O(|C |0.8 ). Therefore we conclude that the complexity of the model is much better than the worst case O(V 2 ), and in fact it does somewhat better than the on-line window-based methods which scale like O(|C|). 4 4.1 Experiments Evaluation methods We conduct experiments on the word analogy task of Mikolov et al. (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark Dim. 100 100 100 300 300 300 300 300 300 300 300 300 300 300 1000 1000 300 300 Size 1.5B 1.6B 1.6B 1B 1.6B 1.5B 1.5B 1.6B 6B 6B 6B 6B 6B 6B 6B 6B 42B 42B Sem. 55.9 4.2 67.5 61 16.1 54.2 65.2 80.8 6.3 36.7 56.6 63.6 73.0 77.4 57.3 66.1 38.4 81.9 Syn. 50.1 16.4 54.3 61 52.6 64.8 63.0 61.5 8.1 46.6 63.0 67.4 66.0 67.0 68.9 65.1 58.2 69.3 Tot. 53.2 10.8 60.3 61 36.1 60.0 64.0 70.3 7.3 42.1 60.1 65.7 69.1 71.7 63.7 65.6 49.2 75.0 dataset for NER (Tjong Kim Sang and De Meulder, 2003). Word analogies. The word analogy task consists of questions like, “a is to"
D14-1162,N13-1090,0,0.148116,"recognition. 1 Introduction Semantic vector space models of language represent each word with a real-valued vector. These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference. For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman. This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009). The two main model families for learning word vectors a"
D14-1162,P13-1045,1,0.148116,"Missing"
D14-1162,P10-1040,0,0.148116,"n addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al. (2012) (HSMN) and Collobert and Weston (2008) (CW). We trained the CBOW model using the word2vec tool8 . The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better. We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first 8 We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10. 1539 shown for neural vectors in (Turian et al., 2010). 4.4 Overall 75 70 65 60 55 50 Wiki2010 1B tokens Wiki2014 1.6B tokens Gigaword5 4.3B tokens Gigaword5 + Wiki2014 6B tokens Common Crawl 42B tokens Figure 3: Accuracy on the analogy task for 300dimensional vectors trained on different corpora. entries are updated to assimilate new knowledge, whereas Gigaword is a fixed news repository with outdated and possibly incorrect information. Model Analysis: Run-time The total run-time is split between populating X and training the model. The former depends on many factors, including window size, vocabulary size, and corpus size. Though we did not do"
D14-1162,I13-1183,1,0.148116,"and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). Named entity recognition. The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous. We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We adopt the BIO2 annotation standard, as well as all the preprocessing steps described in (Wang and Manning, 2013). We use a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model (Finkel et al., 2005). A total of 437,905 discrete features were generated for the CoNLL2003 training dataset. In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features. With these features as input, we trained a conditional random field (CRF) with exactly the same setup as the CRFjoin model of (Wang and Manning, 2013). 4.2 Corpora and training details We trained our model on five corpora of varying sizes: a 2010 Wikipedi"
D14-1162,W03-0419,0,\N,Missing
D14-1162,P12-1092,1,\N,Missing
D14-1164,doddington-etal-2004-automatic,0,0.0363712,"ollected. The set of these sentences x, marked with the entity mentions for e1 and e2 , becomes the input to the relation extractor, which then produces a set of relations which hold between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotat"
D14-1164,I13-1081,0,0.165932,"Missing"
D14-1164,P05-1053,0,0.0494187,"e sentences x, marked with the entity mentions for e1 and e2 , becomes the input to the relation extractor, which then produces a set of relations which hold between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y"
D14-1164,P11-1055,0,0.0991499,"he MIML-RE model, as shown in Surdeanu et al. (2012). The outer plate corresponds to each of the n entity pairs in our knowledge base. Each entity pair has a set of mention pairs Mi , and a corresponding plate in the diagram for each mention pair in Mi . The variable x represents the input mention pair, whereas y represents the positive and negative relations for the given pair of entities. The latent variable z denotes a mention-level prediction for each input. The weight vector for the multinomial z classifier is given by wz , and there is a weight vector wj for each binary y classifier. by Hoffmann et al. (2011) and Riedel et al. (2010), addresses the assumptions of distantly supervised relations extractors in a principled way by positing a latent mention-level annotation. The model groups mentions according to their entity pair – for instance, every mention pair with Obama and Hawaii would be grouped together. A latent variable zi is created for every mention i, where zi ∈ R ∪ {None} takes a single relation label, or a no relation marker. We create |R |binary variables y representing the known positive and negative relations for the entity pair. A set of binary classifiers (log-linear factors in the"
D14-1164,N13-1095,0,0.0186917,"tively, it nonetheless makes a number of na¨ıve assumptions. First – explicit in the formulation of the approach – it assumes that every mention expresses some relation, and furthermore expresses the known relation(s). For instance, the sentence Obama visited Hawaii would be erroneously treated as a positive example of the born in relation. Second, it implicitly assumes that our knowledge base is complete: entity mentions with no known relation are treated as negative examples. The first of these assumptions is addressed by multi-instance multi-label (MIML) learning, described in Section 2.4. Min et al. (2013) address the second assumption by extending the MIML model with additional latent variables, while Xu et al. (2013) allow feedback from a coarse relation extractor to augment labels from the knowledge base. These latter two approaches are compatible with but are not implemented in this work. 2.4 Multi-Instance Multi-Label Learning The multi-instance multi-label (MIML-RE) model of Surdeanu et al. (2012), which builds upon work 1557 ... ... ... ... One way of expressing the generalization error ˆ is through its mean-squared error of a hypothesis h with the true hypothesis h: 2 ˆ E[(h(x) − h(x))"
D14-1164,P09-1113,0,0.464521,"produces a set of relations which hold between the mentions. We are predominantly interested in the second step – classifying a set of pairs of entity mentions into the relations they express. Figure 1 gives the general setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide the relations between the two entities."
D14-1164,P14-2119,0,0.527375,"Missing"
D14-1164,N13-1008,0,0.016599,"setting for relation extraction, with entity pairs Barack Obama and Hawaii, and Barack Obama and president. Traditionally, relation extraction has fallen into one of four broad approaches: supervised classification, as in the ACE task (Doddington et al., 2004; GuoDong et al., 2005; Surdeanu and Ciaramita, 2007), distant supervision (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011; Roth and Klakow, 2013) deterministic rule-based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010), and translation from open domain information extraction schema (Riedel et al., 2013). We focus on the first two of these approaches. 2.2 Supervised Relation Extraction Relation extraction can be naturally cast as a supervised classification problem. A corpus of relation mentions is collected, and each mention x is annotated with the relation y, if any, it expresses. The classifier’s output is then aggregated to decide the relations between the two entities. However, annotating supervised training data is generally expensive to perform at large scale. Although resources such as Freebase or the TAC KBP knowledge base have on the order of millions of training tuples over entitie"
D14-1164,D12-1042,1,0.317947,"complete: entity mentions with no known relation are treated as negative examples. The first of these assumptions is addressed by multi-instance multi-label (MIML) learning, described in Section 2.4. Min et al. (2013) address the second assumption by extending the MIML model with additional latent variables, while Xu et al. (2013) allow feedback from a coarse relation extractor to augment labels from the knowledge base. These latter two approaches are compatible with but are not implemented in this work. 2.4 Multi-Instance Multi-Label Learning The multi-instance multi-label (MIML-RE) model of Surdeanu et al. (2012), which builds upon work 1557 ... ... ... ... One way of expressing the generalization error ˆ is through its mean-squared error of a hypothesis h with the true hypothesis h: 2 ˆ E[(h(x) − h(x)) ] 2 ˆ = E[E[(h(x) − h(x)) |x]] Z 2 ˆ |x]p(x)dx. = E[(h(x) − h(x)) x The integrand can be further broken into bias and variance terms: Figure 2: The MIML-RE model, as shown in Surdeanu et al. (2012). The outer plate corresponds to each of the n entity pairs in our knowledge base. Each entity pair has a set of mention pairs Mi , and a corresponding plate in the diagram for each mention pair in Mi . The v"
D14-1164,P13-2117,0,0.0512823,"Missing"
D14-1164,P12-1087,0,0.600124,"Missing"
D14-1217,W07-1427,1,0.430608,"Missing"
D14-1217,C12-1042,0,0.0606249,"ions is still very hard to attain. More recently, a pioneering text-to-3D scene generation prototype system has been presented by WordsEye (Coyne and Sproat, 2001). The authors demonstrated the promise of text to scene generation systems but also pointed out some fundamental issues which restrict the success of their system: much spatial knowledge is required which is hard to obtain. As a result, users have to use unnatural language (e.g., “the stool is 1 feet to the south of the table”) to express their intent. Follow up work has attempted to collect spatial knowledge through crowd-sourcing (Coyne et al., 2012), but does not address the learning of spatial priors. We address the challenge of handling natural language for scene generation, by learning spatial knowledge from 3D scene data, and using it to infer unstated implicit constraints. Our work is similar in spirit to recent work on generating 2D clipart for sentences using probabilistic models learned from data (Zitnick et al., 2013). 8.3 Automatic Scene Layout Work on scene layout has focused on determining good furniture layouts by optimizing energy functions that capture the quality of a proposed layout. These energy functions are encoded fr"
D14-1217,D13-1128,0,0.00868314,"he kitchen and that the cake should be placed on a plate. The pioneering WordsEye system (Coyne and Sproat, 2001) addressed the text-to-3D task and is an inspiration for our work. However, there are many remaining gaps in this broad area. Among them, there is a need for research into learning spatial knowledge representations from data, and for connecting them to language. Representing unstated facts is a challenging problem unaddressed by prior work and the focus of our contribution. This problem is a counterpart to the image description problem (Kulkarni et al., 2011; Mitchell et al., 2012; Elliott and Keller, 2013), which has so far remained largely unexplored by the community. We present a representation for this form of spatial knowledge that we learn from 3D scene data and connect to natural language. We will show how this representation is useful for grounding language and for inferring unstated facts, i.e., the pragmatics of language describing physical environments. We demonstrate the use of this representation in the task of text-to-3D scene genera2028 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2028–2038, c October 25-29, 2014, Doha, Qata"
D14-1217,D13-1197,0,0.0355704,"ng spatial relations, generating 3D scenes from text, and automatically laying out 3D scenes. 8.1 Spatial knowledge and relations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By restricting the discourse domain to a micro-world with simple geometri"
D14-1217,D10-1040,0,0.0160563,"tial knowledge and relations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input for manipulating scene"
D14-1217,E12-1076,0,0.0209685,"tions for the cake in the kitchen and that the cake should be placed on a plate. The pioneering WordsEye system (Coyne and Sproat, 2001) addressed the text-to-3D task and is an inspiration for our work. However, there are many remaining gaps in this broad area. Among them, there is a need for research into learning spatial knowledge representations from data, and for connecting them to language. Representing unstated facts is a challenging problem unaddressed by prior work and the focus of our contribution. This problem is a counterpart to the image description problem (Kulkarni et al., 2011; Mitchell et al., 2012; Elliott and Keller, 2013), which has so far remained largely unexplored by the community. We present a representation for this form of spatial knowledge that we learn from 3D scene data and connect to natural language. We will show how this representation is useful for grounding language and for inferring unstated facts, i.e., the pragmatics of language describing physical environments. We demonstrate the use of this representation in the task of text-to-3D scene genera2028 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2028–2038, c Octo"
D14-1217,P10-1083,0,0.00925978,"ing out 3D scenes. 8.1 Spatial knowledge and relations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input"
D14-1217,P13-2014,0,0.011952,"iew-based object referent resolution (§7.2) and in disambiguating geometric interpretations of “on” (§7.3). Model Comparison Figure 8 shows a comparison of scenes generated by our model versus several simpler baselines. The top row shows the impact of modeling the support hierarchy and the relative positions in the layout of the scene. The bottom row shows that the learned spatial relations can give a more accurate layout than the naive predefined spatial relations, since it captures pragmatic implicatures of language, e.g., left is only used for directly left and not top left or bottom left (Vogel et al., 2013). Figure 12: Left: chair is selected using “the chair to the right of the table” or “the object to the right of the table”. Chair is not selected for “the cup to the right of the table”. Right: Different view results in different chair being selected for the input “the chair to the right of the table”. After a scene is generated, the user can refer to objects with their categories and with spatial relations between them. Objects are disambiguated by both category and view-centric spatial relations. We use the WordNet hierarchy to resolve hyponym or hypernym referents to objects in the scene. I"
D14-1217,H89-1033,0,0.339508,"unding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input for manipulating scenes. However, generalization to more complex objects and spatial relations is still very hard to attain. More recently, a pioneering text-to-3D scene generation prototype system has been presented by WordsEye (Coyne and Sproat, 2001). The authors demonstrated the promise of text to scene generation systems but also pointed out some fundamental is"
D14-1217,Q13-1016,0,0.144174,", desk). 8 Related Work There is related prior work in the topics of modeling spatial relations, generating 3D scenes from text, and automatically laying out 3D scenes. 8.1 Spatial knowledge and relations Prior work that required modeling spatial knowledge has defined representations specific to the task addressed. Typically, such knowledge is manually provided or crowdsourced – not learned from data. For instance, WordsEye (Coyne et al., 2010) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and connecting language to spatial relationships (Vogel and Jurafsky, 2010; Golland et al., 2010; Artzi and Zettlemoyer, 2013). Most of this work focuses on learning a mapping from text to formal representations, and does not model 2036 implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed. 8.2 Text to Scene Systems Early work on the SHRDLU system (Winograd, 1972) gives a good formalization of the linguistic manipulation of objects in 3D scenes. By re"
D14-1217,P14-5010,1,0.0102955,"r,red) The table[nsubj] is next[dobj] to[prep] the chair[pobj] . next_to(table,chair) There is a table[nsubj] next[advmod] to[prep] a chair[pobj] . next_to(table,chair) Table 4: Example dependency patterns for extracting attributes and spatial relations. 6 Text to Scene generation We generate 3D scenes from brief scene descriptions using our learned priors. 6.1 Scene Template Parsing During scene template parsing we identify the scene type, the objects present in the scene, their attributes, and the relations between them. The input text is first processed using the Stanford CoreNLP pipeline (Manning et al., 2014). The scene type is determined by matching the words in the utterance against a list of known scene types from the scene dataset. To identify objects, we look for noun phrases and use the head word as the category, filtering with WordNet (Miller, 1995) to determine which objects are visualizable (under the physical object synset, excluding locations). We use the Stanford coreference system to determine when the same object is being referred to. To identify properties of the objects, we extract other adjectives and nouns in the noun phrase. We also match dependency patterns such as “X is made o"
D14-1217,Q13-1005,0,\N,Missing
D15-1075,H05-1079,0,0.0281669,"rease in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 ‡ Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground 632 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. A man inspects the uniform of a figure in some East As"
D15-1075,W15-4002,1,0.42771,"all three neural network models, suggesting that research into significantly higher capacity versions of these models would be productive. Figure 3: The neural network classification architecture: for each sentence embedding model evaluated in Tables 6 and 7, two identical copies of the model are run with the two sentences as input, and their outputs are used as the two 100d inputs shown here. sideration possible strong neural models for NLI that directly compare the two inputs at the word or phrase level. Our neural network classifier, depicted in Figure 3 (and based on a one-layer model in Bowman et al. 2015), is simply a stack of three 200d tanh layers, with the bottom layer taking the concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself. We test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997). The word embeddings for all of the mode"
D15-1075,S14-2001,0,0.0154894,"ent, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases. Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 §4.3; Marelli et al. 2014b). For an example of the pitfalls surrounding entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean. The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event, but could also be reasonably labeled as neutral if that assumption is not made. In order to ensure that our labeling scheme assigns a single correct label to every pair, we must select one of these approaches across the board, but both choices present problems. If we opt not to assume that events are coreferent, then"
D15-1075,W04-3205,0,0.0156665,"Missing"
D15-1075,W03-0906,0,0.156506,"Missing"
D15-1075,marelli-etal-2014-sick,0,0.145481,"ent, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases. Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 §4.3; Marelli et al. 2014b). For an example of the pitfalls surrounding entity coreference, consider the sentence pair A boat sank in the Pacific Ocean and A boat sank in the Atlantic Ocean. The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event, but could also be reasonably labeled as neutral if that assumption is not made. In order to ensure that our labeling scheme assigns a single correct label to every pair, we must select one of these approaches across the board, but both choices present problems. If we opt not to assume that events are coreferent, then"
D15-1075,P08-1118,1,0.178349,"Missing"
D15-1075,P15-2070,0,0.23443,"Missing"
D15-1075,D14-1162,1,0.134204,"e concatenated sentence representations as input and the top layer feeding a softmax classifier, all trained jointly with the sentence embedding model itself. We test three sentence embedding models, each set to use 100d phrase and sentence embeddings. Our baseline sentence embedding model simply sums the embeddings of the words in each sentence. In addition, we experiment with two simple sequence embedding models: a plain RNN and an LSTM RNN (Hochreiter and Schmidhuber, 1997). The word embeddings for all of the models are initialized with the 300d reference GloVe vectors (840B token version, Pennington et al. 2014) and fine-tuned as part of training. In addition, all of the models use an additional tanh neural network layer to map these 300d embeddings into the lower-dimensional phrase and sentence embedding space. All of the models are randomly initialized using standard techniques and trained using AdaDelta (Zeiler, 2012) minibatch SGD until performance on the development set stops improving. We applied L2 regularization to all models, manually tuning the strength coefficient λ for each, and additionally applied dropout (Srivastava et al., 2014) to the inputs and outputs of the sen3.4 Analysis and dis"
D15-1075,W07-1401,0,0.11712,"ts such as common lexical resources and evaluation sets. We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP’s full suite of lexical resources. Our initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set (Giampiccolo et al., 2007). We report results in Table 4. Each of the models Partition We distribute the corpus with a prespecified train/test/development split. The test and development sets contain 10k examples each. Each original ImageFlickr caption occurs in only one of the three sets, and all of the examples in the test and development sets have been validated. Parses The distributed corpus includes parses produced by the Stanford PCFG Parser 3.5.2 (Klein and Manning, 2003), trained on the standard training set as well as on the Brown Corpus (Francis and Kucera 1979), which we found to improve the parse quality of"
D15-1075,D13-1170,1,0.072644,"Missing"
D15-1075,P03-1054,1,0.0911655,"Missing"
D15-1075,S14-2055,0,0.0386092,"Missing"
D15-1075,W14-1610,0,0.0526904,"ifiers, and neural network-based models. We find that two models achieve comparable performance: a feature-rich classifier model and a neural network model centered around a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber 1997). We further evaluate the LSTM model by taking advantage of its ready support for transfer learning, and show that it can be adapted to an existing NLI challenge task, yielding the best reported performance by a neural network model and approaching the overall state of the art. 2 plementary training data. Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases. Existing resources suffer from a subtler issue that impacts even projects using only humanprovided annotations: indeterminacies of event and entity coreference lead to insurmountable indeterminacy concerning the correct semantic label (de Marneffe et al. 2008 §4.3; Marelli et al. 2014b)."
D15-1075,P12-2018,1,0.254878,", including models lacking crossbigram features (Feature 6), and lacking all lexical features (Features 4–6). We report results both on the test set and the training set to judge overfitting. for removing all lexicalized features. On our large corpus in particular, there is a substantial jump in accuracy from using lexicalized features, and another from using the very sparse cross-bigram features. The latter result suggests that there is value in letting the classifier automatically learn to recognize structures like explicit negations and adjective modification. A similar result was shown in Wang and Manning (2012) for bigram features in sentiment analysis. It is surprising that the classifier performs as well as it does without any notion of alignment or tree transformations. Although we expect that richer models would perform better, the results suggest that given enough data, cross bigrams with the noisy part-of-speech overlap constraint can produce an effective model. Lexicalized Classifier Unlike the RTE datasets, SNLI’s size supports approaches which make use of rich lexicalized features. We evaluate a simple lexicalized classifier to explore the ability of non-specialized models to exploit these"
D15-1075,W09-3714,1,0.152106,"rs to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time. 1 ‡ Introduction The semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, natural language inference (NLI) — characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning. NLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground 632 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632–642, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. A man inspects the uniform of a figure in some East Asian country. contradiction The man is sleeping An o"
D15-1075,W07-1406,0,0.068274,"Missing"
D15-1075,P14-5008,0,0.0402786,"eate the sentence pair. A gold label reflects a consensus of three votes from among the author and the four annotators. fluent, correctly spelled English, with a mix of full sentences and caption-style noun phrase fragments, though punctuation and capitalization are often omitted. The corpus is available under a CreativeCommons Attribution-ShareAlike license, the same license used for the Flickr30k source captions. It can be downloaded at: nlp.stanford.edu/projects/snli/ 3.1 Excitement Open Platform models The first class of models is from the Excitement Open Platform (EOP, Pad´o et al. 2014; Magnini et al. 2014)—an open source platform for RTE research. EOP is a tool for quickly developing NLI systems while sharing components such as common lexical resources and evaluation sets. We evaluate on two algorithms included in the distribution: a simple edit-distance based algorithm and a classifier-based algorithm, the latter both in a bare form and augmented with EOP’s full suite of lexical resources. Our initial goal was to better understand the difficulty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by run"
D15-1075,H89-1033,0,0.539141,"and the lexicalized model show similar performance when trained on the current full corpus, the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets. We were struck by the speed with which the lexicalized classifier outperforms its unlexicalized 638 73.95 76.78 78.22 Unlexicalized Lexicalized this kind of inference through lexical cues can lead them astray. Some of these examples have qualities reminiscent of Winograd schemas (Winograd, 1972; Levesque, 2013). For example, all the models wrongly predict entailment for A young girl throws sand toward the ocean/A girl can’t stand the ocean, presumably because of distributional associations between throws and can’t stand. Analysis of the models’ predictions also yields insights into the extent to which they grapple with event and entity coreference. For the most part, the original image prompts contained a focal element that the caption writer identified with a syntactic subject, following information structuring conventions associating subjects and topics in English (Ward and Birner"
D15-1075,Q14-1006,0,0.347432,"ese are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6). The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artificially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup1 http://aclweb.org/aclwiki/index.php? title=Textual_Entailment_Resource_Pool 633 York and A tourist visited the city. Assuming coreference between New York and the city justifies labeling the pair as an entailment, but without that assumption the city could be taken to refer to a specific unknown city, leaving the pair neutral. This kind of indeterminacy of label can"
D15-1166,buck-etal-2014-n,0,0.0255292,"Missing"
D15-1166,J07-3002,0,0.0441986,"9 0.34 0.36 0.34 0.32 Table 6: AER scores – results of various models on the RWTH English-German alignment data. weight per target word. Nevertheless, as shown in Table 6, we were able to achieve AER scores comparable to the one-to-many alignments obtained by the Berkeley aligner (Liang et al., 2006).16 We also found that the alignments produced by local attention models achieve lower AERs than those of the global one. The AER obtained by the ensemble, while good, is not better than the local-m AER, suggesting the well-known observation that AER and translation scores are not well correlated (Fraser and Marcu, 2007). Due to space constraint, we can only show alignment visualizations in the arXiv version of our paper.17 5.5 Sample Translations We show in Table 5 sample translations in both directions. It it appealing to observe the effect of attentional models in correctly translating names such as “Miranda Kerr” and “Roger Dow”. Non-attentional models, while producing sensible names from a language model perspective, lack the direct connections from the source side to make correct translations. We also observed an interesting case in the second English-German example, which requires translating the doubl"
D15-1166,P15-1001,0,0.187251,"-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.1 A B C D X Y Z &lt;eos&gt; &lt;eos&gt; X Y Z Figure 1: Neural machine translation – a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z. Here, &lt;eos&gt; marks the end of a sentence. 1 Introduction Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-of-sentence symbol &lt;eos&gt; is reached. It then starts emitting one target word at a time, as illustrated in Figure 1. NMT 1 All our code and models are publicly available at http: //nlp.stanford.edu/projects/nmt. is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences. This means the model does not have to explicitly store gigantic phrase table"
D15-1166,D13-1176,0,0.0859748,"rget word at a time and hence decomposes the conditional probability as: log p(y|x) = Xm j=1 log p (yj |y&lt;j , s) (1) A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the re2 There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task. However, as we detail later, our model is much simpler and can achieve good performance for NMT. 3 All sentences are assumed to terminate with a special “end-of-sentence” token &lt;eos&gt;. cent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit"
D15-1166,N03-1017,0,0.0945494,"l &lt;eos&gt; is reached. It then starts emitting one target word at a time, as illustrated in Figure 1. NMT 1 All our code and models are publicly available at http: //nlp.stanford.edu/projects/nmt. is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences. This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT (Koehn et al., 2003). In parallel, the concept of “attention” has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (Chorowski et al., 2014), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015). In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and al"
D15-1166,N06-1014,0,0.0495717,"word yt in the location-based alignment functions and (b) the input word yt−1 in the content-based functions. 15 With concat, the perplexities achieved by different models are 6.7 (global), 7.1 (local-m), and 7.1 (local-p). Method global (location) local-m (general) local-p (general) ensemble Berkeley Aligner AER 0.39 0.34 0.36 0.34 0.32 Table 6: AER scores – results of various models on the RWTH English-German alignment data. weight per target word. Nevertheless, as shown in Table 6, we were able to achieve AER scores comparable to the one-to-many alignments obtained by the Berkeley aligner (Liang et al., 2006).16 We also found that the alignments produced by local attention models achieve lower AERs than those of the global one. The AER obtained by the ensemble, while good, is not better than the local-m AER, suggesting the well-known observation that AER and translation scores are not well correlated (Fraser and Marcu, 2007). Due to space constraint, we can only show alignment visualizations in the arXiv version of our paper.17 5.5 Sample Translations We show in Table 5 sample translations in both directions. It it appealing to observe the effect of attentional models in correctly translating name"
D15-1166,P15-1002,1,0.270447,"ion architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.1 A B C D X Y Z &lt;eos&gt; &lt;eos&gt; X Y Z Figure 1: Neural machine translation – a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z. Here, &lt;eos&gt; marks the end of a sentence. 1 Introduction Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-of-sentence symbol &lt;eos&gt; is reached. It then starts emitting one target word at a time, as illustrated in Figure 1. NMT 1 All our code and models are publicly available at http: //nlp.stanford.edu/projects/nmt. is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences. This means the model does not have"
D15-1166,P02-1040,0,0.133018,"entional approach with an additional constraint added to the training objective to make sure the model pays equal attention to all parts of the image during the caption generation process. Such a constraint can 11 If n is the number of LSTM cells, the input size of the first LSTM layer is 2n; those of subsequent layers are n. We evaluate the effectiveness of our models on the WMT translation tasks between English and German in both directions. newstest2013 (3000 sentences) is used as a development set to select our hyperparameters. Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences) and newstest2015 (2169 sentences). Following (Luong et al., 2015), we report translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work and (b) NIST13 BLEU to be comparable with WMT results. 4.1 Training Details All our models are trained on the WMT’14 training data consisting of 4.5M sentences pairs (116M English words, 110M German words). Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages. Words not in these shortlisted vocabularies are converted in"
D15-1166,D14-1179,0,\N,Missing
D16-1245,P14-1005,0,0.0446754,"Missing"
D16-1245,P15-1136,1,0.815934,"rett and Klein, 2013). These models are typically trained with heuristic loss functions that assign costs to different error types, as in the heuristic loss we describe in Section 3.1 (Fernandes et al., 2012; Durrett et al., 2013; Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Martschat and Strube, 2015; Wiseman et al., 2016). To the best of our knowledge reinforcement learning has not been applied to coreference resolution before. However, imitation learning algorithms such as SEARN (Daum´e III et al., 2009) have been used to train coreference resolvers (Daum´e III, 2006; Ma et al., 2014; Clark and Manning, 2015). These algorithms also directly optimize for coreference evaluation metrics, but they require an expert policy to learn from instead of relying on rewards alone. 6 Conclusion We propose using reinforcement learning to directly optimize mention-ranking models for coreference evaluation metrics, obviating the need for hyperparameters that must be carefully selected for each particular language, dataset, and evaluation metric. Our reward-rescaling approach also increases the model’s accuracy, resulting in significant gains over the current state-of-the-art. Acknowledgments We thank Kelvin Guu, W"
D16-1245,P16-1061,1,0.783992,"ss as well as benefiting from directly optimizing for coreference metrics. Error analysis shows that using the reward-rescaling loss results in a similar number of mistakes as the heuristic loss, but the mistakes tend to be less severe. 1 Code and trained models https://github.com/clarkkev/deep-coref. are available 2256 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2256–2262, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics at 2 3 Neural Mention-Ranking Model We use the neural mention-ranking model described in Clark and Manning (2016), which we briefly go over in this section. Given a mention m and candidate antecedent c, the mention-ranking model produces a score for the pair s(c, m) indicating their compatibility for coreference with a feedforward neural network. The candidate antecedent may be any mention that occurs before m in the document or NA, indicating that m has no antecedent. Input Layer. For each mention, the model extracts various words (e.g., the mention’s head word) and groups of words (e.g., all words in the mention’s sentence) that are fed into the neural network. Each word is represented by a vector wi ∈"
D16-1245,D13-1203,0,0.384169,"aling. We also test the REINFORCE policy gradient algorithm (Williams, 1992). Our model is a neural mention-ranking model. Mention-ranking models score pairs of mentions for their likelihood of coreference rather than comparing partial coreference clusters. Hence they operate in a simple setting where coreference decisions are made independently. Although they are less expressive than entity-centric approaches to coreference (e.g., Haghighi and Klein, 2010), mention-ranking models are fast, scalable, and simple to train, causing them to be the dominant approach to coreference in recent years (Durrett and Klein, 2013; Wiseman et al., 2015). Having independent actions is particularly useful when applying reinforcement learning because it means a particular action’s effect on the final reward can be computed efficiently. We evaluate the models on the English and Chinese portions of the CoNLL 2012 Shared Task. The REINFORCE algorithm is competitive with a heuristic loss function while the reward-rescaled objective significantly outperforms both1 . We attribute this to reward rescaling being well suited for a ranking task due to its max-margin loss as well as benefiting from directly optimizing for coreferenc"
D16-1245,P13-1012,0,0.196848,"at are tuned via hyperparameters. These hyperparameters are usually given as costs for different error types, which are used to bias the coreference system towards making more or fewer coreference links. In this section we first describe a heuristic loss function incorporating this idea from Wiseman et al. (2015). We then propose new training procedures based on reinforcement learning that instead directly optimize for coreference evaluation metrics. 3.1 Heuristic Max-Margin Objective The heuristic loss from Wiseman et al. is governed by the following error types, which were first proposed by Durrett et al. (2013). Suppose the training set consists of N mentions m1 , m2 , ..., mN . Let C(mi ) denote the set of candidate antecedents of a mention mi (i.e., mentions preceding mi and NA) and T (mi ) denote the set of true antecedents of mi (i.e., mentions preceding mi that are coreferent with it or {NA} if mi has no antecedent). Then we define the following costs for linking mi to a candidate antecedent c ∈ C(mi ):   αFN if c = NA ∧ T (mi ) 6= {NA}    α if c 6= NA ∧ T (mi ) = {NA} FA ∆h (c, mi ) = αWL if c 6= NA ∧ a ∈ / T (mi )    0 if a ∈ T (mi ) for “false new,” “false anaphor,” “wrong link”,"
D16-1245,N10-1061,0,0.0267228,"max-margin coreference objective proposed by Wiseman et al. (2015) by incorporating the reward associated with each coreference decision into the loss’s slack rescaling. We also test the REINFORCE policy gradient algorithm (Williams, 1992). Our model is a neural mention-ranking model. Mention-ranking models score pairs of mentions for their likelihood of coreference rather than comparing partial coreference clusters. Hence they operate in a simple setting where coreference decisions are made independently. Although they are less expressive than entity-centric approaches to coreference (e.g., Haghighi and Klein, 2010), mention-ranking models are fast, scalable, and simple to train, causing them to be the dominant approach to coreference in recent years (Durrett and Klein, 2013; Wiseman et al., 2015). Having independent actions is particularly useful when applying reinforcement learning because it means a particular action’s effect on the final reward can be computed efficiently. We evaluate the models on the English and Chinese portions of the CoNLL 2012 Shared Task. The REINFORCE algorithm is competitive with a heuristic loss function while the reward-rescaled objective significantly outperforms both1 . W"
D16-1245,H05-1004,0,0.0240251,"document mi to a candidate antecedent. Formally, we denote the set of actions available for the ith mention as Ai = {(c, mi ) : c ∈ C(mi )}, where an action (c, m) adds a coreference link between mentions m and c. The mentionranking model assigns each action the score s(c, m) and takes the highest-scoring action at each step. Once the agent has executed a sequence of actions, it observes a reward R(a1:T ), which can be any function. We use the B3 coreference metric for this reward (Bagga and Baldwin, 1998). Although our system evaluation also includes the MUC (Vilain et al., 1995) and CEAFφ4 (Luo, 2005) metrics, we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAFφ4 is slow to compute. Reward Rescaling. Crucially, the actions taken by a mention-ranking model are independent. This means it is possible to change any action ai to a different one a0i ∈ Ai and see what reward the model would have gotten by taking that action instead: R(a1 , ..., ai−1 , a0i , ai+1 , ..., aT ). We use this idea to improve the slack-rescaling parameter ∆ in the maxmargin loss L(θ). Instead of setting its value based on the error type, we compute exactly how much 2"
D16-1245,D14-1225,0,0.0649512,"and Ng, 2009; Durrett and Klein, 2013). These models are typically trained with heuristic loss functions that assign costs to different error types, as in the heuristic loss we describe in Section 3.1 (Fernandes et al., 2012; Durrett et al., 2013; Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Martschat and Strube, 2015; Wiseman et al., 2016). To the best of our knowledge reinforcement learning has not been applied to coreference resolution before. However, imitation learning algorithms such as SEARN (Daum´e III et al., 2009) have been used to train coreference resolvers (Daum´e III, 2006; Ma et al., 2014; Clark and Manning, 2015). These algorithms also directly optimize for coreference evaluation metrics, but they require an expert policy to learn from instead of relying on rewards alone. 6 Conclusion We propose using reinforcement learning to directly optimize mention-ranking models for coreference evaluation metrics, obviating the need for hyperparameters that must be carefully selected for each particular language, dataset, and evaluation metric. Our reward-rescaling approach also increases the model’s accuracy, resulting in significant gains over the current state-of-the-art. Acknowledgme"
D16-1245,Q15-1029,0,0.0411264,"xamples during training, the rewardrescaling model creates significantly fewer wrong links than the heuristic loss, which is trained using a fixed cost of 1.0 for all wrong links. 5 Related Work Mention-ranking models have been widely used for coreference resolution (Denis and Baldridge, 2007; Rahman and Ng, 2009; Durrett and Klein, 2013). These models are typically trained with heuristic loss functions that assign costs to different error types, as in the heuristic loss we describe in Section 3.1 (Fernandes et al., 2012; Durrett et al., 2013; Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Martschat and Strube, 2015; Wiseman et al., 2016). To the best of our knowledge reinforcement learning has not been applied to coreference resolution before. However, imitation learning algorithms such as SEARN (Daum´e III et al., 2009) have been used to train coreference resolvers (Daum´e III, 2006; Ma et al., 2014; Clark and Manning, 2015). These algorithms also directly optimize for coreference evaluation metrics, but they require an expert policy to learn from instead of relying on rewards alone. 6 Conclusion We propose using reinforcement learning to directly optimize mention-ranking models for coreference evaluat"
D16-1245,W12-4501,0,0.35037,"ed estimate of the gradient by sampling a sequence of actions a1:T according to pθ and computing the gradient only over the sample. We take advantage of the independence of actions by using the following gradient estimate, which has lower variance than the standard REINFORCE gradient estimate. ∇θ J(θ) ≈ T P P i=1 a0i ∈Ai [∇θ pθ (a0i )](R(a1 , ..., a0i , ..., aT ) − bi ) where bi is a baseline used to reduce the variance, which we set to Ea0i ∈Ai ∼pθ R(a1 , ..., a0i , ..., aT ). 4 Experiments and Results We run experiments on the English and Chinese portions of the CoNLL 2012 Shared Task data (Pradhan et al., 2012) and evaluate with the MUC, B3 , and CEAFφ4 metrics. Our experiments were run using predicted mentions from Stanford’s rule-based coreference system (Raghunathan et al., 2010). We follow the training methodology from Clark and Manning (2016): hidden layers of sizes M1 = 1000, M2 = M3 = 500, the RMSprop optimizer Prec. MUC Rec. F1 Prec. B3 Rec. Prec. F1 CEAFφ4 Rec. F1 CoNLL 2012 English Test Data Avg. F1 Error Types 77.49 69.75 73.42 79.91 69.30 74.23 6 66.83 56.95 61.50 71.01 56.53 62.95 62.14 53.85 57.70 63.84 54.33 58.70 64.21 FA 65.29 Heuristic Loss 79.63 70.25 74.65 80.08 69.61 74.48 79.19"
D16-1245,D10-1048,1,0.874821,"f actions by using the following gradient estimate, which has lower variance than the standard REINFORCE gradient estimate. ∇θ J(θ) ≈ T P P i=1 a0i ∈Ai [∇θ pθ (a0i )](R(a1 , ..., a0i , ..., aT ) − bi ) where bi is a baseline used to reduce the variance, which we set to Ea0i ∈Ai ∼pθ R(a1 , ..., a0i , ..., aT ). 4 Experiments and Results We run experiments on the English and Chinese portions of the CoNLL 2012 Shared Task data (Pradhan et al., 2012) and evaluate with the MUC, B3 , and CEAFφ4 metrics. Our experiments were run using predicted mentions from Stanford’s rule-based coreference system (Raghunathan et al., 2010). We follow the training methodology from Clark and Manning (2016): hidden layers of sizes M1 = 1000, M2 = M3 = 500, the RMSprop optimizer Prec. MUC Rec. F1 Prec. B3 Rec. Prec. F1 CEAFφ4 Rec. F1 CoNLL 2012 English Test Data Avg. F1 Error Types 77.49 69.75 73.42 79.91 69.30 74.23 6 66.83 56.95 61.50 71.01 56.53 62.95 62.14 53.85 57.70 63.84 54.33 58.70 64.21 FA 65.29 Heuristic Loss 79.63 70.25 74.65 80.08 69.61 74.48 79.19 70.44 74.56 69.21 70.70 69.93 4 57.87 63.03 56.96 63.09 57.99 63.40 2 63.62 53.97 58.40 63.59 54.46 58.67 63.46 55.52 59.23 65.36 WL 65.41 65.73 REINFORCE Reward Rescaling De"
D16-1245,D09-1101,0,0.125062,"cost “false anaphoric” errors. The pronouns in the “telephone conversation” genre often group into extremely large coreference clusters, which means a “wrong link” error can have a very large negative effect on the score. This is reflected in its high average cost of 1.21. After prior2260 itizing these examples during training, the rewardrescaling model creates significantly fewer wrong links than the heuristic loss, which is trained using a fixed cost of 1.0 for all wrong links. 5 Related Work Mention-ranking models have been widely used for coreference resolution (Denis and Baldridge, 2007; Rahman and Ng, 2009; Durrett and Klein, 2013). These models are typically trained with heuristic loss functions that assign costs to different error types, as in the heuristic loss we describe in Section 3.1 (Fernandes et al., 2012; Durrett et al., 2013; Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Martschat and Strube, 2015; Wiseman et al., 2016). To the best of our knowledge reinforcement learning has not been applied to coreference resolution before. However, imitation learning algorithms such as SEARN (Daum´e III et al., 2009) have been used to train coreference resolvers (Daum´e III, 2006; Ma et al., 2"
D16-1245,M95-1005,0,0.14871,"ai links the ith mention in the document mi to a candidate antecedent. Formally, we denote the set of actions available for the ith mention as Ai = {(c, mi ) : c ∈ C(mi )}, where an action (c, m) adds a coreference link between mentions m and c. The mentionranking model assigns each action the score s(c, m) and takes the highest-scoring action at each step. Once the agent has executed a sequence of actions, it observes a reward R(a1:T ), which can be any function. We use the B3 coreference metric for this reward (Bagga and Baldwin, 1998). Although our system evaluation also includes the MUC (Vilain et al., 1995) and CEAFφ4 (Luo, 2005) metrics, we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAFφ4 is slow to compute. Reward Rescaling. Crucially, the actions taken by a mention-ranking model are independent. This means it is possible to change any action ai to a different one a0i ∈ Ai and see what reward the model would have gotten by taking that action instead: R(a1 , ..., ai−1 , a0i , ai+1 , ..., aT ). We use this idea to improve the slack-rescaling parameter ∆ in the maxmargin loss L(θ). Instead of setting its value based on the error type, we com"
D16-1245,P15-1137,0,0.497394,"in hyperparameters that are carefully selected to ensure the model performs well according to coreference evaluation metrics. This complicates training, especially across different languages and datasets where systems may work best with different settings of the hyperparameters. To address this, we explore using two variants of reinforcement learning to directly optimize a coreference system for coreference evaluation metrics. In Christopher D. Manning Computer Science Department Stanford University manning@cs.stanford.edu particular, we modify the max-margin coreference objective proposed by Wiseman et al. (2015) by incorporating the reward associated with each coreference decision into the loss’s slack rescaling. We also test the REINFORCE policy gradient algorithm (Williams, 1992). Our model is a neural mention-ranking model. Mention-ranking models score pairs of mentions for their likelihood of coreference rather than comparing partial coreference clusters. Hence they operate in a simple setting where coreference decisions are made independently. Although they are less expressive than entity-centric approaches to coreference (e.g., Haghighi and Klein, 2010), mention-ranking models are fast, scalabl"
D16-1245,N16-1114,0,0.451405,"rewardrescaling model creates significantly fewer wrong links than the heuristic loss, which is trained using a fixed cost of 1.0 for all wrong links. 5 Related Work Mention-ranking models have been widely used for coreference resolution (Denis and Baldridge, 2007; Rahman and Ng, 2009; Durrett and Klein, 2013). These models are typically trained with heuristic loss functions that assign costs to different error types, as in the heuristic loss we describe in Section 3.1 (Fernandes et al., 2012; Durrett et al., 2013; Bj¨orkelund and Kuhn, 2014; Wiseman et al., 2015; Martschat and Strube, 2015; Wiseman et al., 2016). To the best of our knowledge reinforcement learning has not been applied to coreference resolution before. However, imitation learning algorithms such as SEARN (Daum´e III et al., 2009) have been used to train coreference resolvers (Daum´e III, 2006; Ma et al., 2014; Clark and Manning, 2015). These algorithms also directly optimize for coreference evaluation metrics, but they require an expert policy to learn from instead of relying on rewards alone. 6 Conclusion We propose using reinforcement learning to directly optimize mention-ranking models for coreference evaluation metrics, obviating"
D16-1245,D08-1067,0,\N,Missing
D17-1004,P15-1136,1,0.642607,"Missing"
D17-1004,N16-1097,0,0.0399805,"Missing"
D17-1004,D14-1164,1,0.842999,"Missing"
D17-1004,D11-1142,0,0.028752,"Missing"
D17-1004,D14-1162,1,0.114464,"output) that are carefully tuned for TAC KBP slot filling evaluation. It uses a comprehensive feature set similar to the MIML-RE system for relation extraction (Surdeanu et al., 2012), including lemmatized n-grams, sequence NER tags and POS tags, positions of entities, and various features over dependency paths, etc. In addition to the above models, we also compare our proposed model against an LSTM sequence model without attention mechanism. 4.2 Implementation Details We map words that occur less than 2 times in the training set to a special &lt;UNK&gt; token. We use the pre-trained GloVe vectors (Pennington et al., 2014) to initialize word embeddings. For all the LSTM layers, we find that 2-layer stacked LSTMs generally work better than one-layer LSTMs. We minimize cross-entropy loss over all 42 relations using AdaGrad (Duchi et al., 2011). We apply Dropout with p = 0.5 to CNNs and LSTMs. During training we also find a word dropout strategy to be very effective: we randomly set a token to be &lt;UNK&gt; with a probability p. We set p to be 0.06 for the SDP-LSTM model and 0.04 for all other models. Convolutional neural networks. We follow the 1-dimensional CNN architecture by Nguyen and Grishman (2015) for relation"
D17-1004,W09-2415,0,0.515755,"Missing"
D17-1004,P11-1138,0,0.00978825,"d recall. CNN-based models tend to have higher precision; RNN-based models have better recall. This can be explained by noting that the filters in CNNs are essentially a form of “fuzzy n-gram patterns”. 2 In the TAC KBP cold start slot filling evaluation, a hop-1 slot is transferred to a pseudo-slot which is treated equally as a hop-0 slot. Hop-all precision, recall and F1 are then calculated by combining these pseudo-slot predictions and hop-0 predictions. 3 This system uses the fine-grained NER system in Stanford CoreNLP (Manning et al., 2014) for entity detection and the Illinois Wikifier (Ratinov et al., 2011) for entity linking. 40 Model P Hop-0 R F1 P Hop-1 R F1 Hop-all R F1 P Patterns 63.8 17.7 27.7 49.3 8.6 14.7 58.9 13.3 21.8 LR 36.6 21.9 27.4 15.1 10.1 12.2 25.6 16.3 19.9 + Patterns (2015 winning system) 37.5 24.5 29.7 16.5 12.8 14.4 26.6 19.0 22.2 LR trained on TACRED + Patterns 32.7 36.5 20.6 26.5 25.3 30.7 7.9 11.0 9.5 15.3 8.6 12.8 16.8 20.1 15.3 21.2 16.0 20.6 Our model + Patterns 39.0 28.9 33.2 17.7 13.9 15.6 28.2 21.5 24.4 40.2 31.5 35.3 19.4 16.5 17.8 29.7 24.2 26.7 Table 5: Model performance on TAC KBP 2015 slot filling evaluation, micro-averaged over queries. Hop-0 scores are calcul"
D17-1004,W09-1401,0,0.0156053,"en compared with SDP-LSTM model, our model achieves improved F1 scores on 26 out of the 41 slot types, with the top 5 slot types being org:political/religious affiliation, per:country of death, org:alternate names, per:religion and per:alternate names. We observe that slot types with relatively sparse training examples tend to be improved by using the position-aware attention model. Datasets for relation extraction. Popular general-domain datasets include the ACE dataset (Strassel et al., 2008) and the SemEval-2010 task 8 dataset (Hendrickx et al., 2009). In addition, the BioNLP Shared Tasks (Kim et al., 2009) are yearly efforts on creating datasets and evaluations for biomedical information extraction systems. Deep learning models for relation extraction. Many deep learning models have been proposed for relation extraction, with a focus on end-to-end training using CNNs (Zeng et al., 2014; Nguyen and Grishman, 2015) and RNNs (Zhang et al., 2015). Other popular approaches include using CNN or RNN over dependency paths between entities (Xu et al., 2015a,b), augmenting RNNs with different components (Xu et al., 2016; Zhou et al., 2016), and combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016)"
D17-1004,strassel-etal-2008-linguistic,0,0.0154821,"pes, with the top 5 slot types being org:members, per:country of death, org:shareholders, per:children and per:religion. When compared with SDP-LSTM model, our model achieves improved F1 scores on 26 out of the 41 slot types, with the top 5 slot types being org:political/religious affiliation, per:country of death, org:alternate names, per:religion and per:alternate names. We observe that slot types with relatively sparse training examples tend to be improved by using the position-aware attention model. Datasets for relation extraction. Popular general-domain datasets include the ACE dataset (Strassel et al., 2008) and the SemEval-2010 task 8 dataset (Hendrickx et al., 2009). In addition, the BioNLP Shared Tasks (Kim et al., 2009) are yearly efforts on creating datasets and evaluations for biomedical information extraction systems. Deep learning models for relation extraction. Many deep learning models have been proposed for relation extraction, with a focus on end-to-end training using CNNs (Zeng et al., 2014; Nguyen and Grishman, 2015) and RNNs (Zhang et al., 2015). Other popular approaches include using CNN or RNN over dependency paths between entities (Xu et al., 2015a,b), augmenting RNNs with diffe"
D17-1004,P14-5010,1,0.0869507,". Entity masking. We replace each subject entity in the original sentence with a special &lt;NER&gt;SUBJ token where &lt;NER&gt; is the corresponding NER signature of the subject as provided in TACRED. We do the same processing for object entities. This processing step helps (1) provide a model with entity type information, and (2) prevent a model from overfitting its predictions to specific entities. Multi-channel augmentation. Instead of using only word vectors as input to the network, we augment the input with part-of-speech (POS) and named entity recognition (NER) embeddings. We run Stanford CoreNLP (Manning et al., 2014) to obtain the POS and NER annotations. Dependency-based recurrent neural networks. In dependency-based neural models, shortest dependency paths between entities are often used as input to the neural networks. The intuition is to eliminate tokens that are potentially less relevant 39 Model P R F1 query entity: Mike Penner Traditional Patterns 85.3 23.4 36.8 LR 72.0 47.8 57.5 LR + Patterns 71.4 50.1 58.9 Neural CNN CNN-PE SDP-LSTM LSTM Our model 72.1 68.2 62.0 61.4 67.7 50.3 55.4 54.8 61.7 63.2 Ensemble 69.4 64.8 67.0 Lisa Dillman hop-1 slot: per:title Sportswriter (query) 59.2 61.1 58.2 61.5 6"
D17-1004,D12-1042,1,0.862684,"C KBP 2015 cold start slot filling task (Angeli et al., 2015). At the core of this system are two relation extractors: a pattern-based extractor and a logistic regression (LR) classifier. The pattern-based system uses a total of 4,528 surface patterns and 169 dependency patterns. The logistic regression model was trained on approximately 2 million bootstrapped examples (using a small annotated dataset and high-precision pattern system output) that are carefully tuned for TAC KBP slot filling evaluation. It uses a comprehensive feature set similar to the MIML-RE system for relation extraction (Surdeanu et al., 2012), including lemmatized n-grams, sequence NER tags and POS tags, positions of entities, and various features over dependency paths, etc. In addition to the above models, we also compare our proposed model against an LSTM sequence model without attention mechanism. 4.2 Implementation Details We map words that occur less than 2 times in the training set to a special &lt;UNK&gt; token. We use the pre-trained GloVe vectors (Pennington et al., 2014) to initialize word embeddings. For all the LSTM layers, we find that 2-layer stacked LSTMs generally work better than one-layer LSTMs. We minimize cross-entro"
D17-1004,D12-1048,0,0.0213337,"Missing"
D17-1004,N16-1065,0,0.0959238,"Missing"
D17-1004,P16-1123,0,0.415302,"Missing"
D17-1004,P09-1113,0,0.668972,"e in the form of “knowledge graphs” has become an important knowledge resource. These graphs are now extensively used by search engine companies, both to provide information to end-users and internally to the system, as a way to understand relationships. However, up until now, automatic knowledge extraction has proven sufficiently difficult that most of the facts in these knowledge graphs have been built up by hand. It is therefore a key challenge to show that NLP technology can effectively contribute to this important problem. Existing work on relation extraction (e.g., Zelenko et al., 2003; Mintz et al., 2009; Adel et al., 2016) has been unable to achieve sufficient recall or precision for the results to be usable versus hand-constructed knowledge bases. Supervised training data has been scarce and, while techniques like distant supervision appear to be a promising way to extend knowledge bases at low cost, in practice the training data has often been too noisy for reliable training of relation extraction systems (Angeli et al., 2015). As a result most systems fail to make correct extractions even in apparently straightforward cases like Figure 1, Introduction A basic but highly important challeng"
D17-1004,D15-1062,0,0.246919,"Experiments In this section we evaluate the effectiveness of our proposed model and TACRED on improving slot 38 filling systems. Specifically, we run two sets of experiments: (1) we evaluate model performance on the relation extraction task using TACRED, and (2) we evaluate model performance on the TAC KBP 2015 cold start slot filling task, by training the models on TACRED. 4.1 to the classification of the relation. For the example in Figure 1, the shortest dependency path between the two entities is: [Penner] survived ! brother ! wife ! [Lisa Dillman] We follow the SDP-LSTM model proposed by Xu et al. (2015b). In this model, each shortest dependency path is divided into two separate sub-paths from the subject entity and the object entity to the lowest common ancestor node. Each sub-path is fed into an LSTM network, and the resulting hidden units at each word position are passed into a max-over-time pooling layer to form the output of this sub-path. Outputs from the two sub-paths are then concatenated to form the final representation. Baseline Models We compare our model against the following baseline models for relation extraction and slot filling: TAC KBP 2015 winning system. To judge our propo"
D17-1004,W15-1506,0,0.548999,"GloVe vectors (Pennington et al., 2014) to initialize word embeddings. For all the LSTM layers, we find that 2-layer stacked LSTMs generally work better than one-layer LSTMs. We minimize cross-entropy loss over all 42 relations using AdaGrad (Duchi et al., 2011). We apply Dropout with p = 0.5 to CNNs and LSTMs. During training we also find a word dropout strategy to be very effective: we randomly set a token to be &lt;UNK&gt; with a probability p. We set p to be 0.06 for the SDP-LSTM model and 0.04 for all other models. Convolutional neural networks. We follow the 1-dimensional CNN architecture by Nguyen and Grishman (2015) for relation extraction. This model learns a representation of the input sentence, by first running a series of convolutional operations on the sentence with various filters, and then feeding the output into a max-pooling layer to reduce the dimension. The resulting representation is then fed into a fully-connected layer followed by a softmax layer for relation classification. As an extension, positional embeddings are also introduced into this model to better capture the relative position of each word to the subject and object entities and were shown to achieve improved results. We use “CNN-"
D17-1004,C16-1138,0,0.0282105,"2010 task 8 dataset (Hendrickx et al., 2009). In addition, the BioNLP Shared Tasks (Kim et al., 2009) are yearly efforts on creating datasets and evaluations for biomedical information extraction systems. Deep learning models for relation extraction. Many deep learning models have been proposed for relation extraction, with a focus on end-to-end training using CNNs (Zeng et al., 2014; Nguyen and Grishman, 2015) and RNNs (Zhang et al., 2015). Other popular approaches include using CNN or RNN over dependency paths between entities (Xu et al., 2015a,b), augmenting RNNs with different components (Xu et al., 2016; Zhou et al., 2016), and combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016). Adel et al. (2016) compares the performance of CNN models against traditional approaches on slot filling using a portion of the TAC KBP evaluation data. Attention visualization. Lastly, Figure 6 shows the visualization of attention weights assigned by our model on sampled sentences from the development set. We find that the model learns to pay more attention to words that are informative for the relation (e.g., “graduated from”, “niece” and “chairman”), though it still makes mistakes (e.g., “refused to name"
D17-1004,D15-1206,0,0.464521,"Experiments In this section we evaluate the effectiveness of our proposed model and TACRED on improving slot 38 filling systems. Specifically, we run two sets of experiments: (1) we evaluate model performance on the relation extraction task using TACRED, and (2) we evaluate model performance on the TAC KBP 2015 cold start slot filling task, by training the models on TACRED. 4.1 to the classification of the relation. For the example in Figure 1, the shortest dependency path between the two entities is: [Penner] survived ! brother ! wife ! [Lisa Dillman] We follow the SDP-LSTM model proposed by Xu et al. (2015b). In this model, each shortest dependency path is divided into two separate sub-paths from the subject entity and the object entity to the lowest common ancestor node. Each sub-path is fed into an LSTM network, and the resulting hidden units at each word position are passed into a max-over-time pooling layer to form the output of this sub-path. Outputs from the two sub-paths are then concatenated to form the final representation. Baseline Models We compare our model against the following baseline models for relation extraction and slot filling: TAC KBP 2015 winning system. To judge our propo"
D17-1004,C14-1220,0,0.742239,"ithin the sequence. We formalize the relation extraction task as follows: Let X = [x1 , ..., xn ] denote a sentence, where xi is the i-th token. A subject entity s and an object entity o are identified in the sentence, corresponding to two non-overlapping consecutive spans: Xs = [xs1 , xs1 +1 , . . . , xs2 ] and Xo = [xo1 , xo1 +1 , . . . , xo2 ]. Given the sentence X and the positions of s and o, the goal is to predict a relation r 2 R (R is the set of relations) that holds between s and o or no relation otherwise. Inspired by the position encoding vectors used in Collobert et al. (2011) and Zeng et al. (2014), we define a position sequence relative to the subject entity [ps1 , ..., psn ], where 8 &gt; &lt;i s1 , i &lt; s1 s pi = 0, (1) s 1  i  s2 &gt; : i s2 , i &gt; s2 (3) (4) Here Wh , Wq 2 Rda ⇥d , Ws , Wo 2 Rda ⇥dp and v 2 Rda are learnable parameters of the network, where d is the dimension of hidden states, dp is the dimension of position embeddings, and da is the size of attention layer. Additional parameters of the network include embedding matrices E 2 R|V|⇥d and P 2 R(2L 1)⇥dp , where V is the vocabulary and L is the maximum sentence length. We regard attention weight ai as the relative contribution"
D17-1004,P16-2034,0,0.0925375,"et (Hendrickx et al., 2009). In addition, the BioNLP Shared Tasks (Kim et al., 2009) are yearly efforts on creating datasets and evaluations for biomedical information extraction systems. Deep learning models for relation extraction. Many deep learning models have been proposed for relation extraction, with a focus on end-to-end training using CNNs (Zeng et al., 2014; Nguyen and Grishman, 2015) and RNNs (Zhang et al., 2015). Other popular approaches include using CNN or RNN over dependency paths between entities (Xu et al., 2015a,b), augmenting RNNs with different components (Xu et al., 2016; Zhou et al., 2016), and combining RNNs and CNNs (Vu et al., 2016; Wang et al., 2016). Adel et al. (2016) compares the performance of CNN models against traditional approaches on slot filling using a portion of the TAC KBP evaluation data. Attention visualization. Lastly, Figure 6 shows the visualization of attention weights assigned by our model on sampled sentences from the development set. We find that the model learns to pay more attention to words that are informative for the relation (e.g., “graduated from”, “niece” and “chairman”), though it still makes mistakes (e.g., “refused to name the three”). We als"
D17-1004,H05-1091,0,\N,Missing
D17-1109,N16-1097,0,0.0220822,"Missing"
D17-1109,D14-1164,1,0.894392,"Missing"
D17-1109,D13-1160,1,0.494161,"ve manner. 1 Relation instances Knowledge Base Linked Entities Debbie Reynolds her title entertainer Fisher daughter child of Twitter Figure 1: An example describing entities and relations in knowledge base population. Harnessing the wealth of information present in unstructured text online has been a long standing goal for the natural language processing community. In particular, knowledge base population seeks to automatically construct a knowledge base consisting of relations between entities from a document corpus. Knowledge bases have found many applications including question answering (Berant et al., 2013; Fader et al., 2014; Authors contributed equally. Debbie Reynolds, per:parents, Fisher + Introduction ∗ Debbie Reynolds , title, entertainer Reddy et al., 2014), automated reasoning (Kalyanpur et al., 2012) and dialogue (Han et al., 2015). Evaluating these systems remains a challenge as it is not economically feasible to exhaustively annotate every possible candidate relation from a sufficiently large corpus. As a result, a poolingbased methodology is used in practice to construct datasets, similar to them methodology used in information retrieval (Jones and Rijsbergen, 1975; Harman, 1993). F"
D17-1109,W15-4616,0,0.0287141,"of information present in unstructured text online has been a long standing goal for the natural language processing community. In particular, knowledge base population seeks to automatically construct a knowledge base consisting of relations between entities from a document corpus. Knowledge bases have found many applications including question answering (Berant et al., 2013; Fader et al., 2014; Authors contributed equally. Debbie Reynolds, per:parents, Fisher + Introduction ∗ Debbie Reynolds , title, entertainer Reddy et al., 2014), automated reasoning (Kalyanpur et al., 2012) and dialogue (Han et al., 2015). Evaluating these systems remains a challenge as it is not economically feasible to exhaustively annotate every possible candidate relation from a sufficiently large corpus. As a result, a poolingbased methodology is used in practice to construct datasets, similar to them methodology used in information retrieval (Jones and Rijsbergen, 1975; Harman, 1993). For instance, at the annual NIST TAC KBP evaluation, all relations predicted by participating systems are pooled together, annotated and released as a dataset for researchers to develop and evaluate their systems on. However, during develop"
D17-1109,D15-1076,0,0.0293716,"Missing"
D17-1109,N16-1104,0,0.101056,"Missing"
D17-1109,P14-5010,1,0.0251035,"Missing"
D17-1109,D16-1106,0,0.0216071,"Missing"
D17-1109,P11-1138,0,0.0166738,"Missing"
D17-1109,Q14-1030,0,0.0136056,"bing entities and relations in knowledge base population. Harnessing the wealth of information present in unstructured text online has been a long standing goal for the natural language processing community. In particular, knowledge base population seeks to automatically construct a knowledge base consisting of relations between entities from a document corpus. Knowledge bases have found many applications including question answering (Berant et al., 2013; Fader et al., 2014; Authors contributed equally. Debbie Reynolds, per:parents, Fisher + Introduction ∗ Debbie Reynolds , title, entertainer Reddy et al., 2014), automated reasoning (Kalyanpur et al., 2012) and dialogue (Han et al., 2015). Evaluating these systems remains a challenge as it is not economically feasible to exhaustively annotate every possible candidate relation from a sufficiently large corpus. As a result, a poolingbased methodology is used in practice to construct datasets, similar to them methodology used in information retrieval (Jones and Rijsbergen, 1975; Harman, 1993). For instance, at the annual NIST TAC KBP evaluation, all relations predicted by participating systems are pooled together, annotated and released as a dataset for"
D17-1109,P14-1122,0,0.027738,"Missing"
D18-1008,D14-1159,1,0.817362,"and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This i"
D18-1008,P16-1055,1,0.768267,"and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This includes quantitative reasoning (Kushman et al., 2014; Roy et al., 2015), numerical information extraction (Madaan et al., 2016), and techniques for making numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural modeling. Recent work has shown the promise of sophisticated neural models on semantic role labeling (He et al., 2017). Similar to other such sequence prediction models,"
D18-1008,D12-1062,0,0.0197056,"he most likely role for each span and edge and then discarding any edges and spans that violate the wellformedness (1) and typing constraints (2). We then enforce transitivity constraints (4) by incrementally building a cluster of analogous and equivalent spans. We then resolve the unique facts constraint (3) by keeping only the span with highest FACT edge score. Finally, for every cluster of analogous VALUE spans, we check that the analogy constraint (5) holds and if not, discard the cluster. We also implement an optimal decoder that encodes the TAP constraints as an ILP (Roth and Yih, 2004; Do et al., 2012). The ILP tries to find an optimal decoding according to the model, subject to hard constraints imposed on the solution space. For example, we require that solutions satisfy the ‘connected spans’ constraint: which defines a joint distribution over per-token role labels. We thus obtain spans from this distribution corresponding to vertices of the graph described in Section 4 by merging contiguous rolelabels in the maximum likelihood label sequence predicted by the CRF. Edge prediction with PATH M AX features. For edge prediction, we use the spans identified above to construct span and edge embe"
D18-1008,J02-3001,0,0.03861,"system. Note that with the imposition of global constraints reflecting the structure of analogy, the system yields well-formed charts. Without these constraints, generated charts either have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky a"
D18-1008,P17-1044,0,0.0145695,"numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural modeling. Recent work has shown the promise of sophisticated neural models on semantic role labeling (He et al., 2017). Similar to other such sequence prediction models, e.g., those for named entity recognition (Lample et al., 2016) or semantic role labeling (Zhou and Xu, 2015), our span prediction utilizes a neural CRF. Our model also has an edge-prediction component, which benefits from a simplified version of the PathLSTM model of Roth and Lapata (2016). Our edge-prediction model also uses an embedding concatenation component, which was inspired by recent work on neural coreference resolution (Lee et al., 2017). He et al. (2017) also impose semantic constraints during prediction, but use A∗ search instead"
D18-1008,P14-5010,1,0.00906289,"raphs as defined in Section 4. Given a sentence, the neural model predicts a distribution over role-labeled spans with edges denoting semantic relations between them. Then, we use an ILP to decode while enforcing the TAP constraints defined in Section 4. Figure 4 presents an overview of the architecture. Context-sensitive word embeddings. We first encode the words in a sentence by embedding each token using fixed word embeddings. We also concatenate a few linguistic features to the word embeddings, such as named entity tags and dependency relations. These features are generated using CoreNLP (Manning et al., 2014) and represented by randomly-initialized, learned embeddings for symbols together with the fixed word embedding of each token’s dependency head and the dependency path length between adjacent tokens. The token embeddings are then passed through several stacked convolutional layers (Kim, 2014). While the first convolutional layer can only capture local information, subsequent layers allow for longer-distance reasoning. def l ∈ LR = {FACT, EQUIVALENCE, ANALOGY}. For G so defined to encode a set of valid TAP frames, it must satisfy certain constraints: 1. Well-formedness constraints. For any two"
D18-1008,N13-1090,0,0.0224717,"Missing"
D18-1008,D14-1181,0,0.00241209,". Context-sensitive word embeddings. We first encode the words in a sentence by embedding each token using fixed word embeddings. We also concatenate a few linguistic features to the word embeddings, such as named entity tags and dependency relations. These features are generated using CoreNLP (Manning et al., 2014) and represented by randomly-initialized, learned embeddings for symbols together with the fixed word embedding of each token’s dependency head and the dependency path length between adjacent tokens. The token embeddings are then passed through several stacked convolutional layers (Kim, 2014). While the first convolutional layer can only capture local information, subsequent layers allow for longer-distance reasoning. def l ∈ LR = {FACT, EQUIVALENCE, ANALOGY}. For G so defined to encode a set of valid TAP frames, it must satisfy certain constraints: 1. Well-formedness constraints. For any two vertices v, v 0 ∈ V , their associated spans must not overlap. Furthermore, every vertex must participate in at least one FACT edge, i.e., no disconnected vertices. 2. Typing constraints. FACT relations are always drawn from a VALUE vertex to a nonVALUE vertex. ANALOGY and EQUIVA LENCE are on"
D18-1008,P14-1026,0,0.0350441,"ognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This includes quantitative reasoning (Kushman et al., 2014; Roy et al., 2015), numerical information extraction (Madaan et al., 2016), and techniques for making numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural"
D18-1008,J05-1004,0,0.011188,"imposition of global constraints reflecting the structure of analogy, the system yields well-formed charts. Without these constraints, generated charts either have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak"
D18-1008,D14-1162,1,0.105743,"rning a sentence embedding or hidden layers, the log-linear model simply uses a CRF to predict span labels directly from fixed input features, and then uses a single sigmoid layer to predict edge labels from deterministic edge embeddings, emn . For the neural models, we used three convolutional layers for sentence embedding with a filter size of 3. Every layer other than the input layer used a hidden dimension of 50 with ReLU nonlinearities. We introduced a single dropout layer (p = 0.5) between every two layers in the network (including at the input). We used 50-dimensional GloVe embeddings (Pennington et al., 2014) learned from Wikipedia 2014 and Gigaword 5 as pre-trained word embeddings, and initialized the embeddings for the features randomly. We chose relatively low input- and hidden-vector dimension because of the size of our data. The network was trained for 15 epochs using ADADELTA (Zeiler, 2012) with a learning rate of 1.0. All models were implemented in PyTorch (Paszke et al., 2017). 7 Model Model Log-linear (all feats.) Neural (no feats.) Neural (all feats.) w/o NER w/o dep. w/o CRF P R F1 42.8 41.7 41.5 41.6 41.2 36.1 82.3 79.1 79.2 79.1 77.5 73.1 56.3 54.6 54.4 54.5 53.8 48.3 Table 4: Perform"
D18-1008,N16-1030,0,0.0241819,"Missing"
D18-1008,P09-1077,0,0.0142039,"generated charts either have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced"
D18-1008,D17-1018,0,0.0241294,"Missing"
D18-1008,prasad-etal-2010-exploiting,0,0.0244782,"er have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping"
D18-1008,W04-2401,0,0.0606951,"e begin by picking the most likely role for each span and edge and then discarding any edges and spans that violate the wellformedness (1) and typing constraints (2). We then enforce transitivity constraints (4) by incrementally building a cluster of analogous and equivalent spans. We then resolve the unique facts constraint (3) by keeping only the span with highest FACT edge score. Finally, for every cluster of analogous VALUE spans, we check that the analogy constraint (5) holds and if not, discard the cluster. We also implement an optimal decoder that encodes the TAP constraints as an ILP (Roth and Yih, 2004; Do et al., 2012). The ILP tries to find an optimal decoding according to the model, subject to hard constraints imposed on the solution space. For example, we require that solutions satisfy the ‘connected spans’ constraint: which defines a joint distribution over per-token role labels. We thus obtain spans from this distribution corresponding to vertices of the graph described in Section 4 by merging contiguous rolelabels in the maximum likelihood label sequence predicted by the CRF. Edge prediction with PATH M AX features. For edge prediction, we use the spans identified above to construct"
D18-1008,P16-1113,0,0.0219536,"Missing"
D18-1008,Q15-1001,0,0.0128036,"ature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This includes quantitative reasoning (Kushman et al., 2014; Roy et al., 2015), numerical information extraction (Madaan et al., 2016), and techniques for making numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural modeling. Recent wo"
D18-1008,N15-3001,0,0.0187186,"s values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et"
D18-1008,Q13-1029,0,0.052752,"Missing"
D18-1008,P15-1109,0,0.0332622,"Missing"
D18-1008,P98-1013,0,\N,Missing
D18-1008,C98-1013,0,\N,Missing
D18-1217,Q16-1026,0,0.0706926,"s-labeled examples substantially speed up model convergence. For example, our model trained on six tasks takes about three times as long to converge as the average model trained on one task, a 2x decrease in total training time. 3 Cross-View Training Models CVT relies on auxiliary prediction modules that have restricted views of the input. In this section, we describe specific constructions of the auxiliary prediction modules that are effective for sequence tagging, dependency parsing, and sequence-tosequence learning. 3.1 Bi-LSTM Sentence Encoder All of our models use a two-layer CNN-BiLSTM (Chiu and Nichols, 2016; Ma and Hovy, 2016) sentence encoder. It takes as input a sequence of words xi = [x1i , x2i , ..., xTi ]. First, each word is represented as the sum of an embedding vector and the output of a character-level Convolutional Neural Network, resulting in a sequence of vectors v = [v 1 , v 2 , ..., v T ]. The encoder applies a twolayer bidirectional LSTM (Graves and Schmidhuber, 2005) to these representations. The first layer runs a Long Short-Term Memory unit (Hochreiter and Schmidhuber, 1997) in the forward direction (taking v t as input at each step t) and the backward direction (taking v T −t+"
D18-1217,D17-1070,0,0.0267421,"Jarrett et al., 2009; LeCun et al., 2010) and NLP. Particularly noteworthy for NLP are algorithms for learning effective word embeddings (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining (Dai and Le, 2015; Ramachandran et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Pre-training on other tasks such as machine translation has also been studied (McCann et al., 2017). Other approaches train 1921 “thought vectors” representing sentences through unsupervised (Kiros et al., 2015; Hill et al., 2016) or supervised (Conneau et al., 2017) learning. Self-Training. One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965), which has been successfully applied to NLP tasks such as word-sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006). In each round of training, the classifier, acting as a “teacher,” labels some of the unlabeled data and adds it to the training set. Then, acting as a “student,” it is retrained on the new training set. Many recent approaches (including the consistentency regularization methods discussed below and our own method) train the student with soft t"
D18-1217,D17-1206,0,0.147397,"erent methods as they learn (see Figure 4). Both CVT and multi-task learning improve model generalization: for the same train accuracy, the models get better dev accuracy than purely supervised learning. Interestingly, CVT continues to improve CCG 96 Dev LAS CVT + Multi-Task. We train a single sharedencoder CVT model to perform all of the tasks except machine translation (as it is quite different and requires more training time than the other ones). Multi-task learning improves results on all of the tasks except fine-grained NER, sometimes by large margins. Prior work on many-task NLP such as Hashimoto et al. (2017) uses complicated architectures and training algorithms. Our result shows that simple parameter sharing can be enough for effective many-task learning when the model is big and trained on a large amount of data. Interestingly, multi-task learning works better in conjunction with CVT than with ELMo. We hypothesize that the ELMo models quickly fit to the data primarily using the ELMo vectors, which perhaps hinders the model from learning effective representations that transfer across tasks. We also believe CVT alleviates the danger of the model “forgetting” one task while training on the other o"
D18-1217,N16-1162,0,0.0312685,"applications in computer vision (Jarrett et al., 2009; LeCun et al., 2010) and NLP. Particularly noteworthy for NLP are algorithms for learning effective word embeddings (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining (Dai and Le, 2015; Ramachandran et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Pre-training on other tasks such as machine translation has also been studied (McCann et al., 2017). Other approaches train 1921 “thought vectors” representing sentences through unsupervised (Kiros et al., 2015; Hill et al., 2016) or supervised (Conneau et al., 2017) learning. Self-Training. One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965), which has been successfully applied to NLP tasks such as word-sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006). In each round of training, the classifier, acting as a “teacher,” labels some of the unlabeled data and adds it to the training set. Then, acting as a “student,” it is retrained on the new training set. Many recent approaches (including the consistentency regularization methods discussed below and our own"
D18-1217,P84-1044,0,0.30715,"Missing"
D18-1217,J07-3004,0,0.0186207,"Missing"
D18-1217,N06-2015,0,0.0552748,"Missing"
D18-1217,P18-1031,0,0.0190606,"l., 2015) to quickly train models on new tasks without slow representation learning. 5 Related Work Unsupervised Representation Learning. Early approaches to deep semi-supervised learning pretrain neural models on unlabeled data, which has been successful for applications in computer vision (Jarrett et al., 2009; LeCun et al., 2010) and NLP. Particularly noteworthy for NLP are algorithms for learning effective word embeddings (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining (Dai and Le, 2015; Ramachandran et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Pre-training on other tasks such as machine translation has also been studied (McCann et al., 2017). Other approaches train 1921 “thought vectors” representing sentences through unsupervised (Kiros et al., 2015; Hill et al., 2016) or supervised (Conneau et al., 2017) learning. Self-Training. One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965), which has been successfully applied to NLP tasks such as word-sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006). In each round of training, the classifier, acting as"
D18-1217,D16-1139,0,0.025369,"t a fraction of its attention weights. The second one is trained to predict the next word in the target sequence rather than the current one: pfuture (yit |yi<t , xi ) = θ softmax(Wsfuture afuture t−1 ). Since there is no target sequence for unlabeled examples, we cannot apply teacher forcing to get an output distribution over the vocabulary from the primary decoder at each time step. Instead, we produce hard targets for the auxiliary modules by running the primary decoder with beam search on the input sequence. This idea has previously been applied to sequence-level knowledge distillation by Kim and Rush (2016). 4 Experiments We compare Cross-View Training against several strong baselines on seven tasks: Combinatory Categorial Grammar (CCG) Supertagging: We use data from CCGBank (Hockenmaier and Steedman, 2007). Text Chunking: We use the CoNLL-2000 data (Tjong Kim Sang and Buchholz, 2000). Named Entity Recognition (NER): We use the CoNLL-2003 data (Tjong Kim Sang and De Meulder, 2003). Fine-Grained NER (FGN): We use OntoNotes (Hovy et al., 2006) dataset. the Part-of-Speech (POS) Tagging: We use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993). Dependency Parsing: We use the"
D18-1217,2015.iwslt-evaluation.11,1,0.790648,"Missing"
D18-1217,D17-1039,1,0.734144,"ld be used like skip-thought vectors (Kiros et al., 2015) to quickly train models on new tasks without slow representation learning. 5 Related Work Unsupervised Representation Learning. Early approaches to deep semi-supervised learning pretrain neural models on unlabeled data, which has been successful for applications in computer vision (Jarrett et al., 2009; LeCun et al., 2010) and NLP. Particularly noteworthy for NLP are algorithms for learning effective word embeddings (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining (Dai and Le, 2015; Ramachandran et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Pre-training on other tasks such as machine translation has also been studied (McCann et al., 2017). Other approaches train 1921 “thought vectors” representing sentences through unsupervised (Kiros et al., 2015; Hill et al., 2016) or supervised (Conneau et al., 2017) learning. Self-Training. One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965), which has been successfully applied to NLP tasks such as word-sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006). In each"
D18-1217,D15-1166,1,0.436971,"r) pbwd-bwd ((u, t, r)|xi ) ∝ es θ − ← −t bwd-bwd (← hu 1 (xi ), h 1 (xi ),r) Each one has some missing context (not seeing either the preceding or following words) for the candidate head and candidate dependent. 1917 3.4 CVT for Sequence-to-Sequence Learning We use an encoder-decoder sequence-to-sequence model with attention (Sutskever et al., 2014; Bahdanau et al., 2015). Each example consists of an input (source) sequence xi = x1i , ..., xTi and output (target) sequence yi = yi1 , ..., yiK . The encoder’s representations are passed into an LSTM decoder using a bilinear attention mechanism (Luong et al., 2015). In particular, at each time step t the decoder computes an attention distribution over source sequence hidden states as αj ∝ j ¯t ¯ t is the decoder’s current hideh Wα h where h den state. The source hidden states weighted by the attention distribution form a context vector: P j ct = j αj h . Next, the context vector and current hidden state are combined into an attention vector at = tanh(Wa [ct , ht ]). Lastly, a softmax layer predicts the next token in the output sequence: p(yit |yi<t , xi ) = softmax(Ws at ). We add two auxiliary decoders when applying CVT. The auxiliary decoders share em"
D18-1217,P17-1194,0,0.0284955,"nt views of the input. On unlabeled data, each one acts as a “teacher” for the other model. In contrast to these methods, our approach trains a single unified model where auxiliary prediction modules see different, but not necessarily independent views of the input. Self Supervision. Self-supervised learning methods train auxiliary prediction modules on tasks where performance can be measured without human-provided labels. Recent work has jointly trained image classifiers with tasks like relative position and colorization (Doersch and Zisserman, 2017), sequence taggers with language modeling (Rei, 2017), and reinforcement learning agents with predicting changes in the environment (Jaderberg et al., 2017). Unlike these approaches, our auxiliary losses are based on self-labeling, not labels deterministically constructed from the input. Multi-Task Learning. There has been extensive prior work on multi-task learning (Caruana, 1997; Ruder, 2017). For NLP, most work has focused on a small number of closely related tasks (Luong et al., 2016; Zhang and Weiss, 2016; Søgaard and Goldberg, 2016; Peng et al., 2017). Manytask systems are less commonly developed. Collobert and Weston (2008) propose a many"
D18-1217,P16-1101,0,0.107459,"antially speed up model convergence. For example, our model trained on six tasks takes about three times as long to converge as the average model trained on one task, a 2x decrease in total training time. 3 Cross-View Training Models CVT relies on auxiliary prediction modules that have restricted views of the input. In this section, we describe specific constructions of the auxiliary prediction modules that are effective for sequence tagging, dependency parsing, and sequence-tosequence learning. 3.1 Bi-LSTM Sentence Encoder All of our models use a two-layer CNN-BiLSTM (Chiu and Nichols, 2016; Ma and Hovy, 2016) sentence encoder. It takes as input a sequence of words xi = [x1i , x2i , ..., xTi ]. First, each word is represented as the sum of an embedding vector and the output of a character-level Convolutional Neural Network, resulting in a sequence of vectors v = [v 1 , v 2 , ..., v T ]. The encoder applies a twolayer bidirectional LSTM (Graves and Schmidhuber, 2005) to these representations. The first layer runs a Long Short-Term Memory unit (Hochreiter and Schmidhuber, 1997) in the forward direction (taking v t as input at each step t) and the backward direction (taking v T −t+1 at each step) → −"
D18-1217,D17-1035,0,0.0223032,"Missing"
D18-1217,P18-1130,0,0.0405661,"Missing"
D18-1217,J93-2004,0,0.0612614,"Missing"
D18-1217,P18-1096,0,0.0742054,"ing, the classifier, acting as a “teacher,” labels some of the unlabeled data and adds it to the training set. Then, acting as a “student,” it is retrained on the new training set. Many recent approaches (including the consistentency regularization methods discussed below and our own method) train the student with soft targets from the teacher’s output distribution rather than a hard label, making the procedure more akin to knowledge distillation (Hinton et al., 2015). It is also possible to use multiple models or prediction modules for the teacher, such as in tri-training (Zhou and Li, 2005; Ruder and Plank, 2018). Consistency Regularization. Recent works add noise (e.g., drawn from a Gaussian distribution) or apply stochastic transformations (e.g., horizontally flipping an image) to the student’s inputs. This trains the model to give consistent predictions to nearby data points, encouraging distributional smoothness in the model. Consistency regularization has been very successful for computer vision applications (Bachman et al., 2014; Laine and Aila, 2017; Tarvainen and Valpola, 2017). However, stochastic input alterations are more difficult to apply to discrete data like text, making consistency reg"
D18-1217,N06-1020,0,0.41196,"pervised representation learning on a large corpus of unlabeled data followed by supervised training. A key disadvantage of pre-training is that the first representation learning phase does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task. Older semi-supervised learning algorithms like self-training do not suffer from this problem because they continually learn about a task on a mix of labeled and unlabeled data. Selftraining has historically been effective for NLP (Yarowsky, 1995; McClosky et al., 2006), but is less commonly used with neural models. This paper presents Cross-View Training (CVT), a new self-training algorithm that works well for neural sequence models. In self-training, the model learns as normal on labeled examples. On unlabeled examples, the model acts as both a teacher that makes predictions about the examples and a student that is trained on those predictions. Although this process has shown value for some tasks, it is somewhat tautological: the model already produces the predictions it is being trained on. Recent research on computer vision addresses this by adding noise"
D18-1217,P16-2038,0,0.023681,"rs with tasks like relative position and colorization (Doersch and Zisserman, 2017), sequence taggers with language modeling (Rei, 2017), and reinforcement learning agents with predicting changes in the environment (Jaderberg et al., 2017). Unlike these approaches, our auxiliary losses are based on self-labeling, not labels deterministically constructed from the input. Multi-Task Learning. There has been extensive prior work on multi-task learning (Caruana, 1997; Ruder, 2017). For NLP, most work has focused on a small number of closely related tasks (Luong et al., 2016; Zhang and Weiss, 2016; Søgaard and Goldberg, 2016; Peng et al., 2017). Manytask systems are less commonly developed. Collobert and Weston (2008) propose a many-task system sharing word embeddings between the tasks, Hashimoto et al. (2017) train a many-task model where the tasks are arranged hierarchically according to their linguistic level, and Subramanian et al. (2018) train a shared-encoder many-task model for the purpose of learning better sentence representations for use in downstream tasks, not for improving results on the original tasks. 6 Conclusion We propose Cross-View Training, a new method for semi-supervised learning. Our approa"
D18-1217,P17-1186,0,0.0230876,"position and colorization (Doersch and Zisserman, 2017), sequence taggers with language modeling (Rei, 2017), and reinforcement learning agents with predicting changes in the environment (Jaderberg et al., 2017). Unlike these approaches, our auxiliary losses are based on self-labeling, not labels deterministically constructed from the input. Multi-Task Learning. There has been extensive prior work on multi-task learning (Caruana, 1997; Ruder, 2017). For NLP, most work has focused on a small number of closely related tasks (Luong et al., 2016; Zhang and Weiss, 2016; Søgaard and Goldberg, 2016; Peng et al., 2017). Manytask systems are less commonly developed. Collobert and Weston (2008) propose a many-task system sharing word embeddings between the tasks, Hashimoto et al. (2017) train a many-task model where the tasks are arranged hierarchically according to their linguistic level, and Subramanian et al. (2018) train a shared-encoder many-task model for the purpose of learning better sentence representations for use in downstream tasks, not for improving results on the original tasks. 6 Conclusion We propose Cross-View Training, a new method for semi-supervised learning. Our approach allows models to"
D18-1217,D14-1162,1,0.108788,"ective representations for language. In particular, the representations could be used like skip-thought vectors (Kiros et al., 2015) to quickly train models on new tasks without slow representation learning. 5 Related Work Unsupervised Representation Learning. Early approaches to deep semi-supervised learning pretrain neural models on unlabeled data, which has been successful for applications in computer vision (Jarrett et al., 2009; LeCun et al., 2010) and NLP. Particularly noteworthy for NLP are algorithms for learning effective word embeddings (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining (Dai and Le, 2015; Ramachandran et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018). Pre-training on other tasks such as machine translation has also been studied (McCann et al., 2017). Other approaches train 1921 “thought vectors” representing sentences through unsupervised (Kiros et al., 2015; Hill et al., 2016) or supervised (Conneau et al., 2017) learning. Self-Training. One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965), which has been successfully applied to NLP tasks such as word-sense"
D18-1217,P17-1161,0,0.0608638,"Missing"
D18-1217,N18-1202,0,0.543363,"on modules to match the primary one across many different views of the input a once, rather than just one view at a time. Virtual Adversarial Training (VAT). VAT (Miyato et al., 2016) works like word dropout, but adds noise to the word embeddings of the student instead of dropping out words. Notably, the noise is chosen adversarially so it most changes the model’s prediction. This method was applied successfully to semi-supervised text classification 1918 Method Shortcut LSTM (Wu et al., 2017) ID-CNN-CRF (Strubell et al., 2017) JMT† (Hashimoto et al., 2017) TagLM* (Peters et al., 2017) ELMo* (Peters et al., 2018) CCG Chunk NER FGN POS Acc. F1 F1 F1 Acc. 95.1 Dep. Parse Translate UAS LAS BLEU 97.53 90.7 86.8 95.8 96.4 97.55 94.7 92.9 91.9 92.2 Biaffine (Dozat and Manning, 2017) Stack Pointer (Ma et al., 2018) 95.7 94.1 95.9 94.2 Stanford (Luong and Manning, 2015) Google (Luong et al., 2017) Supervised Virtual Adversarial Training* Word Dropout* ELMo (our implementation)* ELMo + Multi-task*† CVT* CVT + Multi-task*† CVT + Multi-task + Large*† 23.3 26.1 94.9 95.1 95.2 95.8 95.9 95.7 96.0 96.1 95.1 95.1 95.8 96.5 96.8 96.6 96.9 97.0 91.2 91.8 92.1 92.2 92.3 92.3 92.4 92.6 87.5 87.9 88.1 88.5 88.4 88.7 88.4"
D18-1217,W00-0726,0,0.602417,"Missing"
D18-1217,P95-1026,0,0.866802,"ods perform unsupervised representation learning on a large corpus of unlabeled data followed by supervised training. A key disadvantage of pre-training is that the first representation learning phase does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task. Older semi-supervised learning algorithms like self-training do not suffer from this problem because they continually learn about a task on a mix of labeled and unlabeled data. Selftraining has historically been effective for NLP (Yarowsky, 1995; McClosky et al., 2006), but is less commonly used with neural models. This paper presents Cross-View Training (CVT), a new self-training algorithm that works well for neural sequence models. In self-training, the model learns as normal on labeled examples. On unlabeled examples, the model acts as both a teacher that makes predictions about the examples and a student that is trained on those predictions. Although this process has shown value for some tasks, it is somewhat tautological: the model already produces the predictions it is being trained on. Recent research on computer vision addres"
D18-1217,P16-1147,0,0.0219123,"trained image classifiers with tasks like relative position and colorization (Doersch and Zisserman, 2017), sequence taggers with language modeling (Rei, 2017), and reinforcement learning agents with predicting changes in the environment (Jaderberg et al., 2017). Unlike these approaches, our auxiliary losses are based on self-labeling, not labels deterministically constructed from the input. Multi-Task Learning. There has been extensive prior work on multi-task learning (Caruana, 1997; Ruder, 2017). For NLP, most work has focused on a small number of closely related tasks (Luong et al., 2016; Zhang and Weiss, 2016; Søgaard and Goldberg, 2016; Peng et al., 2017). Manytask systems are less commonly developed. Collobert and Weston (2008) propose a many-task system sharing word embeddings between the tasks, Hashimoto et al. (2017) train a many-task model where the tasks are arranged hierarchically according to their linguistic level, and Subramanian et al. (2018) train a shared-encoder many-task model for the purpose of learning better sentence representations for use in downstream tasks, not for improving results on the original tasks. 6 Conclusion We propose Cross-View Training, a new method for semi-sup"
D18-1244,N16-1097,0,0.0317791,"Missing"
D18-1244,H05-1091,0,0.631258,"ing dependency trees as overlapping paths along the trees (Kambhatla, 2004). However, these models face the challenge of sparse feature spaces and are brittle to lexical variations. More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees. One common approach to leverage dependency information is to perform bottom-up or top-down computation along the parse tree or the subtree below the lowest common ancestor (LCA) of the entities (Miwa and Bansal, 2016). Another popular approach, inspired by Bunescu and Mooney (2005), is to reduce the parse tree to the shortest dependency path between the entities (Xu et al., 2015a,b). However, these models suffer from several 2205 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205–2215 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics drawbacks. Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient, because aligning trees for efficient batch training is usually nontrivial. Models based on the shortest depende"
D18-1244,P18-1192,0,0.0218932,"l et al. (2016) and Zhang et al. (2017) have shown that relatively simple neural models (CNN and augmented LSTM, respectively) can achieve comparable or superior performance to dependency-based models when trained on larger datasets. In this paper, we study dependency-based models in depth and show that with a properly designed architecture, they can outperform and have complementary advantages to sequence models, even in a large-scale setting. Finally, we note that a technique similar to pathcentric pruning has been applied to reduce the space of possible arguments in semantic role labeling (He et al., 2018). The authors showed pruning words too far away from the path between the predicate and the root to be beneficial, but reported the best pruning distance to be 10, which almost always retains the entire tree. Our method differs in that it is applied to the shortest dependency path between entities, and we show that in our technique the best pruning distance is 1 for several dependency-based relation extraction models. 5 Experiments 5.1 Baseline Models We compare our models with several competitive dependency-based and neural sequence models. Dependency-based models. In our main experiments we"
D18-1244,D17-1018,0,0.0284918,"e also Figure 2 left): hsent = f h(L) = f GCN(h(0) ) , (3) where h(l) denotes the collective hidden representations at layer l of the GCN, and f : Rd⇥n ! Rd is a max pooling function that maps from n output vectors to the sentence vector. We also observe that information close to entity tokens in the dependency tree is often central to relation classification. Therefore, we also obtain a subject representation hs from h(L) as follows hs = f h(L) s1 :s2 , (4) as well as an object representation ho similarly. Inspired by recent work on relational learning between entities (Santoro et al., 2017; Lee et al., 2017), we obtain the final representation used for classification by concatenating the sentence and the entity representations, and feeding them 2207 through a feed-forward neural network (FFNN): hfinal = FFNN [hsent ; hs ; ho ] . (5) This hfinal representation is then fed into a linear layer followed by a softmax operation to obtain a probability distribution over relations. 2.3 Contextualized GCN The network architecture introduced so far learns effective representations for relation extraction, but it also leaves a few issues inadequately addressed. First, the input word vectors do not contain c"
D18-1244,P15-2047,0,0.0877104,"l. (2016) showed that combining a CNN with a recurrent neural network (RNN) through a voting scheme can further improve performance. Zhou et al. (2016) and Wang et al. (2016) proposed to use attention mechanisms over RNN and CNN architectures for this task. Apart from neural models over word sequences, incorporating dependency trees into neural models has also been shown to improve relation extraction performance by capturing long-distance relations. Xu et al. (2015b) generalized the idea of dependency path kernels by applying a LSTM network over the shortest dependency path between entities. Liu et al. (2015) first applied a recursive network over the subtrees rooted at the words on the dependency path and then applied a CNN over the path. Miwa and Bansal (2016) applied a Tree-LSTM (Tai et al., 2015), a generalized form of LSTM over dependency trees, in a joint entity and relation extraction setting. They found it to be most effective when applied to the subtree rooted at the LCA of the two entities. More recently, Adel et al. (2016) and Zhang et al. (2017) have shown that relatively simple neural models (CNN and augmented LSTM, respectively) can achieve comparable or superior performance to depen"
D18-1244,P14-5010,1,0.0235463,"Missing"
D18-1244,D17-1159,0,0.40095,"rees are usually difficult to parallelize and thus computationally inefficient, because aligning trees for efficient batch training is usually nontrivial. Models based on the shortest dependency path between the subject and object are computationally more efficient, but this simplifying assumption has major limitations as well. Figure 1 shows a real-world example where crucial information (i.e., negation) would be excluded when the model is restricted to only considering the dependency path. In this work, we propose a novel extension of the graph convolutional network (Kipf and Welling, 2017; Marcheggiani and Titov, 2017) that is tailored for relation extraction. Our model encodes the dependency structure over the input sentence with efficient graph convolution operations, then extracts entity-centric representations to make robust relation predictions. We also apply a novel path-centric pruning technique to remove irrelevant information from the tree while maximally keeping relevant content, which further improves the performance of several dependencybased models including ours. We test our model on the popular SemEval 2010 Task 8 dataset and the more recent, larger TACRED dataset. On both datasets, our model"
D18-1244,P09-1113,0,0.296368,"ched to the path, and K = 1 retains the entire LCA subtree. We combine this pruning strategy with our GCN model, by directly feeding the pruned trees into the graph convolutional layers.2 We show that pruning with K = 1 achieves the best balance between including relevant information (e.g., negation and conjunction) and keeping irrelevant content out of the resulting pruned tree as much as possible. 4 Related Work At the core of fully-supervised and distantlysupervised relation extraction approaches are statistical classifiers, many of which find syntactic information beneficial. For example, Mintz et al. (2009) explored adding syntactic features to a statistical classifier and found them to be useful when sentences are long. Various kernel-based approaches also leverage syntactic information to measure similarity between training and test examples to predict the relation, finding that tree2 For our C-GCN model, the LSTM layer still operates on the full sentence regardless of the pruning. 2208 based kernels (Zelenko et al., 2003) and dependency path-based kernels (Bunescu and Mooney, 2005) are effective for this task. Recent studies have found neural models effective in relation extraction. Zeng et a"
D18-1244,P16-1105,0,0.605348,"nal feature-based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees (Kambhatla, 2004). However, these models face the challenge of sparse feature spaces and are brittle to lexical variations. More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees. One common approach to leverage dependency information is to perform bottom-up or top-down computation along the parse tree or the subtree below the lowest common ancestor (LCA) of the entities (Miwa and Bansal, 2016). Another popular approach, inspired by Bunescu and Mooney (2005), is to reduce the parse tree to the shortest dependency path between the entities (Xu et al., 2015a,b). However, these models suffer from several 2205 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205–2215 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics drawbacks. Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient, because aligning trees for efficient batch tra"
D18-1244,L16-1262,1,0.800641,"Missing"
D18-1244,D14-1162,1,0.108928,"Missing"
D18-1244,E17-1110,0,0.111302,"ct (“Mike Cane”) is also shown, where the shortest dependency path between the entities is highlighted in bold. Note that negation (“not”) is off the dependency path. Introduction Relation extraction involves discerning whether a relation exists between two entities in a sentence (often termed subject and object, respectively). Successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale, such as question answering (Yu et al., 2017), knowledge base population (Zhang et al., 2017), and biomedical knowledge discovery (Quirk and Poon, 2017). Models making use of dependency parses of the input sentences, or dependency-based models, ⇤ Equal contribution. The order of authorship was decided by a tossed coin. have proven to be very effective in relation extraction, because they capture long-range syntactic relations that are obscure from the surface form alone (e.g., when long clauses or complex scoping are present). Traditional feature-based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees (Kambhatla, 2004). However, these models face the challenge of sparse fe"
D18-1244,S10-1057,0,0.181371,"Missing"
D18-1244,P15-1150,1,0.942603,"ization layer is trained jointly with the rest of the network. We show empirically in Section 5 that this augmentation substantially improves the performance over the original model. We note that this relation extraction model is conceptually similar to graph kernel-based models (Zelenko et al., 2003), in that it aims to utilize local dependency tree patterns to inform relation classification. Our model also incorporates crucial off-path information, which greatly improves its robustness compared to shortest dependency pathbased approaches. Compared to tree-structured models (e.g., Tree-LSTM (Tai et al., 2015)), it not only is able to capture more global information through the use of pooling functions, but also achieves substantial speedup by not requiring recursive operations that are difficult to parallelize. For example, we observe that on a Titan Xp GPU, training a Tree-LSTM model over a minibatch of 50 examples takes 6.54 seconds on average, while training the original GCN model takes only 0.07 seconds, and the C-GCN model 0.08 seconds. 3 Incorporating Off-path Information with Path-centric Pruning Dependency trees provide rich structures that one can exploit in relation extraction, but most"
D18-1244,N16-1065,0,0.105779,"Missing"
D18-1244,P16-1123,0,0.369929,"Missing"
D18-1244,D15-1062,0,0.556779,"e models face the challenge of sparse feature spaces and are brittle to lexical variations. More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees. One common approach to leverage dependency information is to perform bottom-up or top-down computation along the parse tree or the subtree below the lowest common ancestor (LCA) of the entities (Miwa and Bansal, 2016). Another popular approach, inspired by Bunescu and Mooney (2005), is to reduce the parse tree to the shortest dependency path between the entities (Xu et al., 2015a,b). However, these models suffer from several 2205 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205–2215 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics drawbacks. Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient, because aligning trees for efficient batch training is usually nontrivial. Models based on the shortest dependency path between the subject and object are computationally more efficient, but this simplifying as"
D18-1244,D15-1206,0,0.668598,"e models face the challenge of sparse feature spaces and are brittle to lexical variations. More recent neural models address this problem with distributed representations built from their computation graphs formed along parse trees. One common approach to leverage dependency information is to perform bottom-up or top-down computation along the parse tree or the subtree below the lowest common ancestor (LCA) of the entities (Miwa and Bansal, 2016). Another popular approach, inspired by Bunescu and Mooney (2005), is to reduce the parse tree to the shortest dependency path between the entities (Xu et al., 2015a,b). However, these models suffer from several 2205 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2205–2215 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics drawbacks. Neural models operating directly on parse trees are usually difficult to parallelize and thus computationally inefficient, because aligning trees for efficient batch training is usually nontrivial. Models based on the shortest dependency path between the subject and object are computationally more efficient, but this simplifying as"
D18-1244,C14-1220,0,0.439647,"l. (2009) explored adding syntactic features to a statistical classifier and found them to be useful when sentences are long. Various kernel-based approaches also leverage syntactic information to measure similarity between training and test examples to predict the relation, finding that tree2 For our C-GCN model, the LSTM layer still operates on the full sentence regardless of the pruning. 2208 based kernels (Zelenko et al., 2003) and dependency path-based kernels (Bunescu and Mooney, 2005) are effective for this task. Recent studies have found neural models effective in relation extraction. Zeng et al. (2014) first applied a one-dimensional convolutional neural network (CNN) with manual features to encode relations. Vu et al. (2016) showed that combining a CNN with a recurrent neural network (RNN) through a voting scheme can further improve performance. Zhou et al. (2016) and Wang et al. (2016) proposed to use attention mechanisms over RNN and CNN architectures for this task. Apart from neural models over word sequences, incorporating dependency trees into neural models has also been shown to improve relation extraction performance by capturing long-distance relations. Xu et al. (2015b) generalize"
D18-1244,D17-1004,1,0.925979,"al UD dependency tree between the subject (“he”) and object (“Mike Cane”) is also shown, where the shortest dependency path between the entities is highlighted in bold. Note that negation (“not”) is off the dependency path. Introduction Relation extraction involves discerning whether a relation exists between two entities in a sentence (often termed subject and object, respectively). Successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale, such as question answering (Yu et al., 2017), knowledge base population (Zhang et al., 2017), and biomedical knowledge discovery (Quirk and Poon, 2017). Models making use of dependency parses of the input sentences, or dependency-based models, ⇤ Equal contribution. The order of authorship was decided by a tossed coin. have proven to be very effective in relation extraction, because they capture long-range syntactic relations that are obscure from the surface form alone (e.g., when long clauses or complex scoping are present). Traditional feature-based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees (Kambhatla, 2"
D18-1244,P16-2034,0,0.141939,"lation, finding that tree2 For our C-GCN model, the LSTM layer still operates on the full sentence regardless of the pruning. 2208 based kernels (Zelenko et al., 2003) and dependency path-based kernels (Bunescu and Mooney, 2005) are effective for this task. Recent studies have found neural models effective in relation extraction. Zeng et al. (2014) first applied a one-dimensional convolutional neural network (CNN) with manual features to encode relations. Vu et al. (2016) showed that combining a CNN with a recurrent neural network (RNN) through a voting scheme can further improve performance. Zhou et al. (2016) and Wang et al. (2016) proposed to use attention mechanisms over RNN and CNN architectures for this task. Apart from neural models over word sequences, incorporating dependency trees into neural models has also been shown to improve relation extraction performance by capturing long-distance relations. Xu et al. (2015b) generalized the idea of dependency path kernels by applying a LSTM network over the shortest dependency path between entities. Liu et al. (2015) first applied a recursive network over the subtrees rooted at the words on the dependency path and then applied a CNN over the path."
D18-1259,P17-1171,0,0.123334,"Missing"
D18-1259,D16-1264,0,0.483551,"is direction. However, existing datasets have limitations that hinder further advancements of machine reasoning over natural language, especially in testing QA systems’ ability to perform multi-hop reasoning, where the system has to reason with information taken from more than one document to arrive at the answer. ∗ These authors contributed equally. The order of authorship is decided through dice rolling. † Work done when WWC was at CMU. First, some datasets mainly focus on testing the ability of reasoning within a single paragraph or document, or single-hop reasoning. For example, in SQuAD (Rajpurkar et al., 2016) questions are designed to be answered given a single paragraph as the context, and most of the questions can in fact be answered by matching the question with a single sentence in that paragraph. As a result, it has fallen short at testing systems’ ability to reason over a larger context. TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al., 2017) create a more challenging setting by using information retrieval to collect multiple documents to form the context given existing question-answer pairs. Nevertheless, most of the questions can be answered by matching the question with a few nearb"
D18-1259,N18-2088,0,0.0132483,"17) as our baseline model. We note that our implementation without weight averaging achieves performance very close to what the authors reported on SQuAD (about 1 point worse in F1 ). Our implemented model subsumes the latest technical advances on question answering, including character-level models, self-attention (Wang et al., 2017), and bi-attention (Seo et al., 2017). Combining these three key components is becoming standard practice, and various state-of-the-art or competitive architectures (Liu et al., 2018; Clark and Gardner, 2017; Wang et al., 2017; Seo et al., 2017; Pan et al., 2017; Salant and Berant, 2018; Xiong et al., 2018) on SQuAD can be viewed as similar to our implemented model. To accommodate yes/no questions, we also add a 3-way classifier after the last recurrent layer to produce the probabilities of “yes”, “no”, and span-based answers. During decoding, we first use the 3-way output to determine whether the answer is “yes”, “no”, or a text span. If it is a text span, we further search for the most probable span. Supporting Facts as Strong Supervision. To evaluate the baseline model’s performance in predicting explainable supporting facts, as well as how much they improve QA performanc"
D18-1259,P17-1147,0,0.124395,"authors contributed equally. The order of authorship is decided through dice rolling. † Work done when WWC was at CMU. First, some datasets mainly focus on testing the ability of reasoning within a single paragraph or document, or single-hop reasoning. For example, in SQuAD (Rajpurkar et al., 2016) questions are designed to be answered given a single paragraph as the context, and most of the questions can in fact be answered by matching the question with a single sentence in that paragraph. As a result, it has fallen short at testing systems’ ability to reason over a larger context. TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al., 2017) create a more challenging setting by using information retrieval to collect multiple documents to form the context given existing question-answer pairs. Nevertheless, most of the questions can be answered by matching the question with a few nearby sentences in one single paragraph, which is limited as it does not require more complex reasoning (e.g., 2369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics over"
D18-1259,N18-1059,0,0.281768,"given existing question-answer pairs. Nevertheless, most of the questions can be answered by matching the question with a few nearby sentences in one single paragraph, which is limited as it does not require more complex reasoning (e.g., 2369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics over multiple paragraphs). Second, existing datasets that target multi-hop reasoning, such as QAngaroo (Welbl et al., 2018) and C OMPLEX W EB Q UESTIONS (Talmor and Berant, 2018), are constructed using existing knowledge bases (KBs). As a result, these datasets are constrained by the schema of the KBs they use, and therefore the diversity of questions and answers is inherently limited. Third, all of the above datasets only provide distant supervision; i.e., the systems only know what the answer is, but do not know what supporting facts lead to it. This makes it difficult for models to learn about the underlying reasoning process, as well as to make explainable predictions. To address the above challenges, we aim at creating a QA dataset that requires reasoning over mu"
D18-1259,P18-1157,0,0.0183116,"ading QA systems on our data, we reimplemented the architecture described in Clark and Gardner (2017) as our baseline model. We note that our implementation without weight averaging achieves performance very close to what the authors reported on SQuAD (about 1 point worse in F1 ). Our implemented model subsumes the latest technical advances on question answering, including character-level models, self-attention (Wang et al., 2017), and bi-attention (Seo et al., 2017). Combining these three key components is becoming standard practice, and various state-of-the-art or competitive architectures (Liu et al., 2018; Clark and Gardner, 2017; Wang et al., 2017; Seo et al., 2017; Pan et al., 2017; Salant and Berant, 2018; Xiong et al., 2018) on SQuAD can be viewed as similar to our implemented model. To accommodate yes/no questions, we also add a 3-way classifier after the last recurrent layer to produce the probabilities of “yes”, “no”, and span-based answers. During decoding, we first use the 3-way output to determine whether the answer is “yes”, “no”, or a text span. If it is a text span, we further search for the most probable span. Supporting Facts as Strong Supervision. To evaluate the baseline model"
D18-1259,P17-1018,0,0.194684,"Missing"
D18-1259,P14-5010,1,0.0146877,"Missing"
D18-1259,Q18-1021,0,0.111105,"to collect multiple documents to form the context given existing question-answer pairs. Nevertheless, most of the questions can be answered by matching the question with a few nearby sentences in one single paragraph, which is limited as it does not require more complex reasoning (e.g., 2369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics over multiple paragraphs). Second, existing datasets that target multi-hop reasoning, such as QAngaroo (Welbl et al., 2018) and C OMPLEX W EB Q UESTIONS (Talmor and Berant, 2018), are constructed using existing knowledge bases (KBs). As a result, these datasets are constrained by the schema of the KBs they use, and therefore the diversity of questions and answers is inherently limited. Third, all of the above datasets only provide distant supervision; i.e., the systems only know what the answer is, but do not know what supporting facts lead to it. This makes it difficult for models to learn about the underlying reasoning process, as well as to make explainable predictions. To address the above challenges, we aim a"
D18-1259,D17-2014,0,0.0126166,"upporting facts necessary to arrive at the answer, when the answer is generated. To this end, we also collect the sentences that determine the answers from crowd workers. These supporting facts can serve as strong supervision for what sentences to pay attention to. Moreover, we can now test the explainability of a model by comparing the predicted supporting facts to the ground truth ones. The overall procedure of data collection is illustrated in Algorithm 1. 3 Processing and Benchmark Settings We collected 112,779 valid examples in total on Amazon Mechanical Turk4 using the ParlAI interface (Miller et al., 2017) (see Appendix A).To isolate potential single-hop questions from the desired multi-hop ones, we first split out a subset of data called train-easy. Specifically, we randomly sampled questions (∼3–10 per Turker) from top-contributing turkers, and categorized all 2371 4 https://www.mturk.com/ Name Desc. Usage train-easy train-medium train-hard dev test-distractor test-fullwiki Total single-hop multi-hop hard multi-hop hard multi-hop hard multi-hop hard multi-hop training training training dev test test # Examples 18,089 56,814 15,661 7,405 7,405 7,405 112,779 Table 1: Data split. The splits trai"
D18-1259,D17-1238,0,0.0421206,"Missing"
D19-1261,P17-1171,0,0.514618,"ain question answering (QA) is an important means for us to make use of knowledge in large text corpora and enables diverse queries without requiring a knowledge schema ahead of time. Enabling such systems to perform multistep inference can further expand our capability to explore the knowledge in these corpora (e.g., see Figure 1). ∗ Equal contribution, order decided by a random number generator. Fueled by the recently proposed large-scale QA datasets such as SQuAD (Rajpurkar et al., 2016, 2018) and TriviaQA (Joshi et al., 2017), much progress has been made in open-domain question answering. Chen et al. (2017) proposed a twostage approach of retrieving relevant content with the question, then reading the paragraphs returned by the information retrieval (IR) component to arrive at the final answer. This “retrieve and read” approach has since been adopted and extended in various open-domain QA systems (Nishida et al., 2018; Kratzwald and Feuerriegel, 2018), but it is inherently limited to answering questions that do not require multi-hop/multi-step reasoning. This is because for many multi-hop questions, not all the relevant context can be obtained in a single retrieval step (e.g., “Ernest Cline” in"
D19-1261,N19-1405,0,0.0429983,"TPOT QA (Yang et al., 2018) are among the largest-scale multi-hop QA datasets to date. While the former is constructed around a knowledge base and the knowledge schema therein, the latter adopts a free-form question generation process in crowdsourcing and span-based evaluation. Both datasets feature a few-document setting where the gold supporting facts are provided along with a small set of distractors to ease the computational burden. However, researchers have shown that this sometimes results in gameable contexts, and thus does not always test the model’s capability of multi-hop reasoning (Chen and Durrett, 2019; Min et al., 2019a). Therefore, in this work, we focus on the fullwiki setting of H OTPOT QA, which features a truly open-domain setting with more diverse questions. Multi-hop QA systems At a broader level, the need for multi-step searches, query task decomposition, and subtask extraction has been clearly recognized in the IR community (Hassan Awadallah et al., 2014; Mehrotra et al., 2016; Mehrotra and Yilmaz, 2017), but multi-hop QA has only recently been studied closely with the release of large-scale datasets. Much research has focused on enabling multi-hop reasoning in question answering"
D19-1261,N19-1240,0,0.0375497,"Missing"
D19-1261,N19-1423,0,0.0442931,"ld first generate a query to retrieve Armada (novel) based on the question, then query for Ernest Cline based on newly gained knowledge in that article. This allows G OLD E N to leverage off-the-shelf, generalpurpose IR systems to scale open-domain multihop reasoning to millions of documents efficiently, and to do so in an interpretable manner. Combined with a QA module that extends BiDAF++ (Clark and Gardner, 2017), our final system outperforms the best previously published system on the open-domain (fullwiki) setting of H OTPOT QA without using powerful pretrained language models like BERT (Devlin et al., 2019). The main contributions of this paper are: (a) a novel iterative retrieve-and-read framework capable of multi-hop reasoning in open-domain QA;1 (b) a natural language query generation approach that guarantees interpretability in the multi-hop evidence gathering process; (c) an efficient training procedure to enable query generation with minimal supervision signal that significantly boosts recall of gold supporting documents in retrieval. 2 Related Work Open-domain question answering (QA) Inspired by the series of TREC QA competitions,2 Chen et al. (2017) were among the first to adapt neural Q"
D19-1261,P19-1259,0,0.353408,"ecognized in the IR community (Hassan Awadallah et al., 2014; Mehrotra et al., 2016; Mehrotra and Yilmaz, 2017), but multi-hop QA has only recently been studied closely with the release of large-scale datasets. Much research has focused on enabling multi-hop reasoning in question answering models in the few-document setting, e.g., by modeling entity graphs (De Cao et al., 2019) or scoring answer candidates against the context (Zhong et al., 2019). These approaches, however, suffer from scalability issues when the number of supporting documents and/or answer candidates grow beyond a few dozen. Ding et al. (2019) apply entity graph modeling to H OTPOT QA, where 2591 they expand a small entity graph starting from the question to arrive at the context for the QA model. However, centered around entity names, this model risks missing purely descriptive clues in the question. Das et al. (2019) propose a neural retriever trained with distant supervision to bias towards paragraphs containing answers to the given questions, which is then used in a multi-step reader-reasoner framework. This does not fundamentally address the discoverability issue in opendomain multi-hop QA, however, because usually not all the"
D19-1261,P19-1222,0,0.210014,"Missing"
D19-1261,P17-1147,0,0.0532482,"retrieved based on merely the question. (Best viewed in color) Introduction Open-domain question answering (QA) is an important means for us to make use of knowledge in large text corpora and enables diverse queries without requiring a knowledge schema ahead of time. Enabling such systems to perform multistep inference can further expand our capability to explore the knowledge in these corpora (e.g., see Figure 1). ∗ Equal contribution, order decided by a random number generator. Fueled by the recently proposed large-scale QA datasets such as SQuAD (Rajpurkar et al., 2016, 2018) and TriviaQA (Joshi et al., 2017), much progress has been made in open-domain question answering. Chen et al. (2017) proposed a twostage approach of retrieving relevant content with the question, then reading the paragraphs returned by the information retrieval (IR) component to arrive at the final answer. This “retrieve and read” approach has since been adopted and extended in various open-domain QA systems (Nishida et al., 2018; Kratzwald and Feuerriegel, 2018), but it is inherently limited to answering questions that do not require multi-hop/multi-step reasoning. This is because for many multi-hop questions, not all the re"
D19-1261,D18-1055,0,0.153929,"Equal contribution, order decided by a random number generator. Fueled by the recently proposed large-scale QA datasets such as SQuAD (Rajpurkar et al., 2016, 2018) and TriviaQA (Joshi et al., 2017), much progress has been made in open-domain question answering. Chen et al. (2017) proposed a twostage approach of retrieving relevant content with the question, then reading the paragraphs returned by the information retrieval (IR) component to arrive at the final answer. This “retrieve and read” approach has since been adopted and extended in various open-domain QA systems (Nishida et al., 2018; Kratzwald and Feuerriegel, 2018), but it is inherently limited to answering questions that do not require multi-hop/multi-step reasoning. This is because for many multi-hop questions, not all the relevant context can be obtained in a single retrieval step (e.g., “Ernest Cline” in Figure 1). More recently, the emergence of multi-hop question answering datasets such as QAngaroo (Welbl et al., 2018) and H OTPOT QA (Yang et al., 2018) has sparked interest in multi-hop QA in the research community. Designed to be more challenging than SQuAD-like datasets, they feature questions that require context of more than one 2590 Proceedin"
D19-1261,P14-5010,1,0.0160563,"eed to be found; in the opendomain fullwiki setting, which we focus on, the models are only given the question and the entire Wikipedia. Models are evaluated on their answer accuracy and explainability, where the former is measured as overlap between the predicted and gold answers with exact match (EM) and unigram F1 , and the latter concerns how well the predicted supporting fact sentences match human annotation (Supporting Fact EM/F1 ). A joint metric is also reported on this dataset, which encourages systems to perform well on both tasks simultaneously. We use the Stanford CoreNLP toolkit (Manning et al., 2014) to preprocess Wikipedia, as well as to generate POS/NER features for the query generators, following (Yang et al., 2018) and (Chen et al., 2017). We always detokenize a generated search query before sending it to Elasticsearch, which has its own preprocessing pipeline. Since all questions in H OTPOT QA require exactly two supporting documents, we fix the number of retrieval steps of G OLD E N Retriever to S = 2. To accommodate arbitrary steps of reasoning in G OLD E N Retriever, a stopping criterion is required to determine when to stop retrieving for more supporting documents and perform few"
D19-1261,N16-1073,0,0.024575,"f distractors to ease the computational burden. However, researchers have shown that this sometimes results in gameable contexts, and thus does not always test the model’s capability of multi-hop reasoning (Chen and Durrett, 2019; Min et al., 2019a). Therefore, in this work, we focus on the fullwiki setting of H OTPOT QA, which features a truly open-domain setting with more diverse questions. Multi-hop QA systems At a broader level, the need for multi-step searches, query task decomposition, and subtask extraction has been clearly recognized in the IR community (Hassan Awadallah et al., 2014; Mehrotra et al., 2016; Mehrotra and Yilmaz, 2017), but multi-hop QA has only recently been studied closely with the release of large-scale datasets. Much research has focused on enabling multi-hop reasoning in question answering models in the few-document setting, e.g., by modeling entity graphs (De Cao et al., 2019) or scoring answer candidates against the context (Zhong et al., 2019). These approaches, however, suffer from scalability issues when the number of supporting documents and/or answer candidates grow beyond a few dozen. Ding et al. (2019) apply entity graph modeling to H OTPOT QA, where 2591 they expan"
D19-1261,P19-1416,0,0.16478,"18) are among the largest-scale multi-hop QA datasets to date. While the former is constructed around a knowledge base and the knowledge schema therein, the latter adopts a free-form question generation process in crowdsourcing and span-based evaluation. Both datasets feature a few-document setting where the gold supporting facts are provided along with a small set of distractors to ease the computational burden. However, researchers have shown that this sometimes results in gameable contexts, and thus does not always test the model’s capability of multi-hop reasoning (Chen and Durrett, 2019; Min et al., 2019a). Therefore, in this work, we focus on the fullwiki setting of H OTPOT QA, which features a truly open-domain setting with more diverse questions. Multi-hop QA systems At a broader level, the need for multi-step searches, query task decomposition, and subtask extraction has been clearly recognized in the IR community (Hassan Awadallah et al., 2014; Mehrotra et al., 2016; Mehrotra and Yilmaz, 2017), but multi-hop QA has only recently been studied closely with the release of large-scale datasets. Much research has focused on enabling multi-hop reasoning in question answering models in the few-"
D19-1261,P19-1613,0,0.36733,"18) are among the largest-scale multi-hop QA datasets to date. While the former is constructed around a knowledge base and the knowledge schema therein, the latter adopts a free-form question generation process in crowdsourcing and span-based evaluation. Both datasets feature a few-document setting where the gold supporting facts are provided along with a small set of distractors to ease the computational burden. However, researchers have shown that this sometimes results in gameable contexts, and thus does not always test the model’s capability of multi-hop reasoning (Chen and Durrett, 2019; Min et al., 2019a). Therefore, in this work, we focus on the fullwiki setting of H OTPOT QA, which features a truly open-domain setting with more diverse questions. Multi-hop QA systems At a broader level, the need for multi-step searches, query task decomposition, and subtask extraction has been clearly recognized in the IR community (Hassan Awadallah et al., 2014; Mehrotra et al., 2016; Mehrotra and Yilmaz, 2017), but multi-hop QA has only recently been studied closely with the release of large-scale datasets. Much research has focused on enabling multi-hop reasoning in question answering models in the few-"
D19-1261,D17-1061,0,0.0595943,"ng document with dk before concatenating it with Ck to make sure the downstream models have access to necessary context. 3.4 Deriving Supervision Signal for Query Generation When deriving supervision signal to train our query generators, the potential search space is enormous for each step of reasoning even if we constrain ourselves to predicting spans from the context. This is aggravated by multiple hops of reasoning required by the question. One solution to this issue is to train the query generators with reinforcement learning (RL) techniques (e.g., REINFORCE (Sutton et al., 2000)), where (Nogueira and Cho, 2017) and (Buck et al., 2018) are examples of one-step query generation with RL. However, it is computationally inefficient, and has high variance especially for the second reasoning step and forward, because the context depends greatly on what queries have been chosen previously and their search results. Instead, we propose to leverage the limited supervision we have about the gold supporting documents d1 , . . . , dS to narrow down the search space. The key insight we base our approach on is that at any step of open-domain multi-hop reasoning, there is some semantic overlap between the retrieval"
D19-1261,P18-2124,0,0.0537826,"Missing"
D19-1261,D16-1264,0,0.112844,"the search results that it cannot be easily retrieved based on merely the question. (Best viewed in color) Introduction Open-domain question answering (QA) is an important means for us to make use of knowledge in large text corpora and enables diverse queries without requiring a knowledge schema ahead of time. Enabling such systems to perform multistep inference can further expand our capability to explore the knowledge in these corpora (e.g., see Figure 1). ∗ Equal contribution, order decided by a random number generator. Fueled by the recently proposed large-scale QA datasets such as SQuAD (Rajpurkar et al., 2016, 2018) and TriviaQA (Joshi et al., 2017), much progress has been made in open-domain question answering. Chen et al. (2017) proposed a twostage approach of retrieving relevant content with the question, then reading the paragraphs returned by the information retrieval (IR) component to arrive at the final answer. This “retrieve and read” approach has since been adopted and extended in various open-domain QA systems (Nishida et al., 2018; Kratzwald and Feuerriegel, 2018), but it is inherently limited to answering questions that do not require multi-hop/multi-step reasoning. This is because for"
D19-1261,N18-1059,0,0.216028,"However, centered around entity names, this model risks missing purely descriptive clues in the question. Das et al. (2019) propose a neural retriever trained with distant supervision to bias towards paragraphs containing answers to the given questions, which is then used in a multi-step reader-reasoner framework. This does not fundamentally address the discoverability issue in opendomain multi-hop QA, however, because usually not all the evidence can be directly retrieved with the question. Besides, the neural retrieval model lacks explainability, which is crucial in real-world applications. Talmor and Berant (2018), instead, propose to answer multi-hop questions at scale by decomposing the question into sub-questions and perform iterative retrieval and question answering, which shares very similar motivations as our work. However, the questions studied in that work are based on logical forms of a fixed schema, which yields additional supervision for question decomposition but limits the diversity of questions. More recently, Min et al. (2019b) apply a similar idea to H OTPOT QA, but this approach similarly requires additional annotations for decomposition, and the authors did not apply it to iterative r"
D19-1261,Q18-1021,0,0.238614,"ragraphs returned by the information retrieval (IR) component to arrive at the final answer. This “retrieve and read” approach has since been adopted and extended in various open-domain QA systems (Nishida et al., 2018; Kratzwald and Feuerriegel, 2018), but it is inherently limited to answering questions that do not require multi-hop/multi-step reasoning. This is because for many multi-hop questions, not all the relevant context can be obtained in a single retrieval step (e.g., “Ernest Cline” in Figure 1). More recently, the emergence of multi-hop question answering datasets such as QAngaroo (Welbl et al., 2018) and H OTPOT QA (Yang et al., 2018) has sparked interest in multi-hop QA in the research community. Designed to be more challenging than SQuAD-like datasets, they feature questions that require context of more than one 2590 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2590–2602, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics document to answer, testing QA systems’ abilities to infer the answer in the presence of multiple pieces of ev"
D19-1261,D18-1259,1,0.89555,"retrieval (IR) component to arrive at the final answer. This “retrieve and read” approach has since been adopted and extended in various open-domain QA systems (Nishida et al., 2018; Kratzwald and Feuerriegel, 2018), but it is inherently limited to answering questions that do not require multi-hop/multi-step reasoning. This is because for many multi-hop questions, not all the relevant context can be obtained in a single retrieval step (e.g., “Ernest Cline” in Figure 1). More recently, the emergence of multi-hop question answering datasets such as QAngaroo (Welbl et al., 2018) and H OTPOT QA (Yang et al., 2018) has sparked interest in multi-hop QA in the research community. Designed to be more challenging than SQuAD-like datasets, they feature questions that require context of more than one 2590 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2590–2602, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics document to answer, testing QA systems’ abilities to infer the answer in the presence of multiple pieces of evidence and to efficiently find the"
de-marneffe-etal-2006-generating,levy-andrew-2006-tregex,0,\N,Missing
de-marneffe-etal-2006-generating,W03-2401,0,\N,Missing
de-marneffe-etal-2006-generating,A00-2018,0,\N,Missing
de-marneffe-etal-2006-generating,N03-1022,0,\N,Missing
de-marneffe-etal-2006-generating,J03-4003,0,\N,Missing
de-marneffe-etal-2006-generating,P03-1054,1,\N,Missing
de-marneffe-etal-2014-universal,de-marneffe-etal-2006-generating,1,\N,Missing
de-marneffe-etal-2014-universal,W09-2307,1,\N,Missing
de-marneffe-etal-2014-universal,J03-4003,0,\N,Missing
de-marneffe-etal-2014-universal,W08-1301,1,\N,Missing
de-marneffe-etal-2014-universal,P03-1054,1,\N,Missing
de-marneffe-etal-2014-universal,P06-1033,1,\N,Missing
de-marneffe-etal-2014-universal,W13-3721,1,\N,Missing
de-marneffe-etal-2014-universal,P08-1109,1,\N,Missing
de-marneffe-etal-2014-universal,P06-1055,0,\N,Missing
de-marneffe-etal-2014-universal,C12-1147,0,\N,Missing
de-marneffe-etal-2014-universal,petrov-etal-2012-universal,0,\N,Missing
de-marneffe-etal-2014-universal,J05-1004,0,\N,Missing
de-marneffe-etal-2014-universal,P05-1013,1,\N,Missing
de-marneffe-etal-2014-universal,N13-1070,0,\N,Missing
de-marneffe-etal-2014-universal,W13-2308,0,\N,Missing
de-marneffe-etal-2014-universal,P13-2103,0,\N,Missing
de-marneffe-etal-2014-universal,P13-2017,1,\N,Missing
E17-2075,D18-1418,0,0.0583829,"Missing"
E17-2075,P16-1154,0,0.0246776,"and reinforcement learning with carefully designed action spaces (Young et al., 2013). However, the large, hand-designed action and state spaces make this class of models brittle and unscalable, and in practice most deployed dialogue systems remain hand-written, rule-based systems. Recently, neural network models have achieved 468 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 468–473, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Architecture text summarization (Gu et al., 2016; Ling et al., 2016; Gulcehre et al., 2016). We therefore augment the attention encoder-decoder model with an attention-based copy mechanism in the style of (Jia and Liang, 2016). In this scheme, during decoding we compute our new logits vector ˜ t, h ˜ 0 , at ] where at as ot = U [h t [1:m] [1:m] is the concatenated attention scores of the encoder hidden states, and we are now predicting over a vocabulary of size |V |+ m. The model, thus, either predicts a token yt from V or copies a token xi from the encoder input context, via the attention score ati . Rather than copy over any token mention"
E17-2075,W14-4337,0,0.0626987,"neural architectures improves performance on various sequence-to-sequence tasks including code generation, machine translation, and All of our architectures use an LSTM cell as the recurrent unit (Hochreiter and Schmidhuber, 1997) with a bias of 1 added to the forget gate in the style of (Zaremba et al., 2015). 469 3 3.1 Avg. # of Utterances Per Dialogue Vocabulary Size Training Dialogues Validation Dialogues Test Dialogues # of Distinct Entities # of Entity (or Slot) Types Experiments Data For our experiments, we used dialogues extracted from the Dialogue State Tracking Challenge 2 (DSTC2) (Henderson et al., 2014), a restaurant reservation system dataset. While the goal of the original challenge was building a system for inferring dialogue state, for our study, we use the version of the data from Bordes and Weston (2016), which ignores the dialogue state annotations, using only the raw text of the dialogues. The raw text includes user and system utterances as well as the API calls the system would make to the underlying KB in response to the user’s queries. Our model then aims to predict both these system utterances and API calls, each of which is regarded as a turn of the dialogue. We use the train/va"
E17-2075,P16-1002,0,0.015846,"rittle and unscalable, and in practice most deployed dialogue systems remain hand-written, rule-based systems. Recently, neural network models have achieved 468 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 468–473, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Architecture text summarization (Gu et al., 2016; Ling et al., 2016; Gulcehre et al., 2016). We therefore augment the attention encoder-decoder model with an attention-based copy mechanism in the style of (Jia and Liang, 2016). In this scheme, during decoding we compute our new logits vector ˜ t, h ˜ 0 , at ] where at as ot = U [h t [1:m] [1:m] is the concatenated attention scores of the encoder hidden states, and we are now predicting over a vocabulary of size |V |+ m. The model, thus, either predicts a token yt from V or copies a token xi from the encoder input context, via the attention score ati . Rather than copy over any token mentioned in the encoder dialogue context, our model is trained to only copy over entities of the knowledge base mentioned in the dialogue context, as this provides a conceptually intui"
E17-2075,N16-1014,0,0.0160263,"den layer and cell sizes were set to 353, identified through our search. We applied gradient clipping with a clipvalue of 10 to avoid gradient explosions during training. The attention, output parameters, word embeddings, and LSTM weights were randomly initialized from a uniform unit-scaled distribution in the style of (Sussillo and Abbott, 2015). 3.3 14 1,229 1,618 500 1,117 452 8 • BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002), which has also been used in past literature for evaluating dialogue systems (Ritter et al., 2011; Li et al., 2016). We calculate average BLEU score over all responses generated by the system, and primarily report these scores to gauge our model’s ability to accurately generate the language patterns seen in DSTC2. • Entity F1 : Each system response in the test data defines a gold set of entities. To compute an entity F1 , we micro-average over the entire set of system dialogue responses. This metric evaluates the model’s ability to generate relevant entities from the underlying knowledge base and to capture the semantics of the user-initiated dialogue flow. Metrics Evaluation of dialogue systems is known t"
E17-2075,E17-1042,0,0.13938,"Missing"
E17-2075,P16-1057,0,0.00660341,"Missing"
E17-2075,D16-1230,0,0.0100096,"Missing"
E17-2075,D15-1166,1,0.0425182,"ting x1 , . . . , xm denote these tokens, we first embed these tokens using a trained embedding function φemb that maps each token to a fixed-dimensional vector. These mappings are fed into the encoder to produce contextsensitive hidden representations h1 , . . . , hm . The vanilla Seq2Seq decoder predicts the tokens of the ith system response si by first computing decoder hidden states via the recurrent unit. ˜ 1, . . . , h ˜ n as the hidden states of the We denote h decoder and y1 , . . . , yn as the output tokens. We extend this decoder with an attention-based model (Bahdanau et al., 2015; Luong et al., 2015a), where, at every time step t of the decoding, an attention score ati is computed for each hidden state hi of the encoder, using the attention mechanism of (Vinyals et al., 2015b). Formally this attention can be described by the following equations: ˜ t) uti = v T tanh(W1 hi + W2 h ati (1) Softmax(uti ) m X ati hi i=1 ˜ t, h ˜0 ] U [h t (3) yt = Softmax(ot ) (5) = ˜0 = h t ot = In our best performing model, we augment the inputs to the encoder by adding entity type features. Classes present in the knowledge base of the dataset, namely the 8 distinct entity types referred to in Table 1, are e"
E17-2075,P02-1040,0,0.118016,"validation subset of the data. Dropout keep rates ranged from 0.75 to 0.95. We used word embeddings with size 300, and hidden layer and cell sizes were set to 353, identified through our search. We applied gradient clipping with a clipvalue of 10 to avoid gradient explosions during training. The attention, output parameters, word embeddings, and LSTM weights were randomly initialized from a uniform unit-scaled distribution in the style of (Sussillo and Abbott, 2015). 3.3 14 1,229 1,618 500 1,117 452 8 • BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002), which has also been used in past literature for evaluating dialogue systems (Ritter et al., 2011; Li et al., 2016). We calculate average BLEU score over all responses generated by the system, and primarily report these scores to gauge our model’s ability to accurately generate the language patterns seen in DSTC2. • Entity F1 : Each system response in the test data defines a gold set of entities. To compute an entity F1 , we micro-average over the entire set of system dialogue responses. This metric evaluates the model’s ability to generate relevant entities from the underlying knowledge base"
E17-2075,D11-1054,0,0.021992,"ith size 300, and hidden layer and cell sizes were set to 353, identified through our search. We applied gradient clipping with a clipvalue of 10 to avoid gradient explosions during training. The attention, output parameters, word embeddings, and LSTM weights were randomly initialized from a uniform unit-scaled distribution in the style of (Sussillo and Abbott, 2015). 3.3 14 1,229 1,618 500 1,117 452 8 • BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002), which has also been used in past literature for evaluating dialogue systems (Ritter et al., 2011; Li et al., 2016). We calculate average BLEU score over all responses generated by the system, and primarily report these scores to gauge our model’s ability to accurately generate the language patterns seen in DSTC2. • Entity F1 : Each system response in the test data defines a gold set of entities. To compute an entity F1 , we micro-average over the entire set of system dialogue responses. This metric evaluates the model’s ability to generate relevant entities from the underlying knowledge base and to capture the semantics of the user-initiated dialogue flow. Metrics Evaluation of dialogue"
E17-2075,P16-1014,0,\N,Missing
H05-1049,W04-3205,0,0.00936627,"particular we use the top 3 senses of both words to determine synsets. • Hypernym Match: v is a “kind of” M (v), as determined by WordNet. Note that this feature is asymmetric. • WordNet Similarity: v and M (v) are similar according to WordNet::Similarity (Pedersen et al., 2004). In particular, we use the measure described in (Resnik, 1995). We found it useful to only use similarities above a fixed threshold to ensure precision. • LSA Match: v and M (v) are distributionally similar according to a freely available Latent Semantic Indexing package,2 or for verbs similar according to VerbOcean (Chklovski and Pantel, 2004). • POS Match: v and M (v) have the same part of speech. • No Match: M (v) is NULL. Although the above conditions often produce reasonable matchings between text and hypothesis, we found the recall of these lexical resources to be far from adequate. More robust lexical resources would almost certainly boost performance. 5.2 Path substitution cost model Our PathSub(v → v ′ , M (v) → M (v ′ )) model is also based upon a sliding scale cost based upon the following conditions: • Exact Match: M (v) → M (v ′ ) is an en edge in T with the same label. • Partial Match: M (v) → M (v ′ ) is an en edge in"
H05-1049,P03-1054,1,0.0248412,"es, where dependency relationships, as in (Lin and Pantel, 2001), are characterized by the path between vertices. Given this representation, we judge entailment by measuring not only how many of the hypothesis vertices are matched to the text but also how well the relationships between vertices in the hypothesis are preserved in their textual counterparts. For the remainder of the section we outline how we produce graphs from text, and in the next section we introduce our graph matching model. 3.2 From Text To Graphs Starting with raw English text, we use a version of the parser described in (Klein and Manning, 2003), to obtain a parse tree. Then, we derive a dependency tree representation of the sentence using a slightly modified version of Collins’ head propagation rules (Collins, 1999), which make main verbs not auxiliaries the head of sentences. Edges in the dependency graph are labeled by a set of hand-created tgrep expressions. These labels represent “surface” syntax relationships such as subj for subject and amod for adjective modifier, similar to the relations in Minipar (Lin and Pantel, 2001). The dependency graph is the basis for our graphical representation, but it is enhanced in the following"
H05-1049,N03-1022,0,0.468123,"approximate entailment using the amount of the sentence’s semantic content which is contained in the text. We present results on the Recognizing Textual Entailment dataset (Dagan et al., 2005), and show that our approach outperforms Bag-Of-Words and TF-IDF models. In addition, we explore common sources of errors in our approach and how to remedy them. One sub-task underlying these applications is the ability to recognize semantic entailment; whether one piece of text follows from another. In contrast to recent work which has successfully utilized logicbased abductive approaches to inference (Moldovan et al., 2003; Raina et al., 2005b), we adopt a graphbased representation of sentences, and use graph matching approach to measure the semantic overlap of text. Graph matching techniques have proven to be a useful approach for tractable approximate matching in other domains including computer vision. In the domain of language, graphs provide a natural way to express the dependencies between words and phrases in a sentence. Furthermore, graph matching also has the advantage of providing a framework for structural matching of phrases that would be difficult to resolve at the level of individual words. 1 Intr"
H05-1049,P02-1040,0,0.10426,"sets the pairs are divided into 7 tasks – each containing roughly the same number of entailed and not-entailed instances – which were used as both motivation and means for obtaining and constructing the data items. We will use the following toy example to illustrate our representation and matching technique: Text: In 1994, Amazon.com was founded by Jeff Bezos. Hypothesis: Bezos established a company. 1 Usually a single sentence, but occasionally longer. 388 3.1 The Need for Dependencies Perhaps the most common representation of text for assessing content is “Bag-Of-Words” or “Bag-of-NGrams” (Papineni et al., 2002). However, such representations lose syntactic information which can be essential to determining entailment. Consider a Question Answer system searching for an answer to When was Israel established? A representation which did not utilize syntax would probably enthusiastically return an answer from (the 2005 RTE text): The National Institute for Psychobiology in Israel was established in 1979. In this example, it’s important to try to match relationships as well as words. In particular, any answer to the question should preserve the dependency between Israel and established. However, in the pro"
H05-1049,N04-3012,0,0.0388857,"where progressively higher costs are 390 given based upon the following conditions: • Exact Match: v and M (v) are identical words/ phrases. • Stem Match: v and M (v)’s stems match or one is a derivational form of the other; e.g., matching coaches to coach. • Synonym Match: v and M (v) are synonyms according to WordNet (Fellbaum, 1998). In particular we use the top 3 senses of both words to determine synsets. • Hypernym Match: v is a “kind of” M (v), as determined by WordNet. Note that this feature is asymmetric. • WordNet Similarity: v and M (v) are similar according to WordNet::Similarity (Pedersen et al., 2004). In particular, we use the measure described in (Resnik, 1995). We found it useful to only use similarities above a fixed threshold to ensure precision. • LSA Match: v and M (v) are distributionally similar according to a freely available Latent Semantic Indexing package,2 or for verbs similar according to VerbOcean (Chklovski and Pantel, 2004). • POS Match: v and M (v) have the same part of speech. • No Match: M (v) is NULL. Although the above conditions often produce reasonable matchings between text and hypothesis, we found the recall of these lexical resources to be far from adequate. Mor"
H05-1049,P05-1073,1,0.0598875,"rdNet, including verbs and their adjacent particles (e.g., blow off in He blew off his work) . 2. Dependency Folding: As in (Lin and Pantel, 2001), we found it useful to fold certain dependencies (such as modifying prepositions) so that modifiers became labels connecting the modifier’s governor and dependent directly. For instance, in the text graph in Figure 2, we have changed in from a word into a relation between its head verb and the head of its NP complement. 3. Semantic Role Labeling: We also augment the graph representation with Probank-style semantic roles via the system described in (Toutanova et al., 2005). Each predicate adds an arc labeled with the appropriate semantic role to the head of the argument phrase. This helps to create links between words which share a deep semantic relation not evident in the surface syntax. Additionally, modifying phrases are labeled with their semantic types (e.g., in 1991 is linked by a Temporal edge in the text graph of Figure 2), which should be useful in Question Answering tasks. 4. Coreference Links: Using a co-rereference resolution tagger, coref links are added through389 out the graph. These links allowed connecting the referent entity to the vertices of"
H05-1049,C00-1043,0,\N,Missing
H05-1049,J03-4003,0,\N,Missing
H05-1049,W07-1401,0,\N,Missing
I05-3005,W00-1201,0,0.0138143,"Missing"
I05-3005,J93-2004,0,0.0267039,"Missing"
I05-3005,W04-3236,0,0.0200273,"Missing"
I05-3005,W96-0213,0,0.283886,"ish suffix tokens in WSJ have more than one tag. Most English suffixes are derivational and inflectional suffixes like able, -s and -ed. Such functional suffixes are used to indicate word classes or syntactic function. Chinese, however, has no inflectional suffixes and only a few derivational suffixes and so suffixes may not be as good a cue for word classes. Finally, since Chinese has no derivational morpheme for nominalization, it is difficult to distinguish a nominalization and a verb. 5.2 Our model builds on research into loglinear models by Ng and Low (2004), Toutanova et al., (2003) and Ratnaparkhi (1996). The first research uses independent maximum entropy classifiers, with a sequence model imposing categorical valid tag sequence constraints. The latter two use maximum entropy Markov models (MEMM) (McCallum et al., 2000), that use log-linear models to obtain the probabilities of a state transition given an observation and the previous state, as illustrated in Figure 1 (a). These points suggest that morpheme identity, which is the major feature used in previous research on unknown words in English and German, will be insufficient in Chinese. This suggests the need for more sophisticated featur"
I05-3005,P99-1023,0,0.0284072,"am prefixes and suffixes for n up to 4.10 An example is: Sequence features We examined several tag sequence features from both left and right side of the current word. We use the term lexical features to refer to features derived from the identity of a word, and tag sequence features refer to features derived from the tags of surrounding words. 䌘㹟 INFORMATION-BAG &quot;folder&quot; Wi=䌘㹟 “a folder” FAFFIX={(prefix1,䌘), (prefix2,䌘), (prefix3,䌘 㹟), (suffix1,㹟), (suffix2,㹟), (suffix3,䌘㹟)} These features have been shown to be useful in previous research on English (Toutanova et al, 2003, Brants 2000, Thede and Harper 1999) 5.5.2 CTBMorph (CTBM) The models9 in Table 7 list the different tag sequence features used; they also use the same lexical features from the model 2Rw+2Lw shown in Table 6. The table shows that Model Lt+LLt conditioning on the previous tag and the conjunction of the two previous While affix information can be very informative, we showed earlier that affixes in Chinese are sparse, short, and ambiguous. Thus as our first new feature we used a POS-vector of the set of tags a given affix could have. We used the training set to build a morpheme/POS dictionary with the possible tags for each 8 We a"
I05-3005,N03-1033,1,0.296525,"Missing"
I05-3005,C02-1145,0,0.0347657,"Missing"
I05-3005,A00-1031,0,\N,Missing
I05-3027,P03-2039,0,0.0148303,"of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled. A CRF allows us to utilize a large number of n-gram features and different state sequence 168 based features and also provides an intuitive framework for the use of morphological features. 3 3.1 Feature engineering Features The linguistic features used in our model fall into three categories: character identity n-grams, morphological and character reduplication features. For each state, the character identity features (Ng & Low 2004, Xue & Shen 2003, Goh et al. 2003) are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions. Specifically, we used four types of unigram feature functions, designated as C0 (current character), C1 (next character), C-1 (previous character), C-2 (the character two characters back). Furthermore, four types of bi-gram features were used, and are notationally designated here as conjunctions of the previously specified unigram features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0. Given that unknown words are normally more than one character long, when repres"
I05-3027,W04-3236,0,0.101092,"the sentence, X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled. A CRF allows us to utilize a large number of n-gram features and different state sequence 168 based features and also provides an intuitive framework for the use of morphological features. 3 3.1 Feature engineering Features The linguistic features used in our model fall into three categories: character identity n-grams, morphological and character reduplication features. For each state, the character identity features (Ng & Low 2004, Xue & Shen 2003, Goh et al. 2003) are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions. Specifically, we used four types of unigram feature functions, designated as C0 (current character), C1 (next character), C-1 (previous character), C-2 (the character two characters back). Furthermore, four types of bi-gram features were used, and are notationally designated here as conjunctions of the previously specified unigram features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0. Given that unknown words are normally more t"
I05-3027,C04-1081,0,0.795857,"as character identity, morphological and character reduplication features. Because our morphological features were extracted from the training corpora automatically, our system was not biased toward any particular variety of Mandarin. Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers. Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR). 1 2 Algorithm Our system builds on research into conditional random field (CRF), a statistical sequence modeling framework first introduced by Lafferty et al. (2001). Work by Peng et al. (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. Gaussian priors were used to prevent overfitting and a quasi-Newton method was used for parameter optimization. The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: Introduction The 2005 Sighan Bakeoff included four different corpora, Academia Sinica (AS), City University of Hong Kong (HK), Peking University (PK), and Microsoft"
I05-3027,W03-1719,0,0.00712342,", such that each character is labeled either as the beginning of a word or the continuation of one. Gaussian priors were used to prevent overfitting and a quasi-Newton method was used for parameter optimization. The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: Introduction The 2005 Sighan Bakeoff included four different corpora, Academia Sinica (AS), City University of Hong Kong (HK), Peking University (PK), and Microsoft Research Asia (MSR), each of which has its own definition of a word. In the 2003 Sighan Bakeoff (Sproat & Emerson 2003), no single model performed well on all corpora included in the task. Rather, systems tended to do well on corpora largely drawn from a set of similar Mandarin varieties to the one they were originally developed for. Across corPλ (Y |X ) = 1 § exp ¨ ¦¦ λ Z(X ) © c∈C k k · f k (Y , X , c )¸ c  Y is the label sequence for the sentence, X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled. A CRF allows us to utilize a large number of n-gram features and different state sequence 168 based f"
I05-3027,W03-1728,0,0.0089699,"X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled. A CRF allows us to utilize a large number of n-gram features and different state sequence 168 based features and also provides an intuitive framework for the use of morphological features. 3 3.1 Feature engineering Features The linguistic features used in our model fall into three categories: character identity n-grams, morphological and character reduplication features. For each state, the character identity features (Ng & Low 2004, Xue & Shen 2003, Goh et al. 2003) are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions. Specifically, we used four types of unigram feature functions, designated as C0 (current character), C1 (next character), C-1 (previous character), C-2 (the character two characters back). Furthermore, four types of bi-gram features were used, and are notationally designated here as conjunctions of the previously specified unigram features, C0C1, C-1C0, C-1C1, C-2C-1, and C2C0. Given that unknown words are normally more than one character"
I05-3027,P04-1059,0,\N,Missing
I05-3027,W03-1726,0,\N,Missing
I11-1001,W09-3607,0,0.0323256,"to their name, topic models generally only identify the topic or area of a paper (such as ‘parsing’ or ‘speech recognition’), and neither provide nor label different cross-cutting aspects like techniques used or the application domain of the paper. As a case study, we examine the articles published in the computational linguistics community. We study the influence of the community’s sub-fields, such as parsing and machine translation, using the FOCUS, TECHNIQUE, and DO MAIN phrases extracted from the articles. We use the document collection from the ACL Anthology dataset2 (Bird et al., 2008; Radev et al., 2009), since it has full text of papers available. To get the the sub-fields of the community, we use latent Dirichlet allocation (Blei et al., 2003) to find topics and label them by hand.3 However, our general approach can be used to study any case of the influence of academic communities, including looking more broadly at the influence of statistics or economics across the social sciences. We study how communities influence each other in terms of techniques that are reused, and show how some communities ‘mature’ so that the results they produce get adopted as tools for solving other problems. For"
I11-1001,bird-etal-2008-acl,0,0.0429323,"raightforward; true to their name, topic models generally only identify the topic or area of a paper (such as ‘parsing’ or ‘speech recognition’), and neither provide nor label different cross-cutting aspects like techniques used or the application domain of the paper. As a case study, we examine the articles published in the computational linguistics community. We study the influence of the community’s sub-fields, such as parsing and machine translation, using the FOCUS, TECHNIQUE, and DO MAIN phrases extracted from the articles. We use the document collection from the ACL Anthology dataset2 (Bird et al., 2008; Radev et al., 2009), since it has full text of papers available. To get the the sub-fields of the community, we use latent Dirichlet allocation (Blei et al., 2003) to find topics and label them by hand.3 However, our general approach can be used to study any case of the influence of academic communities, including looking more broadly at the influence of statistics or economics across the social sciences. We study how communities influence each other in terms of techniques that are reused, and show how some communities ‘mature’ so that the results they produce get adopted as tools for solvin"
I11-1001,H05-1091,0,0.0534548,"r the dynamics and the overall influence of computational linguistics subfields. We introduce a dataset of abstracts labeled with the three categories.4 2 Related Work While there is some connection to keyphrase selection in text summarization (Radev et al., 2002), extracting FOCUS, TECHNIQUE and DO MAIN phrases is fundamentally a form of information extraction, and there has been a wide variety of prior work in this area. Some work, including the seminal (Hearst, 1992), identified patterns (ISA relations) using hand-written rules, while other work has learned patterns over dependency graphs (Bunescu and Mooney, 2005). This work builds 2 http://www.aclweb.org/anthology In this paper, we use the terms communities, subcommunities and sub-fields interchangeably. 3 4 The dataset is available at http://cs.stanford. edu/people/sonal/fta for the research community. 2 present → (direct object) work → (preposition on) propose → (direct object) using → (direct object) TECHNIQUE apply → (direct object) extend → (direct object) system → (preposition for) DOMAIN task → (preposition of) framework → (preposition for) Table 1: Some examples of semantic extraction patterns that extract information from dependency trees of"
I11-1001,W99-0613,0,0.0267171,"munity. 2 present → (direct object) work → (preposition on) propose → (direct object) using → (direct object) TECHNIQUE apply → (direct object) extend → (direct object) system → (preposition for) DOMAIN task → (preposition of) framework → (preposition for) Table 1: Some examples of semantic extraction patterns that extract information from dependency trees of sentences. A pattern is of the form T → (d), where T is the trigger word and d is the dependency that the trigger word’s node has with its successor. on previous successful use of bootstrapping learning techniques in NLP (Yarowsky, 1995; Collins and Singer, 1999; Riloff and Jones, 1999); in its use of dependency patterns it is perhaps especially close to (Yangarber et al., 2000). Topic models have been used to study popularity of communities (Griffiths and Steyvers, 2004), the history of ideas (Hall et al., 2008), and scholarly impact of papers (Gerrish and Blei, 2010). However, topic models do not extract detailed information from text as we do. Still, we use topicto-word distributions from topic models as a way of describing sub-fields. Demner-Fushman and Lin (2007) used hand written knowledge extractors to extract information, such as population a"
I11-1001,J07-1005,0,0.0192202,"on previous successful use of bootstrapping learning techniques in NLP (Yarowsky, 1995; Collins and Singer, 1999; Riloff and Jones, 1999); in its use of dependency patterns it is perhaps especially close to (Yangarber et al., 2000). Topic models have been used to study popularity of communities (Griffiths and Steyvers, 2004), the history of ideas (Hall et al., 2008), and scholarly impact of papers (Gerrish and Blei, 2010). However, topic models do not extract detailed information from text as we do. Still, we use topicto-word distributions from topic models as a way of describing sub-fields. Demner-Fushman and Lin (2007) used hand written knowledge extractors to extract information, such as population and intervention, in their clinical question-answering system to improve ranking of relevant abstracts. Our categorization of key aspects is applicable for broader range of communities, and we learn the patterns by bootstrapping. Li et al. (2010) used semantic metadata to create a semantic digital library for chemistry and identified experimental paragraphs using keywords features. Xu et al. (2006) and Ruch et al. (2007) proposed systems, in clinical-trials and biomedical domain, respectively, to classify senten"
I11-1001,A00-1039,0,0.011787,"apply → (direct object) extend → (direct object) system → (preposition for) DOMAIN task → (preposition of) framework → (preposition for) Table 1: Some examples of semantic extraction patterns that extract information from dependency trees of sentences. A pattern is of the form T → (d), where T is the trigger word and d is the dependency that the trigger word’s node has with its successor. on previous successful use of bootstrapping learning techniques in NLP (Yarowsky, 1995; Collins and Singer, 1999; Riloff and Jones, 1999); in its use of dependency patterns it is perhaps especially close to (Yangarber et al., 2000). Topic models have been used to study popularity of communities (Griffiths and Steyvers, 2004), the history of ideas (Hall et al., 2008), and scholarly impact of papers (Gerrish and Blei, 2010). However, topic models do not extract detailed information from text as we do. Still, we use topicto-word distributions from topic models as a way of describing sub-fields. Demner-Fushman and Lin (2007) used hand written knowledge extractors to extract information, such as population and intervention, in their clinical question-answering system to improve ranking of relevant abstracts. Our categorizati"
I11-1001,P95-1026,0,0.0388813,"the research community. 2 present → (direct object) work → (preposition on) propose → (direct object) using → (direct object) TECHNIQUE apply → (direct object) extend → (direct object) system → (preposition for) DOMAIN task → (preposition of) framework → (preposition for) Table 1: Some examples of semantic extraction patterns that extract information from dependency trees of sentences. A pattern is of the form T → (d), where T is the trigger word and d is the dependency that the trigger word’s node has with its successor. on previous successful use of bootstrapping learning techniques in NLP (Yarowsky, 1995; Collins and Singer, 1999; Riloff and Jones, 1999); in its use of dependency patterns it is perhaps especially close to (Yangarber et al., 2000). Topic models have been used to study popularity of communities (Griffiths and Steyvers, 2004), the history of ideas (Hall et al., 2008), and scholarly impact of papers (Gerrish and Blei, 2010). However, topic models do not extract detailed information from text as we do. Still, we use topicto-word distributions from topic models as a way of describing sub-fields. Demner-Fushman and Lin (2007) used hand written knowledge extractors to extract informa"
I11-1001,D08-1038,1,0.876588,", they still have a lot of influence on recent papers mainly due to techniques like expectation maximization and hidden Markov models. Therefore, our results show that overall they have been the most influential fields in the last two decades. Probability theory, unlike speech recognition, is traditionally not a separate sub-field of computational linguistics, but it is an important topic since many papers use and work on probabilistic approaches. We also show that the study of influence is different from studying popularity or hotness of communities, such as in (Griffiths and Steyvers, 2004; Hall et al., 2008), which is based on the expected number of papers published in the community in a given year. Contributions We introduce a new categorization of key aspects of scientific articles, which is (1) FOCUS: main contribution, (2) TECHNIQUE: method or tool used, and (3) DOMAIN: application domain. We extract the aspects by matching semantic patterns to dependency trees and learn the patterns using bootstrapping. We propose a new definition of influence of a research community in terms of its key aspects adopted as techniques by the other communities. We present a case study on the computational lingu"
I11-1001,C92-2082,0,0.0468448,"ity using the the three aspects extracted from its articles, both for verifying the results of our system, and for showing novel results for the dynamics and the overall influence of computational linguistics subfields. We introduce a dataset of abstracts labeled with the three categories.4 2 Related Work While there is some connection to keyphrase selection in text summarization (Radev et al., 2002), extracting FOCUS, TECHNIQUE and DO MAIN phrases is fundamentally a form of information extraction, and there has been a wide variety of prior work in this area. Some work, including the seminal (Hearst, 1992), identified patterns (ISA relations) using hand-written rules, while other work has learned patterns over dependency graphs (Bunescu and Mooney, 2005). This work builds 2 http://www.aclweb.org/anthology In this paper, we use the terms communities, subcommunities and sub-fields interchangeably. 3 4 The dataset is available at http://cs.stanford. edu/people/sonal/fta for the research community. 2 present → (direct object) work → (preposition on) propose → (direct object) using → (direct object) TECHNIQUE apply → (direct object) extend → (direct object) system → (preposition for) DOMAIN task → ("
I11-1001,de-marneffe-etal-2006-generating,1,0.0855426,"Missing"
I11-1001,J02-4001,0,\N,Missing
I13-1060,D11-1139,0,0.0208955,"l aforementioned works that address the issue of weight undertraining, another recent work that looks at this problem is Wang and Manning (2013). They examined the objective function of dropout training prescribed by Hinton et al. (2012), and proposed an approximation of dropout by directly sampling from a Gaussian approximation. The resulting algorithm is orders of magnitude faster than the iterative algorithm given by Hinton et al. (2012). It will be interesting to compare our proposed method with this method in the future. The use of group-sparsity norms in NLP research is relatively rare. Martins et al. (2011) proposed the use of structure-inducing norms, in particular `1 `2 norm (group lasso), for learning the structure of classifiers. A key observation is that during testing, most of the runtime is consumed in the feature instantiation stage. Since `1 `2 norm discards whole groups of features at a time, there are fewer feature templates that need to be instantiated, therefore it could give a significant runtime speedup. Our use of `2 `1 norm takes a different flavor in that we explore the internal structure of the model. There is, however, one recent paper that also makes use of `2 `1 norm for a"
I13-1060,N12-1086,0,0.0236253,"e of structure-inducing norms, in particular `1 `2 norm (group lasso), for learning the structure of classifiers. A key observation is that during testing, most of the runtime is consumed in the feature instantiation stage. Since `1 `2 norm discards whole groups of features at a time, there are fewer feature templates that need to be instantiated, therefore it could give a significant runtime speedup. Our use of `2 `1 norm takes a different flavor in that we explore the internal structure of the model. There is, however, one recent paper that also makes use of `2 `1 norm for a similar reason. Das and Smith (2012) employed `2 `1 norm for learning sparse structure in a network formed by lexical predicates and semantic frames. Their results show that `2 `1 norm yields much more compact models than `1 or `2 norms, with superior performance in learning to expand lexicons. However, the optimization problem in Das and Smith (2012) was much simpler since all parameters can only take on positive values, and therefore they could directly use a gradient-descent method with specialized edge condition checks. In our work, we have shown a general-purpose method 8 Conclusion Our results show that previous automatic"
I13-1060,N10-1141,0,0.0462326,"Missing"
I13-1060,C00-2124,0,0.0521983,"Missing"
I13-1060,P05-1003,0,0.220953,"norm encourages sparsity within feature groups. We demonstrate how this property can be leveraged as a competition mechanism to induce groups of diverse experts, and introduce a new formulation of elitist lasso MaxEnt in the FOBOS optimization framework (Duchi and Singer, 2009). Results on Named Entity Recognition task suggest that this method gives consistent improvements over a standard logistic regression model, and is more effective than conventional induction schemes for experts. 1 One popular method that attempts to mitigate this problem is logarithmic opinion pools (LOP) (Heskes, 1998; Smith et al., 2005; Sutton et al., 2007). LOP works in a similar fashion to a product of experts model, in which a number of experts are individually assembled first, and then their predictions are combined multiplicatively to create an ensemble model. Experts are generated by first partitioning the feature space into subsets, then an independent model (expert) is trained on each subset of features (Sutton et al., 2007). Under the assumption that strongly correlated features are partitioned into separate subsets, this method effectively forces the models to learn how to make predictions with each subset of the"
I13-1060,1993.eamt-1.1,0,0.228443,"re we evaluate results on the development set after each optimization iteration, and terminate the procedure after not observing a performance increase on the development set in 25 continuous iterations. We found 150 to be a good λ value for `2 , 0.1 for `1 and 0.1 for `2 `1 . Similarly, we tuned the number of experts for both LOP baselines and the elitist LOP induction scheme. All model parameters were initialized to a random value in [−0.1, 0.1]. Results are measured in precision (P), recall (R) and F1 score. Statistical significance is measured using the paired bootstrap resampling method (Efron and Tibshirani, 1993). We compare our results against a MaxEnt baseline model with both `1 and `2 regularization, as well as an automatic expert induction model with `1 norm. Two commonly used LOP expert induction schemes were also compared. In the first scheme (LOP random), experts are constructed by randomly partitioning the features into n subsets, where n is the number of experts. Each expert therefore has n1 th of the full feature set. The second scheme (LOP sequential) works in a similar way, but instead of random partition, we create feature subsets sequentially and preserve the relative order of the featur"
I13-1060,P05-1045,1,0.0551049,"Missing"
I13-1060,P10-1040,0,0.0103684,"s sorting the parameter vector θ, which is a O(n log n) operation. When the number of features within each group is large, this could be prohibitively expensive. An approximate solution simply replaces Mgˆ(λ0 ) with the size of the group M . Kowalski and Torr´esani (2009) found that approximated elitist lasso works nearly as well as the exact version, but is faster and easier to implement. We adopt this approximation in our experiments. With this, we have described a general-purpose method for solving any convex optimization problem with `2 `1 norm. 6 Table 1: MaxEnt features. the one used in Turian et al. (2010) where we evaluate results on the development set after each optimization iteration, and terminate the procedure after not observing a performance increase on the development set in 25 continuous iterations. We found 150 to be a good λ value for `2 , 0.1 for `1 and 0.1 for `2 `1 . Similarly, we tuned the number of experts for both LOP baselines and the elitist LOP induction scheme. All model parameters were initialized to a random value in [−0.1, 0.1]. Results are measured in precision (P), recall (R) and F1 score. Statistical significance is measured using the paired bootstrap resampling meth"
I13-1060,P10-1052,0,0.0678232,"Missing"
I13-1060,N06-1012,0,\N,Missing
I13-1060,W03-0419,0,\N,Missing
I13-1183,D11-1014,1,0.0532743,"log Z(x) l=1 The change in node potential function from ψ to ψ 0 does not affect the inference procedure, and thus we can employ the same dynamic programming algorithm as in a CRF to calculate the log sum over Z(x) and the expectation of feature parameters. We adopted the simple L-BFGS algorithm for training weights in this model (Liu and Nocedal, 1989). Although L-BFGS is in general slower than mini-batch SGD – another common optimization algorithm used to train neural networks (Bengio et al., 2006, inter alia), it has been found to be quite stable and suitable for learning neural networks (Socher et al., 2011). The gradient of a parameter ω(k,j) is calculated as the following: |X ||xl | ∂ψ 0 (xl , yl i ) ∂ω(k,j) l=1 i=1  0 l l  ∂ψ (x , y i ) − EP (yl |xl ) ∂ω(k,j) XX ∂L = ∂ω(k,j)  The partial derivative of the potential function ∂ψ 0 (xl , yl i ) can be calculated using the back∂ω(k,j) propagation procedure, identical to how gradients of a standard Multilayer Perceptron are calculated. The gradient calculation for output layer parameters ∆ and edge parameters Λ follow the same form. We apply `2 -regularization to prevent overfitting. 4 Empirical Evaluation We evaluate the CRF and SLNN models on"
I13-1183,P10-1040,0,0.119735,"he `2 regularization parameter σ (variance in Gaussian prior), and found 20 to be a stable value. Overall tuning σ does not affect the qualitative results in our experiments. We terminate L-BFGS training when the average improvement is less than 1e3. All model parameters are initialized to a random value in [−0.1, 0.1] in order to break symmetry. We did not explicitly tune the features used in CRF to optimize for performance, since feature engineering is not the focus of this study. However, overall we found that the feature set we used is competitive with CRF results from earlier literature (Turian et al., 2010; Collobert et al., 2011). For models that embed hidden layers, we set the number of hidden nodes to 300. 2 Results are reported on the standard evaluation metrics of entity/chunk precision, recall and F1 measure. For experiments with continuous space feature representations (a.k.a., word embeddings), we took the word embeddings (130K words, 50 dimensions) used in Collobert et al. (2011), which were trained for 2 months over Wikipedia text. 3 All sequences of numbers are replaced with num (e.g., “PS1” would become “PSnum”), sentence boundaries are padded with token PAD, and unknown words are g"
I13-1183,P05-1045,1,0.0351236,"Missing"
I13-1183,W00-0726,0,0.0964689,"Percent. We converted the data to CoNLL2003 type format using the same method applied to the ACE data. We used a comprehensive set of features that comes with the standard distribution of Stanford NER model (Finkel et al., 2005). A total number of 437,905 features were generated for the CoNLL-2003 training dataset. 4.2 Syntactic Chunking In Syntactic Chunking, we tag each word with its phrase type. For example, tag B-NP indicates a word starts a noun phrase, and I-PP marks an intermediate word of a prepositional phrase. We test the models on the standard CoNLL-2000 shared task evaluation set (Sang and Buchholz, 2000). This dataset comes from the Penn Treebank. The training set contains 211K words (8.9K sentences), and the test set contains 47K words (2K sentences). The set of features used for this task is: • Current word and tag • Word pairs: wi ∧ wi+1 for i ∈ {−1, 0} • Tags: (ti ∧ ti+1 ) for i ∈ −1, 0; (t−1 , t0 , ti+1 ); • The Disjunctive word set of the previous and next 4 positions A total number of 317794 features were generated on this dataset. 4.3 Experimental Setup In all experiments, we used the development portion of the CoNLL-2003 data to tune the `2 regularization parameter σ (variance in Gau"
I13-1183,W03-0419,0,\N,Missing
J08-2002,P98-1013,0,0.3465,"Missing"
J08-2002,W04-2412,0,0.0487494,"Missing"
J08-2002,W05-0620,0,0.4847,"Missing"
J08-2002,A00-2018,0,0.874707,"the February 2004 data. For the February 2004 data, we used the standard split into training, development, and test sets—the annotations from sections 02–21 formed the training set, section 24 the development, and section 23 the test set. The set of argument labels considered is the set of core argument labels (ARG0 through ARG5) plus the modiﬁer labels (see Figure 1). The training set contained 85,392 propositions, the test set 4,615, and the development set 2,626. We evaluate semantic role labeling models on gold-standard parse trees and parse trees produced by Charniak’s automatic parser (Charniak 2000). For gold-standard parse trees, we preprocess the trees to discard empty constituents and strip functional tags. Using the trace information provided by empty constituents is very useful for improving performance (Palmer, Gildea, and Kingsbury 2005; Pradhan, Ward et al. 2005), but we have not used this information so that we can compare our results to previous work and since automatic systems that recover it are not widely available. 3.2 Evaluation Measures Since 2004, there has been a precise, standard evaluation measure for semantic role labeling, formulated by the organizers of the CoNLL s"
J08-2002,P05-1022,0,0.0428178,"f forward quotes.8 We ﬁrst present results of our local and joint model using the parses provided as part of the CoNLL 2005 data (and having wrong forward quotes) in Figure 18. We then report results from the same local and joint model, and the joint model using the top ﬁve Charniak parses, where the parses have correct representation of the forward quotes in Figure 19. For these results we used the version of the Charniak parser from 4 May 2005. The results were very similar to the results we obtained with the version from 18 March 2005. We did not experiment with the new re-ranking model of Charniak and Johnson (2005), even though it improves upon Charniak (2000) signiﬁcantly. For comparison, the system we submitted to CoNLL 2005 had an F-Measure of 78.45 on the WSJ Test set. The winning system (Punyakanok, Roth, and Yih 2005) had an F-Measure of 79.44 and our current system has an F-Measure of 80.32. For the Brown Test set, our submitted version had an F-Measure of 67.71, the winning system had 67.75, and our current system has 68.81. Figure 20 shows the per-label performance of our joint model using the top ﬁve Charniak parse trees on the Test WSJ test set. The columns show the Precision, Recall, F-Measu"
J08-2002,W05-0622,0,0.524278,"this role. We propose such a model, with a very rich graphical model structure, which is globally conditioned on the observation (the parse tree).2 Such a model is formally a Conditional Random Field (CRF) (Lafferty, McCallum, and Pereira 2001). However, note that in practice this term has previously been used almost exclusively to describe the restricted case of linear chain Conditional Markov Random Fields (sequence models) (Lafferty, McCallum, and Pereira 2001; Sha and Pereira 2003), or at least models that have strong Markov properties, which allow efﬁcient dynamic programming algorithms (Cohn and Blunsom 2005). Instead, we consider a densely connected CRF structure, with no Markov properties, and use approximate inference by re-ranking the n-best solutions of a simpler model with stronger independence assumptions (for which exact inference is possible). Such a rich graphical model can represent many dependencies but there are two dangers—one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohibitive, and the other is that if too many dependencies are encoded, the model will over-ﬁt the training data and will not generalize"
J08-2002,W06-1673,1,0.810223,"Missing"
J08-2002,J02-3001,0,0.938356,"y motivated dependencies in features over sets of the random variables. Our re-ranking approach, like the approach to parse re-ranking of Collins (2000), employs a simpler model—a local semantic role labeling algorithm—as a ﬁrst pass to generate a set of n likely complete assignments of labels to all parse tree nodes. The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings. 2. Related Work There has been a substantial amount of work on automatic semantic role labeling, starting with the statistical model of Gildea and Jurafsky (2002). Researchers have worked on deﬁning new useful features, and different system architectures and models. Here we review the work most closely related to ours, concentrating on methods for incorporating joint information and for increasing robustness to parser error. 2 That is, it deﬁnes a conditional distribution of labels of all nodes given the parse tree. 163 Computational Linguistics Volume 34, Number 2 2.1 Methods for Incorporating Joint Information Gildea and Jurafsky (2002) propose a method to model global dependencies by including a probability distribution over multi-sets of semantic r"
J08-2002,W05-0623,1,0.666119,"Missing"
J08-2002,P04-1042,1,0.876043,"Missing"
J08-2002,H05-1081,0,0.104916,"Missing"
J08-2002,J05-1004,0,0.561776,"Missing"
J08-2002,N04-1030,0,0.662826,"We use log-linear models for multi-class classiﬁcation for the local models. Because they produce probability distributions, identiﬁcation and classiﬁcation models can be chained in a principled way, as in Equation (1). The baseline features we used for the local identiﬁcation and classiﬁcation models are outlined in Figure 3. These features are a subset of the features used in previous work. The standard features at the top of the ﬁgure were deﬁned by Gildea and Jurafsky (2002), and the rest are other useful lexical and structural features identiﬁed in more recent work (Surdeanu et al. 2003; Pradhan et al. 2004; Xue and Palmer 2004). We also incorporated several novel features which we describe next. Figure 3 Baseline features. 172 Toutanova, Haghighi, and Manning A Global Joint Model for SRL Figure 4 Example of displaced arguments. 4.1 Additional Features for Displaced Constituents We found that a large source of errors for ARG0 and ARG1 stemmed from cases such as those illustrated in Figure 4, where arguments were dislocated by raising or control verbs. Here, the predicate, expected, does not have a subject in the typical position— indicated by the empty NP—because the auxiliary is has raised the"
J08-2002,P05-1072,0,0.0975432,"guments. For instance, the dependency between the meal and the children for the sentence in example (4) will not be captured because these phrases are not in the same local tree according to Penn Treebank syntax. 2.2 Increasing Robustness to Parser Error There have been multiple approaches to reducing the sensitivity of semantic role labeling systems to syntactic parser error. Promising approaches have been to consider multiple syntactic analyses—the top k parses from a single or multiple full parsers (Punyakanok, Roth, and Yih 2005), or a shallow parse and a full parse (M`arquez et al. 2005; Pradhan et al. 2005), or several types of full syntactic parses (Pradhan, Ward et al. 2005). Such techniques are important for achieving good performance: The top four systems in the CoNLL 2005 shared task competition all used multiple syntactic analyses (Carreras and M`arquez 2005). These previous methods develop special components to combine the labeling decisions obtained using different syntactic annotation. The method of Punyakanok, Roth, and Yih (2005) uses ILP to derive a consistent set of arguments, each of which could be derived using a different parse tree. Pradhan, Ward et al. (2005) use stacking to tr"
J08-2002,W05-0639,0,0.0394384,"Missing"
J08-2002,W04-2421,0,0.290116,"ns over a baseline system using only the features of Gildea and Jurafsky (2002). The performance gain due to joint information over a system using all features was not reported. The joint information captured by this model is limited by the n-gram Markov assumption of the language model over labels. In our work, we improve the modeling of joint dependencies by looking at longer-distance context, by deﬁning richer features over the sequence of labels and input features, and by estimating the model parameters discriminatively. A system which can integrate longer-distance dependencies is that of Punyakanok et al. (2004) and Punyakanok, Roth, and Yih (2005). The idea is to build a semantic role labeling system that is based on local classiﬁers but also uses a global component that ensures that several linguistically motivated global constraints on argument frames are satisﬁed. The constraints are categorical and speciﬁed by hand. For example, one global constraint is that the argument phrases cannot overlap—that is, if a node is labeled with a non-NONE label, all of its descendants have to be labeled NONE. The proposed framework is integer linear programming (ILP), which makes it possible to ﬁnd the most like"
J08-2002,N03-1028,0,0.0262055,". To estimate the probability that a certain node gets the role AGENT, we need to know if any of the other nodes were labeled with this role. We propose such a model, with a very rich graphical model structure, which is globally conditioned on the observation (the parse tree).2 Such a model is formally a Conditional Random Field (CRF) (Lafferty, McCallum, and Pereira 2001). However, note that in practice this term has previously been used almost exclusively to describe the restricted case of linear chain Conditional Markov Random Fields (sequence models) (Lafferty, McCallum, and Pereira 2001; Sha and Pereira 2003), or at least models that have strong Markov properties, which allow efﬁcient dynamic programming algorithms (Cohn and Blunsom 2005). Instead, we consider a densely connected CRF structure, with no Markov properties, and use approximate inference by re-ranking the n-best solutions of a simpler model with stronger independence assumptions (for which exact inference is possible). Such a rich graphical model can represent many dependencies but there are two dangers—one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohi"
J08-2002,P03-1002,0,0.297495,"izer of Equation (1). We use log-linear models for multi-class classiﬁcation for the local models. Because they produce probability distributions, identiﬁcation and classiﬁcation models can be chained in a principled way, as in Equation (1). The baseline features we used for the local identiﬁcation and classiﬁcation models are outlined in Figure 3. These features are a subset of the features used in previous work. The standard features at the top of the ﬁgure were deﬁned by Gildea and Jurafsky (2002), and the rest are other useful lexical and structural features identiﬁed in more recent work (Surdeanu et al. 2003; Pradhan et al. 2004; Xue and Palmer 2004). We also incorporated several novel features which we describe next. Figure 3 Baseline features. 172 Toutanova, Haghighi, and Manning A Global Joint Model for SRL Figure 4 Example of displaced arguments. 4.1 Additional Features for Displaced Constituents We found that a large source of errors for ARG0 and ARG1 stemmed from cases such as those illustrated in Figure 4, where arguments were dislocated by raising or control verbs. Here, the predicate, expected, does not have a subject in the typical position— indicated by the empty NP—because the auxilia"
J08-2002,W04-3212,0,0.834943,"They can be obtained by ﬁrst mapping all non-core argument labels in the guessed and correct labelings to NONE. Coarse Modiﬁer Argument Measures (C OARSE ARGM). Sometimes it is sufﬁcient to know a given span has a modiﬁer role, without knowledge of the speciﬁc role label. In addition, deciding exact modiﬁer argument labels was one of the decisions with highest disagreement among annotators (Palmer, Gildea, and Kingsbury 2005). To estimate performance under this setting, we relabel all ARGM - X arguments to ARGM in the proposed and correct labeling. Such a performance measure was also used by Xue and Palmer (2004). Note that these measures do not exclude the core arguments but instead consider the core plus a coarse version of the modiﬁer arguments. Thus for C OARSE ARGM 169 Computational Linguistics Volume 34, Number 2 ALL we count {0} as a true positive span, {1, 2} , {3, 4}, and {7, 8, 9} as false positive, and {1, 2, 3, 4} and {7, 8, 9} as false negative. Identiﬁcation Measures (I D). These measure how well we do on the ARG vs. NONE distinction. For the purposes of this evaluation, all spans labeled with a non-NONE label are considered to have the generic label ARG. For example, to compute CORE I D"
J08-2002,C98-1013,0,\N,Missing
J12-2003,J96-1002,0,0.0251094,"Missing"
J12-2003,de-marneffe-etal-2006-generating,1,0.142503,"Missing"
J12-2003,W09-3012,0,0.274288,"Missing"
J12-2003,W09-1401,0,0.0233968,"e new annotations help conﬁrm our hypothesis that veridicality judgments are shaped by a variety of other linguistic and contextual factors beyond lexical meanings. The nature of such cues is central to linguistic pragmatics and fundamental to a range of natural language processing (NLP) tasks, including information extraction, opinion detection, and textual entailment. Veridicality is prominent in BioNLP, where identifying negations (Chapman et al. 2001; Elkin et al. 2005; Huang and Lowe 2007; Pyysalo et al. 2007; Morante and Daelemans 2009) and hedges or “speculations” (Szarvas et al. 2008; Kim et al. 2009) is crucial to proper textual understanding. Recently, more attention has been devoted to veridicality within NLP, with the 2010 workshop on negation and speculation in natural language processing (Morante and Sporleder 2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al. 2010), where the goal was to distinguish uncertain events from the rest. The centrality of veridicality assessment to tasks like event and relation extraction is arguably still not fully appreciated, however. At present the vast majority of information extraction systems work at roughly the c"
J12-2003,P03-1054,1,0.0286283,"on the training set. Predicate classes. Saur´ı (2008) deﬁnes classes of predicates (nouns and verbs) that project the same veridicality value onto the events they introduce. The classes also deﬁne the grammatical relations that need to hold between the predicate and the event it introduces, because grammatical contexts matter for veridicality. Different veridicality values will indeed be assigned to X in He doesn’t know that X and in He doesn’t know if X. The classes have names like ANNOUNCE, CONFIRM, CONJECTURE, and SAY. Like Saur´ı, we used dependency graphs produced by the Stanford parser (Klein and Manning 2003; de Marneffe, MacCartney, and Manning 2006) to follow the path from the target event to the root of the sentence. If a predicate in the path was contained in one of the classes and the grammatical relation matched, we added both the lemma of the predicate as a feature and a feature marking the predicate class. 6 The maximum entropy formulation differs from the standard multi-class logistic regression model by having a parameter value for each class giving logit terms for how a feature’s value affects the outcome probability relative to a zero feature, whereas in the standard multi-class logis"
J12-2003,N03-5008,1,0.65703,"o give the log-likelihood for the exact distribution from the Turkers (which thus gives an upper-bound) as well as a log-likelihood for a baseline model which uses only the overall distribution of classes in the training data. A maximum entropy classiﬁer is an instance of a generalized linear model with a logit link function. It is almost exactly equivalent to the standard multi-class (also called polytomous or multinomial) logistic regression model from statistics, and readers more familiar with this presentation can think of it as such. In all our experiments, we use the Stanford Classiﬁer (Manning and Klein 2003) with a Gaussian prior (also known as L2 regularization) set to N (0, 1).6 4.1 Features The features were selected through 10-fold cross-validation on the training set. Predicate classes. Saur´ı (2008) deﬁnes classes of predicates (nouns and verbs) that project the same veridicality value onto the events they introduce. The classes also deﬁne the grammatical relations that need to hold between the predicate and the event it introduces, because grammatical contexts matter for veridicality. Different veridicality values will indeed be assigned to X in He doesn’t know that X and in He doesn’t kno"
J12-2003,W09-1105,0,0.0316776,"ghlights the sort of vagueness and ambiguity that can affect veridicality. These new annotations help conﬁrm our hypothesis that veridicality judgments are shaped by a variety of other linguistic and contextual factors beyond lexical meanings. The nature of such cues is central to linguistic pragmatics and fundamental to a range of natural language processing (NLP) tasks, including information extraction, opinion detection, and textual entailment. Veridicality is prominent in BioNLP, where identifying negations (Chapman et al. 2001; Elkin et al. 2005; Huang and Lowe 2007; Pyysalo et al. 2007; Morante and Daelemans 2009) and hedges or “speculations” (Szarvas et al. 2008; Kim et al. 2009) is crucial to proper textual understanding. Recently, more attention has been devoted to veridicality within NLP, with the 2010 workshop on negation and speculation in natural language processing (Morante and Sporleder 2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al. 2010), where the goal was to distinguish uncertain events from the rest. The centrality of veridicality assessment to tasks like event and relation extraction is arguably still not fully appreciated, however. At present the v"
J12-2003,C10-2117,0,0.178904,"Missing"
J12-2003,N07-2036,0,0.011388,"ilding 460, Stanford CA 94305, USA. E-mail: cgpotts@stanford.edu. Submission received: 4 April 2011; revised submission received: 1 October 2011; accepted for publication: 30 November 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 2 call this event veridicality, building on logical, linguistic, and computational insights about the relationship between language and reader commitment (Montague 1969; Barwise 1981; Giannakidou 1994, 1995, 1999, 2001; Zwarts 1995; Asher and Lascarides 2003; Karttunen and Zaenen 2005; Rubin, Liddy, and Kando 2005; Rubin 2007; Saur´ı 2008). The central goal of this article is to begin to identify the linguistic and contextual factors that shape readers’ veridicality judgments.1 There is a long tradition of tracing veridicality to ﬁxed properties of lexical items (Kiparsky and Kiparsky 1970; Karttunen 1973). On this view, a lexical item L is veridical if the meaning of L applied to argument p entails the truth of p. For example, because both true and false things can be believed, one should not infer directly from A believes that S that S is true, making believe non-veridical. Conversely, realize appears to be veri"
J12-2003,D08-1027,0,0.120747,"Missing"
J12-2003,W08-0606,0,0.0142166,"ct veridicality. These new annotations help conﬁrm our hypothesis that veridicality judgments are shaped by a variety of other linguistic and contextual factors beyond lexical meanings. The nature of such cues is central to linguistic pragmatics and fundamental to a range of natural language processing (NLP) tasks, including information extraction, opinion detection, and textual entailment. Veridicality is prominent in BioNLP, where identifying negations (Chapman et al. 2001; Elkin et al. 2005; Huang and Lowe 2007; Pyysalo et al. 2007; Morante and Daelemans 2009) and hedges or “speculations” (Szarvas et al. 2008; Kim et al. 2009) is crucial to proper textual understanding. Recently, more attention has been devoted to veridicality within NLP, with the 2010 workshop on negation and speculation in natural language processing (Morante and Sporleder 2010). Veridicality was also at the heart of the 2010 CoNLL shared task (Farkas et al. 2010), where the goal was to distinguish uncertain events from the rest. The centrality of veridicality assessment to tasks like event and relation extraction is arguably still not fully appreciated, however. At present the vast majority of information extraction systems wor"
J12-2003,W05-1206,0,0.155297,"Missing"
J12-2003,W10-3001,0,\N,Missing
J12-2003,W07-1401,0,\N,Missing
J12-2003,W10-3100,0,\N,Missing
J13-1009,C88-1002,0,0.722196,"Missing"
J13-1009,E89-1001,0,0.607443,"cal lexical features. Arabic MWE Identification. Very little prior work exists on Arabic MWE identification. Attia (2006) demonstrated a method for integrating MWE knowledge into a lexical-functional grammar, but gave no experimental results. Siham Boulaknadel and Aboutajdine (2008) evaluated several lexical association measures in isolation for MWE identification in newswire. More recently, Attia et al. (2010b) compared crosslingual projection methods (using Wikipedia and English Wordnet) with standard n-gram classification methods. French Statistical Constituency Parsing. Abeillé (1988) and Abeillé and Schabes (1989) identified the linguistic and computational attractiveness of lexicalized grammars for modeling non-compositional constructions in French well before DOP. They developed a small tree adjoining grammar (TAG) of 1,200 elementary trees and 4,000 lexical items 219 Computational Linguistics Volume 39, Number 1 that included MWEs. Recent statistical parsing work on French has included stochastic tree insertion grammars (STIG), which are related to TAGs, but with a restricted adjunction operation.23 Both Seddah, Candito, and Crabbé (2009) and Seddah (2010) showed that STIGs underperform CFG-based pa"
J13-1009,P05-1038,0,0.651979,"(Scha 1990, page 13). Most DOP work, however, has focused on parameter estimation issues with a view to improving overall parsing performance rather than explicit modeling of idioms. Given developments in linguistics, and to a lesser degree DOP, in modeling MWEs, it is curious that most NLP work on MWE identification has not utilized syntax. Moreover, the words-with-spaces idea, which Sag et al. (2002) dismissed as unattractive on both theoretical and computational grounds,22 has continued to appear in NLP evaluations such as dependency parsing (Nivre and Nilsson 2004), constituency parsing (Arun and Keller 2005), and shallow parsing (Korkontzelos and Manandhar 2010). In all cases, the conclusion was drawn that pre-grouping MWEs improves task accuracy. Because the yields (and thus the labelings) of the evaluation sentences were modified, however, the experiments were not strictly comparable. Moreover, gold pre-grouping was usually assumed, as was the case in most FTB parsing evaluations after Arun and Keller (2005). The words-with-spaces strategy is especially unattractive for MRLs because (1) it intensifies the sparsity problem in the lexicon; and (2) it is not robust to morphological and syntactic p"
J13-1009,W10-1408,0,0.0530816,"Missing"
J13-1009,P10-1112,0,0.018634,"f t in the treebank. Lexicalizing a rule makes it very specific, so we generally want to avoid lexicalization with rare words. Empirically, we found that this penalty reduces overfitting. Type-Based Inference Algorithm. To learn the parameters θ we use the collapsed, block Gibbs sampler of Liang, Jordan, and Klein (2010). We sample binary variables bs associated with each sampling site s in the treebank. The key idea is to select a block 6 The Stanford parser is a product model which scores parses with both a dependency grammar and a PCFG. We extract the TSG from the manually split PCFG only. Bansal and Klein (2010) also experimented with manual grammar features in an all-fragments (parametric) TSG for English. 206 Green, de Marneffe, and Manning Parsing Models for Identifying Multiword Expressions NP+ PUNC− (1) N+ N− PUNC+ (2) “ Jacques Chirac “ Figure 2 Example of two conflicting sites of the same type in a training tree. Define the type of a def site t(z, s) = (Δns:0 , Δns:1 ). Sites (1) and (2) have the same type because t(z, s1 ) = t(z, s2 ). The two sites conflict, however, because the probabilities of setting bs1 and bs2 both depend on counts for the tree fragment rooted at NP. Consequently, sites"
J13-1009,J04-4004,0,0.0265214,"cy arcs or bracketed spans), thus the results are not strictly comparable. From an application perspective, pre-grouping assumes high accuracy identification, which may not be available for all languages. Our goal differs considerably from these two studies, which attempt to improve parsing via MWE information. In contrast, we tune statistical parsers for MWE identification. 8.3 Related Experiments on Arabic and French Arabic Statistical Constituency Parsing. Kulick, Gabbard, and Marcus (2006) were the first to parse the sections of the ATB used in this article. They adapted the Bikel parser (Bikel 2004) and improved accuracy primarily through punctuation equivalence classing and the Kulick tag set. The ATB was subsequently revised (Maamouri, Bies, and Kulick 2008), and Maamouri, Bies, and Kulick (2009) produced the first results on the revision for our split of the revised corpus. They only reported development set results with gold POS tags, however. Petrov (2009) adapted the Berkeley parser to the ATB, and we later provided a parameterization that dramatically improved his baseline (Green and Manning 2010). We also adapted the Stanford parser to the ATB, and provided the first results for"
J13-1009,N03-2002,0,0.0134091,"ntence), or consists entirely of capital letters  If none of the above, deterministically extract one- and two-character suffixes 203 Computational Linguistics Volume 39, Number 1 3.4 Factored Lexicon with Morphological Features We will apply our models to Arabic and French, yet we have not dealt with the lexical sparsity induced by rich morphology (see Table 5 for a comparison to English). One way to combat sparsity is to parse a factored representation of the terminals, where factors might be the word form, the lemma, or grammatical features such as gender, number, and person (φ features) (Bilmes and Kirchoff 2003; Koehn and Hoang 2007, inter alia). The basic parser lexicon estimates the generative probability of a word given a tag p(w|t) from word/tag pairs observed in the training set. Additionally, the lexicon includes parameter estimates p(t|s) for unknown word signatures s produced by the unknown word models (see Section 3.3). At parsing time, the lexicon scores each input word type w according to its observed count in the training set c(w). We define the unsmoothed and smoothed parameter estimates: p(t|w) = psmooth (t|w) = c(t, w) c(w) c(t, w) + αp(t|s) c(w) + α (1) (2) We then compute the desire"
J13-1009,W06-1620,0,0.159429,"of Computer Science and Linguistics. E-mail: manning@stanford.edu. Submission received: October 1, 2011; revised submission received: June 9, 2012; accepted for publication: August 3, 2012. No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 1 including dependency parsing (Nivre and Nilsson 2004), supertagging (Blunsom and Baldwin 2006), sentence generation (Hogan et al. 2007), machine translation (Carpuat and Diab 2010), and shallow parsing (Korkontzelos and Manandhar 2010). The standard approach to MWE identification is n-gram classification. This technique is simple. Given a corpus, all n-grams are extracted, filtered using heuristics, and assigned feature vectors. Each coordinate in the feature vector is a real-valued quantity such as log likelihood or pointwise mutual information. A binary classifier is then trained to render a MWE/non-MWE decision. All entries into the 2008 MWE Shared Task (Evert 2008) utilized variant"
J13-1009,C92-3126,0,0.262763,"is of the transitive verb kicked and its two arguments. TSGs are weakly equivalent to CFGs, but with greater strong generative capacity (Joshi and Schabes 1997). TSGs can store lexicalized tree fragments as rules. Consequently, if we have seen [MWV kicked the bucket] several times before, we can store that whole lexicalized fragment in the grammar. We consider the non-parametric probabilistic TSG (PTSG) model of Cohn, Goldwater, and Blunsom (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a data-oriented parsing (DOP) model (Scha 1990; Bod 1992) with Bayesian parameter estimation. A PTSG is a 5-tuple V, Σ, R, ♦, θ where c ∈ V are non-terminals; t ∈ Σ are terminals; e ∈ R are elementary trees;5 ♦ ∈ V is a unique start symbol; and θc,e ∈ θ are parameters for each tree fragment. A PTSG derivation is created by successively applying the substitution operator to the leftmost frontier node (denoted by c+ ). All other nodes are internal (denoted by c− ). In the supervised setting, DP-TSG grammar extraction reduces to a segmentation problem. We have a treebank T that we segment into the set R, a process that is modeled with Bayes’ rule: p("
J13-1009,W09-3821,0,0.0245326,"(2) E XP, in which MWEs were marked with a flat structure. For both representations, they also gave results in which coordinated phrase structures were flattened. In the published experiments, they mistakenly removed half of the corpus, believing that the multi-terminal (per POS tag) annotations of MWEs were XML errors (Schluter and van Genabith 2007). MFT. (Schluter and van Genabith 2007) Manual revision to 3,800 sentences. Major changes included coordination raising, an expanded POS tag set, and the correction of annotation errors. Like A RUN -C ONT, MFT contains concatenated MWEs. FTB-UC. (Candito and Crabbé 2009) A version of the functionally annotated section that makes a distinction between MWEs that are “syntactically regular” and those that are not. Syntactically regular MWEs were given internal structure, whereas all other MWEs were grouped. For example, nouns followed by adjectives, such as loi agraire (‘land law’) or Union monétaire et économique (‘monetary and economic Union’) were considered syntactically regular. They are MWEs because the choice of adjective is arbitrary (loi agraire and not ∗ loi agricole, similarly to (‘coal black’) but not (∗ ‘crow black’), for example), but their syntact"
J13-1009,candito-etal-2010-statistical,0,0.0732072,"Missing"
J13-1009,W10-1409,0,0.0379731,"Missing"
J13-1009,N10-1029,0,0.0192466,"October 1, 2011; revised submission received: June 9, 2012; accepted for publication: August 3, 2012. No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 1 including dependency parsing (Nivre and Nilsson 2004), supertagging (Blunsom and Baldwin 2006), sentence generation (Hogan et al. 2007), machine translation (Carpuat and Diab 2010), and shallow parsing (Korkontzelos and Manandhar 2010). The standard approach to MWE identification is n-gram classification. This technique is simple. Given a corpus, all n-grams are extracted, filtered using heuristics, and assigned feature vectors. Each coordinate in the feature vector is a real-valued quantity such as log likelihood or pointwise mutual information. A binary classifier is then trained to render a MWE/non-MWE decision. All entries into the 2008 MWE Shared Task (Evert 2008) utilized variants of this technique. Broadly speaking, n-gram classification methods measure word co-o"
J13-1009,N09-1062,0,0.0399445,"Missing"
J13-1009,P12-1022,0,0.505035,"Missing"
J13-1009,constant-tellier-2012-evaluating,0,0.131555,"Missing"
J13-1009,P10-4002,0,0.0452481,"Missing"
J13-1009,N09-1037,1,0.544537,"Missing"
J13-1009,D11-1067,1,0.823799,"Missing"
J13-1009,C10-1045,1,0.134693,"the first to parse the sections of the ATB used in this article. They adapted the Bikel parser (Bikel 2004) and improved accuracy primarily through punctuation equivalence classing and the Kulick tag set. The ATB was subsequently revised (Maamouri, Bies, and Kulick 2008), and Maamouri, Bies, and Kulick (2009) produced the first results on the revision for our split of the revised corpus. They only reported development set results with gold POS tags, however. Petrov (2009) adapted the Berkeley parser to the ATB, and we later provided a parameterization that dramatically improved his baseline (Green and Manning 2010). We also adapted the Stanford parser to the ATB, and provided the first results for non-gold tokenization. Attia et al. (2010a) developed an Arabic unknown word model for the Berkeley parser based on signatures, much like those in Table 3. More recently, Huang and Harper (2011) presented a discriminative lexical model for Arabic that can encode arbitrary local lexical features. Arabic MWE Identification. Very little prior work exists on Arabic MWE identification. Attia (2006) demonstrated a method for integrating MWE knowledge into a lexical-functional grammar, but gave no experimental result"
J13-1009,P84-1058,0,0.718277,"bank. Dependency representations may be more appropriate for these idiom classes. 2.3 French MWEs In French, there is a lexicographic tradition of compiling MWE lists. For example, Gross (1986) shows that whereas French dictionaries contain about 1,500 single-word adverbs there are over 5,000 multiword adverbs. MWEs occur in every part of speech (POS) category (e.g., noun trousse de secours (‘first-aid kit’); verb faire main-basse [do hand-low] (‘seize’); adverb comme dans du beurre [as in butter] (‘easily’); adjective à part entière (‘wholly’)). Motivated by the prevalence of MWEs in French, Gross (1984) developed a linguistic theory known as Lexicon-Grammar. In this theory, MWEs are classified according to their global POS tags (noun, verb, adverb, adjective), and described in terms of the sequence of the POS tags of the words that constitute the MWE (e.g., “N de N” garde d’enfant [guard of child] (‘daycare’), pied de guerre [foot of war] (‘at the ready’)) (Gross 1986). In other words, MWEs are represented by a flat structure. The Lexicon-Grammar distinguishes between units that are fixed and have to appear as is (en tout et pour tout [in all and for all] (‘in total’)) and units that accept"
J13-1009,C86-1001,0,0.800946,"iqa (‘neighborly’) D+A C+D+A:  !1  .   D+A C+D+A:  1  3 al-bariia w-al-baHariia (‘land and sea’) A D+N: These idiom types usually do not cross constituent boundaries, so constituency parsers are well suited for modeling them. The other three classes of Ashraf (2012)—verbsubject, verbal, and adverbial—tend to cross constituent boundaries, so they are difficult to represent in a PTB-style treebank. Dependency representations may be more appropriate for these idiom classes. 2.3 French MWEs In French, there is a lexicographic tradition of compiling MWE lists. For example, Gross (1986) shows that whereas French dictionaries contain about 1,500 single-word adverbs there are over 5,000 multiword adverbs. MWEs occur in every part of speech (POS) category (e.g., noun trousse de secours (‘first-aid kit’); verb faire main-basse [do hand-low] (‘seize’); adverb comme dans du beurre [as in butter] (‘easily’); adjective à part entière (‘wholly’)). Motivated by the prevalence of MWEs in French, Gross (1984) developed a linguistic theory known as Lexicon-Grammar. In this theory, MWEs are classified according to their global POS tags (noun, verb, adverb, adjective), and described in ter"
J13-1009,P05-1071,0,0.0380349,"Missing"
J13-1009,D07-1028,0,0.110528,"Missing"
J13-1009,I11-1025,0,0.0176747,"aamouri, Bies, and Kulick (2009) produced the first results on the revision for our split of the revised corpus. They only reported development set results with gold POS tags, however. Petrov (2009) adapted the Berkeley parser to the ATB, and we later provided a parameterization that dramatically improved his baseline (Green and Manning 2010). We also adapted the Stanford parser to the ATB, and provided the first results for non-gold tokenization. Attia et al. (2010a) developed an Arabic unknown word model for the Berkeley parser based on signatures, much like those in Table 3. More recently, Huang and Harper (2011) presented a discriminative lexical model for Arabic that can encode arbitrary local lexical features. Arabic MWE Identification. Very little prior work exists on Arabic MWE identification. Attia (2006) demonstrated a method for integrating MWE knowledge into a lexical-functional grammar, but gave no experimental results. Siham Boulaknadel and Aboutajdine (2008) evaluated several lexical association measures in isolation for MWE identification in newswire. More recently, Attia et al. (2010b) compared crosslingual projection methods (using Wikipedia and English Wordnet) with standard n-gram cla"
J13-1009,J98-4004,0,0.124524,"supervised parsing models for Arabic and French. Now we show how to construct MWE-aware training resources for them. The corpora used in our experiments are the Penn Arabic Treebank (ATB) (Maamouri et al. 2004) and the French Treebank (FTB) (Abeillé, Clément, and Kinyon 2003). Prior to parsing, both treebanks require significant pre-processing, which we perform automatically.8 Because parsing evaluation metrics are sensitive to the terminal/non-terminal ratio (Rehbein and van Genabith 2007), we only remove nonterminal labels in the case of unary rewrites of the same category (e.g., NP → NP) (Johnson 1998). Table 5 compares the pre-processed corpora with the WSJ section of the PTB. Appendix C compares the annotation consistency of the ATB, FTB, and WSJ. 5.1 Arabic Treebank We work with parts 1–3 (newswire) of the ATB,9 which contain documents from three different news agencies. In addition to phrase structure markers, each syntactic tree also contains per-token morphological analyses. 8 Tree manipulation is automated with Tregex/Tsurgeon (Levy and Andrew 2006). Our pre-processing package is available at http://nlp.stanford.edu/software/lex-parser.shtml. 9 LDC catalog numbers: LDC2008E61, LDC200"
J13-1009,P03-1054,1,0.0692462,"es of part of speech than parts of speech. Surface statistics may erroneously predict that only the former is an MWE and the latter is not. More worrisome is that the statistics for the two n-grams are separate, thus missing an obvious generalization. In this article, we show that statistical parsing models generalize more effectively over arbitrary-length multiword expressions. This approach has not been previously demonstrated. To show its effectiveness, we build two parsing models for MWE identification. The first model is based on a context-free grammar (CFG) with manual rule refinements (Klein and Manning 2003). This parser also includes a novel lexical model—the factored lexicon—that incorporates morphological features. The second model is based on tree substitution grammar (TSG), a formalism with greater strong generative capacity that can store larger structural tree fragments, some of which are lexicalized. We apply the models to Modern Standard Arabic (henceforth MSA, or simply “Arabic”) and French, two morphologically rich languages (MRLs). The lexical sparsity (in finite corpora) induced by rich morphology poses a particular challenge for n-gram classification. Relative to English, French has"
J13-1009,D07-1091,0,0.0174518,"ely of capital letters  If none of the above, deterministically extract one- and two-character suffixes 203 Computational Linguistics Volume 39, Number 1 3.4 Factored Lexicon with Morphological Features We will apply our models to Arabic and French, yet we have not dealt with the lexical sparsity induced by rich morphology (see Table 5 for a comparison to English). One way to combat sparsity is to parse a factored representation of the terminals, where factors might be the word form, the lemma, or grammatical features such as gender, number, and person (φ features) (Bilmes and Kirchoff 2003; Koehn and Hoang 2007, inter alia). The basic parser lexicon estimates the generative probability of a word given a tag p(w|t) from word/tag pairs observed in the training set. Additionally, the lexicon includes parameter estimates p(t|s) for unknown word signatures s produced by the unknown word models (see Section 3.3). At parsing time, the lexicon scores each input word type w according to its observed count in the training set c(w). We define the unsmoothed and smoothed parameter estimates: p(t|w) = psmooth (t|w) = c(t, w) c(w) c(t, w) + αp(t|s) c(w) + α (1) (2) We then compute the desired parameter p(w|t) as"
J13-1009,N10-1089,0,0.434284,"June 9, 2012; accepted for publication: August 3, 2012. No rights reserved. This work was authored as part of the Contributor’s official duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 1 including dependency parsing (Nivre and Nilsson 2004), supertagging (Blunsom and Baldwin 2006), sentence generation (Hogan et al. 2007), machine translation (Carpuat and Diab 2010), and shallow parsing (Korkontzelos and Manandhar 2010). The standard approach to MWE identification is n-gram classification. This technique is simple. Given a corpus, all n-grams are extracted, filtered using heuristics, and assigned feature vectors. Each coordinate in the feature vector is a real-valued quantity such as log likelihood or pointwise mutual information. A binary classifier is then trained to render a MWE/non-MWE decision. All entries into the 2008 MWE Shared Task (Evert 2008) utilized variants of this technique. Broadly speaking, n-gram classification methods measure word co-occurrence. Suppose that a corpus contains more occurren"
J13-1009,P11-2122,0,0.0448892,"Missing"
J13-1009,levy-andrew-2006-tregex,0,0.0089729,"Missing"
J13-1009,P03-1056,1,0.271894,"s a particular challenge for n-gram classification. Relative to English, French has a richer array of morphological features— such as grammatical gender and verbal conjugation for aspect and voice. Arabic also has richer morphology including gender and dual number. It has pervasive verbinitial matrix clauses, although preposed subjects are also possible. For languages like these it is well known that constituency parsing models designed for English often do not generalize well. Therefore, we focus on the interplay among language, annotation choices, and parsing model design for each language (Levy and Manning 2003; Kübler 2005, inter alia), although our methods are ultimately very general. Our modeling strategy for MWEs is simple: We mark them with flat bracketings in phrase structure trees. This representation implicitly assumes a locality constraint on idioms, an assumption with a precedent in linguistics (Marantz 1997, inter alia). Of course, it is easy to find non-local idioms that do not correspond to surface constituents or even contiguous strings (O’Grady 1998). Utterances such as All hell seemed to break loose and The cat got Mary’s tongue are clearly idiomatic, yet the idiomatic elements are d"
J13-1009,N10-1082,0,0.0233521,"Missing"
J13-1009,P99-1041,0,0.034676,"uations after Arun and Keller (2005). The words-with-spaces strategy is especially unattractive for MRLs because (1) it intensifies the sparsity problem in the lexicon; and (2) it is not robust to morphological and syntactic processes such as inflection and phrasal expansion. 8.2 Syntactic Methods for MWE Identification There is a voluminous literature on MWE identification, so we focus on syntaxbased methods. The classic statistical approach to MWE identification, Xtract (Smadja 1993), used an incremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically extracted dependency relationships to find MWEs. To our knowledge, 22 Sag et al. (2002) showed how to integrate MWE information into a non-probabilistic head-driven phrase structure grammar for English. 218 Green, de Marneffe, and Manning Parsing Models for Identifying Multiword Expressions Wehrli (2000) was the first to propose the use of a syntactic parser for multiword expression identification. No empirical results were provided, however, and the MWEaugmented scoring function for the output of his symbolic parser was left to future r"
J13-1009,maamouri-etal-2008-enhancing,0,0.0513029,"Missing"
J13-1009,J93-2004,0,0.0557089,"Missing"
J13-1009,P06-1055,0,0.215711,"s of z Change in counts by setting m sites in S Table 4 defines notation. The data likelihood is given by the latent state z and the  n (z) parameters θ: p(z|θ ) = z∈z θc,ec,e . Integrating out the parameters, we have: p(z) =   (αc P0 (e|c))nc,e (z) e c∈ V n (z) αc c,· (6) where xn = x(x + 1) . . . (x + n − 1) is the rising factorial. Base Distribution. The base distribution P0 is the same maximum likelihood PCFG used in the Stanford parser.6 After applying the manual grammar features, we perform simple right binarization, collapse unary rules, and replace rare words with their signatures (Petrov et al. 2006). For each non-terminal type c, we learn a stop probability qc ∼ Beta(1, 1). Under P0 , the probability of generating a tree fragment A+ → B− C+ composed of nonterminals is P0 (A+ → B− C+ ) = pMLE (A → B C)qB (1 − qC ) (7) Unlike Cohn, Goldwater, and Blunsom (2009), we penalize lexical insertion: P0 (c → t) = pMLE (c → t)p(t) (8) where p(t) is equal to the MLE unigram probability of t in the treebank. Lexicalizing a rule makes it very specific, so we generally want to avoid lexicalization with rare words. Empirically, we found that this penalty reduces overfitting. Type-Based Inference Algorit"
J13-1009,N07-1051,0,0.060662,"Missing"
J13-1009,P09-2012,0,0.0247629,"g, DP-TSG grammar extraction reduces to a segmentation problem. We have a treebank T that we segment into the set R, a process that is modeled with Bayes’ rule: p(R |T) ∝ p(T |R) p(R) (5) Because the tree fragments completely specify each tree, p(T |R) is either 0 or 1, so all work is performed by the prior over the set of elementary trees. The DP-TSG contains a DP prior for each c ∈ V and generates a tree fragment e rooted at non-terminal c according to: θc |c, αc , P0 (·|c) ∼ DP(αc , P0 ) e|θc ∼ θc 4 Similar models were developed independently by O’Donnell, Tenenbaum, and Goodman (2009) and Post and Gildea (2009). 5 We use the terms tree fragment and elementary tree interchangeably. 205 Computational Linguistics Volume 39, Number 1 Table 4 DP-TSG notation. For consistency, we largely follow the notation of Liang, Jordan, and Klein (2010). αc P0 (e|c) x S S b = {bs }s∈S z m n = {nc,e } ΔnS:m DP concentration parameter for each non-terminal type c ∈ V CFG base distribution Set of all non-terminal nodes in the treebank Set of sampling sites (one for each x ∈ x) A block of sampling sites, where S ⊆ S Binary variables to be sampled (bs = 1 for frontier nodes) Latent state of the segmented treebank Number o"
J13-1009,ramisch-etal-2010-mwetoolkit,0,0.0156972,"Missing"
J13-1009,D07-1066,0,0.0769972,"Missing"
J13-1009,seddah-2010-exploring,0,0.128531,"cy Parsing. Abeillé (1988) and Abeillé and Schabes (1989) identified the linguistic and computational attractiveness of lexicalized grammars for modeling non-compositional constructions in French well before DOP. They developed a small tree adjoining grammar (TAG) of 1,200 elementary trees and 4,000 lexical items 219 Computational Linguistics Volume 39, Number 1 that included MWEs. Recent statistical parsing work on French has included stochastic tree insertion grammars (STIG), which are related to TAGs, but with a restricted adjunction operation.23 Both Seddah, Candito, and Crabbé (2009) and Seddah (2010) showed that STIGs underperform CFG-based parsers on the FTB. In their experiments, MWEs were grouped. Appendix B describes additional prior work on CFG-based FTB parsing. French MWE Identification. Statistical French MWE identification has only been investigated recently. We previously reported the first results on the FTB using a parser for MWE identification (Green et al. 2011). Contemporaneously, Watrin and Francois (2011) applied n-gram methods to a French corpus of multiword adverbs (Laporte, Nakamura, and Voyatzi 2008). Constant and Tellier (2012) used a linear chain conditional random"
J13-1009,W10-1410,0,0.118491,"Missing"
J13-1009,boulaknadel-etal-2008-multi,0,0.106692,"Missing"
J13-1009,J93-1007,0,0.0327018,"ts were not strictly comparable. Moreover, gold pre-grouping was usually assumed, as was the case in most FTB parsing evaluations after Arun and Keller (2005). The words-with-spaces strategy is especially unattractive for MRLs because (1) it intensifies the sparsity problem in the lexicon; and (2) it is not robust to morphological and syntactic processes such as inflection and phrasal expansion. 8.2 Syntactic Methods for MWE Identification There is a voluminous literature on MWE identification, so we focus on syntaxbased methods. The classic statistical approach to MWE identification, Xtract (Smadja 1993), used an incremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically extracted dependency relationships to find MWEs. To our knowledge, 22 Sag et al. (2002) showed how to integrate MWE information into a non-probabilistic head-driven phrase structure grammar for English. 218 Green, de Marneffe, and Manning Parsing Models for Identifying Multiword Expressions Wehrli (2000) was the first to propose the use of a syntactic parser for multiword expression identification. No empirical results"
J13-1009,N03-1033,1,0.0158233,"Missing"
J13-1009,E93-1045,0,0.190071,"ree fragments have fixed estimates according to a single sample from the DP-TSG: θc,e = nc,e (z) + αc P0 (e|c) nc, (z) + αc (9) This MAP grammar has an infinite rule set, however, because elementary trees with zero count in n have some residual probability under P0 . We discard all zero-count trees except for the zero-count CFG rules in P0 . Scores for these rules follow from Equation (9) with nc,e (z) = 0. This grammar represents most of the probability mass and permits inference using dynamic programming (Cohn, Goldwater, and Blunsom 2009). Because the derivations of a TSG are context-free (Vijay-Shanker and Weir 1993), we can form a CFG of the derivation sequences and use a synchronous CFG to translate the most probable CFG parse to its TSG derivation. Consider a unique tree fragment ei rooted at cj with frontier γ, which is a sequence of terminals and non-terminals. We encode this fragment as an SCFG rule of the form [cj → γ , cj → i, ck , cl , . . . ] (10) where ck , cl , . . . is a finite-length sequence of the non-terminal frontier nodes in γ.7 The SCFG translates the input string to a sequence of tree fragment indices. Because the TSG substitution operator applies to the leftmost frontier node, the be"
J13-1009,W11-0813,0,0.0232652,"Missing"
J13-1009,chrupala-etal-2008-learning,0,\N,Missing
J13-1009,E06-1047,0,\N,Missing
J13-1009,W10-3704,0,\N,Missing
J15-4006,W14-2517,0,0.0213315,"Missing"
J15-4006,P15-1002,0,0.00708762,"aring that occurs within deep representations can theoretically give an exponential representational advantage, and, in practice, offers improved learning systems. The general approach to building Deep Learning systems is compelling and powerful: The researcher defines a model architecture and a top-level loss function and then both the parameters and the representations of the model self-organize so as to minimize this loss, in an end-to-end learning framework. We are starting to see the power of such deep systems in recent work in neural machine translation (Sutskever, Vinyals, and Le 2014; Luong et al. 2015). Finally, I have been an advocate for focusing more on compositionality in models, for language in particular, and for artificial intelligence in general. Intelligence requires being able to understand bigger things from knowing about smaller parts. In particular for language, understanding novel and complex sentences crucially depends on being able to construct their meaning compositionally from smaller parts—words and multiword expressions—of which they are constituted. Recently, there have been many, many papers showing how systems can be improved by using distributed word representations"
J15-4006,D14-1162,1,0.105989,"Missing"
K15-1031,P15-1002,1,0.900207,"to the first hidden layer. We then repeatedly build up hidden representations as follows, for l = 1, . . . , n:   (1) h(l) = f W (l) h(l−1) + b(l) (Devlin et al., 2014), we found that using the rectified linear function, max{0, x}, proposed in (Nair and Hinton, 2010), works better than tanh. The rectified linear function was used in (Vaswani et al., 2013) as well. Furthermore, while these works use a fixed learning rate throughout, we found that having a simple learning rate schedule is useful in training well-performing deep NLMs. This has also been demonstrated in (Sutskever et al., 2014; Luong et al., 2015) and is detailed in Section 3. We do not perform any gradient clipping and notice that learning is more stable when short sentences of length less than or equal to 2 are removed. Bias terms are used for all hidden layers as well as the softmax layer as described earlier, which is slightly different from other work such as (Vaswani et al., 2013). All these details contribute to our success in training deep NLMs. For simplicity, the same vocabulary is used for both the embedding and the softmax matrices.2 In addition, we adopt the standard softmax to take advantage of GPUs in performing large ma"
K15-1031,W12-2703,0,0.0229401,"achieve decent gains in using NLMs for MT domain adaptation. 1 layer 2 layers 3 layers 4 layers All 11.3 11.2 11.0 10.9 LMs + WP 11.3 11.4 11.1 11.2 LMs 11.4 11.5 11.4 11.3 Table 4: Reranking Conditions – (T ER-B LEU)/2 scores when reranking the web-forum baseline. 5 Related Work It is worth mentioning another active line of research in building end-to-end neural MT systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015). These methods have not yet demonstrated success on challenging language pairs such as English-Chinese. Arsoy et al. (2012) have preliminarily examined deep NLMs for speech recognition, however, we believe, this is the first work that puts deep NLMs into the context of MT. 4 Analysis 4.1 1 layer 2 layers 3 layers 4 layers 5.5 Cross−Entropy dev T↓ B↑ baseline 62.2 18.7 Reranking 1 layer (5.42, 0.51) 60.1 22.0 2 layers (5.50, 0.51) 60.7 21.5 3 layers (5.34 0.43) 59.9 21.4 vs. baseline +2.3‡ +3.3‡ vs. 1 layer +0.2 -0.6 System NLM Training We show in Figure 1 the learning curves for various NLMs, demonstrating that deep nets are better than the shallow NLM with a single hidden layer. Starting from minibatch 20K, the r"
K15-1031,N10-2003,1,0.83114,"isfically significant according to the test described in (Riezler and Maxwell, 2005). The fact that the improvements in terms of the intrinsic metrics listed in Table 1 do translate into gains in translation quality is interesting. It reinforces the trend reported in (Luong et al., 2015) that better source-conditioned perplexities lead to better translation scores. This phenomon is a useful result as in the past, many intrinsic metrics, e.g., alignment error rate, do not necessarily correlate with MT quality metrics. MT Reranking with NLMs Our MT models are built using the Phrasal MT toolkit (Cer et al., 2010). In addition to the standard dense feature set6 , we include a variety of sparse features for rules, word pairs, and word classes, as described in (Green et al., 2014). Our decoder uses three language models.7 We use a tuning set of 396K words in the newswire and web domains and tune our systems using online expected error rate training as in (Green et al., 2014). Our tuning metric is (B LEU-T ER)/2. We run a discriminative reranker on the 1000best output of a decoder with MERT. The features used in reranking include all the dense features, 5 As a reference point, though not directly comparab"
K15-1031,W05-0908,0,0.0342934,"atures. This second tuning set consists of 33K words of web-forum text and is important to obtain good improvements with reranking. 3.4 Results As shown in Table 2, it is not obvious if the depth2 model is better than the single layer one, both of which are what past work used. In contrast, reranking with deep NLMs of three or four layers are clearly better, yielding average improvements of 1.0 T ER / 1.0 B LEU points over the baseline and 0.5 T ER / 0.5 B LEU points over the system reranked with the 1-layer model, all of which are statisfically significant according to the test described in (Riezler and Maxwell, 2005). The fact that the improvements in terms of the intrinsic metrics listed in Table 1 do translate into gains in translation quality is interesting. It reinforces the trend reported in (Luong et al., 2015) that better source-conditioned perplexities lead to better translation scores. This phenomon is a useful result as in the past, many intrinsic metrics, e.g., alignment error rate, do not necessarily correlate with MT quality metrics. MT Reranking with NLMs Our MT models are built using the Phrasal MT toolkit (Cer et al., 2010). In addition to the standard dense feature set6 , we include a var"
K15-1031,P14-1129,0,0.500859,"ning Computer Science Department, Stanford University, Stanford, CA, 94305 {lmthang, mkayser, manning}@stanford.edu Abstract has been an active body of work recently in utilizing neural language models (NLMs) to improve translation quality. However, to the best of our knowledge, work in this direction only makes use of NLMs with either one or two hidden layers. For example, Schwenk (2010, 2012) and Son et al. (2012) used shallow NLMs with a single hidden layer for reranking. Vaswani et al. (2013) considered two-layer NLMs for decoding but provided no comparison among models of various depths. Devlin et al. (2014) reported only a small gain when decoding with a two-layer NLM over a single layer one. There have not been clear results on whether adding more layers to NLMs helps. In this paper, we demonstrate that deep NLMs with three or four layers are better than those with fewer layers in terms of the perplexity and the translation quality. We detail how we combine various techniques from past work to successfully train deep NLMs that condition on both the source and target contexts. When reranking nbest lists of a strong web-forum MT baseline, our deep models achieve an additional improvement of 0.5 T"
K15-1031,W12-2702,0,0.0286283,"5 epoch). The vocabularies are limited to the top 40K frequent words for both Chinese and English. All words not in (c,w)∈T While self-normalization does not lead to speed up in training, it allows trained models to be applied efficiently at test time without computing the normalization factors. This is similar in flavor to NCE but allows for flexibility (through α) in how hard we want to “squeeze” the normalization factors. Training deep NLMs. We follow (Devlin et al., 2014) to train self-normalized NLMs, conditioning on both the source and target contexts. Unlike 2 Some work (Schwenk, 2010; Schwenk et al., 2012) utilize a smaller softmax vocabulary, called short-list. 3 The test set is from BOLT and labeled as p1r6 dev. 4 We used an alignment heuristic similar to Devlin et al. (2014) but applicable to our phrase-based MT system. 306 Models 1 layer 2 layers 3 layers 4 layers Valid 9.39 9.20 8.64 8.10 Test 8.99 8.96 8.13 7.71 |log Z| 0.51 0.50 0.43 0.35 System Table 1: Training NLMs – validation and test perplexities achieved by self-normalized NLMs of various depths. We report the |log Z |value (base e), similar to Devlin et al. (2014), to indicate how good each model is in pushing the log normalizati"
K15-1031,W14-3360,1,0.839987,"1 do translate into gains in translation quality is interesting. It reinforces the trend reported in (Luong et al., 2015) that better source-conditioned perplexities lead to better translation scores. This phenomon is a useful result as in the past, many intrinsic metrics, e.g., alignment error rate, do not necessarily correlate with MT quality metrics. MT Reranking with NLMs Our MT models are built using the Phrasal MT toolkit (Cer et al., 2010). In addition to the standard dense feature set6 , we include a variety of sparse features for rules, word pairs, and word classes, as described in (Green et al., 2014). Our decoder uses three language models.7 We use a tuning set of 396K words in the newswire and web domains and tune our systems using online expected error rate training as in (Green et al., 2014). Our tuning metric is (B LEU-T ER)/2. We run a discriminative reranker on the 1000best output of a decoder with MERT. The features used in reranking include all the dense features, 5 As a reference point, though not directly comparable, Devlin et al. (2014) achieved 0.68 for |log Z |on a different test set with the same self-normalized constant α = 0.1. 6 Consisting of forward and backward translat"
K15-1031,N12-1005,0,0.118993,"Missing"
K15-1031,D13-1140,0,0.31935,"2015 Association for Computational Linguistics other language model, NLMs specify a distribution, p(w|c), to predict the next word w given a context c. The first step is to lookup embeddings for words in the context and concatenate them to form an input, h(0) , to the first hidden layer. We then repeatedly build up hidden representations as follows, for l = 1, . . . , n:   (1) h(l) = f W (l) h(l−1) + b(l) (Devlin et al., 2014), we found that using the rectified linear function, max{0, x}, proposed in (Nair and Hinton, 2010), works better than tanh. The rectified linear function was used in (Vaswani et al., 2013) as well. Furthermore, while these works use a fixed learning rate throughout, we found that having a simple learning rate schedule is useful in training well-performing deep NLMs. This has also been demonstrated in (Sutskever et al., 2014; Luong et al., 2015) and is detailed in Section 3. We do not perform any gradient clipping and notice that learning is more stable when short sentences of length less than or equal to 2 are removed. Bias terms are used for all hidden layers as well as the softmax layer as described earlier, which is slightly different from other work such as (Vaswani et al.,"
K15-1031,D13-1176,0,\N,Missing
K15-1031,P15-1001,0,\N,Missing
K16-1029,P15-1001,0,0.0586064,"Missing"
K16-1029,W16-2323,0,0.0211382,"Missing"
K16-1029,2012.eamt-1.60,0,0.0170422,"Missing"
K16-1029,D13-1176,0,0.118751,"cabulary, represented by a dash in Figure 1. Our Approach We first give a brief overview of Neural Machine Translation before describing the model architecture of interest, the deep multi-layer recurrent model with LSTM. We then explain the different types of NMT weights together with our approaches to pruning and retraining. 3.1 Neural Machine Translation Neural machine translation aims to directly model the conditional probability p(y|x) of translating a source sentence, x1 , . . . , xn , to a target sentence, y1 , . . . , ym . It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The encoder computes a representation s for each source sentence. Based on that source representation, the decoder generates a translation, one target word at a time, and hence, decomposes the log conditional probability as: Xm log p(y|x) = log p (yt |y&lt;t , s) (1) 3.2 Understanding NMT Weights Figure 2 shows the same system in more detail, highlighting the different types of parameters, or weights, in the model. We will go through the architecture from bottom to top. First, a vocabulary is chosen for each language, assuming that the top V frequent w"
K16-1029,2015.iwslt-evaluation.11,1,0.823463,"Missing"
K16-1029,P16-1100,1,0.753895,"Missing"
K16-1029,D15-1166,1,0.162855,"gure 1: A simplified diagram of NMT. Manning, 2015; Sennrich et al., 2016), EnglishTurkish (Sennrich et al., 2016), and English-Czech (Jean et al., 2015b; Luong and Manning, 2016). Figure 1 gives an example of an NMT system. While NMT has a significantly smaller memory footprint than traditional phrase-based approaches (which need to store gigantic phrase-tables and language models), the model size of NMT is still prohibitively large for mobile devices. For example, a recent state-of-the-art NMT system requires over 200 million parameters, resulting in a storage size of hundreds of megabytes (Luong et al., 2015a). Though the trend for bigger and deeper neural networks has brought great progress, it has also introduced over-parameterization, resulting in long running times, overfitting, and the storage size issue discussed above. A solution to the overparameterization problem could potentially aid all three issues, though the first (long running times) is outside the scope of this paper. Introduction Neural Machine Translation (NMT) is a simple new architecture for translating texts from one language into another (Sutskever et al., 2014; Cho et al., 2014). NMT is a single deep neural network that is"
K16-1029,P15-1002,0,0.264429,"gure 1: A simplified diagram of NMT. Manning, 2015; Sennrich et al., 2016), EnglishTurkish (Sennrich et al., 2016), and English-Czech (Jean et al., 2015b; Luong and Manning, 2016). Figure 1 gives an example of an NMT system. While NMT has a significantly smaller memory footprint than traditional phrase-based approaches (which need to store gigantic phrase-tables and language models), the model size of NMT is still prohibitively large for mobile devices. For example, a recent state-of-the-art NMT system requires over 200 million parameters, resulting in a storage size of hundreds of megabytes (Luong et al., 2015a). Though the trend for bigger and deeper neural networks has brought great progress, it has also introduced over-parameterization, resulting in long running times, overfitting, and the storage size issue discussed above. A solution to the overparameterization problem could potentially aid all three issues, though the first (long running times) is outside the scope of this paper. Introduction Neural Machine Translation (NMT) is a simple new architecture for translating texts from one language into another (Sutskever et al., 2014; Cho et al., 2014). NMT is a single deep neural network that is"
K16-1029,D15-1107,0,0.136286,"Missing"
K16-1029,P16-1009,0,0.0201133,"Missing"
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
K17-3002,D16-1238,0,0.0239177,"affine neural dependency parser presented by Dozat and Manning (2017), which uses a well-tuned LSTM network to produce vector representations for each word, then uses those vector representations in novel biaffine classifiers to predict the head token of each dependent and the class of the resulting edge. In order to adapt it to the wide variety of different treebanks in Universal Dependencies, we make two noteworthy extensions to the system: first, we incorporate a word representation built up from character sequences using an LSTM, theorizing that 1 For other neural graph-based parsers, cf. Cheng et al. (2016); Hashimoto et al. (2016); Zhang et al. (2016) 20 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 20–30, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. Edge j → i Labels for j → i arc-depi arc-headj rel-depi rel-headj ReLU ... RNN ... Embed hROOTi ROOT . PUNCT Figure 1: The architecture of our parser. Arrows indicate structural dependence, but not necessarily trainable parameters. dents.2 These vectors are then used in two biaffine classifiers: the first computes a score for each pair of token"
K17-3002,Q16-1023,0,0.216584,"Missing"
K17-3002,D15-1041,0,0.27538,"0 −1 0.1 0 1 2 3 4 5 CLAS difference 6 7 8 0.0 −8 −6 −4 −2 0 CLAS difference 2 4 Figure 6: Performance difference between parsers using our taggers and parsers without tags (left) and between parsers using UDPipe v1.1’s tags and parsers without tags (right), with both histograms fit to skew normal distributions 5 5.1 Ablation Studies extent the tagger network is actually necessary, for a number of reasons: presumably whatever predictive patterns it learns from the token sequences would also be learnable by the parser network; errors by the tagger are likely to be propagated by the parser; and Ballesteros et al. (2015) found that POS tags are drastically less important for character-based parsers. In order to examine how useful the POS tag information is to our character-based system, we trained an additional set of parsers without UPOS or XPOS input, comparing them to the other two, with the differences graphed in Figure 6. We find that the variant with no POS tag input is likewise significantly worse than our reported model according to a Wilcoxon test (p < 0.001), but not statistically different from the one trained with UDPipe tags (p > 0.05). This suggests that predicted POS tags are still useful for a"
K17-3002,H05-1066,0,0.0417606,"Missing"
K17-3002,D16-1147,0,0.00732944,"all no matter what word i is). After deciding on a head yi0 for word i, we use another biaffine transformation—this time involving the (rel) hidden vectors—to produce a predicted label: (1) hi (arc-dep) = H (arc-head) W (arc) hi (rel) si (6) >(rel-head) =h 0(arc) yi (rel-dep) + W (rel) hi (7) (rel-dep) U(rel) hi ⊕h (10) (rel-head)  0(arc) yi + b(rel) 0(arc) In order to produce a prediction yi for token i, we use a biaffine classifier involving the (arc) 0(rel) yi 2 Interestingly, other researchers have found similar approaches to be beneficial for other tasks; cf. Reed and de Freitas (2016); Miller et al. (2016); Daniluk et al. (2017) 3 We adopt the convention of using lowercase italics for scalars, lowercase bold for vectors, uppercase italics for matrices, and uppercase bold for tensors. We maintain this convention when indexing and stacking; so ai is the ith vector of matrix A, and matrix A is the stack of all vectors ai . (rel) = arg max sij j (11) Again, each term in line 10 has an intutive interpretation: the first term relates to the probability of observing a label given the information in both h(rel) vectors (e.g. the probability of the label det given word i is the with head cat); the secon"
K17-3002,L16-1680,0,0.0158075,"Missing"
K17-3002,P09-1040,0,0.0343378,"+ 9.99 25 25 20 15 10 5 0.60x + 8.70 0 −5 Effect of Nonprojectivity Discrepancy 0 5 10 Nonprojectivity in test 20 15 10 5 0 −5 15 −4 −3 −2 −1 0 1 2 Nonprojectivity difference 3 4 (a) Difference in CLAS between our parser and UDPipe v1.1 (b) Difference in CLAS between our parser and UDPipe v1.1 as a function of the nonprojectivity of the test set as a function of the difference between the nonprojectivity of the test and training sets Figure 3: How the percent of nonprojective arcs in the training and test set influence accuracy of our graph-based and a transition-based parser the swap action (Nivre, 2009). This makes any arbitrary nonprojective arc possible, but increases the number of transition steps required to produce that arc. One valid concern is that this might bias the model toward producing projective arcs; in our graph-based system, by contrast, there’s little reason to think nonprojective arcs should be harder to predict than projective ones. Here we aim to explore how the fraction of nonprojective arcs in a treebank affects the performance of the two types of systems. and content labeled attachment score. Our system achieves the highest aggregated score on all five of these metrics"
K17-3002,L16-1262,1,0.815119,"Missing"
K17-3002,P16-2067,0,0.0455897,"Missing"
K17-3002,W16-1603,0,\N,Missing
K17-3002,E17-1063,0,\N,Missing
K18-2016,Q17-1010,0,0.0263046,"beddings E. Once the encoder hidden states henc are obtained with a single-layer BiLSTM, each decoder step is unrolled as follows: dec hdec j = LSTMdec (Eyj−1 , hj−1 ), αij ∝ cj = (11) X (12) POS/UFeats Tagger Our tagger follows closely that of (Dozat et al., 2017), with a few extensions. As in that work, the core of the tagger is a highway BiLSTM (Srivastava et al., 2015) with inputs coming from the concatenation of three sources: (1) a pretrained word embedding, from the word2vec embeddings provided with the task when available (Mikolov et al., 2013), and from fastText embeddings otherwise (Bojanowski et al., 2017); (2) a trainable frequent word embedding, for all words that occurred at least seven times in the training set; and (3) a character-level embedding, generated from a unidirectional LSTM over characters in each word. UPOS is predicted by first transforming each word’s BiLSTM state with a fully-connected (FC) layer, then applying an affine classifier: (tag) hi = BiLSTMi (u) vi (u) (x1 , . . . , xn ), (14) = FC(u) (hi ), (15) (u)  P yik |X = softmaxk W (u) vi  . (16) To predict XPOS, we similarly start with transforming the BiLSTM states with an FC layer. In order to further ensure consistency"
K18-2016,N18-1202,0,0.0401956,"Missing"
K18-2016,K18-2005,0,0.021674,"e table, our system achieves competitive performance on nearly all of the metrics when macro-averaged over all treebanks. Moreover, it achieves the top performance on several metrics when evaluated only on big treebanks, showing that our systems can effectively leverage statistical patterns in the data. Where it is not the top performing system, our system also achieved competitive results on each of the metrics on these treebanks. This is encouraging considering that our system is comprised of single-system components, whereas some of the best performing teams used ensembles (e.g., HIT-SCIR (Che et al., 2018)). When taking a closer look, we find that our UFeats classifier is very accurate on these treebanks as well. Not only did it achieve the top performance on UFeats F1 , but also it helped the parser achieve top MLAS as well on big treebanks, even when the parser is not the best-performing as evaluated by other metrics. We also note the contribution from our consistency modeling in the POS tagger/UFeats classifier: in both settings the individual metrics (UPOS, XPOS, and UFeats) achieve a lower advantage margin over the reference systems when compared to the AllTags metric, showing that these r"
K18-2016,P17-1177,0,0.0277604,"g an unfortunate bug, our corrected system would have placed the 2nd , 1st , and 3rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on lowresource treebank categories on all metrics by a large margin. We further show the effectiveness of different model components through extensive ablation studies. 1 Introduction Dependency parsing is an important component in various natural langauge processing (NLP) systems for semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018), and machine translation (Chen et al., 2017). However, most research has treated dependency parsing in isolation, and largely ignored upstream NLP components that prepare relevant data for the parser, e.g., tokenizers and lemmatizers (Zeman et al., 2017). In reality, however, these upstream systems are still far from perfect. To this end, in our submission to the CoNLL 2018 UD Shared Task, we built a raw-textto-CoNLL-U pipeline system that performs all tasks required by the Shared Task (Zeman et al., ∗ 2 System Description In this section, we present detailed descriptions for each component of our neural pipeline system, namely the toke"
K18-2016,K18-2001,0,0.0875805,"Missing"
K18-2016,K17-3002,1,0.873642,"Missing"
K18-2016,K17-3001,1,0.891623,"Missing"
K18-2016,P18-1031,0,0.0497704,"Missing"
K18-2016,D17-1159,0,0.0382803,"le system submission achieved very competitive performance on big treebanks. Moreover, after fixing an unfortunate bug, our corrected system would have placed the 2nd , 1st , and 3rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on lowresource treebank categories on all metrics by a large margin. We further show the effectiveness of different model components through extensive ablation studies. 1 Introduction Dependency parsing is an important component in various natural langauge processing (NLP) systems for semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018), and machine translation (Chen et al., 2017). However, most research has treated dependency parsing in isolation, and largely ignored upstream NLP components that prepare relevant data for the parser, e.g., tokenizers and lemmatizers (Zeman et al., 2017). In reality, however, these upstream systems are still far from perfect. To this end, in our submission to the CoNLL 2018 UD Shared Task, we built a raw-textto-CoNLL-U pipeline system that performs all tasks required by the Shared Task (Zeman et al., ∗ 2 System Description In this section, we present"
K18-2016,D18-1244,1,0.800673,"rmance on big treebanks. Moreover, after fixing an unfortunate bug, our corrected system would have placed the 2nd , 1st , and 3rd on the official evaluation metrics LAS, MLAS, and BLEX, and would have outperformed all submission systems on lowresource treebank categories on all metrics by a large margin. We further show the effectiveness of different model components through extensive ablation studies. 1 Introduction Dependency parsing is an important component in various natural langauge processing (NLP) systems for semantic role labeling (Marcheggiani and Titov, 2017), relation extraction (Zhang et al., 2018), and machine translation (Chen et al., 2017). However, most research has treated dependency parsing in isolation, and largely ignored upstream NLP components that prepare relevant data for the parser, e.g., tokenizers and lemmatizers (Zeman et al., 2017). In reality, however, these upstream systems are still far from perfect. To this end, in our submission to the CoNLL 2018 UD Shared Task, we built a raw-textto-CoNLL-U pipeline system that performs all tasks required by the Shared Task (Zeman et al., ∗ 2 System Description In this section, we present detailed descriptions for each component o"
K19-1079,D14-1162,1,0.106957,"ore, for k &lt; 100, the GPT2-117 overlap is generally much higher than human levels. Both these phenomena can be seen in Table 2, where, for k = 10, GPT2-117 copies words such as queen more often than both the Fusion Model and the human-written story. Sentence embedding similarity To capture a higher-level notion of semantic similarity, we measure story-prompt sentence similarity – the cosine similarity of story-prompt sentence pairs, averaged by taking the mean over all pairs. Sentences are represented by the embedding method of Arora et al. (2017) – a weighted average of the GloVe embeddings (Pennington et al., 2014) of the words, with the first principal component removed. As shown in Figure 1, we find a similar pattern as for n-gram similarity: GPT2-117 generates sentences that are more similar to the prompt than the Fusion Model for all k, and both models’ prompt similarity decreases as k increases. 5 A good story generation model should produce coherent text with a logical ordering of events. Similarly, the underlying language model should be a good coherence scorer – assigning higher probability to coherent text than incoherent text. Barzilay and Lapata (2008) evaluate a coherence scorer by measuring"
K19-1079,J08-1001,0,\N,Missing
K19-1079,D18-1431,0,\N,Missing
K19-1079,N19-1170,1,\N,Missing
K19-1079,P19-3019,0,\N,Missing
K19-1079,W18-5712,0,\N,Missing
K19-1079,N19-1423,0,\N,Missing
K19-1079,D17-1238,0,\N,Missing
K19-1079,P18-1102,0,\N,Missing
K19-1079,N16-1014,0,\N,Missing
L16-1139,S07-1074,1,0.813316,"Missing"
L16-1139,D11-1072,0,0.361144,"Missing"
L16-1139,N03-5008,1,0.688171,"Wikipedia articles. To ensure that our training data is natural language (and not, e.g., lists or tables), we only include text marked as paragraphs (i.e., enclosed between HTML tags &lt;P&gt; and &lt;/P&gt;). The relevant training subset for a target string then consists of example contexts with anchor texts containing the string.7 We take spans of up to 100 tokens to the left and another 100 to the right. Given this training data, we applied standard machine learning techniques to perform supervised disambiguation of entities. We trained a maximum entropy multi-class classifier8 for each target string (Manning and Klein, 2003). Then, given a mention of the target string in the test data, we applied its classifier to the context of the mention, and returned the corresponding article. We did not construct classifiers for strings whose training data maps to a unique entity. Instead, in those cases, a default classifier falls back to LNRM cascade’s output. From each context, we extracted features (see Figure 4) commonly used for supervised classification in the WSD 7 The target string is a substring of the anchor text after case normalization. 8 `2 -regularization setting (Agirre and Lopez de Lacalle, 2007; Zhong and N"
L16-1139,W04-0807,0,0.150416,"Missing"
L16-1139,W04-0811,0,0.0452795,"Missing"
L16-1139,spitkovsky-chang-2012-cross,1,0.851634,"scends Wikipedia by including anchors (i) from the greater web; and (ii) to Wikipedia pages that may not (yet) exist. For the purposes of NED, it could make sense to discard all but the articles that correspond to named entities. We keep everything, however, since not all articles have a known entity type, and because we would like to construct a resource that is generally useful for disambiguating concepts. Our dictionary can disambiguate mentions directly, simply by returning the highest-scoring entry for a given string. The construction of this dictionary is explained with more details in (Spitkovsky and Chang, 2012). We developed several variants of the dictionary: EXCT, LNRM, FUZZ and HEUR. For a given string-article pair, where the string has been observed as the anchor-text of a total of y inter-Wikipedia and v external links, of which x (and, respectively, u) pointed to a page that is represented by the article in the pair, we set the pair’s score to be a ratio (x + u)/(y + v). We call this dictionary exact (EXCT), as it matches precisely the raw strings found using the methods outlined above. For example, Figure 1 shows all eight articles that have been referred to by the string “Hank Williams.” Not"
L16-1139,P10-4014,0,0.0259469,"lein, 2003). Then, given a mention of the target string in the test data, we applied its classifier to the context of the mention, and returned the corresponding article. We did not construct classifiers for strings whose training data maps to a unique entity. Instead, in those cases, a default classifier falls back to LNRM cascade’s output. From each context, we extracted features (see Figure 4) commonly used for supervised classification in the WSD 7 The target string is a substring of the anchor text after case normalization. 8 `2 -regularization setting (Agirre and Lopez de Lacalle, 2007; Zhong and Ng, 2010): • the anchor text; • the unordered set of lemmas in the span; • lemma for noun/verb/adjective in a four-token window around the anchor text; • lemma/word for noun/verb/adjective before and after the anchor text; • word/lemma/part-of-speech bigram and trigrams including the anchor text. 5.1. Variations Over the course of developing our system, we tested several variations of the core algorithm: Classifier: We tried maximum entropy models (MAXENT) and support vector machines (SVM). Dictionary: A dictionary influences supervised classification in two places. First, when building the training da"
L16-1262,W13-2308,0,0.0114723,"g diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Span"
L16-1262,W06-2920,0,0.443151,"clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies in the enhanced representation and miscellaneous information. The format is illustrated in Figure 3, with the French sentence from Figure 2. To support work on treebanks in this format,"
L16-1262,W09-2307,1,0.329071,"standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Ja"
L16-1262,P11-1061,1,0.573621,"UNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapt"
L16-1262,W08-1301,1,0.58058,"Missing"
L16-1262,de-marneffe-etal-2006-generating,1,0.213978,"Missing"
L16-1262,de-marneffe-etal-2014-universal,1,0.831309,"Missing"
L16-1262,E14-4028,0,0.0377832,"Missing"
L16-1262,N15-3011,1,0.696846,"Missing"
L16-1262,D07-1013,1,0.230402,"hocolat . le fille adorer le dessert a` le chocolat . DET NOUN VERB DET NOUN ADP DET NOUN PUNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged"
L16-1262,P13-2017,1,0.877648,"Missing"
L16-1262,W15-2127,0,0.0113361,"n English, we also obtain parallel representations between prepositional phrases and subordinate clauses, which are in practice often introduced by a preposition, as in (5). nmod case nsubj (5) a. Sue 1662 det left after the rehearsal advcl nsubj b. Sue Language mark nsubj left after we did The choice to make content words the backbone of the syntactic representations may seem to be at odds with the strong tendency in modern syntactic theory to give priority to functional heads, a tendency that is found in both constituency-based and dependency-based approaches to syntax (Brug´e et al., 2012; Osborne and Maxwell, 2015). We believe, however, that this conflict is more apparent than real. The UD view is that we need to recognize both lexical and functional heads, but in order to maximize parallelism across languages, only lexical heads are inferable from the topology of our tree structures. Functional heads are instead represented as specifying features of content words, using dedicated relation labels, features which can alternatively be specified through morphological processes. In the dependency grammar tradition, this is very close to the view of Tesni`ere (1959), according to whom dependencies hold betwe"
L16-1262,petrov-etal-2012-universal,1,0.717175,"exist to build consistent resources for many languages, and the UD project is a merger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we p"
L16-1262,rosa-etal-2014-hamledt,1,0.832586,"Missing"
L16-1262,L16-1376,1,0.208211,"fferent languages. For instance, while the universal UD scheme has a single relation acl for adnominal clauses, several languages make use of the subtype acl:relcl to distinguish relative clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies i"
L16-1262,E12-2021,1,0.581828,"Missing"
L16-1262,stepanek-pajas-2010-querying,0,0.067769,"Missing"
L16-1262,P13-2103,1,0.625022,"hese resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Spanish and Swedish). The first proposal for incorporating morphology was made by Tsarfaty (2013). The second version of HamleDT (Rosa et al., 2014) provided Stanford/Google annotation for 30 languages by automatically harmonizing treebanks with different native annotations. These efforts were followed by the development of the universal Stanford dependencies (USD), revising Stanford Dependencies for cross-linguistic annotations in light of the Google scheme (de Marneffe et al., 2014). UD is the result of merging all these initiatives into a single coherent framework, based on the universal Stanford dependencies, an extended version of the Google universal tag set, a revised subset of the"
L16-1262,I08-3008,1,0.20904,"the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-"
L16-1262,zeman-etal-2012-hamledt,1,0.729155,"Missing"
L16-1262,zeman-2008-reusable,1,0.897534,"erger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we present version 1 of the universal guidelines and explain the underlying d"
L16-1376,P15-1034,1,0.026186,"pendency parser (Chen and Manning, 2014). Second, our converters can be used either in combination with a constituency parser or a dependency parser to obtain UD graphs for any sentence which can then be utilized in downstream NLU tasks. Schuster et al. (2015), for example, used a preliminary version of the converter to obtain enhanced++ UD graphs from constituency trees. This system uses the UD graphs as input for a parser from image descriptions to a scene representation that captures relationships between objects in a visual scene. Further, the open domain information extractor in CoreNLP (Angeli et al., 2015) also already uses UD graphs to extract relations between entities. 5. Comparison to AMR Representing the meaning of sentences as directed graphs has a long tradition in computational linguistics, which goes back to at least Shieber (1984). One graph-based semantic representation that received significant attention in recent years is the Abstract Meaning Representation (AMR) (Banarescu et al., 2013). AMR also encodes sentences as directed graphs but compared to UD graphs, it aims to abstract further away from the surface form of sentences. To achieve this goal, it encodes sentences using PropB"
L16-1376,W13-2322,0,0.0834542,"graphs as input for a parser from image descriptions to a scene representation that captures relationships between objects in a visual scene. Further, the open domain information extractor in CoreNLP (Angeli et al., 2015) also already uses UD graphs to extract relations between entities. 5. Comparison to AMR Representing the meaning of sentences as directed graphs has a long tradition in computational linguistics, which goes back to at least Shieber (1984). One graph-based semantic representation that received significant attention in recent years is the Abstract Meaning Representation (AMR) (Banarescu et al., 2013). AMR also encodes sentences as directed graphs but compared to UD graphs, it aims to abstract further away from the surface form of sentences. To achieve this goal, it encodes sentences using PropBank framesets (Palmer et al., 2005) and approximately 100 fixed relations. This makes AMR a deeper and more canonicalized semantic representation as compared to UD graphs. While these are obviously desirable properties, we nevertheless believe that our representation has some advantages over AMR, especially when it is being used in shallow natural language understanding tasks. In terms of expressivi"
L16-1376,W07-1427,1,0.320748,"Missing"
L16-1376,D14-1082,1,0.543862,"hrase structure trees to basic English UD trees on the individual genres of the English Web Treebank corpus. We use the manually corrected English UD corpus v1.1 (Nivre et al., 2015) as a gold standard. 4.4. Applications We believe there are two main applications of our converters. First, our basic UD converter can be used to automatically convert existing treebanks of phrase structure trees to treebanks of UD trees for training dependency parsers. For example, we have successfully converted the entire Penn Treebank (Marcus et al., 1993) to train models for a neural network dependency parser (Chen and Manning, 2014). Second, our converters can be used either in combination with a constituency parser or a dependency parser to obtain UD graphs for any sentence which can then be utilized in downstream NLU tasks. Schuster et al. (2015), for example, used a preliminary version of the converter to obtain enhanced++ UD graphs from constituency trees. This system uses the UD graphs as input for a parser from image descriptions to a scene representation that captures relationships between objects in a visual scene. Further, the open domain information extractor in CoreNLP (Angeli et al., 2015) also already uses U"
L16-1376,de-marneffe-etal-2006-generating,1,0.123679,"Missing"
L16-1376,de-marneffe-etal-2014-universal,1,0.653001,"Missing"
L16-1376,N10-1131,0,0.0174015,"ford Parser. Keywords: Universal Dependencies, semantic representation, treebank conversion 1. Introduction nsubj Since its first version, the Stanford Dependencies (SD) representation (de Marneffe et al., 2006) has had the status of being both a syntactic and a shallow semantic representation. This dual status is also reflected in the usage of SD in natural language processing tasks which broadly fall into two categories. The first category is composed of tasks that require a syntactic tree such as source-side reordering for machine translation (e.g., Genzel (2010)) and sentence compression (Galanis and Androutsopoulos, 2010). For these tasks, a sound syntactic representation is more important than the relations between individual words. The second and much larger category is composed of a wide range of shallow natural language understanding (NLU) tasks such as biomedical text mining (e.g., Airola et al. (2008)), open domain relation extraction (e.g., Mausam et al. (2012)), and unsupervised semantic parsing (Poon and Domingos, 2009). For these tasks, the relations between content words are more important than the overall tree structure. Not surprisingly, we observe a similar divide if we look at which one of the t"
L16-1376,C10-1043,0,0.0372525,"as part of Stanford CoreNLP and the Stanford Parser. Keywords: Universal Dependencies, semantic representation, treebank conversion 1. Introduction nsubj Since its first version, the Stanford Dependencies (SD) representation (de Marneffe et al., 2006) has had the status of being both a syntactic and a shallow semantic representation. This dual status is also reflected in the usage of SD in natural language processing tasks which broadly fall into two categories. The first category is composed of tasks that require a syntactic tree such as source-side reordering for machine translation (e.g., Genzel (2010)) and sentence compression (Galanis and Androutsopoulos, 2010). For these tasks, a sound syntactic representation is more important than the relations between individual words. The second and much larger category is composed of a wide range of shallow natural language understanding (NLU) tasks such as biomedical text mining (e.g., Airola et al. (2008)), open domain relation extraction (e.g., Mausam et al. (2012)), and unsupervised semantic parsing (Poon and Domingos, 2009). For these tasks, the relations between content words are more important than the overall tree structure. Not surprisingly"
L16-1376,levy-andrew-2006-tregex,0,0.0109411,"lassic Collins head finder rules, some of them also take surface tokens into account which is necessary for distinguishing between main verbs and auxiliaries. We traverse the constituency tree in depth-first order and use these rules to obtain and store the head of each constituent, resulting in a tree in which every node has exactly one surface token as head. The head of each token is then simply the head of its lowest ancestor whose head is not the token itself. To determine the relation types, we define for each grammatical relation a set of tree patterns in the form of tregex expressions (Levy and Galen, 2006). For each headdependent pair we try to find a pattern that matches the subtree rooted at the lowest common ancestor of the head and the dependent. If such a pattern exists, we assign its corresponding grammatical relation to the head-dependent pair. In the rare cases where no pattern matches, we assign the most general relation dep. This procedure allows us to obtain correct dependency trees in most cases. Two phenomena, however, require additional consideration. First, as previously mentioned, the UD representation defines several multi-word expressions with function words that behave like a"
L16-1376,J93-2004,0,0.0582797,"head-dependent pair. In the rare cases where no pattern matches, we assign the most general relation dep. This procedure allows us to obtain correct dependency trees in most cases. Two phenomena, however, require additional consideration. First, as previously mentioned, the UD representation defines several multi-word expressions with function words that behave like a single word such as because of or in case. Extracting the correct structure for these expressions is often challenging because many of these expressions are not a constituent according to the Penn Treebank annotation guidelines (Marcus et al., 1993). We resolve this issue by preprocessing all phrase structure trees that contain multi-word expressions such that the entire expression forms a constituent. Second, the outlined procedure often attaches wh-words in questions to the wrong head. For a question such as What does Peter seem to have?, our procedure would attach what to the head of the matrix clause, seem, instead of the head of the embedded clause, have. If we were only concerned with converting manually annotated treebanks, we could resolve these ambiguities by making use of the indexed empty nodes in the phrase structure trees, a"
L16-1376,D12-1048,0,0.106741,"ge processing tasks which broadly fall into two categories. The first category is composed of tasks that require a syntactic tree such as source-side reordering for machine translation (e.g., Genzel (2010)) and sentence compression (Galanis and Androutsopoulos, 2010). For these tasks, a sound syntactic representation is more important than the relations between individual words. The second and much larger category is composed of a wide range of shallow natural language understanding (NLU) tasks such as biomedical text mining (e.g., Airola et al. (2008)), open domain relation extraction (e.g., Mausam et al. (2012)), and unsupervised semantic parsing (Poon and Domingos, 2009). For these tasks, the relations between content words are more important than the overall tree structure. Not surprisingly, we observe a similar divide if we look at which one of the three SD representations is being used for the individual downstream tasks. Most systems that require a syntactic representation use basic SD trees which are guaranteed to be a strict surface syntax tree. On the other hand, most systems that are concerned with the relations between content words use the collapsed or CCprocessed SD representations. Thes"
L16-1376,P13-2017,0,0.0161844,"Missing"
L16-1376,L16-1262,1,0.15069,"Missing"
L16-1376,W13-3728,0,0.189982,"n lists of specific expressions and modify the graph structure whenever we encounter one of them. Our converter generates enhanced and enhanced++ dependency graphs as described in the previous sections, with one exception. Currently, we don’t propagate object or nominal modifier relations in clauses with conjoined verb phrases such as “the store buys and sells cameras”. The reason for this is that there are also many cases such as “she was reading or watching a movie” where it would be wrong to add these relations and there are no syntactic cues that would allow us to distinguish these cases. Nyblom et al. (2013) successfully used a machine learning approach to solve this problem for Finnish but as there currently exists no corpus annotated with enhanced English UD graphs, we leave this to future work. 4.3. Evaluation We evaluate our basic converter against the manually checked English UD treebank v1.1 (Nivre et al., 2015) which contains annotations for all sentences in the EWT corpus (English Web Treebank, Linguistic Data Consortium release LDC2012T13). We convert all phrase structure trees in the EWT corpus to basic UD trees and compare the output with the manually checked trees using the official C"
L16-1376,J05-1004,0,0.156654,"ses UD graphs to extract relations between entities. 5. Comparison to AMR Representing the meaning of sentences as directed graphs has a long tradition in computational linguistics, which goes back to at least Shieber (1984). One graph-based semantic representation that received significant attention in recent years is the Abstract Meaning Representation (AMR) (Banarescu et al., 2013). AMR also encodes sentences as directed graphs but compared to UD graphs, it aims to abstract further away from the surface form of sentences. To achieve this goal, it encodes sentences using PropBank framesets (Palmer et al., 2005) and approximately 100 fixed relations. This makes AMR a deeper and more canonicalized semantic representation as compared to UD graphs. While these are obviously desirable properties, we nevertheless believe that our representation has some advantages over AMR, especially when it is being used in shallow natural language understanding tasks. In terms of expressivity, UD graphs have the advantage that they encode the meaning of sentences in terms of relations between surface form tokens and they are therefore as expressive as natural language. The expressivity of AMR, on 2375 the other hand, i"
L16-1376,D09-1001,0,0.00807233,". The first category is composed of tasks that require a syntactic tree such as source-side reordering for machine translation (e.g., Genzel (2010)) and sentence compression (Galanis and Androutsopoulos, 2010). For these tasks, a sound syntactic representation is more important than the relations between individual words. The second and much larger category is composed of a wide range of shallow natural language understanding (NLU) tasks such as biomedical text mining (e.g., Airola et al. (2008)), open domain relation extraction (e.g., Mausam et al. (2012)), and unsupervised semantic parsing (Poon and Domingos, 2009). For these tasks, the relations between content words are more important than the overall tree structure. Not surprisingly, we observe a similar divide if we look at which one of the three SD representations is being used for the individual downstream tasks. Most systems that require a syntactic representation use basic SD trees which are guaranteed to be a strict surface syntax tree. On the other hand, most systems that are concerned with the relations between content words use the collapsed or CCprocessed SD representations. These representations may be graphs instead of trees, and may cont"
L16-1376,W15-2812,1,0.653218,"auses with conjoined phrases, only the first conjunct has explicit relations to the governor and the dependents of the conjoined phrase. In the enhanced UD graph, nsubj:xsubj 3. The enhanced++ UD representation The enhanced representation provides reasonable analyses for most English sentences. However, there are some constructions in English that lead to an analysis which is suboptimal for many NLU systems that try to extract relationships between entities, such as open domain relation extraction (e.g., Mausam et al. (2012)), or extracting relationships between objects in image descriptions (Schuster et al., 2015). One set of problematic constructions involves partitive noun phrases such as both of the girls in which both of the acts semantically as a quantificational determiner. In the basic UD representation, however, both is the head of such a partitive phrase while the semantically very similar phrase both girls is headed by girls: 2372 det:qmod mwe mwe nsubj nmod:of case det Both of the girls are reading aux both of the girls are reading det nsubj aux both girls are reading Considering that many relation extraction systems use simple dependency tree patterns to extract entities and their relations"
L16-1376,P84-1075,0,0.159033,"r et al. (2015), for example, used a preliminary version of the converter to obtain enhanced++ UD graphs from constituency trees. This system uses the UD graphs as input for a parser from image descriptions to a scene representation that captures relationships between objects in a visual scene. Further, the open domain information extractor in CoreNLP (Angeli et al., 2015) also already uses UD graphs to extract relations between entities. 5. Comparison to AMR Representing the meaning of sentences as directed graphs has a long tradition in computational linguistics, which goes back to at least Shieber (1984). One graph-based semantic representation that received significant attention in recent years is the Abstract Meaning Representation (AMR) (Banarescu et al., 2013). AMR also encodes sentences as directed graphs but compared to UD graphs, it aims to abstract further away from the surface form of sentences. To achieve this goal, it encodes sentences using PropBank framesets (Palmer et al., 2005) and approximately 100 fixed relations. This makes AMR a deeper and more canonicalized semantic representation as compared to UD graphs. While these are obviously desirable properties, we nevertheless bel"
L16-1376,P13-2103,0,0.0243219,"arted to laugh”. The basic SD representation of this sentence lacks a direct relation between the controlled verb laugh and its controller, Fred, while in the CCprocessed SD representation, this relation is made explicit with an additional subject edge. xcomp mark Fred started to laugh nsubj The popularity of these extended representations suggests that their existence plays a major role in the popularity of Stanford Dependencies. In recent years, there has been a lot of interest in extending Stanford Dependencies to other languages, including morphologically rich ones (McDonald et al., 2013; Tsarfaty, 2013; de Marneffe et al., 2014). These individual projects ultimately led to the Universal Dependencies (UD) initiative (Nivre et al., 2016) whose goal is to develop crosslinguistically consistent treebank annotations for as many languages as possible. While this project recognizes the status of dependency formalisms as semantic representations and based many design decisions on their impact on NLU tasks, the majority of efforts so far have focused on the development of the basic UD representation and the annotation of treebanks. Both de Marneffe et al. (2014) and Nivre et al. (2016) also mention"
N03-1016,J98-2004,0,0.762807,"coverage grammars and long sentences, even cubic algorithms can be far too expensive in practice. Two primary types of methods for accelerating parse selection have been proposed. Roark (2001) and Ratnaparkhi (1999) use a beam-search strategy, in which only the best n parses are tracked at any moment. Parsing time is linear and can be made arbitrarily fast by reducing n. This is a greedy strategy, and the actual Viterbi (highest probability) parse can be pruned from the beam because, while it is globally optimal, it may not be locally optimal at every parse stage. Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al. (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. In best-first parsing, one builds a figure-of-merit (FOM) over parser items, and uses the FOM to decide the order in which agenda items should be processed. This approach also dramatically reduces the work done during parsing, though it, too, gives no guarantee that the first parse returned is the actual Viterbi parse (nor does it maintain a worst-case cubic time bound). We discuss best-first parsing further in section 3.3. Both of these speed-up techniques are based"
N03-1016,W98-1115,0,0.422114,"ences, even cubic algorithms can be far too expensive in practice. Two primary types of methods for accelerating parse selection have been proposed. Roark (2001) and Ratnaparkhi (1999) use a beam-search strategy, in which only the best n parses are tracked at any moment. Parsing time is linear and can be made arbitrarily fast by reducing n. This is a greedy strategy, and the actual Viterbi (highest probability) parse can be pruned from the beam because, while it is globally optimal, it may not be locally optimal at every parse stage. Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al. (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. In best-first parsing, one builds a figure-of-merit (FOM) over parser items, and uses the FOM to decide the order in which agenda items should be processed. This approach also dramatically reduces the work done during parsing, though it, too, gives no guarantee that the first parse returned is the actual Viterbi parse (nor does it maintain a worst-case cubic time bound). We discuss best-first parsing further in section 3.3. Both of these speed-up techniques are based on greedy models of pars"
N03-1016,H90-1053,0,0.0156624,"ever, when dealing with wide-coverage grammars and long sentences, even cubic algorithms can be far too expensive in practice. Two primary types of methods for accelerating parse selection have been proposed. Roark (2001) and Ratnaparkhi (1999) use a beam-search strategy, in which only the best n parses are tracked at any moment. Parsing time is linear and can be made arbitrarily fast by reducing n. This is a greedy strategy, and the actual Viterbi (highest probability) parse can be pruned from the beam because, while it is globally optimal, it may not be locally optimal at every parse stage. Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al. (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. In best-first parsing, one builds a figure-of-merit (FOM) over parser items, and uses the FOM to decide the order in which agenda items should be processed. This approach also dramatically reduces the work done during parsing, though it, too, gives no guarantee that the first parse returned is the actual Viterbi parse (nor does it maintain a worst-case cubic time bound). We discuss best-first parsing further in section 3.3. Both of these"
N03-1016,W97-0302,0,0.0947193,"to create new edges. For example, NP:[0,2] might be removed from the agenda, and, if there were a rule S → NP VP and VP :[2,8] was already entered into the chart, the edge S:[0,8] would be formed, and added to the agenda if it were not in the chart already. The way an A* parser differs from a classic chart parser is that, like a best-first parser, agenda edges are processed according to a priority. In best-first parsing, this priority is called a figure-of-merit (FOM), and is based on various approximations to P (e|s), the fraction of parses of a sentence s which include an edge e (though see Goodman (1997) for an alternative notion of FOM). Edges which seem promising are explored first; others can wait on the agenda indefinitely. Note that even if we did know P (e|s) exactly, we still would not know whether e occurs in any best parse of s. Nonetheless, good FOMs empirically lead quickly to good parses. Best-first parsing aims to find a (hopefully good) parse quickly, but gives no guarantee that the first parse discovered is the Viterbi parse, nor does it allow one to recognize the Viterbi parse when it is found. In A* parsing, we wish to construct priorities which will speed up parsing, yet sti"
N03-1016,J98-4004,0,0.0807007,"rie encodings of rules. S1 XLR N SXL Outside-Trie Rules NP → XNP→ · NN NN 0.4 XNP→ · NN → DT JJ 0.75 XNP→ · NN → DT NN 0.25 SXR Edges Blocked F Original Rules NP → DT JJ NN 0.3 NP → DT NN NN 0.1 Figure 6: Fraction of edges saved by using various estimate methods, for two rule encodings. O - TRIE is a deterministic right-branching trie encoding (Leermakers, 1992) with weights pushed left (Mohri, 1997). I - TRIE is a non-deterministic leftbranching trie with weights on rule entry as in Charniak et al. (1998). states, such as by annotating nodes with their parent and even grandparent categories (Johnson, 1998). This annotation multiplies out the state space, giving a much larger grammar, and projecting back to the unannotated state set can be used as an outside estimate. Second, and perhaps more importantly, this technique can be applied to lexical parsing, where the state projections are onto the delexicalized PCFG symbols and/or onto the word-word dependency structures. This is particularly effective when the tree model takes a certain factored form; see Klein and Manning (2003) for details. 3.3 Parsing Performance Following (Charniak et al., 1998), we parsed unseen sentences of length 18–26 from"
N03-1016,W01-1812,1,0.920619,"een words). An inside parse of an edge e = X:[i, j] is a derivation in G from X to i wj . Let βG (e, s) denote the log-probability of a best inside parse of e (its Viterbi inside score).1 We will drop the G, s, and even e when context permits. Our parser, like a best-first parser, maintains estimates b(e, s) of β(e, s) which begin at −∞, only increase over time, and always represent the score of the best parses of their edges e discovered so far. Optimality means that for any e, b(e, s) will equal βG (e, s) when e is removed from the agenda. If one uses b(e, s) to prioritize edges, we show in Klein and Manning (2001a), that the parser is optimal over arbitrary PCFGs, and a wide range of control strategies. This is proved using an extension of Dijkstra’s algorithm to a certain kind of hypergraph associated with parsing, shown in figure 1(b): parse items are nodes in the hypergraph, hyperarcs take sets of parse items to their result item, and hyperpaths map to parses. Reachability from start corresponds to parseability, and shortest paths to Viterbi parses. 1 Our use of inside score and outside score evokes the same picture as talk about inside and outside probabilities, but note that in this paper inside"
N03-1016,P01-1044,1,0.840817,"een words). An inside parse of an edge e = X:[i, j] is a derivation in G from X to i wj . Let βG (e, s) denote the log-probability of a best inside parse of e (its Viterbi inside score).1 We will drop the G, s, and even e when context permits. Our parser, like a best-first parser, maintains estimates b(e, s) of β(e, s) which begin at −∞, only increase over time, and always represent the score of the best parses of their edges e discovered so far. Optimality means that for any e, b(e, s) will equal βG (e, s) when e is removed from the agenda. If one uses b(e, s) to prioritize edges, we show in Klein and Manning (2001a), that the parser is optimal over arbitrary PCFGs, and a wide range of control strategies. This is proved using an extension of Dijkstra’s algorithm to a certain kind of hypergraph associated with parsing, shown in figure 1(b): parse items are nodes in the hypergraph, hyperarcs take sets of parse items to their result item, and hyperpaths map to parses. Reachability from start corresponds to parseability, and shortest paths to Viterbi parses. 1 Our use of inside score and outside score evokes the same picture as talk about inside and outside probabilities, but note that in this paper inside"
N03-1016,J97-2003,0,0.024176,"S1 SX R SX M LR S S1 X LR SX 100 90 80 70 60 50 40 30 20 10 0 SX TRUE SX L SX S NULL U LL SXR NULL Inside-Trie Rules NP → XDT JJ NN 0.3 NP → XDT NN NN 0.1 XDT JJ → DT JJ 1.0 XDT NN → DT NN 1.0 Figure 5: Two trie encodings of rules. S1 XLR N SXL Outside-Trie Rules NP → XNP→ · NN NN 0.4 XNP→ · NN → DT JJ 0.75 XNP→ · NN → DT NN 0.25 SXR Edges Blocked F Original Rules NP → DT JJ NN 0.3 NP → DT NN NN 0.1 Figure 6: Fraction of edges saved by using various estimate methods, for two rule encodings. O - TRIE is a deterministic right-branching trie encoding (Leermakers, 1992) with weights pushed left (Mohri, 1997). I - TRIE is a non-deterministic leftbranching trie with weights on rule entry as in Charniak et al. (1998). states, such as by annotating nodes with their parent and even grandparent categories (Johnson, 1998). This annotation multiplies out the state space, giving a much larger grammar, and projecting back to the unannotated state set can be used as an outside estimate. Second, and perhaps more importantly, this technique can be applied to lexical parsing, where the state projections are onto the delexicalized PCFG symbols and/or onto the word-word dependency structures. This is particularl"
N03-1016,J01-2004,0,0.0257984,"chieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation. Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time. 1 Introduction PCFG parsing algorithms with worst-case cubic-time bounds are well-known. However, when dealing with wide-coverage grammars and long sentences, even cubic algorithms can be far too expensive in practice. Two primary types of methods for accelerating parse selection have been proposed. Roark (2001) and Ratnaparkhi (1999) use a beam-search strategy, in which only the best n parses are tracked at any moment. Parsing time is linear and can be made arbitrarily fast by reducing n. This is a greedy strategy, and the actual Viterbi (highest probability) parse can be pruned from the beam because, while it is globally optimal, it may not be locally optimal at every parse stage. Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al. (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. In best-first parsing, one builds a"
N03-1016,J03-4003,0,\N,Missing
N03-1033,A00-1031,0,0.11107,"Missing"
N03-1033,P98-1029,0,0.133187,"Missing"
N03-1033,J95-4004,0,0.136762,"2 ), or centered tags (t−1 , t0 , t+1 ) respectively. Again, with roughly equivalent feature sets, the left context is better than the right, and the centered context is better than either unidirectional context. line for this task high, while substantial annotator noise creates an unknown upper bound on the task. 3.2 Lexicalization Lexicalization has been a key factor in the advance of statistical parsing models, but has been less exploited for tagging. Words surrounding the current word have been occasionally used in taggers, such as (Ratnaparkhi, 1996), Brill’s transformation based tagger (Brill, 1995), and the HMM model of Lee et al. (2000), but nevertheless, the only lexicalization consistently included in tagging models is the dependence of the part of speech tag of a word on the word itself. In maximum entropy models, joint features which look at surrounding words and their tags, as well as joint features of the current word and surrounding words are in principle straightforward additions, but have not been incorporated into previous models. We have found these features to be very useful. We explore here lexicalization both alone and in combination with preceding and following tag histo"
N03-1033,A88-1019,0,0.138214,"Missing"
N03-1033,W02-1001,0,0.159577,"ned to maximize the conditional likelihood over the training data of that node. At test time, the sequence with the highest product of local conditional scores is calculated and returned. We can always find the exact maximizing sequence, but only in the case of an acyclic net is it guaranteed to be the maximum likelihood sequence. 3 Experiments The part of speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III (Marcus et al., 1994). We extracted tagged sentences from the parse trees.5 We split the data into training, development, and test sets as in (Collins, 2002). Table 1 lists character5 Note that these tags (and sentences) are not identical to those obtained from the tagged/pos directories of the same disk: hundreds of tags in the RB/RP/IN set were changed to be more consistent in the parsed/mrg version. Maybe we were the last to discover this, but we’ve never seen it in print. istics of the three splits.6 Except where indicated for the model BEST, all results are on the development set. One innovation in our reporting of results is that we present whole-sentence accuracy numbers as well as the traditional per-tag accuracy measure (over all tokens,"
N03-1033,P99-1069,0,0.144081,"Missing"
N03-1033,W02-1002,1,0.136285,"he tag at a certain position, the obvious thing to do is to explicitly include in the local model all predictive features, no matter on which side of the target position they lie. There are two good formal reasons to expect that a model explicitly conditioning on both sides at each position, like figure 1(c) could be advantageous. First, because of smoothing effects and interaction with other conditioning features (like the words), left-to-right factors like P(t0 |t−1 , w0 ) do not always suffice when t0 is implicitly needed to determine t−1 . For example, consider a case of observation bias (Klein and Manning, 2002) for a first-order left-toright CMM. The word to has only one tag (TO) in the PTB tag set. The TO tag is often preceded by nouns, but rarely by modals (MD). In a sequence will to fight, that trend indicates that will should be a noun rather than a modal verb. However, that effect is completely lost in a CMM like (a): P(twill |will, hstar ti) prefers the modal tagging, and P(TO |to, twill ) is roughly 1 regardless of twill . While the model has an arrow between the two tag positions, that path of influence is severed.3 The same problem exists in the other direction. If we use the symmetric righ"
N03-1033,P00-1034,0,0.118545,"Missing"
N03-1033,W96-0213,0,0.139267,"one wants to find a coherent sentence interpretation). Further, while some tag errors matter much more than others, to a first cut getting a single tag wrong in many of the more common ways (e.g., proper noun vs. common noun; noun vs. verb) would lead to errors in a subsequent processor such as an information extraction system or a parser that would greatly degrade results for the entire sentence. Finally, the fact that the measure has much more dynamic range has some appeal when reporting tagging results. The per-state models in this paper are log-linear models, building upon the models in (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000), though some models are in fact strictly simpler. The features in the models are defined using templates; there are different templates for rare words aimed at learning the correct tags for unknown words.7 We present the results of three classes of experiments: experiments with directionality, experiments with lexicalization, and experiments with smoothing. 3.1 Experiments with Directionality In this section, we report experiments using log-linear CMMs to populate nets with various structures, exploring the relative value of neighboring words’ tags. Table 2 l"
N03-1033,P99-1023,0,0.086251,"Missing"
N03-1033,W00-1308,1,0.229218,"erent sentence interpretation). Further, while some tag errors matter much more than others, to a first cut getting a single tag wrong in many of the more common ways (e.g., proper noun vs. common noun; noun vs. verb) would lead to errors in a subsequent processor such as an information extraction system or a parser that would greatly degrade results for the entire sentence. Finally, the fact that the measure has much more dynamic range has some appeal when reporting tagging results. The per-state models in this paper are log-linear models, building upon the models in (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000), though some models are in fact strictly simpler. The features in the models are defined using templates; there are different templates for rare words aimed at learning the correct tags for unknown words.7 We present the results of three classes of experiments: experiments with directionality, experiments with lexicalization, and experiments with smoothing. 3.1 Experiments with Directionality In this section, we report experiments using log-linear CMMs to populate nets with various structures, exploring the relative value of neighboring words’ tags. Table 2 lists the discussed networks. All n"
N03-1033,W99-0606,1,\N,Missing
N03-1033,J93-2004,0,\N,Missing
N03-1033,C98-1029,0,\N,Missing
N06-1006,H05-1079,0,0.0893309,"the core of weighted abduction theorem proving consider matching an individual node of the hypothesis (e.g. rose(e1)) with something from the text (e.g. fell(e1)), just as in the graph-matching approach. The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms—as in Moldovan et al. (2003) but not Raina et al. (2005). Then the theorem prover may generate intermediate forms in the proof, but, nevertheless, individual terms are resolved locally without reference to global context. Finally, a few efforts (Akhmatova, 2005; Fowler et al., 2005; Bos and Markert, 2005) have tried to translate sentences into formulas of first-order logic, in order to test logical entailment with a theorem prover. While in principle this approach does not suffer from the limitations we describe below, in practice it has not borne much fruit. Because few problem sentences can be accurately translated to logical form, and because logical entailment is a strict standard, recall tends to be poor. The simple graph matching formulation of the problem belies three important issues. First, the above systems assume a form of upward monotonicity: if a good match is found with a part of"
N06-1006,de-marneffe-etal-2006-generating,1,0.369258,"Missing"
N06-1006,H05-1049,1,0.379173,"e. The simplest approach is to base the entailment prediction on the degree of semantic overlap between the text and hypothesis using models based on bags of words, bags of n-grams, TF-IDF scores, or something similar (Jijkoun and de Rijke, 2005). Such models have serious limitations: semantic overlap is typically a symmetric relation, whereas entailment is clearly not, and, because overlap models do not account for syntactic or semantic structure, they are easily fooled by examples like ID 2081. A more structured approach is to formulate the entailment prediction as a graph matching problem (Haghighi et al., 2005; de Salvo Braz et al., 2005). In this formulation, sentences are represented as normalized syntactic dependency graphs (like the one shown in figure 1) and entailment is approximated with an alignment between the graph representing the hypothesis and a portion of the corresponding graph(s) representing the text. Each possible alignment of the graphs has an associated score, and the score of the best alignment is used as an approximation to the strength of the entailment: a betteraligned hypothesis is assumed to be more likely to be entailed. To enable incremental search, alignment scores are"
N06-1006,P88-1012,0,0.0610457,"variety of approximate search techniques. Haghighi et al. (2005) 42 divide the search into two steps: in the first step they consider node scores only, which relaxes the problem to a weighted bipartite graph matching that can be solved in polynomial time, and in the second step they add the edges scores and hillclimb the alignment via an approximate local search. A third approach, exemplified by Moldovan et al. (2003) and Raina et al. (2005), is to translate dependency parses into neo-Davidsonian-style quasilogical forms, and to perform weighted abductive theorem proving in the tradition of (Hobbs et al., 1988). Unless supplemented with a knowledge base, this approach is actually isomorphic to the graph matching approach. For example, the graph in figure 1 might generate the quasi-LF rose(e1), nsubj(e1, x1), sales(x1), nn(x1, x2), Mitsubishi(x2), dobj(e1, x3), percent(x3), num(x3, x4), 46(x4). There is a term corresponding to each node and arc, and the resolution steps at the core of weighted abduction theorem proving consider matching an individual node of the hypothesis (e.g. rose(e1)) with something from the text (e.g. fell(e1)), just as in the graph-matching approach. The two models become disti"
N06-1006,P03-1054,1,0.0200203,"ble about their semantic content. We use typed dependency graphs, which contain a node for each word and labeled edges representing the grammatical relations between words. Figure 1 gives the typed dependency graph for ID 971. This representation contains much of the information about words and relations between them, and is relatively easy to compute from a syntactic parse. However many semantic phenomena are not represented properly; particularly egregious is the inability to represent quantification and modality. We parse input sentences to phrase structure trees using the Stanford parser (Klein and Manning, 2003), a statistical syntactic parser trained on the Penn TreeBank. To ensure correct parsing, we preprocess the sentences to collapse named entities into new dedicated tokens. Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). After parsing, contiguous collocations which appear in WordNet (Fellbaum, 1998) are identified and grouped. We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules (de Marneffe et al., 2006). In these rules, heads of constituents are first identified using a modi"
N06-1006,levy-andrew-2006-tregex,0,0.0121484,"Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). After parsing, contiguous collocations which appear in WordNet (Fellbaum, 1998) are identified and grouped. We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules (de Marneffe et al., 2006). In these rules, heads of constituents are first identified using a modified version of the Collins head rules that favor semantic heads (such as lexical verbs rather than auxiliaries), and dependents of heads are typed using tregex patterns (Levy and Andrew, 2006), an extension of the tgrep pattern language. The nodes in the final graph are then annotated with their associated word, part-of-speech (given by the parser), lemma (given by a finite-state transducer described by Minnen et al. (2001)) and named-entity tag. 3.2 Alignment graphs representing the hypothesis and the text. An alignment consists of a mapping from each node (word) in the hypothesis graph to a single node in the text graph, or to null.3 Figure 1 gives the alignment for ID 971. The space of alignments is large: there are O((m + 1)n ) possible alignments for a hypothesis graph with n"
N06-1006,W05-1201,0,0.108372,"of the text, assuming that we allow an alignment with “loose” arc correspondence.2 Under this candidate alignment, the lexical alignments are perfect, and the only imperfect alignment is the subject arc of were is mismatched in the two. A robust inference guesser will still likely conclude that there is entailment. We propose that all three problems can be resolved in a two-stage architecture, where the alignment phase is followed by a separate phase of entailment determination. Although developed independently, the same division between alignment and classification has also been proposed by Marsi and Krahmer (2005), whose textual system is developed and evaluated on parallel translations into Dutch. Their classification phase features an output space of five semantic relations, and performs well at distinguishing entailing sentence pairs. Finding aligned content can be done by any search procedure. Compared to previous work, we emphasize structural alignment, and seek to ignore issues like polarity and quantity, which can be left to a subsequent entailment decision. For example, the scoring function is designed to encourage antonym matches, and ignore the negation of verb predicates. The ideas clearly g"
N06-1006,W03-0430,0,0.00852042,"ut words and relations between them, and is relatively easy to compute from a syntactic parse. However many semantic phenomena are not represented properly; particularly egregious is the inability to represent quantification and modality. We parse input sentences to phrase structure trees using the Stanford parser (Klein and Manning, 2003), a statistical syntactic parser trained on the Penn TreeBank. To ensure correct parsing, we preprocess the sentences to collapse named entities into new dedicated tokens. Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). After parsing, contiguous collocations which appear in WordNet (Fellbaum, 1998) are identified and grouped. We convert the phrase structure trees to typed dependency graphs using a set of deterministic handcoded rules (de Marneffe et al., 2006). In these rules, heads of constituents are first identified using a modified version of the Collins head rules that favor semantic heads (such as lexical verbs rather than auxiliaries), and dependents of heads are typed using tregex patterns (Levy and Andrew, 2006), an extension of the tgrep pattern language. The nodes in the final graph are then anno"
N06-1006,N03-1022,0,0.1205,"e extract features representing high-level characteristics of the entailment problem, and pass the resulting feature vector to a statistical classifier trained on development data. We report results on data from the 2005 Pascal RTE Challenge which surpass previously reported results for alignment-based systems. 1 Introduction During the last five years there has been a surge in work which aims to provide robust textual inference in arbitrary domains about which the system has no expertise. The best-known such work has occurred within the field of question answering (Pasca and Harabagiu, 2001; Moldovan et al., 2003); more recently, such work has continued with greater focus in addressing the PASCAL Recognizing Textual Entailment (RTE) Challenge (Dagan et al., 2005) and within the U.S. Government AQUAINT program. Substantive progress on this task is key to many text and natural language applications. If one could tell that Protestors chanted slogans opposing a free trade agreement was a match for people demonstrating against free trade, then one could offer a form of semantic search not available with current keywordbased search. Even greater benefits would flow to richer and more semantically complex NLP"
N06-1006,H05-1047,0,0.0223103,"with a factored alignment score. The last issue arising in the graph matching approaches is the inherent confounding of alignment and entailment determination. The way to show that one graph element does not follow from another is to make the cost of aligning them high. However, since we are embedded in a search for the lowest cost alignment, this will just cause the system to choose an alternate alignment rather than recognizing a non-entailment. In ID 152, we would like the hypothesis to align with the first part of the text, to 1 This is the same problem labeled and addressed as context in Tatu and Moldovan (2005). 43 be able to prove that civilians are not members of law enforcement agencies and conclude that the hypothesis does not follow from the text. But a graphmatching system will to try to get non-entailment by making the matching cost between civilians and members of law enforcement agencies be very high. However, the likely result of that is that the final part of the hypothesis will align with were civilians at the end of the text, assuming that we allow an alignment with “loose” arc correspondence.2 Under this candidate alignment, the lexical alignments are perfect, and the only imperfect al"
N09-1037,W00-0717,0,0.0125962,"s We defined features over both the parse rules and the named entities. Most of our features are over one or the other aspects of the structure, but not both. Both the named entity and parsing features utilize the words of the sentence, as well as orthographic and distributional similarity information. For each word we computed a word shape which encoded information about capitalization, length, and inclusion of numbers and other non-alphabetic characters. For the distributional similarity information, we had to first train a distributional similarity model. We trained the model described in (Clark, 2000), with code downloaded from his website, on several hundred million words from the British national corpus, and the English Gigaword corpus. The model we trained had 200 clusters, and we used it to assign each word in the training and test data to one of the clusters. For the named entity features, we used a fairly standard feature set, similar to those described in (Finkel et al., 2005). For parse features, we used the exact same features as described in (Finkel and Manning, 2008). When computing those features, we removed all of the named entity information from the rules, so that these feat"
N09-1037,P97-1003,0,0.0267671,"named entity Egyptian Islamic Jihad helped the parser to get its surrounding context correct, because it is improbable to attach a PP headed by with to an organization. At the same time, the surrounding context helped the joint model correctly identify Egyptian Islamic Jihad as an organization and not a person. The baseline parser also incorrectly added an extra level of structure to the person name Osama Bin Laden, while the joint model found the correct structure. 6 Related Work A pioneering antecedent for our work is (Miller et al., 2000), who trained a Collins-style generative 332 parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task. Their sentence augmentations were similar to ours, but they did not make use of features due to the generative nature of their model. This approach was not followed up on in other work, presumably because around this time nearly all the activity in named entity and relation extraction moved to the use of discriminative sequence models, which allowed the flexible specification of feature templates that are very useful for these tasks. The present model is able to bring to"
N09-1037,P08-1109,1,0.287498,"Missing"
N09-1037,P05-1045,1,0.0149194,"ization, length, and inclusion of numbers and other non-alphabetic characters. For the distributional similarity information, we had to first train a distributional similarity model. We trained the model described in (Clark, 2000), with code downloaded from his website, on several hundred million words from the British national corpus, and the English Gigaword corpus. The model we trained had 200 clusters, and we used it to assign each word in the training and test data to one of the clusters. For the named entity features, we used a fairly standard feature set, similar to those described in (Finkel et al., 2005). For parse features, we used the exact same features as described in (Finkel and Manning, 2008). When computing those features, we removed all of the named entity information from the rules, so that these features were just over the parse information and not at all over the named entity information. Lastly, we have the joint features. We included as features each augmented rule and each augmented label. This allowed the model to learn that certain types of phrasal nodes, such as NPs are more likely to be named entities, and that certain entities were more likely to occur in certain contexts a"
N09-1037,W06-1673,1,0.776867,"n Islamic Jihad with ties (b) to Osama Bin Laden Figure 4: An example for which the joint model helped with both parse structure and named entity recognition. The individual models (a) incorrectly attach the PP, label Egyptian Islamic Jihad as a person, and incorrectly add extra internal structure to Osama Bin Laden. The joint model (b) gets both the structure and the named entity correct. model of Chinese word segmentation and parts of speech using a single perceptron. An alternative approach to joint modeling is to take a pipelined approach. Previous work on linguistic annotation pipelines (Finkel et al., 2006; Hollingshead and Roark, 2007) has enforced consistency from one stage to the next. However, these models are only used at test time; training of the components is still independent. These models also have the potential to suffer from search errors and are not guaranteed to find the optimal output. 7 Conclusion We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks. Our model 333 is based on a discriminative constituency parser, with the data, grammar, and features carefully constructed for the joint task. In the f"
N09-1037,P07-1120,0,0.0212537,"ties (b) to Osama Bin Laden Figure 4: An example for which the joint model helped with both parse structure and named entity recognition. The individual models (a) incorrectly attach the PP, label Egyptian Islamic Jihad as a person, and incorrectly add extra internal structure to Osama Bin Laden. The joint model (b) gets both the structure and the named entity correct. model of Chinese word segmentation and parts of speech using a single perceptron. An alternative approach to joint modeling is to take a pipelined approach. Previous work on linguistic annotation pipelines (Finkel et al., 2006; Hollingshead and Roark, 2007) has enforced consistency from one stage to the next. However, these models are only used at test time; training of the components is still independent. These models also have the potential to suffer from search errors and are not guaranteed to find the optimal output. 7 Conclusion We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks. Our model 333 is based on a discriminative constituency parser, with the data, grammar, and features carefully constructed for the joint task. In the future, we would like to add oth"
N09-1037,N06-2015,0,0.070362,"Missing"
N09-1037,J93-2004,0,0.0461349,"Missing"
N09-1037,A00-2030,0,0.0175603,"named entity information encoded within the parse. In this example, the named entity Egyptian Islamic Jihad helped the parser to get its surrounding context correct, because it is improbable to attach a PP headed by with to an organization. At the same time, the surrounding context helped the joint model correctly identify Egyptian Islamic Jihad as an organization and not a person. The baseline parser also incorrectly added an extra level of structure to the person name Osama Bin Laden, while the joint model found the correct structure. 6 Related Work A pioneering antecedent for our work is (Miller et al., 2000), who trained a Collins-style generative 332 parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task. Their sentence augmentations were similar to ours, but they did not make use of features due to the generative nature of their model. This approach was not followed up on in other work, presumably because around this time nearly all the activity in named entity and relation extraction moved to the use of discriminative sequence models, which allowed the flexible specification of feature templates that are"
N09-1037,W08-2121,0,0.0802919,"Missing"
N09-1037,W05-0636,0,0.0462511,"these pieces of information, and system performance suffers. But, unThis paper begins to address this problem by building a joint model of both parsing and named entity recognition. Vapnik has observed (Vapnik, 1998; Ng and Jordan, 2002) that “one should solve the problem directly and never solve a more general problem as an intermediate step,” implying that building a joint model of two phenomena is more likely to harm performance on the individual tasks than to help it. Indeed, it has proven very difficult to build a joint model of parsing and semantic role labeling, either with PCFG trees (Sutton and McCallum, 2005) or with dependency trees. The CoNLL 2008 shared task (Surdeanu et al., 2008) was intended to be about joint dependency parsing and semantic role labeling, but the top performing systems decoupled the tasks and outperformed the systems which attempted to learn them jointly. Despite these earlier results, we found that combining parsing and named entity recognition modestly improved performance on both tasks. Our joint model produces an output which has consistent parse structure and named entity spans, and does a better job at both tasks than separate models with the same features. We first pr"
N09-1037,C08-1133,0,0.0216764,"Missing"
N09-1037,P08-1101,0,0.027566,"roaches. There have been other attempts in NLP to jointly model multiple levels of structure, with varying degrees of success. Most work on joint parsing and semantic role labeling (SRL) has been disappointing, despite obvious connections between the two tasks. Sutton and McCallum (2005) attempted to jointly model PCFG parsing and SRL for the CoNLL 2005 shared task, but were unable to improve performance on either task. The CoNLL 2008 shared task (Surdeanu et al., 2008) was joint dependency parsing and SRL, but the top performing systems decoupled the tasks, rather than building joint models. Zhang and Clark (2008) successfully built a joint VP VBD NP NP NNS PP IN NP NP PP IN NP NP NNS PP TO NP NML were members of the [Egyptian Islamic Jihad]PER ties with to NNP NNP NNP [Osama Bin Laden]PER (a) VP VBD NP NNS PP IN PP NP DT IN NP NamedEntity-ORG* NP PP NNS TO NP-PER* NNP-PER NNP-PER NNP-PER were members of the Egyptian Islamic Jihad with ties (b) to Osama Bin Laden Figure 4: An example for which the joint model helped with both parse structure and named entity recognition. The individual models (a) incorrectly attach the PP, label Egyptian Islamic Jihad as a person, and incorrectly add extra internal str"
N09-1068,W04-3237,0,0.0120507,"ween domains in the more structurally complicated task is inherently more difficult. Our model’s gains over the A LL DATA baseline are quite small, but we tested their significance using a sentence-level paired t-test (over all of the data combined) and found them to be significant at p < 10−5 . We are unsure why some domains improved while others did not. It is not simply a consequence of training set size, but may be due to qualities of the domains themselves. 5 Related Work We already discussed the relation of our work to (Daum´e III, 2007) in Section 2.4. Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. Their work is limited to two domains, a source and a target, and their algorithm has a two stage process: First, train a classifier on the source data, and then use the learned weights from that classifier as the mean for a Gaussian prior when training a new model on just the target data. Daum´e III and Marcu (2006) also took a Bayesian approach to domain adaptation, but structured their model in a very different way. In their model, it is assumed that each datum within a domain is either a domain-specific datum, or a general datum, and then domain-specific and g"
N09-1068,J03-4003,0,0.0178534,"ts of speech of the sentence. We used the same features as (McDonald et al., 2005), augmented with information about whether or not a dependent is the first dependent (information they did not have). 4.2 Data For our dependency parsing experiments, we used LDC2008T04 OntoNotes Release 2.0 data (Hovy et al., 2006). This dataset is still in development, and includes data from seven different domains, labeled for a number of tasks, including PCFG trees. The domains span both newswire and speech from multiple sources. We converted the PCFG trees into dependency trees using the Collins head rules (Collins, 2003). We also omitted the WSJ portion of the data, because it follows a different annotation scheme from the other domains.7 For each of the remaining six domains, we aimed for an 75/25 data split, but because we divided the data using the provided sections, this split was fairly rough. The number of training and test sentences for each domain are specified in the Table 3, along with our results. 4.3 Experimental Results and Discussion We compared the same four domain adaptation models for dependency parsing as we did for the named entity experiments, once again setting σ = 1.0 and σd = 0.1. Unlik"
N09-1068,P07-1033,0,0.838751,"Missing"
N09-1068,C96-1058,0,0.0722631,"t performs on a more structurally complex task than sequence modeling. To our knowledge, the discriminatively trained dependency model we used has not been previously published, but it is very similar to recent work on discriminative constituency parsing (Finkel and Manning, 2008). Due to space restrictions, we cannot give a complete treatment of the model, but will give an overview. We built a CRF-based model, optimizing the likelihood of the parse, conditioned on the words and parts of speech of the sentence. At the heart of our model is the Eisner dependency grammar chartparsing algorithm (Eisner, 1996), which allows for efficient computation of inside and outside scores. The Eisner algorithm, originally designed for generative parsing, decomposes the probability of a dependency parse into the probabilities of each attachment of a dependent to its parent, and the probabilities of each parent stopping taking dependents. These probabilities can be conditioned on the child, parent, and direction of the dependency. We used a slight modification of the algorithm which allows each probability to also be conditioned on whether there is a previous dependent. While the unmodified version of the algor"
N09-1068,P08-1109,1,0.292065,"Missing"
N09-1068,P05-1045,1,0.0097153,"pendent variances significantly improved performance, though we found no gains from using different values for the different domain specific variances. The values were set based on development data. 3 Named Entity Recognition For our first set of experiments, we used a linearchain, conditional random field (CRF) model, trained for named entity recognition (NER). The use of CRFs for sequence modeling has become standard so we will omit the model details; good explanations can be found in a number of places (Lafferty et al., 2001; Sutton and McCallum, 2007). Our features were based on those in (Finkel et al., 2005). 3.1 Data We used three named entity datasets, from the CoNLL 2003, MUC-6 and MUC-7 shared tasks. CoNLL is British newswire, while MUC-6 and MUC-7 are both American newswire. Arguably MUC-6 and MUC-7 should not count as separate domains, but because they were annotated separately, for different shared tasks, we chose to treat them as such, and feel that our experimental results justify the distinction. We used the standard train and test sets for each domain, which for CoNLL corresponds to the (more difficult) testb set. For details about the number of training and test words in each dataset,"
N09-1068,N06-2015,0,0.0206286,"nt to optimize our weights because our function evaluation was too slow to use L-BFGS. We did not encounter this problem in this setting. 608 parent, dependent (or none, if it is a stopping decision), direction of attachment, whether there is a previous dependent in that direction, and the words and parts of speech of the sentence. We used the same features as (McDonald et al., 2005), augmented with information about whether or not a dependent is the first dependent (information they did not have). 4.2 Data For our dependency parsing experiments, we used LDC2008T04 OntoNotes Release 2.0 data (Hovy et al., 2006). This dataset is still in development, and includes data from seven different domains, labeled for a number of tasks, including PCFG trees. The domains span both newswire and speech from multiple sources. We converted the PCFG trees into dependency trees using the Collins head rules (Collins, 2003). We also omitted the WSJ portion of the data, because it follows a different annotation scheme from the other domains.7 For each of the remaining six domains, we aimed for an 75/25 data split, but because we divided the data using the provided sections, this split was fairly rough. The number of tr"
N09-1068,P05-1012,0,0.0302317,"ll directed in both cases, it is just the underlying graphical model used to compute the likelihood of a parse which changes from a directed model to an undirected model. 6 In (Finkel and Manning, 2008) we used stochastic gradient descent to optimize our weights because our function evaluation was too slow to use L-BFGS. We did not encounter this problem in this setting. 608 parent, dependent (or none, if it is a stopping decision), direction of attachment, whether there is a previous dependent in that direction, and the words and parts of speech of the sentence. We used the same features as (McDonald et al., 2005), augmented with information about whether or not a dependent is the first dependent (information they did not have). 4.2 Data For our dependency parsing experiments, we used LDC2008T04 OntoNotes Release 2.0 data (Hovy et al., 2006). This dataset is still in development, and includes data from seven different domains, labeled for a number of tasks, including PCFG trees. The domains span both newswire and speech from multiple sources. We converted the PCFG trees into dependency trees using the Collins head rules (Collins, 2003). We also omitted the WSJ portion of the data, because it follows a"
N10-1075,W09-0701,0,0.0402762,"Missing"
N10-1075,J01-2001,0,0.0201892,"ild on many recent developments in computational morphology and NLP for Bantu languages. Badenhorst et al. (2009) found substantial variation in a speech recognition corpus for 9 Southern Bantu languages, where accurate models could also be built with limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 0.75 F-val Prec. Rec. 0.65 Label class KWF Final 0.403 0.684 0.265 0.796 0.842 0.599 Label/No-Label KWF Final 0.713 0.950 0.570 0.972 0.953 0.929 Table 4: Micro-F, precision and recall, compared with the o"
N10-1075,H05-1085,0,0.0622577,"Missing"
N10-1075,N09-1036,0,0.0148165,"phology and NLP for Bantu languages. Badenhorst et al. (2009) found substantial variation in a speech recognition corpus for 9 Southern Bantu languages, where accurate models could also be built with limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 0.75 F-val Prec. Rec. 0.65 Label class KWF Final 0.403 0.684 0.265 0.796 0.842 0.599 Label/No-Label KWF Final 0.713 0.950 0.570 0.972 0.953 0.929 Table 4: Micro-F, precision and recall, compared with the oracle keyword system. KWF = Oracle Keyword Filter. 0.55 Chichewa"
N10-1075,C08-1056,0,0.0127058,"too recent to test here, Pitman-Yor has been used for segmentation with accuracy comparable to the HDP model but with greater efficiency (Mochihashi et al., 2009). Biosurveillance systems currently use simple rule-based pre-processing for subword models. Dara et al. (2008) found only modest gains, although the data was limited to English. For text message classification, prior work is limited to identifying SPAM (Healy et al., 2005; Hidalgo et al., 2006; Cormack et al., 2007), where specialized algorithms and feature representations were also found to improve accuracy. For written variation, Kobus et al. (2008) focussed on SMSspecific abbreviations in French. Unlike their data, SMS-specific abbreviations were not present in our data. This is consistent with the reports on SMS practices in the related isiXhosa language (Deumert and Masinyana, 2008), but it may also be because the data we used contained professional communications not personal messages. 517 We have demonstrated that subword modeling in Chichewa leads to significant gains in classifying text messages according to medical labels, reducing the error from 1 in 4 to 1 in 20 in a system that should generalize to other languages with similar"
N10-1075,P09-1012,0,0.0226219,"not easily adapted to more complex morphologies, but we were able to test and evaluate Morfessor and the earlier Linguistica (Goldsmith, 2001). Both were more accurate for segmentation than our adaptation of Goldwater et al. (2009), but with lower recall. For the reasons discussed in Section 5.3 this meant less accuracy in classification. Goldwater et al. have also used the Pitman-Yor algorithm for morphological modeling (Goldwater et al., 2006). In results too recent to test here, Pitman-Yor has been used for segmentation with accuracy comparable to the HDP model but with greater efficiency (Mochihashi et al., 2009). Biosurveillance systems currently use simple rule-based pre-processing for subword models. Dara et al. (2008) found only modest gains, although the data was limited to English. For text message classification, prior work is limited to identifying SPAM (Healy et al., 2005; Hidalgo et al., 2006; Cormack et al., 2007), where specialized algorithms and feature representations were also found to improve accuracy. For written variation, Kobus et al. (2008) focussed on SMSspecific abbreviations in French. Unlike their data, SMS-specific abbreviations were not present in our data. This is consistent"
N10-1075,W09-0702,0,0.0210571,"Missing"
N10-1075,W09-0710,0,0.0136508,"ation, or any NLP task with the Chichewa, but we build on many recent developments in computational morphology and NLP for Bantu languages. Badenhorst et al. (2009) found substantial variation in a speech recognition corpus for 9 Southern Bantu languages, where accurate models could also be built with limited data. Morphological segmentation improved Swahili-English machine translation in De Pauw et al. (2009), even in the absense of gold standard reference segmentations, as was the case here. The complexity and necessity of modeling non-contiguous morphemes in Bantu languages is discussed by Pretorius et al. (2009). Computational morphology (Goldsmith, 2001; Creutz, 2006; Kurimo et al., 2008; Johnson and Goldwater, 2009; Goldwater et al., 2009) has begun to play a prominent role in machine translation and speech recognition for morphologically rich languages (Goldwater and McClosky, 2005; Tachbelie et al., 2009). In the current-state-of-the-art, a combination of the ParaMor (Monson et al., 2008) and Morfessor (Creutz, 2006) algorithms achieved 0.75 F-val Prec. Rec. 0.65 Label class KWF Final 0.403 0.684 0.265 0.796 0.842 0.599 Label/No-Label KWF Final 0.713 0.950 0.570 0.972 0.953 0.929 Table 4: Micro-F"
N10-1075,D09-1026,1,0.634205,"g that the failure of the word-optimal baseline model is not just due to a lack of training items. 5.5 Other models investigated Much recent work in text classification has been in machine-learning, comparing models over constant features. We tested SVMs and joint learning strategies. The gains were significant but small and did not closed the gap between systems with and without subword modeling. We therefore omit these for space and scope. However, one interesting result came from extending the feature space with topics derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al. (2009). This produced significant gains (micro-F=0.029), halving the remaining gap with the English system, but only when the topics were derived from modeling noncontiguous morpheme sequences, not words-alone or segmented morphemes. We found that the different surface forms of each word cooccurred less often 516 than chance (0.46 as often as chance for the different forms of odwala) forming disjunctive distributions. We suspect that this acts as a bias against robust unsupervised clustering of the different forms. 6 Related Work To our best knowledge, no prior researchers have worked on subword mod"
N10-1080,W08-0312,0,0.027162,"nfigurations of TERp, WER, several configurations of METEOR, as well as additive combinations of these metrics. The TERp configurations include the default configuration of TERp and TERpA: the configuration of TERp that was trained to match human judgments for NIST Metrics MATR (Matthew Snover and Schwartz, 2008; Przybocki et al., 2008). For METEOR, we used the standard METEOR English parameters (α = 0.8, β = 2.5, γ = 0.4), and the English parameters for the ranking METEOR (α = 0.95, β = 0.5, γ = 0.5),4 which was tuned to maximize the metric’s correlation with WMT-07 human ranking judgements (Agarwal and Lavie, 2008). The default METEOR parameters favor longer translations than the other metrics, since high α values place much more weight on unigram recall than precision. Since this may put models tuned to METEOR at a disadvantage when being evaluated by the other metrics, we also use a variant of the standard English model and of ranking METEOR with α set to 0.5, as this weights both recall and precision equally. For each iteration of MERT, 20 random restarts were used in addition to the best performing point discovered during earlier iterations of training.5 4 Agarwal and Lavie (2008) report γ = 0.45, h"
N10-1080,E06-1032,0,0.137662,"c. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR1 (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006)."
N10-1080,D09-1030,0,0.210252,".0 NIST vs. BLEU:4 51.5 WER vs. TERp 51.5 METR:0.5 vs METR 51.5 TERp vs. BLEU:4 51.0 BLEU:4 vs. METR R:0.5 50.5 p-value 0.0028 0.02 0.089 0.089 0.11 0.11 0.11 0.22 0.26 0.26 0.31 0.31 0.36 0.36 0.47 < 0.001 0.052 0.069 0.11 0.14 0.36 0.36 0.36 0.42 0.47 Table 5: Select pairwise preference for models trained to different evaluation metrics. For A vs. B, preferred indicates how often A was preferred to B. We bold the better training metric for statistically significant differences. duced by experts by having multiple workers complete each HIT and then combining their answers (Snow et al., 2008; Callison-Burch, 2009). We perform a pairwise comparison of the translations produced for the first 200 sentences of our Chinese to English test data (MT03) and our Arabic to English test data (dev07). The HITs consist of a pair of machine translated sentences and a single human generated reference translation. The reference is chosen at random from those available for each sentence. Capitalization of the translated sentences is restored using an HMM based truecaser (Lita et al., 2003). Turkers are instructed to “. . . select the machine translation generated sentence that is easiest to read and best conveys what i"
N10-1080,N10-2003,1,0.719283,"hat restart points are provided, we use the same series of random restart points for each model. During each iteration of MERT, the random seed is based on the MERT iteration number. Thus, while a different set of random points is selected during each MERT iteration, on any given iteration all models use the same set of points. This prevents models from doing better or worse just because they received different starting points. However, it is still possible that certain random starting points are better for some evaluation metrics than others. 4 Experiments Experiments were run using Phrasal (Cer et al., 2010), a left-to-right beam search decoder that achieves a matching BLEU score to Moses (Koehn et al., 2007) on a variety of data sets. During decoding we made use of a stack size of 100, set the distortion limit to 6, and retrieved 20 translation options for each unique source phrase. Using the selected metrics, we train both Chinese to English and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the"
N10-1080,W08-0336,1,0.371777,"-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P (e|f ) < 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6 Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. 558 Manning, 2008). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) using all of the English material from the parallel data employed to train the phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008"
N10-1080,D08-1024,0,0.0433274,"e language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a num"
N10-1080,N09-1025,0,0.0142183,"rtance the metric assigns to precision and recall. The fact that the WER, TER and TERp models perform very similarly suggests that current phrasebased translation systems lack either the features or the model structure to take advantage of swap edit operations. The situation might be improved by using a model that does a better job of both capturing the structure of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here. However, like other metrics, we expect performance gains still will be sensitiv"
N10-1080,D08-1089,1,0.909049,"Missing"
N10-1080,P06-1121,0,0.0202158,"o METEOR can be made more robust by setting α to 0.5, which 562 balances the importance the metric assigns to precision and recall. The fact that the WER, TER and TERp models perform very similarly suggests that current phrasebased translation systems lack either the features or the model structure to take advantage of swap edit operations. The situation might be improved by using a model that does a better job of both capturing the structure of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here"
N10-1080,N03-1017,0,0.0603839,"ned using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based on a well ranking 2009 NIST submission (Galley et al., 2009). The phrase table was extracted using all of the allowed resources for the constrained Arabic to English track. Word alignment was performed using the Berkeley cross-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P (e|f ) < 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6 Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. 558 Manning, 2008). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2"
N10-1080,P07-2045,0,0.0214612,"ng each iteration of MERT, the random seed is based on the MERT iteration number. Thus, while a different set of random points is selected during each MERT iteration, on any given iteration all models use the same set of points. This prevents models from doing better or worse just because they received different starting points. However, it is still possible that certain random starting points are better for some evaluation metrics than others. 4 Experiments Experiments were run using Phrasal (Cer et al., 2010), a left-to-right beam search decoder that achieves a matching BLEU score to Moses (Koehn et al., 2007) on a variety of data sets. During decoding we made use of a stack size of 100, set the distortion limit to 6, and retrieved 20 translation options for each unique source phrase. Using the selected metrics, we train both Chinese to English and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based"
N10-1080,N06-1014,0,0.0140937,"and Arabic to English models.6 The Chinese to English models are trained using NIST MT02 and evaluated on NIST MT03. The Arabic to English experiments use NIST MT06 for training and GALE dev07 for evaluation. The resulting models are scored using all of the standalone metrics used during training. 4.1 Arabic to English Our Arabic to English system was based on a well ranking 2009 NIST submission (Galley et al., 2009). The phrase table was extracted using all of the allowed resources for the constrained Arabic to English track. Word alignment was performed using the Berkeley cross-EM aligner (Liang et al., 2006). Phrases were extracted using the grow heuristic (Koehn et al., 2003). However, we threw away all phrases that have a P (e|f ) < 0.0001 in order to reduce the size of the phrase table. From the aligned data, we also extracted a hierarchical reordering model that is similar to popular lexical reordering models (Koehn et al., 2007) but that models swaps containing more than just one phrase (Galley and returned during an earlier iteration of MERT. 6 Given the amount of time required to train a TERpA model, we only present TERpA results for Chinese to English. 558 Manning, 2008). A 5-gram languag"
N10-1080,P03-1020,0,0.00807775,"differences. duced by experts by having multiple workers complete each HIT and then combining their answers (Snow et al., 2008; Callison-Burch, 2009). We perform a pairwise comparison of the translations produced for the first 200 sentences of our Chinese to English test data (MT03) and our Arabic to English test data (dev07). The HITs consist of a pair of machine translated sentences and a single human generated reference translation. The reference is chosen at random from those available for each sentence. Capitalization of the translated sentences is restored using an HMM based truecaser (Lita et al., 2003). Turkers are instructed to “. . . select the machine translation generated sentence that is easiest to read and best conveys what is stated in the reference”. Differences between the two machine translations are emphasized by being underlined and bold faced.9 The resulting HITs are made available only to workers in the United States, as pilot experiments indicated this results in more consistent preference judgments. Three preference judgments are obtained for each pair of translations and are combined using weighted majority vote. As shown in table 5, in many cases the quality of the transla"
N10-1080,niessen-etal-2000-evaluation,0,0.0382506,"to tune the metric to human judgments on a specific language and variation of the evaluation task (e.g., ranking candidate translations vs. reproducing judgments of translations adequacy and fluency). 2.3 Translation Edit Rate TER (Snover et al., 2006) searches for the shortest sequence of edit operations needed to turn a candidate translation into one of the reference translations. The allowable edits are the insertion, deletion, and substitution of individual words and swaps of adjacent sequences of words. The swap operation differentiates TER from the simpler word error rate (WER) metric (Nießen et al., 2000), which only makes use of insertions, deletions, and substitutions. Swaps prevent phrase reorderings from being excessively penalized. Once the shortest sequence of operations is found,3 TER is calculated simply as the number of required edits divided by the reference translation length, or average reference translation length when multiple are available (4). TER = min edits avg ref length (4) TER-Plus (TERp) (Snover et al., 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric’s agreement with human judgments. TERp also introduces three new edit"
N10-1080,J03-1002,0,0.00225018,"ing toolkit (Stolcke, 2002) using all of the English material from the parallel data employed to train the phrase table as well as Xinhua Chinese English Parallel News (LDC2002E18).7 The resulting decoding model has 16 features that are optimized during MERT. 4.2 Chinese to English For our Chinese to English system, our phrase table was built using 1,140,693 sentence pairs sampled from the GALE Y2 training data. The Chinese sentences were word segmented using the 2008 version of Stanford Chinese Word Segmenter (Chang et al., 2008; Tseng et al., 2005). Phrases were extracted by running GIZA++ (Och and Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic (Koehn et al., 2003). From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Koehn et al., 2007). A 5-gram language model was created with the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. The resulting decoding model has 14 features to be trained. 5 Results As seen in tables 1 and 2, the evaluation metric we use during training has a substantia"
N10-1080,P03-1021,0,0.771788,"based metrics like TER or WER. Human preferences for METEOR trained models varies depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgment"
N10-1080,P02-1040,0,0.0968379,"ole in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate"
N10-1080,2006.amta-papers.25,0,0.790114,"tively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR1 (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006). Their better correlations suggest that we might obtain higher quality translations by making use of these new metrics when training our models. We expect that training on a specific metric will produce the best performing model according to that met1 MET"
N10-1080,W09-0441,0,0.314159,"g translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However, BLEU does have a number of shortcomings. It doesn’t penalize n-gram scrambling (Callison-Burch et al., 2006), and since it isn’t aware of synonymous words or phrases, it can inappropriately penalize translations that use them. Recently, there have been efforts to develop better evaluation metrics. Metrics such as Translation Edit Rate (TER) (Snover et al., 2006; Snover et al., 2009) and METEOR1 (Lavie and Denkowski, 2009) perform a more sophisticated analysis of the translations being evaluated and the scores they produce tend to achieve a better correlation with human judgments than those produced by BLEU (Snover et al., 2009; Lavie and Denkowski, 2009; Przybocki et al., 2008; Snover et al., 2006). Their better correlations suggest that we might obtain higher quality translations by making use of these new metrics when training our models. We expect that training on a specific metric will produce the best performing model according to that met1 METEOR: Metric for Evalua"
N10-1080,D08-1027,1,0.129613,"Missing"
N10-1080,D07-1080,0,0.0464296,"depending on the source language. Since using BLEU or NIST produces models that are more robust to evaluation by other metrics and perform well in human judgments, we conclude they are still the best choice for training. 1 Introduction Since their introduction, automated measures of machine translation quality have played a critical role in the development and evolution of SMT systems. While such metrics were initially intended for evaluation, popular training methods such as minimum error rate training (MERT) (Och, 2003) and margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003; Watanabe et al., 2007; Chiang et al., 2008) train translation models toward a specific evaluation metric. This makes the quality of the resulting model dependent on how accurately the automatic metric actually reflects human preferences. The most popular metric for both comparing systems and tuning MT models has been BLEU. While BLEU (Papineni et al., 2002) is relatively simple, scoring translations according to their n-gram overlap with reference translations, it still achieves a reasonable correlation with human judgments of translation quality. It is also robust enough to use for automatic optimization. However"
N10-1080,D09-1006,0,0.0246061,"ture of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al., 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. Human results indicate that edit distance trained models such as WER and TERp tend to produce lower quality translations than BLEU or NIST trained models. Tuning to METEOR works reasonably well for Chinese, but is not a good choice for Arabic. We suspect that the newer RYPT metric (Zaidan and Callison-Burch, 2009), which directly makes use of human adequacy judgements of substrings, would obtain better human results than the automated metrics presented here. However, like other metrics, we expect performance gains still will be sensitive to how the mechanics of the metric interact with the structure and feature set of the decoding model being used. BLEU and NIST’s strong showing in both the machine and human evaluation results indicates that they are still the best general choice for training model parameters. We emphasize that improved metric correlations with human judgments do not imply that models"
N10-1080,I05-3027,1,\N,Missing
N10-1091,C96-1058,0,0.66391,"Missing"
N10-1091,D07-1097,0,0.436482,"29 83.02 76.18 82.02 Table 1: Labeled attachment scores (LAS) and unlabeled attachment scores (UAS) for the base models. The parsers are listed in descending order of LAS in the development partition. Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance, the previous work has left several questions unanswered. Here we answer the following questions, in the context of English dependency parsing: 1. When co"
N10-1091,W99-0623,0,0.128418,"partition. Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance, the previous work has left several questions unanswered. Here we answer the following questions, in the context of English dependency parsing: 1. When combining models at parsing time, what is the best scoring model for candidate dependencies during re-parsing? Can a meta classifier improve over unsupervised voting? 2. Are (potentially-expensive) re"
N10-1091,W08-2123,0,0.0225964,"Missing"
N10-1091,P08-1108,0,0.669281,".95 85.96 88.64 85.61 88.14 85.36 88.06 83.90 86.70 83.53 86.17 82.51 85.35 Out of domain LAS UAS 80.48 86.08 78.74 84.18 78.55 83.68 77.23 82.39 76.69 82.57 77.29 83.02 76.18 82.02 Table 1: Labeled attachment scores (LAS) and unlabeled attachment scores (UAS) for the base models. The parsers are listed in descending order of LAS in the development partition. Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance,"
N10-1091,N06-2033,0,0.372368,"3 82.39 76.69 82.57 77.29 83.02 76.18 82.02 Table 1: Labeled attachment scores (LAS) and unlabeled attachment scores (UAS) for the base models. The parsers are listed in descending order of LAS in the development partition. Introduction Several ensemble models have been proposed for the parsing of syntactic dependencies. These approaches can generally be classified in two categories: models that integrate base parsers at learning time, e.g., using stacking (Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009), and approaches that combine independently-trained models only at parsing time (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009). In the latter case, the correctness of the final dependency tree is ensured by: (a) selecting entire trees proposed by the base parsers (Henderson and Brill, 1999); or (b) re-parsing the pool of dependencies proposed by the base models (Sagae and Lavie, 2006). The latter approach was shown to perform better for constituent parsing (Henderson and Brill, 1999). While all these models achieved good performance, the previous work has left several questions unanswered. Here we answer the following questions, in the context of English dependency"
N10-1091,W08-2121,1,0.202862,"When combining models at parsing time, what is the best scoring model for candidate dependencies during re-parsing? Can a meta classifier improve over unsupervised voting? 2. Are (potentially-expensive) re-parsing strategies justified for English? What percentage of trees are not well-formed if one switches to a light word-by-word voting scheme? 3. How important is the integration of base parsers at learning time? 4. How do ensemble models compare against state-of-the-art supervised parsers? 2 Setup In our experiments we used the syntactic dependencies from the CoNLL 2008 shared task corpus (Surdeanu et al., 2008). We used seven base parsing models in this paper: six are variants of the Malt parser1 and the seventh is the projective version of MSTParser that uses only first-order features2 (or MST for short). The six Malt 1 http://maltparser.org/ http://sourceforge.net/projects/ mstparser/ 2 649 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 649–652, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics Unweighted # of parsers 3 4 5 6 7 LAS 86.03 86.79 86.98 87.14 86.81 UAS 89.44 90.14 90.33 90.51 90.21 Weighted by"
N10-1091,N09-2066,0,\N,Missing
N10-1129,P06-1067,0,0.47311,"mpare the IBM and ITG constraints, concluding that although the ITG constraints permit more flexible re-orderings, the IBM constraints result in higher BLEU scores. Since our work falls under the IBM paradigm, we consider cost models for those constraints. We have said that linear distortion is the simplest cost model. The primary criticism of linear distortion is that it is unlexicalized, thus penalizing all re-orderings equally (Khalilov et al., 2009). When extended to phrases as in Equation (1), linear distortion is also agnostic to internal phrase alignments. To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. Maximum likelihood estimates for inbound, outbound, and pairwise transitions are computed from automatic word alignments. But no estimate of future cost is included, and their model cannot easily accommodate features defined over the entire translation sequence. As for experimental results, they use a distortion limit that is half of what we report, and compare against a baseline that lacks a distortion model entirely. Neither their model nor ours requires generation of lattices prior to search (Zhang et al., 2007; Niehues and Kolss, 2009)."
N10-1129,W09-0437,0,0.0109281,"SBJ            ! Followers of all of the Christian and Islamic sects engaged in waiting for them Figure 1: The oracle translation for this Arabic VOS sentence would be pruned during search using typical distortion parameters. The Arabic phrases read right-to-left, but we have ordered the sentence from left-to-right in order to clearly illustrate the re-ordering problem. ders in which significant re-ordering is required, the distortion limit can eliminate the oracle, or “best,” translation prior to search, placing an artificial limit on translation performance (Auli et al., 2009). To illustrate this problem, consider the ArabicEnglish example in Figure 1. Assuming that the English translation is constructed left-to-right, the verb CAJ shaaraka must be translated after the noun phrase (NP) subject. If P phrases are used to translate the Arabic source s to the English target t, then the (unsigned) linear distortion is given by Introduction D(s, t) = p1f irst + It is well-known that translation performance in Moses-style (Koehn et al., 2007) machine translation (MT) systems deteriorates when high distortion is allowed. The linear distortion cost model used in these syst"
N10-1129,P08-1087,0,0.00964604,"tion is required. By not modeling source side features like agreement— which, in Arabic, appears between both verb and  step k 0 1 2 3 4     Fk 3 5 7 0 0 ∆cost 3 2 2 −7 0 dlimit-4 D(s, t) 1 0 0 4 3 8 D(s, t) + ∆cost 4 2 2 −3 3 8 Figure 2: Translation sequence in which the distortion limit is reached and the decoder is forced to cover the first skipped word. Future cost estimation penalizes the two monotone steps, yet total distortion cost remains unchanged. subject, and adjective and noun—baseline phrasebased systems rely on the language model to specify an appropriate target word order (Avramidis and Koehn, 2008). Returning to Figure 1, we could have an alternate hypothesis They waited for the followers of the Christian and Islamic sects, which is acceptable English and has low distortion, but is semantically inconsistent with the Arabic. 3 The Cost Model In this section we describe the new distortion cost model, which has four independent components. 3.1 Future Cost Estimation Despite its lack of sophistication, linear distortion is a surprisingly effective baseline cost model for phrase-based MT systems. It can be computed in constant time, gives non-decreasing values that are good for search, and d"
N10-1129,N10-2003,1,0.167101,"Missing"
N10-1129,J07-2003,0,0.0151208,"probabilities for the outermost classes. Noise in the alignments along with the few cases of longdistance movement are penalized heavily. For Arabic, this property works in our favor as we do not want extreme movement (as we might with Chinese or German). But C OUNTS applies a uniform penalty for all movement that exceeds the outermost class boundaries, making it more prone to search errors than even linear distortion despite its favorable performance when tested in isolation. Finally, we note that previous attempts to improve re-ordering during search (particularly long-distance re-ordering (Chiang, 2007)) have delivered remarkable gains for languages like Chinese, but improvements for Arabic have been less exceptional. By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). 6 Prior Work There is an expansive literature on re-ordering in statistical MT. We first review the development of re-ordering constraints, then describe previous cost models for those constraints in beam search decoders. Because we allow re-o"
N10-1129,P05-1066,0,0.0967593,"ituents must often be identified and precisely re-ordered. The VOS configuration is especially challenging for Arabic-English MT. It usually appears when the direct object is short—e.g., pronominal—and the subject is long. For example, translation of the VOS sentence in Figure 1 requires both a high distortion limit to accommodate the subject movement and tight restrictions on the movement of the PP. The particularity of these requirements in Arabic and other languages, and the difficulty of modeling them in phrase-based systems, has inspired significant work in source language preprocessing (Collins et al., 2005; Habash and Sadat, 2006; Habash, 2007). Finally, we observe that target language models cannot always select appropriate translations when basic word order transformation is required. By not modeling source side features like agreement— which, in Arabic, appears between both verb and  step k 0 1 2 3 4     Fk 3 5 7 0 0 ∆cost 3 2 2 −7 0 dlimit-4 D(s, t) 1 0 0 4 3 8 D(s, t) + ∆cost 4 2 2 −3 3 8 Figure 2: Translation sequence in which the distortion limit is reached and the decoder is forced to cover the first skipped word. Future cost estimation penalizes the two monotone steps, yet total d"
N10-1129,D08-1089,1,0.917659,"score. When we say linear distortion, we refer to the “simple distortion model” of Koehn et al. (2003) that is shown in Equation (1) and is converted to a cost by multiplying by −1. When extended to phrases,  868 the key property of this model is that monotone decoding gives the least costly translation path. Reorderings internal to extracted phrases are not penalized. In practice, we commonly see n-best lists of hypotheses with linear distortion costs equal to zero. More sophisticated local phrase re-ordering models have been proposed (Tillmann, 2004; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008), but these are typically used in addition to linear distortion. 2.2 Arabic Linguistic Essentials In this paper we use Arabic-English as a case study since we possess a strong experimental baseline. But we expect that the technique presented could be even more effective for high distortion language pairs such as Chinese-English and Hindi-English. Since the analysis that follows is framed in terms of Arabic, we point out several linguistic features that motivate our approach. From the perspective of the three criteria used to specify basic word order typology (Greenberg, 1966), Arabic is somewh"
N10-1129,N04-1039,0,0.0490678,"mit (Figure 5). As expected, future cost estimation alone does not increase performance at the lower distortion limit. We also observe that the effect of conditioning on evidence is significant: the C OUNTS model is categorically worse than all other models. To understand why, we randomly sampled 500 sentences from the excluded UN data and computed the log-likelihoods of the alignments according to the different models.7 In this test, C OUNTS is clearly better with a score of 7 We approximated linear distortion using a Laplacian distribution with estimated parameters µ ˆ = 0.51 and ˆb = 1.76 (Goodman, 2004). 873 −23388 versus, for example, the inbound model at −38244. The explanation is due in part to optimization. The two discriminative models often give very low probabilities for the outermost classes. Noise in the alignments along with the few cases of longdistance movement are penalized heavily. For Arabic, this property works in our favor as we do not want extreme movement (as we might with Chinese or German). But C OUNTS applies a uniform penalty for all movement that exceeds the outermost class boundaries, making it more prone to search errors than even linear distortion despite its favor"
N10-1129,N06-2013,0,0.00623196,"identified and precisely re-ordered. The VOS configuration is especially challenging for Arabic-English MT. It usually appears when the direct object is short—e.g., pronominal—and the subject is long. For example, translation of the VOS sentence in Figure 1 requires both a high distortion limit to accommodate the subject movement and tight restrictions on the movement of the PP. The particularity of these requirements in Arabic and other languages, and the difficulty of modeling them in phrase-based systems, has inspired significant work in source language preprocessing (Collins et al., 2005; Habash and Sadat, 2006; Habash, 2007). Finally, we observe that target language models cannot always select appropriate translations when basic word order transformation is required. By not modeling source side features like agreement— which, in Arabic, appears between both verb and  step k 0 1 2 3 4     Fk 3 5 7 0 0 ∆cost 3 2 2 −7 0 dlimit-4 D(s, t) 1 0 0 4 3 8 D(s, t) + ∆cost 4 2 2 −3 3 8 Figure 2: Translation sequence in which the distortion limit is reached and the decoder is forced to cover the first skipped word. Future cost estimation penalizes the two monotone steps, yet total distortion cost remains u"
N10-1129,2007.mtsummit-papers.29,0,0.0935294,"re-ordered. The VOS configuration is especially challenging for Arabic-English MT. It usually appears when the direct object is short—e.g., pronominal—and the subject is long. For example, translation of the VOS sentence in Figure 1 requires both a high distortion limit to accommodate the subject movement and tight restrictions on the movement of the PP. The particularity of these requirements in Arabic and other languages, and the difficulty of modeling them in phrase-based systems, has inspired significant work in source language preprocessing (Collins et al., 2005; Habash and Sadat, 2006; Habash, 2007). Finally, we observe that target language models cannot always select appropriate translations when basic word order transformation is required. By not modeling source side features like agreement— which, in Arabic, appears between both verb and  step k 0 1 2 3 4     Fk 3 5 7 0 0 ∆cost 3 2 2 −7 0 dlimit-4 D(s, t) 1 0 0 4 3 8 D(s, t) + ∆cost 4 2 2 −3 3 8 Figure 2: Translation sequence in which the distortion limit is reached and the decoder is forced to cover the first skipped word. Future cost estimation penalizes the two monotone steps, yet total distortion cost remains unchanged. subje"
N10-1129,W05-0831,0,0.0236372,"were first introduced by Berger et al. (1996) in the context of the IBM translation models. The IBM constraints treat the source word sequence as a coverage set C that is processed sequentially. A source token is “covered” when it is aligned with a new target token. For a fixed value of k, we may leave up to k − 1 positions uncovered and return to them later. We can alter the constraint slightly such that for the first uncovered position u ∈ / C we can cover position j when j−u<k j∈ /C which is the definition of the distortion limit used in Moses. Variations of the IBM constraints also exist (Kanthak et al., 2005), as do entirely different regimes like the hierarchical ITG constraints, which represent the source as a sequence of blocks that can be iteratively merged and inverted (Wu, 1996). Zens and Ney (2003) exhaustively compare the IBM and ITG constraints, concluding that although the ITG constraints permit more flexible re-orderings, the IBM constraints result in higher BLEU scores. Since our work falls under the IBM paradigm, we consider cost models for those constraints. We have said that linear distortion is the simplest cost model. The primary criticism of linear distortion is that it is unlexi"
N10-1129,W09-2310,0,0.011333,"hical ITG constraints, which represent the source as a sequence of blocks that can be iteratively merged and inverted (Wu, 1996). Zens and Ney (2003) exhaustively compare the IBM and ITG constraints, concluding that although the ITG constraints permit more flexible re-orderings, the IBM constraints result in higher BLEU scores. Since our work falls under the IBM paradigm, we consider cost models for those constraints. We have said that linear distortion is the simplest cost model. The primary criticism of linear distortion is that it is unlexicalized, thus penalizing all re-orderings equally (Khalilov et al., 2009). When extended to phrases as in Equation (1), linear distortion is also agnostic to internal phrase alignments. To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. Maximum likelihood estimates for inbound, outbound, and pairwise transitions are computed from automatic word alignments. But no estimate of future cost is included, and their model cannot easily accommodate features defined over the entire translation sequence. As for experimental results, they use a distortion limit that is half of what we report, and compare against a"
N10-1129,N03-1017,0,0.00905336,"eterization. 2 2.1 Background Search in Phrase-based MT  Given a J token source input string f = fiJ , we  I seek the most probable I token translation e = ei . The Moses phrase-baseddecoder models the posterior probability pλ eI1 |f1J directly according to a log-linear model (Och and Ney, 2004), which gives the decision rule ( M ) X  I J eˆ = arg max λm hm e1 , f1 I,eI1 where hm eI1 , f1J m=1 are M arbitrary feature functions over sentence pairs, and λm are feature weights set using a discriminative training method like MERT (Och, 2003). This search is made tractable by the use of beams (Koehn et al., 2003). Hypotheses are pruned from the beams according the sum of the current model score and a future cost estimate for the uncovered source words. Since the number of reordering possibilities for those words is very large— in theory it is exponential—an inadmissible heuristic is typically used to estimate future cost. The baseline distortion cost model is a weighted feature in this framework and affects beam pruning only through the current model score. When we say linear distortion, we refer to the “simple distortion model” of Koehn et al. (2003) that is shown in Equation (1) and is converted to"
N10-1129,P07-2045,0,0.0321216,"ion limit can eliminate the oracle, or “best,” translation prior to search, placing an artificial limit on translation performance (Auli et al., 2009). To illustrate this problem, consider the ArabicEnglish example in Figure 1. Assuming that the English translation is constructed left-to-right, the verb CAJ shaaraka must be translated after the noun phrase (NP) subject. If P phrases are used to translate the Arabic source s to the English target t, then the (unsigned) linear distortion is given by Introduction D(s, t) = p1f irst + It is well-known that translation performance in Moses-style (Koehn et al., 2007) machine translation (MT) systems deteriorates when high distortion is allowed. The linear distortion cost model used in these systems is partly at fault. It includes no estimate of future distortion cost, thereby increasing the risk of search errors. Linear distortion also penalizes all re-orderings equally, even when appropriate re-orderings are performed. Because linear distortion, which is a soft constraint, does not effectively constrain search, a distortion limit is imposed on the translation model. But hard constraints are ultimately undesirable since they prune the search space. For la"
N10-1129,W04-3250,0,0.0170905,"e Arabic source is written right-to-left. bilities, and the alignment penalty. The main objective of this paper is to improve performance at very high distortion limits. Table 4 shows performance at a distortion limit of 15. To the set of baselines we add L EX, which is the lexicalized re-ordering model of Galley and Manning (2008). This model was shown to outperform other lexicalized re-ordering models in common use. Statistical significance was computed with the approximate randomization test of Riezler and Maxwell (2005), which is less sensitive to Type I errors than bootstrap re-sampling (Koehn, 2004). 5 Discussion The new distortion cost model allows us to triple the distortion limit while maintaining a statistically significant improvement over the M OSES L INEAR baseline at the lower distortion limit for three of the four test sets. More importantly, we can raise the distortion limit in the D ISCRIM +F UTURE configuration at minimal cost: a statistically insignificant −0.2 BLEU performance decrease on average. We also see a considerable improvement over both the M OSES L INEAR and L EX baselines at the high distortion limit (Figure 5). As expected, future cost estimation alone does not"
N10-1129,N06-1014,0,0.367963,"ighted probabilities), word penalty, phrase penalty, linear distortion, and language model score. We disable baseline linear distortion when evaluating the other distortion cost models. To tune parameters, we run MERT with the Downhill Simplex algorithm on the MT04 dataset. For all models, we use 20 random starting points and generate 300-best lists. We use the NIST MT09 constrained track training data, but remove the UN and comparable data.6 The reduced training bitext has 181k aligned sentences with 6.20M English and 5.73M Arabic tokens. We create word alignments using the Berkeley Aligner (Liang et al., 2006) and take the intersection of the alignments in both directions. Phrase pairs with a maximum target or source length of 7 tokens are extracted using the method of Och and Ney (2004). We build a 5-gram language model from the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40), in addition to all of the target side training data permissible in the NIST MT09 constrained competition. We manually remove Giga6 Removal of the UN data does not affect the baseline at a distortion limit of 5, and lowers the higher distortion baseline by −1.40 BLEU. The NIST MT09 data is available at http://www."
N10-1129,P08-1114,0,0.0476741,"boundaries, making it more prone to search errors than even linear distortion despite its favorable performance when tested in isolation. Finally, we note that previous attempts to improve re-ordering during search (particularly long-distance re-ordering (Chiang, 2007)) have delivered remarkable gains for languages like Chinese, but improvements for Arabic have been less exceptional. By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). 6 Prior Work There is an expansive literature on re-ordering in statistical MT. We first review the development of re-ordering constraints, then describe previous cost models for those constraints in beam search decoders. Because we allow re-ordering during search, we omit discussion of the many different methods for preprocessing the source input prior to monotonic translation. Likewise, we do not recite prior work in re-ranking translations. Re-ordering constraints were first introduced by Berger et al. (1996) in the context of the IBM translation models. The IBM constraints treat the sour"
N10-1129,2007.mtsummit-papers.43,0,0.174814,"el.2 By definition, the model has a least cost translation path: monotone. Therefore, we can add to the baseline calculation D(s, t) the cost of skipping back to the first uncovered source word and then translating the remaining positions monotonically. It can be verified by induction on |C |that this is an admissible heuristic. Formally, let j represent the first uncovered index in the source coverage set C. Let C j represent the subset of C starting from position j. Finally, let j 0 represent the leftmost position in phrase p applied at translation step k. Then the future cost estimate Fk 2 Moore and Quirk (2007) propose an alternate future cost formulation. However, their model seems prone to the same deterioration in performance shown in Table 1. They observed decreased translation quality above a distortion limit of 5. is Fk = |C j |+ (j 0 BASELINE F UTURE C OST + |p |+ 1 − j) if > j 0 otherwise j0 For k > 0, we add the difference between the current future cost estimate and the previous cost estimate ∆cost = Fk − Fk−1 to the linear penalty D(s, t).3 Table 2 shows that, as expected, the difference between the baseline and augmented models is statistically insignificant at a low distortion limit. Ho"
N10-1129,W09-0435,0,0.0105672,"aizan and Papineni (2006) proposed a lexicalized, generative distortion model. Maximum likelihood estimates for inbound, outbound, and pairwise transitions are computed from automatic word alignments. But no estimate of future cost is included, and their model cannot easily accommodate features defined over the entire translation sequence. As for experimental results, they use a distortion limit that is half of what we report, and compare against a baseline that lacks a distortion model entirely. Neither their model nor ours requires generation of lattices prior to search (Zhang et al., 2007; Niehues and Kolss, 2009). Lexicalized re-ordering models are the other significant approach to re-ordering. These models make local predictions about the next phrase to be translated during decoding, typically assigning costs to one of three categories: monotone, swap, or discontinuous. Both generative (Tillmann, 2004; Och and Ney, 2004; Koehn et al., 2007) and discriminative training (Tillmann and Zhang, 2005; Zens and 874 Ney, 2006; Liang et al., 2006) algorithms have been proposed. Recently, Galley and Manning (2008) introduced a hierarchical model capable of analyzing alignments beyond adjacent phrases. Our discr"
N10-1129,J04-4002,0,0.873905,"s. Together these two extensions allow us to triple the distortion limit in our NIST MT09 Arabic-English system while maintaining a statistically significant improvement over the low distortion baseline. At the high distortion limit, we also show a +2.32 BLEU average gain over Moses with an equivalent distortion parameterization. 2 2.1 Background Search in Phrase-based MT  Given a J token source input string f = fiJ , we  I seek the most probable I token translation e = ei . The Moses phrase-baseddecoder models the posterior probability pλ eI1 |f1J directly according to a log-linear model (Och and Ney, 2004), which gives the decision rule ( M ) X  I J eˆ = arg max λm hm e1 , f1 I,eI1 where hm eI1 , f1J m=1 are M arbitrary feature functions over sentence pairs, and λm are feature weights set using a discriminative training method like MERT (Och, 2003). This search is made tractable by the use of beams (Koehn et al., 2003). Hypotheses are pruned from the beams according the sum of the current model score and a future cost estimate for the uncovered source words. Since the number of reordering possibilities for those words is very large— in theory it is exponential—an inadmissible heuristic is typi"
N10-1129,P03-1021,0,0.0282777,"LEU average gain over Moses with an equivalent distortion parameterization. 2 2.1 Background Search in Phrase-based MT  Given a J token source input string f = fiJ , we  I seek the most probable I token translation e = ei . The Moses phrase-baseddecoder models the posterior probability pλ eI1 |f1J directly according to a log-linear model (Och and Ney, 2004), which gives the decision rule ( M ) X  I J eˆ = arg max λm hm e1 , f1 I,eI1 where hm eI1 , f1J m=1 are M arbitrary feature functions over sentence pairs, and λm are feature weights set using a discriminative training method like MERT (Och, 2003). This search is made tractable by the use of beams (Koehn et al., 2003). Hypotheses are pruned from the beams according the sum of the current model score and a future cost estimate for the uncovered source words. Since the number of reordering possibilities for those words is very large— in theory it is exponential—an inadmissible heuristic is typically used to estimate future cost. The baseline distortion cost model is a weighted feature in this framework and affects beam pruning only through the current model score. When we say linear distortion, we refer to the “simple distortion model” o"
N10-1129,2001.mtsummit-papers.68,0,0.0335555,"iods that overlapped with the development and test sets. The language model is smoothed with the modified Kneser-Ney algorithm, retaining only trigrams, 4grams, and 5-grams that occurred two, three, and three times, respectively, in the training data. We remove from the test sets source tokens not present in the phrase tables. For the discriminative distortion models, we tag the pre-processed input using the log-linear POS tagger of Toutanova et al. (2003). After decoding, we strip any punctuation that appears at the beginning of a translation. 4.2 Results In Table 3 we report uncased BLEU-4 (Papineni et al., 2001) scores at the distortion limit (5) of our most competitive baseline Arabic-English system. M OSES L INEAR uses the linear distortion model present in Moses. C OUNTS is a separate baseline with a discrete cost model that uses unlexicalized maximum likelihood estimates for the same classes present in the discriminative model. To show the effect of the components in our combined distortion model, we give separate results for linear distortion with future cost estimation (F UTURE) and for the combined discriminative distortion model (D IS CRIM +F UTURE) with all four features: linear distortion w"
N10-1129,W05-0908,0,0.164027,"e translation (M OSES L INEAR-d15). D ISCRIM +F UTURE (dlimit=15) correctly guides the search. The Arabic source is written right-to-left. bilities, and the alignment penalty. The main objective of this paper is to improve performance at very high distortion limits. Table 4 shows performance at a distortion limit of 15. To the set of baselines we add L EX, which is the lexicalized re-ordering model of Galley and Manning (2008). This model was shown to outperform other lexicalized re-ordering models in common use. Statistical significance was computed with the approximate randomization test of Riezler and Maxwell (2005), which is less sensitive to Type I errors than bootstrap re-sampling (Koehn, 2004). 5 Discussion The new distortion cost model allows us to triple the distortion limit while maintaining a statistically significant improvement over the M OSES L INEAR baseline at the lower distortion limit for three of the four test sets. More importantly, we can raise the distortion limit in the D ISCRIM +F UTURE configuration at minimal cost: a statistically insignificant −0.2 BLEU performance decrease on average. We also see a considerable improvement over both the M OSES L INEAR and L EX baselines at the hi"
N10-1129,P05-1069,0,0.0201684,"ion limit that is half of what we report, and compare against a baseline that lacks a distortion model entirely. Neither their model nor ours requires generation of lattices prior to search (Zhang et al., 2007; Niehues and Kolss, 2009). Lexicalized re-ordering models are the other significant approach to re-ordering. These models make local predictions about the next phrase to be translated during decoding, typically assigning costs to one of three categories: monotone, swap, or discontinuous. Both generative (Tillmann, 2004; Och and Ney, 2004; Koehn et al., 2007) and discriminative training (Tillmann and Zhang, 2005; Zens and 874 Ney, 2006; Liang et al., 2006) algorithms have been proposed. Recently, Galley and Manning (2008) introduced a hierarchical model capable of analyzing alignments beyond adjacent phrases. Our discriminative distortion framework is not designed as a replacement for lexicalized re-ordering models, but as a substitute for linear distortion. Finally, we comment on differences between our Arabic-English results and the well-known high distortion system of Zollmann et al. (2008), who find optimal baseline performance at a distortion limit of 9. First, they use approximately two orders"
N10-1129,N04-4026,0,0.721121,"and affects beam pruning only through the current model score. When we say linear distortion, we refer to the “simple distortion model” of Koehn et al. (2003) that is shown in Equation (1) and is converted to a cost by multiplying by −1. When extended to phrases,  868 the key property of this model is that monotone decoding gives the least costly translation path. Reorderings internal to extracted phrases are not penalized. In practice, we commonly see n-best lists of hypotheses with linear distortion costs equal to zero. More sophisticated local phrase re-ordering models have been proposed (Tillmann, 2004; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008), but these are typically used in addition to linear distortion. 2.2 Arabic Linguistic Essentials In this paper we use Arabic-English as a case study since we possess a strong experimental baseline. But we expect that the technique presented could be even more effective for high distortion language pairs such as Chinese-English and Hindi-English. Since the analysis that follows is framed in terms of Arabic, we point out several linguistic features that motivate our approach. From the perspective of the three criteria used to sp"
N10-1129,N03-1033,1,0.0224686,"tortion baseline by −1.40 BLEU. The NIST MT09 data is available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/. 872 word documents that were released during periods that overlapped with the development and test sets. The language model is smoothed with the modified Kneser-Ney algorithm, retaining only trigrams, 4grams, and 5-grams that occurred two, three, and three times, respectively, in the training data. We remove from the test sets source tokens not present in the phrase tables. For the discriminative distortion models, we tag the pre-processed input using the log-linear POS tagger of Toutanova et al. (2003). After decoding, we strip any punctuation that appears at the beginning of a translation. 4.2 Results In Table 3 we report uncased BLEU-4 (Papineni et al., 2001) scores at the distortion limit (5) of our most competitive baseline Arabic-English system. M OSES L INEAR uses the linear distortion model present in Moses. C OUNTS is a separate baseline with a discrete cost model that uses unlexicalized maximum likelihood estimates for the same classes present in the discriminative model. To show the effect of the components in our combined distortion model, we give separate results for linear dist"
N10-1129,P96-1021,0,0.116875,"ly. A source token is “covered” when it is aligned with a new target token. For a fixed value of k, we may leave up to k − 1 positions uncovered and return to them later. We can alter the constraint slightly such that for the first uncovered position u ∈ / C we can cover position j when j−u<k j∈ /C which is the definition of the distortion limit used in Moses. Variations of the IBM constraints also exist (Kanthak et al., 2005), as do entirely different regimes like the hierarchical ITG constraints, which represent the source as a sequence of blocks that can be iteratively merged and inverted (Wu, 1996). Zens and Ney (2003) exhaustively compare the IBM and ITG constraints, concluding that although the ITG constraints permit more flexible re-orderings, the IBM constraints result in higher BLEU scores. Since our work falls under the IBM paradigm, we consider cost models for those constraints. We have said that linear distortion is the simplest cost model. The primary criticism of linear distortion is that it is unlexicalized, thus penalizing all re-orderings equally (Khalilov et al., 2009). When extended to phrases as in Equation (1), linear distortion is also agnostic to internal phrase align"
N10-1129,P03-1019,0,0.0627552,"e token is “covered” when it is aligned with a new target token. For a fixed value of k, we may leave up to k − 1 positions uncovered and return to them later. We can alter the constraint slightly such that for the first uncovered position u ∈ / C we can cover position j when j−u<k j∈ /C which is the definition of the distortion limit used in Moses. Variations of the IBM constraints also exist (Kanthak et al., 2005), as do entirely different regimes like the hierarchical ITG constraints, which represent the source as a sequence of blocks that can be iteratively merged and inverted (Wu, 1996). Zens and Ney (2003) exhaustively compare the IBM and ITG constraints, concluding that although the ITG constraints permit more flexible re-orderings, the IBM constraints result in higher BLEU scores. Since our work falls under the IBM paradigm, we consider cost models for those constraints. We have said that linear distortion is the simplest cost model. The primary criticism of linear distortion is that it is unlexicalized, thus penalizing all re-orderings equally (Khalilov et al., 2009). When extended to phrases as in Equation (1), linear distortion is also agnostic to internal phrase alignments. To remedy thes"
N10-1129,W06-3108,0,0.620891,"pruning only through the current model score. When we say linear distortion, we refer to the “simple distortion model” of Koehn et al. (2003) that is shown in Equation (1) and is converted to a cost by multiplying by −1. When extended to phrases,  868 the key property of this model is that monotone decoding gives the least costly translation path. Reorderings internal to extracted phrases are not penalized. In practice, we commonly see n-best lists of hypotheses with linear distortion costs equal to zero. More sophisticated local phrase re-ordering models have been proposed (Tillmann, 2004; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008), but these are typically used in addition to linear distortion. 2.2 Arabic Linguistic Essentials In this paper we use Arabic-English as a case study since we possess a strong experimental baseline. But we expect that the technique presented could be even more effective for high distortion language pairs such as Chinese-English and Hindi-English. Since the analysis that follows is framed in terms of Arabic, we point out several linguistic features that motivate our approach. From the perspective of the three criteria used to specify basic word ord"
N10-1129,I08-1068,0,0.118544,"Missing"
N10-1129,W07-0401,0,0.0358094,"deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. Maximum likelihood estimates for inbound, outbound, and pairwise transitions are computed from automatic word alignments. But no estimate of future cost is included, and their model cannot easily accommodate features defined over the entire translation sequence. As for experimental results, they use a distortion limit that is half of what we report, and compare against a baseline that lacks a distortion model entirely. Neither their model nor ours requires generation of lattices prior to search (Zhang et al., 2007; Niehues and Kolss, 2009). Lexicalized re-ordering models are the other significant approach to re-ordering. These models make local predictions about the next phrase to be translated during decoding, typically assigning costs to one of three categories: monotone, swap, or discontinuous. Both generative (Tillmann, 2004; Och and Ney, 2004; Koehn et al., 2007) and discriminative training (Tillmann and Zhang, 2005; Zens and 874 Ney, 2006; Liang et al., 2006) algorithms have been proposed. Recently, Galley and Manning (2008) introduced a hierarchical model capable of analyzing alignments beyond a"
N10-1129,C08-1144,0,0.0566492,"uous. Both generative (Tillmann, 2004; Och and Ney, 2004; Koehn et al., 2007) and discriminative training (Tillmann and Zhang, 2005; Zens and 874 Ney, 2006; Liang et al., 2006) algorithms have been proposed. Recently, Galley and Manning (2008) introduced a hierarchical model capable of analyzing alignments beyond adjacent phrases. Our discriminative distortion framework is not designed as a replacement for lexicalized re-ordering models, but as a substitute for linear distortion. Finally, we comment on differences between our Arabic-English results and the well-known high distortion system of Zollmann et al. (2008), who find optimal baseline performance at a distortion limit of 9. First, they use approximately two orders of magnitude more training data, which allows them to extract much longer phrases (12 tokens v. our maximum of 7). In this setting, many Arabic-English reorderings can be captured in the phrase table. Second, their “Full” system uses three language models each trained with significantly more data than our single model. Finally, although they use a lexicalized re-ordering model, no details are given about the baseline distortion cost model. 7 Conclusion We have presented a discriminative"
N10-1129,P02-1040,0,\N,Missing
N10-1129,D08-1076,0,\N,Missing
N10-1140,P05-1032,0,0.0481617,"Missing"
N10-1140,N10-2003,1,0.273755,"ating to not . . . anymore in Fig. 2). Beam search algorithm. 1 create initial hypothesis H∅ ; add it to S0g 2 for j = 0 to J 3 if j > 0 then 4 for n = 1 to N c 5 for each Hnew in consolidate(Hjn ) 6 add Hnew to Sjg 7 if j < J then 8 for n = 1 to N g 9 Hold := Hjn 10 u := first uncovered source word of Hold 11 for m = u to u + distortionLimit 12 for each (sk , tk ) in translation options(m) 13 if source sk does not overlap Hold then 14 Hnew :=combine(Hold , sk , tk ) c 15 add Hnew to Sj+l , where l = |sk | g 16 return arg max(SJ ) 3 Decoder The core engine of our phrase-based system, Phrasal (Cer et al., 2010), is a multi-stack decoder similar to Moses (Koehn, 2004), which we extended to support variable-size gaps in the source and the target. In Moses, partial translation hypotheses are arranged into different stacks according to the total number of input words they cover. At every translation step, stacks are pruned using partial translation cost and a lower bound on the estimated future cost. Pruning is implemented using both threshold and histogram pruning, and Moses allows for hypothesis recombination between hypotheses that are indistinguishable according to the underlying models. The key dif"
N10-1140,N09-1025,0,0.0185371,"Joshua, two standard implementations of conventional and hierarchical phrase-based decoding. We found that allowing discontinuities in the source is more useful than target discontinuities in our system, though we found that this turns out to also be the case with the hierarchical phrases of Joshua. In future work, we plan to extend the parameterization of phrasebased lexicalized reordering models to be sensitive to these discontinuities, and we will also consider adding syntactic features to our models to penalize discontinuities that are not syntactically motivated (Marton and Resnik, 2008; Chiang et al., 2009). The discontinuous phrase-based MT system described in this work is part of Phrasal, an opensource phrase-based system available for download at http://nlp.stanford.edu/software/phrasal . Acknowledgements The authors thank three anonymous reviewers, Dan Jurafsky, Spence Green, Steven Bethard, Daniel Cer, Chris Callison-Burch, and Pi-Chuan Chang for their helpful comments. This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. Refe"
N10-1140,J07-2003,0,0.200458,"y find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the current partial translation, but also a set of subphrases that may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems such as Hiero (Chiang, 2007), even though 2-SCFG systems also allow phrasal discontinuities. Part of this difference may be due to a difference of expressiveness, since 2-SCFG models impose hard hierarchical constraints that our models do not impose. Recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (i) source: ai bj ck dl target: bm dn ap ct (ii) ai bj ak bl am bn (iii) ai bj ak bl am bn Figure 1:"
N10-1140,2009.eamt-1.10,0,0.177847,"evious attempt to incorporate gaps is de973 scribed in (Simard et al., 2005). Simard et al. presents an extension to Moses that allows gaps in both source and target phrases, though each of their gap symbols must span exactly one word. This fact makes decoding simpler, since the position of all target words in a translation hypothesis is known as soon as the hypothesis is laid down, but fixed-size discontinuous phrases are less general and increase sparsity. By comparison, our gaps may span any number of words, so we have an increased ability to flexibly match the input sentence effectively. (Crego and Yvon, 2009) also handles gaps, though this work is applicable to an n-gram-based SMT framework (Mari`oo et al., 2006), which is fairly different from the phrase-based framework. 8 Conclusions In this paper, we presented a generalization of conventional phrase-based decoding to handle discontinuities in both source and target phrases. Our system significantly outperforms Moses and Joshua, two standard implementations of conventional and hierarchical phrase-based decoding. We found that allowing discontinuities in the source is more useful than target discontinuities in our system, though we found that thi"
N10-1140,W05-1507,0,0.0542351,"Missing"
N10-1140,N03-1017,0,0.18783,"commonly held belief that the tree-based parameterization of systems such as Hiero and Joshua is crucial to their good performance against Moses. 1 Introduction Phrase-based machine translation models (Och and Ney, 2004) advanced the state of the art by extending the basic translation unit from words to phrases. By conditioning translations on more than a single word, a statistical machine translation (SMT) system benefits from the larger context of a phrase pair to properly handle multi-word units and local reorderings. Experimentally, it was found that longer phrases yield better MT output (Koehn et al., 2003). However, while it is computationally feasible at training time to extract phrase pairs of nearly unbounded size (Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test time tend to be fairly short. Indeed, data sparsity often forces conventional phrase-based systems to segment test sentences into small phrases, and therefore to translate dependent words (e.g., the French ne . . . pas) separately instead of jointly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of"
N10-1140,P07-2045,0,0.025929,"tly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow variable-size discontinuities in both source and target phrases. Since each sentence pair can be more flexibly decomposed into translation units, it is possible to exploit the rich context of longer (possibly discontinuous) phrases to improve translation quality. Our decoder provides two extensions to Moses (Koehn et al., 2007): (a) to cope with source gaps, we follow (Lopez, 2007) to efficiently find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the current partial translation, but also a set of subphrases that may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to bin"
N10-1140,koen-2004-pharaoh,0,0.584336,"(Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test time tend to be fairly short. Indeed, data sparsity often forces conventional phrase-based systems to segment test sentences into small phrases, and therefore to translate dependent words (e.g., the French ne . . . pas) separately instead of jointly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow variable-size discontinuities in both source and target phrases. Since each sentence pair can be more flexibly decomposed into translation units, it is possible to exploit the rich context of longer (possibly discontinuous) phrases to improve translation quality. Our decoder provides two extensions to Moses (Koehn et al., 2007): (a) to cope with source gaps, we follow (Lopez, 2007) to efficiently find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the cur"
N10-1140,W09-0424,0,0.0386882,"Missing"
N10-1140,N06-1014,0,0.0442448,"Missing"
N10-1140,D07-1104,0,0.351235,"eyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow variable-size discontinuities in both source and target phrases. Since each sentence pair can be more flexibly decomposed into translation units, it is possible to exploit the rich context of longer (possibly discontinuous) phrases to improve translation quality. Our decoder provides two extensions to Moses (Koehn et al., 2007): (a) to cope with source gaps, we follow (Lopez, 2007) to efficiently find all discontinuous phrases in the training data that also appear in the input sentence; (b) to enable target discontinuities, we augment translation hypotheses to not only record the current partial translation, but also a set of subphrases that may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems s"
N10-1140,C08-1064,0,0.0961671,"case of Joshua, we used the growdiag-final heuristic since this gave better results. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections 4 We use Moses’ default orientations: monotone, swap, and discontinuous. As far as this reordering model is concerned, we treat discontinuous phrases as continuous, i.e., we simply ignore what lies within gaps to determine phrase orientation. 5 (Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. As noted in (Lopez, 2008), Hiero can represent the same information with hierarchical rules of the form uX, Xu, and XuX. Hiero actually models lexicalized reordering patterns that (Tillmann, 2004) does not account for, e.g., a transformation from X1 uX2 v to X2 u′ v ′ X1 . 970 of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data. This data represents a total of about 700 million words. We manually removed documents of Gigaword that were released during periods that overlap with those of our development and test sets. The language model was smoothed with the modified Kneser-Ney algori"
N10-1140,J06-4004,0,0.0337724,"Missing"
N10-1140,P08-1114,0,0.0273213,"ly outperforms Moses and Joshua, two standard implementations of conventional and hierarchical phrase-based decoding. We found that allowing discontinuities in the source is more useful than target discontinuities in our system, though we found that this turns out to also be the case with the hierarchical phrases of Joshua. In future work, we plan to extend the parameterization of phrasebased lexicalized reordering models to be sensitive to these discontinuities, and we will also consider adding syntactic features to our models to penalize discontinuities that are not syntactically motivated (Marton and Resnik, 2008; Chiang et al., 2009). The discontinuous phrase-based MT system described in this work is part of Phrasal, an opensource phrase-based system available for download at http://nlp.stanford.edu/software/phrasal . Acknowledgements The authors thank three anonymous reviewers, Dan Jurafsky, Spence Green, Steven Bethard, Daniel Cer, Chris Callison-Burch, and Pi-Chuan Chang for their helpful comments. This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement sh"
N10-1140,J04-4002,0,0.938227,"3 BLEU on average on five ChineseEnglish NIST test sets), even though both Joshua and our system support discontinuous phrases. Since the key difference between these two systems is that ours is not hierarchical—i.e., our system uses a string-based decoder instead of CKY, and it imposes no hard hierarchical reordering constraints during training and decoding—this paper sets out to challenge the commonly held belief that the tree-based parameterization of systems such as Hiero and Joshua is crucial to their good performance against Moses. 1 Introduction Phrase-based machine translation models (Och and Ney, 2004) advanced the state of the art by extending the basic translation unit from words to phrases. By conditioning translations on more than a single word, a statistical machine translation (SMT) system benefits from the larger context of a phrase pair to properly handle multi-word units and local reorderings. Experimentally, it was found that longer phrases yield better MT output (Koehn et al., 2003). However, while it is computationally feasible at training time to extract phrase pairs of nearly unbounded size (Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test ti"
N10-1140,P03-1021,0,0.0409179,"ods that overlap with those of our development and test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in SRILM (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data. For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence. We used the 1664 sentences of MT06 for tuning and development and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment four times with different initial conditions. We used n-best lists of size 200. In the final evaluations, we report results using both TER version 0.7.25 (Snover et al., 2006) and BLEU-4 (both uncased). 6 Results We start by comparing some translations generated by the best configurations of Joshua, Moses, and our phrase-based decoder, systems we will empirically evaluate later in this section. Fig. 5 shows translation"
N10-1140,2001.mtsummit-papers.68,0,0.034792,"test sets. The language model was smoothed with the modified Kneser-Ney algorithm as implemented in SRILM (Stolcke, 2002), and we only kept 4-grams and 5-grams that occurred at least three times in the training data. For tuning and testing, we use the official NIST MT evaluation data for Chinese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence. We used the 1664 sentences of MT06 for tuning and development and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment four times with different initial conditions. We used n-best lists of size 200. In the final evaluations, we report results using both TER version 0.7.25 (Snover et al., 2006) and BLEU-4 (both uncased). 6 Results We start by comparing some translations generated by the best configurations of Joshua, Moses, and our phrase-based decoder, systems we will empirically evaluate later in this section. Fig. 5 shows translations of our development set MT06, which were selected because our"
N10-1140,W05-0908,0,0.0181823,"Missing"
N10-1140,2006.amta-papers.25,0,0.0154965,"nese from 2003 to 2008 (MT03 to MT08), which all have four English references for each input sentence. We used the 1664 sentences of MT06 for tuning and development and all other sets for testing. Parameter tuning was done with minimum error rate training (Och, 2003), which was used to maximize IBM BLEU-4 (Papineni et al., 2001). Since MERT is prone to search errors, especially with large numbers of parameters, we ran each tuning experiment four times with different initial conditions. We used n-best lists of size 200. In the final evaluations, we report results using both TER version 0.7.25 (Snover et al., 2006) and BLEU-4 (both uncased). 6 Results We start by comparing some translations generated by the best configurations of Joshua, Moses, and our phrase-based decoder, systems we will empirically evaluate later in this section. Fig. 5 shows translations of our development set MT06, which were selected because our system makes a crucial use of discontinuous phrases. In the first example, the Chi... , which typically transnese input contains lates as when. Lacking an entry for the input phrase in its phrase table, Moses is unable to translate this segment appropriately, and must instead split this ph"
N10-1140,W09-2303,0,0.410798,"partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems such as Hiero (Chiang, 2007), even though 2-SCFG systems also allow phrasal discontinuities. Part of this difference may be due to a difference of expressiveness, since 2-SCFG models impose hard hierarchical constraints that our models do not impose. Recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (i) source: ai bj ck dl target: bm dn ap ct (ii) ai bj ak bl am bn (iii) ai bj ak bl am bn Figure 1: 2-SCFG systems such as Hiero are unable to independently generate translation units a, b, c, and d with the following types of alignments: (i) inside-out (Wu, 1997); (ii) cross-serial DTU (Søgaard and Kuhn, 2009); (iii) “bonbon” (Simard et al., 2005). Standard phrasebased decoders c"
N10-1140,W09-3805,0,0.158911,"Missing"
N10-1140,N04-4026,0,0.0205417,"alignment from cross-EM Viterbi alignment using the Moses grow-diag heuristic in the case Moses and our system. In the case of Joshua, we used the growdiag-final heuristic since this gave better results. In order to train a competitive baseline given our computational resources, we built a large 5-gram language model using the Xinhua and AFP sections 4 We use Moses’ default orientations: monotone, swap, and discontinuous. As far as this reordering model is concerned, we treat discontinuous phrases as continuous, i.e., we simply ignore what lies within gaps to determine phrase orientation. 5 (Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. As noted in (Lopez, 2008), Hiero can represent the same information with hierarchical rules of the form uX, Xu, and XuX. Hiero actually models lexicalized reordering patterns that (Tillmann, 2004) does not account for, e.g., a transformation from X1 uX2 v to X2 u′ v ′ X1 . 970 of the Gigaword corpus (LDC2007T40) in addition to the target side of the parallel data. This data represents a total of about 700 million words. We manually removed documents of Gigaword that were released during periods that ove"
N10-1140,D07-1077,0,0.048909,"Missing"
N10-1140,P06-1098,0,0.152278,"Missing"
N10-1140,P06-1123,0,0.0310399,"at may be appended to the partial translation at some later stages of decoding. With these enhancements, our best discontinuous system outperforms Moses with lexicalized reordering by 0.77 BLEU and 1.53 TER points on average. We also show that our approach compares favorably to binary synchronous context-free grammar (2-SCFG) systems such as Hiero (Chiang, 2007), even though 2-SCFG systems also allow phrasal discontinuities. Part of this difference may be due to a difference of expressiveness, since 2-SCFG models impose hard hierarchical constraints that our models do not impose. Recent work (Wellington et al., 2006; Søgaard and Kuhn, 2009; Søgaard and 966 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 966–974, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (i) source: ai bj ck dl target: bm dn ap ct (ii) ai bj ak bl am bn (iii) ai bj ak bl am bn Figure 1: 2-SCFG systems such as Hiero are unable to independently generate translation units a, b, c, and d with the following types of alignments: (i) inside-out (Wu, 1997); (ii) cross-serial DTU (Søgaard and Kuhn, 2009); (iii) “bonbon” (Simard et al., 2005). Standar"
N10-1140,J97-3002,0,0.0265619,"Missing"
N10-1140,2005.eamt-1.39,0,0.031452,"inst Moses. 1 Introduction Phrase-based machine translation models (Och and Ney, 2004) advanced the state of the art by extending the basic translation unit from words to phrases. By conditioning translations on more than a single word, a statistical machine translation (SMT) system benefits from the larger context of a phrase pair to properly handle multi-word units and local reorderings. Experimentally, it was found that longer phrases yield better MT output (Koehn et al., 2003). However, while it is computationally feasible at training time to extract phrase pairs of nearly unbounded size (Zhang and Vogel, 2005; CallisonBurch et al., 2005), phrase pairs applicable at test time tend to be fairly short. Indeed, data sparsity often forces conventional phrase-based systems to segment test sentences into small phrases, and therefore to translate dependent words (e.g., the French ne . . . pas) separately instead of jointly. We present a solution to this sparsity problem by going beyond using only continuous phrases, and instead define our translation unit as any subset of words of a sentence, i.e., a discontinuous phrase. We generalize conventional multi-beam string-based decoding (Koehn, 2004) to allow v"
N10-1140,P02-1040,0,\N,Missing
N10-1140,H05-1095,0,\N,Missing
N10-1140,D08-1076,0,\N,Missing
N10-2003,W09-2307,1,0.558961,"ensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Grammatical Relations During translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Ra"
N10-2003,D08-1089,1,0.894263,"construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/sof"
N10-2003,P09-1087,1,0.142109,"rget language do a poor job of modeling long distance syntactic relationships. For example, if there are a number of intervening words between a verb and its subject, n-gram language models will often not be of much help in selecting the verb form that agrees with the subject. The target side dependency language model feature captures these long distance relationships by providing a dependency score for the target translations produced by the decoder. This is done using an efficient quadratic time algorithm that operates within the main decoding loop rather than in a separate reranking stage (Galley and Manning, 2009). Discriminative Distortion The standard distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically sign"
N10-2003,N10-1129,1,0.124195,"d distortion cost model used in phrase-based MT systems such as Moses has two problems. First, it does not estimate the future cost of known required moves, thus increasing search errors. Second, the model penalizes distortion linearly, even when appropriate reorderings are performed. To address these problems, we used the Phrasal feature API to design a new discriminative distortion model that predicts word movement during translation and that estimates future cost. These extensions allow us to triple the distortion limit and provide a statistically significant improvement over the baseline (Green et al., 2010). Discriminative Reordering with Chinese Grammatical Relations During translation, a source sentence can be more accurately reordered if the system knows something about the syntactic relationship between the words in the phrases being reordered. The discriminative reordering with Chinese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models"
N10-2003,N03-1017,0,0.00907943,"ases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license"
N10-2003,P07-2045,0,0.0488706,"Missing"
N10-2003,koen-2004-pharaoh,0,0.121277,"f) (model_name) Running this command will first create word level alignments for the sentences in source.txt and target.txt using the Berkeley cross-EM aligner 1 http://www.itl.nist.gov/iad/mig/tests /mt/2009/ResultsRelease/currentArabic.html 9 Proceedings of the NAACL HLT 2010: Demonstration Session, pages 9–12, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 3 Decoder Decoding Engines The package includes two decoding engines, one that implements the left-toright beam search algorithm that was first introduced with the Pharaoh machine translation system (Koehn, 2004), and another that provides a recently developed decoding algorithm for translating with discontinuous phrases (Galley and Manning, 2010). Both engines use features written to a common but extensible feature API, which allows features to be written once and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading"
N10-2003,N06-1014,0,0.141897,"ematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by"
N10-2003,niessen-etal-2000-evaluation,0,0.0200338,"ature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assist in the development of new"
N10-2003,J03-1002,0,0.00159764,"e and then loaded into either engine. Discontinuous phrases provide a mechanism for systematically translating grammatical constructions. As seen in Fig. 1, using discontinuous phrases allows us to successfully capture that the Chinese construction 当 X 的 can be translated as when X. Multithreading The decoder has robust support for multithreading, allowing it to take full advantage of modern hardware that provides multiple CPU cores. As shown in Fig. 2, decoding speed scales well when the number of threads being used is increased from one to four. However, increasing the 2 Optionally, GIZA++ (Och and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to m"
N10-2003,P03-1021,0,0.0139183,"and Ney, 2003) can also be used to create the word-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. tranlations per minute Figure 1: Chinese-to-English translation using discontinuous phrases. 1 2 3 4 5 6 7 8 Cores Figure 2: Multicore translations per minute on a system with two Intel Xeon L5530 processors running at 2.40GHz. threads past four results in only marginal additional gains as the cost of managing the resources shared betwe"
N10-2003,P02-1040,0,0.102389,"ord-to-word alignments. 10 35 25 15 (Liang et al., 2006).2 From the word-to-word alignments, the system extracts a phrase table (Koehn et al., 2003) and hierarchical reordering model (Galley and Manning, 2008). Two ngram language models are trained on the target.txt sentences: one over lowercased target sentences that will be used by the Phrasal decoder and one over the original source sentences that will be used for truecasing the MT output. Finally, the system trains the feature weights for the decoding model using minimum error rate training (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2002) on the development data given by dev.source.txt and dev.ref. The toolkit is distributed under the GNU general public license (GPL) and can be downloaded from http:// nlp.stanford.edu/software/phrasal. tranlations per minute Figure 1: Chinese-to-English translation using discontinuous phrases. 1 2 3 4 5 6 7 8 Cores Figure 2: Multicore translations per minute on a system with two Intel Xeon L5530 processors running at 2.40GHz. threads past four results in only marginal additional gains as the cost of managing the resources shared between the threads is starting to overwhelm the value provided b"
N10-2003,W09-0441,0,0.0154617,"ese grammatical relations feature examines the path between words in a source-side dependency tree and uses it to evaluate the appropriateness of candidate phrase reorderings (Chang et al., 2009). 5 Other components Training Decoding Models The package includes a comprehensive toolset for training decoding models. It supports MERT training using coordinate descent, Powell’s method, line search along random search directions, and downhill Simplex. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al., 2009), mWER (Nießen et al., 2000), and PER (Tillmann et al., 1997). It is also possible to plug in other new user-created evaluation metrics. Conditional Phrase Table Extraction Rather than first building a massive phrase table from a parallel corpus and then filtering it down to just what is needed for a specific data set, our toolkit supports the extraction of just those phrases that might be used on a given evaluation set. In doing so, it dramatically reduces the time required to build the phrase table and related data structures such as reordering models. Feature Extraction API In order to assi"
N12-1007,P98-1012,0,0.662252,"tandard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B 3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference clusters. • CEAF (Luo, 2005): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters. • NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information. Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set. For the automatic setting, we must apply a different metric since the number of system chains may"
N12-1007,D08-1029,0,0.380528,", corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Name"
N12-1007,W10-4305,0,0.0546697,"Missing"
N12-1007,N10-2003,1,0.806423,"these two entities from other entities with similar references (e.g., “Steve Jones” or “Apple Corps”). As with the mention strings, the contexts may originate in different writing systems. We consider both highand low-resource approaches for mapping contexts to a common representation. 4 The Hungarian algorithm finds an optimal minimum-cost alignment. For pairwise costs between tokens, we used the Levenshtein edit distance Machine Translation (MT) For the high-resource setting, if lang(mi ) 6= English, then we translate both mi and its context si to English with an MT system. We use Phrasal (Cer et al., 2010), a phrase-based system which, like most public MT systems, lacks a transliteration module. We believe that this approach yields the most accurate context mapping for highresource language pairs (like English-Arabic). Polylingual Topic Model (PLTM) The polylingual topic model (PLTM) (Mimno et al., 2009) is a generative process in which document tuples— groups of topically-similar documents—share a topic distribution. The tuples need not be sentence-aligned, so training data is easier to obtain. For example, one document tuple might be the set of Wikipedia articles (in all languages) for Steve"
N12-1007,N01-1007,0,0.0412679,"orld, in which electronic media played a prominent role. A key issue for the outside world was the aggregation of information that appeared simultaneously in English, French, and various Arabic dialects. To our knowledge, we are the first to consider clustering entity mentions across languages without a priori knowledge of the quantity or types of real-world entities (a knowledge base). The cross-lingual setting introduces several challenges. First, we cannot assume a prototypical name format. For example, the Anglo-centric first/middle/last prototype used in previous name modeling work (cf. (Charniak, 2001)) does not apply to Arabic names like Abdullah ibn Abd Al-Aziz Al-Saud or Chinese names like Hu Jintao (referred to as Mr. Hu, not Mr. Jintao). Second, organization names often require both transliteration and translation. For example, the Arabic é»Qå  KñÓ È Qg Corp’ contains PPñ .  ‘General Motors  KñÓ È Qg. ‘General Motors’, transliterations of PPñ  but a translation of é»Qå ‘Corporation’. Our models are organized as a pipeline. First, for each document, we perform standard mention detection and coreference resolution. Then, we use pairwise cross-lingual similarity models to measure"
N12-1007,D07-1020,0,0.0851166,"o development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a com"
N12-1007,W99-0613,0,0.0802752,"Missing"
N12-1007,P10-1087,0,0.0685873,"Missing"
N12-1007,N09-1019,0,0.0412775,"Missing"
N12-1007,W04-0701,0,0.0255374,"n be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (200"
N12-1007,N04-1001,0,0.0272882,"1 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to oth"
N12-1007,D08-1089,1,0.805081,"xamples by running a Bernoulli trial for each aligned name pair in the corpus. If the coin was heads, we replaced the English name with another English name chosen randomly from the corpus. MT Context Mapping For the MT context mapping method, we trained Phrasal with all data permitted under the NIST OpenMT Ar-En 2009 constrained track evaluation. We built a 5-gram language model from the Xinhua and AFP sections of the Gigaword corpus (LDC2007T07), in addition to all of the target side training data. In addition to the baseline Phrasal feature set, we used the lexicalized re-ordering model of Galley and Manning (2008). PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles. Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia’s 272 language editions (de Melo and Weikum, 2010). To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus. The 8 We tokenized all English documents with packages from the Stanford parser (Klein and Manning, 2003). For Arabic documents, we used Mada (Habash and Rambow, 2005"
N12-1007,N04-1002,0,0.0189204,"corpus into development and test sections. However, the usual method of splitting by document would not confine all mentions of each entity to one side of the split. We thus split the corpus by global entity id. We assigned one-third of the entities to development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and"
N12-1007,P05-1071,0,0.0233673,"lley and Manning (2008). PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles. Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia’s 272 language editions (de Melo and Weikum, 2010). To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus. The 8 We tokenized all English documents with packages from the Stanford parser (Klein and Manning, 2003). For Arabic documents, we used Mada (Habash and Rambow, 2005) for orthographic normalization and clitic segmentation. 9 LDC Catalog numbers LDC2009E82 and LDC2009E88. topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008"
N12-1007,A00-1020,0,0.0578477,"ed the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 201"
N12-1007,2010.amta-papers.12,0,0.0351357,"apsed, token-based sampler, except the conditional probability p(Ea = E|E−a , Ca ) = 0 if Ca cannot be merged with the chains in cluster E. This property makes the model non-exchangeable, but in practice non-exchangeable models are sometimes useful (Blei 64 Training Data and Procedures We trained our system for Arabic-English crosslingual entity clustering.8 Maxent Mention Similarity The Maxent mention similarity model requires a parallel name list for training. Name pair lists can be obtained from the LDC (e.g., LDC2005T34 contains nearly 450,000 parallel Chinese-English names) or Wikipedia (Irvine et al., 2010). We extracted 12,860 name pairs from the parallel Arabic-English translation treebanks,9 although our experiments show that the model achieves high accuracy with significantly fewer training examples. We generated a uniform distribution of training examples by running a Bernoulli trial for each aligned name pair in the corpus. If the coin was heads, we replaced the English name with another English name chosen randomly from the corpus. MT Context Mapping For the MT context mapping method, we trained Phrasal with all data permitted under the NIST OpenMT Ar-En 2009 constrained track evaluation."
N12-1007,P03-1054,1,0.0146576,"l feature set, we used the lexicalized re-ordering model of Galley and Manning (2008). PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles. Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia’s 272 language editions (de Melo and Weikum, 2010). To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus. The 8 We tokenized all English documents with packages from the Stanford parser (Klein and Manning, 2003). For Arabic documents, we used Mada (Habash and Rambow, 2005) for orthographic normalization and clitic segmentation. 9 LDC Catalog numbers LDC2009E82 and LDC2009E88. topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task"
N12-1007,H05-1083,0,0.018586,"ier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 2011). Our work incorporates elements of the firs"
N12-1007,H05-1004,0,0.0231428,"NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B 3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference clusters. • CEAF (Luo, 2005): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters. • NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information. Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set. For the automatic setting, we must apply a different metric since the number of system chains may differ 3 (Cai and Strube, from the reference. We use Bsys 3 2010), a variant of B that was shown to penalize both twinle"
N12-1007,W07-0804,0,0.0158747,"n and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work"
N12-1007,W03-0405,0,0.0160123,"tity id. We assigned one-third of the entities to development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other"
N12-1007,I11-1029,0,0.0779128,"Missing"
N12-1007,D09-1092,0,0.0592649,"n algorithm finds an optimal minimum-cost alignment. For pairwise costs between tokens, we used the Levenshtein edit distance Machine Translation (MT) For the high-resource setting, if lang(mi ) 6= English, then we translate both mi and its context si to English with an MT system. We use Phrasal (Cer et al., 2010), a phrase-based system which, like most public MT systems, lacks a transliteration module. We believe that this approach yields the most accurate context mapping for highresource language pairs (like English-Arabic). Polylingual Topic Model (PLTM) The polylingual topic model (PLTM) (Mimno et al., 2009) is a generative process in which document tuples— groups of topically-similar documents—share a topic distribution. The tuples need not be sentence-aligned, so training data is easier to obtain. For example, one document tuple might be the set of Wikipedia articles (in all languages) for Steve Jobs. Let D be a set of document tuples, where there is one document in each tuple for each of L languages. Each language has vocabulary Vl and each document dlt has Ntl tokens. We specify a fixed-size set of topics K. The PLTM generates the document tuples as follows: Polylingual Topic Model θt ∼ Dir(α"
N12-1007,C10-2121,1,0.8753,"t coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name u"
N12-1007,W09-1121,0,0.0206938,"s are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B 3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference clusters. • CEAF (Luo, 2005): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters. • NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information. Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set. For the automatic setting, we must apply a different metric since the number of system chains may differ 3 (Cai and Strube, from the reference. We use Bsys 3 2010), a variant of B that was shown to penalize both twinless reference chains and spurious system chains more fairly. Evaluation Corpus The automatic evaluation of cross-lingual coreference systems requires"
N12-1007,P11-1080,0,0.0833628,"included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, S"
N12-1007,strassel-etal-2008-linguistic,0,0.0300007,"ed Mada (Habash and Rambow, 2005) for orthographic normalization and clitic segmentation. 9 LDC Catalog numbers LDC2009E82 and LDC2009E88. topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B 3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis"
N12-1007,N10-1073,0,0.0571859,"Missing"
N12-1007,W09-0210,0,0.0132437,"let the data dictate the number of entity clusters. We thus consider a non-parametric Bayesian mixture model where the mixtures are multinomial distributions over the entity contexts S. Specifically, we consider a DPMM, which automatically infers the number of mixtures. Each Ca has an associated mixture θa : Ca |θa ∼ Mult(θa ) θa |G ∼ G G|α, G0 ∼ DP(α, G0 ) α ∼ Gamma(1, 1) where α is the concentration parameter of the DP prior and G0 is the base distribution with support V . For our experiments, we set G0 = Dir(π1 , . . . , πV ), where πi = PV (wi ). For inference, we use the Gibbs sampler of Vlachos et al. (2009), which can incorporate pairwise constraints. The sampler is identical to a standard collapsed, token-based sampler, except the conditional probability p(Ea = E|E−a , Ca ) = 0 if Ca cannot be merged with the chains in cluster E. This property makes the model non-exchangeable, but in practice non-exchangeable models are sometimes useful (Blei 64 Training Data and Procedures We trained our system for Arabic-English crosslingual entity clustering.8 Maxent Mention Similarity The Maxent mention similarity model requires a parallel name list for training. Name pair lists can be obtained from the LDC"
N12-1007,song-strassel-2008-entity,0,\N,Missing
N12-1007,C98-1012,0,\N,Missing
N12-1007,J98-4003,0,\N,Missing
N12-1049,C04-1180,0,0.0112467,"then applying before to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a rule449 to-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (Σ, S, V, W, R, θ). The alphabet Σ and start symbol S retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v ∈ V we define an (infinite) set Wv corresponding to the possible instances of type v. Concretely, if v = Sequence, our set Wv ∈ W could contain elements corresponding to Friday, last Friday, Nov. 27th , etc. Each node in the tr"
N12-1049,W10-2903,0,0.0247231,"rm. This logical form importantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a month and day, or as a week ago, or as last x – substituting today’s day of the week for x. Each of these correspond to a completely different parse. Recent work by Clarke et al. (2010) and Liang et al. (2011) similarly relax supervision to require only annotated answers rather than full logical forms. For example, Liang et al. (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our syst"
N12-1049,S10-1074,0,0.0411276,"logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide par447 ticularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type system is described in"
N12-1049,W05-1506,0,0.0117311,"on, e.g., 5 weeks. • Combining a non-Nil and Nil element with no change to the temporal expression, e.g., a week. The lexicalization of the Nil type allows the algorithm to take hints from these supporting words. We proceed to describe learning the parameters of this grammar. 4 Learning We present a system architecture, described in Figure 3. We detail the inference procedure in Section 4.1 and training in Section 4.2. 4.1 Inference To provide a list of candidate expressions with their associated probabilities, we employ a k-best CKY parser. Specifically, we implement Algorithm 3 described in Huang and Chiang (2005), providing an O(Gn3 k log k) algorithm with respect to the grammar size G, phrase length n, and beam size k. We set the beam size to 2000. 1 In the case of complex sequences (e.g., Friday the 13th ) an A∗ search is performed to find overlapping ranges in the two sequences; the origin rs (0) is updated to refer to the closest such match to the reference time. 451 Revisiting the notion of pragmatic ambiguity, in a sense the most semantically complete output of the system would be a distribution – an utterance of Friday would give a distribution over Fridays rather than a best guess of its groun"
N12-1049,P04-1061,1,0.622234,"hood update; while the update for µ incorporates a Bayesian prior N (µ0 , σ0 ): ¯ θ ,M ¯ µ,σ ) begin M-Step(M 0 ¯ θ , α) θ := bayesianPosterior(M 0 ¯ σ := mlePosterior(Mµ,σ ) ¯ µ,σ , σ 0 , N ) µ0 := bayesianPosterior(M return (θ0 , µ0 , σ 0 ) end v u σ =u t 1 P 0 X p µ = split on the characters ‘-’ and ‘/,’ which often delimit a boundary between temporal entities. Beyond this preprocessing, no language-specific information about the meanings of the words are introduced, including syntactic parses, POS tags, etc. The algorithm operates similarly to the EM algorithms used for grammar induction (Klein and Manning, 2004; Carroll and Charniak, 1992). However, unlike grammar induction, we are allowed a certain amount of supervision by requiring that the predicted temporal expression match the annotation. Our expected statistics are therefore more accurately our normalized expected counts of valid parses. 452 σ 02 µ0 + σ02 P σ02 P σ 02 + (4) ¯ µ,σ (i,p)∈M ¯ µ,σ (i,p)∈M 0 (i − µ0 )2 · p ¯ µ,σ (i,p)∈M ¯ µ,σ (i,p)∈M i·p p (5) As the parameters improve, the parser more efficiently prunes incorrect parses and the beam incorporates valid parses for longer and longer phrases. For instance, in the first iteration the m"
N12-1049,S10-1072,0,0.0770558,"our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide par447 ticularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type system is described in Section 3.1 while the grammar is outlined in Section 3.2 and described in detail in Sections 3.3 and 3.4. 3.1 Temporal Expression Types We represent temporal expressions as either a Range, Sequence, or Duration. We describe these, the Function type, and the miscellaneous Number and Nil types below: Range [and Instant] A period between two dates (or times)"
N12-1049,D11-1140,0,0.0264037,"re to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a rule449 to-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (Σ, S, V, W, R, θ). The alphabet Σ and start symbol S retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v ∈ V we define an (infinite) set Wv corresponding to the possible instances of type v. Concretely, if v = Sequence, our set Wv ∈ W could contain elements corresponding to Friday, last Friday, Nov. 27th , etc. Each node in the tree defines a pair (v, w) su"
N12-1049,P11-1060,0,0.0599823,"ortantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a month and day, or as a week ago, or as last x – substituting today’s day of the week for x. Each of these correspond to a completely different parse. Recent work by Clarke et al. (2010) and Liang et al. (2011) similarly relax supervision to require only annotated answers rather than full logical forms. For example, Liang et al. (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither"
N12-1049,P00-1010,0,0.225962,"lar in structure to a dependency grammar, but representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide par447 ticularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositi"
N12-1049,J88-2003,0,0.129585,"number without any temporal meaning attached. This comes into play representing expressions such as 2 weeks. The other is the Nil type – denoting terms which are not directly contributing to the semantic meaning of the expression. This is intended for words such as a or the, which serve as cues without bearing temporal content themselves. The Nil type is lexicalized with the word it generates. Omitted Phenomena The representation described is a simplification of the complexities of time. Notably, a body of work has focused on reasoning about events or states relative to temporal expressions. Moens and Steedman (1988) describes temporal expressions relating to changes of state; Condoravdi (2010) explores NPI licensing in temporal expressions. Broader context is also not Range f (Duration) : Range Duration catRight Number Duration next Numn∗100 Day 2 days (a) catRight(t, 2D ) catRight(t, −) next 2D Num(2) 1D 2 days (b) Figure 2: The grammar – (a) describes the CFG parse of the temporal types. Words are tagged with their nonterminal entry, above which only the types of the expressions are maintained; (b) describes the corresponding combination of the temporal instances. The parse in (b) is deterministic give"
N12-1049,puscasu-2004-framework,0,0.15957,"representing a logical form. Our proposed lexical entries and grammar combination rules can be thought of as paralleling the lexical entries and predicates, and the implicit combination rules respectively in this framework. Rather than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide par447 ticularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type s"
N12-1049,S10-1071,0,0.121794,"Missing"
N12-1049,S10-1062,0,0.0889829,"than querying from a finite database, however, our system must compare temporal expression within an infinite timeline. Furthermore, our system is run using neither lexical cues nor intelligent initialization. Related work on interpreting temporal expressions has focused on constructing hand-crafted interpretation rules (Mani and Wilson, 2000; Saquete et al., 2003; Puscasu, 2004; Grover et al., 2010). Of these, HeidelTime (Str¨otgen and Gertz, 2010) and SUTime (Chang and Manning, 2012) provide par447 ticularly strong competition. Recent probabilistic approaches to temporal resolution include UzZaman and Allen (2010), who employ a parser to produce deep logical forms, in conjunction with a CRF classifier. In a similar vein, Kolomiyets and Moens (2010) employ a maximum entropy classifier to detect the location and temporal type of expressions; the grounding is then done via deterministic rules. 3 Representation We define a compositional representation of time; a type system is described in Section 3.1 while the grammar is outlined in Section 3.2 and described in detail in Sections 3.3 and 3.4. 3.1 Temporal Expression Types We represent temporal expressions as either a Range, Sequence, or Duration. We descr"
N12-1049,P01-1067,0,0.0952896,"last week. We can construct a meaning by applying the modifier last to week – creating the previous week; and then applying before to week and last week. We construct a paradigm for parsing temporal phrases consisting of a standard PCFG over temporal types with each parse rule defining a function to apply to the child nodes, or the word being generated. At the root of the tree, we recursively apply the functions in the parse tree to obtain a final temporal value. One can view this formalism as a rule449 to-rule translation (Bach, 1976; Allen, 1995, p. 263), or a constrained Synchronous PCFG (Yamada and Knight, 2001). Our approach contrasts with common approaches, such as CCG grammars (Steedman, 2000; Bos et al., 2004; Kwiatkowski et al., 2011), giving us more flexibility in the composition rules. Figure 2 shows an example of the grammar. Formally, we define our temporal grammar G = (Σ, S, V, W, R, θ). The alphabet Σ and start symbol S retain their usual interpretations. We define a set V to be the set of types, as described in Section 3.1 – these act as our nonterminals. For each v ∈ V we define an (infinite) set Wv corresponding to the possible instances of type v. Concretely, if v = Sequence, our set W"
N12-1049,D07-1071,0,0.0752271,"rk Our approach draws inspiration from a large body of work on parsing expressions into a logical form. The latent parse parallels the formal semantics in previous work, e.g., Montague semantics. Like these representations, a parse – in conjunction with the reference time – defines a set of matching entities, in this case the grounded time. The matching times can be thought of as analogous to the entities in a logical model which satisfy a given expression. Supervised approaches to logical parsing prominently include Zelle and Mooney (1996), Zettlemoyer and Collins (2005), Kate et al. (2005), Zettlemoyer and Collins (2007), inter alia. For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. This logical form importantly contains all the predicates and entities used in their parse. We loosen the supervision required in these systems by allowing the parse to be entirely latent; the annotation of the grounded time neither defines, nor gives any direct cues about the elements of the parse, since many parses evaluate to the same grounding. To demonstrate, the grounding for a week ago could be described by specifying a month and day, or as a week ago, or as last x – substit"
N12-1049,S10-1010,0,\N,Missing
N12-1049,chang-manning-2012-sutime,1,\N,Missing
N13-1006,P11-1062,0,0.0227296,"e complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale OntoNotes Chinese-English parallel NER data showed that Soft-align method, which allows"
N13-1006,J92-4003,0,0.0240595,"rson), LOC (Location), Chinese NER Templates 00: 1 (class bias param) 01: wi+k , −1 ≤ k ≤ 1 02: wi+k−1 ◦ wi+k , 0 ≤ k ≤ 1 03: shape(wi+k ), −4 ≤ k ≤ 4 04: prefix(wi , k), 1 ≤ k ≤ 4 05: prefix(wi−1 , k), 1 ≤ k ≤ 4 06: suffix(wi , k), 1 ≤ k ≤ 4 07: suffix(wi−1 , k), 1 ≤ k ≤ 4 08: radical(wi , k), 1 ≤ k ≤ len(wi ) Unigram Features yi ◦ 00 – 08 Bigram Features yi−1 ◦ yi ◦ 00 – 08 NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). To our knowledge, this work is the first use of word clustering features for Chinese NER. A C++ implementation of the Brown word clustering algorithms (Brown et al., 1992) was used to obtain the word clusters (Liang, 2005).8 Raw text was obtained from the fifth edition of Chinese Gigaword (LDC2011T13). One million paragraphs from Xinhua news section were randomly selected, and the Stanford Word Segmenter with LDC standard was applied to segment Chinese text into words.9 About 46 million words were obtained which were clustered into 1,000 word classes. Table 1: Basic features of Chinese NER. 5 ORG (Organization) and GPE (Geo-Political Entities), and discarded the others. Since the bilingual corpus is only aligned at the document level, we performed sentence alig"
N13-1006,W10-2906,0,0.0980242,"ual cues can help to recognize errors a monolingual tagger would make, allowing us to produce more accurately tagged bitext. Each side of the tagged bitext can then be used to expand the original monolingual training dataset, which may lead to higher accuracy in the monolingual taggers. Previous work such as Li et al. (2012) and Kim et al. (2012) demonstrated that bilingual corpus annotated with NER labels can be used to improve monolingual tagger performance. But a major drawback of their approaches are the need for manual annotation efforts to create such corpora. To avoid this requirement, Burkett et al. (2010) suggested a “multi-view” learning scheme based on re-ranking. Noisy output of a “strong” tagger is used as training data to learn parameters of a log-linear re-ranking model with additional bilingual features, simulated by a “weak” tagger. The learned parameters are then reused with the “strong” tagger to re-rank its own outputs for unseen inputs. Designing good “weak” taggers so that they complement the “view” of bilingual features in the log-linear re-ranker is crucial to the success of this algorithm. Unfortunately there is no principled way of designing such “weak” taggers. In this paper,"
N13-1006,P10-1065,0,0.0271909,"d the difference discussed in Section 1, their re-ranking strategy may lose the correct named entity results if they are not included in the top-N outputs. Furthermore, we consider the word alignment probabilities in our method which can reduce the influence of word alignment errors. Finally, we test our method on a large standard publicly available corpus (8,249 sentences), while they used a much smaller (200 sentences) manually annotated bilingual NER corpus for results validation. In addition to bilingual corpora, bilingual dictionaries are also useful resources. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities"
N13-1006,P11-1061,0,0.0689837,"ults validation. In addition to bilingual corpora, bilingual dictionaries are also useful resources. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Depende"
N13-1006,1993.eamt-1.1,0,0.475415,"Missing"
N13-1006,P05-1045,1,0.0203067,"Geo-Political Entities), and discarded the others. Since the bilingual corpus is only aligned at the document level, we performed sentence alignment using the Champollion Tool Kit (CTK).4 After removing sentences with no aligned sentence, a total of 8,249 sentence pairs were retained. We used the BerkeleyAligner,5 to produce word alignments over the sentence-aligned datasets. BerkeleyAligner also gives posterior probabilities Pa for each aligned word pair. We used the CRF-based Stanford NER tagger (using Viterbi decoding) as our baseline monolingual NER tool.6 English features were taken from Finkel et al. (2005). Table 1 lists the basic features of Chinese NER, where ◦ means string concatenation and yi is the named entity tag of the ith word wi . Moreover, shape(wi ) is the shape of wi , such as date and number. prefix/suffix(wi , k) denotes the k-characters prefix/suffix of wi . radical(wi , k) denotes the radical of the k th Chinese character of wi .7 len(wi ) is the number of Chinese characters in wi . To make the baseline CRF taggers stronger, we added word clustering features to improve generalization over unseen data for both Chinese and English. Word clustering features have been successfully"
N13-1006,I11-1030,1,0.656986,"es. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and"
N13-1006,N06-2015,0,0.0588506,"al constraints. We can re-express Eq. (13) as follows: Y Y (λayc ye )Ia (18) λyac ye = X X X XX j zjy log Pjy y∈Y zayc ye Pa log λyac ye (19) a∈A yc ∈Y ye ∈Y We name the set of constraints above Soft-align, which has the same constraints as Soft-tag, i.e., Eqs. (8), (9), (15) and (16). 4 Experimental Setup We conduct experiments on the latest OntoNotes 4.0 corpus (LDC2011T03). OntoNotes is a large, manually annotated corpus that contains various text genres and annotations, such as part-of-speech tags, named entity labels, syntactic parse trees, predicateargument structures and co-references (Hovy et al., 2006). Aside from English, this corpus also contains several Chinese and Arabic corpora. Some of these corpora contain bilingual parallel documents. We used the Chinese-English parallel corpus with named entity labels as our development and test data. This corpus includes about 400 document pairs (chtb 0001-0325, ectb 1001-1078). We used oddnumbered documents as development data and evennumbered documents as test data. We used all other portions of the named entity annotated corpus as training data for the monolingual systems. There were a total of ∼660 Chinese documents (∼16k sentences) and ∼1,400"
N13-1006,D09-1127,0,0.013019,"Missing"
N13-1006,P12-1073,0,0.292443,"ut entities. For example, in Figure 1, the word “本 (Ben)” is common in Chinese but rarely appears as a translated foreign name. However, its aligned word on the English side (“Ben”) provides a strong clue that this is a person name. Judicious use of this type of bilingual cues can help to recognize errors a monolingual tagger would make, allowing us to produce more accurately tagged bitext. Each side of the tagged bitext can then be used to expand the original monolingual training dataset, which may lead to higher accuracy in the monolingual taggers. Previous work such as Li et al. (2012) and Kim et al. (2012) demonstrated that bilingual corpus annotated with NER labels can be used to improve monolingual tagger performance. But a major drawback of their approaches are the need for manual annotation efforts to create such corpora. To avoid this requirement, Burkett et al. (2010) suggested a “multi-view” learning scheme based on re-ranking. Noisy output of a “strong” tagger is used as training data to learn parameters of a log-linear re-ranking model with additional bilingual features, simulated by a “weak” tagger. The learned parameters are then reused with the “strong” tagger to re-rank its own out"
N13-1006,P08-1068,0,0.0356057,"es). OntoNotes annotates 18 named entity types, such as person, location, date and money. In this paper, we selected the four most common named entity types, i.e., PER (Person), LOC (Location), Chinese NER Templates 00: 1 (class bias param) 01: wi+k , −1 ≤ k ≤ 1 02: wi+k−1 ◦ wi+k , 0 ≤ k ≤ 1 03: shape(wi+k ), −4 ≤ k ≤ 4 04: prefix(wi , k), 1 ≤ k ≤ 4 05: prefix(wi−1 , k), 1 ≤ k ≤ 4 06: suffix(wi , k), 1 ≤ k ≤ 4 07: suffix(wi−1 , k), 1 ≤ k ≤ 4 08: radical(wi , k), 1 ≤ k ≤ len(wi ) Unigram Features yi ◦ 00 – 08 Bigram Features yi−1 ◦ yi ◦ 00 – 08 NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). To our knowledge, this work is the first use of word clustering features for Chinese NER. A C++ implementation of the Brown word clustering algorithms (Brown et al., 1992) was used to obtain the word clusters (Liang, 2005).8 Raw text was obtained from the fifth edition of Chinese Gigaword (LDC2011T13). One million paragraphs from Xinhua news section were randomly selected, and the Stanford Word Segmenter with LDC standard was applied to segment Chinese text into words.9 About 46 million words were obtained which were clustered into 1,000 word classes. Table 1: Basic features of Chinese NER."
N13-1006,P09-1039,0,0.0137924,"above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale OntoNotes Chinese-English parallel NER data"
N13-1006,J94-2001,0,0.580131,"Missing"
N13-1006,N04-1043,0,0.0626188,"s) and ∼1,400 English documents (∼39k sentences). OntoNotes annotates 18 named entity types, such as person, location, date and money. In this paper, we selected the four most common named entity types, i.e., PER (Person), LOC (Location), Chinese NER Templates 00: 1 (class bias param) 01: wi+k , −1 ≤ k ≤ 1 02: wi+k−1 ◦ wi+k , 0 ≤ k ≤ 1 03: shape(wi+k ), −4 ≤ k ≤ 4 04: prefix(wi , k), 1 ≤ k ≤ 4 05: prefix(wi−1 , k), 1 ≤ k ≤ 4 06: suffix(wi , k), 1 ≤ k ≤ 4 07: suffix(wi−1 , k), 1 ≤ k ≤ 4 08: radical(wi , k), 1 ≤ k ≤ len(wi ) Unigram Features yi ◦ 00 – 08 Bigram Features yi−1 ◦ yi ◦ 00 – 08 NER (Miller et al., 2004) and dependency parsing (Koo et al., 2008). To our knowledge, this work is the first use of word clustering features for Chinese NER. A C++ implementation of the Brown word clustering algorithms (Brown et al., 1992) was used to obtain the word clusters (Liang, 2005).8 Raw text was obtained from the fifth edition of Chinese Gigaword (LDC2011T13). One million paragraphs from Xinhua news section were randomly selected, and the Stanford Word Segmenter with LDC standard was applied to segment Chinese text into words.9 About 46 million words were obtained which were clustered into 1,000 word classes"
N13-1006,D10-1069,0,0.0403525,"Missing"
N13-1006,C04-1197,0,0.0188073,"alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale"
N13-1006,N01-1026,0,0.135301,"Finally, we test our method on a large standard publicly available corpus (8,249 sentences), while they used a much smaller (200 sentences) manually annotated bilingual NER corpus for results validation. In addition to bilingual corpora, bilingual dictionaries are also useful resources. Huang and Vogel (2002) and Chen et al. (2010) proposed approaches for extracting bilingual named entity pairs from unannotated bitext, in which verification is based on bilingual named entity dictionaries. However, large-scale bilingual named entity dictionaries are difficult to obtain for most language pairs. Yarowsky and Ngai (2001) proposed a projection method that transforms high-quality analysis results of one language, such as English, into other languages on the basis of word alignment. Das and Petrov (2011) applied the above idea to part-ofspeech tagging with a more complex model. Fu et al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Const"
N13-1006,D10-1030,0,0.0481815,"al. (2011) projected English named entities onto Chinese by carefully designed heuristic rules. Although this type of method does not require manually annotated bilingual corpora or dictionaries, errors in source language results, wrong word alignments and inconsistencies between the languages limit application of this method. Constraint-based monolingual methods by using ILP have been successfully applied to many natural language processing tasks, such as Semantic Role Labeling (Punyakanok et al., 2004), Dependency Parsing (Martins et al., 2009) and Textual Entailment (Berant et al., 2011). Zhuang and Zong (2010) proposed a joint inference method for bilingual semantic role labeling with ILP. However, their approach requires training an alignment model with a manually annotated corpus. 60 9 Conclusions We proposed a novel ILP based inference algorithm with bilingual constraints for NER. This method can jointly infer bilingual named entities without using any annotated bilingual corpus. We investigate various bilingual constraints: hard and soft constraints. Out empirical study on largescale OntoNotes Chinese-English parallel NER data showed that Soft-align method, which allows inconsistent named entit"
N15-1128,J04-3004,0,0.0233313,"ploiting similarities between word vectors is useful for guiding the classifier by expanding the training set, it is not robust enough to use for labeling entities directly. For example, for our development dataset, when θ was set as 0.4, 16 out of 41 unlabeled entities that were expanded into the training set as positive 3 We take the cautious approach of finding similar entities only to the seed entities and not the learned entities. The algorithm can be modified to find similar entities to learned entities as well. Cautious approaches have been shown to be better for bootstrapped learning (Abney, 2004). 4 We tried expanding just the positive entities and just the negative entities. Their relative performance, though higher than the baselines, varied between the datasets. Thus, for conciseness, we present results only for expanding both positives and negatives. 1217 entities were false positives.5 Thus, labeling entities solely based on similarity scores resulted in lower performance. A classifier, on the other hand, can use other sources of information as features to predict an entity’s label. We compute the distributed vector representations using the continuous bag-of-words model (Mikolov"
N15-1128,J92-4003,0,0.0700726,"vious iterations) as positive and entities belonging to all other labels as negative. To improve generalization, we also sample the unlabeled entities that are not function words as negative. To train with a balanced dataset, we randomly sub-sample the negatives such that the number of negative instances is equal to the number of positive instances. The features for the entities are similar to Gupta and Manning (2014): edit distances from positive and negative entities, relative frequency of the entity words in the seed dictionaries, word classes computed using the Brown clustering algorithm (Brown et al., 1992; Liang, 2005), and pattern TF-IDF score. The last feature gives higher scores to entities that are extracted by many learned patterns and have low frequency in the dataset. In our experiments, we call this classifier as NotExpanded. 4 Approach The lack of labeled data to train a good entity classifier is one of the challenges in bootstrapped learning. We use distributed representations of words, in the form of word vectors, to guide the entity classifier by expanding its training set. As explained in the previous section, we train a one-vs-all entity classifier in each iteration of the bootst"
N15-1128,D13-1079,0,0.0196675,"1998) and its boostrapped adaptation (Collins and Singer, 1999) require disjoint views of the features of the data. Whitney and Sarkar (2012) proposed a modified Yarowsky algorithm that used label propagation on graphs, inspired by Subramanya et al. (2010) algorithm that used a large labeled data for domain adaptation. In this paper, we use the setting of bootstrapped pattern-based entity extraction (Riloff, 1996; Thelen and Riloff, 2002). This can be viewed as a form of the Yarowsky algorithm, with pattern learning as an additional step. Pattern based approaches have been widely used for IE (Chiticariu et al., 2013; Fader et al., 2011; Etzioni et al., 2005). Patterns are useful in two ways: they are good features, and they identify promising candidate entities. Recently, Gupta and Manning (2014) improved pattern scoring (Step 2 in Algorithm 1) using predicted labels of unlabeled entities. For entity scoring (Step 3), they used an average of feature values to predict the scores. We use the same framework but focus on improving the entity classifiers. In most IE systems, including ours, word classes or word vectors are used as features in a classifier (Haghighi and Klein, 2006; Ratinov and Roth, 2009). To"
N15-1128,D11-1142,0,0.0888621,"lts suggest that distributed representations can provide good directions for generalization in a bootstrapping system. 1 Introduction Bootstrapped or distantly-supervised learning is a form of semi-supervised learning, in which supervision is provided by seed examples. Supervised machine learning systems, on the other hand, require hand-labeling sufficient data to train a model, which can be costly and time consuming. Bootstrapped information extraction (IE) has become even more pertinent with the ever-growing amount of data coupled with the emergence of open IE systems (Carlson et al., 2010; Fader et al., 2011) and shared tasks like TAC-KBP.1 Limited supervision provided in bootstrapped systems, though an attractive quality, is also one of 1 its main challenges. When seed sets are small, noisy, or do not cover the label space, the bootstrapped classifiers do not generalize well. We use a major guiding inspiration of deep learning: we can learn a lot about syntactic and semantic similarities between words in an unsupervised fashion and capture this information in word vectors. This distributed representation can inform an inductive bias to generalize in a bootstrapping system. In this paper, we prese"
N15-1128,W14-1611,1,0.896126,"at used label propagation on graphs, inspired by Subramanya et al. (2010) algorithm that used a large labeled data for domain adaptation. In this paper, we use the setting of bootstrapped pattern-based entity extraction (Riloff, 1996; Thelen and Riloff, 2002). This can be viewed as a form of the Yarowsky algorithm, with pattern learning as an additional step. Pattern based approaches have been widely used for IE (Chiticariu et al., 2013; Fader et al., 2011; Etzioni et al., 2005). Patterns are useful in two ways: they are good features, and they identify promising candidate entities. Recently, Gupta and Manning (2014) improved pattern scoring (Step 2 in Algorithm 1) using predicted labels of unlabeled entities. For entity scoring (Step 3), they used an average of feature values to predict the scores. We use the same framework but focus on improving the entity classifiers. In most IE systems, including ours, word classes or word vectors are used as features in a classifier (Haghighi and Klein, 2006; Ratinov and Roth, 2009). To the best of our knowledge, our work is the first to use distributed representations of words to improve a bootstrapped system by expanding the training set. 3 Background In a bootstra"
N15-1128,N06-1041,0,0.0218552,"have been widely used for IE (Chiticariu et al., 2013; Fader et al., 2011; Etzioni et al., 2005). Patterns are useful in two ways: they are good features, and they identify promising candidate entities. Recently, Gupta and Manning (2014) improved pattern scoring (Step 2 in Algorithm 1) using predicted labels of unlabeled entities. For entity scoring (Step 3), they used an average of feature values to predict the scores. We use the same framework but focus on improving the entity classifiers. In most IE systems, including ours, word classes or word vectors are used as features in a classifier (Haghighi and Klein, 2006; Ratinov and Roth, 2009). To the best of our knowledge, our work is the first to use distributed representations of words to improve a bootstrapped system by expanding the training set. 3 Background In a bootstrapped pattern-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific label from unlabeled text (Riloff, 1996; Collins and Singer, 1999) using patterns, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). We"
N15-1128,C92-2082,0,0.0659662,"d vectors are used as features in a classifier (Haghighi and Klein, 2006; Ratinov and Roth, 2009). To the best of our knowledge, our work is the first to use distributed representations of words to improve a bootstrapped system by expanding the training set. 3 Background In a bootstrapped pattern-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific label from unlabeled text (Riloff, 1996; Collins and Singer, 1999) using patterns, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). We use lexicosyntactic surface word patterns to extract entities from unlabeled text starting with seed dictionaries for multiple classes. Algorithm 1 gives an overview. In this paper, we focus on improving the entity clas1216 sifier (Step 3) by expanding its training data using distributed vector representations of words. Algorithm 1 Bootstrapped Pattern-based Entity Extraction Given: Text D, labels L, seed entities El ∀l ∈ L while not-terminating-condition (e.g. precision is high) do for l ∈ L do 1. Label D with El 2. Create patterns ar"
N15-1128,D14-1162,1,0.106478,"label. We compute the distributed vector representations using the continuous bag-of-words model (Mikolov et al., 2013a; Mikolov et al., 2013b) implemented in the word2vec toolkit.6 We train 200-dimensional vector representations on a combined dataset of a 2014 Wikipedia dump (1.6 billion tokens), a sample of 50 million tweets from Twitter (200 million tokens), and an in-domain dataset of all MedHelp forums (400 million tokens). We removed words that occurred less than 20 times, resulting in a vocabulary of 89k words. We call this dataset Wiki+Twit+MedHelp. We used the parameters suggested in Pennington et al. (2014): negative sampling with 10 samples and a window size of 10. We ran the model for 3 iterations. 5 Experimental Setup We present results on the same experimental setup, dataset, and seed lists as used in Gupta and Manning (2014). The task is to extract drug-and-treatment (DT) entities in sentences from four forums on the MedHelp user health discussion website: 1. Asthma, 2. Acne, 3. Adult Type II Diabetes (called Diabetes), and 4. Ear Nose & Throat (called ENT). A DT entity is defined as a pharmaceutical drug, or any treatment or intervention mentioned that may help a symptom or a condition. Th"
N15-1128,W09-1119,0,0.0374984,"IE (Chiticariu et al., 2013; Fader et al., 2011; Etzioni et al., 2005). Patterns are useful in two ways: they are good features, and they identify promising candidate entities. Recently, Gupta and Manning (2014) improved pattern scoring (Step 2 in Algorithm 1) using predicted labels of unlabeled entities. For entity scoring (Step 3), they used an average of feature values to predict the scores. We use the same framework but focus on improving the entity classifiers. In most IE systems, including ours, word classes or word vectors are used as features in a classifier (Haghighi and Klein, 2006; Ratinov and Roth, 2009). To the best of our knowledge, our work is the first to use distributed representations of words to improve a bootstrapped system by expanding the training set. 3 Background In a bootstrapped pattern-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific label from unlabeled text (Riloff, 1996; Collins and Singer, 1999) using patterns, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). We use lexicosyntactic surfa"
N15-1128,D10-1017,0,0.0240266,"http://www.medhelp.org 1215 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1215–1220, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics rithms (Yarowsky, 1995) have been shown to be successful at bootstrapping (Collins and Singer, 1999). Co-training (Blum and Mitchell, 1998) and its boostrapped adaptation (Collins and Singer, 1999) require disjoint views of the features of the data. Whitney and Sarkar (2012) proposed a modified Yarowsky algorithm that used label propagation on graphs, inspired by Subramanya et al. (2010) algorithm that used a large labeled data for domain adaptation. In this paper, we use the setting of bootstrapped pattern-based entity extraction (Riloff, 1996; Thelen and Riloff, 2002). This can be viewed as a form of the Yarowsky algorithm, with pattern learning as an additional step. Pattern based approaches have been widely used for IE (Chiticariu et al., 2013; Fader et al., 2011; Etzioni et al., 2005). Patterns are useful in two ways: they are good features, and they identify promising candidate entities. Recently, Gupta and Manning (2014) improved pattern scoring (Step 2 in Algorithm 1)"
N15-1128,W02-1028,0,0.748168,"2015 Association for Computational Linguistics rithms (Yarowsky, 1995) have been shown to be successful at bootstrapping (Collins and Singer, 1999). Co-training (Blum and Mitchell, 1998) and its boostrapped adaptation (Collins and Singer, 1999) require disjoint views of the features of the data. Whitney and Sarkar (2012) proposed a modified Yarowsky algorithm that used label propagation on graphs, inspired by Subramanya et al. (2010) algorithm that used a large labeled data for domain adaptation. In this paper, we use the setting of bootstrapped pattern-based entity extraction (Riloff, 1996; Thelen and Riloff, 2002). This can be viewed as a form of the Yarowsky algorithm, with pattern learning as an additional step. Pattern based approaches have been widely used for IE (Chiticariu et al., 2013; Fader et al., 2011; Etzioni et al., 2005). Patterns are useful in two ways: they are good features, and they identify promising candidate entities. Recently, Gupta and Manning (2014) improved pattern scoring (Step 2 in Algorithm 1) using predicted labels of unlabeled entities. For entity scoring (Step 3), they used an average of feature values to predict the scores. We use the same framework but focus on improving"
N15-1128,P12-1065,0,0.0891496,"aining, co-training, and label propagation. Yarowsky’s style of self-training algo2 http://www.nist.gov/tac/2014/KBP http://www.medhelp.org 1215 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1215–1220, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics rithms (Yarowsky, 1995) have been shown to be successful at bootstrapping (Collins and Singer, 1999). Co-training (Blum and Mitchell, 1998) and its boostrapped adaptation (Collins and Singer, 1999) require disjoint views of the features of the data. Whitney and Sarkar (2012) proposed a modified Yarowsky algorithm that used label propagation on graphs, inspired by Subramanya et al. (2010) algorithm that used a large labeled data for domain adaptation. In this paper, we use the setting of bootstrapped pattern-based entity extraction (Riloff, 1996; Thelen and Riloff, 2002). This can be viewed as a form of the Yarowsky algorithm, with pattern learning as an additional step. Pattern based approaches have been widely used for IE (Chiticariu et al., 2013; Fader et al., 2011; Etzioni et al., 2005). Patterns are useful in two ways: they are good features, and they identif"
N15-1128,C00-2136,0,0.0533017,"er (Haghighi and Klein, 2006; Ratinov and Roth, 2009). To the best of our knowledge, our work is the first to use distributed representations of words to improve a bootstrapped system by expanding the training set. 3 Background In a bootstrapped pattern-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific label from unlabeled text (Riloff, 1996; Collins and Singer, 1999) using patterns, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). We use lexicosyntactic surface word patterns to extract entities from unlabeled text starting with seed dictionaries for multiple classes. Algorithm 1 gives an overview. In this paper, we focus on improving the entity clas1216 sifier (Step 3) by expanding its training data using distributed vector representations of words. Algorithm 1 Bootstrapped Pattern-based Entity Extraction Given: Text D, labels L, seed entities El ∀l ∈ L while not-terminating-condition (e.g. precision is high) do for l ∈ L do 1. Label D with El 2. Create patterns around labeled entities. Learn good patterns and use the"
N15-1128,P95-1026,0,0.330168,"y entities. We show that classifiers trained with expanded sets of entities perform better on extracting drug-and-treatment entities from four online health forums from MedHelp.2 2 Related Work Bootstrapping has many variants, such as self-training, co-training, and label propagation. Yarowsky’s style of self-training algo2 http://www.nist.gov/tac/2014/KBP http://www.medhelp.org 1215 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1215–1220, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics rithms (Yarowsky, 1995) have been shown to be successful at bootstrapping (Collins and Singer, 1999). Co-training (Blum and Mitchell, 1998) and its boostrapped adaptation (Collins and Singer, 1999) require disjoint views of the features of the data. Whitney and Sarkar (2012) proposed a modified Yarowsky algorithm that used label propagation on graphs, inspired by Subramanya et al. (2010) algorithm that used a large labeled data for domain adaptation. In this paper, we use the setting of bootstrapped pattern-based entity extraction (Riloff, 1996; Thelen and Riloff, 2002). This can be viewed as a form of the Yarowsky"
N15-1128,W99-0613,0,\N,Missing
N18-1105,D16-1131,0,0.0261869,"ara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (2017), none of these works attempt to output the co-indexing that is used for analyzing gapping constructions. And again, none of these works try to reconstruct elided material. Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of these methods consider gapping constructions. 7 Conclusion We presented two methods to recover elided predicates in sentences with gapping. Our experiments suggest that both methods work equally well in a realistic end-to-end setting. While in general, recall is still low, the oracle experiments suggest that both methods can recover elided predicates from correct dependency trees, which suggests that as parsers become more and more accurate, the gap recovery accuracy should also increase. We also demonstrated that our method can be used to automatically add the enhanced UD represe"
N18-1105,W03-1005,0,0.0844519,"Norwegian (Haugereid, 2017). The grammar-based parser built with augmented transition networks (Woods, 1970) provided an extension in the form of the SYSCONJ operation (Woods, 1973) to parse some gapping constructions, but also this approach lacked explicit reconstruction mechanisms and provided only limited coverage. There also exists a long line of work on postprocessing surface-syntax constituency trees to recover traces in the PTB (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Gabbard et al., 2006), pre-processing sentences such that they contain tokens for traces before parsing (Dienes and Dubey, 2003b), or directly parsing sentences to either PTB-style trees with empty elements or pre-processed trees that can be deterministically converted to PTB-style trees (Collins, 1163 conj conj xcomp xcomp obl nsubj:pass obl nsubj:pass xcomp obl nsubj:pass tänks Ullnaområdet öka med 9000 , tänks0 Märsta industriområde öka0 med 7000 , tänks00 Jordbro öka00 med 4000 , ... is-thought Ullna-area increase with 9000 , is-thought Märsta industrial-area increase with 7000 , is-thought Jordbro increase with 4000 , .... ‘The Ullna area is expected to grow by 9,000 (new workplaces), the Märsta industrial area b"
N18-1105,P03-1055,0,0.0917889,"Norwegian (Haugereid, 2017). The grammar-based parser built with augmented transition networks (Woods, 1970) provided an extension in the form of the SYSCONJ operation (Woods, 1973) to parse some gapping constructions, but also this approach lacked explicit reconstruction mechanisms and provided only limited coverage. There also exists a long line of work on postprocessing surface-syntax constituency trees to recover traces in the PTB (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Gabbard et al., 2006), pre-processing sentences such that they contain tokens for traces before parsing (Dienes and Dubey, 2003b), or directly parsing sentences to either PTB-style trees with empty elements or pre-processed trees that can be deterministically converted to PTB-style trees (Collins, 1163 conj conj xcomp xcomp obl nsubj:pass obl nsubj:pass xcomp obl nsubj:pass tänks Ullnaområdet öka med 9000 , tänks0 Märsta industriområde öka0 med 7000 , tänks00 Jordbro öka00 med 4000 , ... is-thought Ullna-area increase with 9000 , is-thought Märsta industrial-area increase with 7000 , is-thought Jordbro increase with 4000 , .... ‘The Ullna area is expected to grow by 9,000 (new workplaces), the Märsta industrial area b"
N18-1105,K17-3002,1,0.838127,"number of sentences in our corpus for each of the gap types. 4.2 Parsing experiments Parser We used the parser by Dozat and Manning (2017) for parsing to the two different intermediate dependency representations. This parser is a graph-based parser (McDonald et al., 2005) that uses a biLSTM to compute token representations and then uses a multi-layer perceptron with biaffine attention to compute arc and label scores. Setup We trained the parser on the COMBINED training corpus with gold tokenization, and predicted fine-grained and universal part-of-speech tags, for which we used the tagger by Dozat et al. (2017). We trained the tagger on the COMBINED training corpus. As pre-trained embeddings, we used the word2vec (Mikolov et al., 2013) embeddings that were provided for the CoNLL 2017 Shared Task (Zeman et al., 2017), and we used the same hyperparameters as Dozat et al. (2017). Evaluation We evaluated the parseability of the two dependency representations using labeled and unlabeled attachment scores (LAS and UAS). Further, to specifically evaluate how well parsers are able to parse gapping constructions according to the two annotation schemes, we also computed the LAS and UAS just for the head token"
N18-1105,W17-0406,0,0.133513,"Missing"
N18-1105,W11-2912,0,0.0234673,"literature. As Swedish is a Germanic language like English and thus shares many structural properties, we cannot conclude that our method is applicable to any language based on just this experiment. However, given that our method does not rely on language-specific structural patterns, we expect it to work well for a wide range of languages. but given that UD treebanks are annotated with orphan relations, using the the COMPOSITE procedure would require additional manual annotations in practice. 6 Related work Gapping constructions have been little studied in NLP, but several approaches (e.g., Dukes and Habash 2011; Simkó and Vincze 2017) parse to dependency trees with empty nodes. Seeker et al. (2012) compared three ways of parsing with empty heads: adding a transition that inserts empty nodes, using composite relation labels for nodes that depend on an elided node, and pre-inserting empties before parsing. These papers all focus on recovering nodes for elided function words such as auxiliaries; none of them attempt to recover and resolve the content word elisions of gapping. Ficler and Goldberg (2016) modified PTB annotations of argument-cluster coordinations (ACCs), i.e., gapping constructions with t"
N18-1105,P16-2012,0,0.0257928,"ctice. 6 Related work Gapping constructions have been little studied in NLP, but several approaches (e.g., Dukes and Habash 2011; Simkó and Vincze 2017) parse to dependency trees with empty nodes. Seeker et al. (2012) compared three ways of parsing with empty heads: adding a transition that inserts empty nodes, using composite relation labels for nodes that depend on an elided node, and pre-inserting empties before parsing. These papers all focus on recovering nodes for elided function words such as auxiliaries; none of them attempt to recover and resolve the content word elisions of gapping. Ficler and Goldberg (2016) modified PTB annotations of argument-cluster coordinations (ACCs), i.e., gapping constructions with two post-verbal orphan phrases, which make up a subset of the gapping constructions in the PTB. While the modified annotation style leads to higher parsing accuracy of ACCs, it is specific to ACCs and does not generalize to other gapping constructions. Moreover, they did not reconstruct gapped ACC clauses. Traditional grammarbased chart parsers (Kay, 1980; Klein and Manning, 2001) did handle empty nodes and so could in principle provide a parse of gapping sentences though additional mechanisms"
N18-1105,N06-1024,0,0.0516315,"been noted for the English Resource Grammar (Flickinger, 2017, p.c.) and for an HPSG implementation for Norwegian (Haugereid, 2017). The grammar-based parser built with augmented transition networks (Woods, 1970) provided an extension in the form of the SYSCONJ operation (Woods, 1973) to parse some gapping constructions, but also this approach lacked explicit reconstruction mechanisms and provided only limited coverage. There also exists a long line of work on postprocessing surface-syntax constituency trees to recover traces in the PTB (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Gabbard et al., 2006), pre-processing sentences such that they contain tokens for traces before parsing (Dienes and Dubey, 2003b), or directly parsing sentences to either PTB-style trees with empty elements or pre-processed trees that can be deterministically converted to PTB-style trees (Collins, 1163 conj conj xcomp xcomp obl nsubj:pass obl nsubj:pass xcomp obl nsubj:pass tänks Ullnaområdet öka med 9000 , tänks0 Märsta industriområde öka0 med 7000 , tänks00 Jordbro öka00 med 4000 , ... is-thought Ullna-area increase with 9000 , is-thought Märsta industrial-area increase with 7000 , is-thought Jordbro increase wi"
N18-1105,J97-4002,0,0.223721,", 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (2017), none of these works attempt to output the co-indexing that is used for analyzing gapping constructions. And again, none of these works try to reconstruct elided material. Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of these methods consider gapping constructions. 7 Conclusion We presented two methods to recover elided predicates in sentences with gapping. Our experiments suggest that both methods work equally well in a realistic end-to-end setting. While in general, recall is still low, the oracle experiments suggest that both methods can recover elided predicates from correct dependency trees, which suggests that as parsers become more and more accurate, the gap recovery accuracy should also increase."
N18-1105,P16-2016,0,0.0187532,"ordbro öka00 med 4000 , ... is-thought Ullna-area increase with 9000 , is-thought Märsta industrial-area increase with 7000 , is-thought Jordbro increase with 4000 , .... ‘The Ullna area is expected to grow by 9,000 (new workplaces), the Märsta industrial area by 7,000, Jordbro by 4,000, ...’ Figure 2: Dependency graph for part of the sentence sv-ud-train-1102 as output by the ORPHAN procedure. The system correctly predicts the copy nodes for the matrix and the embedded verb, and correctly attaches the arguments to the copy nodes. 1997; Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (2017), none of these works attempt to output the co-indexing that is used for analyzing gapping constructions. And again, none of these works try to reconstruct elided material. Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 201"
N18-1105,J07-3004,0,0.0665134,"Missing"
N18-1105,P01-1044,1,0.481659,"tion words such as auxiliaries; none of them attempt to recover and resolve the content word elisions of gapping. Ficler and Goldberg (2016) modified PTB annotations of argument-cluster coordinations (ACCs), i.e., gapping constructions with two post-verbal orphan phrases, which make up a subset of the gapping constructions in the PTB. While the modified annotation style leads to higher parsing accuracy of ACCs, it is specific to ACCs and does not generalize to other gapping constructions. Moreover, they did not reconstruct gapped ACC clauses. Traditional grammarbased chart parsers (Kay, 1980; Klein and Manning, 2001) did handle empty nodes and so could in principle provide a parse of gapping sentences though additional mechanisms would be needed for reconstruction. In practice, though, dealing with gapping in a grammar-based framework is not straightforward and can lead to a combinatorial explosion that slows down parsing in general, as has been noted for the English Resource Grammar (Flickinger, 2017, p.c.) and for an HPSG implementation for Norwegian (Haugereid, 2017). The grammar-based parser built with augmented transition networks (Woods, 1970) provided an extension in the form of the SYSCONJ operati"
N18-1105,Q17-1031,0,0.169729,"crease with 9000 , is-thought Märsta industrial-area increase with 7000 , is-thought Jordbro increase with 4000 , .... ‘The Ullna area is expected to grow by 9,000 (new workplaces), the Märsta industrial area by 7,000, Jordbro by 4,000, ...’ Figure 2: Dependency graph for part of the sentence sv-ud-train-1102 as output by the ORPHAN procedure. The system correctly predicts the copy nodes for the matrix and the embedded verb, and correctly attaches the arguments to the copy nodes. 1997; Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (2017), none of these works attempt to output the co-indexing that is used for analyzing gapping constructions. And again, none of these works try to reconstruct elided material. Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of the"
N18-1105,J93-2004,0,0.0604335,"path between want and play, we also have to make a copy of write to reconstruct the UD graph of the gapped clause. 4 Experiments Both methods rely on a dependency parser followed by a post-processing step. We evaluated the individual steps and the end-to-end performance. 4.1 Data We used the UD English Web Treebank v2.1 (henceforth EWT; Silveira et al., 2014; Nivre et al., 2017) for training and evaluating parsers. As the treebank is relatively small and therefore only contains very few sentences with gapping, we also extracted gapping constructions from the WSJ and Brown portions of the PTB (Marcus et al., 1993) and the GENIA corpus (Ohta et al., 2002). Further, we copied sentences from the Wikipedia page on gapping7 and from published papers on gapping. The sentences in the EWT already contain annotations with the orphan relation and copy nodes for the enhanced representation, and we manually added both of these annotations for the remaining examples. The composite relations can 7 https://en.wikipedia.org/wiki/Gapping, accessed on Aug 24, 2017. be automatically obtained from the enhanced representation by removing the copy nodes and concatenating the dependency labels, which we did to build the trai"
N18-1105,H05-1066,0,0.335074,"Missing"
N18-1105,2016.lilt-13.1,0,0.178264,"ayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (2017), none of these works attempt to output the co-indexing that is used for analyzing gapping constructions. And again, none of these works try to reconstruct elided material. Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of these methods consider gapping constructions. 7 Conclusion We presented two methods to recover elided predicates in sentences with gapping. Our experiments suggest that both methods work equally well in a realistic end-to-end setting. While in general, recall is still low, the oracle experiments suggest that both methods can recover elided predicates from correct dependency trees, which suggests that as parsers become more and more accurate, the gap recovery accuracy should also increase. We also demonstrated that our method can be used to auto"
N18-1105,C04-1157,0,0.771587,"id, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (2017), none of these works attempt to output the co-indexing that is used for analyzing gapping constructions. And again, none of these works try to reconstruct elided material. Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of these methods consider gapping constructions. 7 Conclusion We presented two methods to recover elided predicates in sentences with gapping. Our experiments suggest that both methods work equally well in a realistic end-to-end setting. While in general, recall is still low, the oracle experiments suggest that both methods can recover elided predicates from correct dependency trees, which suggests that as parsers become more and more accurate, the gap recovery accuracy should also increase. We also demonst"
N18-1105,L16-1262,1,0.828827,"k Gapping constructions have been little studied in NLP, but several approaches (e.g., Dukes and Habash 2011; Simkó and Vincze 2017) parse to dependency trees with empty nodes. Seeker et al. (2012) compared three ways of parsing with empty heads: adding a transition that inserts empty nodes, using composite relation labels for nodes that depend on an elided node, and pre-inserting empties before parsing. These papers all focus on recovering nodes for elided function words such as auxiliaries; none of them attempt to recover and resolve the content word elisions of gapping. Ficler and Goldberg (2016) modified PTB annotations of argument-cluster coordinations (ACCs), i.e., gapping constructions with two post-verbal orphan phrases, which make up a subset of the gapping constructions in the PTB. While the modified annotation style leads to higher parsing accuracy of ACCs, it is specific to ACCs and does not generalize to other gapping constructions. Moreover, they did not reconstruct gapped ACC clauses. Traditional grammarbased chart parsers (Kay, 1980; Klein and Manning, 2001) did handle empty nodes and so could in principle provide a parse of gapping sentences though additional mechanisms"
N18-1105,S15-2153,0,0.0745365,"Missing"
N18-1105,D14-1162,1,0.106118,"Missing"
N18-1105,P06-1023,0,0.0419441,"område öka0 med 7000 , tänks00 Jordbro öka00 med 4000 , ... is-thought Ullna-area increase with 9000 , is-thought Märsta industrial-area increase with 7000 , is-thought Jordbro increase with 4000 , .... ‘The Ullna area is expected to grow by 9,000 (new workplaces), the Märsta industrial area by 7,000, Jordbro by 4,000, ...’ Figure 2: Dependency graph for part of the sentence sv-ud-train-1102 as output by the ORPHAN procedure. The system correctly predicts the copy nodes for the matrix and the embedded verb, and correctly attaches the arguments to the copy nodes. 1997; Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017). However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (2017), none of these works attempt to output the co-indexing that is used for analyzing gapping constructions. And again, none of these works try to reconstruct elided material. Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen,"
N18-1105,W17-0416,1,0.856912,"ions that are useful for NLP tasks. UD defines two types of representation: the basic UD representation which is a strict surface syntax dependency tree and the enhanced UD representation (Schuster and Manning, 2016) which may be a graph instead of a tree and may contain additional nodes. The analysis of gapping in the enhanced representation makes use of copy nodes for elided predicates and additional edges for elided arguments, which we both try to automatically reconstruct in this paper. In the simple case in which only one predicate was elided, there is exactly one 2 See Johnson (2014) or Schuster et al. (2017) for a more comprehensive overview of cross-linguistically attested gapping constructions. 1157 copy node for the elided predicate, which leads to a structure that is identical to the structure of the same sentence without a gap.3 conj nsubj cc obj nsubj obj John bought books and Mary bought0 flowers If a clause contains a more complex gap, the enhanced representation contains copies for all content words that are required to attach the remnants. have existed if nothing had been elided. For example, in the following sentence, the verb bought, which would have been attached to the head of the f"
N18-1105,L16-1376,1,0.905765,"Missing"
N18-1105,C12-2105,0,0.105338,"y, the number of composite relations is unbounded: xcomp conj&gt;xcomp&gt;xcomp&gt;xcomp&gt;obj The rationale for not copying all arguments is again to keep the graph simple, while still encoding all relations between content words. Arguments can be arbitrarily complex and it seems misguided to copy entire subtrees of arguments which, e.g., could contain multiple adverbial clauses. Note that linking to existing nodes would not work in the case of verb clusters because they do not satisfy the subtree constraint. 3 3.1 conj&gt;cc obj Methods Composite relations Our first method adapts one of the procedures by Seeker et al. (2012), which represents gaps in dependency trees by attaching dependents of an elided predicate with composite relations. These relations represent the dependency path that would 3 To enhance the readability of our examples, we place the copy node in the sentence where the elided predicate would have been pronounced. However, as linear order typically does not matter for extracting information with dependency patterns, our procedures only try to recover the structure of canonical sentences but not their linear order. conj&gt;nsubj det conj&gt;cc ... and Mary a play 3.2 Orphan procedure Our second method"
N18-1105,silveira-etal-2014-gold,0,0.147706,"Missing"
N18-1105,W17-6527,0,0.0216789,"is a Germanic language like English and thus shares many structural properties, we cannot conclude that our method is applicable to any language based on just this experiment. However, given that our method does not rely on language-specific structural patterns, we expect it to work well for a wide range of languages. but given that UD treebanks are annotated with orphan relations, using the the COMPOSITE procedure would require additional manual annotations in practice. 6 Related work Gapping constructions have been little studied in NLP, but several approaches (e.g., Dukes and Habash 2011; Simkó and Vincze 2017) parse to dependency trees with empty nodes. Seeker et al. (2012) compared three ways of parsing with empty heads: adding a transition that inserts empty nodes, using composite relation labels for nodes that depend on an elided node, and pre-inserting empties before parsing. These papers all focus on recovering nodes for elided function words such as auxiliaries; none of them attempt to recover and resolve the content word elisions of gapping. Ficler and Goldberg (2016) modified PTB annotations of argument-cluster coordinations (ACCs), i.e., gapping constructions with two post-verbal orphan ph"
N18-1105,C00-2137,0,0.0547731,"tly at p &lt; 0.01. ORPHAN COMPOSITE Development UASg LASg UASg 72.36 68.36 72.56* 62.41 64.73*** 49.45 Test LASg 65.79*** 46.24 Table 4: Labeled (LASg ) and unlabeled attachment score (UASg ) of head tokens of remnants for parsers trained and evaluated on the UD representation (ORPHAN) and the composite relations representation (COMPOSITE) on the development and test sets of the COMBINED treebank. Results that differ significantly are marked with * (p &lt; 0.05) or *** (p &lt; 0.001). tistical significance of pairwise comparisons, we performed two-tailed approximate randomization tests (Noreen, 1989; Yeh, 2000) with an adapted version of the sigf package (Padó, 2006). Results Table 3 shows the overall parsing results on the development and test sets of the two treebanks. There was no significant difference between the parser that was trained on the UD representation (ORPHAN) and the parser trained on the composite representation (COMPOSITE) when tested on the EWT data sets, which is not surprising considering that there is just one sentence with gapping each in the development and the test split. When evaluated on the GAPPING datasets, the OR PHAN parser performs significantly better (p &lt; 0.01) in t"
N18-1105,P02-1018,0,\N,Missing
N18-1105,P11-2037,0,\N,Missing
N18-1105,P04-1082,0,\N,Missing
N18-1105,P16-1088,0,\N,Missing
P01-1044,J93-2004,0,\N,Missing
P01-1044,P97-1003,0,\N,Missing
P02-1017,P93-1035,0,0.0742551,"Missing"
P02-1017,P95-1031,0,0.135505,"Missing"
P02-1017,W00-0717,0,0.171732,"treebank, a 1000 element vector was made by counting how often each co-occurred with each of the 500 most common words immediately to the left or right in Treebank text and additional 1994–96 WSJ newswire. These vectors were length-normalized, and then rank-reduced by an SVD, keeping the 50 largest singular vectors. The resulting vectors were clustered into 200 word classes by a weighted k-means algorithm, and then grammar induction operated over these classes. We do not believe that the quality of our tags matches that of the better methods of Sch¨utze (1995), much less the recent results of Clark (2000). Nevertheless, using these tags as input still gave induced structure substantially above right-branching. Figure 8 shows      80 70 60 50 40 30 20 10 0 0.35M  0.30M   0.25M 0.20M    0.15M   F1 log-likelihood 0.10M    0.05M   0.00M 0 4 8 12 16 20 24 28 32 36 40 Iterations Figure 10: F1 is non-decreasing until convergence. the performance with induced tags compared to correct tags. Overall F1 has dropped, but, interestingly, VP and S recall are higher. This seems to be due to a marked difference between the induced tags and the treebank tags: nouns are scattered among a"
P02-1017,W01-0713,0,0.886212,"induced and actual part-of-speech tags as input data, and examine extensions to the basic model. We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task. 1 Introduction The task of inducing hierarchical syntactic structure from observed yields alone has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), or to build better language models (Baker, 1979; Chen, 1995). In previous work, we presented a conditional model over trees which gave the best published results for unsupervised parsing of the ATIS corpus (Klein and Manning, 2001b). However, it suffered from several drawbacks, primarily stemming from the conditional model used for induction. Here, we improve on that model in several ways. First, we construct a generative model which utilizes the same features. Then, we extend the model to allow mul"
P02-1017,W01-0714,1,0.788755,"1968; Wolff, 1988). In the early 1990s, attempts were made to do grammar induction by parameter search, where the broad structure of the grammar is fixed in advance and only parameters are induced (Lari and Young, 1990; Carroll and Charniak, 1992). 1 However, this appeared unpromising and most recent work has returned to using structure search. Note that both approaches are local. Structure search requires ways of deciding locally which merges will produce a coherent, globally good grammar. To the extent that such approaches work, they work because good local heuristics have been engineered (Klein and Manning, 2001a; Clark, 2001). 1 On this approach, the question of which rules are included or excluded becomes the question of which parameters are zero. End S 0 VBD 3 End 4 5 0 1 2 3 4 5 0 0 0 1 1 1 2 PP 3 IN 4 5 NN 0 Factory 1 payrolls 2 fell 3 in 4 September 5 Start NNS Start NN VP 2 Start NP 1 2 3 2 3 4 4 5 5 Span End Label Constituent 0 h0,5i 1 2 3 S 4 5NN NNS VBD IN NN h0,2i NP NN NNS h2,5i VP VBD IN NN h3,5i PP IN NN h0,1i NN NN h1,2i NNS NNS h2,3i VBD VBD h3,4i IN IN h4,5i NN NNS Context –  – VBD NNS –  VBD –   – NNS NN – VBD NNS – IN VBD – NN IN –  (a) (b) (c) Figure 1: (a) Example parse tr"
P02-1017,P92-1017,0,0.399736,"Missing"
P02-1017,E95-1020,0,0.661882,"Missing"
P02-1017,C00-2139,0,0.426843,"Missing"
P02-1017,H93-1047,0,\N,Missing
P02-1017,P93-1034,0,\N,Missing
P03-1054,J98-2004,0,0.133778,"Missing"
P03-1054,W98-1115,0,0.155127,"Missing"
P03-1054,A00-2018,0,0.135229,"Missing"
P03-1054,P01-1017,0,0.134591,"Missing"
P03-1054,P96-1025,0,0.13596,"Missing"
P03-1054,P99-1059,0,0.15953,"Missing"
P03-1054,W01-0521,0,0.107575,"Missing"
P03-1054,J93-1005,0,0.128122,"Missing"
P03-1054,J98-4004,0,0.1401,"nd structural conditioning. 1 Experimental Setup To facilitate comparison with previous work, we trained our models on sections 2–21 of the WSJ section of the Penn treebank. We used the first 20 files (393 sentences) of section 22 as a development set (devset). This set is small enough that there is noticeable variance in individual results, but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill-climb. All of section 23 was used as a test set for the final model. For each model, input trees were annotated or transformed in some way, as in Johnson (1998). Given a set of transformed trees, we viewed the local trees as grammar rewrite rules in the standard way, and used (unsmoothed) maximum-likelihood estimates for rule probabilities.5 To parse the grammar, we used a simple array-based Java implementation of a generalized CKY parser, which, for our final best model, was able to exhaustively parse all sentences in section 23 in 1GB of memory, taking approximately 3 sec for average length sentences.6 5 The tagging probabilities were smoothed to accommodate unknown words. The quantity P(tag|wor d) was estimated as follows: words were split into on"
P03-1054,P01-1044,1,0.13755,"Missing"
P03-1054,P95-1037,0,0.127613,"Missing"
P03-1054,J03-4003,0,\N,Missing
P03-1056,A00-2018,0,\N,Missing
P03-1056,J98-4004,0,\N,Missing
P03-1056,J82-3004,0,\N,Missing
P03-1056,W99-0623,0,\N,Missing
P03-1056,W00-1201,0,\N,Missing
P03-1056,C02-1126,0,\N,Missing
P03-1056,J03-4003,0,\N,Missing
P03-1056,P98-1115,0,\N,Missing
P03-1056,C98-1111,0,\N,Missing
P03-1056,C02-1145,0,\N,Missing
P04-1042,A00-2018,0,0.0258632,"Missing"
P04-1042,W03-1005,0,0.352649,"g null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional i"
P04-1042,P03-1055,0,0.702112,"g null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional i"
P04-1042,J02-3001,0,0.0282752,"ctic parsing is as an aid to semantic interpretation, in pursuit of broader goals of natural language understanding. Proponents of traditional ‘deep’ or ‘precise’ approaches to syntax, such as GB, CCG, HPSG, LFG, or TAG, have argued that sophisticated grammatical formalisms are essential to resolving various hidden relationships such as the source phrase of moved whphrases in questions and relativizations, or the controller of clauses without an overt subject. Knowledge of these hidden relationships is in turn essential to semantic interpretation of the kind practiced in the semantic parsing (Gildea and Jurafsky, 2002) and QA (Pasca and Harabagiu, 2001) literatures. However, work in statistical parsing has for the most part put these needs aside, being content to recover surface context-free (CF) phrase structure trees. This perhaps reflects the fact that context-free phrase structure grammar (CFG) is in some sense at the the heart of the majority of both formal and computational syntactic research. Although, upon introducing it, Chomsky (1956) rejected CFG as an adequate framework for natural language description, the majority of work in the last half century has used context-free structural descriptions a"
P04-1042,P02-1018,0,0.103897,"e problems SBAR WHNP-1 0 S NP VP PRP VBZ NP it sees *T*-1 Figure 1: Example of empty and nonlocal annotations from the Penn Treebank of English, including null complementizers (0), relativization (*T*-1), rightextraposition (*ICH*-2), and syntactic control (*-3). 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English, despite the disparity in type and frequency of various non-local dependency constructions for varying languages (Kruijff, 2002). Collins (1999)’s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divi"
P04-1042,N04-1013,0,0.0243171,"elative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from section 23 for testin"
P04-1042,J93-4001,0,0.0114419,"investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and tree"
P04-1042,P03-1054,1,0.155811,"Missing"
P04-1042,2000.iwpt-1.20,0,0.121449,"he first option is to postprocess CF parse trees, which we have closely investigated in this paper. The second is to incorporate nonlocal dependency information into the category structure of CF trees. This was the approach taken by Dienes and Dubey (2003a,b) and Dienes (2003); it is also practiced in recent work on broad-coverage CCG parsing (Hockenmaier, 2003). The third would be to incorporate nonlocal dependency information into the edge structure parse trees, allowing discontinuous constituency to be explicitly represented in the parse chart. This approach was tentatively investigated by Plaehn (2000). As the syntactic diversity of languages for which treebanks are available grows, it will become increasingly important to compare these three approaches. 7 Acknowledgements This work has benefited from feedback from Dan Jurafsky and three anonymous reviewers, and from presentation at the Institute of Cognitive Science, University of Colorado at Boulder. The authors are also grateful to Dan Klein and Jenny Finkel for use of maximum-entropy software they wrote. This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligenc"
P04-1042,P02-1035,0,0.0422572,"r resolving nonlocal relative pronoun dependencies. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. Dienes and Dubey (2003a,b) and Dienes (2003) approached the problem by preidentifying empty categories using an HMM on unparsed strings and threaded the identified empties into the category structure of a context-free parser, finding that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from"
P04-1042,A97-1014,0,0.476477,"ng that this method compared favorably with both Collins’ and Johnson’s. Traditional LFG parsing, in both non-stochastic (Kaplan and Maxwell, 1993) and stochastic (Riezler et al., 2002; Kaplan et al., 2004) incarnations, also divides the labor of local and nonlocal dependency identification into two phases, starting with context-free parses and continuing by augmentation with functional information. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from section 23 for testing. Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. Consistent with prior work and with common practice in statistical parsing, we stripped categories of all functiona"
P04-1042,J03-4003,0,\N,Missing
P04-1042,P03-1013,0,\N,Missing
P04-1061,P93-1035,0,0.0488106,"duct model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved"
P04-1061,P95-1031,0,0.0254692,"tributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of linguistic con"
P04-1061,W00-0717,0,0.0418312,"ic model, P(B) is uniform over binary trees. Then, for each hi, j i, the subspan and context pair (i s j , i−1 si ∼ j s j +1 ) is generated via a classconditional independence model: P(s, B) = P(B) Y P(i s j |bi j )P(i−1 si ∼ j s j +1 |bi j ) hi, j i data clustering methods. In the most common case, the items are words, and one uses distributions over adjacent words to induce word classes. Previous work has shown that even this quite simple representation allows the induction of quite high quality word classes, largely corresponding to traditional parts of speech (Finch, 1993; Sch¨utze, 1995; Clark, 2000). A typical pattern would be that stocks and treasuries both frequently occur before the words fell and rose, and might therefore be put into the same class. Clark (2001) and Klein and Manning (2002) show that this approach can be successfully used for discovering syntactic constituents as well. However, as one might expect, it is easier to cluster word sequences (or word class sequences) than to tell how to put them together into trees. In particular, if one is given all contiguous subsequences (subspans) from a corpus of sentences, most natural clusters will not represent valid constituents"
P04-1061,W01-0713,0,0.402317,"constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is ev"
P04-1061,C96-1058,0,0.531625,". h and a are head and argument words, respectively, while i , j , and k are positions between words. attach to the verb. But then, given a NOUN NOUN VERB sequence, both nouns will attach to the verb – there is no way that the model can learn that verbs have exactly one subject. We now turn to an improved dependency model that addresses this problem. 3 An Improved Dependency Model The dependency models discussed above are distinct from dependency models used inside highperformance supervised probabilistic parsers in several ways. First, in supervised models, a head outward process is modeled (Eisner, 1996; Collins, 1999). In such processes, heads generate a sequence of arguments outward to the left or right, conditioning on not only the identity of the head and direction of the attachment, but also on some notion of distance or valence. Moreover, in a head-outward model, it is natural to model stop steps, where the final argument on each side of a head is always the special symbol STOP. Models like Paskin (2002) avoid modeling STOP by generating the graph skeleton G first, uniformly at random, then populating the words of s conditioned on G. Previous work (Collins, 1999) has stressed the impor"
P04-1061,P02-1017,1,0.666116,"age in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures (Baker, 1979; Chen, 1995), and work interested in the strong generative capacity of models, where the unobserved structure itself is evaluated (van Zaanen, 2000; Clark, 2001; Klein and Manning, 2002). This paper falls into the latter category; we will be inducing models of linguistic constituency and dependency with the goal of recovering linguistically plausible structures. We make no claims as to the cognitive plausibility of the induction mechanisms we present here; however, the ability of these systems to recover substantial linguistic patterns from surface yields alone does speak to the strength of support for these patterns in the data, and hence undermines arguments based on “the poverty of the stimulus” (Chomsky, 1965). 2 Unsupervised Dependency Parsing Most recent progress in uns"
P04-1061,P92-1017,0,0.283648,"inear constituency. The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing. We also demonstrate that the combined model works and is robust cross-linguistically, being able to exploit either attachment or distributional regularities that are salient in the data. 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994). Researchers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), to build better language models (Baker, 1979; Chen, 1995), and to examine cognitive issues in language learning (Solan et al., 2003). An important distinction should be drawn between work primarily interested in the weak generative capacity of models, where modeling hierarchical structure is only useful insofar as it leads"
P04-1061,E95-1020,0,0.316771,"Missing"
P04-1061,C00-2139,0,0.120736,"Missing"
P04-1061,J03-4003,0,\N,Missing
P04-1061,H93-1047,0,\N,Missing
P04-1061,C80-1026,0,\N,Missing
P05-1045,P04-1056,0,0.120344,"me, and we penalize for each occurrance of that word which is not also labeled speaker. For the start and end times the penalty is multiplied in based on how many words are in the entity. For the speaker, the penalty is only multiplied in once. We used a hand selected penalty of exp −4.0. Approach B&M LT-RMN B&M GLT-RMN Local+Viterbi NonLoc+Gibbs LOC ORG MISC PER ALL – – 88.16 88.51 – – 80.83 81.72 – – 78.51 80.43 – – 90.36 92.29 80.09 82.30 85.51 86.86 Table 5: F1 scores of the local CRF and non-local models on the CoNLL 2003 named entity recognition dataset. We also provide the results from Bunescu and Mooney (2004) for comparison. CMU Seminar Announcements Approach STIME ETIME SPEAK S&M CRF 97.5 97.5 88.3 S&M Skip-CRF 96.7 97.2 88.1 Local+Viterbi 96.67 97.36 83.39 NonLoc+Gibbs 97.11 97.89 84.16 LOC ALL 77.3 80.4 89.98 90.00 90.2 90.6 91.85 92.29 Table 6: F1 scores of the local CRF and non-local models on the CMU Seminar Announcements dataset. We also provide the results from Sutton and McCallum (2004) for comparison. At inference time, we then sample from the Markov chain defined by this transition probability. 7 Results and Discussion 6 Combining Sequence Models In the previous section we defined two m"
P05-1045,C02-1025,0,0.0776927,"most recent previous instance of that same token in a prior sentence of the same document. Note that this violates the Markov property, but is achieved by slightly relaxing the requirement of exact inference. Instead of finding the maximum likelihood sequence over the entire document, they classify one sentence at a time, allowing them to condition on the maximum likelihood sequence of previous sentences. This approach is quite effective for enforcing label consistency in many NLP tasks, however, it permits a forward flow of information only, which is not sufficient for all cases of interest. Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document. This approach has the added advantage of allowing the training procedure to automatically learn good weightings for these “global” features relative to the local ones. However, this approach cannot easily be extended to incorporate other types of non-local structure. The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly models long-distance dependencies, and Sutto"
P05-1045,W03-0424,0,0.101188,"onal cost. Taking 100 samples dramatically increases test time. Averaged over 3 runs on both Viterbi and Gibbs, CoNLL testing time increased from 55 to 1738 seconds, and CMU Seminar Announcements testing time increases from 189 to 6436 seconds. 8 Related Work Several authors have successfully incorporated a label consistency constraint into probabilistic sequence model named entity recognition systems. Mikheev et al. (1999) and Finkel et al. (2004) incorporate label consistency information by using adhoc multi-stage labeling procedures that are effective but special-purpose. Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document. Note that this violates the Markov property, but is achieved by slightly relaxing the requirement of exact inference. Instead of finding the maximum likelihood sequence over the entire document, they classify one sentence at a time, allowing them to condition on the maximum likelihood sequence of previous sentences. This approach is quite effective for enforcing label consistency in many NLP tasks, however, it permits a forward"
P05-1045,W04-1217,1,0.116918,"Missing"
P05-1045,E95-1008,0,0.0995047,"F-based statistical NER system, because by looking only at local evidence it is unclear whether it is a person or organization. The first occurrence of Tanjug provides ample evidence that it is an organization, however, and by enforcing label consistency the system should be able to get it right. We show how to incorporate constraints of this form into a CRF model by using Gibbs sampling instead of the Viterbi algorithm as our inference procedure, and demonstrate that this technique yields significant improvements on two established IE tasks. 1 Prior uses in NLP of which we are aware include: Kim et al. (1995), Della Pietra et al. (1997) and Abney (1997). 363 Proceedings of the 43rd Annual Meeting of the ACL, pages 363–370, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics the news agency Tanjug ... reported airport , Tanjug said . Figure 1: An example of the label consistency problem excerpted from a document in the CoNLL 2003 English dataset. 2 Gibbs Sampling for Inference in Sequence Models quence obtained by changing the state at any one position i, and the distribution over these possible transitions is just In hidden state sequence models such as HMMs, CMMs, and CRFs, it"
P05-1045,W02-2019,0,0.127595,"l is the computational cost. Taking 100 samples dramatically increases test time. Averaged over 3 runs on both Viterbi and Gibbs, CoNLL testing time increased from 55 to 1738 seconds, and CMU Seminar Announcements testing time increases from 189 to 6436 seconds. 8 Related Work Several authors have successfully incorporated a label consistency constraint into probabilistic sequence model named entity recognition systems. Mikheev et al. (1999) and Finkel et al. (2004) incorporate label consistency information by using adhoc multi-stage labeling procedures that are effective but special-purpose. Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document. Note that this violates the Markov property, but is achieved by slightly relaxing the requirement of exact inference. Instead of finding the maximum likelihood sequence over the entire document, they classify one sentence at a time, allowing them to condition on the maximum likelihood sequence of previous sentences. This approach is quite effective for enforcing label consistency in many NLP tasks, ho"
P05-1045,E99-1001,0,0.127711,"Missing"
P05-1045,J97-4005,0,\N,Missing
P05-1046,N04-1015,0,0.0181962,"onditional Random Field (CRF) sequence models to the problem of parsing the headers of research papers. There has also been some previous work on unsupervised learning of field segmentation models in particular domains. Pasula et al. (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty. However, their system does not explicitly learn a field segmentation model for the citations, and encodes a large amount of hand-supplied information about name forms, abbreviation schemes, and so on. More recently, Barzilay and Lee (2004) defined content models, which can be viewed as field segmentation models occurring at the level of discourse. They perform unsupervised learning of these models from sets of news articles which describe similar events. The fields in that case are the topics discussed in those articles. They consider a very different set of applications from the present work, and show that the learned topic models improve performance on two discourse-related tasks: information ordering and extractive document summarization. Most importantly, their learning method differs significantly from ours; they use a com"
P05-1046,J97-1003,0,0.0212744,"ation ordering and extractive document summarization. Most importantly, their learning method differs significantly from ours; they use a complex and special purpose algorithm, which is difficult to adapt, while we see our contribution to be a demonstration of the interplay between model family and learned structure. Because the structure of the HMMs they learn is similar to ours it seems that their system could benefit from the techniques of this paper. Finally, Blei and Moreno (2001) use an HMM augmented by an aspect model to automatically segment documents, similar in goal to the system of Hearst (1997), but using techniques more similar to the present work. 7 Conclusions In this work, we have examined the task of learning field segmentation models using unsupervised learning. In two different domains, classified advertisements and bibliographic citations, we showed that by constraining the model class we were able to restrict the search space of EM to models of interest. We used unsupervised learning methods with 400 documents to yield field segmentation models of a similar quality to those learned using supervised learning with 50 documents. We demonstrated that further refinements of the"
P05-1046,N04-1042,0,0.0611895,"e cast as supervised learning of field segmentation models, using various model families and applied to various domains. McCallum et al. (1999) were the first to compare a number of supervised methods for learning HMMs for parsing bibliographic citations. The authors explicitly claim that the domain would be suitable for unsupervised learning, but they do not present experimental results. McCallum et al. (2000) applied supervised learning of Maximum Entropy Markov Models (MEMMs) to the domain of parsing Frequently Asked Question (FAQ) lists into their component field structure. More recently, Peng and McCallum (2004) applied supervised learning of Conditional Random Field (CRF) sequence models to the problem of parsing the headers of research papers. There has also been some previous work on unsupervised learning of field segmentation models in particular domains. Pasula et al. (2002) performs limited unsupervised segmentation of bibliographic citations as a small part of a larger probabilistic model of identity uncertainty. However, their system does not explicitly learn a field segmentation model for the citations, and encodes a large amount of hand-supplied information about name forms, abbreviation sc"
P05-1073,P98-1013,0,0.0556257,"parse trees on PropBank. We propose a discriminative log-linear joint model for semantic role labeling, which incorporates more global features and achieves superior performance in comparison to state-of-the-art models. To deal with the computational complexity of the task, we employ dynamic programming and reranking approaches. We present performance results on the February 2004 version of PropBank on gold-standard parse trees as well as results on automatic parses generated by Charniak’s parser (Charniak, 2000). 1 Introduction The release of semantically annotated corpora such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004). Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1). These features usually characterize aspects of individual arguments and the predicate. It is evident that the labels and the features of arguments are highly correlated. For example, there are hard constraints – that arguments cannot overlap 2 Semantic Role"
P05-1073,W04-2412,0,0.192254,"Missing"
P05-1073,A00-2018,0,0.169312,"Missing"
P05-1073,J02-3001,0,0.906079,"e Stanford University Stanford, CA, 94305 Aria Haghighi Dept of Computer Science Stanford University Stanford, CA, 94305 Christopher D. Manning Dept of Computer Science Stanford University Stanford, CA, 94305 kristina@cs.stanford.edu aria42@stanford.edu manning@cs.stanford.edu Abstract with each other or the predicate, and also soft constraints – for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the active voice will have a THEME argument prior to an AGENT argument. Several systems have incorporated such dependencies, for example, (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Thompson et al., 2003) and several systems submitted in the CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, we show that there are greater gains to be had by modeling joint information about a verb’s argument structure. Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between ar"
P05-1073,J93-2004,0,0.0342212,"eling, we assume the existence of a separate parsing model that can assign a parse tree t to each sentence, and the task then is to label each node in the parse tree with the semantic role of the phrase it dominates, or NONE, if the phrase does not fill any role. We do stress however that the joint framework and features proposed here can also be used when only a shallow parse (chunked) representation is available as in the CoNLL-2004 shared task (Carreras and M`arquez, 2004). In the February 2004 version of the PropBank corpus, annotations are done on top of the Penn TreeBank II parse trees (Marcus et al., 1993). Possible labels of arguments in this corpus are the core argument labels ARG [0, 1, 2, 3, 4, 5], and the modifier argument labels. The core arguments ARG [3, 4, 5] do not have consistent global roles and tend to be verb specific. There are about 14 modifier labels such as ARGM - LOC and ARGM - TMP, for location and temporal modifiers respectively.1 Figure 1 shows an example parse tree annotated with semantic roles. We distinguish between models that learn to label nodes in the parse tree independently, called local models, and models that incorporate dependencies among the labels of multiple"
P05-1073,N04-1030,0,0.77673,"ord, CA, 94305 Aria Haghighi Dept of Computer Science Stanford University Stanford, CA, 94305 Christopher D. Manning Dept of Computer Science Stanford University Stanford, CA, 94305 kristina@cs.stanford.edu aria42@stanford.edu manning@cs.stanford.edu Abstract with each other or the predicate, and also soft constraints – for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the active voice will have a THEME argument prior to an AGENT argument. Several systems have incorporated such dependencies, for example, (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Thompson et al., 2003) and several systems submitted in the CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, we show that there are greater gains to be had by modeling joint information about a verb’s argument structure. Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments. We show how t"
P05-1073,W04-2421,0,0.0181359,"tion for CORE arguments in F-Measure and Frame Accuracy respectively. Model Local Joint C ORE F1 84.1 85.8 Acc. 66.5 72.7 F1 81.4 82.9 A RGM Acc. 55.6 60.8 Table 5: Performance of local and joint models on identification+classification on section 23, using Charniak automatically generated parse trees. References 6 Related Work Several semantic role labeling systems have successfully utilized joint information. (Gildea and Jurafsky, 2002) used the empirical probability of the set of proposed arguments as a prior distribution. (Pradhan et al., 2004) train a language model over label sequences. (Punyakanok et al., 2004) use a linear programming framework to ensure that the only argument frames which get probability mass are ones that respect global constraints on argument labels. The key differences of our approach compared to previous work are that our model has all of the following properties: (i) we do not assume a finite Markov horizon for dependencies among node labels, (ii) we include features looking at the labels of multiple argument nodes and internal features of these nodes, and (iii) we train a discriminative model capable of incorporating these long-distance dependencies. 7 Conclusions Reflecting"
P05-1073,P03-1002,0,0.537983,"his way the training set for the classification models is smaller. Note that we don’t do any hard pruning at the identification stage in testing and can find the exact labeling of the complete parse tree, which is the maximizer of Equation 1. Thus we do not have accuracy loss as in the two-pass hard prune strategy described in (Pradhan et al., 2005). In previous work, various machine learning methods have been used to learn local classifiers for role labeling. Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al., 2004), decision trees (Surdeanu et al., 2003), and log-linear models (Xue and Palmer, 2004). In this work we use log-linear models for multi-class classification. One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification A problem with this approach is that a maximizing labeling of the nodes could possibly violate the constraint that argument nodes should not overlap with each other. Therefore, to produce a consistent set of arguments with local classifiers, we must have a way of enforcing the non-overlapping constraint. Standard Features (Gildea and Jurafsky, 2002) P HRASE"
P05-1073,W04-3212,0,0.684627,"e-art models. To deal with the computational complexity of the task, we employ dynamic programming and reranking approaches. We present performance results on the February 2004 version of PropBank on gold-standard parse trees as well as results on automatic parses generated by Charniak’s parser (Charniak, 2000). 1 Introduction The release of semantically annotated corpora such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004). Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1). These features usually characterize aspects of individual arguments and the predicate. It is evident that the labels and the features of arguments are highly correlated. For example, there are hard constraints – that arguments cannot overlap 2 Semantic Role Labeling: Task Definition and Architectures Consider the pair of sentences, • [The GM-Jaguar pact] AGENT gives [the car market]RECIPIENT [a much-needed boost] THEME • [A much-needed boost] THEME was given to"
P05-1073,A00-2031,0,\N,Missing
P05-1073,C04-1197,0,\N,Missing
P05-1073,J03-4003,0,\N,Missing
P05-1073,C98-1013,0,\N,Missing
P05-1073,W05-0620,0,\N,Missing
P05-1073,J05-1004,0,\N,Missing
P06-1141,P04-1056,0,0.642739,"Computational Linguistics told that Albert Einstein ... proved on seeing Einstein at the Figure 1: An example of the label consistency problem. Here we would like our model to encourage entities Albert Einstein and Einstein to get the same label, so as to improve the chance that both are labeled PERSON. nately cannot model this due to their Markovian assumption. Recent approaches attempting to capture nonlocal dependencies model the non-local dependencies directly, and use approximate inference algorithms, since exact inference is in general, not tractable for graphs with non-local structure. Bunescu and Mooney (2004) define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities. Sutton and McCallum (2004) augment a sequential CRF with skip-edges i.e. edges between different occurrences of a token, in a document. Both these approaches use loopy belief propagation (Pearl, 1988; Yedidia et al., 2000) for approximate inference. Finkel et al. (2005) hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data. They then employ Gibbs sampling (Geman and G"
P06-1141,C02-1025,0,0.0281981,"this intuition to approximate the aggregate information about labels assigned to other occurrences of the entity by the nonlocal model, with the aggregate information about labels assigned to other occurrences of the entity by the sequence model. This intuition enables us to learn weights for non-local dependencies in two stages; we first get predictions from a regular sequential CRF and in turn use aggregate information about predictions made by the CRF as extra features to train a second CRF. • Most work has looked to model non-local dependencies only within a document (Finkel et al., 2005; Chieu and Ng, 2002; Sutton and McCallum, 2004; Bunescu and Mooney, 2004). Our model can capture the weaker but still important consistency constraints across the whole document collection, whereas previous work has not, for reasons of tractability. Capturing label-consistency at the level of the whole test corpus is particularly helpful for token sequences that appear only once in their documents, but occur a few times over the corpus, since they do not have strong nonlocal information from within the document. • For training our second-stage CRF, we need to get predictions on our train data as well as test dat"
P06-1141,W03-0424,0,0.0272092,"rly work in discriminative NER employed two stage approaches that are broadly similar to ours, but the effectiveness of this approach appears to have been overlooked in more recent work. Mikheev et al. (1999) exploit label consistency information within a document using relatively ad hoc multi-stage labeling procedures. Borth1127 wick (1999) used a two-stage approach similar to ours with CMM’s where Reference Resolution features which encoded the frequency of occurrences of other entities similar to the current token sequence, were derived from the output of the first stage. Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a previous sentence of the same document. This violates the Markov property and therefore instead of finding the maximum likelihood sequence over the entire document (exact inference), they label one sentence at a time, which allows them to condition on the maximum likelihood sequence of previous sentences. While this approach is quite effective for enforcing label consistency in many NLP tasks, it permits a forward flow of information only, which can result in loss"
P06-1141,P05-1045,1,0.823638,"ncies model the non-local dependencies directly, and use approximate inference algorithms, since exact inference is in general, not tractable for graphs with non-local structure. Bunescu and Mooney (2004) define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities. Sutton and McCallum (2004) augment a sequential CRF with skip-edges i.e. edges between different occurrences of a token, in a document. Both these approaches use loopy belief propagation (Pearl, 1988; Yedidia et al., 2000) for approximate inference. Finkel et al. (2005) hand-set penalties for inconsistency in entity labeling at different occurrences in the text, based on some statistics from training data. They then employ Gibbs sampling (Geman and Geman, 1984) for dealing with their local feature weights and their non-local penalties to do approximate inference. We present a simple two-stage approach where our second CRF uses features derived from the output of the first CRF. This gives us the advantage of defining a rich set of features to model non-local dependencies, and also eliminates the need to do approximate inference, since we do not explicitly cap"
P06-1141,W02-2019,0,0.0168119,"Gibbs sampling. Early work in discriminative NER employed two stage approaches that are broadly similar to ours, but the effectiveness of this approach appears to have been overlooked in more recent work. Mikheev et al. (1999) exploit label consistency information within a document using relatively ad hoc multi-stage labeling procedures. Borth1127 wick (1999) used a two-stage approach similar to ours with CMM’s where Reference Resolution features which encoded the frequency of occurrences of other entities similar to the current token sequence, were derived from the output of the first stage. Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a previous sentence of the same document. This violates the Markov property and therefore instead of finding the maximum likelihood sequence over the entire document (exact inference), they label one sentence at a time, which allows them to condition on the maximum likelihood sequence of previous sentences. While this approach is quite effective for enforcing label consistency in many NLP tasks, it permits a forward flow of information onl"
P06-1141,E99-1001,0,0.0315345,"04), because they do not need to make any initial assumptions about which nodes should be connected and they too model dependencies between whole token sequences representing entities and between entity token sequences and their token supersequences that are entities. The disadvantage of their approach is the relatively ad-hoc selection of penalties and the high computational cost of running Gibbs sampling. Early work in discriminative NER employed two stage approaches that are broadly similar to ours, but the effectiveness of this approach appears to have been overlooked in more recent work. Mikheev et al. (1999) exploit label consistency information within a document using relatively ad hoc multi-stage labeling procedures. Borth1127 wick (1999) used a two-stage approach similar to ours with CMM’s where Reference Resolution features which encoded the frequency of occurrences of other entities similar to the current token sequence, were derived from the output of the first stage. Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a previous sentence of the same document. This violates"
P06-1141,N03-1028,0,0.10188,"n a single model, like the more complex existing approaches. This also enables us to do inference efficiently since our inference time is merely the inference time of two sequential CRF’s; in contrast Finkel et al. (2005) reported an increase in running time by a factor of 30 over the sequential CRF, with their Gibbs sampling approximate inference. In all, our approach is simpler, yields higher F1 scores, and is also much more computationally efficient than existing approaches modeling nonlocal dependencies. 2 Conditional Random Fields We use a Conditional Random Field (Lafferty et al., 2001; Sha and Pereira, 2003) since it represents the state of the art in sequence modeling and has also been very effective at Named Entity Recognition. It allows us both discriminative training that CMMs offer as well and the bi-directional flow of probabilistic information across the sequence that HMMs allow, thereby giving us the best of both worlds. Due to the bi-directional flow of information, CRFs guard against the myopic locally attractive decisions that CMMs make. It is customary to use the Viterbi algorithm, to find the most probably state sequence during inference. A large number of possibly redundant and corr"
P06-1141,C00-2137,0,0.0388894,"Missing"
P07-1035,A00-2018,0,0.0114619,"es (“tags”), the observations they generate represent the words themselves, and the tree structure represents syntactic dependencies between pairs of tags. To validate the model, we test unsupervised learning of tags conditioned on a given dependency tree structure. This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996). Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000), manually split the tagset into a finer-grained one (Klein and Manning, 2003a), or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006). We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training. Model-based unsupervised learning techniques have historically lacked good methods for choosing the number of unseen components. For example, kmeans or EM clustering require advance specification of the number of mixture components. But"
P07-1035,J03-4003,0,0.0616793,"t word categories (“tags”), the observations they generate represent the words themselves, and the tree structure represents syntactic dependencies between pairs of tags. To validate the model, we test unsupervised learning of tags conditioned on a given dependency tree structure. This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996). Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000), manually split the tagset into a finer-grained one (Klein and Manning, 2003a), or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006). We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training. Model-based unsupervised learning techniques have historically lacked good methods for choosing the number of unseen components. For example, kmeans or EM clustering require advance specification of the number of mixtur"
P07-1035,N06-1041,0,0.00851842,"eger names, and are not guaranteed to correlate with the POS tag definitions. We found that the choice of α0 and β (the concentration parameters) did not affect the output much, while the value of ρ (the parameter for the base Dirichlet distribution) made a much larger difference. For all reported experiments, we set α0 = β = 10 and varied ρ. We use several metrics to evaluate the word classes. First, we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap, and then computing tagging accuracy (Smith and Eisner, 2005; Haghighi and Klein, 2006).8 Additionally, we compute the mutual information of the learned clusters with the gold tags, and we compute the cluster F-score (Ghosh, 2003). See Table 1 for results of the different models, parameter settings, and metrics. Given the variance in the number of classes learned it is a little difficult to interpret these results, but it is clear that the Markov child model is the best; it achieves superior performance to the independent child model on all metrics, while learning fewer word classes. The poor performance of the simultaneous model warrants further investigation, but we observed t"
P07-1035,P03-1054,1,0.0696074,"elves, and the tree structure represents syntactic dependencies between pairs of tags. To validate the model, we test unsupervised learning of tags conditioned on a given dependency tree structure. This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996). Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000), manually split the tagset into a finer-grained one (Klein and Manning, 2003a), or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006). We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training. Model-based unsupervised learning techniques have historically lacked good methods for choosing the number of unseen components. For example, kmeans or EM clustering require advance specification of the number of mixture components. But the introduction of nonparametric priors such as the Dirichlet process (Ferg"
P07-1035,J93-2004,0,0.0307199,"e modification of the independent for the sequence generating DP. Lastly, we have the child tree is trivial: we have two copies of each of 277 t ∈c(t) the variables πk , one each for the left and the right. Generation of dependents on the right is completely independent of that for the left. The modifications of the other models are similar, but now there are separate sets of πk variables for the Markov child model, and separate Lk and λk variables for the simultaneous child model, for each of the left and right. For both experiments, we used dependency trees extracted from the Penn Treebank (Marcus et al., 1993) using the head rules and dependency extractor from Yamada and Matsumoto (2003). As is standard, we used WSJ sections 2–21 for training, section 22 for development, and section 23 for testing. 6.1 Unsupervised POS Learning In the first experiment, we do unsupervised part-ofspeech learning conditioned on dependency trees. To be clear, the input to our algorithm is the dependency structure skeleton of the corpus, but not the POS tags, and the output is a labeling of each of the words in the tree for word class. Since the model knows nothing about the POS annotation, the new classes have arbitrar"
P07-1035,P06-1055,0,0.0594092,"del, we test unsupervised learning of tags conditioned on a given dependency tree structure. This is useful, because coarse-grained syntactic categories, such as those used in the Penn Treebank (PTB), make insufficient distinctions to be the basis of accurate syntactic parsing (Charniak, 1996). Hence, state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves (Collins, 2003; Charniak, 2000), manually split the tagset into a finer-grained one (Klein and Manning, 2003a), or learn finer grained tag distinctions using a heuristic learning procedure (Petrov et al., 2006). We demonstrate that the tags learned with our model are correlated with the PTB POS tags, and furthermore that they improve the accuracy of an automatic parser when used in training. Model-based unsupervised learning techniques have historically lacked good methods for choosing the number of unseen components. For example, kmeans or EM clustering require advance specification of the number of mixture components. But the introduction of nonparametric priors such as the Dirichlet process (Ferguson, 1973) enabled development of infinite mixture models, in which the num- 2 Finite Trees ber of hi"
P07-1035,P05-1044,0,0.0222345,"asses have arbitrary integer names, and are not guaranteed to correlate with the POS tag definitions. We found that the choice of α0 and β (the concentration parameters) did not affect the output much, while the value of ρ (the parameter for the base Dirichlet distribution) made a much larger difference. For all reported experiments, we set α0 = β = 10 and varied ρ. We use several metrics to evaluate the word classes. First, we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap, and then computing tagging accuracy (Smith and Eisner, 2005; Haghighi and Klein, 2006).8 Additionally, we compute the mutual information of the learned clusters with the gold tags, and we compute the cluster F-score (Ghosh, 2003). See Table 1 for results of the different models, parameter settings, and metrics. Given the variance in the number of classes learned it is a little difficult to interpret these results, but it is clear that the Markov child model is the best; it achieves superior performance to the independent child model on all metrics, while learning fewer word classes. The poor performance of the simultaneous model warrants further inves"
P07-1035,W03-3023,0,0.0159766,", we have the child tree is trivial: we have two copies of each of 277 t ∈c(t) the variables πk , one each for the left and the right. Generation of dependents on the right is completely independent of that for the left. The modifications of the other models are similar, but now there are separate sets of πk variables for the Markov child model, and separate Lk and λk variables for the simultaneous child model, for each of the left and right. For both experiments, we used dependency trees extracted from the Penn Treebank (Marcus et al., 1993) using the head rules and dependency extractor from Yamada and Matsumoto (2003). As is standard, we used WSJ sections 2–21 for training, section 22 for development, and section 23 for testing. 6.1 Unsupervised POS Learning In the first experiment, we do unsupervised part-ofspeech learning conditioned on dependency trees. To be clear, the input to our algorithm is the dependency structure skeleton of the corpus, but not the POS tags, and the output is a labeling of each of the words in the tree for word class. Since the model knows nothing about the POS annotation, the new classes have arbitrary integer names, and are not guaranteed to correlate with the POS tag definitio"
P08-1044,W96-0213,0,0.0946589,"nels (uh-huh, mm-hm), guesses (where the transcribers were unsure of the correct words), and full words (everything else). Error rates for each of these types can be found in Table 1. The remainder of our analysis considers only the 36159 invocabulary full words in the reference transcriptions (70 OOV full words are excluded). We collected the following features for these words: Speaker sex Male or female. Broad syntactic class Open class (e.g., nouns and verbs), closed class (e.g., prepositions and articles), or discourse marker (e.g., okay, well). Classes were identified using a POS tagger (Ratnaparkhi, 1996) trained on the tagged Switchboard corpus. Log probability The unigram log probability of each word, as listed in the system’s language model. Word length The length of each word (in phones), determined using the most frequent pronunciation BefRep FirRep MidRep LastRep AfRep BefFP AfFP BefFr AfFr yeah i i i think you should um ask for the ref- recommendation Figure 1: Example illustrating disfluency features: words occurring before and after repetitions, filled pauses, and fragments; first, middle, and last words in a repeated sequence. found for that word in the recognition lattices. 3.2 Resu"
P08-1109,P05-1022,0,0.611349,"Missing"
P08-1109,W00-0717,0,0.014091,"wo kinds of experiments: a discriminatively trained model, which used only the rules and no other grammar features, and a featurebased model which did make use of grammar features. Both models had access to the lexicon features. We viewed this as equivalent to the more elaborate, smoothed unknown word models that are common in many PCFG parsers, such as (Klein and Manning, 2003; Petrov et al., 2006). We preprocessed the words in the sentences to obtain two extra pieces of information. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features. The full set of features, along with an explanation of our notation, is listed in Table 1. 5 Experiments For all experiments, we trained and tested on the Penn treebank (PTB) (Marcus et al., 1993). We used 963 States 1,428 1,428 7,613 Binary Rules 5,818 22,376 28,240 Unary Rules 423 613 823 Table 2: Gramm"
P08-1109,W05-0622,0,0.122071,"Missing"
P08-1109,P81-1022,0,0.811297,"arses sum to unity, is defined over all structures as well as all labelings of those structures. We define τ (s) to be the set of all possible parse trees for the given sentence licensed by the grammar G. P(t|s; θ ) = where 1 φ (r|s; θ ) Zs ∏r∈t (1) Zs = ∑t∈τ (s) ∏r∈t ′ φ (r|s; θ ) The above model is not well-defined over all CFGs. Unary rules of the form N i → N j can form cycles, leading to infinite unary chains with infinite mass. However, it is standard in the parsing literature to transform grammars into a restricted class of CFGs so as to permit efficient parsing. Binarization of rules (Earley, 1970) is necessary to obtain cubic parsing time, and closure of unary chains is required for finding total probability mass (rather than just best parses) (Stolcke, 1995). To address this issue, we define our model over a restricted class of S VP NP NN NNS Factory payrolls VBD fell PP IN NN in September Phrasal rules r1 = S0,5 → NP0,2 VP2,5 |Factory payrolls fell in September r3 = VP2,5 → VBD2,3 PP3,5 |Factory payrolls fell in September ... Lexicon rules r5 = NN0,1 → Factory |Factory payrolls fell in September r6 = NNS1,2 → payrolls |Factory payrolls fell in September ... (a) PCFG Structure (b) Rul"
P08-1109,P04-1013,0,0.0125913,"sentences, the disk I/O is easily worth the time compared to recomputation. The first time we see a sentence this method is still about one third faster than if we did not do the prefiltering, and on subsequent iterations the improvement is closer to tenfold. 3 Stochastic Optimization Methods Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al., 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent. Standard deterministic optimization routines such as L-BFGS (Liu and Nocedal, 1989) make little progress in the initial iterations, often requiring several passes through the data in order to satisfy sufficient descent conditions placed on line searches. In our experiments SGD converged to a lower objective function value than L-BFGS, however it required far 962 Log Likelihood 2.4 −1.5 −2 −2.5 SGD L−BFGS −3 −3.5 0 5 10 15 20 25 30 35 40 45 50 Passes Figure 2: WSJ15 objective value for L-BFGS and SGD versus passes through the data. SGD ultimately conve"
P08-1109,P01-1042,0,0.505681,"b(p(r p )), ds(we )i PP feature: hb(t), wi unary? if right child is a PP then hr, ws i hb(t), lc(w)i simplified rule: VP features: ht, ds(w)i base labels of states if some child is a verb tag, then rule, ht, ds(w−1 )i dist sim bigrams: with that child replaced by the word ht, ds(w+1 )i all dist. sim. bigrams below hb(t), ds(w)i rule, and base parent state Unaries which span one word: hb(t), ds(w−1 )i dist sim bigrams: hb(t), ds(w+1 )i same as above, but trigrams hr, wi hp(t), wi heavy feature: hr, ds(w)i ht, unk(w)i whether the constituent is “big” hb(p(r)), wi hb(t), unk(w)i as described in (Johnson, 2001) hb(p(r)), ds(w)i these experiments; the length 15 models had a batch size of 15 and we allowed twenty passes through the data.3 The length 40 models had a batch size of 30 and we allowed ten passes through the data. We used development data to decide when the models had converged. Additionally, we provide generative numbers for training on the entire PTB to give a sense of how much performance suffered from the reduced training data (generative-all in Table 4). The full results for WSJ15 are shown in Table 3 and for WSJ40 are shown in Table 4. The WSJ15 models were each trained on a single Du"
P08-1109,P03-1054,1,0.113337,"to be infeasible. Our features are divided into two types: lexicon features, which are over words and tags, and grammar features which are over the local subtrees and corresponding span/split (both have access to the entire sentence). We ran two kinds of experiments: a discriminatively trained model, which used only the rules and no other grammar features, and a featurebased model which did make use of grammar features. Both models had access to the lexicon features. We viewed this as equivalent to the more elaborate, smoothed unknown word models that are common in many PCFG parsers, such as (Klein and Manning, 2003; Petrov et al., 2006). We preprocessed the words in the sentences to obtain two extra pieces of information. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features. The full set of features, along with an explanation of our notation, is li"
P08-1109,J93-2004,0,0.050021,"mation. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features. The full set of features, along with an explanation of our notation, is listed in Table 1. 5 Experiments For all experiments, we trained and tested on the Penn treebank (PTB) (Marcus et al., 1993). We used 963 States 1,428 1,428 7,613 Binary Rules 5,818 22,376 28,240 Unary Rules 423 613 823 Table 2: Grammar size for each of our models. the standard splits, training on sections 2 to 21, testing on section 23 and doing development on section 22. Previous work on (non-reranking) discriminative parsing has given results on sentences of length ≤ 15, but most parsing literature gives results on either sentences of length ≤ 40, or all sentences. To properly situate this work with respect to both sets of literature we trained models on both length ≤ 15 (WSJ15) and length ≤ 40 (WSJ40), and we a"
P08-1109,P06-1055,0,0.229172,"tures are divided into two types: lexicon features, which are over words and tags, and grammar features which are over the local subtrees and corresponding span/split (both have access to the entire sentence). We ran two kinds of experiments: a discriminatively trained model, which used only the rules and no other grammar features, and a featurebased model which did make use of grammar features. Both models had access to the lexicon features. We viewed this as equivalent to the more elaborate, smoothed unknown word models that are common in many PCFG parsers, such as (Klein and Manning, 2003; Petrov et al., 2006). We preprocessed the words in the sentences to obtain two extra pieces of information. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Secondly, we compute a class for each word based on the unknown word model of Klein and Manning (2003); this model takes into account capitalization, digits, dashes, and other character-level features. The full set of features, along with an explanation of our notation, is listed in Table 1. 5 Exp"
P08-1109,W97-0301,0,0.134616,"Missing"
P08-1109,J95-2002,0,0.142927,"iven sentence licensed by the grammar G. P(t|s; θ ) = where 1 φ (r|s; θ ) Zs ∏r∈t (1) Zs = ∑t∈τ (s) ∏r∈t ′ φ (r|s; θ ) The above model is not well-defined over all CFGs. Unary rules of the form N i → N j can form cycles, leading to infinite unary chains with infinite mass. However, it is standard in the parsing literature to transform grammars into a restricted class of CFGs so as to permit efficient parsing. Binarization of rules (Earley, 1970) is necessary to obtain cubic parsing time, and closure of unary chains is required for finding total probability mass (rather than just best parses) (Stolcke, 1995). To address this issue, we define our model over a restricted class of S VP NP NN NNS Factory payrolls VBD fell PP IN NN in September Phrasal rules r1 = S0,5 → NP0,2 VP2,5 |Factory payrolls fell in September r3 = VP2,5 → VBD2,3 PP3,5 |Factory payrolls fell in September ... Lexicon rules r5 = NN0,1 → Factory |Factory payrolls fell in September r6 = NNS1,2 → payrolls |Factory payrolls fell in September ... (a) PCFG Structure (b) Rules r Figure 1: A parse tree and the corresponding rules over which potentials and features are defined. CFGs which limits unary chains to not have any repeated state"
P08-1109,W04-3201,1,0.909986,"algorithm for a particular sentence is O(n3 ), where n is the length of the sentence. 2.3 !  θ2 ∑ ∑ θi fi(r, s) − Zs + ∑ 2σi 2 (3) r∈t i i And the partial derivatives of the log likelihood, with respect to the model weights are, as usual, the difference between the empirical counts and the model expectations: !   ∂L θi = fi (r, s) − Eθ [ fi |s] + 2 (4) ∑ ∑ ∂ θi σ (t,s)∈D r∈t 1 In our implementation of the inside-outside algorithm, we then need to keep two inside and outside scores for each span: one from before and one from after the application of unary rules. 961 Parallelization Unlike (Taskar et al., 2004), our algorithm has the advantage of being easily parallelized (see footnote 7 in their paper). Because the computation of both the log likelihood and the partial derivatives involves summing over each tree individually, the computation can be parallelized by having many clients which each do the computation for one tree, and one central server which aggregates the information to compute the relevant information for a set of trees. Because we use a stochastic optimization method, as discussed in Section 3, we compute the objective for only a small portion of the training data at a time, typica"
P08-1109,P06-1110,0,0.0222213,"hieved only small gains. We suspect that this is in part due to the grammar that they chose – the grammar of (Klein and Manning, 2003), which was hand annotated with the intent of optimizing performance of a PCFG. This 965 grammar is fairly sparse – for any particular state there are, on average, only a few rules with that state as a parent – so the learning algorithm may have suffered because there were few options to discriminate between. Starting with this grammar we found it difficult to achieve gains as well. Additionally, their long training time (several months for WSJ15, according to (Turian and Melamed, 2006)) made feature engineering difficult; they were unable to really explore the space of possible features. More recent is the work of (Turian and Melamed, 2006; Turian et al., 2007), which improved both the training time and accuracy of (Taskar et al., 2004). They define a simple linear model, use boosted decision trees to select feature conjunctions, and a line search to optimize the parameters. They use an agenda parser, and define their atomic features, from which the decision trees are constructed, over the entire state being considered. While they make extensive use of features, their setup"
P08-1118,W07-1427,1,0.171238,"Missing"
P08-1118,W04-3205,0,0.0679853,"well as world knowledge (WK). We consider contradictions in category (1) ‘easy’ because they can often be automatically detected without full sentence comprehension. For example, if words in the two passages are antonyms and the sentences are reasonably similar, especially in polarity, a contradiction occurs. Additionally, little external information is needed to gain broad coverage of antonymy, negation, and numeric mismatch contradictions; each involves only a closed set of words or data that can be obtained using existing resources and techniques (e.g., WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)). However, contradictions in category (2) are more difficult to detect automatically because they require precise models of sentence meaning. For instance, 1041 to find the contradiction in example 8 (table 1), it is necessary to learn that X said Y did nothing wrong and X accuses Y are incompatible. Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. Example 9 provides an even"
P08-1118,de-marneffe-etal-2006-generating,1,0.0221103,"Missing"
P08-1118,W07-1401,0,0.101463,"Missing"
P08-1118,W97-1311,0,0.0472477,"sentences is often a good cue of non-entailment (Vanderwende et al., 2006), it is not sufficient for contradiction detection which requires more precise comprehension of the consequences of sentences. Assessing event coreference is also essential: for texts to contradict, they must 1039 Proceedings of ACL-08: HLT, pages 1039–1047, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics refer to the same event. The importance of event coreference was recognized in the MUC information extraction tasks in which it was key to identify scenarios related to the same event (Humphreys et al., 1997). Recent work in text understanding has not focused on this issue, but it must be tackled in a successful contradiction system. Our system includes event coreference, and we present the first detailed examination of contradiction detection performance, on the basis of our typology. 2 Related work Little work has been done on contradiction detection. The PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) focused on textual inference in any domain. Condoravdi et al. (2003) first recognized the importance of handling entail"
P08-1118,P03-1054,1,0.00685307,"http://nlp.stanford.edu/projects/contradiction. of an event is acquired over time (e.g., a rising death toll) or various parties have divergent views of an event (e.g., example 9 in table 1). 4 System overview Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al., 2006), but adds a stage for event coreference decision. 4.1 Linguistic analysis The first stage computes linguistic representations containing information about the semantic content of the passages. The text and hypothesis are converted to typed dependency graphs produced by the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006). To improve the dependency graph as a pseudo-semantic representation, collocations in WordNet and named entities are collapsed, causing entities and multiword relations to become single nodes. 4.2 Alignment between graphs The second stage provides an alignment between text and hypothesis graphs, consisting of a mapping from each node in the hypothesis to a unique node in the text or to null. The scoring measure uses node similarity (irrespective of polarity) and structural information based on the dependency graphs. Similarity measures and structural information are"
P08-1118,N06-1006,1,0.801024,"Missing"
P08-1118,P02-1047,0,0.0619565,"ymy, negation, and numeric mismatch contradictions; each involves only a closed set of words or data that can be obtained using existing resources and techniques (e.g., WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)). However, contradictions in category (2) are more difficult to detect automatically because they require precise models of sentence meaning. For instance, 1041 to find the contradiction in example 8 (table 1), it is necessary to learn that X said Y did nothing wrong and X accuses Y are incompatible. Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. Example 9 provides an even more difficult instance of contradiction created by a lexical discrepancy. Structural issues also create contradictions (examples 6 and 7). Lexical complexities and variations in the function of arguments across verbs can make recognizing these contradictions complicated. Even when similar verbs are used and argument differences exist, structural differences may indicate non-entailment or"
P08-1118,W06-3907,0,0.0192452,"ivity features. The context in which a verb phrase is embedded may give rise to contradiction, as in example 5 (table 1). Negation influences some factivity patterns: Bill forgot to take his wallet contradicts Bill took his wallet while Bill did not forget to take his wallet does not contradict Bill took his wallet. For each text/hypothesis pair, we check the (grand)parent of the text word aligned to the hypothesis verb, and generate a feature based on its factivity class. Factivity classes are formed by clustering our expansion of the PARC lists of factive, implicative and non-factive verbs (Nairn et al., 2006) according to how they create contradiction. Modality features. Simple patterns of modal reasoning are captured by mapping the text and hypothesis to one of six modalities ((not )possible, (not )actual, (not )necessary), according to the presence of predefined modality markers such as can or maybe. A feature is produced if the text/hypothesis modality pair gives rise to a contradiction. For instance, the following pair will be mapped to the contradiction judgment (possible, not possible): T: The trial court may allow the prevailing party reasonable attorney fees as part of costs. H: The prevai"
P08-1118,W07-1029,0,0.0356958,"Missing"
P08-1118,P08-1008,0,0.164003,"Missing"
P08-1118,W05-1206,0,0.0893975,"ch turned violent when a woman stabbed her partner because she didn’t want to watch the game. (2) A woman passionately wanted to watch the game. We also mark as contradictions pairs reporting contradictory statements. The following sentences refer to the same event (de Menezes in a subway station), and display incompatible views of this event: (1) Eyewitnesses said de Menezes had jumped over the turnstile at Stockwell subway station. (2) The documents leaked to ITV News suggest that Menezes walked casually into the subway station. This example contains an “embedded contradiction.” Contrary to Zaenen et al. (2005), we argue that recognizing embedded contradictions is important for the application of a contradiction detection system: if John thinks that he is incompetent, and his boss believes that John is not being given a chance, one would like to detect that the targeted information in the two sentences is contradictory, even though the two sentences can be true simultaneously. 3.2 Typology of contradictions Contradictions may arise from a number of different constructions, some overt and others that are comID 1 Type Antonym Text Capital punishment is a catalyst for more crime. 2 Negation 3 Numeric 4"
P08-1118,W07-1412,0,0.0152219,"the amount of available information. Contradiction detection could also be applied to intelligence reports, demonstrating which information may need further verification. In bioinforThis pair is contradictory: defused rockets cannot go off, and thus cannot injure anyone. Detecting contradictions appears to be a harder task than detecting entailments. Here, it is relatively easy to identify the lack of entailment: the first sentence involves no injuries, so the second is unlikely to be entailed. Most entailment systems function as weak proof theory (Hickl et al., 2006; MacCartney et al., 2006; Zanzotto et al., 2007), but contradictions require deeper inferences and model building. While mismatching information between sentences is often a good cue of non-entailment (Vanderwende et al., 2006), it is not sufficient for contradiction detection which requires more precise comprehension of the consequences of sentences. Assessing event coreference is also essential: for texts to contradict, they must 1039 Proceedings of ACL-08: HLT, pages 1039–1047, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics refer to the same event. The importance of event coreference was recognized in th"
P08-1118,W03-0906,0,\N,Missing
P08-1118,W07-1415,0,\N,Missing
P08-1118,N03-1022,0,\N,Missing
P08-1118,W07-1400,0,\N,Missing
P08-2012,N07-1030,0,0.748371,"entions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier’s evaluation of that pair. Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b), the type of classifier (Denis and Baldridge, 2007), and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b). This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an"
P08-2012,P05-1045,1,0.0745529,"te that there are no changes from the D&B- STYLE baseline system at training time. This constraint ensures that whenever xhi,ji = xhj,ki = 1 it must also be the case that xhi,ki = 1. 3 Experiments We used lp solve3 to solve our ILP optimization problems. We ran experiments on two datasets. We used the MUC-6 formal training and test data, as well as the NWIRE and BNEWS portions of the ACE (Phase 2) corpus. This corpus had a third portion, NPAPER , but we found that several documents where too long for lp solve to find a solution.4 We added named entity (NE) tags to the data using the tagger of Finkel et al. (2005). The ACE data is already annotated with NE tags, so when they conflicted they overrode the tags output by the tagger. We also added part of speech (POS) tags to the data using the tagger of Toutanova et al. (2003), and used the tags to decide if mentions were plural or singular. The ACE data is labeled with mention type (pronominal, nominal, and name), but the MUC6 data is not, so the POS and NE tags were used to infer this information. Our feature set was simple, and included many features from (Soon et al., 2001), including the pronoun, string match, definite and demonstrative NP, number an"
P08-2012,P04-1018,0,0.685312,"mentions, and then at test time we use an ILP solver equipped with transitivity constraints to find the most likely legal assignment to the variables which represent the pairwise decisions.1 Our results show a significant improvement compared to the na¨ıve use of the pairwise classifier. Other work on global models of coreference (as 1 A legal assignment is one which respects transitive closure. 45 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 45–48, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics opposed to pairwise models) has included: Luo et al. (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; McCallum and Wellner (2004) who defined several conditional random field-based models; Ng (2005) who took a reranking approach; and Culotta et al. (2006) who use a probabilistic first-order logic model. 2 Coreference Resolution For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by Denis and"
P08-2012,C02-1139,0,0.149563,"a decision tree classifier to label pairs of mentions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier’s evaluation of that pair. Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b), the type of classifier (Denis and Baldridge, 2007), and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b). This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity"
P08-2012,P02-1014,0,0.0775731,"a decision tree classifier to label pairs of mentions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier’s evaluation of that pair. Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b), the type of classifier (Denis and Baldridge, 2007), and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b). This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity"
P08-2012,P04-1020,0,0.0320256,"classifier’s evaluation of that pair. Much work that followed improved upon this strategy, by improving the features (Ng and Cardie, 2002b), the type of classifier (Denis and Baldridge, 2007), and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b). This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. More recently, Denis and Baldridge (2007) utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers. However, when encoding constraints into their ILP solver, they di"
P08-2012,P05-1020,0,0.0266841,"na¨ıve use of the pairwise classifier. Other work on global models of coreference (as 1 A legal assignment is one which respects transitive closure. 45 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 45–48, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics opposed to pairwise models) has included: Luo et al. (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; McCallum and Wellner (2004) who defined several conditional random field-based models; Ng (2005) who took a reranking approach; and Culotta et al. (2006) who use a probabilistic first-order logic model. 2 Coreference Resolution For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by Denis and Baldridge (2007). 2.1 Pairwise Classification Our baseline systems are based on a logistic classifier over pairs of mentions. The probability of a pair of mentions takes the standard logistic form:  P (xhi,ji |mi , mj ; θ) = 1 + e−f (mi ,mj )"
P08-2012,J01-4004,0,0.904457,"ature weights we wish to learn; and xhi,ji is a boolean variable which takes value 1 if mi and mj are coreferent, and 0 if they are not. The log likelihood of a document is the sum of the log likelihoods of all pairs of mentions: L(x|m; θ) = X mi ,mj log P (xhi,ji |mi , mj ; θ) ∈m2 (2) where m is the set of mentions in the document, and x is the set of variables representing each pairwise coreference decision xhi,ji . Note that this model is degenerate, because it assigns probability mass to nonsensical clusterings. Specifically, it will allow xhi,ji = xhj,ki = 1 while xhi,ki = 0. Prior work (Soon et al., 2001; Denis and Baldridge, 2007) has generated training data for pairwise classifiers in the following manner. For each mention, work backwards through the preceding mentions in the document until you come to a true coreferent mention. Create negative examples for all intermediate mentions, and a positive example for the mention and its correct antecedent. This 46 approach made sense for Soon et al. (2001) because testing proceeded in a similar manner: for each mention, work backwards until you find a previous mention which the classifier thinks is coreferent, add a link, and terminate the search."
P08-2012,N03-1033,1,0.0520554,"ve3 to solve our ILP optimization problems. We ran experiments on two datasets. We used the MUC-6 formal training and test data, as well as the NWIRE and BNEWS portions of the ACE (Phase 2) corpus. This corpus had a third portion, NPAPER , but we found that several documents where too long for lp solve to find a solution.4 We added named entity (NE) tags to the data using the tagger of Finkel et al. (2005). The ACE data is already annotated with NE tags, so when they conflicted they overrode the tags output by the tagger. We also added part of speech (POS) tags to the data using the tagger of Toutanova et al. (2003), and used the tags to decide if mentions were plural or singular. The ACE data is labeled with mention type (pronominal, nominal, and name), but the MUC6 data is not, so the POS and NE tags were used to infer this information. Our feature set was simple, and included many features from (Soon et al., 2001), including the pronoun, string match, definite and demonstrative NP, number and gender agreement, proper name and appositive features. We had additional features for NE tags, head matching and head substring matching. 3.1 Evaluation Metrics The MUC scorer (Vilain et al., 1995) is a popular c"
P08-2012,M95-1005,0,0.940869,"Missing"
P08-2012,N07-1011,0,\N,Missing
P09-1034,P06-2003,0,0.0627553,"Missing"
P09-1034,W05-0909,0,0.0605981,"e system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references a"
P09-1034,2005.mtsummit-papers.11,0,0.00383163,"Missing"
P09-1034,C04-1072,0,0.0380796,"y 5–10 points.1 4.3 Baseline Metrics We consider four baselines. They are small regression models as described in Section 2 over component scores of four widely used MT metrics. To alleviate possible nonlinearity, we add all features in linear and log space. Each baselines carries the name of the underlying metric plus the suffix -R.2 B LEU R includes the following 18 sentence-level scores: BLEU-n and n-gram precision scores (1 ≤ n ≤ 4); BLEU brevity penalty (BP); BLEU score divided by BP. To counteract BLEU’s brittleness at the sentence level, we also smooth BLEU-n and n-gram precision as in Lin and Och (2004). N IST R consists of 16 features. NIST-n scores (1 ≤ n ≤ 10) and information-weighted n-gram precision scores (1 ≤ n ≤ 4); NIST brevity penalty (BP); and NIST score divided by BP. 1 Due to space constraints, we only show results for “tieaware” predictions. See Pad´o et al. (2009) for a discussion. 2 The regression models can simulate the behaviour of each component by setting the weights appropriately, but are strictly more powerful. A possible danger is that the parameters overfit on the training set. We therefore verified that the three non-trivial “baseline” regression models indeed confer"
P09-1034,W05-0904,0,0.324729,"translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references and MT hypotheses. We operationalize meaning equivalence by bidirectional textual entailment (RTE, D"
P09-1034,E06-1032,0,0.104031,"tnesses are terming it terrorism. Introduction Constant evaluation is vital to the progress of machine translation (MT). Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). BLEU and NIST measure MT quality by using the strong correlation between human judgments and the degree of n-gram overlap between a system hypothesis translation and one or more reference translations. The resulting scores are cheap and objective. However, studies such as Callison-Burch et al. (2006) have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be “gamed” by permuting word order; (3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The conte"
P09-1034,C08-1066,1,0.800593,"ile the original RTE task is asymmetric, MT evaluation needs to determine meaning equivalence, which is a symmetric relation. We do this by checking for entailment in both directions (see Figure 1). Operationally, this ensures we detect translations which either delete or insert material. Clearly, there are also differences between the two tasks. An important one is that RTE assumes the well-formedness of the two sentences. This is not generally true in MT, and could lead to degraded linguistic analyses. However, entailment relations are more sensitive to the contribution of individual words (MacCartney and Manning, 2008). In Example 2, the modal modifiers break the entailment between two otherwise identical sentences: (2) HYP: Peter is certainly from Lincolnshire. REF: Peter is possibly from Lincolnshire. This means that the prediction of TE hinges on correct semantic analysis and is sensitive to misanalyses. In contrast, human MT judgments behave robustly. Translations that involve individual errors, like (2), are judged lower than perfect ones, but usually not crucially so, since most aspects are still rendered correctly. We thus expect even noisy RTE features to be predictive for translation quality. This"
P09-1034,W08-0309,0,0.0715095,"Gibbs sampling (see de Marneffe et al. (2007)). Entailment features. In the third stage, the system produces roughly 100 features for each aligned premise-hypothesis pair. A small number of them are real-valued (mostly quality scores), but most are binary implementations of small linguistic theories whose activation indicates syntactic and se4 4.1 Experimental Evaluation Experiments Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five- or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al., 2008). An alternative that has been adopted by the yearly WMT evaluation shared tasks since 2008 is the collection of pairwise preference judgments between pairs of MT hypotheses which can be elicited (somewhat) more reliably. We demonstrate that our approach works well for both types of annotation and different corpora. Experiment 1 models absolute scores on Asian newswire, and Experiment 2 pairwise preferences on European speech and news data. 4.2 Evaluation We evaluate the output of our models both on the sentence and on the system level. At the sentence level, we can correlate predictions in Ex"
P09-1034,N06-1006,1,0.583147,"Missing"
P09-1034,P08-1007,0,0.286549,"ed towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quality with a model that attempts to comprehensively assess meaning equivalence between references and MT hypotheses. We operati"
P09-1034,I08-1042,0,0.0473301,"Missing"
P09-1034,P06-1114,0,0.0466837,". 3 Textual Entailment vs. MT Evaluation Our novel approach to MT evaluation exploits the similarity between MT evaluation and textual entailment (TE). TE was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning patterns than classical, strict logical entailment. Textual entailment is defined informally as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if “a human reading P would infer that H is most likely true”. Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006). The relation between textual entailment and MT evaluation is shown in Figure 1. Perfect MT output and the reference translation entail each other (top). Translation problems that impact semantic equivalence, e.g., deletion or addition of material, can break entailment in one or both directions (bottom). On the modelling level, there is common ground between RTE and MT evaluation: Both have to distinguish between valid and invalid variation to determine whether two texts convey the same information or not. For example, to recognize the bidirectional entailment in Ex. (1), RTE must account for"
P09-1034,hovy-etal-2006-automated,0,0.0208131,"a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar ideas have been applied by Owczarzak et al. (2008) to LFG parses, and by Liu and Gildea (2005) to features derived from phrase-structure tress. This approach has also been successful for the related task of summarization evaluation (Hovy et al., 2006). The most comparable work to ours is Gim´enez and M´arquez (2008). Our results agree on the crucial point that the use of a wide range of linguistic knowledge in MT evaluation is desirable and important. However, Gim´enez and M´arquez advocate the use of a bottom-up development process that builds on a set of “heterogeneous”, independent metrics each of which measures overlap with respect to one linguistic level. In contrast, our aim is to provide a “top-down”, integrated motivation for the features we integrate through the textual entailment recognition paradigm. 8 Conclusion and Outlook In"
P09-1034,N06-1058,0,0.0523827,"ependents of predicates is unreliable on the WMT data. Other differences do reflect more fundamental differences between the two tasks (cf. Section 3). For example, RTE puts high weights onto quantifier and polarity features, both of which have the potential of influencing entailment decisions, but are (at least currently) unimportant for MT evaluation. 7 Related Work Researchers have exploited various resources to enable the matching between words or n-grams that are semantically close but not identical. Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al. (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. These approaches reduce the risk that a good translation is rated poorly due to lexical deviation, but do not address the problem that a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar ideas have been applied by Owcza"
P09-1034,P03-1021,0,0.00335176,"Missing"
P09-1034,W09-0404,1,0.763215,"Missing"
P09-1034,P02-1040,0,0.104893,"fferent settings. The combination metric outperforms the individual scores, but is bested by the entailment-based metric. Combining the entailment and traditional features yields further improvements. 1 (1) HYP: However, this was declared terrorism by observers and witnesses. REF: Nevertheless, commentators as well as eyewitnesses are terming it terrorism. Introduction Constant evaluation is vital to the progress of machine translation (MT). Since human evaluation is costly and difficult to do reliably, a major focus of research has been on automatic measures of MT quality, pioneered by BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). BLEU and NIST measure MT quality by using the strong correlation between human judgments and the degree of n-gram overlap between a system hypothesis translation and one or more reference translations. The resulting scores are cheap and objective. However, studies such as Callison-Burch et al. (2006) have identified a number of problems with BLEU and related n-gram-based scores: (1) BLEUlike metrics are unreliable at the level of individual sentences due to data sparsity; (2) BLEU metrics can be “gamed” by permuting word order; (3) for some corpora and languages,"
P09-1034,2007.tmi-papers.19,0,0.082174,"ment REF: Three aid workers were kidnapped by pirates. Figure 1: Entailment status between an MT system hypothesis and a reference translation for equivalent (top) and non-equivalent (bottom) translations. 2 Regression-based MT Quality Prediction Current MT metrics tend to focus on a single dimension of linguistic information. Since the importance of these dimensions tends not to be stable across language pairs, genres, and systems, performance of these metrics varies substantially. A simple strategy to overcome this problem could be to combine the judgments of different metrics. For example, Paul et al. (2007) train binary classifiers on a feature set formed by a number of MT metrics. We follow a similar idea, but use a regularized linear regression to directly predict human ratings. Feature combination via regression is a supervised approach that requires labeled data. As we show in Section 5, this data is available, and the resulting model generalizes well from relatively small amounts of training data. 3 Textual Entailment vs. MT Evaluation Our novel approach to MT evaluation exploits the similarity between MT evaluation and textual entailment (TE). TE was introduced by Dagan et al. (2005) as a"
P09-1034,2006.amta-papers.25,0,0.0532152,"(3) for some corpora and languages, the correlation to human ratings is very low even at the system level; (4) scores are biased towards statistical MT; (5) the quality gap between MT and human translations is not reflected in equally large BLEU differences. ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred. A number of metrics have been designed to account for paraphrase, either by making the matching more intelligent (TER, Snover et al. (2006)), or by using linguistic evidence, mostly lexical similarity (METEOR, Banerjee and Lavie (2005); MaxSim, Chan and Ng (2008)), or syntactic overlap (Owczarzak et al. (2008); Liu and Gildea (2005)). Unfortunately, each metrics tend to concentrate on one particular type of linguistic information, none of which always correlates well with human judgments. Our paper proposes two strategies. We first explore the combination of traditional scores into a more robust ensemble metric with linear regression. Our second, more fundamental, strategy replaces the use of loose surrogates of translation quali"
P09-1034,W09-0441,0,0.0147185,"tness and improved correlations for the regression models. An exception is BLEU-1 and NIST-4 on Expt. 1 (Ar, Ch), which perform 0.5–1 point better at the sentence level. T ER R includes 50 features. We start with the standard TER score and the number of each of the four edit operations. Since the default uniform cost does not always correlate well with human judgment, we duplicate these features for 9 non-uniform edit costs. We find it effective to set insertion cost close to 0, as a way of enabling surface variation, and indeed the new TERp metric uses a similarly low default insertion cost (Snover et al., 2009). M ETEOR R 4.4 consists of METEOR v0.7. Combination Metrics The following three regression models implement the methods discussed in Sections 2 and 3. M T R combines the 85 features of the four baseline models. It uses no entailment features. RTE R uses the 70 entailment features described in Section 3.1, but no M T R features. M T +RTE R uses all M T R and RTE R features, combining matching and entailment evidence.3 5 Expt. 1: Predicting Absolute Scores Data. Our first experiment evaluates the models we have proposed on a corpus with traditional annotation on a seven-point scale, namely the"
P09-1034,W06-1610,0,0.214402,"or mismatches between dependents of predicates is unreliable on the WMT data. Other differences do reflect more fundamental differences between the two tasks (cf. Section 3). For example, RTE puts high weights onto quantifier and polarity features, both of which have the potential of influencing entailment decisions, but are (at least currently) unimportant for MT evaluation. 7 Related Work Researchers have exploited various resources to enable the matching between words or n-grams that are semantically close but not identical. Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al. (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. These approaches reduce the risk that a good translation is rated poorly due to lexical deviation, but do not address the problem that a translation may contain many long matches while lacking coherence and grammaticality (cf. the bottom example in Table 2). Thus, incorporation of syntactic knowledge has been the focus of another line of research. Amig´o et al. (2006) use the degree of overlap between the dependency trees of reference and hypothesis as a predictor of translation quality. Similar"
P09-1034,W07-1401,0,\N,Missing
P09-1087,W03-3017,0,0.182869,"Missing"
P09-1087,J04-4002,0,0.010408,"Missing"
P09-1087,N04-1021,0,0.0913612,"Missing"
P09-1087,P03-1021,0,0.00500391,"Missing"
P09-1087,2001.mtsummit-papers.68,0,0.103583,"Missing"
P09-1087,D08-1012,0,0.0322584,"Missing"
P09-1087,P05-1034,0,0.184492,"Missing"
P09-1087,W97-0301,0,0.0828007,"Missing"
P09-1087,W05-0908,0,0.11709,"Missing"
P09-1087,P08-1066,0,0.0730585,"ntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of McDonald et al. (2005b) for machine translation, we incrementally build dependency structure left-toright in time O(n2 ) during decoding. Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases. This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008), which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline (Koehn et al., 2007) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores. These results are found to be statistically very significant (p ≤ .01). 2 &lt;root> who do you think they hired &lt;root> WP VB PRP VB PRP VBD . 0 1 2 3 4 5 6 7 ? Figure 1: A dependency tree with directed edges going from heads to modifiers. The edge between who and h"
P09-1087,2006.amta-papers.25,0,0.0438387,"Missing"
P09-1087,N03-1033,1,0.092602,"Missing"
P09-1087,P07-1031,0,0.0117656,"Missing"
P09-1087,D07-1077,0,0.0409968,"Missing"
P09-1087,P96-1021,0,0.117053,"Missing"
P09-1087,P08-1025,0,0.0382139,"Missing"
P09-1087,D07-1090,0,0.0117605,"Missing"
P09-1087,W08-0336,1,0.906959,"Missing"
P09-1087,P05-1033,0,0.211608,"Missing"
P09-1087,P07-1033,0,0.0263438,"Missing"
P09-1087,P05-1067,0,0.072379,"Missing"
P09-1087,P99-1059,0,0.101932,"systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m ) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1 The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1) ), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m−1) ), i.e., O(n3m ). In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed. Of course, this comparison holds only for approximate algorithms. Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP). 2 Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English"
P09-1087,C96-1058,0,0.118578,"Missing"
P09-1087,W02-1039,0,0.107882,"Missing"
P09-1087,P07-1019,0,0.0420279,"Missing"
P09-1087,W05-1507,0,0.020767,"arcu et al., 2006; Shen et al., 2008), and often outperform phrase-based systems (Och and Ney, 2004; Koehn et al., 2003) on target-language fluency and adequacy. However, their benefits generally come with high computational costs, particularly when chart parsing, such as CKY, is integrated with language models of high orders (Wu, 1996). Indeed, synchronous CFG parsing with m-grams runs in O(n3m ) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1 The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1) ), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m−1) ), i.e., O(n3m ). In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed. Of course, this comparison holds only for approximate algorithms. Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP). 2 Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_"
P09-1087,J99-4005,0,0.0703363,"parsing with m-grams runs in O(n3m ) time, where n is the length of the sentence.1 Furthermore, synchronous CFG approaches often only marginally outperform the most com1 The algorithmic complexity of (Wu, 1996) is O(n3+4(m−1) ), though Huang et al. (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O(n3+3(m−1) ), i.e., O(n3m ). In comparison, phrase-based decoding can run in linear time if a distortion limit is imposed. Of course, this comparison holds only for approximate algorithms. Since exact MT decoding is NP complete (Knight, 1999), there is no exact search algorithm for either phrase-based or syntactic MT that runs in polynomial time (unless P = NP). 2 Results of the 2008 NIST Open MT evaluation (http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English and Arabic-English tasks incorporate synchronous CFG models, score differences with the best phrase-based system were insignificantly small. 773 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 773–781, c Suntec, Singapore, 2-7 August 2009."
P09-1087,N03-1017,0,0.00937669,"Missing"
P09-1087,P07-2045,0,0.00641963,"without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of McDonald et al. (2005b) for machine translation, we incrementally build dependency structure left-toright in time O(n2 ) during decoding. Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases. This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008), which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline (Koehn et al., 2007) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides improvements on five different test sets, with an overall gain of 0.92 in TER and 0.45 in BLEU scores. These results are found to be statistically very significant (p ≤ .01). 2 &lt;root> who do you think they hired &lt;root> WP VB PRP VB PRP VBD . 0 1 2 3 4 5 6 7 ? Figure 1: A dependency tree with directed edges going from heads to modifiers. The edge between who and hired causes this tree to be non-projective. Such a head-modifier relationship is difficult to represent with a CFG, sin"
P09-1087,W06-1606,0,0.0614139,"Missing"
P09-1087,P05-1012,0,0.782472,"/tests/mt/2008/doc/ mt08_official_results_v0.html) reveal that, while many of the best systems in the Chinese-English and Arabic-English tasks incorporate synchronous CFG models, score differences with the best phrase-based system were insignificantly small. 773 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 773–781, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP dency structure is built as a by-product of phrasebased decoding, without reliance on a dynamicprogramming or chart parsing algorithm such as CKY or Earley. Adapting the approach of McDonald et al. (2005b) for machine translation, we incrementally build dependency structure left-toright in time O(n2 ) during decoding. Most interestingly, the time complexity of non-projective dependency parsing remains quadratic as the order of the language model increases. This provides a compelling advantage over previous dependency language models for MT (Shen et al., 2008), which use a 5-gram LM only during reranking. In our experiments, we build a competitive baseline (Koehn et al., 2007) incorporating a 5-gram LM trained on a large part of Gigaword and show that our dependency language model provides imp"
P09-1087,P02-1040,0,\N,Missing
P09-1087,H05-1066,0,\N,Missing
P09-1087,W07-0702,0,\N,Missing
P09-1087,D08-1076,0,\N,Missing
P10-1018,de-marneffe-etal-2006-generating,1,0.0316939,"Missing"
P10-1018,W09-3920,1,0.905684,"Missing"
P10-1018,W10-0719,1,0.812257,"Missing"
P10-1018,P94-1009,0,0.125376,"no’ but conveys this only partially to the hearer. c. The speaker is uncertain of ‘yes’ or ‘no’ and conveys this uncertainty to the hearer. d. The speaker is uncertain of ‘yes’ or ‘no’, but the hearer infers one of those with confidence. Related work Indirect speech acts are studied by Clark (1979), Perrault and Allen (1980), Allen and Perrault (1980) and Asher and Lascarides (2003), who identify a wide range of factors that govern how speakers convey their intended messages and how hearers seek to uncover those messages from uncertain and conflicting signals. In the computational literature, Green and Carberry (1994, 1999) provide an extensive model that interprets and generates indirect answers to polar questions. They propose a logical inference model which makes use of discourse plans and coherence relations to infer categorical answers. However, to adequately interpret indirect answers, the uncertainty inherent in some answers needs to be captured (de Marneffe et al., 2009). While a straightforward ‘yes’ or ‘no’ response is clear in some indirect answers, such as in (1), the intended answer is less certain in other cases (2):1 (1) A: Do you think that’s a good idea, that we just begin to ignore these"
P10-1018,J99-3004,0,0.191727,"Missing"
P10-1018,J80-3003,0,0.513184,"to learn meanings that can drive pragmatic inference in dialogue. This paper demonstrates to some extent that meaning can be grounded from text in this way. 2 a. The speaker is certain of ‘yes’ or ‘no’ and conveys that directly and successfully to the hearer. b. The speaker is certain of ‘yes’ or ‘no’ but conveys this only partially to the hearer. c. The speaker is uncertain of ‘yes’ or ‘no’ and conveys this uncertainty to the hearer. d. The speaker is uncertain of ‘yes’ or ‘no’, but the hearer infers one of those with confidence. Related work Indirect speech acts are studied by Clark (1979), Perrault and Allen (1980), Allen and Perrault (1980) and Asher and Lascarides (2003), who identify a wide range of factors that govern how speakers convey their intended messages and how hearers seek to uncover those messages from uncertain and conflicting signals. In the computational literature, Green and Carberry (1994, 1999) provide an extensive model that interprets and generates indirect answers to polar questions. They propose a logical inference model which makes use of discourse plans and coherence relations to infer categorical answers. However, to adequately interpret indirect answers, the uncertainty inher"
P10-1018,D08-1027,0,0.0518347,"Missing"
P10-1018,P03-1054,1,0.00969769,"Missing"
P10-1018,D08-1103,0,\N,Missing
P10-1074,P06-1084,0,0.0135816,"was on joint parsing and semantic role labeling, but the best systems (Johansson and Nugues, 2008) were the ones which completely decoupled the tasks. While negative results are rarely published, this was not the first failed attempt at joint parsing and semantic role labeling (Sutton and McCallum, 2005). There have been some recent successes with joint modeling. Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. No discussion of joint modeling would be complete without mention of (Miller et al., 2000), who trained a Collinsstyle generative parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task. One significant limitation for many joint models is the lack of jointly annotated data."
P10-1074,P05-1001,0,0.0633343,"Missing"
P10-1074,W06-1655,0,0.0130916,"s 4 Base Models i=1 Our hierarchical joint model is composed of three separate models, one for just named entity recognition, one for just parsing, and one for joint parsing and named entity recognition. In this section we will review each of these models individually. 4.1 The partition function Zs serves as a normalizer. It requires summing over the set ys of all possible segmentations and labelings for the sentence s: Zs = Semi-CRF for Named Entity Recognition |y| XX exp{θ · f (s, yi , yi−1 )} (8) y∈ys i=1 2 For our named entity recognition model we use a semi-CRF (Sarawagi and Cohen, 2004; Andrew, 2006). Semi-CRFs are very similar to the more popular linear-chain CRFs, but with several key advantages. Semi-CRFs segment and label the text simultaneously, whereas a linear-chain CRF will only label each word, and segmentation is implied by the labels assigned to the words. When Both models will have one node per word for non-entity words. 3 While converting a semi-CRF into a parser results in much slower inference than a linear-chain CRF, it is still significantly faster than a treebank parser due to the reduced number of labels. 4 There can also be features over single entities, but these can"
P10-1074,P97-1003,0,0.112613,"erceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. No discussion of joint modeling would be complete without mention of (Miller et al., 2000), who trained a Collinsstyle generative parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task. One significant limitation for many joint models is the lack of jointly annotated data. We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. However, the performance of our model, trained using the OntoNotes corpus (Hovy et al., 2006), fell short of separate parsing and named One"
P10-1074,P07-1033,0,0.0416387,"Missing"
P10-1074,N09-1068,1,0.829269,"cal segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. No discussion of joint modeling would be complete without mention of (Miller et al., 2000), who trained a Collinsstyle generative parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task. One significant limitation for many joint models is the lack of jointly annotated data. We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. However, the performance of our model, trained using the OntoNotes corpus (Hovy et al., 2006), fell short of separate parsing and named One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abunda"
P10-1074,N09-1037,1,0.68906,"cal segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. No discussion of joint modeling would be complete without mention of (Miller et al., 2000), who trained a Collinsstyle generative parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task. One significant limitation for many joint models is the lack of jointly annotated data. We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. However, the performance of our model, trained using the OntoNotes corpus (Hovy et al., 2006), fell short of separate parsing and named One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abunda"
P10-1074,D09-1015,1,0.834581,"ally improve performance has proven challenging. The CoNLL 2008 shared task (Surdeanu et al., 2008) was on joint parsing and semantic role labeling, but the best systems (Johansson and Nugues, 2008) were the ones which completely decoupled the tasks. While negative results are rarely published, this was not the first failed attempt at joint parsing and semantic role labeling (Sutton and McCallum, 2005). There have been some recent successes with joint modeling. Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. No discussion of joint modeling would be complete without mention of (Miller et al., 2000), who trained a Collinsstyle generative parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotati"
P10-1074,P08-1109,1,0.767984,"nt model parameters θ. The partial derivatives of θ are then given by !  X X ∂L fi (r, s) − Eθ [fi |s] = ∂θi r∈t Because we use a tree representation, it is easy to ensure that the features used in the NER model are identical to those in the joint parsing and named entity model, because the joint model (which we will discuss in Section 4.3) is also based on a tree representation where each entity corresponds to a single node in the tree. 4.2 (t,s)∈D CRF-CFG for Parsing Our parsing model is the discriminatively trained, conditional random field-based context-free grammar parser (CRF-CFG) of (Finkel et al., 2008). The relationship between a CRF-CFG and a PCFG is analogous to the relationship between a linearchain CRF and a hidden Markov model (HMM) for modeling sequence data. Let t be a complete parse tree for sentence s, and each local subtree r ∈ t encodes both the rule from the grammar, and the span and split information (e.g NP(7,9) → JJ(7,8) NN(8,9) which covers the last two words in Figure 1). The feature function f (r, s) computes the features, which are defined over a local subtree r and the words of the sentence. Let θ be the vector of feature weights. The log-likelihood of tree t over senten"
P10-1074,P08-1043,0,0.0299073,"oupled the tasks. While negative results are rarely published, this was not the first failed attempt at joint parsing and semantic role labeling (Sutton and McCallum, 2005). There have been some recent successes with joint modeling. Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. No discussion of joint modeling would be complete without mention of (Miller et al., 2000), who trained a Collinsstyle generative parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task. One significant limitation for many joint models is the lack of jointly annotated data. We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance a"
P10-1074,N06-2015,0,0.246576,"00), who trained a Collinsstyle generative parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task. One significant limitation for many joint models is the lack of jointly annotated data. We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. However, the performance of our model, trained using the OntoNotes corpus (Hovy et al., 2006), fell short of separate parsing and named One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data. In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model. Our model utilizes a hierarchical prior to link the feat"
P10-1074,W08-2123,0,0.0174519,"on with Non-Jointly Labeled Data Jenny Rose Finkel and Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305 {jrfinkel|manning}@cs.stanford.edu Abstract jointly should improve performance. Because a named entity should correspond to a node in the parse tree, strong evidence about either aspect of the model should positively impact the other aspect. However, designing joint models which actually improve performance has proven challenging. The CoNLL 2008 shared task (Surdeanu et al., 2008) was on joint parsing and semantic role labeling, but the best systems (Johansson and Nugues, 2008) were the ones which completely decoupled the tasks. While negative results are rarely published, this was not the first failed attempt at joint parsing and semantic role labeling (Sutton and McCallum, 2005). There have been some recent successes with joint modeling. Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segme"
P10-1074,A00-2030,0,0.010731,"cent successes with joint modeling. Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. No discussion of joint modeling would be complete without mention of (Miller et al., 2000), who trained a Collinsstyle generative parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotations for the MUC-7 shared task. One significant limitation for many joint models is the lack of jointly annotated data. We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. However, the performance of our model, trained using the OntoNotes corpus (Hov"
P10-1074,W08-2121,0,0.0753312,"Missing"
P10-1074,W05-0636,0,0.00934801,"ove performance. Because a named entity should correspond to a node in the parse tree, strong evidence about either aspect of the model should positively impact the other aspect. However, designing joint models which actually improve performance has proven challenging. The CoNLL 2008 shared task (Surdeanu et al., 2008) was on joint parsing and semantic role labeling, but the best systems (Johansson and Nugues, 2008) were the ones which completely decoupled the tasks. While negative results are rarely published, this was not the first failed attempt at joint parsing and semantic role labeling (Sutton and McCallum, 2005). There have been some recent successes with joint modeling. Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. No discussion of joint modeling would be comp"
P10-1074,P09-1055,0,0.0126038,"int models which actually improve performance has proven challenging. The CoNLL 2008 shared task (Surdeanu et al., 2008) was on joint parsing and semantic role labeling, but the best systems (Johansson and Nugues, 2008) were the ones which completely decoupled the tasks. While negative results are rarely published, this was not the first failed attempt at joint parsing and semantic role labeling (Sutton and McCallum, 2005). There have been some recent successes with joint modeling. Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. No discussion of joint modeling would be complete without mention of (Miller et al., 2000), who trained a Collinsstyle generative parser (Collins, 1997) over a syntactic structure augmented with the template entity and template relations annotati"
P10-1074,P08-1101,0,0.0264038,"strong evidence about either aspect of the model should positively impact the other aspect. However, designing joint models which actually improve performance has proven challenging. The CoNLL 2008 shared task (Surdeanu et al., 2008) was on joint parsing and semantic role labeling, but the best systems (Johansson and Nugues, 2008) were the ones which completely decoupled the tasks. While negative results are rarely published, this was not the first failed attempt at joint parsing and semantic role labeling (Sutton and McCallum, 2005). There have been some recent successes with joint modeling. Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Adler and Elhadad (2006) presented an HMMbased approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. No discussion of joint modeling would be complete without mention of (Miller et al., 2000), who trained a Collinsstyle generativ"
P11-1163,W09-1402,0,0.0948592,"Missing"
P11-1163,P09-1068,0,0.0290898,"either protein entity mentions (PROT) or other events as arguments. The latter is what allows for nested event structures. Existing dependency parsing models can be adapted to produce these semantic structures instead of syntactic dependencies. We built a global reranking parser model using multiple decoders from MSTParser (McDonald et al., 2005; McDonald et al., 2005b). The main contributions of this paper are the following: Event structures in open domain texts are frequently highly complex and nested: a “crime” event can cause an “investigation” event, which can lead to an “arrest” event (Chambers and Jurafsky, 2009). The same observation holds in specific domains. For example, the BioNLP’09 shared task (Kim et al., 2009) focuses on the extraction of nested biomolecular 1. We demonstrate that parsing is an attractive apevents, where, e.g., a REGULATION event causes a proach for extracting events, both nested and TRANSCRIPTION event (see Figure 1a for a detailed otherwise. 1 example). Despite this observation, many stateWhile our approach only works on trees, we show how we of-the-art supervised event extraction models still can handle directed acyclic graphs in Section 5. 1626 Proceedings of the 49th Annu"
P11-1163,P05-1022,0,0.556932,"se in the BioNLP’09 domain used for evaluation entities (PROTEINs) are given (but including entity recognition is an obvious extension of our model). Our parsers are several instances of MSTParser2 (McDonald et al., 2005; McDonald et al., 2005b) configured with different decoders. However, our approach is agnostic to the actual parsing models used and could easily be adapted to other dependency parsers. The output from the reranking parser is 2 http://sourceforge.net/projects/mstparser/ 1628 converted back to the original event representation and passed to a reranker component (Collins, 2000; Charniak and Johnson, 2005), tailored to optimize the task-specific evaluation metric. Note that although we use the biomedical event domain from the BioNLP’09 shared task to illustrate our work, the core of our approach is almost domain independent. Our only constraints are that each event mention be activated by a phrase that serves as an event anchor, and that the event-argument structures be mapped to a dependency tree. The conversion between event and dependency structures and the reranker metric are the only domain dependent components in our approach. 3.1 Converting between Event Structures and Dependencies As in"
P11-1163,N09-1037,1,0.67237,"d show that our approach obtains competitive results. 2 Related Work The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified annotation of syntactic and semantic elements, which is not always feasible, and focused only on named entities and binary relations. On the other hand, our approach focuses on event structures that are nested and have an arbitrary number of arguments. We do not need 1627 a unified syntactic and semantic representation (but we"
P11-1163,D09-1015,1,0.789522,"d show that our approach obtains competitive results. 2 Related Work The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified annotation of syntactic and semantic elements, which is not always feasible, and focused only on named entities and binary relations. On the other hand, our approach focuses on event structures that are nested and have an arbitrary number of arguments. We do not need 1627 a unified syntactic and semantic representation (but we"
P11-1163,N06-2015,0,0.00709008,"the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified annotation of syntactic and semantic elements, which is not always feasible, and focused only on named entities and binary relations. On the other hand, our approach focuses on event structures that are nested and have an arbitrary number of arguments. We do not need 1627 a unified syntactic and semantic representation (but we can and do extract features from the underlying syntactic structure of the text). Finkel and Manning (2009b) a"
P11-1163,P08-1067,0,0.014766,"ctures When decoding, the parser finds the highest scoring tree which incorporates global properties of the sentence. However, its features are edge-factored and thus unable to take into account larger contexts. To incorporate arbitrary global features, we employ a two-step reranking parser. For the first step, we extend our parser to output its n-best parses instead of just its top scoring parse. In the second step, a discriminative reranker rescores each parse and reorders the n-best list. Rerankers have been successfully used in syntactic parsing (Collins, 2000; Charniak and Johnson, 2005; Huang, 2008) and semantic role labeling (Toutanova et al., 2008). Rerankers provide additional advantages in our case due to the mismatch between the dependency structures that the parser operates on and their corresponding event structures. We convert the output from the parser to event structures (Section 3.1) before including them in the reranker. This allows the reranker to capture features over the actual event structures rather than their original dependency trees which may contain extraneous portions.8 Furthermore, this lets the reranker optimize the actual BioNLP F1 score. The parser, on the other"
P11-1163,N10-1095,0,0.0140743,"r detection and reranking. Decoder names include the features order (1 or 2) followed by the projectivity (P = projective, N = non-projective). decoder; number of different decoders producing the parse (when using multiple decoders). 4 Experimental Results To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. Rerankers can also be used to perform model combination (Toutanova et al., 2008; Zhang et al., 2009; Johnson and Ural, 2010). While we use a single parsing model, it has multiple decoders.10 When combining multiple decoders, we concatenate their n-best lists and extract the unique parses. Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 260 abstracts, 2,447 sentences, and 3,182 events. Throughout our experiments, we report BioNLP F1 scores with approximate span and recursive event matching (as described in the shared task"
P11-1163,J93-2004,0,0.0404709,"ose a wide range of features for event extraction. Our analysis indicates that features which model the global event structure yield considerable performance improvements, which proves that modeling event structure jointly is beneficial. 3. We evaluate on the biomolecular event corpus from the the BioNLP’09 shared task and show that our approach obtains competitive results. 2 Related Work The pioneering work of Miller et al. (1997) was the first, to our knowledge, to propose parsing as a framework for information extraction. They extended the syntactic annotations of the Penn Treebank corpus (Marcus et al., 1993) with entity and relation mentions specific to the MUC-7 evaluation (Chinchor et al., 1997) — e.g., EMPLOYEE OF relations that hold between person and organization named entities — and then trained a generative parsing model over this combined syntactic and semantic representation. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al., 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. However, both these works require a unified a"
P11-1163,N10-1004,1,0.636022,"and extract the unique parses. Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 260 abstracts, 2,447 sentences, and 3,182 events. Throughout our experiments, we report BioNLP F1 scores with approximate span and recursive event matching (as described in the shared task definition). For preprocessing, we parsed all documents using the self-trained biomedical McClosky-CharniakJohnson reranking parser (McClosky, 2010). We bias the anchor detector to favor recall, allowing the parser and reranker to determine which event anchors will ultimately be used. When performing nbest parsing, n = 50. For parser feature pruning, α = 0.001. Table 1a shows the performance of each of the decoders when using gold event anchors. In both cases where n-best decoding is available, the reranker improves performance over the 1-best parsers. We also present the results from a reranker trained from multiple decoders which is our highest scoring model.11 In Table 1b, we present the output for the predicted anchor scenario. In the"
P11-1163,W11-1806,1,0.744235,"Missing"
P11-1163,P05-1012,0,0.160804,"main. Figure 1 shows a sentence and its converted form from the biomedical domain with four events: two POSITIVE REGULATION events, anchored by the phrase “acts as a costimulatory signal,” and two TRANSCRIPTION events, both anchored on “gene transcription.” All events take either protein entity mentions (PROT) or other events as arguments. The latter is what allows for nested event structures. Existing dependency parsing models can be adapted to produce these semantic structures instead of syntactic dependencies. We built a global reranking parser model using multiple decoders from MSTParser (McDonald et al., 2005; McDonald et al., 2005b). The main contributions of this paper are the following: Event structures in open domain texts are frequently highly complex and nested: a “crime” event can cause an “investigation” event, which can lead to an “arrest” event (Chambers and Jurafsky, 2009). The same observation holds in specific domains. For example, the BioNLP’09 shared task (Kim et al., 2009) focuses on the extraction of nested biomolecular 1. We demonstrate that parsing is an attractive apevents, where, e.g., a REGULATION event causes a proach for extracting events, both nested and TRANSCRIPTION even"
P11-1163,H05-1066,0,0.137371,"Missing"
P11-1163,E06-1011,0,0.0666491,"Missing"
P11-1163,W10-1905,0,0.0151987,"Missing"
P11-1163,N10-1123,0,0.0865664,"c and semantic representation (but we can and do extract features from the underlying syntactic structure of the text). Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. In this work, we focus on more complex structures (events instead of named entities) and we explore more global features through our reranking layer. In the biomedical domain, two recent papers proposed joint models for event extraction based on Markov logic networks (MLN) (Riedel et al., 2009; Poon and Vanderwende, 2010). Both works propose elegant frameworks where event anchors and arguments are jointly predicted for all events in the same sentence. One disadvantage of MLN models is the requirement that a human expert develop domainspecific predicates and formulas, which can be a cumbersome process because it requires thorough domain understanding. On the other hand, our approach maintains the joint modeling advantage, but our model is built over simple, domain-independent features. We also propose and analyze a richer feature space that captures more information on the global event structure in a sentence."
P11-1163,W09-1406,0,0.100515,"27 a unified syntactic and semantic representation (but we can and do extract features from the underlying syntactic structure of the text). Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. In this work, we focus on more complex structures (events instead of named entities) and we explore more global features through our reranking layer. In the biomedical domain, two recent papers proposed joint models for event extraction based on Markov logic networks (MLN) (Riedel et al., 2009; Poon and Vanderwende, 2010). Both works propose elegant frameworks where event anchors and arguments are jointly predicted for all events in the same sentence. One disadvantage of MLN models is the requirement that a human expert develop domainspecific predicates and formulas, which can be a cumbersome process because it requires thorough domain understanding. On the other hand, our approach maintains the joint modeling advantage, but our model is built over simple, domain-independent features. We also propose and analyze a richer feature space that captures more information on the global ev"
P11-1163,D10-1001,0,0.0332233,"Missing"
P11-1163,J08-2002,1,0.586574,"rt in text. This result is consistent with observations in previous work (Bj¨orne et al., 2009). • Token-level: The form, lemma, and whether the token is present in a gazetteer of known anchor words.4 • Surface context: The above token features extracted from a context of two words around the current token. Additionally, we build token bigrams in this context window, and model them with similar features. • Syntactic context: We model all syntactic dependency paths up to depth two starting from the token to be classified. These paths are built from Stanford syntactic dependencies (Marneffe and Manning, 2008). We extract token features from the first and last token in these paths. We also generate combination features by concatenating: (a) the last token in each path with the sequence of dependency labels along the corresponding path; and (b) the word to be classified, the last token in each path, and the sequence of dependency labels in that path. • Bag-of-word and entity count: Extracted from (a) the entire sentence, and (b) a window of five words around the token to be classified. 3.3 Parsing Event Structures Given the entities and event anchors from the previous stages in the pipeline, the par"
P11-1163,D09-1161,0,0.00955178,"mpact of event anchor detection and reranking. Decoder names include the features order (1 or 2) followed by the projectivity (P = projective, N = non-projective). decoder; number of different decoders producing the parse (when using multiple decoders). 4 Experimental Results To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. Rerankers can also be used to perform model combination (Toutanova et al., 2008; Zhang et al., 2009; Johnson and Ural, 2010). While we use a single parsing model, it has multiple decoders.10 When combining multiple decoders, we concatenate their n-best lists and extract the unique parses. Our experiments use the BioNLP’09 shared task corpus (Kim et al., 2009) which includes 800 biomedical abstracts (7,449 sentences, 8,597 events) for training and 150 abstracts (1,450 sentences, 1,809 events) for development. The test set includes 260 abstracts, 2,447 sentences, and 3,182 events. Throughout our experiments, we report BioNLP F1 scores with approximate span and recursive event matching (as des"
P11-1163,W08-2121,1,\N,Missing
P11-1163,W09-1401,0,\N,Missing
P11-1163,W08-1301,1,\N,Missing
P11-1163,C08-1095,0,\N,Missing
P11-1163,W09-1201,1,\N,Missing
P11-1163,W12-2400,0,\N,Missing
P11-1163,M98-1009,0,\N,Missing
P12-1092,D10-1113,0,0.0328734,"th human similarity ratings on pairs of words, such as WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991) and RG (Rubenstein and Goodenough, 1965), have been widely used to evaluate vector-space models. Motivated to evaluate composition models, Mitchell and Lapata (2008) introduced a dataset where an intransitive verb, presented with a subject noun, is com880 pared to another verb chosen to be either similar or dissimilar to the intransitive verb in context. The context is short, with only one word, and only verbs are compared. Erk and Pad´o (2008), Thater et al. (2011) and Dinu and Lapata (2010) evaluated word similarity in context with a modified task where systems are to rerank gold-standard paraphrase candidates given the SemEval 2007 Lexical Substitution Task dataset. This task only indirectly evaluates similarity as only reranking of already similar words are evaluated. 6 Conclusion We presented a new neural network architecture that learns more semantic word representations by using both local and global context in learning. These learned word embeddings can be used to represent word contexts as low-dimensional weighted average vectors, which are then clustered to form differen"
P12-1092,D08-1094,0,0.0132498,"Missing"
P12-1092,P08-1028,0,0.238889,"aches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). Two other recent papers (Dhillon et al., 2011; Reddy et al., 2011) present models for constructing word representations that deal with context. It would be interesting to evaluate those models on our new dataset. Many datasets with human similarity ratings on pairs of words, such as WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991) and RG (Rubenstein and Goodenough, 1965), have been widely used to evaluate vector-space models. Motivated to evaluate composition models, Mitchell and Lapata (2008) introduced a dataset where an intransitive verb, presented with a subject noun, is com880 pared to another verb chosen to be either similar or dissimilar to the intransitive verb in context. The context is short, with only one word, and only verbs are compared. Erk and Pad´o (2008), Thater et al. (2011) and Dinu and Lapata (2010) evaluated word similarity in context with a modified task where systems are to rerank gold-standard paraphrase candidates given the SemEval 2007 Lexical Substitution Task dataset. This task only indirectly evaluates similarity as only reranking of already similar wor"
P12-1092,I11-1079,0,0.0263083,"are competitive in word similarity tasks. Most of the previous vector-space models use a single vector to represent a word even though many words have multiple meanings. The multi-prototype approach has been widely studied in models of categorization in psychology (Rosseel, 2002; Griffiths et al., 2009), while Sch¨utze (1998) used clustering of contexts to perform word sense discrimination. Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). Two other recent papers (Dhillon et al., 2011; Reddy et al., 2011) present models for constructing word representations that deal with context. It would be interesting to evaluate those models on our new dataset. Many datasets with human similarity ratings on pairs of words, such as WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991) and RG (Rubenstein and Goodenough, 1965), have been widely used to evaluate vector-space models. Motivated to evaluate composition models, Mitchell and Lapata (2008) introduced a dataset where an intransitive verb, presented with a subject noun, is com880 pared to another verb chosen to be either similar or dis"
P12-1092,D10-1114,0,0.732839,"rious NLP tasks, one major limitation common to most of these models is that they assume only one representation for each word. This single-prototype representation is problematic because many words have multiple meanings, which can be wildly different. Using one representation simply cannot capture the different meanings. Moreover, using all contexts of a homonymous or polysemous word to build a single prototype could hurt the representation, which cannot represent any one of the meanings well as it is influenced by all meanings of the word. Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multiprototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. We show how our model can readily adopt the multi-prototype approach. We present a way to use our learned single-prototype embeddings to represent each context window, which can then be used by clustering to perform word sense discrimination (Sch¨utze, 1998). In order to learn multiple prototypes, we first gather the fixed-sized context windows of all occurrences of a word (we use 5 words before and after the word occurrence). Each context is re"
P12-1092,N10-1013,0,0.84753,"e WordSim-353 dataset (Finkelstein et al., 2001), which consists of 353 pairs of nouns. Each pair is presented without context and associated with 13 to 16 human judgments on similarity and relatedness on a scale from 0 to 10. For example, (cup,drink) received an average score of 7.25, while (cup,substance) received an average score of 1.92. Table 3 shows our results compared to previous methods, including C&W’s language model and the hierarchical log-bilinear (HLBL) model (Mnih and Hinton, 2008), which is a probabilistic, linear neural model. We downloaded these embeddings from Turian et al. (2010). These embeddings were trained on the smaller corpus RCV1 that contains one year of Reuters English newswire, and show similar correlations on the dataset. We report the result of 877 our re-implementation of C&W’s model trained on Wikipedia, showing the large effect of using a different corpus. Our model is able to learn more semantic word embeddings and noticeably improves upon C&W’s model. Note that our model achieves higher correlation (64.2) than either using local context alone (C&W: 55.3) or using global context alone (Our Model-g: 22.8). We also found that correlation can be further i"
P12-1092,J98-1004,0,0.0619562,"Missing"
P12-1092,D08-1027,1,0.0618006,"Missing"
P12-1092,D11-1014,1,0.460564,"Missing"
P12-1092,I11-1127,0,0.0697611,"Missing"
P12-1092,P10-1040,0,0.853332,"e models is the WordSim-353 dataset (Finkelstein et al., 2001), which consists of 353 pairs of nouns. Each pair is presented without context and associated with 13 to 16 human judgments on similarity and relatedness on a scale from 0 to 10. For example, (cup,drink) received an average score of 7.25, while (cup,substance) received an average score of 1.92. Table 3 shows our results compared to previous methods, including C&W’s language model and the hierarchical log-bilinear (HLBL) model (Mnih and Hinton, 2008), which is a probabilistic, linear neural model. We downloaded these embeddings from Turian et al. (2010). These embeddings were trained on the smaller corpus RCV1 that contains one year of Reuters English newswire, and show similar correlations on the dataset. We report the result of 877 our re-implementation of C&W’s model trained on Wikipedia, showing the large effect of using a different corpus. Our model is able to learn more semantic word embeddings and noticeably improves upon C&W’s model. Note that our model achieves higher correlation (64.2) than either using local context alone (C&W: 55.3) or using global context alone (Our Model-g: 22.8). We also found that correlation can be further i"
P12-1092,S10-1011,0,\N,Missing
P12-2018,P11-1015,0,0.751596,"Missing"
P12-2018,N10-1120,0,0.0364208,"6) (3316,7308) (5000,5000) (1000,1000) (25k,25k) (799,628) (980,973) (992,995) l 21 20 3 24 787 231 345 261 269 CV 10 10 10 10 10 N N N N |V | 21K 5713 6299 24K 51K 392K 22K 32K 25K ∆ 0.8 1.3 0.8 0.8 1.5 0.4 2.9 1.8 0.5 Table 1: Dataset statistics. (N+ , N− ): number of positive and negative examples. l: average number of words per example. CV: number of crossvalidation splits, or N for train/test split. |V |: the vocabulary size. ∆: upper-bounds of the differences required to be statistically significant at the p &lt; 0.05 level. CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010).2 MPQA: Opinion polarity subtask of the MPQA dataset (Wiebe et al., 2005).3 Subj: The subjectivity dataset with subjective reviews and objective plot summaries (Pang and Lee, 2004). RT-2k: The standard 2000 full-length movie review dataset (Pang and Lee, 2004). 2.3 SVM with NB features (NBSVM) Otherwise identical to the SVM, except we use x(k) = ˜ f (k) , where ˜ f (k) = ˆ r◦ˆ f (k) is the elementwise product. While this does very well for long documents, we find that an interpolation between MNB and SVM performs excellently for all documents and we report results using this model: w0 = (1 −"
P12-2018,P04-1035,0,0.735588,"0.8 1.3 0.8 0.8 1.5 0.4 2.9 1.8 0.5 Table 1: Dataset statistics. (N+ , N− ): number of positive and negative examples. l: average number of words per example. CV: number of crossvalidation splits, or N for train/test split. |V |: the vocabulary size. ∆: upper-bounds of the differences required to be statistically significant at the p &lt; 0.05 level. CR: Customer review dataset (Hu and Liu, 2004) processed like in (Nakagawa et al., 2010).2 MPQA: Opinion polarity subtask of the MPQA dataset (Wiebe et al., 2005).3 Subj: The subjectivity dataset with subjective reviews and objective plot summaries (Pang and Lee, 2004). RT-2k: The standard 2000 full-length movie review dataset (Pang and Lee, 2004). 2.3 SVM with NB features (NBSVM) Otherwise identical to the SVM, except we use x(k) = ˜ f (k) , where ˜ f (k) = ˆ r◦ˆ f (k) is the elementwise product. While this does very well for long documents, we find that an interpolation between MNB and SVM performs excellently for all documents and we report results using this model: w0 = (1 − β)w ¯ + βw Dataset RT-s CR MPQA Subj. RT-2k IMDB AthR XGraph BbCrypt (4) IMDB: A large movie review dataset with 50k fulllength reviews (Maas et al., 2011).4 AthR, XGraph, BbCrypt:"
P12-2018,P05-1015,0,0.904185,"confident. 4 3 We use the provided tokenizations when they exist. If not, we split at spaces for unigrams, and we filter out anything that is not [A-Za-z] for bigrams. We do Datasets and Task We compare with published results on the following datasets. Detailed statistics are shown in table 1. Experiments and Results 4.1 Experimental setup 2 http://www.cs.uic.edu/∼liub/FBS/sentiment-analysis.html http://www.cs.pitt.edu/mpqa/ 4 http://ai.stanford.edu/∼amaas/data/sentiment 5 http://people.csail.mit.edu/jrennie/20Newsgroups 3 RT-s: Short movie reviews dataset containing one sentence per review (Pang and Lee, 2005). 91 not use stopwords, lexicons or other resources. All results reported use α = 1, C = 1, β = 0.25 for NBSVM, and C = 0.1 for SVM. For comparison with other published results, we use either 10-fold cross-validation or train/test split depending on what is standard for the dataset. The CV column of table 1 specifies what is used. The standard splits are used when they are available. The approximate upper-bounds on the difference required to be statistically significant at the p &lt; 0.05 level are listed in table 1, column ∆. 4.2 MNB is better at snippets (Moilanen and Pulman, 2007) suggests tha"
P12-2018,D11-1014,1,0.774967,"4.2 MNB is better at snippets (Moilanen and Pulman, 2007) suggests that while “statistical methods” work well for datasets with hundreds of words in each example, they cannot handle snippets datasets and some rule-based system is necessary. Supporting this claim are examples such as not an inhumane monster6 , or killing cancer that express an overall positive sentiment with negative words. Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al., 2011). These works seem promising as they perform better than many sophisticated, rule-based methods used as baselines in (Nakagawa et al., 2010). However, we find that several NB/SVM variants in fact do better than these state-of-the-art methods, even compared to methods that use lexicons, reversal rules, or unsupervised pretraining. The results are in table 2. Our SVM-uni results are consistent with BoFnoDic and BoF-w/Rev used in (Nakagawa et al., 2010) and BoWSVM in (Pang and Lee, 2004). (Nakagawa et al., 2010) used a SVM with secondorder polynomial kernel and additional features. With the only"
P13-1031,2007.mtsummit-papers.3,0,0.172644,"Missing"
P13-1031,P12-1016,1,0.0338413,"d the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using"
P13-1031,W08-0304,1,0.824052,"crimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with σ=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses"
P13-1031,N10-2003,1,0.951564,"). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Adaptive Online Algorithms 2.2 Machine translation is an unusual machine learning setting because multiple correct translations exist and decoding is comparatively expensive. When we have a large feature set and therefore want to tune on a large data set, batch methods are infeasible. Online methods can converge faster, and in practice they often find better solutions"
P13-1031,W08-0336,1,0.0716331,"es baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with σ=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) impleme"
P13-1031,N12-1047,0,0.33688,"sed in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Lin"
P13-1031,D08-1024,0,0.396709,"in MT where good sparse features may fire very infrequently. We would instead like to take larger steps for sparse features and smaller steps for dense features. 2.1 1/2 wt = wt−1 − ηΣt ∇`t (wt−1 ) (2) −1 &gt; Σ−1 t = Σt−1 + ∇`t (wt−1 )∇`t (wt−1 ) = t X i=1 ∇`i (wi−1 )∇`i (wi−1 )&gt; (3) A diagonal approximation to Σ can be used for a high-dimensional vector wt . In this case, AdaGrad is simple to implement and computationally cheap. Consider a single dimension j, and P let scalars vt = wt,j , gt = ∇j `t (wt−1 ), Gt = ti=1 gi2 , then the update rule is −1/2 vt = vt−1 − η Gt Gt = Gt−1 + gt gt2 MIRA Chiang et al. (2008) described an adaption of MIRA (Crammer et al., 2006) to MT. MIRA makes the following update: wt = arg min w We specify the loss function for MT in section 3.1. 1 kw − wt−1 k22 + `t (w) 2η (6) The first term expresses conservativity: the weight should change as little as possible based on a single example, ensuring that it is never beneficial to overshoot the minimum. The relationship to SGD can be seen by linearizing the loss function `t (w) ≈ `t (wt−1 ) + (w − wt−1 )&gt; ∇`t (wt−1 ) and taking the derivative of (6). The result is exactly (1). AROW Chiang (2012) adapted AROW (Crammer et al., 200"
P13-1031,N09-1025,0,0.1009,"mes a mini-batch of examples. 3.2 Updating and Regularization Algorithm 1 lines 9–11 compute the adaptive learning rate, update the weights, and apply regularization. Section 2.1 explained the AdaGrad learning rate computation. To update and regularize the weights we apply the Forward-Backward Splitting (FOBOS) (Duchi and Singer, 2009) framework, which separates the two operations. The two-step FOBOS update is wt− 1 = wt−1 − ηt−1 ∇`t−1 (wt−1 ) 2 (13) search. Some of the features generalize, but many do not. This was well understood in previous work, so heuristic filtering was usually applied (Chiang et al., 2009, inter alia). In contrast, we need only select an appropriate regularization strength λ. Specifically, when r(w) = λkwk1 , the closedform solution to (14) is h i wt = sign(wt− 1 ) |wt− 1 |− ηt−1 λ (15) 2 where [x]+ = max(x, 0) is the clipping function that in this case sets a weight to 0 when it falls below the threshold ηt−1 λ. It is straightforward to adapt this to AdaGrad with diagonal Σ by setting 1 2 each dimension of ηt−1,j = ηΣt,jj and by taking element-wise products. We find that ∇`t−1 (wt−1 ) only involves several hundred active features for the current example (or mini-batch). Howev"
P13-1031,D08-1089,1,0.124908,"ier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Al"
P13-1031,W12-3154,0,0.0117732,"by exploiting sparse features. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and applies to other recent discriminative methods for machine translation. 1 To learn good weights for the sparse features, most algorithms—including ours—benefit from more tuning data, and the natural source is the training bitext. However, the bitext presents two problems. First, it has a single reference, sometimes of lower quality than the multiple references in tuning sets from MT competitions. Second, large bitexts often comprise many text genres (Haddow and Koehn, 2012), a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a significant domain adaptation problem when evaluating on standard test sets. Our analysis separates and quantifies these two issues. Introduction Sparse, overlapping features such as words and ngram contexts improve many NLP systems such as parsers and taggers. Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an acti"
P13-1031,W11-2130,0,0.13411,"Missing"
P13-1031,2012.iwslt-papers.17,0,0.195332,"ing features such as words and ngram contexts improve many NLP systems such as parsers and taggers. Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from t"
P13-1031,P12-1031,0,0.0207883,"g set, we improve significantly over all other models. PRO learns a smaller model with the PT+AL+LO feature set which is surprising given that it applies L2 regularization (AdaGrad uses L1 ). We speculate that this may be an consequence of stochastic learning. Our algorithm decodes each example with a new weight vector, thus exploring more of the search space for the same tuning set. 4.4 Bitext Tuning Experiment Tables 2 and 3 show that adding tuning examples improves translation quality. Nevertheless, even the larger tuning set is small relative to the bitext from which rules were extracted. He and Deng (2012) and Simianer et al. (2012) showed significant translation quality gains by tuning on the bitext. However, their bitexts matched the genre of their test sets. Our bitexts, like those of most large-scale systems, do not. Domain mismatch matters for the dense feature set (Haddow and Koehn, 2012). We show that it also matters for feature-rich MT. Before aligning each bitext, we randomly sampled and sequestered 5k and 15k sentence tuning sets, and a 5k test set. We prevented overlap beDA MT04 MT04 MT04 MT04 5ktest 5ktest DB MT06 MT568 bitext5k bitext15k bitext5k bitext15k |A| 70k 70k 70k 70k 82k 8"
P13-1031,D11-1125,0,0.173043,"lly means choosing a hinge loss. On the other hand, AdaGrad/linearized AROW only requires that the gradient of the loss function can be computed efficiently. Adaptive Online MT Algorithm 1 shows the full algorithm introduced in this paper. AdaGrad (lines 9–10) is a crucial piece, but the loss function, regularization technique, and parallelization strategy described in this section are equally important in the MT setting. 3.1 Pairwise Logistic Loss Function Algorithm 1 lines 5–8 describe the gradient computation. We cast MT tuning as pairwise ranking (Herbrich et al., 1999, inter alia), which Hopkins and May (2011) applied to MT. The pairwise approach results in simple, convex loss functions suitable for online learning. The idea is that for any two derivations, the ranking predicted by the model should be consistent with the ranking predicted by a gold sentence-level metric G like BLEU+1 (Lin and Och, 2004). Consider a single source sentence f with associated references e1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Let M (d) = w · φ(d) be the model score. For any derivation d+ that is better than d− under G, we desire pairwise agreement such"
P13-1031,N07-1008,0,0.0932284,"Missing"
P13-1031,P03-1054,1,0.008716,"w-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to B"
P13-1031,P07-2045,0,0.0116504,"some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, c Sofia, Bul"
P13-1031,N09-1069,0,0.198652,"oolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 311–321, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Adaptive Online Algorithms 2.2 Machine translation is an unusual machine learning setting because multiple correct translations exist and decoding is comparatively expensive. When we have a large feature set and therefore want to tune on a large data set, batch methods are infeasible. Online methods can converge faster, and in practice they often find better solutions (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia). Recall that stochastic gradient descent (SGD), a fundamental online method, updates weights w according to wt = wt−1 − η∇`t (wt−1 ) (1) with loss function1 `t (w) of the tth example, (sub)gradient of the loss with respect to the parameters ∇`t (wt−1 ), and learning rate η. SGD is sensitive to the learning rate η, which is difficult to set in an MT system that mixes frequent “dense” features (like the language model) with sparse features (e.g., for translation rules). Furthermore, η applies to each coordinate in the gradient, an undesirable property in"
P13-1031,P06-1096,0,0.115406,"ight lexicalized reordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4 4.2 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized"
P13-1031,N06-1014,0,0.0250986,"ight lexicalized reordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4 4.2 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized"
P13-1031,N12-1023,0,0.0600191,"statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Ou"
P13-1031,C04-1072,0,0.037745,"regularization technique, and parallelization strategy described in this section are equally important in the MT setting. 3.1 Pairwise Logistic Loss Function Algorithm 1 lines 5–8 describe the gradient computation. We cast MT tuning as pairwise ranking (Herbrich et al., 1999, inter alia), which Hopkins and May (2011) applied to MT. The pairwise approach results in simple, convex loss functions suitable for online learning. The idea is that for any two derivations, the ranking predicted by the model should be consistent with the ranking predicted by a gold sentence-level metric G like BLEU+1 (Lin and Och, 2004). Consider a single source sentence f with associated references e1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Let M (d) = w · φ(d) be the model score. For any derivation d+ that is better than d− under G, we desire pairwise agreement such that     G e(d+ ), e1:k &gt; G e(d− ), e1:k 313 ⇐⇒ M (d+ ) &gt; M (d− ) 2 According to experiments not reported in this paper. Ensuring pairwise agreement is the same as ensuring w · [φ(d+ ) − φ(d− )] &gt; 0. For learning, we need to select derivation pairs (d+ , d− ) to compute difference vectors x+ ="
P13-1031,W10-2925,0,0.0155326,"gradient” method of Langford et al. (2009) (Algorithm 2). A fixed threadpool of workers computes gradients in parallel and sends them to a master thread, which updates a central weight vector. Crucially, the weight updates need not be applied in order, so synchronization is unnecessary; the workers only idle at the end of an epoch. The consequence is that the update in line 8 of Algorithm 2 is with respect to gradient gt0 with t0 ≤ t. Langford et al. (2009) gave convergence results for 314 stale updating, but the bounds do not apply to our setting since we use L1 regularization. Nevertheless, Gimpel et al. (2010) applied this framework to other non-convex objectives and obtained good empirical results. Bilingual Ar-En Zh-En Monolingual Sentences Tokens Tokens 6.6M 9.3M 375M 538M 990M Table 1: Bilingual and monolingual corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). Our asynchronous, stochastic method has practical appeal for MT. During a tuning run, the online method decodes the tuning set under many more weight vectors than a MERT-style batch method. This characteristic may result in broader exploration of the"
P13-1031,maamouri-etal-2008-enhancing,0,0.0478018,"ature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with σ=0.1). Second"
P13-1031,J93-2004,0,0.0522379,"to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of Galley and Manning (2008), the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional “sparse” feature sets. Discrimina3 We tokenized the English with packages from the Stanford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional basel"
P13-1031,N10-1069,0,0.0335846,"dimensional models from a uniform starting point. Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient. Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on:"
P13-1031,P02-1038,0,0.168102,"Missing"
P13-1031,P12-1002,0,0.684228,"nslation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in t"
P13-1031,P06-1091,0,0.110701,"Missing"
P13-1031,D07-1080,0,0.155024,"ithms. 6 Related Work Our work relates most closely to that of Hasler et al. (2012b), who tuned models containing both sparse and dense features with Moses. A discriminative phrase table helped them improve slightly over a dense, online MIRA baseline, but their best results required initialization with MERT-tuned weights and re-tuning a single, shared weight for the discriminative phrase table with MERT. In contrast, our algorithm learned good high dimensional models from a uniform starting point. Chiang (2012) adapted AROW to MT and extended previous work on online MIRA (Chiang et al., 2008; Watanabe et al., 2007). It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient. Simianer et al. (2012) investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing (McDonald et al., 2010), which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized"
P13-1031,J04-4002,0,0.248212,"table exist in shared memory, obviating the need for remote queries. tive phrase table (PT): indicators for each rule in the phrase table. Alignments (AL): indicators for phrase-internal alignments and deleted (unaligned) source words. Discriminative reordering (LO): indicators for eight lexicalized reordering classes, including the six standard monotone/swap/discontinuous classes plus the two simpler Moses monotone/non-monotone classes. 4 4.2 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal (Cer et al., 2010), a phrasebased system based on alignment templates (Och and Ney, 2004). The corpora3 in our experiments (Table 1) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM (Stolcke, 2002) to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 4.1 Feature Templates The baseline “dense” model contains 19"
P13-1031,N12-1026,0,0.119789,"Missing"
P13-1031,P03-1021,0,0.245203,"t half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features (Hasler et al., 2012a). Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia). We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al., 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal (Cer et al., 2010) toolkit, which is freely available. 311 Proceedings of the 51st Annual Meeting of the Association for Computational"
P13-1031,P11-2074,0,0.15203,"Missing"
P13-1031,P02-1040,0,0.113953,"ing to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). Tuning Algorithms The primary baseline is the dense feature set tuned with MERT (Och, 2003). The Phrasal implementation uses the line search algorithm of Cer et al. (2008), uniform initialization, and 20 random starting points.4 We tuned according to BLEU-4 (Papineni et al., 2002). We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L2 regularization with σ=0.1). Second, we ran the k-best batch MIRA (kb-MIRA) (Cherry and Foster, 2012) implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. We ran their code with standard settings. Moses5 also contains the discriminative phrase table implementatio"
P13-1031,W05-0908,0,0.152488,"Missing"
P13-1031,2012.iwslt-evaluation.4,0,\N,Missing
P13-1031,D08-1076,0,\N,Missing
P13-1045,P04-1013,0,0.544519,"erson (2003) was the first to show that neural networks can be successfully used for large scale parsing. He introduced a left-corner parser to estimate the probabilities of parsing decisions conditioned on the parsing history. The input to Henderson’s model consists of pairs of frequent words and their part-of-speech (POS) tags. Both the original parsing system and its probabilistic interpretation (Titov and Henderson, 2007) learn features that represent the parsing history and do not provide a principled linguistic representation like our phrase representations. Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains. Costa et al. (2003) apply recursive neural networks to re-rank possible phrase attachments in an incremental parser. Their work is the first to show that RNNs can capture enough information to make correct parsing decisions, but they only test on a subset of 2000 sentences. Menchetti et al. (2005) use RNNs to re-rank different parses. For their results on full sentence parsing, they rerank candidate trees created by the Collins pa"
P13-1045,W05-1506,0,0.0157004,"nuous nature of the word vectors, the probability of such a CVG rule application is not comparable to probabilities provided by a PCFG since the latter sum to 1 for all children. Assuming that node p1 has syntactic category P1 , we compute the second parent vector via:    a (2) (A,P1 ) p =f W . p(1) We use this knowledge to speed up inference via two bottom-up passes through the parsing chart. During the first one, we use only the base PCFG to run CKY dynamic programming through the tree. The k = 200-best parses at the top cell of the chart are calculated using the efficient algorithm of (Huang and Chiang, 2005). Then, the second pass is a beam search with the full CVG model (including the more expensive matrix multiplications of the SU-RNN). This beam search only considers phrases that appear in the top 200 parses. This is similar to a re-ranking setup but with one main difference: the SU-RNN rule score computation at each node still only has access to its child vectors, not the whole tree or other global features. This allows the second pass to be very fast. We use this setup in our experiments below. The score of the last parent in this trigram is computed via:   T s p(2) = v (A,P1 ) p(2) + log"
P13-1045,J92-4003,0,0.182319,"s in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions. However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others. But, even with large corpora, many Introduction Syntactic parsing is a central task in natural language processing because of its importance in mediating between linguistic expression and meaning. For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (Callison-Burch, 2008). Syntactic descriptions standardly use coarse discrete categories such as NP for noun phrases or PP for prepositional phrases. However, recent work has shown"
P13-1045,P12-1092,1,0.205939,"n ≈ queen (Mikolov et al., 2013). Collobert and Weston (2008) introduced a new model to compute such an embedding. The idea is to construct a neural network that outputs high scores for windows that occur in a large unlabeled corpus and low scores for windows where one word is replaced by a random word. When such a network is optimized via gradient ascent the derivatives backpropagate into the word embedding matrix X. In order to predict correct scores the vectors in the matrix capture co-occurrence statistics. For further details and evaluations of these embeddings, see (Turian et al., 2010; Huang et al., 2012). The resulting X matrix is used as follows. Assume we are given a sentence as an ordered list of m words. Each word w has an index [w] = i into the columns of the embedding matrix. This index is used to retrieve the word’s vector representation aw using a simple multiplication with a binary vector e, which is zero everywhere, except Compositional Vector Grammars This section introduces Compositional Vector Grammars (CVGs), a model to jointly find syntactic structure and capture compositional semantic information. CVGs build on two observations. Firstly, that a lot of the structure and regular"
P13-1045,D08-1021,0,0.0313765,"ervations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others. But, even with large corpora, many Introduction Syntactic parsing is a central task in natural language processing because of its importance in mediating between linguistic expression and meaning. For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (Callison-Burch, 2008). Syntactic descriptions standardly use coarse discrete categories such as NP for noun phrases or PP for prepositional phrases. However, recent work has shown that parsing results can be greatly improved by defining more fine-grained syntactic 455 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455–465, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics sets of discrete states and recursive deep learning models that jointly learn classifiers and continuous feature representations for variable-sized inputs. n-grams w"
P13-1045,C12-2054,0,0.0497241,"every node. Our syntactically untied RNNs outperform them by a significant margin. The idea of untying has also been successfully used in deep learning applied to vision (Le et al., 2010). This paper uses several ideas of (Socher et al., 2011b). The main differences are (i) the dual representation of nodes as discrete categories and vectors, (ii) the combination with a PCFG, and (iii) the syntactic untying of weights based on child categories. We directly compare models with fully tied and untied weights. Another work that represents phrases with a dual discrete-continuous representation is (Kartsaklis et al., 2012). 3 3.1 Word Vector Representations In most systems that use a vector representation for words, such vectors are based on cooccurrence statistics of each word and its context (Turney and Pantel, 2010). Another line of research to learn distributional word vectors is based on neural language models (Bengio et al., 2003) which jointly learn an embedding of words into an n-dimensional feature space and use these embeddings to predict how suitable a word is in its context. These vector representations capture interesting linear relationships (up to some accuracy), such as king−man+woman ≈ queen (M"
P13-1045,P05-1022,0,0.0600912,"Missing"
P13-1045,P03-1054,1,0.0562259,"the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments. 1 (riding a bike,VP, ) (a bike,NP, (riding,V, ) (a,Det, ) ) (bike,NN, ) Figure 1: Example of a CVG tree with (category,vector) representations at each node. The vectors for nonterminals are computed via a new type of recursive neural network which is conditioned on syntactic categories from a PCFG. categories, which better capture phrases with similar behavior, whether through manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexi"
P13-1045,A00-2018,0,0.477832,"manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions. However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others. But, even with large corpora, many Introduction Syntactic parsing is a central task in natural language processing b"
P13-1045,P97-1003,0,0.167285,"Missing"
P13-1045,J03-4003,0,0.195632,"hether through manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions. However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others. But, even with large corpora, many Introduction Syntactic parsing is a central task in natural lang"
P13-1045,D12-1096,0,0.214919,"red parser (Klein and Manning, 2003b)) and other parsers that use richer state representations: the Berkeley parser (Petrov and Klein, 2007), Collins parser (Collins, 1997), SSN: a statistical neural network parser (Henderson, 2004), Factored PCFGs (Hall and Klein, 2012), CharniakSelfTrain: the self-training approach of McClosky et al. (2006), which bootstraps and parses additional large corpora multiple times, Charniak-RS: the state of the art self-trained and discriminatively re-ranked Charniak-Johnson parser combining (Charniak, 2000; McClosky et al., 2006; Charniak and Johnson, 2005). See Kummerfeld et al. (2012) for more comparisons. We compare also to a standard RNN ‘CVG (RNN)’ and to the proposed CVG with SU-RNNs. 4.3 CVG 0.79 0.43 0.29 0.27 0.31 0.32 0.31 0.22 0.19 0.41 Table 2: Detailed comparison of different parsers. performance and were faster than 50-,100- or 200dimensional ones. We hypothesize that the larger word vector sizes, while capturing more semantic knowledge, result in too many SU-RNN matrix parameters to train and hence perform worse. 4.2 Stanford 1.02 0.64 0.40 0.37 0.44 0.39 0.48 0.35 0.28 0.62 Model Analysis Analysis of Error Types. Table 2 shows a detailed comparison of differe"
P13-1045,P05-1010,0,0.252159,"Missing"
P13-1045,N06-1020,0,0.319562,"Missing"
P13-1045,P08-1109,1,0.432055,"ype of recursive neural network which is conditioned on syntactic categories from a PCFG. categories, which better capture phrases with similar behavior, whether through manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions. However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional simil"
P13-1045,P02-1031,0,0.020181,"estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others. But, even with large corpora, many Introduction Syntactic parsing is a central task in natural language processing because of its importance in mediating between linguistic expression and meaning. For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (Callison-Burch, 2008). Syntactic descriptions standardly use coarse discrete categories such as NP for noun phrases or PP for prepositional phrases. However, recent work has shown that parsing results can be greatly improved by defining more fine-grained syntactic 455 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455–465, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics sets of discrete states and recursive deep learning models that jointly learn classifiers and continuous feature repr"
P13-1045,N07-1051,0,0.181023,"Missing"
P13-1045,P06-1055,0,0.178353,"rns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments. 1 (riding a bike,VP, ) (a bike,NP, (riding,V, ) (a,Det, ) ) (bike,NN, ) Figure 1: Example of a CVG tree with (category,vector) representations at each node. The vectors for nonterminals are computed via a new type of recursive neural network which is conditioned on syntactic categories from a PCFG. categories, which better capture phrases with similar behavior, whether through manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of"
P13-1045,D12-1105,0,0.0155869,".edu. 2 Improving Discrete Syntactic Representations As mentioned in the introduction, there are several approaches to improving discrete representations for parsing. Klein and Manning (2003a) use manual feature engineering, while Petrov et al. (2006) use a learning algorithm that splits and merges the syntactic categories in order to maximize likelihood on the treebank. Their approach splits categories into several dozen subcategories. Another approach is lexicalized parsers (Collins, 2003; Charniak, 2000) that describe each category with a lexical item, usually the head word. More recently, Hall and Klein (2012) combine several such annotation schemes in a factored parser. We extend the above ideas from discrete representations to richer continuous ones. The CVG can be seen as factoring discrete and continuous parsing in one model. Another different approach to the above generative models is to learn discriminative parsers using many well designed features (Taskar et al., 2004; Finkel et al., 2008). We also borrow ideas from this line of research in that our parser combines the generative PCFG model with discriminatively learned RNNs. Deep Learning and Recursive Deep Learning Early attempts at using"
P13-1045,E03-1002,0,0.0419821,"ne syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b). The CVG model merges ideas from both generative models that assume discrete syntactic categories and discriminative models that are trained using continuous vectors. We will first briefly introduce single word vector representations and then describe the CVG objective function, tree scoring and inference. Henderson (2003) was the first to show that neural networks can be successfully used for large scale parsing. He introduced a left-corner parser to estimate the probabilities of parsing decisions conditioned on the parsing history. The input to Henderson’s model consists of pairs of frequent words and their part-of-speech (POS) tags. Both the original parsing system and its probabilistic interpretation (Titov and Henderson, 2007) learn features that represent the parsing history and do not provide a principled linguistic representation like our phrase representations. Other related work includes (Henderson, 2"
P13-1045,D12-1110,1,0.410492,"standard RNNs can be used for parsing, see Socher et al. (2011b). The standard RNN requires a single composition function to capture all types of compositions: adjectives and nouns, verbs and nouns, adverbs and adjectives, etc. Even though this function is a powerful one, we find a single neural network weight matrix cannot fully capture the richness of compositionality. Several extensions are possible: A two-layered RNN would provide more expressive power, however, it is much harder to train because the resulting neural network becomes very deep and suffers from vanishing gradient problems. Socher et al. (2012) proposed to give every single word a matrix and a vector. The matrix is then applied to the sibling node’s vector during the composition. While this results in a powerful composition function that essentially depends on the words being combined, the number of model parameters explodes and the composition functions do not capture the syntactic commonalities between similar POS tags or syntactic categories. Based on the above considerations, we propose the Compositional Vector Grammar (CVG) that conditions the composition function at each node on discrete syntactic categories extracted from a w"
P13-1045,W04-3201,1,0.382076,"computed via a new type of recursive neural network which is conditioned on syntactic categories from a PCFG. categories, which better capture phrases with similar behavior, whether through manual feature engineering (Klein and Manning, 2003a) or automatic learning (Petrov et al., 2006). However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity. Two strands of work therefore attempt to go further. First, recent work in discriminative parsing has shown gains from careful engineering of features (Taskar et al., 2004; Finkel et al., 2008). Features in such parsers can be seen as defining effective dimensions of similarity between categories. Second, lexicalized parsers (Collins, 2003; Charniak, 2000) associate each category with a lexical item. This gives a fine-grained notion of semantic similarity, which is useful for tackling problems like ambiguous attachment decisions. However, this approach necessitates complex shrinkage estimation schemes to deal with the sparsity of observations of the lexicalized categories. In many natural language systems, single words and n-grams are usefully described by thei"
P13-1045,W06-2902,0,0.0177603,"ge scale parsing. He introduced a left-corner parser to estimate the probabilities of parsing decisions conditioned on the parsing history. The input to Henderson’s model consists of pairs of frequent words and their part-of-speech (POS) tags. Both the original parsing system and its probabilistic interpretation (Titov and Henderson, 2007) learn features that represent the parsing history and do not provide a principled linguistic representation like our phrase representations. Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains. Costa et al. (2003) apply recursive neural networks to re-rank possible phrase attachments in an incremental parser. Their work is the first to show that RNNs can capture enough information to make correct parsing decisions, but they only test on a subset of 2000 sentences. Menchetti et al. (2005) use RNNs to re-rank different parses. For their results on full sentence parsing, they rerank candidate trees created by the Collins parser (Collins, 2003). Similar to their work, we use the idea of letting discrete categories reduce"
P13-1045,P07-1080,0,0.592471,"e models that are trained using continuous vectors. We will first briefly introduce single word vector representations and then describe the CVG objective function, tree scoring and inference. Henderson (2003) was the first to show that neural networks can be successfully used for large scale parsing. He introduced a left-corner parser to estimate the probabilities of parsing decisions conditioned on the parsing history. The input to Henderson’s model consists of pairs of frequent words and their part-of-speech (POS) tags. Both the original parsing system and its probabilistic interpretation (Titov and Henderson, 2007) learn features that represent the parsing history and do not provide a principled linguistic representation like our phrase representations. Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains. Costa et al. (2003) apply recursive neural networks to re-rank possible phrase attachments in an incremental parser. Their work is the first to show that RNNs can capture enough information to make correct parsing decisions, but they only test on"
P13-1045,P10-1040,0,0.436676,"such as king−man+woman ≈ queen (Mikolov et al., 2013). Collobert and Weston (2008) introduced a new model to compute such an embedding. The idea is to construct a neural network that outputs high scores for windows that occur in a large unlabeled corpus and low scores for windows where one word is replaced by a random word. When such a network is optimized via gradient ascent the derivatives backpropagate into the word embedding matrix X. In order to predict correct scores the vectors in the matrix capture co-occurrence statistics. For further details and evaluations of these embeddings, see (Turian et al., 2010; Huang et al., 2012). The resulting X matrix is used as follows. Assume we are given a sentence as an ordered list of m words. Each word w has an index [w] = i into the columns of the embedding matrix. This index is used to retrieve the word’s vector representation aw using a simple multiplication with a binary vector e, which is zero everywhere, except Compositional Vector Grammars This section introduces Compositional Vector Grammars (CVGs), a model to jointly find syntactic structure and capture compositional semantic information. CVGs build on two observations. Firstly, that a lot of the"
P13-1045,N13-1090,0,\N,Missing
P13-1106,P02-1051,0,0.0382439,"in, China, 150001 Stanford, CA 94305 mengqiu@cs.stanford.edu car@ir.hit.edu.cn manning@cs.stanford.edu Abstract sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers. Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as"
P13-1106,W03-2201,0,0.0168632,"ment and Bilingual Named Entity Recognition Using Dual Decomposition Mengqiu Wang Wanxiang Che Christopher D. Manning Stanford University Harbin Institute of Technology Stanford University Stanford, CA 94305 Harbin, China, 150001 Stanford, CA 94305 mengqiu@cs.stanford.edu car@ir.hit.edu.cn manning@cs.stanford.edu Abstract sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers. Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated"
P13-1106,P91-1034,0,0.0787714,"and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines. 1 Because human translation in general preserves semantic equivalence, bi-texts represent two perspectives on the same semantic content (Burkett et al., 2010b). As a result, we can find complementary cues in the two languages that help to disambiguate named entity mentions (Brown et al., 1991). For example, the English word “Jordan” can be either a last name or a country. Without sufficient context it can be difficult to distinguish the two; however, in Chinese, these two senses are disambiguated: “乔丹” as a last name, and “约旦” as a country name. Introduction We study the problem of Named Entity Recognition (NER) in a bilingual context, where the goal is to annotate parallel bi-texts with named entity tags. This is a particularly important problem for machine translation (MT) since entities such as person names, locations, organizations, etc. carry much of the information expressed"
P13-1106,P04-1056,0,0.0200656,"how we enforce agreement between every aligned word 1080 pair. Martins et al. (2011b) presented a new DD method that combines the power of DD with the augmented Lagrangian method. They showed that their method can achieve faster convergence than traditional sub-gradient methods in models with many overlapping components (Martins et al., 2011a). This method is directly applicable to our work. Another promising direction for improving NER performance is in enforcing global label consistency across documents, which is an idea that has been greatly explored in the past (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005). More recently, Rush et al. (2012) and Chieu and Teow (2012) have shown that combining local prediction models with global consistency models, and enforcing agreement via DD is very effective. It is straightforward to incorporate an additional global consistency model into our model for further improvements. Our joint alignment and NER decoding approach is inspired by prior work on improving alignment quality through encouraging agreement between bi-directional models (Liang et al., 2006; DeNero and Macherey, 2011). Instead of enforcing agreement in the alignment space b"
P13-1106,N10-1015,0,0.0814644,"ford.edu car@ir.hit.edu.cn manning@cs.stanford.edu Abstract sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers. Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading error"
P13-1106,W10-2906,0,0.160361,"ford.edu car@ir.hit.edu.cn manning@cs.stanford.edu Abstract sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers. Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading error"
P13-1106,P10-1065,0,0.127673,"asal constraints are indirectly imposed by entity spans. We also differ in the implementation details, where in their case belief propagation is used in both training and Viterbi inference. Burkett et al. (2010a) presented a supervised learning method for performing joint parsing and word alignment using log-linear models over parse trees and an ITG model over alignment. The model demonstrates performance improvements in both parsing and alignment, but shares the common limitations of other supervised work in that it requires manually annotated bilingual joint parsing and word alignment data. Chen et al. (2010) also tackled the problem of joint alignment and NER. Their method employs a set of heuristic rules to expand a candidate named entity set generated by monolingual taggers, and then rank those candidates using a bilingual named entity dictionary. Our approach differs in that we provide a probabilistic formulation of the problem and do not require pre-existing NE dictionaries. 9 Conclusion We introduced a graphical model that combines two HMM word aligners and two CRF NER taggers into a joint model, and presented a dual decomposition inference method for performing efficient decoding over this"
P13-1106,E09-1020,0,0.0436736,"Missing"
P13-1106,P08-2007,0,0.0200217,"ani, 1993). For alignment experiments, we train two uni1 The exact feature set and the CRF implementation can be found here: http://nlp.stanford.edu/ software/CRF-NER.shtml directional HMM models as our baseline and monolingual alignment models. The parameters of the HMM were initialized by IBM Model 1 using the agreement-based EM training algorithms from Liang et al. (2006). Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011). Word alignment evaluation is done over the sections of OntoNotes that have matching goldstandard word alignment annotations from GALE Y1Q4 dataset.2 This subset contains 288 documents and 3,391 sentence pairs. We will refer to this subset as wa-subset. This evaluation set is over 20 times larger than the 150 sentences set used in most past evaluations (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011). Alignments input to the B I -NER model are produced by thresholding the averaged posterior probability at 0.5. In joi"
P13-1106,P11-1043,0,0.160678,"4-9 2013. 2013 Association for Computational Linguistics B-ORG I-ORG I-ORG [O] B-LOC O O Xinhua News Agency Beijing Feb 16 e1 e2 e3 e4 e5 e6 f1 f2 f3 f4 f5 f6 新华社 ， 北京 ， 二月 十六 O O B-ORG O B-GPE O Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences. The [O] tag is an example of a wrong tag assignment. The dashed alignment link between e3 and f2 is an example of alignment error. previous applications of the DD method in NLP, where the model typically factors over two components and agreement is to be sought between the two (Rush et al., 2010; Koo et al., 2010; DeNero and Macherey, 2011; Chieu and Teow, 2012), our method decomposes the larger graphical model into many overlapping components where each alignment edge forms a separate factor. We design clique potentials over the alignment-based edges to encourage entity tag agreements. Our method does not require any manual annotation of word alignments or named entities over the bilingual training data. The aforementioned B I -NER model assumes fixed alignment input given by an underlying word aligner. But the entity span and type predictions given by the NER models contain complementary information for correcting alignment e"
P13-1106,D09-1127,0,0.0229834,") are baseline output. distance swapping phenomena. The two unidirectional HMMs also have strong disagreements over the alignments, and the resulting baseline aligner output only recovers two links. If we were to take this alignment as fixed input, most likely we would not be able to recover the error over e11 , but the joint decoding method successfully recovered 4 more links, and indirectly resulted in the NER tagging improvement discussed above. 8 Related Work The idea of employing bilingual resources to improve over monolingual systems has been explored by much previous work. For example, Huang et al. (2009) improved parsing performance using a bilingual parallel corpus. In the NER domain, Li et al. (2012) presented a cyclic CRF model very similar to our B I -NER model, and performed approximate inference using loopy belief propagation. The feature-rich CRF formulation of bilingual edge potentials in their model is much more powerful than our simple PMI-based bilingual edge model. Adding a richer bilingual edge model might well further improve our results, and this is a possible direction for further experimentation. However, a big drawback of this approach is that training such a feature-rich mo"
P13-1106,P12-1073,0,0.193826,"d.edu Abstract sentence. Recognizing them provides useful information for phrase detection and word sense disambiguation (e.g., “melody” as in a female name has a different translation from the word “melody” in a musical sense), and can be directly leveraged to improve translation quality (Babych and Hartley, 2003). We can also automatically construct a named entity translation lexicon by annotating and extracting entities from bi-texts, and use it to improve MT performance (Huang and Vogel, 2002; Al-Onaizan and Knight, 2002). Previous work such as Burkett et al. (2010b), Li et al. (2012) and Kim et al. (2012) have also demonstrated that bitexts annotated with NER tags can provide useful additional training sources for improving the performance of standalone monolingual taggers. Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information c"
P13-1106,D10-1125,0,0.131861,", Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics B-ORG I-ORG I-ORG [O] B-LOC O O Xinhua News Agency Beijing Feb 16 e1 e2 e3 e4 e5 e6 f1 f2 f3 f4 f5 f6 新华社 ， 北京 ， 二月 十六 O O B-ORG O B-GPE O Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences. The [O] tag is an example of a wrong tag assignment. The dashed alignment link between e3 and f2 is an example of alignment error. previous applications of the DD method in NLP, where the model typically factors over two components and agreement is to be sought between the two (Rush et al., 2010; Koo et al., 2010; DeNero and Macherey, 2011; Chieu and Teow, 2012), our method decomposes the larger graphical model into many overlapping components where each alignment edge forms a separate factor. We design clique potentials over the alignment-based edges to encourage entity tag agreements. Our method does not require any manual annotation of word alignments or named entities over the bilingual training data. The aforementioned B I -NER model assumes fixed alignment input given by an underlying word aligner. But the entity span and type predictions given by the NER models contain complementary information"
P13-1106,N06-1014,0,0.234913,"ore the distinction between B- and Itags. We report standard NER measures (entity precision (P), recall (R) and F1 score) on the test set. Statistical significance tests are done using the paired bootstrap resampling method (Efron and Tibshirani, 1993). For alignment experiments, we train two uni1 The exact feature set and the CRF implementation can be found here: http://nlp.stanford.edu/ software/CRF-NER.shtml directional HMM models as our baseline and monolingual alignment models. The parameters of the HMM were initialized by IBM Model 1 using the agreement-based EM training algorithms from Liang et al. (2006). Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011). Word alignment evaluation is done over the sections of OntoNotes that have matching goldstandard word alignment annotations from GALE Y1Q4 dataset.2 This subset contains 288 documents and 3,391 sentence pairs. We will refer to this subset as wa-subset. This evaluation set is over 20 times larger than"
P13-1106,ma-2006-champollion,0,0.027723,"such as the neighborhood constraint proposed by DeNero and Macherey (2011) can be easily integrated into our model. The neighborhood constraint enforces that if fj is aligned to ei , then fj can only be aligned to ei+1 or ei−1 (with a small penalty), but not any other word position. We report results of adding neighborhood constraints to our model in Section 6. 4 Experimental Setup We evaluate on the large OntoNotes (v4.0) corpus (Hovy et al., 2006) which contains manually 1077 annotated NER tags for both Chinese and English. Document pairs are sentence aligned using the Champollion Tool Kit (Ma, 2006). After discarding sentences with no aligned counterpart, a total of 402 documents and 8,249 parallel sentence pairs were used for evaluation. We will refer to this evaluation set as full-set. We use odd-numbered documents as the dev set and evennumbered documents as the blind test set. We did not perform parameter tuning on the dev set to optimize performance, instead we fix the initial learning rate to 0.5 and maximum iterations to 1,000 in all DD experiments. We only use the dev set for model development. The Stanford CRF-based NER tagger was used as the monolingual component in our models"
P13-1106,D11-1022,0,0.0542004,"Missing"
P13-1106,D11-1001,0,0.0193111,"lored an “up-training” mechanism by using the outputs from a strong monolingual model as ground-truth, and simulated a learning environment where a bilingual model is trained to help a “weakened” monolingual model to recover the results of the strong model. It is worth mentioning that since our method does not require additional training and can take pretty much any existing model as “black-box” during decoding, the richer and more accurate bilingual model learned from Burkett et al. (2010b) can be directly plugged into our model. A similar dual decomposition algorithm to ours was proposed by Riedel and McCallum (2011) for biomedical event detection. In their Model 3, the trigger and argument extraction models are reminiscent of the two monolingual CRFs in our model; additional binding agreements are enforced over every protein pair, similar to how we enforce agreement between every aligned word 1080 pair. Martins et al. (2011b) presented a new DD method that combines the power of DD with the augmented Lagrangian method. They showed that their method can achieve faster convergence than traditional sub-gradient methods in models with many overlapping components (Martins et al., 2011a). This method is directl"
P13-1106,1993.eamt-1.1,0,0.272285,"so on, we select the four most commonly seen named entity types for evaluation. They are person, location, organization and GPE. All entities of these four types are converted to the standard BIO format, and background tokens and all other entity types are marked with tag O. When we consider label agreements over aligned word pairs in all bilingual agreement models, we ignore the distinction between B- and Itags. We report standard NER measures (entity precision (P), recall (R) and F1 score) on the test set. Statistical significance tests are done using the paired bootstrap resampling method (Efron and Tibshirani, 1993). For alignment experiments, we train two uni1 The exact feature set and the CRF implementation can be found here: http://nlp.stanford.edu/ software/CRF-NER.shtml directional HMM models as our baseline and monolingual alignment models. The parameters of the HMM were initialized by IBM Model 1 using the agreement-based EM training algorithms from Liang et al. (2006). Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (DeNero and"
P13-1106,P05-1045,1,0.0630441,"After discarding sentences with no aligned counterpart, a total of 402 documents and 8,249 parallel sentence pairs were used for evaluation. We will refer to this evaluation set as full-set. We use odd-numbered documents as the dev set and evennumbered documents as the blind test set. We did not perform parameter tuning on the dev set to optimize performance, instead we fix the initial learning rate to 0.5 and maximum iterations to 1,000 in all DD experiments. We only use the dev set for model development. The Stanford CRF-based NER tagger was used as the monolingual component in our models (Finkel et al., 2005). It also serves as a stateof-the-art monolingual baseline for both English and Chinese. For English, we use the default tagger setting from Finkel et al. (2005). For Chinese, we use an improved set of features over the default tagger, which includes distributional similarity features trained on large amounts of nonoverlapping data.1 We train the two CRF models on all portions of the OntoNotes corpus that are annotated with named entity tags, except the parallel-aligned portion which we reserve for development and test purposes. In total, there are about 660 training documents (∼16k sentences)"
P13-1106,D10-1001,0,0.394667,"ags. This is a particularly important problem for machine translation (MT) since entities such as person names, locations, organizations, etc. carry much of the information expressed in the source In this work, we first develop a bilingual NER model (denoted as B I -NER) by embedding two monolingual CRF-based NER models into a larger undirected graphical model, and introduce additional edge factors based on word alignment (WA). Because the new bilingual model contains many cyclic cliques, exact inference is intractable. We employ a dual decomposition (DD) inference algorithm (Bertsekas, 1999; Rush et al., 2010) for performing approximate inference. Unlike most 1073 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1073–1082, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics B-ORG I-ORG I-ORG [O] B-LOC O O Xinhua News Agency Beijing Feb 16 e1 e2 e3 e4 e5 e6 f1 f2 f3 f4 f5 f6 新华社 ， 北京 ， 二月 十六 O O B-ORG O B-GPE O Figure 1: Example of NER labels between two word-aligned bilingual parallel sentences. The [O] tag is an example of a wrong tag assignment. The dashed alignment link between e3 and f2 is an example of alignment error"
P13-1106,D12-1131,0,0.0138707,"Martins et al. (2011b) presented a new DD method that combines the power of DD with the augmented Lagrangian method. They showed that their method can achieve faster convergence than traditional sub-gradient methods in models with many overlapping components (Martins et al., 2011a). This method is directly applicable to our work. Another promising direction for improving NER performance is in enforcing global label consistency across documents, which is an idea that has been greatly explored in the past (Sutton and McCallum, 2004; Bunescu and Mooney, 2004; Finkel et al., 2005). More recently, Rush et al. (2012) and Chieu and Teow (2012) have shown that combining local prediction models with global consistency models, and enforcing agreement via DD is very effective. It is straightforward to incorporate an additional global consistency model into our model for further improvements. Our joint alignment and NER decoding approach is inspired by prior work on improving alignment quality through encouraging agreement between bi-directional models (Liang et al., 2006; DeNero and Macherey, 2011). Instead of enforcing agreement in the alignment space based on best sequences found by Viterbi, we could opt to"
P13-1106,P09-1104,0,0.0110565,"t experiments, we train two uni1 The exact feature set and the CRF implementation can be found here: http://nlp.stanford.edu/ software/CRF-NER.shtml directional HMM models as our baseline and monolingual alignment models. The parameters of the HMM were initialized by IBM Model 1 using the agreement-based EM training algorithms from Liang et al. (2006). Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011). Word alignment evaluation is done over the sections of OntoNotes that have matching goldstandard word alignment annotations from GALE Y1Q4 dataset.2 This subset contains 288 documents and 3,391 sentence pairs. We will refer to this subset as wa-subset. This evaluation set is over 20 times larger than the 150 sentences set used in most past evaluations (DeNero and Klein, 2008; Haghighi et al., 2009; DeNero and Macherey, 2011). Alignments input to the B I -NER model are produced by thresholding the averaged posterior probability at 0.5. In joint NER and alignment ex"
P13-1106,N06-2015,0,0.0268304,"her by having them all agree with the edge cliques. It is also worth noting that since we decode the alignment models with Viterbi inference, additional constraints such as the neighborhood constraint proposed by DeNero and Macherey (2011) can be easily integrated into our model. The neighborhood constraint enforces that if fj is aligned to ei , then fj can only be aligned to ei+1 or ei−1 (with a small penalty), but not any other word position. We report results of adding neighborhood constraints to our model in Section 6. 4 Experimental Setup We evaluate on the large OntoNotes (v4.0) corpus (Hovy et al., 2006) which contains manually 1077 annotated NER tags for both Chinese and English. Document pairs are sentence aligned using the Champollion Tool Kit (Ma, 2006). After discarding sentences with no aligned counterpart, a total of 402 documents and 8,249 parallel sentence pairs were used for evaluation. We will refer to this evaluation set as full-set. We use odd-numbered documents as the dev set and evennumbered documents as the blind test set. We did not perform parameter tuning on the dev set to optimize performance, instead we fix the initial learning rate to 0.5 and maximum iterations to 1,000"
P14-2021,W01-0501,0,0.156581,"Missing"
P14-2021,P05-1045,1,0.0638419,"Missing"
P14-2021,W03-0419,0,0.103217,"Missing"
P14-2022,koen-2004-pharaoh,0,0.702223,"eses that have exactly the same state can be recombined and efficiently handled via dynamic programming, but there is no special handling for partial agreement. Therefore, features are repeatedly consulted regarding hypotheses that differ only in ways irrelevant to their score, such as coverage of the source sentence. Our decoder bundles hypotheses into equivalence classes so that features can focus on the relevant parts of state. We pay particular attention to the language model because it is responsible for much of the hypothesis state. As the decoder builds translations from left to right (Koehn, 2004), it records the last N − 1 words of each hypothesis so that they can be used as context to score the first N − 1 words of a phrase, where N is the order of the language model. Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes. Our algorithm instead discovers good combinations in a coarse-to-fine manner. The algorithm exploits the fact that hypotheses often share the same suffix and phrases often share the same prefix. These shared suffixes and prefixes allow the algorithm to coarsely reason"
P14-2022,N10-2003,1,0.85227,", improvement can be used to increase speed or accuracy. When tuned to attain the same accuracy, our algorithm is 4.0–7.7 times as fast as the Moses decoder with cube pruning. 1 Introduction Translation speed is critical to making suggestions as translators type, mining for parallel data by translating the web, and running on mobile devices without Internet connectivity. We contribute a fast decoding algorithm for phrase-based machine translation along with an implementation in a new open-source (LGPL) decoder available at http://kheafield.com/code/. Phrase-based decoders (Koehn et al., 2007; Cer et al., 2010; Wuebker et al., 2012) keep track of several types of state with translation hypothe2 Related Work Our previous work (Heafield et al., 2013) developed language model state refinement for bottom130 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 130–135, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics up decoding in syntatic machine translation. In bottom-up decoding, hypotheses can be extended to the left or right, so hypotheses keep track of both their prefix and suffix. The present phra"
P14-2022,W08-0402,0,0.766032,"and Chiang, 2007). Sections 3.2 and later show our contribution. 131 a few nations  diplomatic are which  have diplomatic Figure 3: Target phrases arranged into a trie. Set in italic, leaves reveal parts of the phrase that are irrelevant to the language model. countries Figure 2: Hypothesis suffixes arranged into a trie. The leaves indicate source coverage and any other hypothesis state. arrange the target phrases into a prefix trie. An example is shown in Figure 3. Similar to the hypothesis trie, the depth may be shorter than N − 1 in cases where the language model will provably back off (Li and Khudanpur, 2008). The trie can also be short because the target phrase has fewer than N − 1 words. We currently store this trie data structure directly in the phrase table, though it could also be computed on demand to save memory. Empirically, our phrase table uses less RAM than Moses’s memory-based phrase table. As an optimization, a trie reveals multiple words when there would otherwise be no branching. This allows the search algorithm to make decisions only when needed. Following Heafield et al. (2013), leaves in the trie take the score of the underlying hypothesis or target phrase. Non-leaf nodes take th"
P14-2022,D11-1003,0,0.0660812,"detailed models, pruning after each pass. The key difference in our work is that, rather than refining models in lock step, we effectively refine the language model on demand for hypotheses that score well. Moreover, their work was performed in syntactic machine translation while we address issues specific to phrase-based translation. Our baseline is cube pruning (Chiang, 2007; Huang and Chiang, 2007), which is both a way to organize search and an algorithm to search through cross products of sets. We adopt the same search organization (Section 3.1) but change how cross products are searched. Chang and Collins (2011) developed an exact decoding algorithm based on Lagrangian relaxation. However, it has not been shown to tractably scale to 5-gram language models used by many modern translation systems. 3 0 word 1 word the cat . 2 words cat the cat cat the . the 3 words cat . the cat . cat the . . the cat Figure 1: Stacks to translate the French “le chat .” into English. Filled circles indicate that the source word has been translated. A phrase translates “le chat” as simply “cat”, emphasizing that stacks are organized by the number of source words rather than the number of target words. 3.1 Search Organizat"
P14-2022,P02-1040,0,0.0891183,"illion words on the English side. The bitext contains data from several sources, including news articles, UN proceedings, Hong Kong government documents, online forum data, and specialized sources such as an idiom translation table. We also trained our language model on the English half of this bitext using unpruned interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The system has standard phrase table, length, distortion, and language model features. We plan to implement lexicalized reordering in future work; without this, the test system is 0.53 BLEU (Papineni et al., 2002) point behind a state-of-theart system. We set the reordering limit to R = 15. The phrase table was pre-pruned by applying the same heuristic as Moses: select the top 20 target phrases by score, including the language model. Priority Queue Search proceeds in a best-first fashion controlled by a priority queue. For each source phrase, we convert the compatible hypotheses into a trie. The target phrases were already converted into a trie when the phrase table was loaded. We then push the root (, ) boundary pair into the priority queue. We do this for all source phrases under consideration, put"
P14-2022,J07-2003,0,0.842189,"heses that differ only in ways irrelevant to their score, such as coverage of the source sentence. Our decoder bundles hypotheses into equivalence classes so that features can focus on the relevant parts of state. We pay particular attention to the language model because it is responsible for much of the hypothesis state. As the decoder builds translations from left to right (Koehn, 2004), it records the last N − 1 words of each hypothesis so that they can be used as context to score the first N − 1 words of a phrase, where N is the order of the language model. Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes. Our algorithm instead discovers good combinations in a coarse-to-fine manner. The algorithm exploits the fact that hypotheses often share the same suffix and phrases often share the same prefix. These shared suffixes and prefixes allow the algorithm to coarsely reason over many combinations at once. Our primary contribution is a new search algorithm that exploits the above observations, namely that state can be divided into pieces relevant to each feature and that language model state c"
P14-2022,D08-1012,0,0.175522,"Missing"
P14-2022,2011.iwslt-evaluation.24,1,0.87672,"age of the source sentence and the state of other features. Each source phrase translates to a set of target phrases. Because these phrases will be appended to a hypothesis, the first few words matter the most to the language model. We therefore times the weight of the language model. This has the effect of cancelling out the estimate made 132 3.6 when the phrase was scored in isolation, replacing it with a more accurate estimate based on available context. These score adjustments are efficient to compute because the decoder retained a pointer to “that” in the language model’s data structure (Heafield et al., 2011). 3.4 We build hypotheses from left-to-right and manage stacks just like cube pruning. The only difference is how the k elements of these stacks are selected. When the decoder matches a hypothesis with a compatible source phrase, we immediately evaluate the distortion feature and update future costs, both of which are independent of the target phrase. Our future costs are exactly the same as those used in Moses (Koehn et al., 2007): the highest-scoring way to cover the rest of the source sentence. This includes the language model score within target phrases but ignores the change in language m"
P14-2022,C12-3061,0,0.0506586,"be used to increase speed or accuracy. When tuned to attain the same accuracy, our algorithm is 4.0–7.7 times as fast as the Moses decoder with cube pruning. 1 Introduction Translation speed is critical to making suggestions as translators type, mining for parallel data by translating the web, and running on mobile devices without Internet connectivity. We contribute a fast decoding algorithm for phrase-based machine translation along with an implementation in a new open-source (LGPL) decoder available at http://kheafield.com/code/. Phrase-based decoders (Koehn et al., 2007; Cer et al., 2010; Wuebker et al., 2012) keep track of several types of state with translation hypothe2 Related Work Our previous work (Heafield et al., 2013) developed language model state refinement for bottom130 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 130–135, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics up decoding in syntatic machine translation. In bottom-up decoding, hypotheses can be extended to the left or right, so hypotheses keep track of both their prefix and suffix. The present phrasebased setting is simp"
P14-2022,N13-1116,1,0.84069,"sis trie, the depth may be shorter than N − 1 in cases where the language model will provably back off (Li and Khudanpur, 2008). The trie can also be short because the target phrase has fewer than N − 1 words. We currently store this trie data structure directly in the phrase table, though it could also be computed on demand to save memory. Empirically, our phrase table uses less RAM than Moses’s memory-based phrase table. As an optimization, a trie reveals multiple words when there would otherwise be no branching. This allows the search algorithm to make decisions only when needed. Following Heafield et al. (2013), leaves in the trie take the score of the underlying hypothesis or target phrase. Non-leaf nodes take the maximum score of their descendants. Children of a node are sorted by score. lated, and the reordering limit. Second, the decoder searches through these matches to select k high-scoring hypotheses for placement in the stack. We improve this second step. The decoder provides our algorithm with pairs consisting of a hypothesis and a compatible source phrase. Each source phrase translates to multiple target phrases. The task is to grow these hypotheses by appending a target phrase, yielding n"
P14-2022,P08-1025,0,0.0213756,"tribution in this paper is efficiently ignoring coverage when evaluating the language model. In contrast, syntactic machine translation hypotheses correspond to contiguous spans in the source sentence, so in prior work we simply ran the search algorithm in every span. Another improvement upon Heafield et al. (2013) is that we previously made no effort to exploit common words that appear in translation rules, which are analogous to phrases. In this work, we explicitly group target phrases by common prefixes, doing so directly in the phrase table. Coarse-to-fine approaches (Petrov et al., 2008; Zhang and Gildea, 2008) invoke the decoder multiple times with increasingly detailed models, pruning after each pass. The key difference in our work is that, rather than refining models in lock step, we effectively refine the language model on demand for hypotheses that score well. Moreover, their work was performed in syntactic machine translation while we address issues specific to phrase-based translation. Our baseline is cube pruning (Chiang, 2007; Huang and Chiang, 2007), which is both a way to organize search and an algorithm to search through cross products of sets. We adopt the same search organization (Sect"
P14-2022,W11-2123,1,0.835512,"been found. 133 15 -28.0 Uncased BLEU Average model score -27.5 14 -28.5 -29.0 -29.5 13 This Work Moses 0 1 2 3 4 CPU seconds/sentence This Work Moses 0 1 2 3 4 CPU seconds/sentence Figure 4: Performance of our decoder and Moses for various stack sizes k. Moses (Koehn et al., 2007) revision d6df825 was compiled with all optimizations recommended in the documentation. We use the inmemory phrase table for speed. Tests were run on otherwise-idle identical machines with 32 GB RAM; the processes did not come close to running out of memory. The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run. Timing is based on CPU usage (user plus system) minus loading time, as measured by running on empty input; our decoder is also faster at loading. All results are single-threaded. Model score is comparable across decoders and averaged over all 1677 sentences; higher is better. The relationship between model score and uncased BLEU (Papineni et al., 2002) is noisy, so peak BLEU is not attained by the highest search accuracy. Stack 10 100 1000 10000 Model Moses This -29.96 -29.70 -28.68 -28.54 -27.87 -27.8"
P14-2022,P07-1019,0,0.78424,"ding hypotheses that differ only in ways irrelevant to their score, such as coverage of the source sentence. Our decoder bundles hypotheses into equivalence classes so that features can focus on the relevant parts of state. We pay particular attention to the language model because it is responsible for much of the hypothesis state. As the decoder builds translations from left to right (Koehn, 2004), it records the last N − 1 words of each hypothesis so that they can be used as context to score the first N − 1 words of a phrase, where N is the order of the language model. Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes. Our algorithm instead discovers good combinations in a coarse-to-fine manner. The algorithm exploits the fact that hypotheses often share the same suffix and phrases often share the same prefix. These shared suffixes and prefixes allow the algorithm to coarsely reason over many combinations at once. Our primary contribution is a new search algorithm that exploits the above observations, namely that state can be divided into pieces relevant to each feature and that language model state c"
P14-2022,P07-2045,0,0.166231,"are both approximate, improvement can be used to increase speed or accuracy. When tuned to attain the same accuracy, our algorithm is 4.0–7.7 times as fast as the Moses decoder with cube pruning. 1 Introduction Translation speed is critical to making suggestions as translators type, mining for parallel data by translating the web, and running on mobile devices without Internet connectivity. We contribute a fast decoding algorithm for phrase-based machine translation along with an implementation in a new open-source (LGPL) decoder available at http://kheafield.com/code/. Phrase-based decoders (Koehn et al., 2007; Cer et al., 2010; Wuebker et al., 2012) keep track of several types of state with translation hypothe2 Related Work Our previous work (Heafield et al., 2013) developed language model state refinement for bottom130 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 130–135, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics up decoding in syntatic machine translation. In bottom-up decoding, hypotheses can be extended to the left or right, so hypotheses keep track of both their prefix and suffix"
P14-2032,W06-1655,0,0.329618,"Missing"
P14-2032,I08-1033,0,0.0269024,"option. We propose a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference. Our method is simple and easy to implement. Experiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets. 1 Introduction Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing. As demonstrated by (Shi and Wang, 2007; Bai et al., 2008; Chang et al., 2008; Kummerfeld et al., 2013), the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary cl"
P14-2032,W08-0336,1,0.938361,"a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference. Our method is simple and easy to implement. Experiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets. 1 Introduction Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing. As demonstrated by (Shi and Wang, 2007; Bai et al., 2008; Chang et al., 2008; Kummerfeld et al., 2013), the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: cha"
P14-2032,D10-1125,0,0.136384,"Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al., 2010), bilingual sequence tagging (Wang et al., 2013) and word alignment (DeNero and Macherey, 2011). The idea is that jointly modelling both character-sequence and word information can be computationally challenging, so instead we can try to find outputs that the two models are most likely max (α · φ(y)) y∈GEN(x) F (y|x) is the score of segmentation result y. Searching through the entire GEN(x) space is intractable even with a local model, so a beamsearch algorithm is used. The search algorithm consumes one character input token at a time, and iterates through the existing beams to score two new a"
P14-2032,P13-2018,0,0.0209792,"tively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference. Our method is simple and easy to implement. Experiments on SIGHAN 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets. 1 Introduction Chinese text is written without delimiters between words; as a result, Chinese word segmentation (CWS) is an essential foundational step for many tasks in Chinese natural language processing. As demonstrated by (Shi and Wang, 2007; Bai et al., 2008; Chang et al., 2008; Kummerfeld et al., 2013), the quality and consistency of segmentation has important downstream impacts on system performance in machine translation, POS tagging and parsing. State-of-the-art performance in CWS is high, with F-scores in the upper 90s. Still, challenges remain. Unknown words, also known as out-ofvocabulary (OOV) words, lead to difficulties for word- or dictionary-based approaches. Ambiguity can cause errors when the appropriate segmentation is determined contextually, such as 才能 (“talent”) and 才 / 能 (“just able”) (Gao et al., 2003). There are two primary classes of models: character-based, where the fo"
P14-2032,D10-1001,0,0.245719,"el seeks a segmentation y such that: F (y|x) = j∈|x| if yc∗ = yw∗ then return (yc∗ , yw∗ ) end if for all i ∈ {1 to |x|} do ∀k ∈ {0, 1} : ui (k) = ui (k) + αt (2k − 1)(yiw∗ − yic∗ ) end for end for return (yc∗ , yw∗ ) Combining Models with Dual Decomposition Various mixing approaches have been proposed to combine the above two approaches (Wang et al., 2006; Lin, 2009; Sun et al., 2009; Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al., 2010), bilingual sequence tagging (Wang et al., 2013) and word alignment (DeNero and Macherey, 2011). The idea is that jointly modelling both character-sequence and word information can be computationally challenging,"
P14-2032,W03-1719,0,0.0594456,"ut of character-based CRF, yw is the output of word-based perceptron, and the agreements are expressed as constraints. s.t. is a shorthand for “such that”. Solving this constrained optimization problem directly is difficult. Instead, we take the Lagrangian relaxation of this term as: L (yc , yw , U) = P (yc |x) + F (yw |x) + X (2) ui (yic − yiw ) i∈|x| where U is the set of Lagrangian multipliers that consists of a multiplier ui at each word position i. We can rewrite the original objective with the Lagrangian relaxation as: max min L (yc , yw , U) 3 We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. We use the publicly available Stanford CRF segmenter (Tseng et al., 2005)2 as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard tr"
P14-2032,N09-1007,0,0.743104,"ictionary-lookup maximum matching (Chen and Liu, 1992). More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). Formally, given input x, their model seeks a segmentation y such that: F (y|x) = j∈|x| if yc∗ = yw∗ then return (yc∗ , yw∗ ) end if for all i ∈ {1 to |x|} do ∀k ∈ {0, 1} : ui (k) = ui (k) + αt (2k − 1)(yiw∗ − yic∗ ) end for end for return (yc∗ , yw∗ ) Combining Models with Dual Decomposition Various mixing approaches have been proposed to combine the above two approaches (Wang et al., 2006; Lin, 2009; Sun et al., 2009; Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing"
P14-2032,C10-2139,0,0.725894,"se Word Segmentation with Dual Decomposition Mengqiu Wang Rob Voigt Christopher D. Manning Computer Science Department Linguistics Department Computer Science Department Stanford University Stanford University Stanford University Stanford, CA 94305 Stanford, CA 94305 Stanford, CA 94305 {mengqiu,manning}@cs.stanford.edu robvoigt@stanford.edu Abstract processing are individual Chinese characters (Xue, 2003; Tseng et al., 2005; Zhang et al., 2006; Wang et al., 2010), and word-based, where the units are full words based on some dictionary or training lexicon (Andrew, 2006; Zhang and Clark, 2007). Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from combining these two types of models to exploit their respective strengths, but such approaches are often complex to implement and computationally expensive. In this work, we propose a simple and principled j"
P14-2032,I05-3027,1,0.961013,"timore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2.1 Algorithm 1 Dual decomposition inference algorithm, and modified Viterbi and beam-search algorithms. Character-based Models In the most commonly used contemporary approach to character-based segmentation, first proposed by (Xue, 2003), CWS is seen as a character sequence tagging task, where each character is tagged on whether it is at the beginning, middle, or end of a word. Conditional random fields (CRF) (Lafferty et al., 2001) have been widely adopted for this task, and give state-of-the-art results (Tseng et al., 2005). In a first-order linearchain CRF model, the conditional probability of a label sequence y given a word sequence x is defined as: ∀i ∈ {1 to |x|} : ∀k ∈ {0, 1} : ui (k) = 0 for t ← 1 to T do P ui (yic ) yc∗ = argmax P (yc |x) + y i∈|x| P uj (yjw ) yw∗ = argmax F (yw |x) − y∈GEN(x) |y| P (y|x) = 1X exp (θ · f (x, yt , yt+1 )) Z Viterbi: V1 (1) = 1, V1 (0) = 0 for i = 2 to |x |do ∀k ∈ {0, 1} : Vi (k) = argmaxPi (k|k0 )Vi−1 k0 + t=1 ui (k) end for f (x, yt , yt−1 ) are feature functions that typically include surrounding character n-gram and morphological suffix/prefix features. These types of f"
P14-2032,W06-0121,0,0.0325094,"loyed simple heuristics like dictionary-lookup maximum matching (Chen and Liu, 1992). More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). Formally, given input x, their model seeks a segmentation y such that: F (y|x) = j∈|x| if yc∗ = yw∗ then return (yc∗ , yw∗ ) end if for all i ∈ {1 to |x|} do ∀k ∈ {0, 1} : ui (k) = ui (k) + αt (2k − 1)(yiw∗ − yic∗ ) end for end for return (yc∗ , yw∗ ) Combining Models with Dual Decomposition Various mixing approaches have been proposed to combine the above two approaches (Wang et al., 2006; Lin, 2009; Sun et al., 2009; Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for e"
P14-2032,C10-1132,0,0.0373856,"hing (Chen and Liu, 1992). More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). Formally, given input x, their model seeks a segmentation y such that: F (y|x) = j∈|x| if yc∗ = yw∗ then return (yc∗ , yw∗ ) end if for all i ∈ {1 to |x|} do ∀k ∈ {0, 1} : ui (k) = ui (k) + αt (2k − 1)(yiw∗ − yic∗ ) end for end for return (yc∗ , yw∗ ) Combining Models with Dual Decomposition Various mixing approaches have been proposed to combine the above two approaches (Wang et al., 2006; Lin, 2009; Sun et al., 2009; Sun, 2010; Wang et al., 2010). These mixing models perform well on standard datasets, but are not in wide use because of their high computational costs and difficulty of implementation. Dual decomposition (DD) (Rush et al., 2010) offers an attractive framework for combining these two types of models without incurring high costs in model complexity (in contrast to (Sun et al., 2009)) or decoding efficiency (in contrast to bagging in (Wang et al., 2006; Sun, 2010)). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al., 2010), bilingual"
P14-2032,P13-1106,1,0.896868,"Missing"
P14-2032,O03-4002,0,0.603753,"and wordbased models we use as baselines, review existing approaches to combination, and describe our algorithm for joint decoding with dual decomposition. 193 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 193–198, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2.1 Algorithm 1 Dual decomposition inference algorithm, and modified Viterbi and beam-search algorithms. Character-based Models In the most commonly used contemporary approach to character-based segmentation, first proposed by (Xue, 2003), CWS is seen as a character sequence tagging task, where each character is tagged on whether it is at the beginning, middle, or end of a word. Conditional random fields (CRF) (Lafferty et al., 2001) have been widely adopted for this task, and give state-of-the-art results (Tseng et al., 2005). In a first-order linearchain CRF model, the conditional probability of a label sequence y given a word sequence x is defined as: ∀i ∈ {1 to |x|} : ∀k ∈ {0, 1} : ui (k) = 0 for t ← 1 to T do P ui (yic ) yc∗ = argmax P (yc |x) + y i∈|x| P uj (yjw ) yw∗ = argmax F (yw |x) − y∈GEN(x) |y| P (y|x) = 1X exp (θ"
P14-2032,P07-1106,0,0.871118,"t Better Than One: Chinese Word Segmentation with Dual Decomposition Mengqiu Wang Rob Voigt Christopher D. Manning Computer Science Department Linguistics Department Computer Science Department Stanford University Stanford University Stanford University Stanford, CA 94305 Stanford, CA 94305 Stanford, CA 94305 {mengqiu,manning}@cs.stanford.edu robvoigt@stanford.edu Abstract processing are individual Chinese characters (Xue, 2003; Tseng et al., 2005; Zhang et al., 2006; Wang et al., 2010), and word-based, where the units are full words based on some dictionary or training lexicon (Andrew, 2006; Zhang and Clark, 2007). Sun (2010) details their respective theoretical strengths: character-based approaches better model the internal compositional structure of words and are therefore more effective at inducing new OOV words; word-based approaches are better at reproducing the words of the training lexicon and can capture information from significantly larger contextual spans. Prior work has shown performance gains from combining these two types of models to exploit their respective strengths, but such approaches are often complex to implement and computationally expensive. In this work, we propose a simple and"
P14-2032,N06-2049,0,0.122034,"Missing"
P14-2032,W02-1001,0,\N,Missing
P14-2032,W03-1726,0,\N,Missing
P14-2032,P03-1035,0,\N,Missing
P14-2032,C92-1019,0,\N,Missing
P14-2032,I05-3017,0,\N,Missing
P14-2034,W08-0336,1,0.579857,"Missing"
P14-2034,E06-1047,0,0.0267875,"Missing"
P14-2034,P07-1033,0,0.0505741,"B is the newswire ATB; BN is the Broadcast News treebank; ARZ is the Egyptian treebank. Best results (bold) are statistically significant (p < 0.001) relative to the strongest baseline. 2.2 Domain adaptation In this work, we train our model to segment Arabic text drawn from three domains: newswire, which consists of formal text in MSA; broadcast news, which contains scripted, formal MSA as well as extemporaneous dialogue in a mix of MSA and dialect; and discussion forum posts written primarily in Egyptian dialect. The approach to domain adaptation we use is that of feature space augmentation (Daumé, 2007). Each indicator feature from the model described in Section 2.1 is replaced by N + 1 features in the augmented model, where N is the number of domains from which the data is drawn (here, N = 3). These N + 1 features consist of the original feature and N “domain-specific” features, one for each of the N domains, each of which is active only when both the original feature is present and the current text comes from its assigned domain. 3 Experiments We train and evaluate on three corpora: parts 1–3 of the newswire Arabic Treebank (ATB),1 the Broadcast News Arabic Treebank (BN),2 and parts 1–8 of"
P14-2034,P12-1016,1,0.874572,"Missing"
P14-2034,C10-1045,1,0.177847,"Missing"
P14-2034,N06-2013,0,0.0907452,"Missing"
P14-2034,habash-etal-2012-conventional,0,0.0160012,"nd DeNero and our improvements. Using domain adaptation alone helps performance on two of the three datasets (with a statistically insignificant decrease on broadcast news), and our additional features further improve segmentation on all datasets. Table 2 shows the segmentation scores our model achieves when evaluated on the three test sets, as well as the results for MADA and MADA-ARZ. Our segmenter matches or outperforms both MADA and MADA-ARZ on all metrics and datasets except for the TEDEval metric on the Egyptian dataset, which reflects the greater variety of orthographic normalizations (Habash et al., 2012) implemented in MADA-ARZ. In addition, our segmenter is faster than MADA. Table 3 compares the running times of the three systems. Our segmenter achieves a 7x or more speedup over MADA and MADA-ARZ on all datasets. 4 Error Analysis We randomly sampled 100 of the errors made by our final model on the ARZ development set; see Table 4. These errors fall into three general categories: • typographical errors and annotation inconsistencies in the gold data; • errors that can be fixed with a fuller analysis of just the problematic token, and therefore represent a deficiency in the feature set; and •"
P14-2034,N13-1044,0,0.0761177,"Missing"
P14-2034,maamouri-etal-2006-developing,0,0.0607925,"Missing"
P14-2034,P12-2002,0,0.0308717,"Missing"
P14-2034,P06-3009,0,0.0319288,"Missing"
P14-2034,C00-2137,0,0.0554383,"Missing"
P14-2034,N06-1062,0,\N,Missing
P14-5010,P05-1045,1,0.148222,"ides a PTBstyle tokenizer, extended to reasonably handle noisy and web text. The corresponding components for Chinese and Arabic provide word and clitic segmentation. The tokenizer saves the character offsets of each token in the input text. cleanxml Removes most or all XML tags from the document. ssplit Splits a sequence of tokens into sentences. truecase Determines the likely true case of tokens in text (that is, their likely case in well-edited text), where this information was lost, e.g., for all upper case text. This is implemented with a discriminative model using a CRF sequence tagger (Finkel et al., 2005). pos Labels tokens with their part-of-speech (POS) tag, using a maximum entropy POS tagger (Toutanova et al., 2003). lemma Generates the lemmas (base forms) for all tokens in the annotation. gender Adds likely gender information to names. ner Recognizes named (PERSON, LOCATION, ORGANIZATION, MISC) and numerical (MONEY, NUMBER, DATE, TIME, DURATION, SET) entities. With the default Figure 4: A simple, complete example program. annotators in a pipeline is controlled by standard Java properties in a Properties object. The most basic property to specify is what annotators to run, in what order, as"
P14-5010,J13-4004,1,0.147631,"(IDEOLOGY), nationalities (NATIONALITY), religions (RELIGION), and titles (TITLE). parse Provides full syntactic analysis, including both constituent and dependency representation, based on a probabilistic parser (Klein and Manning, 2003; de Marneffe et al., 2006). sentiment Sentiment analysis with a compositional model over trees using deep learning (Socher et al., 2013). Nodes of a binarized tree of each sentence, including, in particular, the root node of each sentence, are given a sentiment score. dcoref Implements mention detection and both pronominal and nominal coreference resolution (Lee et al., 2013). The entire coreference graph of a text (with head words of mentions as nodes) is provided in the Annotation. Figure 5: An example of a simple custom annotator. The annotator marks the words of possibly multi-word locations that are in a gazetteer. props.setProperty(""tokenize.whitespace"", ""true""); props.setProperty(""ssplit.eolonly"", ""true""); or via command-line flags: -tokenize.whitespace -ssplit.eolonly We do not attempt to describe all the properties understood by each annotator here; they are available in the documentation for Stanford CoreNLP. However, we note that they follow the pattern"
P14-5010,D13-1170,1,0.163373,"Annotator is to provide a simple framework to allow a user to incorporate NE labels that are not annotated in traditional NL corpora. For example, a default list of regular expressions that we distribute in the models file recognizes ideologies (IDEOLOGY), nationalities (NATIONALITY), religions (RELIGION), and titles (TITLE). parse Provides full syntactic analysis, including both constituent and dependency representation, based on a probabilistic parser (Klein and Manning, 2003; de Marneffe et al., 2006). sentiment Sentiment analysis with a compositional model over trees using deep learning (Socher et al., 2013). Nodes of a binarized tree of each sentence, including, in particular, the root node of each sentence, are given a sentiment score. dcoref Implements mention detection and both pronominal and nominal coreference resolution (Lee et al., 2013). The entire coreference graph of a text (with head words of mentions as nodes) is provided in the Annotation. Figure 5: An example of a simple custom annotator. The annotator marks the words of possibly multi-word locations that are in a gazetteer. props.setProperty(""tokenize.whitespace"", ""true""); props.setProperty(""ssplit.eolonly"", ""true""); or via comman"
P14-5010,bethard-etal-2014-cleartk,1,0.157881,"Missing"
P14-5010,N03-1033,1,0.145833,"nese and Arabic provide word and clitic segmentation. The tokenizer saves the character offsets of each token in the input text. cleanxml Removes most or all XML tags from the document. ssplit Splits a sequence of tokens into sentences. truecase Determines the likely true case of tokens in text (that is, their likely case in well-edited text), where this information was lost, e.g., for all upper case text. This is implemented with a discriminative model using a CRF sequence tagger (Finkel et al., 2005). pos Labels tokens with their part-of-speech (POS) tag, using a maximum entropy POS tagger (Toutanova et al., 2003). lemma Generates the lemmas (base forms) for all tokens in the annotation. gender Adds likely gender information to names. ner Recognizes named (PERSON, LOCATION, ORGANIZATION, MISC) and numerical (MONEY, NUMBER, DATE, TIME, DURATION, SET) entities. With the default Figure 4: A simple, complete example program. annotators in a pipeline is controlled by standard Java properties in a Properties object. The most basic property to specify is what annotators to run, in what order, as shown here. But as discussed below, most annotators have their own properties to allow further customization of the"
P14-5010,de-marneffe-etal-2006-generating,1,\N,Missing
P14-5010,chang-manning-2012-sutime,1,\N,Missing
P14-5010,clarke-etal-2012-nlp,0,\N,Missing
P14-5010,P02-1022,0,\N,Missing
P15-1006,D14-1217,1,0.803431,"Missing"
P15-1006,C12-1042,0,0.0459437,"Missing"
P15-1006,D13-1197,0,0.0217911,"rom seed description sentences (top). Additional descriptions provided by other participants from the created scene (bottom). Our dataset contains around 19 scenes per seed sentence, for a total of 1129 scenes. Scenes exhibit variation in the specific objects chosen and their placement. Each scene is described by 3 or 4 other people, for a total of 4358 descriptions. 3.1 Text to Scene Systems 3.2 Related Tasks Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More r"
P15-1006,D10-1040,0,0.0330872,"ions serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships. Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated linguisti"
P15-1006,D14-1086,0,0.0275189,"ences (top). Additional descriptions provided by other participants from the created scene (bottom). Our dataset contains around 19 scenes per seed sentence, for a total of 1129 scenes. Scenes exhibit variation in the specific objects chosen and their placement. Each scene is described by 3 or 4 other people, for a total of 4358 descriptions. 3.1 Text to Scene Systems 3.2 Related Tasks Prior work has generated sentences that describe 2D images (Farhadi et al., 2010; Kulkarni et al., 2011; Karpathy et al., 2014) and referring expressions for specific objects in images (FitzGerald et al., 2013; Kazemzadeh et al., 2014). However, generating scenes is currently out of reach for purely image-based approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to"
P15-1006,H89-1033,0,0.0608002,"d Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships. Pioneering work on the SHRDLU system (Winograd, 1972) demonstrated linguistic manipulation of objects in 3D scenes. However, the discourse domain was restricted to a micro-world with simple geometric shapes to simplify parsing and grounding of natural language input. More recently, prototype text to 3D scene generation systems have been built for broader domains, most notably the WordsEye system (Coyne and Sproat, 2001) and later work by Seversky and Yin (2006). Chang et al. (2014) showed it is possible to learn spatial priors for objects and relations directly from 3D scene data. These systems use manually defined mappings between language and"
P15-1006,Q13-1016,0,0.0443334,"approaches. 3D scene representations serve as an intermediate level of structure between raw image pixels and simpler microcosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of challenges associated with natural scenes. A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes (Gorniak and Roy, 2004; Gorniak and Roy, 2005). More recent work grounds text to object attributes such as color and shape in images (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Golland et al. (2010) ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowdworkers. In contrast, we ground general descriptions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are provided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships. Pioneering work on the SHRDLU system (Winograd, 1972)"
P15-1006,P14-5010,1,0.0427666,"Missing"
P15-1006,P02-1040,0,0.0925837,"Section 5.3, we describe how we use our learned model to augment this model. This rule-based approach is a three-stage process using established NLP systems: 1) The input text is split into multiple sentences and parsed using the Stanford CoreNLP pipeline (Manning et Model To create a model for generating scene templates from text, we train a classifier to learn lexical 1 Available at http://nlp.stanford.edu/data/ text2scene.shtml. 2 Mean 26.2 words, SD 17.4; versus mean 16.6, SD 7.2 for the seed sentences. If one considers seed sentences to be the “reference,” the macro-averaged BLEU score (Papineni et al., 2002) of the Turker descriptions is 12.0. 56 red cup round yellow table green room black top tan love seat black bed open window Figure 4: Some examples extracted from the top 20 highest-weight features in our learned model: lexical terms from the descriptions in our scene corpus are grounded to 3D models within the scene corpus. al., 2014). Head words of noun phrases are identified as candidate object categories, filtered using WordNet (Miller, 1995) to only include physical objects. 2) References to the same object are collapsed using the Stanford coreference system. 3) Properties are attached to"
P15-1006,W11-0147,0,0.0266703,"te a concrete 3D scene visualizing the input description (right). The 3D scene is constructed by retrieving and arranging appropriate 3D models. 2 Task Description the structure of environments is rarely mentioned in natural language (e.g., that most tables are supported on the floor and in an upright orientation). Unfortunately, common 3D representations of objects and scenes used in computer graphics specify only geometry and appearance, and rarely include such information. Prior work in text to 3D scene generation focused on collecting manual annotations of object properties and relations (Rouhizadeh et al., 2011; Coyne et al., 2012), which are used to drive rule-based generation systems. Regrettably, the task of scene generation has not yet benefited from recent related work in NLP. In the text to 3D scene generation task, the input is a natural language description, and the output is a 3D representation of a plausible scene that fits the description and can be viewed and rendered from multiple perspectives. More precisely, given an utterance x as input, the output is a scene y: an arrangement of 3D models representing objects at specified positions and orientations in space. In this paper, we focus"
P15-1006,W14-3102,1,\N,Missing
P15-1034,S13-1035,0,0.0129408,"subsective adjectives collected in Nayak et al. (2014), and prohibit their deletion as a hard constraint. The second concern is with prepositional attachment, and direct object edges. For example, nsubj prep backoff Obama signed the bill into law on Friday prep with prep with −−−−−−→ Bob entails Alice is friends. Analodobj gously, Alice played −−→ baseball on Sunday entails that Alice played on Sunday; but, Obama dobj signed −−→ the bill on Sunday should not entail the awkward phrase *Obama signed on Sunday. We learn these attachment affinities empirically from the syntactic n-grams corpus of Goldberg and Orwant (2013). This gives us counts for how often object and preposition edges occur in the context of the governing verb and relevant neighboring edges. We hypothesize that edges which are frequently seen to co-occur are likely to be essential to the meaning of the sentence. To this end, we compute the probability of seeing an arc of a given type, conditioned on the most specific context we have statistics for. These contexts, and the order we back off to more general contexts, is given in Figure 3. To compute a score s of deleting the edge from the affinity probability p collected from the syntactic n-gr"
P15-1034,W13-3515,1,0.292542,"Chen et al., 2010). However, both of these approaches require careful tuning to the task, and need to be trained explicitly on the KBP relation schema. Soderland et al. (2013) submitted a system to KBP making use of open IE relations and an easily constructed mapping to KBP relations; we use this as a baseline for our empirical evaluation. Prior work has used natural logic for RTE-style textual entailment, as a formalism well-suited for formal semantics in neural networks, and as a framework for common-sense reasoning (MacCartney and Manning, 2009; Watanabe et al., 2012; Bowman et al., 2014; Angeli and Manning, 2013). We adopt the precise semantics of Icard and Moss (2014). Our approach of finding short entailments from a longer utterance is similar in spirit to work on textual entailment for information extraction (Romano et al., 2006). We treat the first stage as a greedy search problem: we traverse a dependency parse tree recursively, at each step predicting whether an edge should yield an independent clause. Importantly, in many cases na¨ıvely yielding a clause on a dependency edge produces an incomplete utterance (e.g., Born in Honolulu, Hawaii, from Figure 1). These are often attributable to control"
P15-1034,D14-1164,1,0.63355,"h a known relation). Then, we take this annotation as itself distant supervision for a correct sequence of actions to take: any sequence which recovers the known relation is correct. We use a small subset of the KBP source documents for 2010 (Ji et al., 2010) and 2013 (Surdeanu, 2013) as our distantly supervised corpus. To try to maximize the density of known relations in the training sentences, we take all sentences which have at least one known relation for every 10 tokens in the sentence, resulting in 43 155 sentences. In addition, we incorporate the 23 725 manually annotated examples from Angeli et al. (2014). 3.3 Inference We train a multinomial logistic regression classifier on our noisy training data, using the features in Table 1. The most salient features are the label of the edge being taken, the incoming edge to the parent of the edge being taken, neighboring edges for both the parent and child of the edge, and the part of speech tag of the endpoints of the edge. The dataset is weighted to give 3× weight to examples in the Recurse class, as precision errors in this class are relatively harmless for accuracy, while recall errors are directly harmful to recall. Inference now reduces to a sear"
P15-1034,P11-1062,0,0.0323999,"parsers, Ollie (Mausam et al., 2012) continues in the same spirit but with learned dependency patterns, improving on the earlier WOE system (Wu and Weld, 2010). The Never Ending Language Learning project (Carlson et al., 2010) has a similar aim, iteratively learning more facts from the internet from a seed set of examples. Exemplar (Mesquita et al., 2013) adapts the open IE framework to nary relationships similar to semantic role labeling, but without the expensive machinery. Open IE triples have been used in a number of applications – for example, learning entailment graphs for new triples (Berant et al., 2011), and matrix factorization for unifying open IE and structured relations (Yao et al., 2012; Riedel et al., 2013). In each of these cases, the concise extractions provided by open IE allow for efficient symbolic methods for entailment, such as Markov logic networks or matrix factorization. Prior work on the KBP challenge can be categorized into a number of approaches. The most common of these are distantly supervised relation extractors (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011), and rule based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al"
P15-1034,O97-1002,0,0.0648425,"A collection of relation mappings was constructed by a single annotator in approximately a day,3 and a relation mapping was learned using the procedure described in this section. We map open IE relations to the KBP schema by searching for co-occurring relations in a large distantly-labeled corpus, and marking open IE and Note that in addition to being a measure related to PMI, this captures a notion similar to alignment by agreement (Liang et al., 2006); the formula can be equivalently written as log [p(rk |ro )p(ro |rk )]. It is also functionally the same as the JC WordNet distance measure (Jiang and Conrath, 1997). Some sample type checked relation mappings are given in Table 4. In addition to intuitive mappings (e.g., found in → Org:Founded), we can note some rare, but high precision pairs (e.g., invest fund of → Org:Founded By). We can also see 3 The official submission we compare against claimed two weeks for constructing their manual mapping, although a version of their system constructed in only 3 hours performs nearly as well. 350 System UW Official∗ Ollie† + Nominal Rels∗ Our System - Nominal Rels† + Nominal Rels∗ + Alt. Name + Alt. Name + Website the noise in distant supervision occasionally pe"
P15-1034,W11-1902,0,0.0137403,"s from substrings of an open IE triple argument. For example, from the triple (Smith; was appointed; acting director of Acme Corporation), they extract that Smith is employed by Acme Corporation. We disallow such extractions, passing the burden of finding correct precise extractions to the open IE system itself (see Section 4). For entity linking, the UW submission uses Tom Lin’s entity linker (Lin et al., 2012); our submission uses the Illinois Wikifier (Ratinov et al., 2011) without the relational inference component, for efficiency. For coreference, UW uses the Stanford coreference system (Lee et al., 2011); we employ a variant of the simple coref system described in (Pink et al., 2014). We report our results in Table 5.4 UW Official refers to the official submission in the 2013 challenge; we show a 3.1 F1 improvement (to 22.7 P 69.8 57.4 57.7 R 11.4 4.8 11.8 F1 19.6 8.9 19.6 64.3 61.9 57.8 58.6 8.6 13.9 17.8 18.6 15.2 22.7 27.1 28.3 Table 5: A summary of our results on the endto-end KBP Slot Filling task. UW official is the submission made to the 2013 challenge. The second row is the accuracy of Ollie embedded in our framework, and of Ollie evaluated with nominal relations from our system. Last"
P15-1034,N06-1014,0,0.037287,"a. This can either be done manually with minimal annotation effort, or automatically from available training data. We use both methods in our TAC-KBP evaluation. A collection of relation mappings was constructed by a single annotator in approximately a day,3 and a relation mapping was learned using the procedure described in this section. We map open IE relations to the KBP schema by searching for co-occurring relations in a large distantly-labeled corpus, and marking open IE and Note that in addition to being a measure related to PMI, this captures a notion similar to alignment by agreement (Liang et al., 2006); the formula can be equivalently written as log [p(rk |ro )p(ro |rk )]. It is also functionally the same as the JC WordNet distance measure (Jiang and Conrath, 1997). Some sample type checked relation mappings are given in Table 4. In addition to intuitive mappings (e.g., found in → Org:Founded), we can note some rare, but high precision pairs (e.g., invest fund of → Org:Founded By). We can also see 3 The official submission we compare against claimed two weeks for constructing their manual mapping, although a version of their system constructed in only 3 hours performs nearly as well. 350 Sy"
P15-1034,D12-1082,0,0.01126,"stem, Open IE v4.0 employs a semantic role component extracting structured SRL frames, alongside a conventional open IE system. Furthermore, the UW submission allows for extracting relations and entities from substrings of an open IE triple argument. For example, from the triple (Smith; was appointed; acting director of Acme Corporation), they extract that Smith is employed by Acme Corporation. We disallow such extractions, passing the burden of finding correct precise extractions to the open IE system itself (see Section 4). For entity linking, the UW submission uses Tom Lin’s entity linker (Lin et al., 2012); our submission uses the Illinois Wikifier (Ratinov et al., 2011) without the relational inference component, for efficiency. For coreference, UW uses the Stanford coreference system (Lee et al., 2011); we employ a variant of the simple coref system described in (Pink et al., 2014). We report our results in Table 5.4 UW Official refers to the official submission in the 2013 challenge; we show a 3.1 F1 improvement (to 22.7 P 69.8 57.4 57.7 R 11.4 4.8 11.8 F1 19.6 8.9 19.6 64.3 61.9 57.8 58.6 8.6 13.9 17.8 18.6 15.2 22.7 27.1 28.3 Table 5: A summary of our results on the endto-end KBP Slot Fill"
P15-1034,W09-3714,1,0.179472,"., 2011), and rule based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010). However, both of these approaches require careful tuning to the task, and need to be trained explicitly on the KBP relation schema. Soderland et al. (2013) submitted a system to KBP making use of open IE relations and an easily constructed mapping to KBP relations; we use this as a baseline for our empirical evaluation. Prior work has used natural logic for RTE-style textual entailment, as a formalism well-suited for formal semantics in neural networks, and as a framework for common-sense reasoning (MacCartney and Manning, 2009; Watanabe et al., 2012; Bowman et al., 2014; Angeli and Manning, 2013). We adopt the precise semantics of Icard and Moss (2014). Our approach of finding short entailments from a longer utterance is similar in spirit to work on textual entailment for information extraction (Romano et al., 2006). We treat the first stage as a greedy search problem: we traverse a dependency parse tree recursively, at each step predicting whether an edge should yield an independent clause. Importantly, in many cases na¨ıvely yielding a clause on a dependency edge produces an incomplete utterance (e.g., Born in Ho"
P15-1034,D12-1048,0,0.647331,"Introduction Open information extraction (open IE) has been shown to be useful in a number of NLP tasks, such as question answering (Fader et al., 2014), relation extraction (Soderland et al., 2010), and information retrieval (Etzioni, 2011). Conventionally, open IE systems search a collection of patterns over either the surface form or dependency tree of a sentence. Although a small set of patterns covers most simple sentences (e.g., subject verb object constructions), relevant relations are often spread across clauses (see Figure 1) or presented in a non-canonical form. Systems like Ollie (Mausam et al., 2012) approach this problem by using a bootstrapping method to create a large corpus of broad-coverage partially lexicalized patterns. Although this is effective at capturing many of these patterns, it can lead to unintuitive behavior on out-of-domain text. For instance, while Obama is president is extracted correctly by Ollie as (Obama; is; president), replacing is with are in cats are felines produces no extractions. Furthermore, existing systems struggle at producing canonical argument forms – for example, in Figure 1 the argument Heinz Fischer of Austria is likely less useful for downstream app"
P15-1034,D13-1043,0,0.0230946,"is a large body of work on open information extraction. One line of work begins with TextRunner (Yates et al., 2007) and ReVerb (Fader et al., 2011), which make use of computationally efficient surface patterns over tokens. With the introduction of fast dependency parsers, Ollie (Mausam et al., 2012) continues in the same spirit but with learned dependency patterns, improving on the earlier WOE system (Wu and Weld, 2010). The Never Ending Language Learning project (Carlson et al., 2010) has a similar aim, iteratively learning more facts from the internet from a seed set of examples. Exemplar (Mesquita et al., 2013) adapts the open IE framework to nary relationships similar to semantic role labeling, but without the expensive machinery. Open IE triples have been used in a number of applications – for example, learning entailment graphs for new triples (Berant et al., 2011), and matrix factorization for unifying open IE and structured relations (Yao et al., 2012; Riedel et al., 2013). In each of these cases, the concise extractions provided by open IE allow for efficient symbolic methods for entailment, such as Markov logic networks or matrix factorization. Prior work on the KBP challenge can be categoriz"
P15-1034,W08-1301,1,0.155052,"Missing"
P15-1034,N13-1095,0,0.00517185,"ments as the known relation. For instance, if we know that Obama was born in Hawaii from the sentence Born in Hawaii, Obama . . . , and an action sequence produces the triple (Obama, born in, Hawaii), then we take that action sequence as a positive sequence. Any sequence of actions which results in a clause which produces no relations is in turn considered a negative sequence. The third case to consider is a sequence of actions which produces a relation, but it is not one of the annotated relations. This arises from the incomplete negatives problem in distantly supervised relation extraction (Min et al., 2013): since our knowledge base is not exhaustive, we cannot be sure if an extracted relation is incorrect or correct but previously unknown. Although many of these unmatched relations are indeed incorrect, the dataset is sufficiently biased towards the STOP action that the occasional false negative hurts end-to-end performance. Therefore, we simply discard such sequences. Given a set of noisy positive and negative sequences, we construct training data for our action classifier. All but the last action in a positive sequence are added to the training set with the label Recurse; the last action is a"
P15-1034,P09-1113,0,0.213867,"n used in a number of applications – for example, learning entailment graphs for new triples (Berant et al., 2011), and matrix factorization for unifying open IE and structured relations (Yao et al., 2012; Riedel et al., 2013). In each of these cases, the concise extractions provided by open IE allow for efficient symbolic methods for entailment, such as Markov logic networks or matrix factorization. Prior work on the KBP challenge can be categorized into a number of approaches. The most common of these are distantly supervised relation extractors (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011), and rule based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010). However, both of these approaches require careful tuning to the task, and need to be trained explicitly on the KBP relation schema. Soderland et al. (2013) submitted a system to KBP making use of open IE relations and an easily constructed mapping to KBP relations; we use this as a baseline for our empirical evaluation. Prior work has used natural logic for RTE-style textual entailment, as a formalism well-suited for formal semantics in neural networks, and as a framework for common-sense"
P15-1034,D11-1142,0,0.0613524,"by the original sentence, and (2) easy to segment into open IE triples. Our approach consists of two stages: we first learn a classifier for splitting a sentence into shorter utterances (Section 3), and then appeal to natural logic (S´anchez Valencia, 1991) to maximally shorten these utterances while maintaining necessary context (Section 4.1). A small set of 14 hand-crafted patterns can then be used to segment an utterance into an open IE triple. Related Work There is a large body of work on open information extraction. One line of work begins with TextRunner (Yates et al., 2007) and ReVerb (Fader et al., 2011), which make use of computationally efficient surface patterns over tokens. With the introduction of fast dependency parsers, Ollie (Mausam et al., 2012) continues in the same spirit but with learned dependency patterns, improving on the earlier WOE system (Wu and Weld, 2010). The Never Ending Language Learning project (Carlson et al., 2010) has a similar aim, iteratively learning more facts from the internet from a seed set of examples. Exemplar (Mesquita et al., 2013) adapts the open IE framework to nary relationships similar to semantic role labeling, but without the expensive machinery. Op"
P15-1034,D14-1089,0,0.0141531,"mith; was appointed; acting director of Acme Corporation), they extract that Smith is employed by Acme Corporation. We disallow such extractions, passing the burden of finding correct precise extractions to the open IE system itself (see Section 4). For entity linking, the UW submission uses Tom Lin’s entity linker (Lin et al., 2012); our submission uses the Illinois Wikifier (Ratinov et al., 2011) without the relational inference component, for efficiency. For coreference, UW uses the Stanford coreference system (Lee et al., 2011); we employ a variant of the simple coref system described in (Pink et al., 2014). We report our results in Table 5.4 UW Official refers to the official submission in the 2013 challenge; we show a 3.1 F1 improvement (to 22.7 P 69.8 57.4 57.7 R 11.4 4.8 11.8 F1 19.6 8.9 19.6 64.3 61.9 57.8 58.6 8.6 13.9 17.8 18.6 15.2 22.7 27.1 28.3 Table 5: A summary of our results on the endto-end KBP Slot Filling task. UW official is the submission made to the 2013 challenge. The second row is the accuracy of Ollie embedded in our framework, and of Ollie evaluated with nominal relations from our system. Lastly, we report our system, our system with nominal relations removed, and our syst"
P15-1034,N07-4013,0,0.009719,"tational Linguistics 2 entailed by the original sentence, and (2) easy to segment into open IE triples. Our approach consists of two stages: we first learn a classifier for splitting a sentence into shorter utterances (Section 3), and then appeal to natural logic (S´anchez Valencia, 1991) to maximally shorten these utterances while maintaining necessary context (Section 4.1). A small set of 14 hand-crafted patterns can then be used to segment an utterance into an open IE triple. Related Work There is a large body of work on open information extraction. One line of work begins with TextRunner (Yates et al., 2007) and ReVerb (Fader et al., 2011), which make use of computationally efficient surface patterns over tokens. With the introduction of fast dependency parsers, Ollie (Mausam et al., 2012) continues in the same spirit but with learned dependency patterns, improving on the earlier WOE system (Wu and Weld, 2010). The Never Ending Language Learning project (Carlson et al., 2010) has a similar aim, iteratively learning more facts from the internet from a seed set of examples. Exemplar (Mesquita et al., 2013) adapts the open IE framework to nary relationships similar to semantic role labeling, but wit"
P15-1034,P11-1138,0,0.00648634,"g structured SRL frames, alongside a conventional open IE system. Furthermore, the UW submission allows for extracting relations and entities from substrings of an open IE triple argument. For example, from the triple (Smith; was appointed; acting director of Acme Corporation), they extract that Smith is employed by Acme Corporation. We disallow such extractions, passing the burden of finding correct precise extractions to the open IE system itself (see Section 4). For entity linking, the UW submission uses Tom Lin’s entity linker (Lin et al., 2012); our submission uses the Illinois Wikifier (Ratinov et al., 2011) without the relational inference component, for efficiency. For coreference, UW uses the Stanford coreference system (Lee et al., 2011); we employ a variant of the simple coref system described in (Pink et al., 2014). We report our results in Table 5.4 UW Official refers to the official submission in the 2013 challenge; we show a 3.1 F1 improvement (to 22.7 P 69.8 57.4 57.7 R 11.4 4.8 11.8 F1 19.6 8.9 19.6 64.3 61.9 57.8 58.6 8.6 13.9 17.8 18.6 15.2 22.7 27.1 28.3 Table 5: A summary of our results on the endto-end KBP Slot Filling task. UW official is the submission made to the 2013 challenge"
P15-1034,N13-1008,0,0.0435974,"ng on the earlier WOE system (Wu and Weld, 2010). The Never Ending Language Learning project (Carlson et al., 2010) has a similar aim, iteratively learning more facts from the internet from a seed set of examples. Exemplar (Mesquita et al., 2013) adapts the open IE framework to nary relationships similar to semantic role labeling, but without the expensive machinery. Open IE triples have been used in a number of applications – for example, learning entailment graphs for new triples (Berant et al., 2011), and matrix factorization for unifying open IE and structured relations (Yao et al., 2012; Riedel et al., 2013). In each of these cases, the concise extractions provided by open IE allow for efficient symbolic methods for entailment, such as Markov logic networks or matrix factorization. Prior work on the KBP challenge can be categorized into a number of approaches. The most common of these are distantly supervised relation extractors (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011), and rule based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010). However, both of these approaches require careful tuning to the task, and need to be trained explicitl"
P15-1034,E06-1052,0,0.00645285,"tions and an easily constructed mapping to KBP relations; we use this as a baseline for our empirical evaluation. Prior work has used natural logic for RTE-style textual entailment, as a formalism well-suited for formal semantics in neural networks, and as a framework for common-sense reasoning (MacCartney and Manning, 2009; Watanabe et al., 2012; Bowman et al., 2014; Angeli and Manning, 2013). We adopt the precise semantics of Icard and Moss (2014). Our approach of finding short entailments from a longer utterance is similar in spirit to work on textual entailment for information extraction (Romano et al., 2006). We treat the first stage as a greedy search problem: we traverse a dependency parse tree recursively, at each step predicting whether an edge should yield an independent clause. Importantly, in many cases na¨ıvely yielding a clause on a dependency edge produces an incomplete utterance (e.g., Born in Honolulu, Hawaii, from Figure 1). These are often attributable to control relationships, where either the subject or object of the governing clause controls the subject of the subordinate clause. We therefore allow the produced clause to sometimes inherit the subject or object of its governor. Th"
P15-1034,C12-1171,0,0.0126293,"ms (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010). However, both of these approaches require careful tuning to the task, and need to be trained explicitly on the KBP relation schema. Soderland et al. (2013) submitted a system to KBP making use of open IE relations and an easily constructed mapping to KBP relations; we use this as a baseline for our empirical evaluation. Prior work has used natural logic for RTE-style textual entailment, as a formalism well-suited for formal semantics in neural networks, and as a framework for common-sense reasoning (MacCartney and Manning, 2009; Watanabe et al., 2012; Bowman et al., 2014; Angeli and Manning, 2013). We adopt the precise semantics of Icard and Moss (2014). Our approach of finding short entailments from a longer utterance is similar in spirit to work on textual entailment for information extraction (Romano et al., 2006). We treat the first stage as a greedy search problem: we traverse a dependency parse tree recursively, at each step predicting whether an edge should yield an independent clause. Importantly, in many cases na¨ıvely yielding a clause on a dependency edge produces an incomplete utterance (e.g., Born in Honolulu, Hawaii, from Fi"
P15-1034,P10-1013,0,0.0182024,"n these utterances while maintaining necessary context (Section 4.1). A small set of 14 hand-crafted patterns can then be used to segment an utterance into an open IE triple. Related Work There is a large body of work on open information extraction. One line of work begins with TextRunner (Yates et al., 2007) and ReVerb (Fader et al., 2011), which make use of computationally efficient surface patterns over tokens. With the introduction of fast dependency parsers, Ollie (Mausam et al., 2012) continues in the same spirit but with learned dependency patterns, improving on the earlier WOE system (Wu and Weld, 2010). The Never Ending Language Learning project (Carlson et al., 2010) has a similar aim, iteratively learning more facts from the internet from a seed set of examples. Exemplar (Mesquita et al., 2013) adapts the open IE framework to nary relationships similar to semantic role labeling, but without the expensive machinery. Open IE triples have been used in a number of applications – for example, learning entailment graphs for new triples (Berant et al., 2011), and matrix factorization for unifying open IE and structured relations (Yao et al., 2012; Riedel et al., 2013). In each of these cases, th"
P15-1034,W12-3022,0,0.0143381,"patterns, improving on the earlier WOE system (Wu and Weld, 2010). The Never Ending Language Learning project (Carlson et al., 2010) has a similar aim, iteratively learning more facts from the internet from a seed set of examples. Exemplar (Mesquita et al., 2013) adapts the open IE framework to nary relationships similar to semantic role labeling, but without the expensive machinery. Open IE triples have been used in a number of applications – for example, learning entailment graphs for new triples (Berant et al., 2011), and matrix factorization for unifying open IE and structured relations (Yao et al., 2012; Riedel et al., 2013). In each of these cases, the concise extractions provided by open IE allow for efficient symbolic methods for entailment, such as Markov logic networks or matrix factorization. Prior work on the KBP challenge can be categorized into a number of approaches. The most common of these are distantly supervised relation extractors (Craven and Kumlien, 1999; Wu and Weld, 2007; Mintz et al., 2009; Sun et al., 2011), and rule based systems (Soderland, 1997; Grishman and Min, 2010; Chen et al., 2010). However, both of these approaches require careful tuning to the task, and need t"
P15-1034,D14-1059,1,\N,Missing
P15-1095,P13-2131,0,0.240881,"essions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). 7.1 End-to-end AMR Parsing We evaluate our NER++ component in the context of end-to-end AMR parsing on two corpora: the newswire section of LDC2014T12 and the split given in Flanigan et al. (2014) of LDC2013E117, both consisting primarily of newswire. We compare two systems: the JAMR parser (Flanigan et al., 2014),2 and the JAMR SRL++ component with our NER++ approach. AMR parsing accuracy is measured with a metric called smatch (Cai and Knight, 2013), which stands for “s(emantic) match.” The metric is the F1 of a best-match between triples implied by the target graph, and triples in the parsed graph – that is, the set of (parent, edge, child) triples in the graph. Our results are given in Table 3. We report much higher recall numbers on both datasets, with only small (≤ 1 point) loss in precision. This is natural considering our approach. A better NER++ system allows for more correct AMR subgraphs to be generated – improving recall – but does not in itself necessarily improve the accuracy of the SRL++ system it is integrated in. More gene"
P15-1095,W13-2322,0,0.241107,"y perform SRL++ over given subgraphs it scores 80 F1 – nearly the inter-annotator agreement of 83 F1 , and far higher than its end to end accuracy of 59 F1 . SRL++ within AMR is relatively easy given a perfect NER++ output, because so much pressure is put on the output of NER++ to carry meaningful information. For example, there’s a strong typecheck feature for the existence and type of any arc just by looking at its end-points, and syntactic dependency features are very informative for removing any remaining ambiguity. If a system is conIntroduction The Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rich, graph-based language for expressing semantics over a broad domain. The formalism is backed by a large datalabeling effort, and it holds promise for enabling a new breed of natural language applications ranging from semantically aware MT to rich broaddomain QA over text-based knowledge bases. Figure 1 shows an example AMR for “he gleefully ran to his dog Rover,” and we give a brief introduction to AMR in Section 2. This paper focuses on AMR parsing, the task of mapping a natural language sentence into an AMR graph. We follow previous work (Flanigan et al., 2014) in dividing AMR pars"
P15-1095,D14-1159,1,0.823924,"all (≤ 1 point) loss in precision. This is natural considering our approach. A better NER++ system allows for more correct AMR subgraphs to be generated – improving recall – but does not in itself necessarily improve the accuracy of the SRL++ system it is integrated in. More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. 7.2 Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR language from the surface form of the text being parsed; although, AMR has a self-admitted English bias. 7 System JAMR Our System JAMR Our System Component Accuracy We"
P15-1095,W06-2920,0,0.0157852,"ecall is consistently higher across both datasets, with only a small loss in precision. In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). 7.1 End-to-end AMR Parsing We evaluate our NER++ component in the context of end-to-end AMR parsing on two corpora: the newswire section of LDC2014T12 and the split given in Flanigan et al. (2014) of LDC2013E117, both consisting primarily of newswire. We compare two systems: the JAMR parser (Flanigan et al., 2014),2 and the JAMR SRL++ component with our NER++ approach. AMR parsing accuracy is measured with a metric called smatch (Cai and Knight, 2013), which stands for “s(emantic) match.” The metric is the F1 of a best-match between triples implied by the target graph, and triples in the par"
P15-1095,J14-1002,0,0.0151142,"ic tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). 2014T12 2013E117 P 67.1 66.6 66.9 65.9 R 53.2 58.3 52.9 59.0 F1 59.3 62.2 59.1 62.3 Table 3: Results on two AMR datasets for JAMR and our NER++ embedded in the JAMR SRL++ component. Note that recall is consistently higher across both datasets, with only a small loss in precision. In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). 7.1 End-to-end AMR Parsing We evaluate our NER++ component in the context of end-to-end AMR parsing on two corpora: the newswire section of LDC2014T12 and the split given in Flanigan et al. (2014) of LDC2013E117, both consistin"
P15-1095,C04-1197,0,0.0408158,"ation, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). 2014T12 2013E117 P 67.1 66.6 66.9 65.9 R 53.2 58.3 52.9 59.0 F1 59.3 62.2 59.1 62.3 Table 3: Results on two AMR datasets for JAMR and our NER++ embedded in the JAMR SRL++ component. Note that recall is consistently higher across both datasets, with only a small loss in precision. In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). 7.1 End-to-end AMR Parsing We evaluate our NER++ component in the context of end-to-end AMR parsing on two corpora: the newswire section of LDC2014T12 and the split given in Flanigan et al. (2"
P15-1095,P05-1045,1,0.0482946,"here, although we leave this to future work. Beyond the connection of our work with Flanii,j X Qi,j = 1 ∀i (2) j Qk,j + Ql,j ≤ 1 Related Work ∀k, l, j; nk = nl (3) where E is the Jaro-Winkler similarity between the title of the node i and the token j, α is a hyperparameter (set to 0.8 in our experiments), and the operator = denotes that two nodes in the AMR graph are both not adjacent and do not have the same title. 988 Dataset gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). 2014T12 2013E117 P 67.1 66.6 66.9 65.9 R 53.2 58.3 52.9 59.0 F1 59.3 62.2 59.1 62.3 Table 3: Results on two AMR datasets for JAMR and our NER++ embedded in the JAMR SRL++ component. Note that recall is consistently higher across both datasets, with only a small loss in precision. In turn, the SRL++ facet"
P15-1095,P14-1134,0,0.179105,"e LDC2013E117 and LDC2014T12 datasets. 1 Figure 1: The AMR graph for He gleefully ran to his dog Rover. We show that improving the generation of low level subgraphs (e.g., Rover gener:op1 ating name −−→ “Rover”) significantly improves end-to-end performance. identification, which adds arcs to link these nodes into a fully connected AMR graph, which we’ll call SRL++ (Section 3.2). We observe that SRL++ is not the hard part of AMR parsing; rather, much of the difficulty in AMR is generating high accuracy concept subgraphs from the NER++ component. For example, when the existing AMR parser JAMR (Flanigan et al., 2014) is given a gold NER++ output, and must only perform SRL++ over given subgraphs it scores 80 F1 – nearly the inter-annotator agreement of 83 F1 , and far higher than its end to end accuracy of 59 F1 . SRL++ within AMR is relatively easy given a perfect NER++ output, because so much pressure is put on the output of NER++ to carry meaningful information. For example, there’s a strong typecheck feature for the existence and type of any arc just by looking at its end-points, and syntactic dependency features are very informative for removing any remaining ambiguity. If a system is conIntroduction"
P15-1095,J02-3001,0,0.0128492,"d Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). 2014T12 2013E117 P 67.1 66.6 66.9 65.9 R 53.2 58.3 52.9 59.0 F1 59.3 62.2 59.1 62.3 Table 3: Results on two AMR datasets for JAMR and our NER++ embedded in the JAMR SRL++ component. Note that recall is consistently higher across both datasets, with only a small loss in precision. In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). 7.1 End-to-end AMR Parsing We evaluate our NER++ component in the context of end-to-end AMR parsing on two corpora: the newswire section of LDC2014T12 and the split gi"
P15-1095,S10-1071,0,0.115176,"Missing"
P15-1095,D14-1058,0,0.0195808,"ER++ system allows for more correct AMR subgraphs to be generated – improving recall – but does not in itself necessarily improve the accuracy of the SRL++ system it is integrated in. More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. 7.2 Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR language from the surface form of the text being parsed; although, AMR has a self-admitted English bias. 7 System JAMR Our System JAMR Our System Component Accuracy We evaluate our aligner on a small set of 100 handlabeled alignments, and evaluate our NER++"
P15-1095,C12-1083,0,0.101618,"Missing"
P15-1095,N15-1040,0,0.145669,"ine Translation, proposed a graphical semantic meaning representation that predates AMR, but is intimately related. They propose a hyper-edge replacement grammar (HRG) approach to parsing into and out of this graphical semantic form. Flanigan et al. (2014) forms the basis of the approach of this paper. Their system introduces the two-stage approach we use: they implement a rule-based alignment to learn a mapping from tokens to subgraphs, and train a variant of a maximum spanning tree parser adapted to graphs and with additional constraints for their relation identifications (SRL++) component. Wang et al. (2015) uses a transition based algorithm to transform dependency trees into AMR parses. They achieve 64/62/63 P/R/F1 with contributions roughly orthogonal to our own. Their transformation action set could be easily augmented by the robust subgraph generation we propose here, although we leave this to future work. Beyond the connection of our work with Flanii,j X Qi,j = 1 ∀i (2) j Qk,j + Ql,j ≤ 1 Related Work ∀k, l, j; nk = nl (3) where E is the Jaro-Winkler similarity between the title of the node i and the token j, α is a hyperparameter (set to 0.8 in our experiments), and the operator = denotes th"
P15-1095,P95-1026,0,0.0173419,"nd the connection of our work with Flanii,j X Qi,j = 1 ∀i (2) j Qk,j + Ql,j ≤ 1 Related Work ∀k, l, j; nk = nl (3) where E is the Jaro-Winkler similarity between the title of the node i and the token j, α is a hyperparameter (set to 0.8 in our experiments), and the operator = denotes that two nodes in the AMR graph are both not adjacent and do not have the same title. 988 Dataset gan et al. (2014), we note that the NER++ component of AMR encapsulates a number of lexical NLP tasks. These include named entity recognition (Nadeau and Sekine, 2007; Finkel et al., 2005), word sense disambiguation (Yarowsky, 1995; Banerjee and Pedersen, 2002), lemmatization, and a number of more domain specific tasks. For example, a full understanding of AMR requires normalizing temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010; Chang and Manning, 2012). 2014T12 2013E117 P 67.1 66.6 66.9 65.9 R 53.2 58.3 52.9 59.0 F1 59.3 62.2 59.1 62.3 Table 3: Results on two AMR datasets for JAMR and our NER++ embedded in the JAMR SRL++ component. Note that recall is consistently higher across both datasets, with only a small loss in precision. In turn, the SRL++ facet of AMR takes many insights from semantic ro"
P15-1095,P11-1060,0,0.0483284,"of (parent, edge, child) triples in the graph. Our results are given in Table 3. We report much higher recall numbers on both datasets, with only small (≤ 1 point) loss in precision. This is natural considering our approach. A better NER++ system allows for more correct AMR subgraphs to be generated – improving recall – but does not in itself necessarily improve the accuracy of the SRL++ system it is integrated in. More generally, parsing to a semantic representation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. 7.2 Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR language from the"
P15-1095,P05-1012,0,0.0306431,"component. Note that recall is consistently higher across both datasets, with only a small loss in precision. In turn, the SRL++ facet of AMR takes many insights from semantic role labeling (Gildea and Jurafsky, 2002; Punyakanok et al., 2004; Srikumar, 2013; Das et al., 2014) to capture the relations between verbs and their arguments. In addition, many of the arcs in AMR have nearly syntactic interpretations (e.g., mod for adjective/adverb modification, op for compound noun expressions). These are similar to representations used in syntactic dependency parsing (de Marneffe and Manning, 2008; McDonald et al., 2005; Buchholz and Marsi, 2006). 7.1 End-to-end AMR Parsing We evaluate our NER++ component in the context of end-to-end AMR parsing on two corpora: the newswire section of LDC2014T12 and the split given in Flanigan et al. (2014) of LDC2013E117, both consisting primarily of newswire. We compare two systems: the JAMR parser (Flanigan et al., 2014),2 and the JAMR SRL++ component with our NER++ approach. AMR parsing accuracy is measured with a metric called smatch (Cai and Knight, 2013), which stands for “s(emantic) match.” The metric is the F1 of a best-match between triples implied by the target gr"
P15-1095,1991.mtsummit-papers.9,0,0.478746,"ntation is has been explored in depth for when the representation is a logical form (Kate et al., 2005; Zettlemoyer and Collins, 2005; Liang et al., 2011). Recent work has applied semantic parsing techniques to representations beyond lambda calculus expressions. For example, work by Berant et al. (2014) parses text into a formal representation of a biological process. Hosseini et al. (2014) solves algebraic word problems by parsing them into a structured meaning representation. In contrast to these approaches, AMR attempts to capture open domain semantics over arbitrary text. 7.2 Interlingua (Mitamura et al., 1991; Carbonell et al., 1999; Levin et al., 1998) are an important inspiration for decoupling the semantics of the AMR language from the surface form of the text being parsed; although, AMR has a self-admitted English bias. 7 System JAMR Our System JAMR Our System Component Accuracy We evaluate our aligner on a small set of 100 handlabeled alignments, and evaluate our NER++ classifier on automatically generated alignments over the whole corpus, On a hand-annotated dataset of 100 AMR parses from the LDC2014T12 corpus,3 our aligner achieves an accuracy of 83.2. This is a measurement of the percentag"
P15-1095,J05-1004,0,0.0304768,"for breaking the word up into a self-contained multi-node unit unpacking the derivational morphology of the word. a semantics more akin to syntactic dependencies (e.g., mod standing in for adjective and adverbial modification), or take on domain-specific meaning (e.g., the month, day, and year arcs of a dateentity). To introduce AMR and its notation in more detail, we’ll unpack the translation of the sentence “he gleefully ran to his dog Rover.” We show in Figure 1 the interpretation of this sentence as an AMR graph. The root node of the graph is labeled run-01, corresponding to the PropBank (Palmer et al., 2005) definition of the verb ran. run-01 has an outgoing ARG0 arc to a node he, with the usual PropBank semantics. The outgoing mod edge from run-01 to glee takes a general purpose semantics corresponding to adjective, adverbial, or other modification of the governor by the dependent. We note that run-01 has a destination arc to dog. The label for destination is taken from a finite set of special arc sense tags similar to the preposition senses found in (Srikumar, 2013). The last portion of the figure parses dog to a node which serves as a type marker similar to named entity types, and Rover into t"
P15-1095,D14-1048,0,0.144022,"The Jaro-Winkler similarity term, in turn, serves as a tie-breaker between equally (un)reliable alignments. There are many packages which can solve this Boolean LP efficiently. We used Gurobi (Gurobi Optimization, 2015). Given a matrix Q that maximizes our objective, we can decode our solved alignment as follows: for each i, align ni to the j s.t. Qi,j = 1. By our constraints, exactly one such j must exist. published as a component of JAMR, and used a rule-based approach to perform alignments. This was shown to work well on the sample of 100 hand-labeled sentences used to develop the system. Pourdamghani et al. (2014) approached the alignment problem in the framework of the IBM alignment models. They rendered AMR graphs as text, and then used traditional machine translation alignment techniques to generate an alignment. We propose a novel alignment method, since our decomposition of the AMR node generation process into a set of actions provides an additional objective for the aligner to optimize, in addition to the accuracy of the alignment itself. We would like to produce the most reliable sequence of actions for the NER++ model to train from, where reliable is taken in the sense defined in Section 4.2. T"
P15-1095,W08-1301,1,\N,Missing
P15-1095,chang-manning-2012-sutime,1,\N,Missing
P15-1095,N13-4004,0,\N,Missing
P15-1136,N04-1038,0,0.0861012,"009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pair"
P15-1136,D08-1031,0,0.173509,"ce the system works in, allowing for efficient training with an exact loss function. We evaluate our system on the English portion of the 2012 CoNLL Shared Task dataset and show that it improves over the current state of the art. 1 Introduction Coreference resolution, the task of identifying mentions in a text that refer to the same real world entity, is an important aspect of text understanding and has numerous applications. Many approaches to coreference resolution learn a scoring function defined over mention pairs to guide the coreference decisions (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). However, such systems do not make use of entity-level information, i.e., features between clusters of mentions instead of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially comple"
P15-1136,W12-4503,0,0.0145586,"Missing"
P15-1136,P14-1005,0,0.462503,"Missing"
P15-1136,D13-1057,0,0.329762,"ncorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions (Raghunathan et al., 2010; Stoyanov and Eisner, 2012; Ma et al., 2014). However, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013). In this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture different linguistic phenomena in coreference resolution. We then describe how the probabilities produced by these models can be used to generate expressive features between clusters of mentions. Using these features, we train an entity-centric incremental coreference system. The entity-centric system builds up coreference chains with agglomerative clustering: each mention starts in its"
P15-1136,H05-1013,0,0.27589,"Missing"
P15-1136,N07-1030,0,0.0127685,"ement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and second highest MUC F1 scores among the other systems. 5 Related Work Both mention pair (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer r"
P15-1136,D13-1203,0,0.850343,"ch information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions (Raghunathan et al., 2010; Stoyanov and Eisner, 2012; Ma et al., 2014). However, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013). In this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture different linguistic phenomena in coreference resolution. We then describe how the probabilities produced by these models can be used to generate expressive features between clusters of mentions. Using these features, we train an entity-centric incremental coreference system. The entity-centric system builds up coreference chains with agglomerative clustering: each m"
P15-1136,Q14-1037,0,0.103735,"e additional features had no substantial impact on scores, suggesting that features derived from pairwise scores are sufficient for capturing this kind of entity-level information. A disagreement between clusters necessarily means there will be disagreements between some of the involved mentions, so features like the average and minimum probability between mention pairs will have lower values when a disagreement is present. Final System Performance In Table 3 we compare the results of our system with the following state-of-the-art approaches: the JOINT and INDEP models of the Berkeley system (Durrett and Klein, 2014) (the JOINT model jointly does NER and entity linking along with coreference); the Prune-and-Score system (Ma et al., 2014); the HOTCoref system (Bj¨orkelund and Kuhn, 2014); the CPL3 M sytem (Chang et al., 2013); and Fernandes et al. We use the full entitycentric clustering algorithm drawing upon scores from both pairwise models. We do not make use of agreement features, as these did not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the e"
P15-1136,P08-2012,1,0.453355,"02; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art resu"
P15-1136,N10-1061,0,0.0208547,"4 58.58 57.94 58.21 58.71 60.44 CEAFφ4 CoNLL Prec. Rec. F1 Avg. F1 57.28 50.82 53.86 60.65 - 53.07 60.00 59.4 52.27 55.61 61.63 68.75 44.34 53.91 61.56 55.33 54.14 54.73 61.23 56.17 54.23 55.18 61.71 59.44 52.98 56.02 63.02 Table 3: Comparison of this work with other state-of-the-art approaches on the test set. and Stoyanov and Eisner (2012) train a classifier to do this with a structured perceptron algorithm. Entity-level information has also been successfully incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Culotta et al., 2006; Poon and Domingos, 2008; Haghighi and Klein, 2010), but these approaches do not directly learn parameters tuned so the system runs effectively at test time, while our imitation learning approach does. Imitation learning has been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum´e et al. (2005) and Ma et al. (2014). Other works use structured perceptron models for the same purpose (Stoyanov and Eisner, 2012; Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014). These systems all heuristically determine which actions are desirable for the system to perform. In contrast,"
P15-1136,N06-2015,0,0.137823,"ng the action costs, we use a linear combination of the B3 (Bagga and Baldwin, 1998) and MUC (Vilain et al., 1995) metrics, which are both commonly used for evaluating coreference systems. The other metric used in our evaluation, Entity-based CEAFE (CEAFφ4 ) (Luo, 2005), was not used because it is expensive to compute. We found weighting B3 three times as much as MUC to be effective on the development set. 4 Experiments and Results Experimental Setup We apply our model to the English portion of the CoNLL 2012 Shared Task data (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The data is split into a training set of 2802 documents, development set of 343 documents, and a test set of 345 documents. We use the provided preprocessing for parse trees, named entity tags, etc. The models are evaluated using three of the most popular metrics for coreference resolution: MUC, B3 , and Entity-based CEAFE (CEAFφ4 ). We also include the average F1 score (CoNLL F1 ) of these three metrics, as is commonly done in CoNLL Shared Tasks. We used the most recent version of the CoNLL scorer (version 8.01), which implements the original definitions of these metrics. Mention Detection"
P15-1136,W97-0319,0,0.189844,"hman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring fu"
P15-1136,W11-1902,0,0.0242415,"s as well as different candidate antecedents. For this reason, we instead train the model with an objective that maximizes the conditional log likelihood of the highest scoring true and false antecedents under the logistic model: X  Lr (θr ) = − max log pθr (t, m) m∈M t∈T (m)  + min log(1 − pθr (f, m)) + λ||θr ||1 f ∈F (m) For both models, we set λ = 0.001 and optimize their objectives using AdaGrad (Duchi et al., 2011). 2.3 Features Our mention pair models use a variety of common features for mention pair classification (for more details see (Bengtson and Roth, 2008; Stoyanov et al., 2010; Lee et al., 2011; Recasens et al., 2013)). These include • Distance features, e.g., the distance between the two mentions in sentences or number of mentions. • Syntactic features, e.g., number of embedded NPs under a mention, POS tags of the first, last, and head word. • Semantic features, e.g., named entity type, speaker identification. • Lexical Features, e.g., the first, last, and head word of the current mention. We also employ a feature conjunction scheme similar to the one described by Durrett and Klein (2013). 3 Entity-Centric Coreference Model Mention pair scores alone are not enough to produce a fina"
P15-1136,P04-1018,0,0.163497,"ference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pairs, but are trained to maximize a global objective function instead of pairwise accuracy. Unlike in our system, these methods typically consider one pair of mentions at a time during inference. Several works have explored using non-local entity-level features in mention-entity models that assign a single mention to a (partially completed) cluster (Luo et al., 2004; Yang et al., 2008; Rahman and Ng, 2011). Our system, however, builds clusters incrementally through merge operations, and so can operate in an easy-first fashion. Raghunathan et al. (2010) take this approach with a rule-based system that runs in multiple passes 1412 Prec. Fernandes et al. 75.91 Chang et al. Bj¨orkelund & Kuhn 74.3 Ma et al. 81.03 Durrett & Klein (INDEP.) 72.27 Durrett & Klein (JOINT) 72.61 This work 76.12 MUC Rec. 65.83 67.46 66.16 69.30 69.91 69.38 F1 70.51 69.48 70.72 72.84 70.75 71.24 72.59 Prec. 65.19 62.71 66.90 60.92 61.18 65.64 B3 Rec. 51.55 54.96 51.10 55.73 56.43 56"
P15-1136,H05-1004,0,0.407602,"Missing"
P15-1136,D14-1225,0,0.5502,"ad of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions (Raghunathan et al., 2010; Stoyanov and Eisner, 2012; Ma et al., 2014). However, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013). In this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture different linguistic phenomena in coreference resolution. We then d"
P15-1136,P02-1014,0,0.856073,"prune the search space the system works in, allowing for efficient training with an exact loss function. We evaluate our system on the English portion of the 2012 CoNLL Shared Task dataset and show that it improves over the current state of the art. 1 Introduction Coreference resolution, the task of identifying mentions in a text that refer to the same real world entity, is an important aspect of text understanding and has numerous applications. Many approaches to coreference resolution learn a scoring function defined over mention pairs to guide the coreference decisions (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). However, such systems do not make use of entity-level information, i.e., features between clusters of mentions instead of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information"
P15-1136,W06-1633,0,0.0100686,"mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean and Riloff, 2004), or correlational clustering (McCallum and Wellner, 2003; Finley and Joachims, 2005). In contrast to these methods, our entity-centric model directly learns how to use pairwise scores to produce a coreference partition that scores highly according to an evaluation metric, and can use the outputs of more than one mention pair model. Recently, coreference models using latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund a"
P15-1136,D08-1068,0,0.0258462,"56.43 56.01 F1 57.58 57.44 58.58 57.94 58.21 58.71 60.44 CEAFφ4 CoNLL Prec. Rec. F1 Avg. F1 57.28 50.82 53.86 60.65 - 53.07 60.00 59.4 52.27 55.61 61.63 68.75 44.34 53.91 61.56 55.33 54.14 54.73 61.23 56.17 54.23 55.18 61.71 59.44 52.98 56.02 63.02 Table 3: Comparison of this work with other state-of-the-art approaches on the test set. and Stoyanov and Eisner (2012) train a classifier to do this with a structured perceptron algorithm. Entity-level information has also been successfully incorporated in coreference systems using joint inference (McCallum and Wellner, 2003; Culotta et al., 2006; Poon and Domingos, 2008; Haghighi and Klein, 2010), but these approaches do not directly learn parameters tuned so the system runs effectively at test time, while our imitation learning approach does. Imitation learning has been employed to train coreference resolvers on trajectories of decisions similar to those that would be seen at test-time by Daum´e et al. (2005) and Ma et al. (2014). Other works use structured perceptron models for the same purpose (Stoyanov and Eisner, 2012; Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014). These systems all heuristically determine which actions are desirable for the syste"
P15-1136,N13-1071,0,0.127003,"Missing"
P15-1136,J01-4004,0,0.594281,"es are also used to prune the search space the system works in, allowing for efficient training with an exact loss function. We evaluate our system on the English portion of the 2012 CoNLL Shared Task dataset and show that it improves over the current state of the art. 1 Introduction Coreference resolution, the task of identifying mentions in a text that refer to the same real world entity, is an important aspect of text understanding and has numerous applications. Many approaches to coreference resolution learn a scoring function defined over mention pairs to guide the coreference decisions (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). However, such systems do not make use of entity-level information, i.e., features between clusters of mentions instead of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally,"
P15-1136,C12-1154,0,0.25037,"clusters of mentions instead of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions (Raghunathan et al., 2010; Stoyanov and Eisner, 2012; Ma et al., 2014). However, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013). In this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture different linguistic phenomena in coreference res"
P15-1136,M95-1005,0,0.92938,"Missing"
P15-1136,P08-1096,0,0.0300813,"ng latent antecedents have gained in popularity and achieved state-of-the-art results (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014). These learn a scoring function over mention pairs, but are trained to maximize a global objective function instead of pairwise accuracy. Unlike in our system, these methods typically consider one pair of mentions at a time during inference. Several works have explored using non-local entity-level features in mention-entity models that assign a single mention to a (partially completed) cluster (Luo et al., 2004; Yang et al., 2008; Rahman and Ng, 2011). Our system, however, builds clusters incrementally through merge operations, and so can operate in an easy-first fashion. Raghunathan et al. (2010) take this approach with a rule-based system that runs in multiple passes 1412 Prec. Fernandes et al. 75.91 Chang et al. Bj¨orkelund & Kuhn 74.3 Ma et al. 81.03 Durrett & Klein (INDEP.) 72.27 Durrett & Klein (JOINT) 72.61 This work 76.12 MUC Rec. 65.83 67.46 66.16 69.30 69.91 69.38 F1 70.51 69.48 70.72 72.84 70.75 71.24 72.59 Prec. 65.19 62.71 66.90 60.92 61.18 65.64 B3 Rec. 51.55 54.96 51.10 55.73 56.43 56.01 F1 57.58 57.44"
P15-1136,W12-4501,0,0.680964,"r-seen, and bias features). For m, the performance metric determining the action costs, we use a linear combination of the B3 (Bagga and Baldwin, 1998) and MUC (Vilain et al., 1995) metrics, which are both commonly used for evaluating coreference systems. The other metric used in our evaluation, Entity-based CEAFE (CEAFφ4 ) (Luo, 2005), was not used because it is expensive to compute. We found weighting B3 three times as much as MUC to be effective on the development set. 4 Experiments and Results Experimental Setup We apply our model to the English portion of the CoNLL 2012 Shared Task data (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The data is split into a training set of 2802 documents, development set of 343 documents, and a test set of 345 documents. We use the provided preprocessing for parse trees, named entity tags, etc. The models are evaluated using three of the most popular metrics for coreference resolution: MUC, B3 , and Entity-based CEAFE (CEAFφ4 ). We also include the average F1 score (CoNLL F1 ) of these three metrics, as is commonly done in CoNLL Shared Tasks. We used the most recent version of the CoNLL scorer (version 8.01), which implemen"
P15-1136,D10-1048,1,0.963683,"on, i.e., features between clusters of mentions instead of pairs. Using entity-level information is valuable because it allows early coreference decisions to inform later ones. For example, finding that Clinton and she corefer makes it more likely that Clinton corefers with Hillary Clinton than Bill Clinton due to gender agreement constraints. Such information has been incorporated successfully into entity-centric coreference systems that build up coreference clusters incrementally, using the information from the partially completed coreference chains produced so far to guide later decisions (Raghunathan et al., 2010; Stoyanov and Eisner, 2012; Ma et al., 2014). However, defining useful features between clusters of mentions and learning an effective policy for incrementally building up clusters can be challenging, and many recent state-of-the-art systems work entirely or almost entirely over pairs of mentions (Fernandes et al., 2012; Durrett and Klein, 2013; Chang et al., 2013). In this paper we introduce a novel coreference system that combines the advantages of mention pair and entity-centric systems with model stacking. We first propose two mention pair models designed to capture different linguistic p"
P15-1136,D09-1101,0,0.306989,"not increase accuracy and complicate the system. Our final model substantially outperforms the other systems on the CoNLL F1 score. The largest improvement is in the B3 metric, which is unsurprising because the entity-centric model primarily optimizes for this during training. However, our model also achieves the highest CEAFφ4 F1 and second highest MUC F1 scores among the other systems. 5 Related Work Both mention pair (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008; Stoyanov et al., 2010; Bj¨orkelund and Farkas, 2012) and mention ranking models (Denis and Baldridge, 2007b; Rahman and Ng, 2009) have been widely used for coreference resolution, and there have been many proposed ways of post-processing the pairwise scores to make predictions. Despite their simplicity, closest-first clustering (Soon et al., 2001) and best-first clustering (Ng and Cardie, 2002) are arguably the most widely used of these approaches. Other work uses global inference with integer linear programming to enforce transitivity (Denis and Baldridge, 2007a; Finkel and Manning, 2008), graph partitioning algorithms (McCallum and Wellner, 2005; Nicolae and Nicolae, 2006), the Dempster-Shafer rule (Kehler, 1997; Bean"
P15-1136,N07-1011,0,\N,Missing
P15-1136,W12-4502,0,\N,Missing
P15-1136,D08-1067,0,\N,Missing
P15-1150,S14-2114,0,0.0300663,"Missing"
P15-1150,P14-1062,0,0.88283,"Missing"
P15-1150,D14-1082,1,0.187036,"4927 train/dev/test split. The sentences are derived from existing image and video description datasets. Each sentence pair is annotated with a relatedness score y ∈ [1, 5], with 1 indicating that the two sentences are completely unrelated, and 5 indicating that the two sentences are very related. Each label is the average of 10 ratings assigned by different human annotators. Here, we use the similarity model described in Sec. 4.2. For the similarity prediction network (Eqs. 15) we use a hidden layer of size 50. We 3 Dependency parses produced by the Stanford Neural Network Dependency Parser (Chen and Manning, 2014). Fine-grained Binary 43.2 44.4 45.7 48.5 48.7 48.0 47.4 49.8 82.4 82.9 85.4 86.8 87.8 87.2 88.1 86.6 LSTM Bidirectional LSTM 2-layer LSTM 2-layer Bidirectional LSTM 46.4 49.1 46.0 48.5 (1.1) (1.0) (1.3) (1.0) 84.9 87.5 86.3 87.2 (0.6) (0.5) (0.6) (1.0) Dependency Tree-LSTM Constituency Tree-LSTM – randomly initialized vectors – Glove vectors, fixed – Glove vectors, tuned 48.4 (0.4) 85.7 (0.4) 43.9 (0.6) 49.7 (0.4) 51.0 (0.5) 82.0 (0.5) 87.5 (0.8) 88.0 (0.3) Table 2: Test set accuracies on the Stanford Sentiment Treebank. For our experiments, we report mean accuracies over 5 runs (standard dev"
P15-1150,N13-1092,0,0.0261622,"Missing"
P15-1150,W13-0112,0,0.0158871,"ions and plays with the case two men are dancing and singing in front of a crowd 3.37 3.19 4.08 4.01 4.00 Table 4: Most similar sentences from a 1000-sentence sample drawn from the SICK test set. The TreeLSTM model is able to pick up on more subtle relationships, such as that between “beach” and “ocean” in the second example. Pennington et al., 2014) have found wide applicability in a variety of NLP tasks. Following this success, there has been substantial interest in the area of learning distributed phrase and sentence representations (Mitchell and Lapata, 2010; Yessenalina and Cardie, 2011; Grefenstette et al., 2013; Mikolov et al., 2013), as well as distributed representations of longer bodies of text such as paragraphs and documents (Srivastava et al., 2013; Le and Mikolov, 2014). Our approach builds on recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011), which we abbreviate as Tree-RNNs in order to avoid confusion with recurrent neural networks. Under the Tree-RNN framework, the vector representation associated with each node of a tree is composed as a function of the vectors corresponding to the children of the node. The choice of composition function gives rise to numerous vari"
P15-1150,P12-1092,1,0.0457691,"e over 5 runs, and error bars have been omitted for clarity. We observe that while the Dependency TreeLSTM does significantly outperform its sequential counterparts on the relatedness task for longer sentences of length 13 to 15 (Fig. 4), it also achieves consistently strong performance on shorter sentences. This suggests that unlike sequential LSTMs, Tree-LSTMs are able to encode semantically-useful structural information in the sentence representations that they compose. 8 Related Work Distributed representations of words (Rumelhart et al., 1988; Collobert et al., 2011; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013; 1563 Ranking by mean word vector cosine similarity a woman is slicing potatoes a woman is cutting potatoes a woman is slicing herbs a woman is slicing tofu a boy is waving at some young runners from the ocean a man and a boy are standing at the bottom of some stairs , which are outdoors a group of children in uniforms is standing at a gate and one is kissing the mother a group of children in uniforms is standing at a gate and there is no one kissing the mother Score Ranking by Dependency Tree-LSTM model Score 0.96 0.92 0.92 a woman is slicing potatoes a woman is cutting"
P15-1150,S14-2131,0,0.0718846,"Missing"
P15-1150,D14-1181,0,0.164747,"Missing"
P15-1150,P03-1054,1,0.0821096,"Missing"
P15-1150,S14-2055,0,0.196572,"p.stanford.edu/projects/glove/. 1561 Method Pearson’s r Spearman’s ρ MSE 0.7993 0.8070 0.8268 0.8414 0.7538 0.7489 0.7721 – 0.3692 0.3550 0.3224 – Mean vectors DT-RNN (Socher et al., 2014) SDT-RNN (Socher et al., 2014) 0.7577 (0.0013) 0.7923 (0.0070) 0.7900 (0.0042) 0.6738 (0.0027) 0.7319 (0.0071) 0.7304 (0.0076) 0.4557 (0.0090) 0.3822 (0.0137) 0.3848 (0.0074) LSTM Bidirectional LSTM 2-layer LSTM 2-layer Bidirectional LSTM 0.8528 0.8567 0.8515 0.8558 0.7911 0.7966 0.7896 0.7965 0.2831 0.2736 0.2838 0.2762 Constituency Tree-LSTM Dependency Tree-LSTM 0.8582 (0.0038) 0.8676 (0.0030) Illinois-LH (Lai and Hockenmaier, 2014) UNAL-NLP (Jimenez et al., 2014) Meaning Factory (Bjerva et al., 2014) ECNU (Zhao et al., 2014) (0.0031) (0.0028) (0.0066) (0.0014) (0.0059) (0.0053) (0.0088) (0.0018) 0.7966 (0.0053) 0.8083 (0.0042) (0.0092) (0.0063) (0.0150) (0.0020) 0.2734 (0.0108) 0.2532 (0.0052) Table 3: Test set results on the SICK semantic relatedness subtask. For our experiments, we report mean scores over 5 runs (standard deviations in parentheses). Results are grouped as follows: (1) SemEval 2014 submissions; (2) Our own baselines; (3) Sequential LSTMs; (4) Tree-structured LSTMs. 6 6.1 Results Sentiment Classificatio"
P15-1150,S14-2001,0,0.108355,". 4.1 with both Dependency Tree-LSTMs (Sec. 3.1) and Constituency Tree-LSTMs (Sec. 3.2). The Constituency Tree-LSTMs are structured according to the provided parse trees. For the Dependency Tree-LSTMs, we produce dependency parses3 of each sentence; each node in a tree is given a sentiment label if its span matches a labeled span in the training set. 5.2 Semantic Relatedness For a given pair of sentences, the semantic relatedness task is to predict a human-generated rating of the similarity of the two sentences in meaning. We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014), consisting of 9927 sentence pairs in a 4500/500/4927 train/dev/test split. The sentences are derived from existing image and video description datasets. Each sentence pair is annotated with a relatedness score y ∈ [1, 5], with 1 indicating that the two sentences are completely unrelated, and 5 indicating that the two sentences are very related. Each label is the average of 10 ratings assigned by different human annotators. Here, we use the similarity model described in Sec. 4.2. For the similarity prediction network (Eqs. 15) we use a hidden layer of size 50. We 3 Dependency parses produced"
P15-1150,D14-1162,1,0.136302,"ford Sentiment Treebank. For our experiments, we report mean accuracies over 5 runs (standard deviations in parentheses). Fine-grained: 5-class sentiment classification. Binary: positive/negative sentiment classification. produce binarized constituency parses4 and dependency parses of the sentences in the dataset for our Constituency Tree-LSTM and Dependency TreeLSTM models. 5.3 Hyperparameters and Training Details The hyperparameters for our models were tuned on the development set for each task. We initialized our word representations using publicly available 300-dimensional Glove vectors5 (Pennington et al., 2014). For the sentiment classification task, word representations were updated during training with a learning rate of 0.1. For the semantic relatedness task, word representations were held fixed as we did not observe any significant improvement when the representations were tuned. Our models were trained using AdaGrad (Duchi et al., 2011) with a learning rate of 0.05 and a minibatch size of 25. The model parameters were regularized with a per-minibatch L2 regularization strength of 10−4 . The sentiment classifier was additionally regularized using dropout (Srivastava et al., 2014) with a dropout"
P15-1150,D12-1110,1,0.330741,"v, 2014). Our approach builds on recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011), which we abbreviate as Tree-RNNs in order to avoid confusion with recurrent neural networks. Under the Tree-RNN framework, the vector representation associated with each node of a tree is composed as a function of the vectors corresponding to the children of the node. The choice of composition function gives rise to numerous variants of this basic framework. TreeRNNs have been used to parse images of natural scenes (Socher et al., 2011), compose phrase representations from word vectors (Socher et al., 2012), and classify the sentiment polarity of sentences (Socher et al., 2013). 9 Conclusion In this paper, we introduced a generalization of LSTMs to tree-structured network topologies. The Tree-LSTM architecture can be applied to trees with arbitrary branching factor. We demonstrated the effectiveness of the Tree-LSTM by applying the architecture in two tasks: semantic relatedness and sentiment classification, outperforming existing systems on both. Controlling for model dimensionality, we demonstrated that Tree-LSTM models are able to outperform their sequential counterparts. Our results suggest"
P15-1150,Q14-1017,1,0.421964,"were regularized with a per-minibatch L2 regularization strength of 10−4 . The sentiment classifier was additionally regularized using dropout (Srivastava et al., 2014) with a dropout rate of 0.5. We did not observe performance gains using dropout on the semantic relatedness task. 4 Constituency parses produced by the Stanford PCFG Parser (Klein and Manning, 2003). 5 Trained on 840 billion tokens of Common Crawl data, http://nlp.stanford.edu/projects/glove/. 1561 Method Pearson’s r Spearman’s ρ MSE 0.7993 0.8070 0.8268 0.8414 0.7538 0.7489 0.7721 – 0.3692 0.3550 0.3224 – Mean vectors DT-RNN (Socher et al., 2014) SDT-RNN (Socher et al., 2014) 0.7577 (0.0013) 0.7923 (0.0070) 0.7900 (0.0042) 0.6738 (0.0027) 0.7319 (0.0071) 0.7304 (0.0076) 0.4557 (0.0090) 0.3822 (0.0137) 0.3848 (0.0074) LSTM Bidirectional LSTM 2-layer LSTM 2-layer Bidirectional LSTM 0.8528 0.8567 0.8515 0.8558 0.7911 0.7966 0.7896 0.7965 0.2831 0.2736 0.2838 0.2762 Constituency Tree-LSTM Dependency Tree-LSTM 0.8582 (0.0038) 0.8676 (0.0030) Illinois-LH (Lai and Hockenmaier, 2014) UNAL-NLP (Jimenez et al., 2014) Meaning Factory (Bjerva et al., 2014) ECNU (Zhao et al., 2014) (0.0031) (0.0028) (0.0066) (0.0014) (0.0059) (0.0053) (0.0088) (0."
P15-1150,D13-1170,1,0.456665,"+ b(h) ,   pˆθ = softmax W (p) hs + b(p) , yˆ = rT pˆθ , rT k=1 Experiments We evaluate our Tree-LSTM architectures on two tasks: (1) sentiment classification of sentences sampled from movie reviews and (2) predicting the semantic relatedness of sentence pairs. In comparing our Tree-LSTMs against sequential LSTMs, we control for the number of LSTM parameters by varying the dimensionality of the hidden states2 . Details for each model variant are summarized in Table 1. 5.1 In this task, we predict the sentiment of sentences sampled from movie reviews. We use the Stanford Sentiment Treebank (Socher et al., 2013). There are two subtasks: binary classification of sentences, and fine-grained classification over five classes: very negative, negative, neutral, positive, and very positive. We use the standard train/dev/test splits of 6920/872/1821 for the binary classification subtask and 8544/1101/2210 for the fine-grained classification subtask (there are fewer examples for the binary subtask since 1 where = [1 2 . . . K] and the absolute value function is applied elementwise. The use of both distance measures h× and h+ is empirically motivated: we find that the combination outperforms the use of either"
P15-1150,P10-1040,0,0.0214585,"point is a mean score over 5 runs, and error bars have been omitted for clarity. We observe that while the Dependency TreeLSTM does significantly outperform its sequential counterparts on the relatedness task for longer sentences of length 13 to 15 (Fig. 4), it also achieves consistently strong performance on shorter sentences. This suggests that unlike sequential LSTMs, Tree-LSTMs are able to encode semantically-useful structural information in the sentence representations that they compose. 8 Related Work Distributed representations of words (Rumelhart et al., 1988; Collobert et al., 2011; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013; 1563 Ranking by mean word vector cosine similarity a woman is slicing potatoes a woman is cutting potatoes a woman is slicing herbs a woman is slicing tofu a boy is waving at some young runners from the ocean a man and a boy are standing at the bottom of some stairs , which are outdoors a group of children in uniforms is standing at a gate and one is kissing the mother a group of children in uniforms is standing at a gate and there is no one kissing the mother Score Ranking by Dependency Tree-LSTM model Score 0.96 0.92 0.92 a woman is slicing potatoe"
P15-1150,D11-1016,0,0.0144923,"s opening the guitar for donations and plays with the case two men are dancing and singing in front of a crowd 3.37 3.19 4.08 4.01 4.00 Table 4: Most similar sentences from a 1000-sentence sample drawn from the SICK test set. The TreeLSTM model is able to pick up on more subtle relationships, such as that between “beach” and “ocean” in the second example. Pennington et al., 2014) have found wide applicability in a variety of NLP tasks. Following this success, there has been substantial interest in the area of learning distributed phrase and sentence representations (Mitchell and Lapata, 2010; Yessenalina and Cardie, 2011; Grefenstette et al., 2013; Mikolov et al., 2013), as well as distributed representations of longer bodies of text such as paragraphs and documents (Srivastava et al., 2013; Le and Mikolov, 2014). Our approach builds on recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2011), which we abbreviate as Tree-RNNs in order to avoid confusion with recurrent neural networks. Under the Tree-RNN framework, the vector representation associated with each node of a tree is composed as a function of the vectors corresponding to the children of the node. The choice of composition function"
P15-1150,S14-2044,0,0.132395,"that during training, components of the gradient vector can grow or decay exponentially over long sequences (Hochreiter, 1998; Bengio et al., 1994). This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence. The LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this problem of learning long-term dependencies by introducing a memory cell that is able to preserve state over long periods of time. While numerous LSTM variants have been described, here we describe the version used by Zaremba and Sutskever (2014). We define the LSTM unit at each time step t to be a collection of vectors in Rd : an input gate it , a forget gate ft , an output gate ot , a memory cell ct and a hidden state ht . The entries of the gating vectors it , ft and ot are in [0, 1]. We refer to d as the memory dimension of the LSTM. The LSTM transition equations are the following:   (1) it = σ W (i) xt + U (i) ht−1 + b(i) ,   ft = σ W (f ) xt + U (f ) ht−1 + b(f ) ,   ot = σ W (o) xt + U (o) ht−1 + b(o) ,   ut = tanh W (u) xt + U (u) ht−1 + b(u) , ct = it ut + ft ct−1 , Long Short-Term Memory Networks ht = ot tanh(ct ), O"
P16-1042,D14-1059,1,0.872036,"nd v are variants of entailment; f and  are variants of negation. 2. This lexical relation between words is projected up to yield a relation between sentences, based on the polarity of the token. For instance, The cat eats animals v some carnivores eat animals. We explain this in more detail below. 3. These sentence level relations are joined together to produce a relation between a premise, and a hypothesis multiple mutations away. For example in Figure 1, if we join v, ≡, v, and f, we get negation (). 2.2 NaturalLI We build our extensions within the framework of NaturalLI, introduced by Angeli and Manning (2014). NaturalLI casts inference as a search problem: given a hypothesis and an arbitrarily large corpus of text, it searches through the space of lexical mutations (e.g., cat → carnivore), with associated costs, until a premise is found. An example search using NaturalLI is given in Figure 1. The relations along the edges denote re1 For clarity we describe a simplified semantics here; NaturalLI implements the semantics described in Icard and Moss (2014). 443 No carnivores eat animals? f w The carnivores No animals eat animals eat animals w w No animals The cat ... eat things eats animals ≡ The cat"
P16-1042,de-marneffe-etal-2014-universal,1,0.847956,"Missing"
P16-1042,P15-1034,1,0.915379,"), with associated costs, until a premise is found. An example search using NaturalLI is given in Figure 1. The relations along the edges denote re1 For clarity we describe a simplified semantics here; NaturalLI implements the semantics described in Icard and Moss (2014). 443 No carnivores eat animals? f w The carnivores No animals eat animals eat animals w w No animals The cat ... eat things eats animals ≡ The cat ate an animal w The cat ate a mouse We recently defined a mapping from Stanford Dependency relations to the associated lexical relation deleting the dependent subtree would induce (Angeli et al., 2015). We adapt this mapping to yield the relation induced by inserting a given dependency edge, corresponding to our deletions in search; we also convert the mapping to use Universal Dependencies (de Marneffe et al., 2014). This now lends a natural deletion operation: at a given node, the subtree rooted at that node can be deleted to induce the associated natural logic relation. For example, we can infer that all truly notorious villains have lairs from the premise all villains have lairs by observing that deleting an amod arc induces the relation w, which in the downward polarity context of villa"
P16-1042,J16-4007,0,0.0519468,"Missing"
P16-1042,P11-1062,0,0.0235313,"point it has found a contradictory candidate premise which has perfect overlap with the premise some cats have tails. Even had we not found the exact premise, this suggests that the hypothesis is likely false. In addition to finding entailments from candidate premises, our system also allows us to encode a notion of likely negation. We can consider the following two statements na¨ıvely sharing every keyword. Each token marked with its polarity: P: polarity ↑ ↓ ↑ ↓ – 5 Related Work This work is similar in many ways to work on recognizing textual entailment – e.g., Schoenmackers et al. (2010), Berant et al. (2011), Lewis and Steedman (2013). In the RTE task, a single premise and a single hypothesis are given as input, and a system must return a judgment of either entailment or nonentailment (in later years, nonentailment is further split into contradiction and independence). These approaches often rely on alignment features, similar to ours, but do not generally scale to large premise sets (i.e., a comprehensive knowledge base). The discourse commitments in Hickl and Bensley (2007) can be thought of as similar to the additional entailed facts we add to the knowledge base (Section 3.3). In another line"
P16-1042,W07-1401,0,0.124919,"Missing"
P16-1042,D14-1159,1,0.857718,"perceived large differences in model scores and the apparent best system should be interpreted cautiously. NaturalLI consistently achieves the best training accuracy, and is more stable between configurations on the test set. For instance, it may be consistently discarding lexically similar but actually contradictory premises that often confuse some subset of the baselines. K NOWBOT is the dialog system presented in Hixon et al. (2015). We report numbers for two 449 System formance of NaturalLI. Both this and the previous class could be further mitigated by having a notion of a process, as in Berant et al. (2014). Other questions are simply not supported by any single sentence in the corpus. For example, A human offspring can inherit blue eyes has no support in the corpus that does not require significant multi-step inferences. A remaining chunk of errors are simply classification errors. For example, Water freezing is an example of a gas changing to a solid is marked as the best hypothesis, supported incorrectly by An ice cube is an example of matter that changes from a solid to a liquid to a gas, which after mutating water to ice cube matches every keyword in the hypothesis. Test Accuracy Solr Only"
P16-1042,W07-1428,0,0.0366455,"Related Work This work is similar in many ways to work on recognizing textual entailment – e.g., Schoenmackers et al. (2010), Berant et al. (2011), Lewis and Steedman (2013). In the RTE task, a single premise and a single hypothesis are given as input, and a system must return a judgment of either entailment or nonentailment (in later years, nonentailment is further split into contradiction and independence). These approaches often rely on alignment features, similar to ours, but do not generally scale to large premise sets (i.e., a comprehensive knowledge base). The discourse commitments in Hickl and Bensley (2007) can be thought of as similar to the additional entailed facts we add to the knowledge base (Section 3.3). In another line of work, Tian et al. (2014) approach the However, we note that all of the keyword pairs are in opposite polarity contexts. We can therefore define a pair of keywords as matching in NaturalLI if the following two conditions hold: (1) their lemmatized surface forms match exactly, and (2) they have the same polarity in the sentence. The second constraint encodes a good approximation for negation. To illustrate, consider the polarity signatures of common operators: 447 a colle"
P16-1042,N15-1086,0,0.341745,"extractions. Of course, this work is not alone in attempting to incorporate strict logical reasoning into question answering systems. The COGEX system (Moldovan et al., 2003) incorporates a theorem prover into a QA system, boosting overall performance on the TREC QA task. Similarly, Watson (Ferrucci et al., 2010) incorporates logical reasoning components alongside shallower methods. This work follows a similar vein, but both the theorem prover and lexical classifier operate over text, without requiring either the premises or axioms to be in logical forms. On the Aristo corpus we evaluate on, Hixon et al. (2015) proposes a dialog system to augment a knowledge graph used for answering the questions. This is in a sense an oracle measure, where a human is consulted while answering the question; although, they show that their additional extractions help answer questions other than the one the dialog was collected for. 6 6.1 Data Processing We make use of two collections of unlabeled corpora for our experiments. The first of these is the Barron’s study guide (BARRON ’ S), consisting of 1200 sentences. This is the corpus used by Hixon et al. (2015) for their conversational dialog engine Knowbot, and theref"
P16-1042,W04-3205,0,0.0829055,"my. Relational Entailment For two verbs v1 and v2 , we define v1 ≤ v2 if the first verb entails the second. In many cases, a verb v1 may entail a verb v2 even if v2 is not a hypernym of v1 . For example, to sell something (hopefully) entails owning that thing. Apart from context-specific cases (e.g., orbit entails launch only for man-made objects), these hold largely independent of context. Note that the usual operators apply to relational entailments – if all cactus owners live in Arizona then all cactus sellers live in Arizona. This information was incorporated using data from V ERB O CEAN (Chklovski and Pantel, 2004), adapting the confidence weights as transition costs. V ERB O CEAN uses lexicosyntactic patterns to score pairs of verbs as candidate participants in a set of relations. We approximate the V ERB O CEAN relations stronger -than(v1 , v2 ) (e.g., to kill is stronger than to wound) and 3.3 Removing the Insertion Transition Inserting words during search poses an inherent problem, as the space of possible words to insert at any position is on the order of the size of the vocabulary. In NaturalLI, this was solved by keeping a trie of possible insertions, and using that to prune this space. This is b"
P16-1042,Q13-1015,0,0.0189971,"contradictory candidate premise which has perfect overlap with the premise some cats have tails. Even had we not found the exact premise, this suggests that the hypothesis is likely false. In addition to finding entailments from candidate premises, our system also allows us to encode a notion of likely negation. We can consider the following two statements na¨ıvely sharing every keyword. Each token marked with its polarity: P: polarity ↑ ↓ ↑ ↓ – 5 Related Work This work is similar in many ways to work on recognizing textual entailment – e.g., Schoenmackers et al. (2010), Berant et al. (2011), Lewis and Steedman (2013). In the RTE task, a single premise and a single hypothesis are given as input, and a system must return a judgment of either entailment or nonentailment (in later years, nonentailment is further split into contradiction and independence). These approaches often rely on alignment features, similar to ours, but do not generally scale to large premise sets (i.e., a comprehensive knowledge base). The discourse commitments in Hickl and Bensley (2007) can be thought of as similar to the additional entailed facts we add to the knowledge base (Section 3.3). In another line of work, Tian et al. (2014)"
P16-1042,P11-1060,0,0.0419188,"large corpus is given as a knowledge base; the task is to find support in this knowledge base for the hypothesis. Our system is in many ways well-suited to the dataset. Although certainly many of the facts require complex reasoning (see Section 6.4), the majority can be answered from a single premise. Unlike FraCaS (Cooper et al., 1996) or the RTE challenges, however, the task does not have explicit premises to run inference from, but rather must infer the truth of the hypothesis from a large collection of supporting text. RTE problem by parsing into Dependency Compositional Semantics (DCS) (Liang et al., 2011). This work particularly relevant in that it also incorporates an evaluation function (using distributional similarity) to augment their theorem prover – although in their case, this requires a translation back and forth between DCS and language. Beltagy et al. (To appear 2016) takes a similar approach, but encoding distributional information directly in entailment rules in a Markov Logic Network (Richardson and Domingos, 2006). Many systems make use of structured knowledge bases for question answering. Semantic parsing methods (Zettlemoyer and Collins, 2005; Liang et al., 2011) use knowledge"
P16-1042,W07-1431,1,0.69958,"briefly review natural logic and NaturalLI – the existing inference engine we use. Much of this paper will extend this system, with additional inferences (Section 3) and a soft lexical classifier (Section 4). 2.1 Natural Logic Natural logic is a formal proof theory that aims to capture a subset of logical inferences by appealing directly to the structure of language, without needing either an abstract logical language (e.g., Markov Logic Networks; Richardson and Domingos (2006)) or denotations (e.g., semantic parsing; Liang and Potts (2015)). We use the logic introduced by the NatLog system (MacCartney and Manning, 2007; 2008; 2009), which was in turn based on earlier theoretical work on Monotonicity Calculus (van Benthem, 1986; S´anchez Valencia, 1991). We adopt the precise semantics of Icard and Moss (2014); we refer the reader to this paper for a more thorough introduction to the formalism. At a high level, natural logic proofs operate by mutating spans of text to ensure that the mutated sentence follows from the original – each step is much like a syllogistic inference. Each mutation in the proof follows three steps: 1. An atomic lexical relation is induced by either inserting, deleting or mutating a spa"
P16-1042,C08-1066,1,0.896547,"Missing"
P16-1042,D12-1048,0,0.0409762,"Network (Richardson and Domingos, 2006). Many systems make use of structured knowledge bases for question answering. Semantic parsing methods (Zettlemoyer and Collins, 2005; Liang et al., 2011) use knowledge bases like Freebase to find support for a complex question. Knowledge base completion (e.g., Chen et al. (2013), Bordes et al. (2011), or Riedel et al. (2013)) can be thought of as entailment, predicting novel knowledge base entries from the original database. In contrast, this work runs inference over arbitrary text without needing a structured knowledge base. Open IE (Wu and Weld, 2010; Mausam et al., 2012) QA approaches – e.g., Fader et al. (2014) are closer to operating over plain text, but still requires structured extractions. Of course, this work is not alone in attempting to incorporate strict logical reasoning into question answering systems. The COGEX system (Moldovan et al., 2003) incorporates a theorem prover into a QA system, boosting overall performance on the TREC QA task. Similarly, Watson (Ferrucci et al., 2010) incorporates logical reasoning components alongside shallower methods. This work follows a similar vein, but both the theorem prover and lexical classifier operate over te"
P16-1042,N03-1022,0,0.0777167,"pletion (e.g., Chen et al. (2013), Bordes et al. (2011), or Riedel et al. (2013)) can be thought of as entailment, predicting novel knowledge base entries from the original database. In contrast, this work runs inference over arbitrary text without needing a structured knowledge base. Open IE (Wu and Weld, 2010; Mausam et al., 2012) QA approaches – e.g., Fader et al. (2014) are closer to operating over plain text, but still requires structured extractions. Of course, this work is not alone in attempting to incorporate strict logical reasoning into question answering systems. The COGEX system (Moldovan et al., 2003) incorporates a theorem prover into a QA system, boosting overall performance on the TREC QA task. Similarly, Watson (Ferrucci et al., 2010) incorporates logical reasoning components alongside shallower methods. This work follows a similar vein, but both the theorem prover and lexical classifier operate over text, without requiring either the premises or axioms to be in logical forms. On the Aristo corpus we evaluate on, Hixon et al. (2015) proposes a dialog system to augment a knowledge graph used for answering the questions. This is in a sense an oracle measure, where a human is consulted wh"
P16-1042,N13-1008,0,0.0274154,"heorem prover – although in their case, this requires a translation back and forth between DCS and language. Beltagy et al. (To appear 2016) takes a similar approach, but encoding distributional information directly in entailment rules in a Markov Logic Network (Richardson and Domingos, 2006). Many systems make use of structured knowledge bases for question answering. Semantic parsing methods (Zettlemoyer and Collins, 2005; Liang et al., 2011) use knowledge bases like Freebase to find support for a complex question. Knowledge base completion (e.g., Chen et al. (2013), Bordes et al. (2011), or Riedel et al. (2013)) can be thought of as entailment, predicting novel knowledge base entries from the original database. In contrast, this work runs inference over arbitrary text without needing a structured knowledge base. Open IE (Wu and Weld, 2010; Mausam et al., 2012) QA approaches – e.g., Fader et al. (2014) are closer to operating over plain text, but still requires structured extractions. Of course, this work is not alone in attempting to incorporate strict logical reasoning into question answering systems. The COGEX system (Moldovan et al., 2003) incorporates a theorem prover into a QA system, boosting"
P16-1042,D10-1106,0,0.031439,"the cats have tails, at which point it has found a contradictory candidate premise which has perfect overlap with the premise some cats have tails. Even had we not found the exact premise, this suggests that the hypothesis is likely false. In addition to finding entailments from candidate premises, our system also allows us to encode a notion of likely negation. We can consider the following two statements na¨ıvely sharing every keyword. Each token marked with its polarity: P: polarity ↑ ↓ ↑ ↓ – 5 Related Work This work is similar in many ways to work on recognizing textual entailment – e.g., Schoenmackers et al. (2010), Berant et al. (2011), Lewis and Steedman (2013). In the RTE task, a single premise and a single hypothesis are given as input, and a system must return a judgment of either entailment or nonentailment (in later years, nonentailment is further split into contradiction and independence). These approaches often rely on alignment features, similar to ours, but do not generally scale to large premise sets (i.e., a comprehensive knowledge base). The discourse commitments in Hickl and Bensley (2007) can be thought of as similar to the additional entailed facts we add to the knowledge base (Section"
P16-1042,P14-1008,0,0.0197886,"nd Steedman (2013). In the RTE task, a single premise and a single hypothesis are given as input, and a system must return a judgment of either entailment or nonentailment (in later years, nonentailment is further split into contradiction and independence). These approaches often rely on alignment features, similar to ours, but do not generally scale to large premise sets (i.e., a comprehensive knowledge base). The discourse commitments in Hickl and Bensley (2007) can be thought of as similar to the additional entailed facts we add to the knowledge base (Section 3.3). In another line of work, Tian et al. (2014) approach the However, we note that all of the keyword pairs are in opposite polarity contexts. We can therefore define a pair of keywords as matching in NaturalLI if the following two conditions hold: (1) their lemmatized surface forms match exactly, and (2) they have the same polarity in the sentence. The second constraint encodes a good approximation for negation. To illustrate, consider the polarity signatures of common operators: 447 a collection of multiple-choice science questions from the New York Regents 4th Grade Science Exams (NYSED, 2014). Each multiple choice option is translated"
P16-1042,P10-1013,0,0.0472927,"in a Markov Logic Network (Richardson and Domingos, 2006). Many systems make use of structured knowledge bases for question answering. Semantic parsing methods (Zettlemoyer and Collins, 2005; Liang et al., 2011) use knowledge bases like Freebase to find support for a complex question. Knowledge base completion (e.g., Chen et al. (2013), Bordes et al. (2011), or Riedel et al. (2013)) can be thought of as entailment, predicting novel knowledge base entries from the original database. In contrast, this work runs inference over arbitrary text without needing a structured knowledge base. Open IE (Wu and Weld, 2010; Mausam et al., 2012) QA approaches – e.g., Fader et al. (2014) are closer to operating over plain text, but still requires structured extractions. Of course, this work is not alone in attempting to incorporate strict logical reasoning into question answering systems. The COGEX system (Moldovan et al., 2003) incorporates a theorem prover into a QA system, boosting overall performance on the TREC QA task. Similarly, Watson (Ferrucci et al., 2010) incorporates logical reasoning components alongside shallower methods. This work follows a similar vein, but both the theorem prover and lexical clas"
P16-1042,W09-3714,1,\N,Missing
P16-1061,D13-1203,0,0.776504,"learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features. 1 Introduction Coreference resolution, the task of identifying which mentions in a text refer to the same realworld entity, is fundamentally a clustering problem. However, many recent state-of-the-art coreference systems operate solely by linking pairs of mentions together (Durrett and Klein, 2013; Martschat and Strube, 2015; Wiseman et al., 2015). An alternative approach is to use agglomerative clustering, treating each mention as a singleton cluster at the outset and then repeatedly merging clusters of mentions deemed to be referring to the same entity. Such systems can take advantage of entity-level information, i.e., features between clusters of mentions instead of between just two mentions. As an example for why this is useful, it is clear that the clusters {Bill Clinton} and 643 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 643–653"
P16-1061,P13-1012,0,0.0519201,"oving Coreference Resolution by Learning Entity-Level Distributed Representations Kevin Clark Computer Science Department Stanford University kevclark@cs.stanford.edu Christopher D. Manning Computer Science Department Stanford University manning@cs.stanford.edu Abstract {Clinton, she} are not referring to the same entity, but it is ambiguous whether the pair of mentions Bill Clinton and Clinton are coreferent. Previous work has incorporated entity-level information through features that capture hard constraints like having gender or number agreement between clusters (Raghunathan et al., 2010; Durrett et al., 2013). In this work, we instead train a deep neural network to build distributed representations of pairs of coreference clusters. This captures entity-level information with a large number of learned, continuous features instead of a small number of hand-crafted categorical ones. Using the cluster-pair representations, our network learns when combining two coreference clusters is desirable. At test time it builds up coreference clusters incrementally, starting with each mention in its own cluster and then merging a pair of clusters each step. It makes these decisions with a novel easy-first cluste"
P16-1061,D08-1031,0,0.0178244,"sing an easy-first ordering of mentions, and using learning to search. The results are shown in Table 3. Feature Ablations. We performed a feature ablation study to determine the importance of the hand-engineered features included in our model. The results are shown in Table 1. We find the small number of non-embedding features substantially improves model performance, especially the distance and string matching features. This is unsurprising, as the additional features are not easily captured by word embeddings and historically such features have been very important in coreference resolvers (Bengtson and Roth, 2008). Pretrained Weights. We compare initializing the cluster-ranking model randomly with initializing it with the weights learned by the mentionranking model. Using pretrained weights greatly improves performance. We believe the clusterranking model has difficulty learning effective weights from scratch due to noise in the signal coming from cluster-level decisions (an overall bad cluster merge may still involve a few corThe Importance of Pretraining. We evaluate the benefit of the two-step pretraining for the 649 rect pairwise links) and the smaller amount of data used to train the cluster-ranki"
P16-1061,P14-1005,0,0.197899,"Missing"
P16-1061,N10-1061,0,0.0414759,"nt neural network running same purpose (Stoyanov and Eisner, 2012; Ferover the candidate antecedent-cluster. However, nandes et al., 2012; Bj¨orkelund and Kuhn, 2014). this is an augmentation to a mention-ranking model, and not fundamentally a clustering model our cluster ranker is. 8 Conclusion as Entity-level information has also been incorpoWe have presented a coreference system that cap rated in coreference systems using joint inference tures entity-level information with distributed rep(McCallum and Wellner, 2003; Poon and Domin resentations of coreference cluster pairs. These gos, 2008; Haghighi and Klein, 2010) and systems { } { } learned, dense, high-dimensional feature vectors provide our cluster-ranking coreference model with a strong ability to distinguish beneficial cluster merges from harmful ones. The model is trained with a learning-to-search algorithm that allows it to learn how local decisions will affect the final coreference score. We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task and report a substantial improvement over the current state-of-the-art. that build up coreference clusters incrementally (Luo et al., 2004; Yang et al., 2008; Raghunathan"
P16-1061,W12-4504,0,0.0516538,"Missing"
P16-1061,P82-1020,0,0.780105,"Missing"
P16-1061,P04-1018,0,0.0154911,"pairs. These gos, 2008; Haghighi and Klein, 2010) and systems { } { } learned, dense, high-dimensional feature vectors provide our cluster-ranking coreference model with a strong ability to distinguish beneficial cluster merges from harmful ones. The model is trained with a learning-to-search algorithm that allows it to learn how local decisions will affect the final coreference score. We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task and report a substantial improvement over the current state-of-the-art. that build up coreference clusters incrementally (Luo et al., 2004; Yang et al., 2008; Raghunathan et al., 2010). We take the latter approach, and in particular combine the cluster-ranking (Rahman and Ng, 2011; Ma et al., 2014) and easy-first (Stoyanov and Eisner, 2012; Clark and Manning, 2015) clustering strategies. These prior systems all express entity-level information in the form of hand-engineered features and constraints instead of entity-level distributed representations that are learned from data. We train our system using a learning-to-search algorithm similar to SEARN (Daum´e III et al., 2009). Learning-to-search style algorithms have been employe"
P16-1061,P15-1136,1,0.658568,"cost function   αFN if a = NA ∧ T (mi ) 6= {NA}    α if a 6= NA ∧ T (mi ) = {NA} FA ∆(a, mi ) = αWL if a 6= NA ∧ a ∈ / T (mi )    0 if a ∈ T (mi ) for “false new,” “false anaphoric,” “wrong link,” and correct coreference decisions. The different error penalties allow the system to be tuned for coreference evaluation metrics by biasing it towards making more or fewer coreference links. Pretraining. As in Wiseman et al. (2015), we found that pretraining is crucial for the mentionranking model’s success. We pretrained the network in two stages, minimizing the following objectives from Clark and Manning (2015): All-Pairs Classification − N P [ P i=1 t∈T (mi ) log p(t, mi ) + P f ∈F (mi ) log(1 − p(f, mi ))] Top-Pairs Classification − N P [ max log p(t, mi ) + min log(1 − p(f, mi ))] i=1 t∈T (mi ) f ∈F (mi ) Where F(mi ) is the set of false antecedents for mi and p(a, mi ) = sigmoid(s(a, mi )). The top pairs objective is a middle ground between the all-pairs classification and mention ranking objectives: it only processes high-scoring mentions, but is probabilistic rather than max-margin. We first pretrained the network with all-pairs classification for 150 epochs and then with top-pairs classificat"
P16-1061,H05-1004,0,0.689094,"algorithm inspired by SEARN (Daum´e III et al., 2009), which is described in Algorithm 1. The algorithm takes as input a dataset D of start states x (in our case documents with each mention in its own singleton coreference cluster) and structured labels y (in our case gold coreference clusters). Its goal is to train the policy π so when it executes from x, reaching a final state e, the resulting loss L(e, y) is small. We use the negative of the B3 coreference metric for this loss (Bagga and Baldwin, 1998). Although our system evaluation also includes the MUC (Vilain et al., 1995) and CEAFφ4 (Luo, 2005) metrics, we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAFφ4 is slow to compute. Reference policies typically refer to the gold labels to find actions that are likely to be beneficial. Our reference policy π ref takes the action that increases the B3 score the most each step, breaking ties randomly. It is generally recommended to use a stochastic mixture of the reference policy and the current learned policy during rollouts when the reference policy is not optimal (Chang et al., 2015b). However, we find only using the reference policy (w"
P16-1061,H05-1013,0,0.0187109,"Missing"
P16-1061,D14-1225,0,0.134117,"a score sc (ci , cj ) representing their compatibility for coreference. This is produced by applying a single fully connected layer of size one to the representation rc (ci , cj ) produced by the cluster-pair encoder: π(M ERGE[cm , c]|x) ∝ esc (cm ,c) π(PASS|x) ∝ esNA (m) During inference, π is executed by taking the highest-scoring (most probable) action at each step. 5.2 The last detail needed is the ordering in which to consider mentions. Cluster-ranking models in prior work order the mentions according to their positions in the document, processing them leftto-right (Rahman and Ng, 2011; Ma et al., 2014). However, we instead sort the mentions in descending order by their highest scoring candidate coreference link according to the mention-ranking model. This causes inference to occur in an easyfirst fashion where hard decisions are delayed until more information is available. Easy-first orderings have been shown to improve the performance of other incremental coreference strategies (Raghunathan et al., 2010; Stoyanov and Eisner, 2012) because they reduce the problem of errors compounding as the algorithm runs. We also find it beneficial to prune the set of candidate antecedents A(m) for each m"
P16-1061,Q15-1029,0,0.147704,"thm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features. 1 Introduction Coreference resolution, the task of identifying which mentions in a text refer to the same realworld entity, is fundamentally a clustering problem. However, many recent state-of-the-art coreference systems operate solely by linking pairs of mentions together (Durrett and Klein, 2013; Martschat and Strube, 2015; Wiseman et al., 2015). An alternative approach is to use agglomerative clustering, treating each mention as a singleton cluster at the outset and then repeatedly merging clusters of mentions deemed to be referring to the same entity. Such systems can take advantage of entity-level information, i.e., features between clusters of mentions instead of between just two mentions. As an example for why this is useful, it is clear that the clusters {Bill Clinton} and 643 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 643–653, c Berlin, Germany, August"
P16-1061,N16-1114,0,0.456127,"Missing"
P16-1061,P08-1096,0,0.0215596,"2008; Haghighi and Klein, 2010) and systems { } { } learned, dense, high-dimensional feature vectors provide our cluster-ranking coreference model with a strong ability to distinguish beneficial cluster merges from harmful ones. The model is trained with a learning-to-search algorithm that allows it to learn how local decisions will affect the final coreference score. We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task and report a substantial improvement over the current state-of-the-art. that build up coreference clusters incrementally (Luo et al., 2004; Yang et al., 2008; Raghunathan et al., 2010). We take the latter approach, and in particular combine the cluster-ranking (Rahman and Ng, 2011; Ma et al., 2014) and easy-first (Stoyanov and Eisner, 2012; Clark and Manning, 2015) clustering strategies. These prior systems all express entity-level information in the form of hand-engineered features and constraints instead of entity-level distributed representations that are learned from data. We train our system using a learning-to-search algorithm similar to SEARN (Daum´e III et al., 2009). Learning-to-search style algorithms have been employed to train corefere"
P16-1061,P02-1014,0,0.0494472,"Missing"
P16-1061,K15-1002,0,0.0378641,"Missing"
P16-1061,D08-1068,0,0.0217548,"Missing"
P16-1061,W12-4501,0,0.613089,"e with a reference policy π ref until reaching an end state e, and computing the resulting loss L(e, y). This rolling out procedure allows the model to learn how a local action will affect the 648 Model Full Model – MENTION – GENRE – DISTANCE – SPEAKER – MATCHING English F1 Chinese F1 65.52 –1.27 –0.25 –2.42 –1.26 –2.07 64.41 –0.74 –2.91 –2.41 –0.93 –3.44 All-Pairs Top-Pairs English F1 Chinese F1 Yes Yes No No Model Full Model – PRETRAINING – EASY- FIRST – L2S Experiments and Results Experimental Setup. We run experiments on the English and Chinese portions of the CoNLL 2012 Shared Task data (Pradhan et al., 2012). The models are evaluated using three of the most popular coreference metrics: MUC, B3 , and Entity-based CEAF (CEAFφ4 ). We generally report the average F1 score (CoNLL F1 ) of the three, which is common practice in coreference evaluation. We used the most recent version of the CoNLL scorer (version 8.01), which implements the original definitions of the metrics. 64.41 –0.24 –0.33 –5.43 English F1 Chinese F1 66.01 –5.01 –0.15 –0.32 64.86 –6.85 –0.12 –0.25 Table 3: CoNLL F1 scores of the cluster-ranking model on the dev sets with various ablations. – PRETRAINING: initializing model parameters"
P16-1061,D10-1048,1,0.94225,"-first strategy improves the performance of the cluster-ranking model. Our final system achieves CoNLL F1 scores of 65.29 for English and 63.66 for Chinese, substantially outperforming other state-of-the-art systems.1 2 System Architecture Mention-Pair Representation rm ReLU(W2h1 + b2) Input Layer h0 ReLU(W1h0 + b1) Candidate Antecedent Features Mention Embeddings Mention Pair and Features Document Features Figure 2: Mention-pair encoder. mentions and pairs of coreference clusters. We assume that a set of mentions has already been extracted from each document using a method such as the one in Raghunathan et al. (2010). 3.1 Mention-Pair Encoder Given a mention m and candidate antecedent a, the mention-pair encoder produces a distributed representation of the pair rm (a, m) ∈ Rd with a feedforward neural network, which is shown in Figure 2. The candidate antecedent may be any mention that occurs before m in the document or NA, indicating that m has no antecedent. We also experimented with models based on Long Short-Term Memory recurrent neural networks (Hochreiter and Schmidhuber, 1997), but found these to perform slightly worse when used in an end-to-end coreference system due to heavy overfitting to the tr"
P16-1061,J01-4004,0,0.0762703,"Missing"
P16-1061,C12-1154,0,0.30872,"oreference clusters. This captures entity-level information with a large number of learned, continuous features instead of a small number of hand-crafted categorical ones. Using the cluster-pair representations, our network learns when combining two coreference clusters is desirable. At test time it builds up coreference clusters incrementally, starting with each mention in its own cluster and then merging a pair of clusters each step. It makes these decisions with a novel easy-first cluster-ranking procedure that combines the strengths of cluster-ranking (Rahman and Ng, 2011) and easy-first (Stoyanov and Eisner, 2012) coreference algorithms. Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made. We address this by using a learning-to-search algorithm inspired by SEARN (Daum´e III et al., 2009) to train our neural network. This approach allows the model to learn which action (a cluster merge) available from the current state (a partially completed coreference clustering) will eventually lead to a high-scoring coreference partition. Our system uses little manual feature engineering, which means it is easily ex"
P16-1061,M95-1005,0,0.881538,"model using a learning-to-search algorithm inspired by SEARN (Daum´e III et al., 2009), which is described in Algorithm 1. The algorithm takes as input a dataset D of start states x (in our case documents with each mention in its own singleton coreference cluster) and structured labels y (in our case gold coreference clusters). Its goal is to train the policy π so when it executes from x, reaching a final state e, the resulting loss L(e, y) is small. We use the negative of the B3 coreference metric for this loss (Bagga and Baldwin, 1998). Although our system evaluation also includes the MUC (Vilain et al., 1995) and CEAFφ4 (Luo, 2005) metrics, we do not incorporate them into the loss because MUC has the flaw of treating all errors equally and CEAFφ4 is slow to compute. Reference policies typically refer to the gold labels to find actions that are likely to be beneficial. Our reference policy π ref takes the action that increases the B3 score the most each step, breaking ties randomly. It is generally recommended to use a stochastic mixture of the reference policy and the current learned policy during rollouts when the reference policy is not optimal (Chang et al., 2015b). However, we find only using"
P16-1061,W12-4502,0,\N,Missing
P16-1061,N06-2015,0,\N,Missing
P16-1061,D08-1067,0,\N,Missing
P16-1061,P15-1137,0,\N,Missing
P16-1100,D15-1041,0,0.0490865,"a new state-of-theart result with 20.7 BLEU score. Our analysis demonstrates that our character models can successfully learn to not only generate well-formed words for Czech, a highly-inflected language with a very complex vocabulary, but also build correct representations for English source words. We provide code, data, and models at http: //nlp.stanford.edu/projects/nmt. 2 Related Work There has been a recent line of work on end-toend character-based neural models which achieve good results for part-of-speech tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), text classification (Zhang et al., 2015), speech recognition (Chan et al., 2016; Bahdanau et al., 2016), and language modeling (Kim et al., 2016; Jozefowicz et al., 2016). However, success has not been shown for cross-lingual tasks such as machine translation.1 Sennrich et al. (2016) propose to segment words into smaller units and translate just like at the word level, which does not learn to understand relationships among words. Our work takes inspiration from (Luong et al., 2013) and (Li et al., 2015). Similar to the former, we build representations for rare words on-the-fly from subword u"
P16-1100,P15-1001,0,0.236991,"NMT translates at the word level. For rare tokens, the character-level components build source representations and recover target <unk&gt;. “_” marks sequence boundaries. 1 Introduction Neural Machine Translation (NMT) is a simple new architecture for getting machines to translate. At its core, NMT is a single deep neural network that is trained end-to-end with several advantages such as simplicity and generalization. Despite being relatively new, NMT has already achieved state-of-the-art translation results for several language pairs such as English-French (Luong et al., 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015), and English-Czech (Jean et al., 2015b). While NMT offers many advantages over traditional phrase-based approaches, such as small memory footprint and simple decoder implementation, nearly all previous work in NMT has used quite restricted vocabularies, crudely treating all other words the same with an <unk&gt; symbol. Sometimes, a post-processing step that patches in unknown words is introduced to alleviate this problem. Luong et al. (2015b) propose to annotate 1054 Proceedings of the 54th Annual Meeting of the Association for Computational Lingui"
P16-1100,W16-2323,0,0.0566179,"Missing"
P16-1100,D13-1176,0,0.0927877,"ires existence of a morphological analyzer. In comparison with (Li et al., 2015), our hybrid architecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character. In contrast, Li et al. (2015) build hierarchical models at the sentence-word level for paragraphs and documents. 3 Background & Our Models Neural machine translation aims to directly model the conditional probability p(y|x) of translating a source sentence, x1 , . . . , xn , to a target sentence, y1 , . . . , ym . It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The encoder computes a representation s for each source sentence. Based on that source 1 Recently, Ling et al. (2015b) attempt character-level NMT; however, the experimental evidence is weak. The authors demonstrate only small improvements over word-level baselines and acknowledge that there are no differences of significance. Furthermore, only small datasets were used without comparable results from past NMT work. 1055 representation, the decoder generates a translation, one target word at a time, and hence, decomposes the log conditional probabili"
P16-1100,P15-1107,0,0.0461192,"tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), text classification (Zhang et al., 2015), speech recognition (Chan et al., 2016; Bahdanau et al., 2016), and language modeling (Kim et al., 2016; Jozefowicz et al., 2016). However, success has not been shown for cross-lingual tasks such as machine translation.1 Sennrich et al. (2016) propose to segment words into smaller units and translate just like at the word level, which does not learn to understand relationships among words. Our work takes inspiration from (Luong et al., 2013) and (Li et al., 2015). Similar to the former, we build representations for rare words on-the-fly from subword units. However, we utilize recurrent neural networks with characters as the basic units; whereas Luong et al. (2013) use recursive neural networks with morphemes as units, which requires existence of a morphological analyzer. In comparison with (Li et al., 2015), our hybrid architecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character. In contrast, Li et al. (2015) build hierarchical models at the sentence-word level for paragraphs and documen"
P16-1100,2015.iwslt-evaluation.11,1,0.810079,"e tokens, the character-level components build source representations and recover target <unk&gt;. “_” marks sequence boundaries. 1 Introduction Neural Machine Translation (NMT) is a simple new architecture for getting machines to translate. At its core, NMT is a single deep neural network that is trained end-to-end with several advantages such as simplicity and generalization. Despite being relatively new, NMT has already achieved state-of-the-art translation results for several language pairs such as English-French (Luong et al., 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015), and English-Czech (Jean et al., 2015b). While NMT offers many advantages over traditional phrase-based approaches, such as small memory footprint and simple decoder implementation, nearly all previous work in NMT has used quite restricted vocabularies, crudely treating all other words the same with an <unk&gt; symbol. Sometimes, a post-processing step that patches in unknown words is introduced to alleviate this problem. Luong et al. (2015b) propose to annotate 1054 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1054–1063, c Berlin, Germany, Augus"
P16-1100,W13-3512,1,0.120422,"of target <unk&gt; with positional information to track their alignments, after which simple word dictionary lookup or identity copy can be performed to replace <unk&gt; in the translation. Jean et al. (2015a) approach the problem similarly but obtain the alignments for unknown words from the attention mechanism. We refer to these as the unk replacement technique. Though simple, these approaches ignore several important properties of languages. First, monolingually, words are morphologically related; however, they are currently treated as independent entities. This is problematic as pointed out by Luong et al. (2013): neural networks can learn good representations for frequent words such as “distinct”, but fail for rare-but-related words like “distinctiveness”. Second, crosslingually, languages have different alphabets, so one cannot naïvely memorize all possible surface word translations such as name transliteration between “Christopher” (English) and “Kry˘stof” (Czech). See more on this problem in (Sennrich et al., 2016). To overcome these shortcomings, we propose a novel hybrid architecture for NMT that translates mostly at the word level and consults the character components for rare words when necess"
P16-1100,D15-1166,1,0.592947,"mong words. Our work takes inspiration from (Luong et al., 2013) and (Li et al., 2015). Similar to the former, we build representations for rare words on-the-fly from subword units. However, we utilize recurrent neural networks with characters as the basic units; whereas Luong et al. (2013) use recursive neural networks with morphemes as units, which requires existence of a morphological analyzer. In comparison with (Li et al., 2015), our hybrid architecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character. In contrast, Li et al. (2015) build hierarchical models at the sentence-word level for paragraphs and documents. 3 Background & Our Models Neural machine translation aims to directly model the conditional probability p(y|x) of translating a source sentence, x1 , . . . , xn , to a target sentence, y1 , . . . , ym . It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The encoder computes a representation s for each source sentence. Based on that source 1 Recently, Ling et al. (2015b) attempt character-level NMT; however, the experimental"
P16-1100,P15-1002,0,0.358261,"cute cat” into “un joli chat”. Hybrid NMT translates at the word level. For rare tokens, the character-level components build source representations and recover target <unk&gt;. “_” marks sequence boundaries. 1 Introduction Neural Machine Translation (NMT) is a simple new architecture for getting machines to translate. At its core, NMT is a single deep neural network that is trained end-to-end with several advantages such as simplicity and generalization. Despite being relatively new, NMT has already achieved state-of-the-art translation results for several language pairs such as English-French (Luong et al., 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015), and English-Czech (Jean et al., 2015b). While NMT offers many advantages over traditional phrase-based approaches, such as small memory footprint and simple decoder implementation, nearly all previous work in NMT has used quite restricted vocabularies, crudely treating all other words the same with an <unk&gt; symbol. Sometimes, a post-processing step that patches in unknown words is introduced to alleviate this problem. Luong et al. (2015b) propose to annotate 1054 Proceedings of the 54th Annual Meeting of the"
P16-1100,P02-1040,0,0.101077,"can be invoked in batch mode. At test time, our strategy is to first run a beam search decoder at the word level to find the best translations given by the word-level NMT. Such translations contains <unk&gt; tokens, so we utilize our character-level decoder with beam search to generate actual words for these <unk&gt;. 5 Experiments We evaluate the effectiveness of our models on the publicly available WMT’15 translation task from English into Czech with newstest2013 (3000 sentences) as a development set and newstest2015 (2656 sentences) as a test set. Two metrics are used: case-sensitive NIST BLEU (Papineni et al., 2002) and chrF3 (Popovi´c, 2015).3 The latter measures the amounts of overlapping character ngrams and has been argued to be a better metric for translation tasks out of English. 5.1 Data Among the available language pairs in WMT’15, all involving English, we choose Czech as a target language for several reasons. First and foremost, Czech is a Slavic language with not only rich and 3 For NIST BLEU, we first run detokenizer.pl and then use mteval-v13a to compute the scores as per WMT guideline. For chrF3 , we utilize the implementation here https://github.com/rsennrich/subword-nmt. # Sents # Tokens"
P16-1100,D14-1162,1,0.105018,"ive unaffected immobile admitting immoveable admittance illiberal unconcern 0.3 nonconscious uncontroversial 0.5 governance management 0.6 cofounders companionships link 0.7 0.8 0.9 1 Figure 4: Barnes-Hut-SNE visualization of source word representations – shown are sample words from the Rare Word dataset. We differentiate two types of embeddings: frequent words in which encoder embeddings are looked up directly and rare words where we build representations from characters. Boxes highlight examples that we will discuss in the text. We use the hybrid model (l) in this visualization. embeddings (Pennington et al., 2014) which were trained on a much larger dataset. System (Luong et al., 2013) Glove (Pennington et al., 2014) (d) (k) (l) Size 1B 6B 42B Our NMT models Word-based 0.3B Hybrid 0.3B Hybrid 0.3B |V | 138K 400K 400K ρ 34.4 38.1 47.8 50K 10K 50K 20.4 42.4 47.1 Table 3: Word similarity task – shown are Spearman’s correlation ρ on the Rare Word dataset of various models (with different vocab sizes |V |). Qualitatively, we visualize embeddings produced by the hybrid model (l) for selected words in the Rare Word dataset. Figure 4 shows the two-dimensional representations of words computed by the Barnes-Hut"
P16-1100,W15-3049,0,0.0279945,"Missing"
P16-1100,P16-1162,0,0.219539,"portant properties of languages. First, monolingually, words are morphologically related; however, they are currently treated as independent entities. This is problematic as pointed out by Luong et al. (2013): neural networks can learn good representations for frequent words such as “distinct”, but fail for rare-but-related words like “distinctiveness”. Second, crosslingually, languages have different alphabets, so one cannot naïvely memorize all possible surface word translations such as name transliteration between “Christopher” (English) and “Kry˘stof” (Czech). See more on this problem in (Sennrich et al., 2016). To overcome these shortcomings, we propose a novel hybrid architecture for NMT that translates mostly at the word level and consults the character components for rare words when necessary. As illustrated in Figure 1, our hybrid model consists of a word-based NMT that performs most of the translation job, except for the two (hypothetically) rare words, “cute” and “joli”, that are handled separately. On the source side, representations for rare words, “cute”, are computed on-thefly using a deep recurrent neural network that operates at the character level. On the target side, we have a separat"
P16-1100,N06-1014,0,0.0642823,"Missing"
P16-1100,D15-1176,0,0.123141,"Missing"
P16-1139,D15-1075,1,0.173704,"volutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and Küchler, 1996; Socher et al., 2011a, see Figure 1b), which propagate information up a binary parse tree. Of these, the TreeRNN appears to be the principled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure (Dowty, 2007, i.a.). TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have 1466 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1466–1477, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics stack the cat the cat composition composition reduce shift transition tracking transition tracking sat down buffer the cat sat tracking down sat down (a) The SPINN model unrolled for two transitions during the processing of the sentence the cat sat down. ‘Tracking’, ‘transition’, and ‘composition’ are neural network layers. Gray arrows indicate connections which are blocked by a gating fu"
P16-1139,P15-2142,0,0.0138497,"tecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time."
P16-1139,D14-1082,1,0.425506,"s upon the TreeRNN architecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive searc"
P16-1139,D16-1053,0,0.0398047,"Missing"
P16-1139,P15-1033,0,0.0232099,"t test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents"
P16-1139,N16-1024,0,0.0202595,"and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g., a natural lan"
P16-1139,P04-1013,0,0.021347,"produces the required parse structure on the fly. This design improves upon the TreeRNN architecture in three ways: At test time, it can simultaneously parse and interpret unparsed sentences, removing the dependence on an external parser at nearly no additional computational cost. Secondly, it supports batched computation for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of opera"
P16-1139,W03-3017,0,0.0499586,"lated work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g., a natural language sentence) by a single left-to-right scan over its tokens. The formalism is widely used in natural language parsing (e.g., Shieber, 1983; Nivre, 2003). A shift-reduce parser accepts a sequence of input tokens x = (x 0, . . . , x N −1 ) and consumes transitions a = (a0, . . . , aT −1 ), where each at ∈ {shift, reduce} specifies one step of the parsing process. In general a parser may also generate these transitions on the fly as it reads the tokens. It proceeds left-to-right through a transition sequence, combining the input tokens x incrementally into a tree structure. For any binary-branching tree structure over N words, this requires T = 2N − 1 transitions through a total of T + 1 states. The parser uses two auxiliary data structures: a s"
P16-1139,P14-1062,0,0.0561508,". This component, the sentence encoder, is generally formulated as a learned parametric function from a sequence of word vectors to a sentence vector, and this function can take a range of different forms. Common sentence encoders include sequencebased recurrent neural network models (RNNs, see Figure 1a) with Long Short-Term Memory (LSTM, first two authors contributed equally. cat cat (a) A conventional sequence-based RNN for two sentences. Introduction ∗ The old the Hochreiter and Schmidhuber, 1997), which accumulate information over the sentence sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and Küchler, 1996; Socher et al., 2011a, see Figure 1b), which propagate information up a binary parse tree. Of these, the TreeRNN appears to be the principled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure (Dowty, 2007, i.a.). TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have 1466 Proceedings of t"
P16-1139,Q16-1032,0,0.0243001,"Missing"
P16-1139,D15-1278,0,0.0436106,"sequentially; convolutional neural networks (Kalchbrenner et al., 2014; Zhang et al., 2015), which accumulate information using filters over short local sequences of words or characters; and tree-structured recursive neural networks (TreeRNNs, Goller and Küchler, 1996; Socher et al., 2011a, see Figure 1b), which propagate information up a binary parse tree. Of these, the TreeRNN appears to be the principled choice, since meaning in natural language sentences is known to be constructed recursively according to a tree structure (Dowty, 2007, i.a.). TreeRNNs have shown promise (Tai et al., 2015; Li et al., 2015; Bowman et al., 2015b), but have 1466 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1466–1477, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics stack the cat the cat composition composition reduce shift transition tracking transition tracking sat down buffer the cat sat tracking down sat down (a) The SPINN model unrolled for two transitions during the processing of the sentence the cat sat down. ‘Tracking’, ‘transition’, and ‘composition’ are neural network layers. Gray arrows indicate connections which are b"
P16-1139,P16-2022,0,0.197438,"Missing"
P16-1139,P83-1017,0,0.426606,"rsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence (e.g., a natural language sentence) by a single left-to-right scan over its tokens. The formalism is widely used in natural language parsing (e.g., Shieber, 1983; Nivre, 2003). A shift-reduce parser accepts a sequence of input tokens x = (x 0, . . . , x N −1 ) and consumes transitions a = (a0, . . . , aT −1 ), where each at ∈ {shift, reduce} specifies one step of the parsing process. In general a parser may also generate these transitions on the fly as it reads the tokens. It proceeds left-to-right through a transition sequence, combining the input tokens x incrementally into a tree structure. For any binary-branching tree structure over N words, this requires T = 2N − 1 transitions through a total of T + 1 states. The parser uses two auxiliary data s"
P16-1139,D11-1014,1,0.416432,"Missing"
P16-1139,P15-1150,1,0.752904,"Missing"
P16-1139,N16-1170,0,0.0699057,"Missing"
P16-1139,N16-1035,0,0.0142436,"ion for both parsed and unparsed sentences, yielding dramatic speedups over There is a fairly long history of work on building neural network-based parsers that use the core operations and data structures from transition-based parsing, of which shift-reduce parsing is a variant (Henderson, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2010; Chen and Manning, 2014; Buys and Blunsom, 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). In addition, there has been recent work proposing models designed primarily for generative language modeling tasks that use this architecture as well (Zhang et al., 2016; Dyer et al., 2016). To our knowledge, SPINN is the first model to use this architecture for the purpose of sentence interpretation, rather than parsing 2 1467 Related work or generation. Socher et al. (2011a,b) present versions of the TreeRNN model which are capable of operating over unparsed inputs. However, these methods require an expensive search process at test time. Our model presents a much faster alternative approach. 3 3.1 Our model: SPINN Background: Shift-reduce parsing SPINN is inspired by shift-reduce parsing (Aho and Ullman, 1972), which builds a tree structure over a sequence"
P16-1139,D14-1162,1,\N,Missing
P16-1139,W07-2218,0,\N,Missing
P16-1223,D14-1162,1,0.109185,"es of LambdaMART since we are only scoring 1/0 loss on the first ranked proposal, rather than using an IR-style metric to score ranked results. We use Stanford’s neural network dependency parser (Chen and Manning, 2014) to parse all our document and question text, and all other features can be extracted without additional tools. For training our neural networks, we only keep the most frequent |V |= 50k words (including entity and placeholder markers), and map all other words to an &lt;unk&gt; token. We choose word embedding size d = 100, and use the 100-dimensional pretrained GloVe word embeddings (Pennington et al., 2014) for initialization. The attention and output parameters are initialized from a uniform distribution between (−0.01, 0.01), and the LSTM weights are initialized from a Gaussian distribution N (0, 0.1). We use hidden size h = 128 for CNN and 256 for Daily Mail. Optimization is carried out using 5https://sourceforge.net/p/lemur/wiki/ RankLib/. vanilla stochastic gradient descent (SGD), with a fixed learning rate of 0.1. We sort all the examples by the length of its passage, and randomly sample a mini-batch of size 32 for each update. We also apply dropout with probability 0.2 to the embedding la"
P16-1223,D13-1020,0,0.759522,"retation of 1Our code is available at https://github.com/danqi/ rc-cnn-dailymail. 2https://en.wikipedia.org/wiki/Reading_ comprehension the text and making complex inferences. Human reading comprehension is often tested by asking questions that require interpretive understanding of a passage, and the same approach has been suggested for testing computers (Burges, 2013). In recent years, there have been several strands of work which attempt to collect human-labeled data for this task – in the form of document, question and answer triples – and to learn machine learning models directly from it (Richardson et al., 2013; Berant et al., 2014; Wang et al., 2015). However, these datasets consist of only hundreds of documents, as the labeled examples usually require considerable expertise and neat design, making the annotation process quite expensive. The subsequent scarcity of labeled examples prevents us from training powerful statistical models, such as deep learning models, and would seem to prevent a system from learning complex textual reasoning capacities. Recently, researchers at DeepMind (Hermann et al., 2015) had the appealing, original idea of exploiting the fact that the abundant news articles of CNN"
P16-1223,P15-1024,0,0.0281503,"Missing"
P16-1223,P15-2115,0,0.064981,"github.com/danqi/ rc-cnn-dailymail. 2https://en.wikipedia.org/wiki/Reading_ comprehension the text and making complex inferences. Human reading comprehension is often tested by asking questions that require interpretive understanding of a passage, and the same approach has been suggested for testing computers (Burges, 2013). In recent years, there have been several strands of work which attempt to collect human-labeled data for this task – in the form of document, question and answer triples – and to learn machine learning models directly from it (Richardson et al., 2013; Berant et al., 2014; Wang et al., 2015). However, these datasets consist of only hundreds of documents, as the labeled examples usually require considerable expertise and neat design, making the annotation process quite expensive. The subsequent scarcity of labeled examples prevents us from training powerful statistical models, such as deep learning models, and would seem to prevent a system from learning complex textual reasoning capacities. Recently, researchers at DeepMind (Hermann et al., 2015) had the appealing, original idea of exploiting the fact that the abundant news articles of CNN and Daily Mail are accompanied by bullet"
P16-1223,D14-1082,1,\N,Missing
P16-1223,D14-1159,1,\N,Missing
P16-1223,N16-1099,0,\N,Missing
P16-1224,D13-1160,1,0.86034,"= 1, 2, 3}. The set of all features is just the cross product of utterance features and logical form features. For example, if x = ‘enlever tout’ and z = remove(all()), then features include: (‘enlever’, all) (‘enlever’, remove) (‘enlever’, (remove, 1, all)) (‘tout’, (remove, 1, all)) (‘tout’, all) (‘tout’, remove) Note that we do not model an explicit alignment or derivation compositionally connecting the utterance and the logical form, in contrast to most traditional work in semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010; Berant et al., 2013), instead following a looser model of semantics similar to (Pasupat and Liang, 2015). Modeling explicit alignments or derivations is only computationally feasible when we are learning from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available. Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we construct a set of logical forms of size n (with exactly n predicates) by combining logical forms o"
P16-1224,P09-1010,0,0.116088,"use precise and consistent languages. Interestingly, our pragmatics model did not help and can even hurt the less successful players who are less precise and consistent. This is expected behavior: the pragmatics model assumes that the human is cooperative and behaving rationally. For the bottom half of the players, this assumption is not true, in which case the pragmatics model is not useful. 6 Related Work and Discussion Our work connects with a broad body of work on grounded language, in which language is used in some environment as a means towards some goal. Examples include playing games (Branavan et al., 2009, 2010; Reckman et al., 2010) interacting with robotics (Tellex et al., 2011, 2014), and following instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013) Semantic parsing utterances to logical forms, which we leverage, plays an important role in these settings (Kollar et al., 2010; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013). What makes this work unique is our new interactive learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for exam"
P16-1224,P10-1129,0,0.0563282,"Missing"
P16-1224,D10-1119,0,0.0429812,"{(h, i, ψ(h.i, d − 1)) |i = 1, 2, 3}. The set of all features is just the cross product of utterance features and logical form features. For example, if x = ‘enlever tout’ and z = remove(all()), then features include: (‘enlever’, all) (‘enlever’, remove) (‘enlever’, (remove, 1, all)) (‘tout’, (remove, 1, all)) (‘tout’, all) (‘tout’, remove) Note that we do not model an explicit alignment or derivation compositionally connecting the utterance and the logical form, in contrast to most traditional work in semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010; Berant et al., 2013), instead following a looser model of semantics similar to (Pasupat and Liang, 2015). Modeling explicit alignments or derivations is only computationally feasible when we are learning from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available. Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we construct a set of logical forms of size n (with exactly n predicates) by com"
P16-1224,P12-1045,0,0.0165039,"(Tellex et al., 2011, 2014), and following instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013) Semantic parsing utterances to logical forms, which we leverage, plays an important role in these settings (Kollar et al., 2010; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013). What makes this work unique is our new interactive learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for example in semantic parsing (Zettlemoyer and Collins, 2007; Chen, 2012), we using it in a truly online setting, taking one pass over the data and measuring online accuracy (Cesa-Bianchi and Lugosi, 2006). To speed up learning, we leverage computational models of pragmatics (Jäger, 2008; Golland et al., 2010; Frank and Goodman, 2012; Smith et al., 2013; Vogel et al., 2013). The main difference is these previous works use pragmatics with a trained base model, whereas we learn the model online. Monroe and Potts (2015) uses learning to improve the pragmatics model. In contrast, we use pragmatics to speed up the learning process by capturing phenomena like mutual excl"
P16-1224,D10-1040,1,0.962541,"fic language. While we would not expect the computer to magically guess ‘remove cyan’ 7→ remove(with(cyan)), it should at least push down the probability of zrm-red because zrm-red intuitively is already well-explained by another utterance ‘remove red’. This phenomenon, mutual exclusivity, was studied by Markman and Wachtel (1988). They found that children, during their language acquisition process, reject a second label for an object and treat it instead as a label for a novel object. The pragmatic computer. To model mutual exclusivity formally, we turn to probabilistic models of pragmatics (Golland et al., 2010; Frank and Goodman, 2012; Smith et al., 2013; Goodman and Lassiter, 2015), which operationalize the ideas of Grice (1975). The central idea in these models is to treat language as a cooperative game between a speaker (human) and a listener (computer) as we are doing, but where the listener has an explicit model of the speaker’s strategy, which in turn models the listener. Formally, let S(x |z) be the speaker’s strategy and L(z |x) be the listener’s 2371 zrm-red zrm-cyan z3 , z4 , . . . pθ (z |x) ‘remove red’ 0.8 0.1 0.1 ‘remove cyan’ 0.6 0.2 0.2 S(x |z) ‘remove red’ 0.57 0.33 0.33 ‘remove cya"
P16-1224,P11-1060,1,0.851422,"0) = {h}, ψ(h, d) = {(h, i, ψ(h.i, d − 1)) |i = 1, 2, 3}. The set of all features is just the cross product of utterance features and logical form features. For example, if x = ‘enlever tout’ and z = remove(all()), then features include: (‘enlever’, all) (‘enlever’, remove) (‘enlever’, (remove, 1, all)) (‘tout’, (remove, 1, all)) (‘tout’, all) (‘tout’, remove) Note that we do not model an explicit alignment or derivation compositionally connecting the utterance and the logical form, in contrast to most traditional work in semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010; Berant et al., 2013), instead following a looser model of semantics similar to (Pasupat and Liang, 2015). Modeling explicit alignments or derivations is only computationally feasible when we are learning from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available. Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we construct a set of logical forms of size n (with ex"
P16-1224,P15-1142,1,0.850238,"res and logical form features. For example, if x = ‘enlever tout’ and z = remove(all()), then features include: (‘enlever’, all) (‘enlever’, remove) (‘enlever’, (remove, 1, all)) (‘tout’, (remove, 1, all)) (‘tout’, all) (‘tout’, remove) Note that we do not model an explicit alignment or derivation compositionally connecting the utterance and the logical form, in contrast to most traditional work in semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010; Berant et al., 2013), instead following a looser model of semantics similar to (Pasupat and Liang, 2015). Modeling explicit alignments or derivations is only computationally feasible when we are learning from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available. Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we construct a set of logical forms of size n (with exactly n predicates) by combining logical forms of smaller sizes according to the grammar rules in Table 1. For each n, we keep the 1"
P16-1224,N13-1127,0,0.021808,"d Zettlemoyer, 2013). What makes this work unique is our new interactive learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for example in semantic parsing (Zettlemoyer and Collins, 2007; Chen, 2012), we using it in a truly online setting, taking one pass over the data and measuring online accuracy (Cesa-Bianchi and Lugosi, 2006). To speed up learning, we leverage computational models of pragmatics (Jäger, 2008; Golland et al., 2010; Frank and Goodman, 2012; Smith et al., 2013; Vogel et al., 2013). The main difference is these previous works use pragmatics with a trained base model, whereas we learn the model online. Monroe and Potts (2015) uses learning to improve the pragmatics model. In contrast, we use pragmatics to speed up the learning process by capturing phenomena like mutual exclusivity (Markman and Wachtel, 1988). We also differ from prior work in several details. First, we model pragmatics in the online learning setting where we use an online update for the pragmatics model. Second, unlikely the reference games where pragmatic effects plays an important role by design, SHRDL"
P16-1224,P10-1083,0,0.0439218,"ho are less precise and consistent. This is expected behavior: the pragmatics model assumes that the human is cooperative and behaving rationally. For the bottom half of the players, this assumption is not true, in which case the pragmatics model is not useful. 6 Related Work and Discussion Our work connects with a broad body of work on grounded language, in which language is used in some environment as a means towards some goal. Examples include playing games (Branavan et al., 2009, 2010; Reckman et al., 2010) interacting with robotics (Tellex et al., 2011, 2014), and following instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013) Semantic parsing utterances to logical forms, which we leverage, plays an important role in these settings (Kollar et al., 2010; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013). What makes this work unique is our new interactive learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for example in semantic parsing (Zettlemoyer and Collins, 2007; Chen, 2012), we using it in a truly online setting, taking one pass over the data"
P16-1224,H89-1033,0,0.13034,"ge consisting of four words— ‘block’, ‘pillar’, ‘slab’, ‘beam’—to successfully communicate what block to pass from A to B. This is only one such language; many others would also work for accomplishing the cooperative goal. This paper operationalizes and explores the idea of language games in a learning setting, which we call interactive learning through language games (ILLG). In the ILLG setting, the two parties do not initially speak a common language, but nonetheless need to collaboratively accomplish a goal. Specifically, we created a game called SHRDLURN,1 in homage to the seminal work of Winograd (1972). As shown in Figure 1, the objective is to transform a start state into a goal state, but the only action the human can take is entering an utterance. The computer parses the utterance and produces a ranked list of possible interpretations according to its current model. The human scrolls through the list and chooses the intended one, simultaneously advancing the state of the blocks and providing feedback to the computer. Both the human and the computer wish to reach the goal state 1 Demo: http://shrdlurn.sidaw.xyz 2368 Proceedings of the 54th Annual Meeting of the Association for Computation"
P16-1224,P07-1121,0,0.0635806,"ively as follows: ψ(h, 0) = {h}, ψ(h, d) = {(h, i, ψ(h.i, d − 1)) |i = 1, 2, 3}. The set of all features is just the cross product of utterance features and logical form features. For example, if x = ‘enlever tout’ and z = remove(all()), then features include: (‘enlever’, all) (‘enlever’, remove) (‘enlever’, (remove, 1, all)) (‘tout’, (remove, 1, all)) (‘tout’, all) (‘tout’, remove) Note that we do not model an explicit alignment or derivation compositionally connecting the utterance and the logical form, in contrast to most traditional work in semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010; Berant et al., 2013), instead following a looser model of semantics similar to (Pasupat and Liang, 2015). Modeling explicit alignments or derivations is only computationally feasible when we are learning from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available. Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we construct a set of logical form"
P16-1224,D07-1071,0,0.141565,"010) interacting with robotics (Tellex et al., 2011, 2014), and following instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013) Semantic parsing utterances to logical forms, which we leverage, plays an important role in these settings (Kollar et al., 2010; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013). What makes this work unique is our new interactive learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for example in semantic parsing (Zettlemoyer and Collins, 2007; Chen, 2012), we using it in a truly online setting, taking one pass over the data and measuring online accuracy (Cesa-Bianchi and Lugosi, 2006). To speed up learning, we leverage computational models of pragmatics (Jäger, 2008; Golland et al., 2010; Frank and Goodman, 2012; Smith et al., 2013; Vogel et al., 2013). The main difference is these previous works use pragmatics with a trained base model, whereas we learn the model online. Monroe and Potts (2015) uses learning to improve the pragmatics model. In contrast, we use pragmatics to speed up the learning process by capturing phenomena lik"
P16-1224,Q13-1005,0,\N,Missing
P16-5005,D13-1176,0,\N,Missing
P16-5005,W04-3248,0,\N,Missing
P16-5005,W03-1502,0,\N,Missing
P16-5005,P04-1021,0,\N,Missing
P16-5005,P15-1001,1,\N,Missing
P16-5005,P02-1040,0,\N,Missing
P16-5005,P15-1002,1,\N,Missing
P16-5005,P08-1045,0,\N,Missing
P16-5005,W09-0438,0,\N,Missing
P16-5005,P13-1059,0,\N,Missing
P16-5005,J98-4003,0,\N,Missing
P16-5005,P16-1162,0,\N,Missing
P16-5005,D07-1091,0,\N,Missing
P16-5005,P09-1067,0,\N,Missing
P16-5005,P07-2045,0,\N,Missing
P16-5005,W04-3250,0,\N,Missing
P17-1086,D10-1119,0,0.215456,"vation. ID features fire on specific rules (by ID). Type features track whether a rule is part of the core language or induced, whether it has been 5 Grammar induction Recall that the main form of supervision is via user definitions, which allows creation of user-defined concepts. In this section, we show how to turn 933 these definitions into new grammar rules that can be used by the system to parse new utterances. Previous systems of grammar induction for semantic parsing were given utterance-program pairs (x, z). Both the GENLEX (Zettlemoyer and Collins, 2005) and higher-order unification (Kwiatkowski et al., 2010) algorithms overgenerate rules that liberally associate parts of x with parts of z. Though some rules are immediately pruned, many spurious rules are undoubtedly still kept. In the interactive setting, we must keep the number of candidates small to avoid a bad user experience, which means a higher precision bar for new rules. Fortunately, the structure of definitions makes the grammar induction task easier. Rather than being given an utterance-program (x, z) pair, we are given a definition, which consists of an utterance x (head) along with the body X = [x1 , . . . , xn ], which is a sequence"
P17-1086,D11-1039,0,0.0416967,"t al., 2017) and robots (Tellex et al., 2011), people need computers to perform well-specified but complex actions. To accomplish this, one route is to use a programming language, but this is inaccessible to most and can be tedious even for experts because the syntax is uncompromising and all statements have to be precise. Another route is to convert natural language into a formal lanFigure 1: Some examples of users building structures using a naturalized language in Voxelurn: http://www.voxelurn.com guage, which has been the subject of work in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011, 2013; Pasupat and Liang, 2015). However, the capability of semantic parsers is still quite primitive compared to the power one wields with a programming language. This gap is increasingly limiting the potential of 929 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 929–938 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1086 can be used by another user. Thus a community of users evolves the language to becomes more efficient over time, in a distributed way, through int"
P17-1086,Q13-1005,0,0.139804,"Missing"
P17-1086,P16-1224,1,0.924518,"is an unnatural concept for non-programmers. Therefore when the choice is not explicit, the parser generates all three possible scoping interpretations, and the model learns which is intended based on the user, the rule, and potentially the context. 3 def: add palm tree def: brown trunk height 3 def: add brown top 3 times repeat 3 [add brown top] def: go to top of tree select very top of has color brown def: add leaves here def: select all sides select left or right or front or back add green Learning interactively from definitions The goal of the user is to build a structure in Voxelurn. In Wang et al. (2016), the user provided interactive supervision to the system by selecting from a list of candidates. This is practical when there are less than tens of candidates, but is completely infeasible for a complex action space such as Voxelurn. Roughly, 10 possible colors over the 3 × 3 × 4 box containing the palm tree in Figure 2 yields 1036 distinct denotations, and many more programs. Obtaining the structures in Figure 1 by selecting candidates alone would be infeasible. This work thus uses definitions in addition to selecting candidates as the supervision signal. Each definition consists of a head u"
P17-1086,D13-1160,1,0.884347,"ward – add green monster – go down 8 – go right and front – add brown floor – add girl – go back and down – add door – add black column 30 – go up 9 – finish door – (some steps for moving are omitted) Deer: initial – bird’s eye view – deer head; up; left 2; back 2; { left antler }; right 2; {right antler} – down 4; front 2; left 3; deer body; down 6; {deer leg front}; back 7; {deer leg back}; left 4; {deer leg back}; front 7; {deer leg front} – (some steps omitted) Introduction In tasks such as analyzing and plotting data (Gulwani and Marron, 2014), querying databases (Zelle and Mooney, 1996; Berant et al., 2013), manipulating text (Kushman and Barzilay, 2013), or controlling the Internet of Things (Campagna et al., 2017) and robots (Tellex et al., 2011), people need computers to perform well-specified but complex actions. To accomplish this, one route is to use a programming language, but this is inaccessible to most and can be tedious even for experts because the syntax is uncompromising and all statements have to be precise. Another route is to convert natural language into a formal lanFigure 1: Some examples of users building structures using a naturalized language in Voxelurn: http://www.voxelurn"
P17-1086,D07-1071,0,0.222975,"Missing"
P17-1086,N13-1103,0,0.0160373,"right and front – add brown floor – add girl – go back and down – add door – add black column 30 – go up 9 – finish door – (some steps for moving are omitted) Deer: initial – bird’s eye view – deer head; up; left 2; back 2; { left antler }; right 2; {right antler} – down 4; front 2; left 3; deer body; down 6; {deer leg front}; back 7; {deer leg back}; left 4; {deer leg back}; front 7; {deer leg front} – (some steps omitted) Introduction In tasks such as analyzing and plotting data (Gulwani and Marron, 2014), querying databases (Zelle and Mooney, 1996; Berant et al., 2013), manipulating text (Kushman and Barzilay, 2013), or controlling the Internet of Things (Campagna et al., 2017) and robots (Tellex et al., 2011), people need computers to perform well-specified but complex actions. To accomplish this, one route is to use a programming language, but this is inaccessible to most and can be tedious even for experts because the syntax is uncompromising and all statements have to be precise. Another route is to convert natural language into a formal lanFigure 1: Some examples of users building structures using a naturalized language in Voxelurn: http://www.voxelurn.com guage, which has been the subject of work i"
P17-1086,P15-1142,1,\N,Missing
P17-1086,J13-2005,1,\N,Missing
P17-1099,D14-1085,0,0.00746156,"in the CNN/Daily Mail dataset, and provided the first abstractive baselines. The same authors then published a neural extractive approach (Nallapati et al., 2017), which uses hierarchical RNNs to select sentences, and found that it significantly outperformed their abstractive result with respect to the ROUGE metric. To our knowledge, these are the only two published results on the full dataset. Prior to modern neural methods, abstractive summarization received less attention than extractive summarization, but Jing (2000) explored cutting unimportant parts of sentences to create summaries, and Cheung and Penn (2014) explore sentence fusion using dependency trees. Pointer-generator networks. The pointer network (Vinyals et al., 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al. (2015) to produce an output sequence consisting of elements from 1076 the input sequence. The pointer network has been used to create hybrid approaches for NMT (Gulcehre et al., 2016), language modeling (Merity et al., 2016), and summarization (Gu et al., 2016; Gulcehre et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016; Zeng et al., 2016). Our approach is close to the Forced"
P17-1099,N16-1012,0,0.935287,"against Argentina on Saturday Vocabulary Distribution Encoder Attention Hidden States Distribution ""beat"" ... <START&gt; Germany Source Text Partial Summary Figure 2: Baseline sequence-to-sequence model with attention. The model may attend to relevant words in the source text to generate novel words, e.g., to produce the novel word beat in the abstractive summary Germany beat Argentina 2-0 the model may attend to the words victorious and win in the source text. et al., 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016). Though these systems are promising, they exhibit undesirable behavior such as inaccurately reproducing factual details, an inability to deal with out-of-vocabulary (OOV) words, and repeating themselves (see Figure 1). In this paper we present an architecture that addresses these three issues in the context of multi-sentence summaries. While most recent abstractive work has focused on headline generation tasks (reducing one or two sentences to a single headline), we believe that longer-text summarization is both more challenging ("
P17-1099,W14-3348,0,0.209625,"as a separate training phase, but found that in the early phase of training, the coverage objective interfered with the main objective, reducing overall performance. Results 6.1 Preliminaries Our results are given in Table 1. We evaluate our models with the standard ROUGE metric (Lin, 2004b), reporting the F1 scores for ROUGE1, ROUGE-2 and ROUGE-L (which respectively measure the word-overlap, bigram-overlap, and longest common sequence between the reference summary and the summary to be evaluated). We obtain our ROUGE scores using the pyrouge package.4 We also evaluate with the METEOR metric (Denkowski and Lavie, 2014), both in exact match mode (rewarding only exact matches between words) and full mode (which additionally rewards matching stems, synonyms and paraphrases).5 In addition to our own models, we also report the lead-3 baseline (which uses the first three sentences of the article as a summary), and compare to the only existing abstractive (Nallapati et al., 2016) and extractive (Nallapati et al., 2017) models on the full dataset. The output of our models is available online.6 Given that we generate plain-text summaries but Nallapati et al. (2016; 2017) generate anonymized summaries (see Section 4)"
P17-1099,W04-1013,0,0.330946,"39 sentences on average, there are many equally valid ways to choose 3 or 4 highlights in this style. Abstraction introduces even more options (choice of phrasing), further decreasing the likelihood of matching the reference summary. For example, smugglers profit from desperate migrants is a valid alternative abstractive summary for the first example in Figure 5, but it scores 0 ROUGE with respect to the reference summary. This inflexibility of ROUGE is exacerbated by only having one reference summary, which has been shown to lower ROUGE’s reliability compared to multiple reference summaries (Lin, 2004a). Due to the subjectivity of the task and thus the diversity of valid summaries, it seems that ROUGE rewards safe strategies such as selecting the first-appearing content, or preserving original phrasing. While the reference summaries do sometimes deviate from these techniques, those deviations are unpredictable enough that the safer strategy obtains higher ROUGE scores on average. This may explain why extractive systems tend to obtain higher ROUGE scores than abstractive, and even extractive systems do not significantly exceed the lead-3 baseline. To explore this issue further, we evaluated"
P17-1099,D16-1096,0,0.0516496,"(7), but with respect to our modified probability distribution P(w) given in equation (9). 2.3 Our loss function is more flexible: because summarization should not require uniform coverage, we only penalize the overlap between each attention distribution and the coverage so far – preventing repeated attention. Finally, the coverage loss, reweighted by some hyperparameter λ , is added to the primary loss function to yield a new composite loss function: losst = − log P(wt∗ ) + λ ∑i min(ati , cti ) Coverage mechanism Repetition is a common problem for sequenceto-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1). We adapt the coverage model of Tu et al. (2016) to solve the problem. In our coverage model, we maintain a coverage vector ct , which is the sum of attention distributions over all previous decoder timesteps: t−1 ct = ∑t 0 =0 at 0 (10) Intuitively, ct is a (unnormalized) distribution over the source document words that represents the degree of coverage that those words have received from the attention mechanism so far. Note that c0 is a zero vector, because on the"
P17-1099,D16-1031,0,0.81163,"cordingly the final coverage vector is penalized if it is more or less than 1. 3 (13) Related Work Neural abstractive summarization. Rush et al. (2015) were the first to apply modern neural networks to abstractive text summarization, achieving state-of-the-art performance on DUC-2004 and Gigaword, two sentence-level summarization datasets. Their approach, which is centered on the attention mechanism, has been augmented with recurrent decoders (Chopra et al., 2016), Abstract Meaning Representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), and direct optimization of the performance metric (Ranzato et al., 2016), further improving performance on those datasets. However, large-scale datasets for summarization of longer text are rare. Nallapati et al. (2016) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization, resulting in the CNN/Daily Mail dataset, and provided the first abstractive baselines. The same authors then published a neural extractive approach (Nallapati et al., 2017), which uses hierarchical RNNs to select sentences, and found that it significantly outperformed their abstractive r"
P17-1099,P16-1154,0,0.588612,"than extractive summarization, but Jing (2000) explored cutting unimportant parts of sentences to create summaries, and Cheung and Penn (2014) explore sentence fusion using dependency trees. Pointer-generator networks. The pointer network (Vinyals et al., 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al. (2015) to produce an output sequence consisting of elements from 1076 the input sequence. The pointer network has been used to create hybrid approaches for NMT (Gulcehre et al., 2016), language modeling (Merity et al., 2016), and summarization (Gu et al., 2016; Gulcehre et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016; Zeng et al., 2016). Our approach is close to the Forced-Attention Sentence Compression model of Miao and Blunsom (2016) and the CopyNet model of Gu et al. (2016), with some small differences: (i) We calculate an explicit switch probability pgen , whereas Gu et al. induce competition through a shared softmax function. (ii) We recycle the attention distribution to serve as the copy distribution, but Gu et al. use two separate distributions. (iii) When a word appears multiple times in the source text, we sum probability mass"
P17-1099,D15-1044,0,0.191464,"tribution Encoder Attention Hidden States Distribution ""beat"" ... <START&gt; Germany Source Text Partial Summary Figure 2: Baseline sequence-to-sequence model with attention. The model may attend to relevant words in the source text to generate novel words, e.g., to produce the novel word beat in the abstractive summary Germany beat Argentina 2-0 the model may attend to the words victorious and win in the source text. et al., 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016). Though these systems are promising, they exhibit undesirable behavior such as inaccurately reproducing factual details, an inability to deal with out-of-vocabulary (OOV) words, and repeating themselves (see Figure 1). In this paper we present an architecture that addresses these three issues in the context of multi-sentence summaries. While most recent abstractive work has focused on headline generation tasks (reducing one or two sentences to a single headline), we believe that longer-text summarization is both more challenging (requiring higher levels of abstraction whil"
P17-1099,A00-1043,0,0.138433,"pMind question-answering dataset (Hermann et al., 2015) for summarization, resulting in the CNN/Daily Mail dataset, and provided the first abstractive baselines. The same authors then published a neural extractive approach (Nallapati et al., 2017), which uses hierarchical RNNs to select sentences, and found that it significantly outperformed their abstractive result with respect to the ROUGE metric. To our knowledge, these are the only two published results on the full dataset. Prior to modern neural methods, abstractive summarization received less attention than extractive summarization, but Jing (2000) explored cutting unimportant parts of sentences to create summaries, and Cheung and Penn (2014) explore sentence fusion using dependency trees. Pointer-generator networks. The pointer network (Vinyals et al., 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al. (2015) to produce an output sequence consisting of elements from 1076 the input sequence. The pointer network has been used to create hybrid approaches for NMT (Gulcehre et al., 2016), language modeling (Merity et al., 2016), and summarization (Gu et al., 2016; Gulcehre et al., 2016; Miao an"
P17-1099,P09-5002,0,0.0298501,"words or named entities (whereas we allow our model to freely learn when to use the pointer), and they do not mix the probabilities from the copy distribution and the vocabulary distribution. We believe the mixture approach described here is better for abstractive summarization – in section 6 we show that the copy mechanism is vital for accurately reproducing rare but in-vocabulary words, and in section 7.2 we observe that the mixture model enables the language model and copy mechanism to work together to perform abstractive copying. Coverage. Originating from Statistical Machine Translation (Koehn, 2009), coverage was adapted for NMT by Tu et al. (2016) and Mi et al. (2016), who both use a GRU to update the coverage vector each step. We find that a simpler approach – summing the attention distributions to obtain the coverage vector – suffices. In this respect our approach is similar to Xu et al. (2015), who apply a coverage-like method to image captioning, and Chen et al. (2016), who also incorporate a coverage mechanism (which they call ‘distraction’) as described in equation (11) into neural summarization of longer text. Temporal attention is a related technique that has been applied to NMT"
P17-1099,D16-1112,0,0.0922688,"ine Translation. In MT, we assume that there should be a roughly oneto-one translation ratio; accordingly the final coverage vector is penalized if it is more or less than 1. 3 (13) Related Work Neural abstractive summarization. Rush et al. (2015) were the first to apply modern neural networks to abstractive text summarization, achieving state-of-the-art performance on DUC-2004 and Gigaword, two sentence-level summarization datasets. Their approach, which is centered on the attention mechanism, has been augmented with recurrent decoders (Chopra et al., 2016), Abstract Meaning Representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), and direct optimization of the performance metric (Ranzato et al., 2016), further improving performance on those datasets. However, large-scale datasets for summarization of longer text are rare. Nallapati et al. (2016) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization, resulting in the CNN/Daily Mail dataset, and provided the first abstractive baselines. The same authors then published a neural extractive approach (Nallapati et al., 2017), which uses hier"
P17-1099,P16-1014,0,\N,Missing
P17-1099,K16-1028,0,\N,Missing
P17-2018,D14-1082,1,0.840483,"eager and arc-swift to induce the correct parse. The state shown is generated by the first six transitions of both systems. However, the transition systems employed in state-of-the-art dependency parsers usually define very local transitions. At each step, only one or two words are affected, with very local attachments made. As a result, distant attachments require long and not immediately obvious transition sequences (e.g., ate→chopsticks in Figure 1, which requires two transitions). This is further aggravated by the usually local lexical information leveraged to make transition predictions (Chen and Manning, 2014; Andor et al., 2016). Introduction Dependency parsing is a longstanding natural language processing task, with its outputs crucial to various downstream tasks including relation extraction (Schmitz et al., 2012; Angeli et al., 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al., 2016). Attractive for their linear time complexity and amenability to conventional classification methods, transition-based dependency parsers have sparked much research interest recently. A transition-based parser makes sequential predictions of transitions between states"
P17-2018,H05-1066,0,0.25272,"Missing"
P17-2018,W08-1301,1,0.621812,"Missing"
P17-2018,D07-1013,0,0.090339,"2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al., 2016). Attractive for their linear time complexity and amenability to conventional classification methods, transition-based dependency parsers have sparked much research interest recently. A transition-based parser makes sequential predictions of transitions between states under the restrictions of a transition system (Nivre, 2003). Transition-based parsers have been shown to excel at parsing shorter-range dependency structures, as well as languages where non-projective parses are less pervasive (McDonald and Nivre, 2007). In this paper, we introduce a novel transition system, arc-swift, which defines non-local transitions that directly induce attachments of distance up to n (n = the number of tokens in the sentence). Such an approach is connected to graph-based dependency parsing, in that it leverages pairwise scores between tokens in making parsing decisions (McDonald et al., 2005). We make two main contributions in this paper. Firstly, we introduce a novel transition system for dependency parsing, which alleviates the difficulty of distant attachments in previous systems by allowing direct attachments anywh"
P17-2018,D12-1029,0,0.54339,"Missing"
P17-2018,W03-3017,0,0.84444,"parsing is a longstanding natural language processing task, with its outputs crucial to various downstream tasks including relation extraction (Schmitz et al., 2012; Angeli et al., 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al., 2016). Attractive for their linear time complexity and amenability to conventional classification methods, transition-based dependency parsers have sparked much research interest recently. A transition-based parser makes sequential predictions of transitions between states under the restrictions of a transition system (Nivre, 2003). Transition-based parsers have been shown to excel at parsing shorter-range dependency structures, as well as languages where non-projective parses are less pervasive (McDonald and Nivre, 2007). In this paper, we introduce a novel transition system, arc-swift, which defines non-local transitions that directly induce attachments of distance up to n (n = the number of tokens in the sentence). Such an approach is connected to graph-based dependency parsing, in that it leverages pairwise scores between tokens in making parsing decisions (McDonald et al., 2005). We make two main contributions in t"
P17-2018,W04-0308,0,0.261888,"e 1 for an example). Parser states are usually written as (σ|i, j|β, A), where σ|i denotes the stack with token i on the top, j|β denotes the buffer with token j at its leftmost, and A the set of dependency arcs. Given a state, the goal of a dependency parser is to predict a transition to a new state that would lead to the correct parse. A transition system defines a set of transitions that are sound and complete for parsers, that is, every transition sequence would derive a well-formed parse tree, and every possible parse tree can also be derived from some transition sequence.1 Arc-standard (Nivre, 2004) is one of the first transition systems proposed for dependency parsing. It defines three transitions: shift, left arc (LArc), and right arc (RArc) (see Figure 2 for definitions, same for the following transition systems), where all arc-inducing transitions operate on the stack. This system builds the parse bottom-up, i.e., a constituent is only attached to its head after it has received all of its dependents. A potential drawback is that during parsing, it is difficult to predict if a constituent has consumed all of its right dependents. Arc-eager (Nivre, 2003) remedies this drawback by defin"
P17-2018,C12-1059,0,0.0498042,"ons in arc-eager. arc-swift is also equivalent to arc-eager in terms of soundness and completeness.4 A caveat is that the worst-case time complexity of arc-swift is O(n2 ) instead of O(n), which existing transition-based parsers enjoy. However, in practice the runtime is nearly 4.2 Results We use static oracles for all transition systems, and for arc-eager we implement oracles that always Shift/Reduce when ambiguity is present (arceager-S/R). We evaluate our parsers with greedy parsing (i.e., beam size 1). The results are shown in Table 1.5 Note that K&G 2016 is trained with a dynamic oracle (Goldberg and Nivre, 2012), Andor 2016 with a CRF-like loss, and both Andor 2016 and Weiss 2015 employed beam search (with sizes 32 and 8, respectively). For each pair of the systems we implemented, we studied the statistical significance of their difference by performing a paired test with 10,000 bootstrap samples on PTB-SD. The resulting pvalues are analyzed with a 10-group BonferroniHolm test, with results shown in Table 2. We note 4 This is easy to show because in arc-eager, all Reduce transitions can be viewed as preparing for a later LArc or RArc transition. We also note that similar to arc-eager transitions, arc"
P17-2018,L16-1262,1,0.871566,"Missing"
P17-2018,J13-4002,0,0.116536,"Missing"
P17-2018,D13-1143,0,0.0313318,"ds are affected, with very local attachments made. As a result, distant attachments require long and not immediately obvious transition sequences (e.g., ate→chopsticks in Figure 1, which requires two transitions). This is further aggravated by the usually local lexical information leveraged to make transition predictions (Chen and Manning, 2014; Andor et al., 2016). Introduction Dependency parsing is a longstanding natural language processing task, with its outputs crucial to various downstream tasks including relation extraction (Schmitz et al., 2012; Angeli et al., 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al., 2016). Attractive for their linear time complexity and amenability to conventional classification methods, transition-based dependency parsers have sparked much research interest recently. A transition-based parser makes sequential predictions of transitions between states under the restrictions of a transition system (Nivre, 2003). Transition-based parsers have been shown to excel at parsing shorter-range dependency structures, as well as languages where non-projective parses are less pervasive (McDonald and Nivre, 2007). In this paper, we introdu"
P17-2018,D14-1162,1,0.109058,"Missing"
P17-2018,P13-1014,0,0.098403,"em for non-projective parsing, with arc-inducing transitions that are very similar to those in arc-swift. A notable difference is that their transitions retain tokens between the head and dependent. Fern´andez-Gonz´alez and G´omezRodr´ıguez (2012) augmented the arc-eager system with transitions that operate on the buffer, which shorten the transition sequence by reducing the number of Shift transitions needed. However, limited by the sparse feature-based classifiers used, both of these parsers just mentioned only allow direct attachments of distance up to 3 and 2, respectively. More recently, Sartorio et al. (2013) extended arc-standard with transitions that directly attach to left and right “spines” of the top two nodes in the stack. While this work shares very similar motivations as arc-swift, it requires additional data structures to keep track of the left and right spines of nodes. This transition system also introduces spurious ambiguity where multiple transition sequences could lead to the same correct parse, which necessitates easy-first training to achieve a more noticeable improvement over arcstandard. In contrast, arc-swift can be easily implemented given the parser state alone, and does not g"
P17-2018,Q16-1023,0,0.0847696,"ly small number of reducible tokens in the stack. &lt;root&gt; I ate fish stack with * . 4 Experiments buffer 4.1 b &lt;root&gt; I ate fish stack Data and Model We use the Wall Street Journal portion of Penn Treebank with standard parsing splits (PTBSD), along with Universal Dependencies v1.3 (Nivre et al., 2016) (EN-UD). PTB-SD is converted to Stanford Dependencies (De Marneffe and Manning, 2008) with CoreNLP 3.3.0 (Manning et al., 2014) following previous work. We report labelled and unlabelled attachment scores (LAS/UAS), removing punctuation from all evaluations. Our model is very similar to that of (Kiperwasser and Goldberg, 2016), where features are extracted from tokens with bidirectional LSTMs, and concatenated for classification. For the three traditional transition systems, features of the top 3 tokens on the stack and the leftmost token in the buffer are concatenated as classifier input. For arc-swift, features of the head and dependent tokens for each arc-inducing transition are concatenated to compute scores for classification, and features of the leftmost buffer token is used for Shift. For other details we defer to Appendix A. The full specification of the model can also be found in our released code online a"
P17-2018,D12-1048,0,0.0248889,"efine very local transitions. At each step, only one or two words are affected, with very local attachments made. As a result, distant attachments require long and not immediately obvious transition sequences (e.g., ate→chopsticks in Figure 1, which requires two transitions). This is further aggravated by the usually local lexical information leveraged to make transition predictions (Chen and Manning, 2014; Andor et al., 2016). Introduction Dependency parsing is a longstanding natural language processing task, with its outputs crucial to various downstream tasks including relation extraction (Schmitz et al., 2012; Angeli et al., 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al., 2016). Attractive for their linear time complexity and amenability to conventional classification methods, transition-based dependency parsers have sparked much research interest recently. A transition-based parser makes sequential predictions of transitions between states under the restrictions of a transition system (Nivre, 2003). Transition-based parsers have been shown to excel at parsing shorter-range dependency structures, as well as languages where non-projective parses are"
P17-2018,P15-1034,1,\N,Missing
P17-2018,P16-1015,0,\N,Missing
P18-2077,J02-3001,0,0.2827,"arsing and labeling decisions sequentially, choosing the labels for each edge only after the edges in the tree have been finalized by an MST algorithm. Wang et al. (2018) take a different approach in their recent work, using a transition-based parser built on stack-LSTMs (Dyer et al., 2015). They extend Choi and McCallum’s (2013) transition system for producing non-projective trees so that it can produce arbitrary DAGs and they modify the stack-LSTM architecture slightly to make the network more powerful. to reflect semantic relationships—such as agent and patient (cf. semantic role labeling (Gildea and Jurafsky, 2002)). The SemEval semantic dependency schemes are also directed acyclic graphs (DAGs) rather than trees, allowing them to annotate function words as being heads without lengthening paths between content words (as in 1b). 2.2 Related work Our approach to semantic dependency parsing is primarily inspired by the success of Dozat and Manning (2017) and Dozat et al. (2017) at syntactic dependency parsing and Peng et al. (2017) at semantic dependency parsing. In Dozat and Manning (2017) and Peng et al. (2017), parsing involves first using a multilayer bidirectional LSTM over word and part-of-speech tag"
P18-2077,S15-2162,0,0.418166,"Missing"
P18-2077,hajic-etal-2012-announcing,0,0.375717,"Missing"
P18-2077,D16-1211,0,0.0125808,"positive score. The labeler scores every label for each pair of words, so we simply assign each predicted edge its highest-scoring label and discard the rest. We can train the system by summing the losses from the two modules, backpropagating error to the labeler only through edges with a non-null gold label. This system is shown graphically in Figure 2. We find that sometimes the loss for one module overwhelms the loss for the other, causing the system to underfit. Thus we add a tunable interpolation constant λ ∈ (0, 1) to even out the two losses. ` = λ`(label) + (1 − λ)`(edge) Augmentations Ballesteros et al. (2016), Dozat et al. (2017), and Ma et al. (2018) find that character-level word embedding models improve performance for syntactic dependency parsing, so we also want to explore the impact it has on semantic dependency parsing. Dozat et al. (2017) confirm that their syntactic parser performs better with POS tags, which leads us to examine whether word lemmas—another form of low-level lexical information—might also improve dependency parsing performance.   (label-dep) (label-head) , hj = Biaff(label) hi (10) = {si,j ≥ 0} ... Embed (3) Bilin(x1 , x2 ) = x> 1 Ux2 Labels 4 4.1 Results Hyperparameters"
P18-2077,C16-1038,0,0.0126464,"−3 Adam β1 0 Adam β2 .95 4.2 Performance We use biaffine classifiers, with no nonlinearities, and a diagonal tensor in the label classifier but not the edge classifier. The system trains at a speed of about 300 sequences/second on an nVidia Titan X and parses about 1,000 sequences/second. Du et al. (2015) and Almeida and Martins (2015) are the systems that won the 2015 shared task (closed track). PTS17: Basic represents the single-task versions of Peng et al. (2017), which they make multitask across the three datasets in Freda3 by adding frustratingly easy domain adaptation (Daum´e III, 2007; Kim et al., 2016) and a third-order decoding mechanism. WCGL18 is Wang et al.’s (2018) transition-based system. Table 1 compares our performance with these systems. Our fully factorized basic system already substantially outperforms Peng et al.’s single-task baseline and also beats out their much more complex multi-task approach. Simply adding in either a character-level word embedding model (similar to Dozat et al.’s (2017)) or a lemma embedding matrix likewise improves performance quite a bit, and including both together generally pushes performance even higher. Many infrequent words were excluded from the f"
P18-2077,W17-6507,0,0.018371,"formalisms, applied to the Penn Treebank (shown in Figure 1, compared to Universal Dependencies (Nivre et al., 2016)): DELPH-IN MRS, or DM (Flickinger et al., 2012; Oepen and Lønning, 2006); Predicate-Argument Structures, or PAS (Miyao and Tsujii, 2004); and Prague Semantic Dependencies, or PSD (Hajic et al., 2012). Whereas syntactic dependencies generally annotate functional relationships between words—such as subject and object—semantic dependencies aim 1 Though efforts have been made to address this limitation; seeDe Marneffe et al. (2006); Nivre et al. (2016); Schuster and Manning (2016); Candito et al. (2017) for examples. 484 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 484–490 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ROOT VERB ARG 2 TOP XCOMP NSUBJ MARK OBJ DET Mary wants to buy a book NSUBJ (a) UD (with enhanced dependencies dashed) VERB ARG 1 TOP ARG 1 ARG 1 ARG 2 ARG 2 COMP ARG 1 TOP ACT- ARG Mary wants to buy a book BV VERB ARG 2 Mary wants to buy a book ACT- ARG DET ARG 1 PAT- ARG Mary wants to buy a book VERB ARG 1 (b) DM PAT- ARG (c) PAS (d) PSD Figure 1: Comparison between s"
P18-2077,P13-1104,0,0.120371,"Missing"
P18-2077,P18-1130,0,0.0228572,"ach pair of words, so we simply assign each predicted edge its highest-scoring label and discard the rest. We can train the system by summing the losses from the two modules, backpropagating error to the labeler only through edges with a non-null gold label. This system is shown graphically in Figure 2. We find that sometimes the loss for one module overwhelms the loss for the other, causing the system to underfit. Thus we add a tunable interpolation constant λ ∈ (0, 1) to even out the two losses. ` = λ`(label) + (1 − λ)`(edge) Augmentations Ballesteros et al. (2016), Dozat et al. (2017), and Ma et al. (2018) find that character-level word embedding models improve performance for syntactic dependency parsing, so we also want to explore the impact it has on semantic dependency parsing. Dozat et al. (2017) confirm that their syntactic parser performs better with POS tags, which leads us to examine whether word lemmas—another form of low-level lexical information—might also improve dependency parsing performance.   (label-dep) (label-head) , hj = Biaff(label) hi (10) = {si,j ≥ 0} ... Embed (3) Bilin(x1 , x2 ) = x> 1 Ux2 Labels 4 4.1 Results Hyperparameters We tuned the hyperparameters for our basic"
P18-2077,D11-1022,0,0.164444,"r each graph coming from several sources. First, it scores each word as either taking dependents or not. For each ordered pair of words, it scores the arc from the first word to the second. Lastly, it scores each possible labeled arc between the two words. The graph that maximizes these scores may not be consistent, with an edge coming from a non-predicate, for example, so they enforce hard constraints in order to prune away invalid semantic graphs. Decisions are not independent, so in order to find the highest-scoring graph that follows these constraints, they use the AD3 decoding algorithm (Martins et al., 2011). Dozat and Manning’s (2017) approach to syntactic dependency parsing is similar, but avoids the possibility of generating invalid trees by fully factorizing the system. Rather than summing the scores from multiple modules and then finding the valid structure that maximizes that sum, the sys3 3.1 Approach Basic approach We can formulate the semantic dependency parsing task as labeling each edge in a directed graph, with null being the label given to pairs with no edge between them. Using only one module that labels each edge in this way would be an unfactorized approach. We can, however, facto"
P18-2077,P07-1033,0,0.207446,"Missing"
P18-2077,de-marneffe-etal-2006-generating,1,0.175928,"Missing"
P18-2077,C04-1204,0,0.302767,"nce (as in Figure 1a) can only represent one of them.1 The 2014 SemEval shared task on BroadCoverage Semantic Dependency Parsing (Oepen et al., 2014) introduced three new dependency representations that do away with the assumption of 2 2.1 Background Semantic dependencies The 2014 SemEval (Oepen et al., 2014, 2015) shared task introduced three new semantic dependency formalisms, applied to the Penn Treebank (shown in Figure 1, compared to Universal Dependencies (Nivre et al., 2016)): DELPH-IN MRS, or DM (Flickinger et al., 2012; Oepen and Lønning, 2006); Predicate-Argument Structures, or PAS (Miyao and Tsujii, 2004); and Prague Semantic Dependencies, or PSD (Hajic et al., 2012). Whereas syntactic dependencies generally annotate functional relationships between words—such as subject and object—semantic dependencies aim 1 Though efforts have been made to address this limitation; seeDe Marneffe et al. (2006); Nivre et al. (2016); Schuster and Manning (2016); Candito et al. (2017) for examples. 484 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 484–490 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ROOT"
P18-2077,W03-3017,0,0.18491,"hough it could push performance even higher). We also find easier or independently motivated ways to improve accuracy—taking advantage of provided lemma or subtoken information provides a boost comparable to one found by drastically increasing system complexity. Further, we observe a high-performing graphbased parser can be adapted to different types of dependency graphs (projective tree, nonprojective tree, directed graph) with only small changes without obviously hurting accuracy. By contrast, transition-based parsers—which were originally designed for parsing projective constituency trees (Nivre, 2003; Aho and Ullman, 1972)—require whole new transition sets or even data structures to generate arbitrary graphs. We feel that this points to graph-based parsers being the most natural way to produce dependency graphs with different structural restrictions. Variations We also consider the impact that slight variations on basic architecture have on final performance in Figure 3. We train twenty models on the DM treebank for each variation we consider, reducing the number of training steps but keeping all other hyperparameters constant. Rank-sum tests (Lehmann et al., 1975) reveal that the basic s"
P18-2077,K17-3002,1,0.924772,"tive trees so that it can produce arbitrary DAGs and they modify the stack-LSTM architecture slightly to make the network more powerful. to reflect semantic relationships—such as agent and patient (cf. semantic role labeling (Gildea and Jurafsky, 2002)). The SemEval semantic dependency schemes are also directed acyclic graphs (DAGs) rather than trees, allowing them to annotate function words as being heads without lengthening paths between content words (as in 1b). 2.2 Related work Our approach to semantic dependency parsing is primarily inspired by the success of Dozat and Manning (2017) and Dozat et al. (2017) at syntactic dependency parsing and Peng et al. (2017) at semantic dependency parsing. In Dozat and Manning (2017) and Peng et al. (2017), parsing involves first using a multilayer bidirectional LSTM over word and part-of-speech tag embeddings. Parsing is then done using directly-optimized selfattention over recurrent states to attend to each word’s head (or heads), and labeling is done with an analgous multi-class classifier. Peng et al.’s (2017) system uses a max-margin classifer on top of a BiLSTM, with the score for each graph coming from several sources. First, it scores each word as eit"
P18-2077,S15-2154,0,0.282412,"Missing"
P18-2077,P15-1033,0,0.0254121,"dependencies dashed) VERB ARG 1 TOP ARG 1 ARG 1 ARG 2 ARG 2 COMP ARG 1 TOP ACT- ARG Mary wants to buy a book BV VERB ARG 2 Mary wants to buy a book ACT- ARG DET ARG 1 PAT- ARG Mary wants to buy a book VERB ARG 1 (b) DM PAT- ARG (c) PAS (d) PSD Figure 1: Comparison between syntactic and semantic dependency schemes tem makes parsing and labeling decisions sequentially, choosing the labels for each edge only after the edges in the tree have been finalized by an MST algorithm. Wang et al. (2018) take a different approach in their recent work, using a transition-based parser built on stack-LSTMs (Dyer et al., 2015). They extend Choi and McCallum’s (2013) transition system for producing non-projective trees so that it can produce arbitrary DAGs and they modify the stack-LSTM architecture slightly to make the network more powerful. to reflect semantic relationships—such as agent and patient (cf. semantic role labeling (Gildea and Jurafsky, 2002)). The SemEval semantic dependency schemes are also directed acyclic graphs (DAGs) rather than trees, allowing them to annotate function words as being heads without lengthening paths between content words (as in 1b). 2.2 Related work Our approach to semantic depen"
P18-2077,S15-2153,0,0.349511,"Missing"
P18-2077,S14-2008,0,0.262264,"tically extracting the low-level relationships between words in a sentence for use in natural language understanding tasks. However, typical syntactic dependency frameworks are limited in the number and types of relationships that can be captured. For example, in the sentence Mary wants to buy a book, the word Mary is the subject of both want and buy—either or both relationships could be useful in a downstream task, but a tree-structured representation of this sentence (as in Figure 1a) can only represent one of them.1 The 2014 SemEval shared task on BroadCoverage Semantic Dependency Parsing (Oepen et al., 2014) introduced three new dependency representations that do away with the assumption of 2 2.1 Background Semantic dependencies The 2014 SemEval (Oepen et al., 2014, 2015) shared task introduced three new semantic dependency formalisms, applied to the Penn Treebank (shown in Figure 1, compared to Universal Dependencies (Nivre et al., 2016)): DELPH-IN MRS, or DM (Flickinger et al., 2012; Oepen and Lønning, 2006); Predicate-Argument Structures, or PAS (Miyao and Tsujii, 2004); and Prague Semantic Dependencies, or PSD (Hajic et al., 2012). Whereas syntactic dependencies generally annotate functional"
P18-2077,oepen-lonning-2006-discriminant,0,0.292816,"Missing"
P18-2077,P17-1186,0,0.186835,"Missing"
P18-2077,D14-1162,1,0.106692,"hether word lemmas—another form of low-level lexical information—might also improve dependency parsing performance.   (label-dep) (label-head) , hj = Biaff(label) hi (10) = {si,j ≥ 0} ... Embed (3) Bilin(x1 , x2 ) = x> 1 Ux2 Labels 4 4.1 Results Hyperparameters We tuned the hyperparameters for our basic system (with no character embeddings or lemmas) fairly extensively on the DM development data. The hyperparameter configuration for our final system is given in Table 2. All input embeddings (word, pretrained, POS, etc.) were concatenated. We used 100-dimensional pretrained GloVe embeddings (Pennington et al., 2014), but linearly transformed them to be 125-dimensional. Only words or lemmas that occurred 7 times or more were included in the word and lemma embedding matrix—including less frequent words appeared to facilitate overfitting. Character-level word embeddings were generated using a one-layer unidirectional LSTM that convolved over three character embeddings at a time, whose end state was linearly transformed to be 100-dimensional. The core BiL(13) Worth noting is that the removal of the maximum spanning tree algorithm and change from softmax cross-entropy to sigmoid cross-entropy in 4 For the lab"
P18-2077,D17-1009,0,0.0626321,"Missing"
P18-2077,L16-1376,1,0.909288,"Missing"
P19-1595,E17-2026,0,0.0300237,"over standard single-task and multi-task training. 1 Task 1 Labels Task 1 Model Task 2 Model Task k Model Task 2 Labels Multi-Task Model distill train Task k Labels Figure 1: Overview of our method. λ is increased linearly from 0 to 1 over the course of training. Introduction Building a single model that jointly learns to perform many tasks effectively has been a longstanding challenge in Natural Language Processing (NLP). However, multi-task NLP remains difficult for many applications, with multi-task models often performing worse than their single-task counterparts (Plank and Alonso, 2017; Bingel and Søgaard, 2017; McCann et al., 2018). Motivated by these results, we propose a way of applying knowledge distillation (Bucilu et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015) so that singletask models effectively teach a multi-task model. Knowledge distillation transfers knowledge from a “teacher” model to a “student” model by training the student to imitate the teacher’s outputs. In “born-again networks” (Furlanello et al., 2018), the teacher and student have the same neural architecture and model size, but surprisingly the student is able to surpass the teacher’s accuracy. Intuitively, distillatio"
P19-1595,S17-2001,0,0.0606625,"Missing"
P19-1595,N19-1423,0,0.64302,"Multi), and also explore performing multiple rounds of distillation (Single→Multi→Single→Multi). Furthermore, we propose a simple teacher annealing method that helps the student model outperform its teachers. Teacher annealing gradually transitions the student from learning from the teacher to learning from the gold labels. This method ensures the student gets a rich training signal early in training, but is not limited to only imitating the teacher. Our experiments build upon recent success in self-supervised pre-training (Dai and Le, 2015; Peters et al., 2018) and multi-task fine-tune BERT (Devlin et al., 2019) to perform the tasks from the GLUE natural language understanding benchmark (Wang et al., 2019). Our training method, which we call Born-Again Multi-tasking (BAM)2 , consistently outperforms standard single-task and multi-task training. Further analysis shows the multi-task models benefit from both better regularization and transfer between related tasks. 1 We use Single→Multi to indicate distilling single-task “teacher” models into a multi-task “student” model. 2 Code will be released at https://github.com/ google-research/google-research/tree/ master/bam 5931 Proceedings of the 57th Annual"
P19-1595,I05-5002,0,0.10658,"Missing"
P19-1595,W07-1401,0,0.302729,"Missing"
P19-1595,D17-1206,0,0.0213535,"omputational Linguistics, pages 5931–5937 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 0 time train 2 Related Work Multi-task learning for neural networks in general (Caruana, 1997) and within NLP specifically (Collobert and Weston, 2008; Luong et al., 2016) has been widely studied. Much of the recent work for NLP has centered on neural architecture design: e.g., ensuring only beneficial information is shared across tasks (Liu et al., 2017; Ruder et al., 2019) or arranging tasks in linguistically-motivated hierarchies (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Sanh et al., 2019). These contributions are orthogonal to ours because we instead focus on the multi-task training algorithm. Distilling large models into small models (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019a) has been shown to improve results for many NLP tasks. There has also been some work on using knowledge distillation to aide in multi-task learning. In reinforcement learning, knowledge distillation has been used to regularize multi-task agents (Parisotto et al., 2016; Teh et al., 2017). In NLP, Tan et al."
P19-1595,P84-1044,0,0.382433,"Missing"
P19-1595,P18-1031,0,0.0283787,"ilment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC), question paraphrase (QQP), textual similarity (STS), sentiment (SST-2), linguistic acceptability (CoLA), and Winograd Schema (WNLI). Training Details. Rather than simply shuffling the datasets for our multi-task models, we follow the task sampling procedure from Bowman et al. (2018), where the probability of training on an example for a particular task τ is proportional to |Dτ |0.75 . This ensures that tasks with very large datasets don’t overly dominate the training. We also use the layerwise-learning-rate trick from Howard and Ruder (2018). If layer 0 is the NN layer closest to the output, the learning rate for a particular layer d is set to BASE LR · αd (i.e., layers closest to the input get lower learning rates). The intuition is that pre-trained layers closer to the input learn more general features, so they shouldn’t be altered much during training. Hyperparameters. For single-task models, we use the same hyperparameters as in the original BERT experiments except we pick a layerwiselearning-rate decay α of 1.0 or 0.9 on the dev set for each task. For multi-task models, we train the model for longer (6 epochs instead of 3) a"
P19-1595,D16-1139,0,0.0607947,"networks in general (Caruana, 1997) and within NLP specifically (Collobert and Weston, 2008; Luong et al., 2016) has been widely studied. Much of the recent work for NLP has centered on neural architecture design: e.g., ensuring only beneficial information is shared across tasks (Liu et al., 2017; Ruder et al., 2019) or arranging tasks in linguistically-motivated hierarchies (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Sanh et al., 2019). These contributions are orthogonal to ours because we instead focus on the multi-task training algorithm. Distilling large models into small models (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019a) has been shown to improve results for many NLP tasks. There has also been some work on using knowledge distillation to aide in multi-task learning. In reinforcement learning, knowledge distillation has been used to regularize multi-task agents (Parisotto et al., 2016; Teh et al., 2017). In NLP, Tan et al. (2019) distill singlelanguage-pair machine translation systems into a many-language system. However, they focus on multilingual rather than multi-task learning, use a more complex training"
P19-1595,D16-1180,0,0.0414928,"ston, 2008; Luong et al., 2016) has been widely studied. Much of the recent work for NLP has centered on neural architecture design: e.g., ensuring only beneficial information is shared across tasks (Liu et al., 2017; Ruder et al., 2019) or arranging tasks in linguistically-motivated hierarchies (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Sanh et al., 2019). These contributions are orthogonal to ours because we instead focus on the multi-task training algorithm. Distilling large models into small models (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019a) has been shown to improve results for many NLP tasks. There has also been some work on using knowledge distillation to aide in multi-task learning. In reinforcement learning, knowledge distillation has been used to regularize multi-task agents (Parisotto et al., 2016; Teh et al., 2017). In NLP, Tan et al. (2019) distill singlelanguage-pair machine translation systems into a many-language system. However, they focus on multilingual rather than multi-task learning, use a more complex training procedure, and only experiment with Single→Multi distillation. Concurrently with ou"
P19-1595,P17-1001,0,0.023061,"hub.com/ google-research/google-research/tree/ master/bam 5931 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5931–5937 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 0 time train 2 Related Work Multi-task learning for neural networks in general (Caruana, 1997) and within NLP specifically (Collobert and Weston, 2008; Luong et al., 2016) has been widely studied. Much of the recent work for NLP has centered on neural architecture design: e.g., ensuring only beneficial information is shared across tasks (Liu et al., 2017; Ruder et al., 2019) or arranging tasks in linguistically-motivated hierarchies (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Sanh et al., 2019). These contributions are orthogonal to ours because we instead focus on the multi-task training algorithm. Distilling large models into small models (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019a) has been shown to improve results for many NLP tasks. There has also been some work on using knowledge distillation to aide in multi-task learning. In reinforcement learning,"
P19-1595,P19-1441,0,0.127615,"l., 2016) has been widely studied. Much of the recent work for NLP has centered on neural architecture design: e.g., ensuring only beneficial information is shared across tasks (Liu et al., 2017; Ruder et al., 2019) or arranging tasks in linguistically-motivated hierarchies (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Sanh et al., 2019). These contributions are orthogonal to ours because we instead focus on the multi-task training algorithm. Distilling large models into small models (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019a) has been shown to improve results for many NLP tasks. There has also been some work on using knowledge distillation to aide in multi-task learning. In reinforcement learning, knowledge distillation has been used to regularize multi-task agents (Parisotto et al., 2016; Teh et al., 2017). In NLP, Tan et al. (2019) distill singlelanguage-pair machine translation systems into a many-language system. However, they focus on multilingual rather than multi-task learning, use a more complex training procedure, and only experiment with Single→Multi distillation. Concurrently with our work, several ot"
P19-1595,N18-1202,0,0.0586427,"ith several other variants (Single→Single and Multi→Multi), and also explore performing multiple rounds of distillation (Single→Multi→Single→Multi). Furthermore, we propose a simple teacher annealing method that helps the student model outperform its teachers. Teacher annealing gradually transitions the student from learning from the teacher to learning from the gold labels. This method ensures the student gets a rich training signal early in training, but is not limited to only imitating the teacher. Our experiments build upon recent success in self-supervised pre-training (Dai and Le, 2015; Peters et al., 2018) and multi-task fine-tune BERT (Devlin et al., 2019) to perform the tasks from the GLUE natural language understanding benchmark (Wang et al., 2019). Our training method, which we call Born-Again Multi-tasking (BAM)2 , consistently outperforms standard single-task and multi-task training. Further analysis shows the multi-task models benefit from both better regularization and transfer between related tasks. 1 We use Single→Multi to indicate distilling single-task “teacher” models into a multi-task “student” model. 2 Code will be released at https://github.com/ google-research/google-research/t"
P19-1595,E17-1005,0,0.0392658,"od consistently improves over standard single-task and multi-task training. 1 Task 1 Labels Task 1 Model Task 2 Model Task k Model Task 2 Labels Multi-Task Model distill train Task k Labels Figure 1: Overview of our method. λ is increased linearly from 0 to 1 over the course of training. Introduction Building a single model that jointly learns to perform many tasks effectively has been a longstanding challenge in Natural Language Processing (NLP). However, multi-task NLP remains difficult for many applications, with multi-task models often performing worse than their single-task counterparts (Plank and Alonso, 2017; Bingel and Søgaard, 2017; McCann et al., 2018). Motivated by these results, we propose a way of applying knowledge distillation (Bucilu et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015) so that singletask models effectively teach a multi-task model. Knowledge distillation transfers knowledge from a “teacher” model to a “student” model by training the student to imitate the teacher’s outputs. In “born-again networks” (Furlanello et al., 2018), the teacher and student have the same neural architecture and model size, but surprisingly the student is able to surpass the teacher’s accuracy"
P19-1595,P16-1162,0,0.0190764,"g, use a more complex training procedure, and only experiment with Single→Multi distillation. Concurrently with our work, several other recent works also explore fine-tuning BERT using multiple tasks (Phang et al., 2018; Liu et al., 2019b; Keskar et al., 2019; Liu et al., 2019a). However, they use only standard transfer or multitask learning, instead focusing on finding beneficial task pairs or designing improved task-specific components on top of BERT. 3 3.1 Methods Multi-Task Setup Model. All of our models are built on top of BERT (Devlin et al., 2019). This model passes byte-pairtokenized (Sennrich et al., 2016) input sentences through a Transformer network (Vaswani et al., 2017), producing a contextualized representation for each token. The vector corresponding to the first input token3 c is passed into a task-specific classifier. For classification tasks, we use a standard softmax layer: softmax(W c). For regression 3 For BERT this is a special token [CLS] that is prepended to each input sequence. tasks, we normalize the labels so they are between 0 and 1 and then use a size-1 NN layer with a sigmoid activation: sigmoid(wT c). In our multi-task models, all of the model parameters are shared across"
P19-1595,D13-1170,1,0.0259998,"Missing"
P19-1595,P16-2038,0,0.0252572,"ing of the Association for Computational Linguistics, pages 5931–5937 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 0 time train 2 Related Work Multi-task learning for neural networks in general (Caruana, 1997) and within NLP specifically (Collobert and Weston, 2008; Luong et al., 2016) has been widely studied. Much of the recent work for NLP has centered on neural architecture design: e.g., ensuring only beneficial information is shared across tasks (Liu et al., 2017; Ruder et al., 2019) or arranging tasks in linguistically-motivated hierarchies (Søgaard and Goldberg, 2016; Hashimoto et al., 2017; Sanh et al., 2019). These contributions are orthogonal to ours because we instead focus on the multi-task training algorithm. Distilling large models into small models (Kim and Rush, 2016; Mou et al., 2016) or ensembles of models into single models (Kuncoro et al., 2016; Liu et al., 2019a) has been shown to improve results for many NLP tasks. There has also been some work on using knowledge distillation to aide in multi-task learning. In reinforcement learning, knowledge distillation has been used to regularize multi-task agents (Parisotto et al., 2016; Teh et al., 20"
P19-1595,N18-1101,0,0.101064,"Missing"
P93-1032,C92-4188,0,0.0308591,"Missing"
P93-1032,P91-1027,0,0.300642,"rogram such as the work presented here. For example, among the 27 verbs that most commonly cooccurred with from, Church and Hanks found 7 for which this 235 PREVIOUS suggests that we should aim for a high precision learner (even at some cost in coverage), and that is the approach adopted here. WORK While work has been done on various sorts of collocation information that can be obtained from text corpora, the only research that I am aware of that has dealt directly with the problem of the automatic acquisition of subcategorization frames is a series of papers by Brent (Brent and Berwick 1991, Brent 1991, Brent 1992). Brent and Betwick (1991) took the approach of trying to generate very high precision data. 2 The input was hand-tagged text from the Penn Treebank, and they used a very simple finite state parser which ignored nearly all the input, but tried to learn from the sentences which seemed least likely to contain false triggers - mainly sentences with pronouns and proper names. 3 This was a consistent strategy which produced promising initial results. However, using hand-tagged text is clearly not a solution to the knowledge acquisition problem (as hand-tagging text is more laborious th"
P93-1032,P89-1010,0,0.0175775,"eeds to be supplemented with further knowledge that is best collected automatically) The desire to combine hand-coded and automatically learned knowledge °Thanks to Julian Kupiec for providing the tagger on which this work depends and for helpful discussions and comments along the way. I am also indebted for comments on an earlier draft to Marti Hearst (whose comments were the most useful!), Hinrich Schfitze, Penni Sibun, Mary Dalrymple, and others at Xerox PARC, where this research was completed during a summer internship; Stanley Peters, and the two anonymous ACL reviewers. 1A point made by Church and Hanks (1989). Arbitrary gaps in listing can be smoothed with a program such as the work presented here. For example, among the 27 verbs that most commonly cooccurred with from, Church and Hanks found 7 for which this 235 PREVIOUS suggests that we should aim for a high precision learner (even at some cost in coverage), and that is the approach adopted here. WORK While work has been done on various sorts of collocation information that can be obtained from text corpora, the only research that I am aware of that has dealt directly with the problem of the automatic acquisition of subcategorization frames is a"
P93-1032,C92-2082,0,0.0725722,"Missing"
P93-1032,P91-1030,0,0.0621452,"Missing"
P93-1032,J90-1003,0,\N,Missing
P93-1032,J93-1005,0,\N,Missing
P93-1032,H91-1067,0,\N,Missing
Q14-1005,P05-1001,0,0.018864,"ata or a small amount of labeled data is available, and a semisupervised settings where labeled data is available, but we can gain predictive power by learning from unlabeled bitext. 2 Related Work Most semi-supervised learning approaches embody the principle of learning from constraints. There are two broad categories of constraints: multi-view constraints, and external knowledge constraints. Examples of methods that explore multi-view constraints include self-training (Yarowsky, 1995; McClosky et al., 2006),2 co-training (Blum and Mitchell, 1998; Sindhwani et al., 2005), multiview learning (Ando and Zhang, 2005; Carlson et al., 2010), and discriminative and generative model combination (Suzuki and Isozaki, 2008; Druck and McCallum, 2010). An early example of using knowledge as constraints in weakly-supervised learning is the work by Collins and Singer (1999). They showed that the addition of a small set of “seed” rules greatly improve a co-training style unsupervised tagger. Chang et al. (2007) proposed a constraint-driven learning (CODL) framework where constraints are used to guide the selection of best self-labeled examples to be included as additional training data in an iterative EM-style proce"
Q14-1005,D08-1092,0,0.00974711,"erior” as in PR. no greater than computing the gradients of ordinary CRF. And empirically, GE tends to perform more accurately than PR (Bellare et al., 2009; Druck, 2011). Obtaining appropriate knowledge resources for constructing constraints remain as a bottleneck in applying GE and PR to new languages. However, a number of past work recognizes parallel bitext as a rich source of linguistic constraints, naturally captured in the translations. As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013a). Burkett et al. (2010) also demonstrated a uptraining (Petrov et al., 2010) setting where tag-induced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging mo"
Q14-1005,W10-2906,0,0.0781101,"o new languages. However, a number of past work recognizes parallel bitext as a rich source of linguistic constraints, naturally captured in the translations. As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013a). Burkett et al. (2010) also demonstrated a uptraining (Petrov et al., 2010) setting where tag-induced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakl"
Q14-1005,P07-1036,0,0.721675,"corpora is a costly and time consuming process. To date, most annotated resources resides within the English language, which hinders the adoption of supervised learning methods in many multilingual environments. To minimize the need for annotation, significant progress has been made in developing unsupervised and semi-supervised approaches to NLP (Collins and Singer 1999; Klein 2005; Liang 2005; Smith 2006; Goldberg 2010; inter alia) . More recent paradigms for semi-supervised learning allow modelers to directly encode knowledge about the task and the domain as constraints to guide learning (Chang et al., 2007; Mann and McCallum, 2010; Ganchev et al., 2010). However, in a multilingual setting, coming up with effective constraints require extensive knowledge of the foreign1 language. Bilingual parallel text (bitext) lends itself as a medium to transfer knowledge from a resource-rich language to a foreign languages. Yarowsky and Ngai (2001) project labels produced by an English tagger to the foreign side of bitext, then use the projected labels to learn a HMM model. More recent work applied the projection-based approach to more language-pairs, and further improved performance through the use of type-"
Q14-1005,N13-1006,1,0.281487,"er, a number of past work recognizes parallel bitext as a rich source of linguistic constraints, naturally captured in the translations. As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013a). Burkett et al. (2010) also demonstrated a uptraining (Petrov et al., 2010) setting where tag-induced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scena"
Q14-1005,W99-0613,0,0.720842,"arning methods have enjoyed great popularity in Natural Language Processing (NLP) over the past decade. The success of supervised methods depends heavily upon the availability of large amounts of annotated training data. Manual curation of annotated corpora is a costly and time consuming process. To date, most annotated resources resides within the English language, which hinders the adoption of supervised learning methods in many multilingual environments. To minimize the need for annotation, significant progress has been made in developing unsupervised and semi-supervised approaches to NLP (Collins and Singer 1999; Klein 2005; Liang 2005; Smith 2006; Goldberg 2010; inter alia) . More recent paradigms for semi-supervised learning allow modelers to directly encode knowledge about the task and the domain as constraints to guide learning (Chang et al., 2007; Mann and McCallum, 2010; Ganchev et al., 2010). However, in a multilingual setting, coming up with effective constraints require extensive knowledge of the foreign1 language. Bilingual parallel text (bitext) lends itself as a medium to transfer knowledge from a resource-rich language to a foreign languages. Yarowsky and Ngai (2001) project labels produ"
Q14-1005,P11-1061,0,0.507538,"up with effective constraints require extensive knowledge of the foreign1 language. Bilingual parallel text (bitext) lends itself as a medium to transfer knowledge from a resource-rich language to a foreign languages. Yarowsky and Ngai (2001) project labels produced by an English tagger to the foreign side of bitext, then use the projected labels to learn a HMM model. More recent work applied the projection-based approach to more language-pairs, and further improved performance through the use of type-level constraints from tag dictionary and feature-rich generative or discriminative models (Das and Petrov, 2011; T¨ackstr¨om et al., 2013). In our work, we propose a new projection-based method that differs in two important ways. First, we never explicitly project the labels. Instead, we project expectations over the labels. This projection 1 For experimental purposes, we designate English as the resource-rich language, and other languages of interest as “foreign”. In our experiments, we simulate the resource-poor scenario using Chinese and German, even though in reality these two languages are quite rich in resources. 55 Transactions of the Association for Computational Linguistics, 2 (2014) 55–66. Ac"
Q14-1005,D09-1009,0,0.0200139,"ed in applications such as NER are the ones like “the words CA, Australia, NY are L OCATION” (Chang et al., 2007). Notice the similarity of this partic2 A multi-view interpretation of self-training is that the selftagged additional data offers new views to learners trained on existing labeled data. 56 ular constraint to the kinds of features one would expect to see in a discriminative MaxEnt model. The difference is that instead of learning the validity (or weight) of this feature from labeled examples — since we do not have them — we can constrain the model using our knowledge of the domain. Druck et al. (2009) also demonstrated that in an active learning setting where annotation budget is limited, it is more efficient to label features than examples. Other sources of knowledge include lexicons and gazetteers (Druck et al., 2007; Chang et al., 2007). While it is straight-forward to see how resources such as a list of city names can give a lot of mileage in recognizing locations, we are also exposed to the danger of over-committing to hard constraints. For example, it becomes problematic with city names that are ambiguous, such as Augusta, Georgia.3 To soften these constraints, Mann and McCallum (201"
Q14-1005,W06-1673,1,0.214946,"of one-best label assignments from English to foreign language can be thought of as a soft version of the method described in (Das and Petrov, 2011) and (Ganchev et al., 2009). Soft projection has its advantage: when the English model is not certain about its predictions, we do not have to commit to the current best prediction. The foreign model has more freedom to form its own belief since any marginal distribution it produces would deviates from a flat distribution by just about the same amount. In general, preserving uncertainties till later is a strategy that has benefited many NLP tasks (Finkel et al., 2006). Hard projection can also be treated as a special case in our framework. We can simply recalibrate posterior marginal of English by assigning probability mass 1 to the most likely outcome, and zero everything else out, effectively taking the argmax of the marginal at each word position. We refer to this version of expectation as the “hard” expectation. In the hard projection setting, GE training resembles a “project-then-train” style semi-supervised CRF training scheme (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013). In such a training scheme, we project the one-best predictions of Englis"
Q14-1005,I05-1075,0,0.0651106,"infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrated by Yarowsky and Ngai (2001), and Xi and Hwa (2005). One problem with projected labels is that they are often too noisy to be directly used as training signals. To mitigate this problem, Das and Petrov (2011) designed a label propagation method to automatically induce a tag lexicon for the foreign language to smooth the projected labels. Fossum and Abney (2005) filter out projection noise by combining projections from from multiple source languages. However, this approach is not always viable since it relies on having parallel bitext from multiple source languages. Li et al. (2012) proposed the use of crowd-sourced Wiktionary as additional resources for inducing tag lexicons. More recently, T¨ackstr¨om et al. (2013) combined token-level and type-level constraints to constrain legitimate label sequences and and recalibrate the probability distri57 bution in a CRF. The tag dictionary used for POS tagging are analogous to the gazetteers and name lexico"
Q14-1005,D13-1205,0,0.095574,"to Ganchev et al. (2009). They used a two-step projection method similar to Das and Petrov (2011) for dependency parsing. Instead of using the projected linguistic structures as ground truth (Yarowsky and Ngai, 2001), or as features in a generative model (Das and Petrov, 2011), they used them as constraints in a PR framework. Our work differs by projecting expectations rather than Viterbi one-best labels. We also choose the GE framework over PR. Experiments in Bellare et al. (2009) and Druck (2011) suggest that in a discriminative model (like ours), GE is more accurate than PR. More recently, Ganchev and Das (2013) further extended this line of work to directly train discriminative sequence models using cross lingual projection with PR. The types of constraints applied in this new work are similar to the ones in the monolingual PR setting proposed by Ganchev et al. (2010), where the total counts of labels of a particular kind are expected to match some fraction of the projected total counts. Our work differ in that we enforce expectation constraints at token level, which gives tighter guidance to learning the model. 3 Approach Given bitext between English and a foreign language, our goal is to learn a C"
Q14-1005,P09-1042,0,0.109391,"rce languages. However, this approach is not always viable since it relies on having parallel bitext from multiple source languages. Li et al. (2012) proposed the use of crowd-sourced Wiktionary as additional resources for inducing tag lexicons. More recently, T¨ackstr¨om et al. (2013) combined token-level and type-level constraints to constrain legitimate label sequences and and recalibrate the probability distri57 bution in a CRF. The tag dictionary used for POS tagging are analogous to the gazetteers and name lexicons used for NER by Chang et al. (2007). Our work is also closely related to Ganchev et al. (2009). They used a two-step projection method similar to Das and Petrov (2011) for dependency parsing. Instead of using the projected linguistic structures as ground truth (Yarowsky and Ngai, 2001), or as features in a generative model (Das and Petrov, 2011), they used them as constraints in a PR framework. Our work differs by projecting expectations rather than Viterbi one-best labels. We also choose the GE framework over PR. Experiments in Bellare et al. (2009) and Druck (2011) suggest that in a discriminative model (like ours), GE is more accurate than PR. More recently, Ganchev and Das (2013) f"
Q14-1005,N06-2015,0,0.0960155,"Missing"
Q14-1005,D09-1005,0,0.202189,"illustrated by Samdani et al. (2012). Another closely related work is the Posterior Regularization (PR) framework by Ganchev et al. (2010). In fact, as Bellare et al. (2009) have shown, in a discriminative model these two methods optimize exactly the same objective.4 The two differ in optimization details: PR uses a EM algorithm to approximate the gradients which avoids the expensive computation of a covariance matrix between features and constraints, whereas GE directly calculates the gradient. However, later results (Druck, 2011) have shown that using the Expectation Semiring techniques of Li and Eisner (2009), one can compute the exact gradients of GE in a Conditional Random Fields (CRF) (Lafferty et al., 2001) at costs 3 This is a city in the state of Georgia in USA, famous for its golf courses. It is ambiguous since both Augusta and Georgia can also be used as person names. 4 The different terminology employed by GE and PR may be confusing to discerning readers, but the “expectation” in the context of GE means the same thing as “marginal posterior” as in PR. no greater than computing the gradients of ordinary CRF. And empirically, GE tends to perform more accurately than PR (Bellare et al., 2009"
Q14-1005,D12-1127,0,0.10084,"Missing"
Q14-1005,N06-1020,0,0.0162888,"standard public datasets. We report results in two settings: a weakly supervised setting where no labeled data or a small amount of labeled data is available, and a semisupervised settings where labeled data is available, but we can gain predictive power by learning from unlabeled bitext. 2 Related Work Most semi-supervised learning approaches embody the principle of learning from constraints. There are two broad categories of constraints: multi-view constraints, and external knowledge constraints. Examples of methods that explore multi-view constraints include self-training (Yarowsky, 1995; McClosky et al., 2006),2 co-training (Blum and Mitchell, 1998; Sindhwani et al., 2005), multiview learning (Ando and Zhang, 2005; Carlson et al., 2010), and discriminative and generative model combination (Suzuki and Isozaki, 2008; Druck and McCallum, 2010). An early example of using knowledge as constraints in weakly-supervised learning is the work by Collins and Singer (1999). They showed that the addition of a small set of “seed” rules greatly improve a co-training style unsupervised tagger. Chang et al. (2007) proposed a constraint-driven learning (CODL) framework where constraints are used to guide the selecti"
Q14-1005,D10-1069,0,0.0178186,"ts, naturally captured in the translations. As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013a). Burkett et al. (2010) also demonstrated a uptraining (Petrov et al., 2010) setting where tag-induced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrated by Yarowsky and Ngai (2001), and Xi and Hwa (2005). One problem with projec"
Q14-1005,N12-1087,0,0.0223623,"cognizing locations, we are also exposed to the danger of over-committing to hard constraints. For example, it becomes problematic with city names that are ambiguous, such as Augusta, Georgia.3 To soften these constraints, Mann and McCallum (2010) proposed the Generalized Expectation (GE) Criteria framework, which encodes constraints as a regularization term over some score function that measures the divergence between the model’s expectation and the target expectation. The connection between GE and CODL is analogous to the relationship between hard (Viterbi) EM and soft EM, as illustrated by Samdani et al. (2012). Another closely related work is the Posterior Regularization (PR) framework by Ganchev et al. (2010). In fact, as Bellare et al. (2009) have shown, in a discriminative model these two methods optimize exactly the same objective.4 The two differ in optimization details: PR uses a EM algorithm to approximate the gradients which avoids the expensive computation of a covariance matrix between features and constraints, whereas GE directly calculates the gradient. However, later results (Druck, 2011) have shown that using the Expectation Semiring techniques of Li and Eisner (2009), one can compute"
Q14-1005,P09-1009,0,0.0303599,"he same thing as “marginal posterior” as in PR. no greater than computing the gradients of ordinary CRF. And empirically, GE tends to perform more accurately than PR (Bellare et al., 2009; Druck, 2011). Obtaining appropriate knowledge resources for constructing constraints remain as a bottleneck in applying GE and PR to new languages. However, a number of past work recognizes parallel bitext as a rich source of linguistic constraints, naturally captured in the translations. As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013a). Burkett et al. (2010) also demonstrated a uptraining (Petrov et al., 2010) setting where tag-induced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it re"
Q14-1005,P08-1076,0,0.00979248,"ta is available, but we can gain predictive power by learning from unlabeled bitext. 2 Related Work Most semi-supervised learning approaches embody the principle of learning from constraints. There are two broad categories of constraints: multi-view constraints, and external knowledge constraints. Examples of methods that explore multi-view constraints include self-training (Yarowsky, 1995; McClosky et al., 2006),2 co-training (Blum and Mitchell, 1998; Sindhwani et al., 2005), multiview learning (Ando and Zhang, 2005; Carlson et al., 2010), and discriminative and generative model combination (Suzuki and Isozaki, 2008; Druck and McCallum, 2010). An early example of using knowledge as constraints in weakly-supervised learning is the work by Collins and Singer (1999). They showed that the addition of a small set of “seed” rules greatly improve a co-training style unsupervised tagger. Chang et al. (2007) proposed a constraint-driven learning (CODL) framework where constraints are used to guide the selection of best self-labeled examples to be included as additional training data in an iterative EM-style procedure. The kind of constraints used in applications such as NER are the ones like “the words CA, Austra"
Q14-1005,Q13-1001,0,0.10327,"Missing"
Q14-1005,P10-1040,0,0.149354,"Missing"
Q14-1005,P13-1106,1,0.767309,"st work recognizes parallel bitext as a rich source of linguistic constraints, naturally captured in the translations. As a result, bitext has been effectively utilized for unsupervised multilingual grammar induction (Alshawi et al., 2000; Snyder et al., 2009), parsing (Burkett and Klein, 2008), and sequence labeling (Naseem et al., 2009). A number of recent work also explored bilingual constraints in the context of simultaneous bilingual tagging, and showed that enforcing agreements between language pairs give superior results than monolingual tagging (Burkett et al., 2010; Che et al., 2013; Wang et al., 2013a). Burkett et al. (2010) also demonstrated a uptraining (Petrov et al., 2010) setting where tag-induced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrat"
Q14-1005,H05-1107,0,0.031817,"strated a uptraining (Petrov et al., 2010) setting where tag-induced bitext can be used as additional monolingual training data to improve monolingual taggers. A major drawback of this approach is that it requires a readily-trained tagging models in each languages, which makes a weakly supervised setting infeasible. Another intricacy of this approach is that it only works when the two models have comparable strength, since mutual agreements are enforced between them. Projection-based methods can be very effective in weakly-supervised scenarios, as demonstrated by Yarowsky and Ngai (2001), and Xi and Hwa (2005). One problem with projected labels is that they are often too noisy to be directly used as training signals. To mitigate this problem, Das and Petrov (2011) designed a label propagation method to automatically induce a tag lexicon for the foreign language to smooth the projected labels. Fossum and Abney (2005) filter out projection noise by combining projections from from multiple source languages. However, this approach is not always viable since it relies on having parallel bitext from multiple source languages. Li et al. (2012) proposed the use of crowd-sourced Wiktionary as additional res"
Q14-1005,N01-1026,0,0.29147,"ed approaches to NLP (Collins and Singer 1999; Klein 2005; Liang 2005; Smith 2006; Goldberg 2010; inter alia) . More recent paradigms for semi-supervised learning allow modelers to directly encode knowledge about the task and the domain as constraints to guide learning (Chang et al., 2007; Mann and McCallum, 2010; Ganchev et al., 2010). However, in a multilingual setting, coming up with effective constraints require extensive knowledge of the foreign1 language. Bilingual parallel text (bitext) lends itself as a medium to transfer knowledge from a resource-rich language to a foreign languages. Yarowsky and Ngai (2001) project labels produced by an English tagger to the foreign side of bitext, then use the projected labels to learn a HMM model. More recent work applied the projection-based approach to more language-pairs, and further improved performance through the use of type-level constraints from tag dictionary and feature-rich generative or discriminative models (Das and Petrov, 2011; T¨ackstr¨om et al., 2013). In our work, we propose a new projection-based method that differs in two important ways. First, we never explicitly project the labels. Instead, we project expectations over the labels. This pr"
Q14-1005,P95-1026,0,0.31061,"anguage pairs on standard public datasets. We report results in two settings: a weakly supervised setting where no labeled data or a small amount of labeled data is available, and a semisupervised settings where labeled data is available, but we can gain predictive power by learning from unlabeled bitext. 2 Related Work Most semi-supervised learning approaches embody the principle of learning from constraints. There are two broad categories of constraints: multi-view constraints, and external knowledge constraints. Examples of methods that explore multi-view constraints include self-training (Yarowsky, 1995; McClosky et al., 2006),2 co-training (Blum and Mitchell, 1998; Sindhwani et al., 2005), multiview learning (Ando and Zhang, 2005; Carlson et al., 2010), and discriminative and generative model combination (Suzuki and Isozaki, 2008; Druck and McCallum, 2010). An early example of using knowledge as constraints in weakly-supervised learning is the work by Collins and Singer (1999). They showed that the addition of a small set of “seed” rules greatly improve a co-training style unsupervised tagger. Chang et al. (2007) proposed a constraint-driven learning (CODL) framework where constraints are u"
Q14-1005,W03-0419,0,\N,Missing
Q14-1017,J10-4006,0,0.0265994,"an et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci, 2010), such as tf-idf. Most of the compositionality algorithms and related datasets capture two-word compositions. For instance, (Mitchell and Lapata, 2010) use twoword phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that 208 can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al"
Q14-1017,de-marneffe-etal-2006-generating,1,0.0740447,"Missing"
Q14-1017,W13-0112,0,0.382139,"an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a"
Q14-1017,P12-1092,1,0.570804,"ly focus on the action and agents in a sentence. They are better able to abstract from the details of word order and syntactic expression. DT-RNNs outperform other recursive and recurrent neural networks, kernelized CCA and a bag-of-words baseline on the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introdu"
Q14-1017,P13-1088,0,0.0382909,"phrases and analyze similarities computed by vector addition, multiplication and others. Compositionality is an active field of research with many different models and representations being explored (Grefenstette et al., 2013), among many others. We compare to supervised compositional models that 208 can learn task-specific vector representations such as constituency tree recursive neural networks (Socher et al., 2011b; Socher et al., 2011a), chain structured recurrent neural networks and other baselines. Another alternative would be to use CCG trees as a backbone for vector composition (K.M. Hermann, 2013). Multimodal Embeddings. Multimodal embedding methods project data from multiple sources such as sound and video (Ngiam et al., 2011) or images and text. Socher et al. (Socher and Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to"
Q14-1017,P12-1038,0,0.898854,"ees. We learn to map the outputs of convolutional neural networks applied to images into the same space and can then compare both sentences and images. This allows us to query images with a sentence and give sentence descriptions to images. the visual scene described and to find appropriate images in the learned, multi-modal sentence-image space. Conversely, when given a query image, we would like to find a description that goes beyond a single label by providing a correct sentence describing it, a task that has recently garnered a lot of attention (Farhadi et al., 2010; Ordonez et al., 2011; Kuznetsova et al., 2012). We use the dataset introduced by (Rashtchian et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to d"
Q14-1017,W10-0721,0,0.88407,"works applied to images into the same space and can then compare both sentences and images. This allows us to query images with a sentence and give sentence descriptions to images. the visual scene described and to find appropriate images in the learned, multi-modal sentence-image space. Conversely, when given a query image, we would like to find a description that goes beyond a single label by providing a correct sentence describing it, a task that has recently garnered a lot of attention (Farhadi et al., 2010; Ordonez et al., 2011; Kuznetsova et al., 2012). We use the dataset introduced by (Rashtchian et al., 2010) which consists of 1000 images, each with 5 descriptions. On all tasks, our model outperforms baselines and related models. 2 Related Work The presented model is connected to several areas of NLP and vision research, each with a large amount of related work to which we can only do some justice given space constraints. Semantic Vector Spaces and Their Compositionality. The dominant approach in semantic vector spaces uses distributional similarities of single words. Often, co-occurrence statistics of a word and its context are used to describe each word (Turney and Pantel, 2010; Baroni and Lenci"
Q14-1017,D11-1014,1,0.629328,"mages are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modalities but finally mapped into a jointly learned multimodal embedding space. Our model for mapping sentences into this space is based on ideas from Recursive Neural Networks (RNNs) (Pollack, 1990; Costa et al., 2003; Socher et al., 2011b). However, unlike all previous RNN models which are based on constituency trees (CTRNNs), our model computes compositional vector representations inside dependency trees. The compositional vectors computed by this new dependency tree RNN (DT-RNN) capture more of the meaning of sentences, where we define meaning in terms of similarity to a “visual representation” of the textual description. DT-RNN induced vector representations of sentences are more robust to changes in the syntactic structure or word order than related models such as CT-RNNs or Recurrent Neural Networks since they naturally"
Q14-1017,D12-1110,1,0.361369,"the tasks of finding an image that fits a sentence description and vice versa. They also give more similar representations to sentences that describe the same image. 1 Introduction Single word vector spaces are widely used (Turney and Pantel, 2010) and successful at classifying single words and capturing their meaning (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013). Since words rarely appear in isolation, the task of learning compositional meaning representations for longer phrases has recently received a lot of attention (Mitchell and Lapata, 2010; Socher et al., 2010; Socher et al., 2012; Grefenstette et al., 2013). Similarly, classifying whole images into a fixed set of classes also achieves very high performance (Le et al., 2012; Krizhevsky et al., 2012). However, similar to words, objects in images are often seen in relationships with other objects which are not adequately described by a single label. In this work, we introduce a model, illustrated in Fig. 1, which learns to map sentences and images into a common embedding space in order to be able to retrieve one from the other. We assume word and image representations are first learned in their respective single modaliti"
Q14-1017,P13-1045,1,0.489913,"d Fei-Fei, 2010) project words and image regions into a common space using kernelized canonical correlation analysis to obtain state of the art performance in annotation and segmentation. Similar to our work, they use unsupervised large text corpora to learn semantic word representations. Among other recent work is that by Srivastava and Salakhutdinov (2012) who developed multimodal Deep Boltzmann Machines. Similar to their work, we use techniques from the broad field of deep learning to represent images and words. Recently, single word vector embeddings have been used for zero shot learning (Socher et al., 2013c). Mapping images to word vectors enabled their system to classify images as depicting objects such as ”cat” without seeing any examples of this class. Related work has also been presented at NIPS (Socher et al., 2013b; Frome et al., 2013). This work moves zero-shot learning beyond single categories per image and extends it to unseen phrases and full length sentences, making use of similar ideas of semantic spaces grounded in visual knowledge. Detailed Image Annotation. Interactions between images and texts is a growing research field. Early work in this area includes generating single words"
Q14-1017,W10-0707,0,\N,Missing
Q14-1017,N13-1090,0,\N,Missing
Q19-1016,D13-1160,0,0.0119652,"performance on our test set is 88.8 F1, theirs is 74.6 F1. Moreover, although CoQA’s answers can be freeform text, their answers are restricted only to extractive text spans. Our dataset contains passages from seven diverse domains, whereas their dataset is built only from Wikipedia articles about people. Related work We organize CoQA’s relation to existing work under the following criteria. Knowledge source We answer questions about text passages—our knowledge source. Another common knowledge source is machine-friendly databases, which organize world facts in the form of a table or a graph (Berant et al., 2013; Pasupat and Liang, 2015; Bordes et al., 2015; Saha et al., 2018; Talmor and Berant, 2018). However, understanding their structure requires expertise, making it challenging to crowd-source large QA datasets without relying on templates. Like passages, other human-friendly sources are 12 Concurrently, Saeidi et al. (2018) created a conversational QA dataset for regulatory text such as tax and visa regulations. Their answers are limited to yes or no along with a positive characteristic of permitting to ask clarification questions when a given question cannot be answered. Elgohary et al. (2018)"
Q19-1016,P18-1060,0,0.0111553,"is Table 8 presents fine-grained results of models and humans on the development set. We observe that humans have the highest disagreement on the unanswerable questions. The human agreement on answers that do not overlap with passage is lower than on answers that do overlap. This is expected because our evaluation metric is based on word overlap rather than on the meaning of words. For the question did Jenny like her new room?, human answers she loved it and yes are both accepted. Finding the perfect evaluation metric for abstractive responses is still a challenging problem (Liu et al., 2016; Chaganty et al., 2018) and beyond the scope of our work. For our models’ performance, seq2seq and PGNet perform well on non-overlapping answers, and DrQA performs well on overlapping answers, thanks to their respective designs. The augmented and combined models improve on both categories. Among the different question types, humans find lexical matches the easiest, followed by paraphrasing, and pragmatics the hardest—this is expected because questions with lexical matches and paraphrasing share some similarity with the passage, thus making them relatively easier to answer 11 We collect children’s stories from MCTest"
Q19-1016,P18-1082,0,0.0455605,"Missing"
Q19-1016,P16-1154,0,0.0100604,"ence-to-sequence with attention model for generating answers (Bahdanau et al., 2015). We append the conversation history and the current question to the passage, as p &lt;q&gt; qi−n &lt;a&gt; ai−n . . . &lt;q&gt; qi−1 &lt;a&gt; ai−1 &lt;q&gt; qi , and feed it into a bidirectional long short-term memory (LSTM) encoder, where n is the size of the history to be used. We generate the answer using an LSTM decoder which attends to the encoder states. Additionally, as the answer words are likely to appear in the original passage, we employ a copy mechanism in the decoder which allows to (optionally) copy a word from the passage (Gu et al., 2016; See et al., 2017). This model is referred to as the Pointer-Generator network, PGNet. A coherent conversation must have smooth transitions between turns. We expect the narrative structure of the passage to influence our conversation flow. We split each passage into 10 uniform chunks, and identify chunks of interest in a given turn and its transition based on rationale spans. Figure 4 shows the conversation flow of the first 10 turns. The starting turns tend to focus on the first few chunks and as the conversation advances, the focus shifts to the later chunks. Moreover, the turn transitions"
Q19-1016,P17-1171,1,0.896185,"rsational Models Models Given a passage p, the conversation history {q1 , a1 , . . . qi−1 , ai−1 }, and a question qi , the task is to predict the answer ai . Gold answers a1 , a2 , . . . , ai−1 are used to predict ai , similar to the setup discussed in Section 3.3. Our task can either be modeled as a conversational response generation problem or a reading comprehension problem. We evaluate 5.2 Reading Comprehension Models The state-of-the-art reading comprehension models for extractive question answering focus on finding a span in the passage that matches the question best (Seo et al., 2016; Chen et al., 2017; Yu et al., 2018). Because their answers are limited to spans, they cannot handle questions 7 We only pick the questions in which none of its answers can be found as a span in the passage. 8 6 whose answers do not overlap with the passage (e.g., Q3 , Q4 , and Q5 in Figure 1). However, this limitation makes them more effective learners than conversational models, which have to generate an answer from a large space of pre-defined vocabulary. We use the Document Reader (DrQA) model of Chen et al. (2017), which has demonstrated strong performance on multiple datasets (Rajpurkar et al., 2016; Labu"
Q19-1016,D18-1241,0,0.379907,"t. We present CoQA as a challenge to the community at https://stanfordnlp.github. io/coqa. 1 Introduction We ask other people a question to either seek or test their knowledge about a subject. Depending on their answer, we follow up with another question and their second answer builds on what has already been discussed. This incremental aspect makes human conversations succinct. An inability to build and maintain common ground in this way is part of why virtual assistants usually don’t seem like competent conversational partners. In this 1 CoQA is pronounced as coca. Concurrent with our work, Choi et al. (2018) also created a conversational dataset with a similar goal, but it differs in many aspects. We discuss the details in Section 7. 2 ∗ The first two authors contributed equally. 1 Transactions of the Association for Computational Linguistics, vol. 7, pp. 1–18, 2019. Action Editor: Scott Wen-tau Yih. Submission batch: 10/2018; Revision batch: 1/2019; Published 5/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. science. The last two are used for out-of-domain evaluation. To summarize, CoQA has the following key characteristics: • It consists of 127k co"
Q19-1016,P17-1167,0,0.0306674,"cts a rationale current affairs, politics, and culture and generates an answer three; for a question With who?, it predicts a rationale Mary and her husband, Rick, and then compresses it into Mary and Rick for improving the fluency; and for a multiple choice question Does this help or hurt their memory of the event? it predicts a rationale this obsession may prevent their brains from remembering and answers hurt. We think there is still great room for improving the combined model and we leave it to future work. 7 Conversational Modeling Our focus is on questions that appear in a conversation. Iyyer et al. (2017) and Talmor and Berant (2018) break down a complex question into a series of simple questions mimicking conversational QA. Our work is closest to Das et al. (2017) and Saha et al. (2018), who perform conversational QA on images and a knowledge graph, respectively, with the latter focusing on questions obtained by paraphrasing templates. In parallel to our work, Choi et al. (2018) also created a dataset of conversations in the form of questions and answers on text passages. In our interface, we show a passage to both the questioner and the answerer, whereas their interface only shows a title to"
Q19-1016,D18-1134,0,0.0285084,"ph (Berant et al., 2013; Pasupat and Liang, 2015; Bordes et al., 2015; Saha et al., 2018; Talmor and Berant, 2018). However, understanding their structure requires expertise, making it challenging to crowd-source large QA datasets without relying on templates. Like passages, other human-friendly sources are 12 Concurrently, Saeidi et al. (2018) created a conversational QA dataset for regulatory text such as tax and visa regulations. Their answers are limited to yes or no along with a positive characteristic of permitting to ask clarification questions when a given question cannot be answered. Elgohary et al. (2018) proposed a sequential question answering dataset collected from Quiz Bowl tournaments, where a sequence contains multiple related questions. These questions are related to the same concept while not focusing on the dialogue aspects (e.g., coreference). Zhou et al. (2018) is another dialogue dataset based on a single movie-related Wikipedia article, in which two workers are asked to chat about the content. Their dataset is more like chit-chat style conversations whereas our dataset focuses on multi-turn question answering. text spans as rationales, and text passages from seven diverse domains."
Q19-1016,N18-1023,0,0.0499559,"Missing"
Q19-1016,D17-2014,0,0.0335616,"Missing"
Q19-1016,P17-4012,0,0.0171088,"ct the next answer. In SQuAD, for computing a model’s performance, each individual prediction is compared against n human answers resulting in n F1 scores, the maximum of which is chosen as the prediction’s F1.10 For each question, we average out F1 across these n sets, both for humans and models. In our final evaluation, we use n = 4 human answers for every question (the original answer and 3 additionally collected answers). The articles a, an, and the and punctuations are excluded in evaluation. 6.2 Experimental Setup For all the experiments of seq2seq and PGNet, we use the OpenNMT toolkit (Klein et al., 2017) and its default settings: 2-layers of LSTMs with 500 hidden units for both the encoder and the decoder. The models are optimized using SGD, with an initial learning rate of 1.0 and a decay rate of 0.5. A dropout rate of 0.3 is applied to all layers. For the DrQA experiments, we use the implementation from the original paper (Chen et al., 2017). We tune the hyperparameters on the development data: the number of turns to use from the conversation history, the number of layers, number of each hidden units per layer, and dropout rate. The best configuration we find is 3 layers of LSTMs with 300 h"
Q19-1016,S18-1119,0,0.0489101,"Missing"
Q19-1016,P18-1077,0,0.041239,"Missing"
Q19-1016,P15-1142,0,0.0159657,"est set is 88.8 F1, theirs is 74.6 F1. Moreover, although CoQA’s answers can be freeform text, their answers are restricted only to extractive text spans. Our dataset contains passages from seven diverse domains, whereas their dataset is built only from Wikipedia articles about people. Related work We organize CoQA’s relation to existing work under the following criteria. Knowledge source We answer questions about text passages—our knowledge source. Another common knowledge source is machine-friendly databases, which organize world facts in the form of a table or a graph (Berant et al., 2013; Pasupat and Liang, 2015; Bordes et al., 2015; Saha et al., 2018; Talmor and Berant, 2018). However, understanding their structure requires expertise, making it challenging to crowd-source large QA datasets without relying on templates. Like passages, other human-friendly sources are 12 Concurrently, Saeidi et al. (2018) created a conversational QA dataset for regulatory text such as tax and visa regulations. Their answers are limited to yes or no along with a positive characteristic of permitting to ask clarification questions when a given question cannot be answered. Elgohary et al. (2018) proposed a sequential que"
Q19-1016,D17-1082,0,0.14884,"many questions. We use conversation history Q1 and A1 to answer Q2 with A2 based on the evidence R2 . Formally, to answer Qn , it depends on the conversation 3 In contrast, in NarrativeQA, the annotators were encouraged to use their own words and copying was not allowed in their interface. 2 Dataset Conversational Answer Type MCTest (Richardson et al., 2013) CNN/Daily Mail (Hermann et al., 2015) Children’s book test (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) NewsQA (Trischler et al., 2017) SearchQA (Dunn et al., 2017) TriviaQA (Joshi et al., 2017) RACE (Lai et al., 2017) Narrative QA (Koˇcisk`y et al., 2018) SQuAD 2.0 (Rajpurkar et al., 2018) CoQA (this work) 7 7 7 7 7 7 7 7 7 7 7 3 Multiple choice Spans Multiple choice Spans Free-form text, Unanswerable Spans Spans Spans Multiple choice Free-form text Spans, Unanswerable Free-form text, Unanswerable; Each answer comes with a text span rationale Domain Children’s stories News Children’s stories Wikipedia Web Search News Jeopardy Trivia Mid/High School Exams Movie Scripts, Literature Wikipedia Children’s Stories, Literature, Mid/High School Exams, News, Wikipedia, Reddit, Science Table 1: Comparison of CoQA wi"
Q19-1016,D14-1162,1,0.109684,"pplied to all layers. For the DrQA experiments, we use the implementation from the original paper (Chen et al., 2017). We tune the hyperparameters on the development data: the number of turns to use from the conversation history, the number of layers, number of each hidden units per layer, and dropout rate. The best configuration we find is 3 layers of LSTMs with 300 hidden units for each layer. A dropout rate of 0.4 is applied to all LSTM layers and a dropout rate of 0.5 is applied to word embeddings. We used Adam to optimize DrQA models. We initialized the word projection matrix with GloVe (Pennington et al., 2014) for conversational models and fastText (Bojanowski et al., 2017) for reading comprehension models, based on empirical performance. We update the projection matrix during training in order to learn embeddings for delimiters such as &lt;q&gt;. A Combined Model Finally, we propose a model that combines the advantages from both conversational models and extractive reading comprehension models. We use DrQA with PGNet in a combined model, in which DrQA first points to the answer evidence in the text, and PGNet naturalizes the evidence into an answer. For example, for Q5 in Figure 1, we expect that DrQA f"
Q19-1016,N16-1014,0,0.0304353,"he passage (see row No span found in Table 8). The augmented DrQA circumvents this problem with additional yes/no tokens, giving it a boost of 12.8 points. When DrQA is fed into PGNet, we empower both DrQA and PGNet—DrQA in producing free-form answers, PGNet in focusing on the rationale Table 7 presents the results of the models on the development and test data. Considering the results on the test set, the seq2seq model performs the worst, generating frequently occurring answers irrespective of whether these answers appear in the passage or not, a well known behavior of conversational models (Li et al., 2016). PGNet alleviates the frequent response problem by focusing on the vocabulary in the passage and it outperforms seq2seq by 17.8 points. However, 10 instead of the passage. This combination outperforms vanilla PGNet and DrQA models by 21.0 and 12.5 points, respectively, and is competitive with the augmented DrQA (65.1 vs. 65.4). History Augmt. DrQA+ size Seq2seq PGNet DrQA DrQA PGNet 0 1 2 all Models vs. Humans The human performance on the test data is 88.8 F1, a strong agreement indicating that the CoQA’s questions have concrete answers. Our best model is 23.4 points behind humans. 24.0 27.5"
Q19-1016,P18-2124,0,0.440792,"with A2 based on the evidence R2 . Formally, to answer Qn , it depends on the conversation 3 In contrast, in NarrativeQA, the annotators were encouraged to use their own words and copying was not allowed in their interface. 2 Dataset Conversational Answer Type MCTest (Richardson et al., 2013) CNN/Daily Mail (Hermann et al., 2015) Children’s book test (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) NewsQA (Trischler et al., 2017) SearchQA (Dunn et al., 2017) TriviaQA (Joshi et al., 2017) RACE (Lai et al., 2017) Narrative QA (Koˇcisk`y et al., 2018) SQuAD 2.0 (Rajpurkar et al., 2018) CoQA (this work) 7 7 7 7 7 7 7 7 7 7 7 3 Multiple choice Spans Multiple choice Spans Free-form text, Unanswerable Spans Spans Spans Multiple choice Free-form text Spans, Unanswerable Free-form text, Unanswerable; Each answer comes with a text span rationale Domain Children’s stories News Children’s stories Wikipedia Web Search News Jeopardy Trivia Mid/High School Exams Movie Scripts, Literature Wikipedia Children’s Stories, Literature, Mid/High School Exams, News, Wikipedia, Reddit, Science Table 1: Comparison of CoQA with existing reading comprehension datasets. nature of questions requires"
Q19-1016,D16-1264,0,0.344946,"ersation history, otherwise its answer could be Virginia or Richmond or something else. In our task, conversation history is indispensable for answering many questions. We use conversation history Q1 and A1 to answer Q2 with A2 based on the evidence R2 . Formally, to answer Qn , it depends on the conversation 3 In contrast, in NarrativeQA, the annotators were encouraged to use their own words and copying was not allowed in their interface. 2 Dataset Conversational Answer Type MCTest (Richardson et al., 2013) CNN/Daily Mail (Hermann et al., 2015) Children’s book test (Hill et al., 2016) SQuAD (Rajpurkar et al., 2016) MS MARCO (Nguyen et al., 2016) NewsQA (Trischler et al., 2017) SearchQA (Dunn et al., 2017) TriviaQA (Joshi et al., 2017) RACE (Lai et al., 2017) Narrative QA (Koˇcisk`y et al., 2018) SQuAD 2.0 (Rajpurkar et al., 2018) CoQA (this work) 7 7 7 7 7 7 7 7 7 7 7 3 Multiple choice Spans Multiple choice Spans Free-form text, Unanswerable Spans Spans Spans Multiple choice Free-form text Spans, Unanswerable Free-form text, Unanswerable; Each answer comes with a text span rationale Domain Children’s stories News Children’s stories Wikipedia Web Search News Jeopardy Trivia Mid/High School Exams Movie Sc"
Q19-1016,D16-1230,0,0.011367,"urns. Error Analysis Table 8 presents fine-grained results of models and humans on the development set. We observe that humans have the highest disagreement on the unanswerable questions. The human agreement on answers that do not overlap with passage is lower than on answers that do overlap. This is expected because our evaluation metric is based on word overlap rather than on the meaning of words. For the question did Jenny like her new room?, human answers she loved it and yes are both accepted. Finding the perfect evaluation metric for abstractive responses is still a challenging problem (Liu et al., 2016; Chaganty et al., 2018) and beyond the scope of our work. For our models’ performance, seq2seq and PGNet perform well on non-overlapping answers, and DrQA performs well on overlapping answers, thanks to their respective designs. The augmented and combined models improve on both categories. Among the different question types, humans find lexical matches the easiest, followed by paraphrasing, and pragmatics the hardest—this is expected because questions with lexical matches and paraphrasing share some similarity with the passage, thus making them relatively easier to answer 11 We collect childr"
Q19-1016,K17-1028,0,0.0244175,"Figure 1 shows a conversation between two humans who are reading a passage, one acting as a questioner and the other as an answerer. In this conversation, every question after the first is dependent on the conversation history. For instance, Q5 (Who?) is only a single word and is impossible to answer without knowing what has already been said. Posing short questions is an effective human conversation strategy, but such questions are difficult for machines to parse. As is well known, state-of-the-art models rely heavily on lexical similarity between a question and a passage (Chen et al., 2016; Weissenborn et al., 2017). At present, there are no largescale reading comprehension datasets that contain questions that depend on a conversation history (see Table 1) and this is what CoQA is mainly developed for.2 The second goal of CoQA is to ensure the naturalness of answers in a conversation. Many existing QA datasets restrict answers to contiguous text spans in a given passage (Table 1). Such answers are not always natural—for example, there is no span-based answer to Q4 (How many?) in Figure 1. In CoQA, we propose that the answers can be free-form text, while for each answer, we also provide a text span from t"
Q19-1016,D18-1233,0,0.205047,"Missing"
Q19-1016,W17-4413,0,0.0277,"Missing"
Q19-1016,Q18-1021,0,0.0380035,"e use 200 stories for the development and the test sets. Augmented DrQA vs. Combined Model Although the performance of the augmented 11 Yes No Fluency Counting Multiple choice Augmt. DrQA DrQA+ PGNet Human 76.2 64.0 37.6 8.8 0.0 72.5 57.5 32.3 24.8 46.4 97.7 96.8 77.2 88.3 94.3 images and videos (Antol et al., 2015; Das et al., 2017; Hori et al., 2018). Naturalness There are various ways to curate questions: removing words from a declarative sentence to create a fill-in-the-blank question (Hermann et al., 2015), using a hand-written grammar to create artificial questions (Weston et al., 2016; Welbl et al., 2018), paraphrasing artificial questions to natural questions (Saha et al., 2018; Talmor and Berant, 2018), or, in our case, letting humans ask natural questions (Rajpurkar et al., 2016; Nguyen et al., 2016). While the former enable collecting large and cheap datasets, the latter enable collecting natural questions. Recent efforts emphasize collecting questions without seeing the knowledge source in order to encourage the independence of question and documents (Joshi et al., 2017; Dunn et al., 2017; Koˇcisk`y et al., 2018). Because we allow a questioner to see the passage, we incorporate measures t"
Q19-1016,P17-1099,1,0.0571326,"Missing"
Q19-1016,P18-1205,0,0.0266707,"ation progresses. Each chunk is one tenth of a passage. The x-axis indicates the turn number and the y -axis indicates the chunk containing the rationale. The height of a chunk indicates the concentration of conversation in that chunk. The width of the bands is proportional to the frequency of transition between chunks from one turn to the next. strong baselines from each modeling type and a combination of the two on CoQA. 5.1 Conversation Flow Sequence-to-sequence (seq2seq) models have shown promising results for generating conversational responses (Vinyals and Le, 2015; Serban et al., 2016; Zhang et al., 2018). Motivated by their success, we use a sequence-to-sequence with attention model for generating answers (Bahdanau et al., 2015). We append the conversation history and the current question to the passage, as p &lt;q&gt; qi−n &lt;a&gt; ai−n . . . &lt;q&gt; qi−1 &lt;a&gt; ai−1 &lt;q&gt; qi , and feed it into a bidirectional long short-term memory (LSTM) encoder, where n is the size of the history to be used. We generate the answer using an LSTM decoder which attends to the encoder states. Additionally, as the answer words are likely to appear in the original passage, we employ a copy mechanism in the decoder which allows to"
Q19-1016,N18-1059,0,0.2696,"ugh the performance of the augmented 11 Yes No Fluency Counting Multiple choice Augmt. DrQA DrQA+ PGNet Human 76.2 64.0 37.6 8.8 0.0 72.5 57.5 32.3 24.8 46.4 97.7 96.8 77.2 88.3 94.3 images and videos (Antol et al., 2015; Das et al., 2017; Hori et al., 2018). Naturalness There are various ways to curate questions: removing words from a declarative sentence to create a fill-in-the-blank question (Hermann et al., 2015), using a hand-written grammar to create artificial questions (Weston et al., 2016; Welbl et al., 2018), paraphrasing artificial questions to natural questions (Saha et al., 2018; Talmor and Berant, 2018), or, in our case, letting humans ask natural questions (Rajpurkar et al., 2016; Nguyen et al., 2016). While the former enable collecting large and cheap datasets, the latter enable collecting natural questions. Recent efforts emphasize collecting questions without seeing the knowledge source in order to encourage the independence of question and documents (Joshi et al., 2017; Dunn et al., 2017; Koˇcisk`y et al., 2018). Because we allow a questioner to see the passage, we incorporate measures to increase independence, although complete independence is not attainable in our setup (Section 3.1)."
Q19-1016,D18-1076,0,0.0231911,"other human-friendly sources are 12 Concurrently, Saeidi et al. (2018) created a conversational QA dataset for regulatory text such as tax and visa regulations. Their answers are limited to yes or no along with a positive characteristic of permitting to ask clarification questions when a given question cannot be answered. Elgohary et al. (2018) proposed a sequential question answering dataset collected from Quiz Bowl tournaments, where a sequence contains multiple related questions. These questions are related to the same concept while not focusing on the dialogue aspects (e.g., coreference). Zhou et al. (2018) is another dialogue dataset based on a single movie-related Wikipedia article, in which two workers are asked to chat about the content. Their dataset is more like chit-chat style conversations whereas our dataset focuses on multi-turn question answering. text spans as rationales, and text passages from seven diverse domains. We hope this work will stir more research in conversational modeling, a key ingredient for enabling natural human–machine communication. Acknowledgments We would like to thank MTurk workers, especially the Master Chatters and the MTC forum members, for contributing to th"
Q19-1016,W17-2623,0,0.339286,"Missing"
Q19-1016,Q17-1010,0,\N,Missing
Q19-1016,Q18-1023,0,\N,Missing
reschke-etal-2014-event,D12-1042,1,\N,Missing
reschke-etal-2014-event,P09-1113,1,\N,Missing
reschke-etal-2014-event,W11-0307,0,\N,Missing
S01-1021,J96-1002,0,0.00227623,"Missing"
S01-1021,W96-0208,0,0.0412505,"Missing"
S13-2013,chang-manning-2012-sutime,1,0.837411,"e SUT IME can also recognize temporal expressions whose values are not specified by TIMEX3, we ran SUT IME in a TIMEX3 compatible mode.3 1 Introduction The importance of modeling temporal information is increasingly apparent in natural language applications, such as information extraction and question answering. Extracting temporal information requires the ability to recognize temporal expressions, and to convert them from text to a normalized form that is easy to process. Temporal tagging systems are designed to address this problem. In this paper, we evaluate the performance of the SUT IME (Chang and Manning, 2012) rule-based temporal tagging system. We evaluate the performance of SUT IME on extracting temporal information in TempEval-3 (UzZaman et al., 2013), which requires systems to automatically annotate documents with temporal information using TimeML (Pustejovsky et al., 2003). The TempEval-3 training data contains gold human annotated data from TimeBank, AQUAINT, and a new dataset of silver data automatically annotated using a combination of TipSem (Llorens et al., 2010) and TRIOS (UzZaman and Allen, 2010), two of the 2.1 SUTime SUT IME is a rule-based temporal tagger built on regular expression"
S13-2013,S10-1063,0,0.047507,"s. Temporal tagging systems are designed to address this problem. In this paper, we evaluate the performance of the SUT IME (Chang and Manning, 2012) rule-based temporal tagging system. We evaluate the performance of SUT IME on extracting temporal information in TempEval-3 (UzZaman et al., 2013), which requires systems to automatically annotate documents with temporal information using TimeML (Pustejovsky et al., 2003). The TempEval-3 training data contains gold human annotated data from TimeBank, AQUAINT, and a new dataset of silver data automatically annotated using a combination of TipSem (Llorens et al., 2010) and TRIOS (UzZaman and Allen, 2010), two of the 2.1 SUTime SUT IME is a rule-based temporal tagger built on regular expression patterns over tokens. Temporal expressions are bounded in their complexity, so many of them can be captured using finite automata. As shown by systems such as FASTUS (Hobbs et al., 1997), a cascade of finite automata can be very effective at extracting information from text. With SUT IME, we follow a similar staged strategy of (i) building up patterns over individual words to find numerical expressions; then (ii) using patterns over words and numerical expressions to"
S13-2013,S10-1071,0,0.17323,"Missing"
S13-2013,S10-1062,0,0.0568853,"esigned to address this problem. In this paper, we evaluate the performance of the SUT IME (Chang and Manning, 2012) rule-based temporal tagging system. We evaluate the performance of SUT IME on extracting temporal information in TempEval-3 (UzZaman et al., 2013), which requires systems to automatically annotate documents with temporal information using TimeML (Pustejovsky et al., 2003). The TempEval-3 training data contains gold human annotated data from TimeBank, AQUAINT, and a new dataset of silver data automatically annotated using a combination of TipSem (Llorens et al., 2010) and TRIOS (UzZaman and Allen, 2010), two of the 2.1 SUTime SUT IME is a rule-based temporal tagger built on regular expression patterns over tokens. Temporal expressions are bounded in their complexity, so many of them can be captured using finite automata. As shown by systems such as FASTUS (Hobbs et al., 1997), a cascade of finite automata can be very effective at extracting information from text. With SUT IME, we follow a similar staged strategy of (i) building up patterns over individual words to find numerical expressions; then (ii) using patterns over words and numerical expressions to find simple temporal expressions; an"
S13-2013,S13-2001,0,0.0467567,"n The importance of modeling temporal information is increasingly apparent in natural language applications, such as information extraction and question answering. Extracting temporal information requires the ability to recognize temporal expressions, and to convert them from text to a normalized form that is easy to process. Temporal tagging systems are designed to address this problem. In this paper, we evaluate the performance of the SUT IME (Chang and Manning, 2012) rule-based temporal tagging system. We evaluate the performance of SUT IME on extracting temporal information in TempEval-3 (UzZaman et al., 2013), which requires systems to automatically annotate documents with temporal information using TimeML (Pustejovsky et al., 2003). The TempEval-3 training data contains gold human annotated data from TimeBank, AQUAINT, and a new dataset of silver data automatically annotated using a combination of TipSem (Llorens et al., 2010) and TRIOS (UzZaman and Allen, 2010), two of the 2.1 SUTime SUT IME is a rule-based temporal tagger built on regular expression patterns over tokens. Temporal expressions are bounded in their complexity, so many of them can be captured using finite automata. As shown by syst"
S13-2013,S10-1010,0,\N,Missing
trippel-etal-2008-lexicon,J96-2002,0,\N,Missing
trippel-etal-2008-lexicon,W06-1007,0,\N,Missing
W00-1308,J96-1002,0,0.0434106,"Missing"
W00-1308,A00-1031,0,0.0480837,"Missing"
W00-1308,A00-2018,0,0.0300378,"Missing"
W00-1308,P97-1032,0,0.012466,"Missing"
W00-1308,W96-0213,0,\N,Missing
W01-0714,A00-2018,0,\N,Missing
W01-0714,J97-4005,0,\N,Missing
W01-0714,P97-1003,0,\N,Missing
W01-0714,E95-1020,0,\N,Missing
W01-1812,J98-2004,0,0.0152326,"ot handle empty elements, cyclic unary productions, or n-ary rules. Stolcke (1995) presents a top-down parser for arbitrary PCFGs, which incorporates elements of ( ) 7 This ( ) has the theoretical – but not clearly useful – advantage of allowing the score combination function to vary per production. (1998) provides an insightful presentation unifying many categorical and probabilistic parsing algorithms in terms of the problem’s semiring structure, but he merely notes this problem (p. 172), and on this basis puts probabilistic agenda-based chart parsers aside. The agenda-based chart parser of Caraballo and Charniak (1998) (used for determining inside probabilities) suffers from exactly this problem: In Appendix A (p. 293), they note that such updates “can be quite expensive in terms of CPU time”, but merely suggest a method of thresholding which delays probability propagation until the amount of unpropagated probability mass has become significant, and suggest that this thresholding allows them to keep the performance of the parser “as O (n3 ) empirically.” We do not present an inside probability algorithm here, but the hypergraphical view of parsing can be developed to give an inside parsing algorithm, as dis"
W01-1812,P81-1022,0,0.15259,". The choice of which formalism to base our work on is thus more aesthetic than substantive, but we believe that the hypergraph presentation allows easier access to a greater variety of algorithmic tools, and presents a clearer, more visually appealing intuition. At any rate, the practical issues described above, and their solutions, which form the bulk of this paper, would be unchanged under either framework. 3 Viterbi Parsing Algorithm Agenda-based active chart parsing (Kay 1980, Pereira and Shieber 1987) is an attractive presentation of the central ideas of tabular methods for CFG parsing. Earley (1970)-style dotted items combine via deduction steps (“the fundamental rule”) in an order-independent manner, such that the same basic algorithm supports top-down, bottom-up, and left-corner parsing, and the parser deals naturally and correctly with the difficult cases of left-recursive rules, empty elements, and unary rules. However, while O n3 methods for parsing PCFGs are well known (Baker 1979, Jelinek et al. 1992, Stolcke 1995), a O n3 probabilistic parser corresponding to active chart parsing for categorical CFGs, has not yet been provided. Producing a probabilistic version of an agenda-drive"
W01-1812,P01-1044,1,0.922282,"ibe an extension of Dijkstra’s algorithm to B-graphs, which runs in time linear in the size of the graph.6 2.4 Practical Issues At this point, one might wonder what is left to be done. We have a reduction which, given a grammar G and a lattice L, allows us to build and score the induced graph. From this graph, we can use reachability algorithms to decide parse existence, and we can use shortest-path algorithms to find best parses. Furthermore, this view can be extended to other problems of parsing. For example, algorithms for summing paths can be adapted to calculate inside probabilities (see Klein and Manning (2001a)). However, there are two primary issues which remain. First, there is the issue of efficiency. Reachability and shortest-path algorithms, such as those cited above, generally run in time linear in the size of the induced graph. However, the size of the induced graph, while polynomial in the size of the lattice L, is exponential in the arity of the grammar G, having a term of jLjarity(G)+1 in its size. The implicit binarization of the grammar done by chart parsers is responsible for their cubic bounds, and we wish to preserve this bound for our Viterbi parsing. Second, one does not, in gener"
W01-1812,H91-1046,0,0.0282073,"terbi chart parser, if we later find a better way to form the NP, we will have to update not only the score of that NP, but also the score of any edge whose current score depends on that NP’s score. This can potentially lead to an extremely inefficient upward propagation of scores every time a new traversal is explored.8 Most exhaustive PCFG parsing work has used the bottom-up CKY algorithm (Kasami 1965, Younger 1967) with Chomsky Normal Form (CNF) Grammars (Baker 1979, Jelinek et al. 1992) or extended CKY parsers that work with n-ary branching grammars, but still not with empty constituents (Kupiec 1991, Chappelier and Rajman 1998). Such bottom-up parsers straightforwardly avoid the above problem, by always building all edges over shorter spans before building edges over longer spans which make use of them. However, such methods do not allow top-down grammar filtering, and often do not handle empty elements, cyclic unary productions, or n-ary rules. Stolcke (1995) presents a top-down parser for arbitrary PCFGs, which incorporates elements of ( ) 7 This ( ) has the theoretical – but not clearly useful – advantage of allowing the score combination function to vary per production. (1998) provid"
W01-1812,J97-2003,0,0.0181335,"he initial states of rules and are introduced in accordance with the grammar strategy (top-down, bottom-up, etc.). To hold the traversals or edges which have not yet been processed, a CP has a data structure called an agenda, which holds both traversals and introduction edges. Items from this agenda can be processed in any order whatsoever, even arbitrarily or randomly, without affecting the final chart contents. In our probabilistic chart parser (PCP), the central data structures are augmented with scores. Grammar rules, which were previously encoded as symbolic DFSAs are scored DFSAs, as in Mohri (1997), with a score for entering the initial state, a score on each transition, and, for each accepting state, a score for accepting in that state. Each edge e is also scored at all times. This value, s ore e (or s ore e; t at a time t), is the best estimate to date of that edge’s true best score,  e . In our algorithm, the estimate will always be conservative: s ore e will always be worse than or equal to  e . The full algorithm is shown in pseudocode in figure 6. It is broadly similar to a standard categorical chart parsing algorithm. However, in order to solve the problem of entering edges int"
W01-1812,P83-1021,0,0.0876085,"parser with an O(n3 ) time bound for arbitrary PCFGs, while preserving as much of the flexibility of symbolic chart parsers as allowed by the inherent ordering of probabilistic dependencies. 1 Introduction An influential view of parsing is as a process of logical deduction, in which a parser is presented as a set of parsing schemata. The grammar rules are the logical axioms, and the question of whether or not a certain category can be constructed over a certain span becomes the question of whether that category can be derived over that span, treating the initial words as starting assumptions (Pereira and Warren 1983, Shieber et al. 1995, Sikkel and Nijholt 1997). But such a viewpoint is less natural when we turn to probabilistic parsers, since probabilities, or, generalizing, scores, are not an organic part of logical systems.1 There is also a deep connection between logic, in particular propositional satisfiability, and directed hypergraphs (Gallo et al. 1993). In this paper, we develop and exploit the third side of this triangle, directly connecting parsing with directed hypergraph algorithms. The advantage of doing this is that scored arcs are a central and well-studied concept of graph theory, and we"
W01-1812,J95-2002,0,0.135611,"Algorithm Agenda-based active chart parsing (Kay 1980, Pereira and Shieber 1987) is an attractive presentation of the central ideas of tabular methods for CFG parsing. Earley (1970)-style dotted items combine via deduction steps (“the fundamental rule”) in an order-independent manner, such that the same basic algorithm supports top-down, bottom-up, and left-corner parsing, and the parser deals naturally and correctly with the difficult cases of left-recursive rules, empty elements, and unary rules. However, while O n3 methods for parsing PCFGs are well known (Baker 1979, Jelinek et al. 1992, Stolcke 1995), a O n3 probabilistic parser corresponding to active chart parsing for categorical CFGs, has not yet been provided. Producing a probabilistic version of an agenda-driven chart parser is not trivial. A central idea of such parsers is that the algorithm is correct and complete regardless of the order in which items on the agenda are processed. Achieving this is straightforward for categorical parsers, but problematic for probabilistic parsers. For example, consider extending an active edge VP!V. NP PP :[1,2] with an NP :[2,5] to form an edge VP ! V NP. PP over [1,5]. In a categorical chart pars"
W02-0811,J96-1002,0,0.00125762,"each (classifier, chosen sense, correct sense) triple. However, most senses are rarely chosen and rarely correct, so most features had zero or singleton support. f i (s, s1 , . . . , sk ) = 1 ⇐⇒ s = si v(s) = P i λi δ(si = s) The indicators δ are true for exactly one sense, and correspond to the simple f i defined above.4 The sense with the largest vote v(s) will be the sense with the highest posterior probability P(s|s 1 , . . . sk ) and will be chosen. For the maximum entropy classifier, we estimate the weights by maximizing the likelihood of a heldout set, using the standard IIS algorithm (Berger et al., 1996). For both weighted schemes, we found that stopping the iterative procedures before convergence gave better results. IIS was halted after 50 rounds, while EM was halted after a single round. Both methods were initialized to uniform starting weights. More importantly than changing the exact weight estimates, moving from method to method triggers broad qualitative changes in what kind of weights are allowed. With majority voting, classifiers all have equal, positive weights. With weighted voting, the weights are no longer required to be equal, but are still non-negative. With maximum entropy wei"
W02-0811,W96-0208,0,0.00775637,"scoring teams’ systems, the combination achieves high performance. We discuss trade-offs and empirical performance. Finally, we present an analysis of the combination, examining how ensemble performance depends on error independence and task difficulty. 1 Introduction The problem of supervised word sense disambiguation (WSD) has been approached using many different classification algorithms, including naive-Bayes, decision trees, decision lists, and memory-based learners. While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies. 1 This was supported by the S ENSEVAL -2 results, where a This paper is based on work supported in part by the National Science Foundation under Grants IIS-0085896 and IIS9982226, by an NSF Graduate Fellowship, and by the Research Collaboration between NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation and CSLI, Stanford University. 1 In fact, we have observed that differences between implementations of a single classifier type, such as smoothing or window size"
W02-0811,S01-1040,0,\N,Missing
W02-0811,W00-1304,0,\N,Missing
W02-0811,W02-1005,0,\N,Missing
W02-0811,P00-1027,0,\N,Missing
W02-0811,P00-1035,0,\N,Missing
W02-0811,N01-1006,0,\N,Missing
W02-0811,J01-3001,0,\N,Missing
W02-0811,J95-4004,0,\N,Missing
W02-0811,P98-1081,0,\N,Missing
W02-0811,C98-1078,0,\N,Missing
W02-0811,W02-1004,0,\N,Missing
W02-0811,P98-1029,0,\N,Missing
W02-0811,C98-1029,0,\N,Missing
W02-0811,A00-2009,0,\N,Missing
W02-1002,J96-1002,0,0.00745968,"Missing"
W02-1002,A00-1031,0,0.0596498,"Missing"
W02-1002,P96-1024,0,0.0255577,"Missing"
W02-1002,P01-1042,0,0.0925924,"Missing"
W02-1002,J98-1006,0,0.0312336,"Missing"
W02-1002,P99-1023,0,0.061857,"Missing"
W02-1002,W00-1308,1,0.410612,"Missing"
W02-1012,1995.tmi-1.18,0,0.0499084,"Missing"
W02-1012,J00-2004,0,0.0876766,"Missing"
W02-1012,C00-2163,0,0.235451,"Missing"
W02-1012,P00-1056,0,0.359957,"Missing"
W02-1012,W99-0604,0,0.112323,"Missing"
W02-1012,C96-2141,0,0.842545,"Missing"
W02-1012,P01-1067,0,0.170331,"Missing"
W02-1012,J93-2003,0,\N,Missing
W02-2030,J99-2004,0,0.0349187,"n the present experiments we have not explored this fully. The nodes in the derivation trees represent combining rule schemas of the HPSG grammar, and not IMPER HCOMP HCOMP BSE_VERB_INFL BSE_VERB_INFL US LET_V1 us SEE_V3 see let Figure 1: Derivation tree for the sentence Let us see phrasal categories of the standard sort. The whole HPSG analyses can be recreated from the derivation trees, using the grammar. The preterminals of the derivation trees are lexical labels. These are much finer grained than Penn Treebank preterminals tags, and more akin to those used in TreeAdjoining Grammar models (Bangalore and Joshi, 1999). There are a total of about 8, 000 lexical labels occurring in the treebank. One might conjecture that a supertagging approach could go a long way toward parse disambiguation. However, an upper bound for such an approach for our corpus is below 55 percent parse selection accuracy, which is the accuracy of an oracle tagger that chooses at random among the parses having the correct tag sequence (Oepen et al., 2002). The semantic dependency trees are labelled with relations most of which correspond to words in the sentence. These labels provide some abstraction because some classes of words have"
W02-2030,P97-1003,0,0.487196,"e use PCFG models to select features for log linear models. Even though this method may be expected to be suboptimal, it proves to be useful. We select features for PCFGs using decision trees and use the same features in a conditional log linear model. We compare the performance of the two models using equivalent features. Our PCFG models are comparable to branching process models for parsing the Penn Treebank, in which the next state of the model depends on a history of features. In most recent parsing work the history consists of a small number of manually selected features (Charniak, 1997; Collins, 1997). Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation. Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. (1998). Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set"
W02-2030,P01-1019,0,0.0436747,"lickinger, 2000). The current preliminary version contains 10,000 sentences of spoken dialog material drawn from the Verbmobil project. The Redwoods treebank makes available the entire HPSG signs for sentence analyses, but we have used in our experiments only small subsets of this representation. These are (i) derivation trees composed of identifiers of lexical items and constructions used to build the analysis, and (ii) semantic dependency trees which encode semantic head-tohead relations. The Redwoods treebank provides deeper semantics expressed in the Minimum Recursion Semantics formalism (Copestake et al., 2001), but in the present experiments we have not explored this fully. The nodes in the derivation trees represent combining rule schemas of the HPSG grammar, and not IMPER HCOMP HCOMP BSE_VERB_INFL BSE_VERB_INFL US LET_V1 us SEE_V3 see let Figure 1: Derivation tree for the sentence Let us see phrasal categories of the standard sort. The whole HPSG analyses can be recreated from the derivation trees, using the grammar. The preterminals of the derivation trees are lexical labels. These are much finer grained than Penn Treebank preterminals tags, and more akin to those used in TreeAdjoining Grammar m"
W02-2030,P98-1083,0,0.0152973,"s models for parsing the Penn Treebank, in which the next state of the model depends on a history of features. In most recent parsing work the history consists of a small number of manually selected features (Charniak, 1997; Collins, 1997). Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation. Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. (1998). Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set (Hermjakob and Mooney, 1997). Our experiments in feature selection using decision trees suggest that single decision trees may not be able to make optimal use of a large number of relevant features. This may be due to the greedy search procedures or to the fact that trees combine information from different features only through partitioning of the space. For example they have di"
W02-2030,P97-1062,0,0.0230583,"Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation. Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. (1998). Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set (Hermjakob and Mooney, 1997). Our experiments in feature selection using decision trees suggest that single decision trees may not be able to make optimal use of a large number of relevant features. This may be due to the greedy search procedures or to the fact that trees combine information from different features only through partitioning of the space. For example they have difficulty in weighing evidence from different features without fully partitioning the space. A common approach to overcoming some of the problems with decision trees – such as reducing their variance or increasing their representational power – has"
W02-2030,P99-1069,0,0.561774,"en et al., 2002). HPSG (Head-driven Phrase Structure Grammar) is a modern constraintbased lexicalist (unification) grammar, described in Pollard and Sag (1994). The Redwoods treebank makes available syntactic and semantic analyses of much greater depth than, for example, the Penn Treebank. Therefore there are a large number of features available that could be used by stochastic models for disambiguation. Other researchers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models a.k.a. Stochastic Unification Based Grammars (Johnson et al., 1999; Riezler et al., 2000). Here we also use log linear models to estimate conditional probabilities of sentence analyses. Since feature selection is almost prohibitive for these models, because of high computational costs, we use PCFG models to select features for log linear models. Even though this method may be expected to be suboptimal, it proves to be useful. We select features for PCFGs using decision trees and use the same features in a conditional log linear model. We compare the performance of the two models using equivalent features. Our PCFG models are comparable to branching process m"
W02-2030,J98-4004,0,0.100644,"Missing"
W02-2030,P95-1037,0,0.0890931,"to branching process models for parsing the Penn Treebank, in which the next state of the model depends on a history of features. In most recent parsing work the history consists of a small number of manually selected features (Charniak, 1997; Collins, 1997). Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation. Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. (1998). Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set (Hermjakob and Mooney, 1997). Our experiments in feature selection using decision trees suggest that single decision trees may not be able to make optimal use of a large number of relevant features. This may be due to the greedy search procedures or to the fact that trees combine information from different features only through partitioning of the space."
W02-2030,C02-2025,1,0.896002,"learned PCFG grammars and log linear models over the same features. 1 Introduction Hand-built NLP grammars frequently have a depth of linguistic representation and constraints not present in current treebanks, giving them potential importance for tasks requiring deeper processing. On the other hand, these manually built grammars need to solve the disambiguation problem to be practically usable. This paper presents work on the problem of probabilistic parse selection from among a set of alternatives licensed by a hand-built grammar in the context of the newly developed Redwoods HPSG treebank (Oepen et al., 2002). HPSG (Head-driven Phrase Structure Grammar) is a modern constraintbased lexicalist (unification) grammar, described in Pollard and Sag (1994). The Redwoods treebank makes available syntactic and semantic analyses of much greater depth than, for example, the Penn Treebank. Therefore there are a large number of features available that could be used by stochastic models for disambiguation. Other researchers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models a.k.a. Stochastic Unification Based Grammars (Johnson et al.,"
W02-2030,P00-1061,0,0.0767778,"(Head-driven Phrase Structure Grammar) is a modern constraintbased lexicalist (unification) grammar, described in Pollard and Sag (1994). The Redwoods treebank makes available syntactic and semantic analyses of much greater depth than, for example, the Penn Treebank. Therefore there are a large number of features available that could be used by stochastic models for disambiguation. Other researchers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models a.k.a. Stochastic Unification Based Grammars (Johnson et al., 1999; Riezler et al., 2000). Here we also use log linear models to estimate conditional probabilities of sentence analyses. Since feature selection is almost prohibitive for these models, because of high computational costs, we use PCFG models to select features for log linear models. Even though this method may be expected to be suboptimal, it proves to be useful. We select features for PCFGs using decision trees and use the same features in a conditional log linear model. We compare the performance of the two models using equivalent features. Our PCFG models are comparable to branching process models for parsing the P"
W02-2030,C98-1080,0,\N,Missing
W03-0428,J97-3003,0,\N,Missing
W03-0428,W96-0213,0,\N,Missing
W03-0428,A97-1030,0,\N,Missing
W03-0428,W99-0612,0,\N,Missing
W03-0428,A97-1029,0,\N,Missing
W04-0902,copestake-flickinger-2000-open,0,0.0772614,"ies the implicature that in some possible solution, three sculptures are indeed exhibited in the same room. Systematic calculation of presuppositions and implicatures has been given less attention in NLP and is less understood than the calculation of meaning. Yet computing and verifying them can provide valuable hints to the system whether it understood the meaning of the text correctly. 4 Morpho-Syntactic Analysis While traditional hand-built grammars often include a rich semantics, we have found their coverage inadequate for the logic puzzles task. For example, the English Resource Grammar (Copestake and Flickinger, 2000) fails to parse any of the sentences in Figure 1 for lack of coverage of some words and of several different syntactic structures; and parsable simplified versions of the text produce dozens of unranked parse trees. For this reason, we use a broadcoverage statistical parser (Klein and Manning, 2003) trained on the Penn Treebank. In addition to robustness, treebank-trained statistical parsers have the benefit of extensive research on accurate ambiguity resolution. Qualitatively, we have found that the output of the parser on logic puzzles is quite good (see §10). After parsing, each word in the"
W04-0902,J02-3001,0,0.0173597,"ambiguities, and incomplete domain knowledge. Recent work in NLP has consequently focused on more robust, broadcoverage techniques, but with the effect of overall shallower levels of processing. Thus, state-of-the-art work on probabilistic parsing (e.g., (Collins, 1999)) provides a good solution to robust, broad coverage parsing with automatic and frequently successful ambiguity resolution, but has largely ignored issues of semantic interpretation. The field of Question Answering (Pasca and Harabagiu, 2001; Moldovan et al., 2003) focuses on simple-fact queries. And socalled semantic parsing (Gildea and Jurafsky, 2002) provides as end output only a flat classification of semantic arguments of predicates, ignoring much of the semantic content, such as quantifiers. A major research question that remains unanswered is whether there are methods for get† Department of Linguistics Stanford University Stanford, CA 94305-2150, USA rog@stanford.edu ting from a robust “parse-anything” statistical parser to a semantic representation precise enough for knowledge representation and automated reasoning, without falling afoul of the same problems that stymied the broad application of traditional approaches. This paper pre"
W04-0902,W01-0521,0,0.0288028,"emmer. A few tree-transformation rules are applied on the parse trees to make them more convenient for combinatorial semantics. Most of them are general, e.g. imposing a binary branching structure on verb phrases, and grouping expressions like “more than”. A few of them correct some parsing errors, such as nouns marked as names and vice-versa. There is growing awareness in the probabilistic parsing literature that mismatches between training and test set genre can degrade parse accuracy, and that small amounts of correct-genre data can be more important than large amounts of wrong-genre data (Gildea, 2001); we have found corroborating evidence in misparsings of noun phrases common in puzzle texts, such as “Sculptures C and E”, which do not appear in the Wall Street Journal corpus. Depending on the severity of this problem, we may hand-annotate a small amount of puzzle texts to include in parser training data. 5 Combinatorial Semantics Work in NLP has shifted from hand-built grammars that need to cover explicitly every sentence structure and that break down on unexpected inputs to more robust statistical parsing. However, grammars that involve precise semantics are still largely hand-built (e.g."
W04-0902,P99-1042,0,0.0587378,"information-extraction and question-answering as a substitute for deep understanding. A prerequisite for successful inference is precise understanding of semantic phenomena like modals and quantifiers, in contrast with much current NLP work that just ignores such items. We believe that representations with a well-defined model-theoretic semantics are required. Finally, the task has a clear evaluation metric because the puzzle texts are designed to yield exactly one correct answer to each multiplechoice question. Moreover, the domain is another example of “found test material” in the sense of (Hirschman et al., 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA. While our current system is not a real world application, we believe that the methods being developed could be used in applications such as a computerized office assistant that must understand requests such as: “Put each file containing a task description in a different directory.” 2 (B) Sculptures E and H (C). . . System Overview This section explains the languages we use to represent"
W04-0902,P03-1054,1,0.0301978,"luable hints to the system whether it understood the meaning of the text correctly. 4 Morpho-Syntactic Analysis While traditional hand-built grammars often include a rich semantics, we have found their coverage inadequate for the logic puzzles task. For example, the English Resource Grammar (Copestake and Flickinger, 2000) fails to parse any of the sentences in Figure 1 for lack of coverage of some words and of several different syntactic structures; and parsable simplified versions of the text produce dozens of unranked parse trees. For this reason, we use a broadcoverage statistical parser (Klein and Manning, 2003) trained on the Penn Treebank. In addition to robustness, treebank-trained statistical parsers have the benefit of extensive research on accurate ambiguity resolution. Qualitatively, we have found that the output of the parser on logic puzzles is quite good (see §10). After parsing, each word in the resulting parse trees is converted to base form by a stemmer. A few tree-transformation rules are applied on the parse trees to make them more convenient for combinatorial semantics. Most of them are general, e.g. imposing a binary branching structure on verb phrases, and grouping expressions like"
W04-0902,N03-1022,0,0.076694,"Missing"
W04-0902,J82-3002,0,0.389051,"nford University Stanford, CA 94305-9040, USA {iddolev|wcmac|manning}@cs.stanford.edu Abstract This paper presents intial work on a system that bridges from robust, broad-coverage natural language processing to precise semantics and automated reasoning, focusing on solving logic puzzles drawn from sources such as the Law School Admission Test (LSAT) and the analytic section of the Graduate Record Exam (GRE). We highlight key challenges, and discuss the representations and performance of the prototype system. 1 Introduction Traditional approaches to natural language understanding (Woods, 1973; Warren and Pereira, 1982; Alshawi, 1992) provided a good account of mapping from surface forms to semantic representations, when confined to a very limited vocabulary, syntax, and world model, and resulting low levels of syntactic/semantic ambiguity. It is, however, difficult to scale these methods to unrestricted, general-domain natural language input because of the overwhelming problems of grammar coverage, unknown words, unresolvable ambiguities, and incomplete domain knowledge. Recent work in NLP has consequently focused on more robust, broadcoverage techniques, but with the effect of overall shallower levels of"
W04-0902,J03-4003,0,\N,Missing
W04-1217,A00-1031,0,0.00462315,"escribing the immediate content and context of each word, including the word itself, the previous and next words, word prefixes and suffix of up to a length of 6 characters, word shapes, and features describing the named entity tags assigned to the previous words. Word shapes refer to a mapping of each word onto equivalence classes that encodes attributes such as length, capitalization, numerals, greek letters, and so on. For instance, “Varicella-zoster” would become Xxxxx, “mRNA” would become xXXX, and “CPA1” would become XXXd. We also incorporated part-ofspeech tagging, using the TnT tagger(Brants, 2000) retrained on the GENIA corpus gold standard partof-speech tagging. We also used various interaction terms (conjunctions) of these base-level features in various ways. The full set of local features is outlined in Table 1. 2.2 External Resources We made use of a number of external resources, including gazetteers, web-querying, use of the surrounding abstract, and frequency counts from the British National Corpus. System Description Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the 88 Word Features TnT POS Prefix/suffix Abbreviations Word Shape P"
W04-1217,W03-0424,0,0.0107123,"ations of each pattern to the web, using the Google API, and obtained the number of hits. The pattern that returned the highest number of hits determined the feature value (e.g., “web-protein”, or “web-RNA”). If no hits were returned by any pattern, a value “O-web” was assigned. This value was also assigned to all words whose frequency was higher than 10 (using yet another value for words with higher frequency did not improve the tagger’s performance). 2.2.4 Abstracts A number of NER systems have made effective use of how the same token was tagged in different parts of the same document (see (Curran and Clark, 2003) and (Mikheev et al., 1999)). A token which appears in an unindicative context in one sentence may appear in a very obvious context in another sentence in the same abstract. To leverage this we tagged each abstract twice, providing for each token a feature indicating whether it was tagged as an entity elsewhere in the abstract. This information was only useful when combined with information on frequency. Table 1: Local Features (+ indicates conjunction) 2.2.1 Frequency Many entries in gazetteers are ambiguous words, occasionally used in the sense that the gazetteer seeks to represent, but at l"
W04-1217,P03-1054,1,0.0225709,"words are a common source of error and their classification is more likely to benefit from the use of external resources. We assigned each word in the training and testing data a frequency category corresponding to its frequency in the British National Corpus, a 100 million word balanced corpus, and used conjunctions of this category and certain other features. 2.3 Deeper Syntactic Features While the local features discussed earlier are all fairly surface level, our system also makes use of deeper syntactic features. We fully parsed the training and testing data using the Stanford Parser of (Klein and Manning, 2003) operating on the TnT part-of-speech tagging – we believe that the unlexicalized nature of this parser makes it a particularly suitable statistical parser to use when there is a large domain mismatch between the training material (Wall Street Journal text) and the target domain, but have not yet carefully evaluated this. Then, for each word in the sentence which is inside a noun phrase, the head and governor of the noun phrase are extracted. These features are not very useful when identifying only two classes (such as GENE and OTHER in the BioCreative task), but they were quite useful for this"
W04-1217,W03-0428,1,0.423703,"Missing"
W04-1217,M98-1021,0,0.0342742,"Missing"
W04-1217,E99-1001,0,0.0346474,"web, using the Google API, and obtained the number of hits. The pattern that returned the highest number of hits determined the feature value (e.g., “web-protein”, or “web-RNA”). If no hits were returned by any pattern, a value “O-web” was assigned. This value was also assigned to all words whose frequency was higher than 10 (using yet another value for words with higher frequency did not improve the tagger’s performance). 2.2.4 Abstracts A number of NER systems have made effective use of how the same token was tagged in different parts of the same document (see (Curran and Clark, 2003) and (Mikheev et al., 1999)). A token which appears in an unindicative context in one sentence may appear in a very obvious context in another sentence in the same abstract. To leverage this we tagged each abstract twice, providing for each token a feature indicating whether it was tagged as an entity elsewhere in the abstract. This information was only useful when combined with information on frequency. Table 1: Local Features (+ indicates conjunction) 2.2.1 Frequency Many entries in gazetteers are ambiguous words, occasionally used in the sense that the gazetteer seeks to represent, but at least as frequently not. So"
W04-1217,M98-1004,0,\N,Missing
W04-1217,M98-1012,0,\N,Missing
W04-1217,M98-1014,0,\N,Missing
W04-3201,P04-1014,0,0.091323,"Missing"
W04-3201,P02-1036,0,0.0719483,"Missing"
W04-3201,P99-1069,0,0.179281,"Missing"
W04-3201,P01-1042,0,0.410478,"regression and support vector machines (SVMs) in flat classification tasks like text categorization, word-sense disambiguation, and relevance routing has been repeatedly demonstrated. For sequence tasks like part-of-speech tagging or named-entity extraction, recent top-performing systems have also generally been based on discriminative sequence models, like conditional Markov models (Toutanova et al., 2003) or conditional random fields (Lafferty et al., 2001). A number of recent papers have considered discriminative approaches for natural language parsing (Johnson et al., 1999; Collins, 2000; Johnson, 2001; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004; Kaplan et al., 2004; Collins, 2004). Broadly speaking, these approaches fall into two categories, reranking and dynamic programming approaches. In reranking methods (Johnson et al., 1999; Collins, 2000; Shen et al., 2003), an initial parser is used to generate a number of candidate parses. A discriminative model is then used to choose between these candidates. In dynamic programming methods, a large number of candidate parse trees are represented compactly in a parse tree forest or chart. Given sufficiently “local” feat"
W04-3201,N04-1013,0,0.0494885,"Missing"
W04-3201,P03-1054,1,0.0643329,"ant fact about this kind of training is that, similar to the basic perceptron approach, it only requires picking up sentences one at a time, checking what the best parse is according to the current primal and dual weights, and adjusting the weights. each model and setting trained and tested on only the sentences of length ≤ 15 words. Aside from the length restriction, we used the standard splits: sections 2-21 for training (9753 sentences), 22 for development (603 sentences), and 23 for final testing (421 sentences). As a baseline, we trained a CNF transformation of the unlexicalized model of Klein and Manning (2003) on this data. The resulting grammar had 3975 non-terminal symbols and contained two kinds of productions: binary nonterminal rewrites and tag-word rewrites.5 The scores for the binary rewrites were estimated using unsmoothed relative frequency estimators. The tagging rewrites were estimated with a smoothed model of P (w|t), also using the model from Klein and Manning (2003). Figure 3 shows the performance of this model (generative): 87.99 F1 on the test set. For the basic max-margin model, we used exactly the same set of allowed rewrites (and therefore the same set of candidate parses) as in"
W04-3201,W03-1012,0,0.0604909,"Missing"
W04-3201,N03-1033,1,0.110006,"generative baseline; this feature added little information, but made the learning phase faster. The second feature was the output of a flat classifier which was trained to predict whether single spans, in isolation, were constituents or not, based on a bundle of features including the list above, but also the following: the preceding, first, last, and following tag in the span, pairs of tags such as preceding-first, last-following, preceding-following, first-last, and the entire tag sequence. Tag features on the test sets were taken from a pretagging of the sentence by the tagger described in Toutanova et al. (2003). While the flat classifier alone was quite poor (P 78.77 / R 63.94 / F1 70.58), the resulting max-margin model (lexical+aux) scored 89.12 F1 . To situate these numbers with respect to other models, the parser in Collins (1999), which is generative, lexicalized, and intricately smoothed scores 88.69 over the same train/test configuration. It is worth considering the cost of this kind of method. At training time, discriminative methods are inherently expensive, since they all involve iteratively checking current model performance on the training set, which means parsing the training set (usuall"
W04-3201,J03-4003,0,\N,Missing
W04-3201,W01-1802,0,\N,Missing
W04-3220,W00-1320,0,0.0720793,"Missing"
W04-3220,J03-4003,0,0.0103255,"of joint inference over verb senses and their subcategorization frames (SCFs). Verb Sense and Subcategorization 2:30:00 2:30:01 2:42:04 ∅ 4 1 12 NP 1 7 0 PP 0 0 3 NPPP 0 4 0 VPto 20 0 0 VPing 33 0 1 Table 2: The learned joint distribution over the senses and subcategorizations of the verb begin (in percent probability). Low probability senses and subcategorizations have been omitted. ing a probabilistic framework to represent subcategorization preferences, where each lexical item has a corresponding distribution over the possible sets of arguments. Modeling these distributions may be useful: Collins (2003) has shown that verb subcategorization information can be used to improve syntactic parsing performance. It has also been recognized that a much more accurate prediction of verb subcategorization preference can be made if conditioned on the sense of the verb. Roland and Jurafsky (2002) conclude that for a given lexical token in English, verb sense is the best determiner of SCF, far outweighing either genre or dialect. Demonstrating the utility of this, Korhonen and Preiss (2003) achieve significant improvement at a verb subcategorization acquisition task by conditioning on the verb sense as pr"
W04-3220,P03-1054,1,0.0254712,"ny possible sentences, a multinomial representation is infeasible, and we instead chose to encode the distribution using a set of probabilistic context free grammars (PCFGs). A PCFG is created for each possible SCF: each PCFG yields only parse trees in which the distinguished verb subcategorizes in the specified manner (but other verbs can parse freely). Given a SCF-specific PCFG, we can determine the probability of the sentence using the inside algorithm, which sums the probabilities of all possible trees in the grammar producing the sentence. To do this, we modified the exact PCFG parser of Klein and Manning (2003). In the independent SCF model, to infer the most likely SCF given a sentence P(C|S), we just find the C that maximizes P(S|C)P(C). (For the independent model, the SCF prior is estimated using MLE from the training examples.) Inference in the joint model over sense and SCF is more complex, and is described below. Learning this model, SCF-specific PCFGs, from our SCF-annotated training data, requires some care. Commonly PCFGs are learned using MLE of rewrite rule probabilities from large sets of treeannotated sentences. Thus to learn SCF-specific PCFGs, it seems that we should select a set of a"
W04-3220,P03-1007,0,0.0336275,"h lexical item has a corresponding distribution over the possible sets of arguments. Modeling these distributions may be useful: Collins (2003) has shown that verb subcategorization information can be used to improve syntactic parsing performance. It has also been recognized that a much more accurate prediction of verb subcategorization preference can be made if conditioned on the sense of the verb. Roland and Jurafsky (2002) conclude that for a given lexical token in English, verb sense is the best determiner of SCF, far outweighing either genre or dialect. Demonstrating the utility of this, Korhonen and Preiss (2003) achieve significant improvement at a verb subcategorization acquisition task by conditioning on the verb sense as predicted by a statistical word sense disambiguation system. Conversely, if different senses have distinct subcategorization preferences, it is reasonable to expect that information about the way a verb subcategorizes in a particular case may be of significant utility in determining the verb’s sense. As an example, Yarowsky (2000) makes use of rich syntactic features to improve the performance of a supervised WSD system. As an illustration of this correlation, Table 2 shows a lear"
W04-3222,A00-2018,0,0.388938,"s to devise a kernel function measures the similarity between inputs and . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking    models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuk"
W04-3222,P97-1003,0,0.182217,"rnel function measures the similarity between inputs and . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking    models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003)."
W04-3222,W01-1802,0,0.0505021,"Missing"
W04-3222,C02-2025,1,0.832357,"IMPER verb HCOMPverb HCOMP verb HCOMP verb HCOMP verb HCOMP verb HCOMP verb HCOMP verb LET V1 PLAN ON V2 HCOMP prep* let (v sorb) plan (v e p itrs) ON HCOMPverb LET V1 US let us HCOMP verb PLAN ON V2 plan HCOMP prep* ON THAT DEIX on that Figure 1: Derivation tree for the sentence Let us plan on that. paths (strings) allows us to explore string kernels on these paths and combine them into tree kernels. We apply these ideas in the context of parse disambiguation for sentence analyses produced by a Head-driven Phrase Structure Grammar (HPSG), the grammar formalism underlying the Redwoods corpus (Oepen et al., 2002). HPSG is a modern constraint-based lexicalist (or “unification”) grammar formalism.1 We build discriminative models using Support Vector Machines for ranking (Joachims, 1999). We compare our proposed representation to previous approaches and show that it leads to substantial improvements in accuracy. 2 The Leaf Projection Paths View of Parse Trees 2.1 Representing HPSG Signs In HPSG, sentence analyses are given in the form of HPSG signs, which are large feature structures containing information about syntactic and semantic properties of the phrases. As in some of the previous work on the Redw"
W04-3222,N04-1012,0,0.0559444,"Missing"
W04-3222,P00-1061,0,0.0341975,"lternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking    models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003). Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing. A good overview of kernels for structured data can be foun"
W04-3222,W03-0402,0,0.0182056,"class of machine learning algorithms, such an explicit representation is not necessary, and which it suffices to devise a kernel function measures the similarity between inputs and . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking    models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees"
W04-3222,P03-1005,0,0.0165955,"2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking    models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003). Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing. A good overview of kernels for structured data can be found in (Gaertner et al., 2002). Here we propose a new representation of parse trees which (i) allows the localization of broader useful context, (ii) paves the way for exploring kernels, and (iii) achieves superior disambiguation accuracy compared to models that use tree representations centered around context-free rules. Compared to the usual notion of discriminative models (placing classes on rich observed"
W04-3222,W02-2030,1,0.94334,"exicalist (or “unification”) grammar formalism.1 We build discriminative models using Support Vector Machines for ranking (Joachims, 1999). We compare our proposed representation to previous approaches and show that it leads to substantial improvements in accuracy. 2 The Leaf Projection Paths View of Parse Trees 2.1 Representing HPSG Signs In HPSG, sentence analyses are given in the form of HPSG signs, which are large feature structures containing information about syntactic and semantic properties of the phrases. As in some of the previous work on the Redwoods corpus (Toutanova et al., 2002; Toutanova and Manning, 2002), we use the derivation trees as the main representation for disambiguation. Derivation trees record the combining rule schemas of the HPSG grammar which were used to license the sign by combining initial lexical types. The derivation tree is also the fundamental data stored in the Redwoods treebank, since the full sign can be reconstructed from it by reference to the grammar. The internal nodes represent, for example, head-complement, head-specifier, and head-adjunct schemas, which were used to license larger signs out of component parts. A derivation tree for the 1 For an introduction to HPS"
W05-0623,W04-2412,0,0.287637,"Missing"
W05-0623,A00-2018,0,0.0651367,"Missing"
W05-0623,J02-3001,0,0.294789,"rn input to a joint model which can Our local model labels nodes in a parse tree independently. We decompose the probability over labels (all argument labels plus NONE), into a product of the probability over ARG and NONE, and a probability over argument labels given that a node is an ARG. This can be seen as chaining an identification and a classification model. The identification model classifies each phrase as either an argument or nonargument and our classification model labels each potential argument with a specific argument label. The two models use the same features. Previous research (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Carreras and M`arquez, 2004) has identified many useful features for local identification and classification. Below we list the features and hand-picked conjunctions of features used in our local models. The ones denoted with asterisks (*) were not present in (Toutanova et al., 2005). Although most of these features have been described in previous work, some features, described in the next section, are – to our knowledge – novel. 173 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 173–176, Ann Arbor, June 2005. 2005 Associat"
W05-0623,N04-1030,0,0.13848,"hich can Our local model labels nodes in a parse tree independently. We decompose the probability over labels (all argument labels plus NONE), into a product of the probability over ARG and NONE, and a probability over argument labels given that a node is an ARG. This can be seen as chaining an identification and a classification model. The identification model classifies each phrase as either an argument or nonargument and our classification model labels each potential argument with a specific argument label. The two models use the same features. Previous research (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Carreras and M`arquez, 2004) has identified many useful features for local identification and classification. Below we list the features and hand-picked conjunctions of features used in our local models. The ones denoted with asterisks (*) were not present in (Toutanova et al., 2005). Although most of these features have been described in previous work, some features, described in the next section, are – to our knowledge – novel. 173 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 173–176, Ann Arbor, June 2005. 2005 Association for Computational"
W05-0623,P05-1073,1,0.899946,"Missing"
W06-1601,P98-1013,0,0.0935632,"Missing"
W06-1601,A00-2018,0,0.0727325,"system proposes. As is customary, we divide the problem into two subtasks: identification (ID) and classification (CL). In the identification task, we identify the set of constituents which fill some role for a 5 Datasets and Evaluation We train our models with verb instances extracted from three parsed corpora: (1) the Wall Street Journal section of the Penn Treebank (PTB), which was parsed by human annotators (Marcus et al., 1993), (2) the Brown Laboratory for Linguistic Information Processing corpus of Wall Street Journal text (BLLIP), which was parsed automatically by the Charniak parser (Charniak, 2000), and (3) the Gigaword corpus of raw newswire text (GW), which we parsed ourselves with the Stanford parser. In all cases, when training a model, 6                    Verb (4 F1) give (+.436) work (+.206)             ! &quot;  pay (+.178)                    look (+.170)  Figure 3: Test set F1 as a function of training set size. rise (+.160) target verb: in our system we use simple rules to extract dependents of the target verb and their grammatical relations. In the classification task, the identified constituents are labeled for their semantic ro"
W06-1601,J02-3001,0,0.915865,"Missing"
W06-1601,P03-1054,1,0.00553425,"ny reasonable parse tree using simple deterministic rules. Our set does not include the relations direct object or indirect object, since this distinction can not be made deterministically on the basis of syntactic structure alone; instead, we opted to number the noun phrase (np), complement clause (cl, xcl), and adjectival complements (acomp) appearing in an unbroken sequence directly after the verb, since this is sufficient to capture the necessary syntactic information. The syntactic relations used in our experiments are computed from the typed dependencies returned by the Stanford Parser (Klein and Manning, 2003). 2 Learning Setting Our goal is to learn a model which relates a verb, its semantic roles, and their possible syntactic realizations. As is the case with most semantic role labeling research, we do not attempt to model the syntax itself, and instead assume the existence of a syntactic parse of the sentence. The parse may be from a human annotator, where available, or from an automatic parser. We can easily run our system on completely unannotated text by first running an automatic tokenizer, part-of-speech tagger, and parser to turn the text into tokenized, tagged sentences with associated pa"
W06-1601,P99-1051,0,0.04176,") = (np#1, ARG2, they/P RP ) (g4 , r4 , w4 ) = (np#2, ARG1, test/N N ) Figure 1: An example sentence taken from the Penn Treebank (wsj 2417), the verb instance extracted from it, and the values of the model variables for this instance. The semantic roles listed are taken from the PropBank annotation, but are not observed in the unsupervised training method. Table 1: The set of syntactic relations we use, where n ∈ {1, 2, 3} and x is a preposition. Korhonen (1998), which used a statistical model to identify verb alternations, relying on an existing taxonomy of possible alternations, as well as Lapata (1999), which searched a large corpus to find evidence of two particular verb alternations. There has also been some work on both clustering and supervised classification of verbs based on their alternation behavior (Stevenson and Merlo, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Finally, Swier and Stevenson (2004) perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data. However, we believe this is the first system to simultaneously discover verb roles and verb linking patterns from unsupervised data using a uni"
W06-1601,J93-2004,0,0.0327029,"the answers before evaluation, the amount of human knowledge that is given to the system is small: it corresponds to the effort required to hand assign a “name” to each label that the system proposes. As is customary, we divide the problem into two subtasks: identification (ID) and classification (CL). In the identification task, we identify the set of constituents which fill some role for a 5 Datasets and Evaluation We train our models with verb instances extracted from three parsed corpora: (1) the Wall Street Journal section of the Penn Treebank (PTB), which was parsed by human annotators (Marcus et al., 1993), (2) the Brown Laboratory for Linguistic Information Processing corpus of Wall Street Journal text (BLLIP), which was parsed automatically by the Charniak parser (Charniak, 2000), and (3) the Gigaword corpus of raw newswire text (GW), which we parsed ourselves with the Stanford parser. In all cases, when training a model, 6                    Verb (4 F1) give (+.436) work (+.206)             ! &quot;  pay (+.178)                    look (+.170)  Figure 3: Test set F1 as a function of training set size. rise (+.160) target verb: in our system we u"
W06-1601,P98-2247,0,0.105503,"Missing"
W06-1601,J01-3003,0,0.0186133,"m the PropBank annotation, but are not observed in the unsupervised training method. Table 1: The set of syntactic relations we use, where n ∈ {1, 2, 3} and x is a preposition. Korhonen (1998), which used a statistical model to identify verb alternations, relying on an existing taxonomy of possible alternations, as well as Lapata (1999), which searched a large corpus to find evidence of two particular verb alternations. There has also been some work on both clustering and supervised classification of verbs based on their alternation behavior (Stevenson and Merlo, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Finally, Swier and Stevenson (2004) perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data. However, we believe this is the first system to simultaneously discover verb roles and verb linking patterns from unsupervised data using a unified probabilistic model. tax. We define a small set of syntactic relations, listed in Table 1, each of which describes a possible syntactic relationship between the verb and a dependent. Our goal was to choose a set that provides sufficient syntactic information for the semantic role de"
W06-1601,P05-1072,0,0.0331593,"Missing"
W06-1601,W05-0625,0,0.0436103,"Missing"
W06-1601,E99-1007,0,0.0303948,"s instance. The semantic roles listed are taken from the PropBank annotation, but are not observed in the unsupervised training method. Table 1: The set of syntactic relations we use, where n ∈ {1, 2, 3} and x is a preposition. Korhonen (1998), which used a statistical model to identify verb alternations, relying on an existing taxonomy of possible alternations, as well as Lapata (1999), which searched a large corpus to find evidence of two particular verb alternations. There has also been some work on both clustering and supervised classification of verbs based on their alternation behavior (Stevenson and Merlo, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Finally, Swier and Stevenson (2004) perform unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data. However, we believe this is the first system to simultaneously discover verb roles and verb linking patterns from unsupervised data using a unified probabilistic model. tax. We define a small set of syntactic relations, listed in Table 1, each of which describes a possible syntactic relationship between the verb and a dependent. Our goal was to choose a set that provides suffi"
W06-1601,C00-2108,0,\N,Missing
W06-1601,A97-1052,0,\N,Missing
W06-1601,C98-1013,0,\N,Missing
W06-1601,P93-1032,1,\N,Missing
W06-1601,C98-2242,0,\N,Missing
W06-1601,J05-1004,0,\N,Missing
W06-1673,N06-1006,1,0.50393,"Missing"
W06-1673,W05-0636,0,0.0294641,"Missing"
W06-1673,P05-1073,1,0.859258,"Missing"
W06-1673,E95-1015,0,0.0263242,"an ) (2) 3 Generating Samples The method we have outlined requires the ability to sample from the conditional distributions in the factored distribution of (1): in our case, the probability of a particular linguistic annotation, conditioned on other linguistic annotations. Note that this differs from the usual annotation task: taking the argmax. But for most algorithms the change is a small and easy change. We discuss how to obtain samples efficiently from a few different annotation models: probabilistic context free grammars (PCFGs), and conditional random fields (CRFs). 3.1 Sampling Parses Bod (1995) discusses parsing with probabilistic tree substitution grammars, which, unlike simple PCFGs, do not have a one-to-one mapping between output parse trees and a derivation (a bag of rules) that produced it, and hence the most-likely derivation may not correspond to the most likely parse tree. He therefore presents a bottom-up approach to sampling derivations from a derivation forest, which does correspond to a sample from the space of parse trees. Goodman (1998) presents a top-down version of this algorithm. Although we use a PCFG for parsing, it is the grammar of (Klein and Manning, 2003), whi"
W06-1673,W04-2412,0,0.0331843,"Missing"
W06-1673,W05-0620,0,0.0248131,"Missing"
W06-1673,A00-2018,0,0.140197,"Missing"
W06-1673,P02-1036,0,0.0213919,"exponential space cost, but in many relevant cases composition is in practice quite practical. Outside of WFSTs, maintaining entire probability distributions is usually infeasible in NLP, because for most intermediate tasks, such as parsing and named entity recognition, there is an exponential number of possible labelings. Nevertheless, for some models, such as most parsing models, these exponential labelings can be compactly represented in a packed form, e.g., (Maxwell and Kaplan, 1995; Crouch, 2005), and subsequent stages can be reengineered to work over these packed representations, e.g., (Geman and Johnson, 2002). However, doing this normally also involves a very high cognitive and engineering effort, and in practice this solution is infrequently adopted. Moreover, in some cases, a subsequent module is incompatible with the packed representation of a previous module and an exponential amount of work is nevertheless required within this architecture. entities and then parses it before passing it to the entailment decider. 2 Approach 2.1 Overview In order to do approximate inference, we model the entire pipeline as a Bayesian network. Each stage in the pipeline corresponds to a variable in the network."
W06-1673,W05-0623,1,0.849529,"Missing"
W06-1673,W05-1506,0,0.0116867,"Missing"
W06-1673,P03-1054,1,0.0460225,".1 Sampling Parses Bod (1995) discusses parsing with probabilistic tree substitution grammars, which, unlike simple PCFGs, do not have a one-to-one mapping between output parse trees and a derivation (a bag of rules) that produced it, and hence the most-likely derivation may not correspond to the most likely parse tree. He therefore presents a bottom-up approach to sampling derivations from a derivation forest, which does correspond to a sample from the space of parse trees. Goodman (1998) presents a top-down version of this algorithm. Although we use a PCFG for parsing, it is the grammar of (Klein and Manning, 2003), which uses extensive statesplitting, and so there is again a many-to-one correspondence between derivations and parses, and we use an algorithm similar to Goodman’s in our work. PCFGs put probabilities on each rule, such as S → NP VP and NN → ‘dog’. The probability of a parse is the product of the probabilities of the rules used to construct the parse tree. A dynamic programing algorithm, the inside algorithm, can be used to find the probability of a sentence. The a1 ,a2 ,...,an−1 Because we are summing out all variables other than the final one, we effectively use only the samples drawn fro"
W06-1673,W05-0625,0,0.028115,"Missing"
W07-1427,W02-1001,0,0.0111527,"Missing"
W07-1427,de-marneffe-etal-2006-generating,1,0.0621763,"Missing"
W07-1427,levy-andrew-2006-tregex,0,0.0313803,"age A core part of an entailment system is the ability to find semantically equivalent patterns in text. Previously, we wrote tedious graph traversal code by hand for each desired pattern. As a remedy, we wrote Semgrex, a pattern language for dependency graphs. We use Semgrex atop the typed dependencies from the Stanford Parser (de Marneffe et al., 2006b), as aligned in the alignment phase, to identify both semantic patterns in a single text and over two aligned pieces of text. The syntax of the language was modeled after tgrep/Tregex, query languages used to find syntactic patterns in trees (Levy and Andrew, 2006). This speeds up the process of graph search and reduces errors that occur in complicated traversal code. 5.1 Semgrex Features Rather than providing regular expression matching of atomic tree labels, as in most tree pattern languages, Semgrex represents nodes as a (nonrecursive) attribute-value matrix. It then uses regular expressions for subsets of attribute values. For example, {word:run;tag:/ˆNN/} refers to any node that has a value run for the attribute word and a tag that starts with NN, while {} refers to any node in the graph. However, the most important part of Semgrex is that it allow"
W07-1427,W07-1431,1,0.380995,"Missing"
W07-1427,N06-1006,1,\N,Missing
W07-1431,P85-1008,0,0.544875,"definition of a calculus of monotonicity (S´anchez Valencia, 1991). A small current of theoretical work has continued up to the present, for example (Zamansky et al., 2006). There has been surprisingly little work on building computational models of natural logic. (Fyodorov et al., 2003) describes a Prolog implementation for a small fragment of English, based on a categorial grammar parser.6 In an unpublished draft, (van Eijck, 2005) describes a preliminary implementation in Haskell. Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985). To our knowledge, the FraCaS results reported here represent the first such evaluation. (Sukkarieh, 2003) describes applying a deductive system to some FraCaS inferences, but does not perform a complete evaluation or report quantitative results. 7 Conclusion Our NatLog implementation of natural logic successfully handles a broad range of inferences involving monotonicity, as demonstrated on the FraCaS test suite. While a post-hoc analysis of performance on the RTE3 Challenge suggests that monotonicityrelated inferences have limited applicability in RTE data, the greater precision of the NatL"
W07-1431,P03-1054,1,0.0165935,"fferent, then h is downward-monotone. (Thus, wine has positive polarity in no meal without wine because it falls under two downward-monotone operators.) 3 The NatLog System Our natural logic system, dubbed the NatLog system, has a three-stage architecture similar to those in (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic pre-preprocessing, (2) alignment, and (3) entailment classification. 3.1 Linguistic pre-processing Relative to other textual inference systems, the NatLog system does comparatively little linguistic preprocessing. We rely on the Stanford parser (Klein and Manning, 2003), a Treebank-trained statistical parser, for tokenization, part-of-speech tagging, and phrase-structure parsing. By far the most important analysis performed at this stage is monotonicity marking, in which we compute the effective mono195 unary operator: without pattern: IN &lt; /ˆ[Ww]ithout$/ argument 1: monotonicity ↓ on dominating PP pattern: __ > PP=proj binary operator: most pattern: JJS &lt; /ˆ[Mm]ost$/ !> QP argument 1: monotonicity 6↑↓ on dominating NP pattern: __ >+(NP) (NP=proj !> NP) argument 2: monotonicity ↑ on dominating S pattern: __ >+(/.*/) (S=proj !> S) Figure 1: Two examples of"
W07-1431,levy-andrew-2006-tregex,0,0.0149269,"tence. For this, we use an adaptation of the marking algorithm of S´anchez Valencia (section 2); however, our choice of a Treebank-trained parser (driven by the goal of broad coverage) requires us to modify the algorithm substantially. Unlike the categorial grammar parses assumed by S´anchez Valencia, the nesting of constituents in phrase-structure parses does not always correspond to the composition of semantic functions, which introduces a number of complications. We define a list of downward-monotone and non-monotone expressions, and for each item we specify its arity and a Tregex pattern (Levy and Andrew, 2006) which permits us to identify its occurrences. We also specify, for each argument, both the monotonicity and another Tregex pattern which helps us to determine the sentence span over which the monotonicity is projected. (Figure 1 shows some example definitions.) The marking process computes these projections, performs monotonicity composition where needed, and marks each token span with its final effective monotonicity. 3.2 Alignment The second stage of processing establishes an alignment between the premise and the hypothesis. While there are many notions of alignment, in this work we have ch"
W07-1431,N06-1006,1,0.869137,"Missing"
W07-1431,W05-1201,0,0.0154889,"e monotonicities along the path from the root to each leaf in order to determine effective polarities. The composition of monotonicities is straightforward. Suppose h = f ◦ g. If either f or g is non-monotone, then so is h. Otherwise, if the monotonicities of f and g are the same, then h is upward-monotone; if they are different, then h is downward-monotone. (Thus, wine has positive polarity in no meal without wine because it falls under two downward-monotone operators.) 3 The NatLog System Our natural logic system, dubbed the NatLog system, has a three-stage architecture similar to those in (Marsi and Krahmer, 2005; MacCartney et al., 2006), comprising (1) linguistic pre-preprocessing, (2) alignment, and (3) entailment classification. 3.1 Linguistic pre-processing Relative to other textual inference systems, the NatLog system does comparatively little linguistic preprocessing. We rely on the Stanford parser (Klein and Manning, 2003), a Treebank-trained statistical parser, for tokenization, part-of-speech tagging, and phrase-structure parsing. By far the most important analysis performed at this stage is monotonicity marking, in which we compute the effective mono195 unary operator: without pattern: IN &lt;"
W07-1431,E06-1052,0,0.0165932,"with our natural logic system yields significant performance gains. 1 Introduction The last five years have seen a surge of interest in the problem of textual inference, that is, automatically determining whether a natural-language hypothesis can be inferred from a given premise. A broad spectrum of approaches have been explored, ranging from shallow-but-robust to deep-but-brittle. Up to now, the most successful approaches have used fairly impoverished semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006). Such methods, while robust and broadly effective, are imprecise, and are easily confounded by ubiquituous inferences involving monotonicity, particularly in negative polarity contexts, as in: P: No case of indigenously acquired rabies infection has been confirmed in the past 2 years. H: No rabies cases have been confirmed. Because it drops important qualifiers in a negative context, the hypothesis does not follow; yet both the lexical content and the predicate-argument structure of the hypothesis closely match the"
W07-1431,W07-1401,0,\N,Missing
W08-0304,P07-2045,0,0.0546783,"h all other candidate translations that have yet to be selected as the 1-best. And, for each of the n n-best lists, this may have to be done up to m − 1 times. 2.2 Search Strategies In this section, we review two search strategies that, in conjunction with the line search just described, can be used to drive MERT. The first, Powell’s method, was advocated by Och (2003) when MERT was first introduced for statistical machine translation. The second, which we call Koehn-coordinate descent (KCD)6 , is used by the MERT utility packaged with the popular Moses statistical machine translation system (Koehn et al., 2007). 6 Moses uses David Chiang’s CMERT package. Within the source file mert.c, the function that implements the overall search strategy, optimize koehn(), is based on Philipp Koehn’s Perl script for MERT optimization that was distributed with Pharaoh. 2.2.1 Powell’s Method 3.1 Powell’s method (Press et al., 2007) attempts to efficiently search the objective by constructing a set of mutually non-interfering search directions. The basic procedure is as follows: (i) A collection of search directions is initialized to be the coordinates of the space being searched; (ii) The objective is minimized by"
W08-0304,N03-1017,0,0.0204375,"hich the results are influenced by some runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. 32 Dev MT02 30.967 30.638 31.681 Test MT03 30.778 30.692 31.754 Test MT05 29.580 29.780 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly o"
W08-0304,P03-1021,0,0.778611,"Minimum error rate training (MERT) is a widely used learning procedure for statistical machine translation models. We contrast three search strategies for MERT: Powell’s method, the variant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 2 Minimum Error Rate Training Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och & Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant"
W08-0304,P02-1038,0,0.0634091,"ariant of coordinate descent found in the Moses MERT utility, and a novel stochastic method. It is shown that the stochastic method obtains test set gains of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 2 Minimum Error Rate Training Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och & Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant and thus cannot be optimized using gradient techniques, Och (2003) provides an approach that performs such training efficiently. In this paper we explore a number of variations on MERT. Fir"
W08-0304,J03-1002,0,0.00325925,"runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. 32 Dev MT02 30.967 30.638 31.681 Test MT03 30.778 30.692 31.754 Test MT05 29.580 29.780 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test"
W08-0304,P02-1040,0,0.106415,"of +0.98 BLEU on MT03 and +0.61 BLEU on MT05. We also present a method for regularizing the MERT objective that achieves statistically significant gains when combined with both Powell’s method and coordinate descent. 1 2 Minimum Error Rate Training Introduction Och (2003) introduced minimum error rate training (MERT) as an alternative training regime to the conditional likelihood objective previously used with log-linear translation models (Och & Ney, 2002). This approach attempts to improve translation quality by optimizing an automatic translation evaluation metric, such as the BLEU score (Papineni et al., 2002). This is accomplished by either directly walking the error surface provided by an evaluation metric w.r.t. the model weights or by using gradientbased techniques on a continuous approximation of such a surface. While the former is piecewise constant and thus cannot be optimized using gradient techniques, Och (2003) provides an approach that performs such training efficiently. In this paper we explore a number of variations on MERT. First, it is shown that performance gains can be had by making use of a stochastic search strategy as compare to that obtained by Powell’s method and Let F be a co"
W08-0304,W05-0908,0,0.0266415,"20 30.191 29.529 29.963 30.674 Table 4: BLEU scores obtained when regularizing using the average loss of adjacent plateaus, left, and the maximum loss of adjacent plateaus, right. The none entry for each search strategy represents the baseline where no regularization is used. Statistically significant test set gains, p &lt; 0.01, over the respective baselines are in bold face. of Powell’s method, diagonal search, with coordinate descent’s robustness to the sudden jumps between regions that result from global line minimization. Using an approximate randomization test for statistical significance (Riezler & Maxwell, 2005), and with KCD as a baseline, the gains obtained by stochastic search on MT03 are statistically significant (p = 0.002), as are the gains on MT05 (p = 0.005). Table 4 indicates that performing regularization by either averaging or taking the maximum of adjacent plateaus during the line search leads to gains for both Powell’s method and KCD. However, no reliable additional gains appear to be had when stochastic search is combined with regularization. It may seem surprising that the regularization gains for Powell & KCD are seen not only in the test sets but on the dev set as well. That is, in t"
W08-0304,P06-2101,0,0.284978,"tion to other data sets. This could be avoided by regularizing the surface in order to eliminate such spurious minima. One candidate for performing such regularization is the continuous approximation of the MERT objective, O = Epw (`). Och (2003) claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective, O = `. However, Zens et al. (2007) found that O = Epw (`) achieved substantially better test set performance than O = `, even though it performs slightly worse on the data used to train the parameters. Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temper8 However, we speculate that similar results could be obtained using a uniform distribution over (−1, 1) 31 ature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. However, the most straightforward implementation of such methods requires a loss that can be applied at the sentence level. If the evaluation metric of interest does not have this property (e.g. BLEU), the loss must be approximated using s"
W08-0304,N04-4026,0,0.0132182,"es obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powell’s method and KCD achieve a very similar level of performance, with KCD modestly outperforming Powell on the MT03 test set while Powell modestly outperforms coordinate descent on the MT05 test set. Moreover, the fact that Powell’s algorithm did not perform better than KCD on the training data10 , and in fact actually performed modestly worse, suggests that"
W08-0304,I05-3027,1,0.564376,"izable number of random restarts should be used in order to minimize the degree to which the results are influenced by some runs receiving starting points that are better in general or perhaps better/worse w.r.t. their specific optimization strategy. 32 Dev MT02 30.967 30.638 31.681 Test MT03 30.778 30.692 31.754 Test MT05 29.580 29.780 30.191 Table 3: BLEU scores obtained by models trained using the three different parameter search strategies: Powell’s method, KCD, and stochastic search. data. The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al., 2005). Phrases were extracted using the typical approach described in Koehn et al. (2003) of running GIZA++ (Och & Ney, 2003) in both directions and then merging the alignments using the grow-diag-final heuristic. From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al., 2007). A 5-gram language model was created using the SRI language modeling toolkit (Stolcke, 2002) and trained using the Gigaword corpus and English sentences from the parallel data. 5 Results As illustrated in table 3, Powe"
W08-0304,D07-1055,0,0.249878,"be 39.1 while all surrounding plateaus have a BLEU score that is &lt; 10. Intuitively, such a minima would be a very bad solution, as the resulting parameters would likely exhibit very poor generalization to other data sets. This could be avoided by regularizing the surface in order to eliminate such spurious minima. One candidate for performing such regularization is the continuous approximation of the MERT objective, O = Epw (`). Och (2003) claimed that this approximation achieved essentially equivalent performance to that obtained when directly using the loss as the objective, O = `. However, Zens et al. (2007) found that O = Epw (`) achieved substantially better test set performance than O = `, even though it performs slightly worse on the data used to train the parameters. Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temper8 However, we speculate that similar results could be obtained using a uniform distribution over (−1, 1) 31 ature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. However, the most straightforward imp"
W08-0336,N06-2013,0,0.0191538,"the role of a phrase table is to build domain and application appropriate larger chunks that are semantically coherent in the translation process. For example, even if the word for smallpox is treated as two one-character words, they can still appear in a phrase like “ →smallpox”, so that smallpox will still be a candidate translation when the system translates “ ” “ ”. Nevertheless, Xu et al. (2004) show that an MT system with a word segmenter outperforms a system working with individual characters in an alignment template approach. On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. U s U s Beyond this, there has been no finer-grained analysis of what style and size of word segmentation is optimal for MT. Moreover, most discussion of segmentation for other tasks relates to the size units to identify in the segmentation standard: whether to join or split noun compounds, for instance. People 224 Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics ge"
W08-0336,E03-1076,0,0.0686921,"d be optimal. This is because the role of a phrase table is to build domain and application appropriate larger chunks that are semantically coherent in the translation process. For example, even if the word for smallpox is treated as two one-character words, they can still appear in a phrase like “ →smallpox”, so that smallpox will still be a candidate translation when the system translates “ ” “ ”. Nevertheless, Xu et al. (2004) show that an MT system with a word segmenter outperforms a system working with individual characters in an alignment template approach. On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. U s U s Beyond this, there has been no finer-grained analysis of what style and size of word segmentation is optimal for MT. Moreover, most discussion of segmentation for other tasks relates to the size units to identify in the segmentation standard: whether to join or split noun compounds, for instance. People 224 Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, c Columbus, Ohio, USA, June 2008. 2008 Association for"
W08-0336,N03-1017,0,0.0587278,"Missing"
W08-0336,W06-0115,0,0.0202511,"Missing"
W08-0336,N06-2024,0,0.0124782,"was neither specifically designed nor optimized for MT, it seems reasonable to investigate whether any segmentation granularity in continuum between character-level and CTB-style segmentation is more effective for MT. In this section, we present a technique for directly optimizing a segmentation property—characters per token average— for translation quality, which yields significant improvements in MT performance. In order to calibrate the average word length produced by our CRF segmenter—i.e., to adjust the rate of word boundary predictions (yt = +1), we apply a relatively simple technique (Minkov et al., 2006) originally devised for adjusting the precision/recall tradeoff of any sequential classifier. Specifically, the weight vector w and feature vector of a trained linear sequence classifier are augmented at test time to include new class-conditional feature functions to bias the classifier towards particular class labels. In our case, since we wish to increase the frequency of word boundaries, we add a feature function:  1 if yt = +1 f0 (x, yt−1 , yt ,t) = 0 otherwise Its weight λ0 controls the extent of which the classifier will make positive predictions, with very large positive λ0 values caus"
W08-0336,W04-3236,0,0.00953984,"over-generates a big MT training lexicon and OOV words in MT test data, and thus causes a problem for MT. To improve a feature-based sequence model for MT, we propose 4 different approaches to deal with named entities, optimal length of word for MT and joint search for segmentation and MT decoding. 5.1 Making Use of External Lexicons One way to improve the consistency of the CRF model is to make use of external lexicons (which are not part of the segmentation training data) to add lexicon-based features. All the features we use are listed in Table 6. Our linguistic features are adopted from (Ng and Low, 2004) and (Tseng et al., 2005). There are three categories of features: Lexicon-based Features (1.1) LBegin (Cn ), n ∈ [−2, 1] (1.2) LMid (Cn ), n ∈ [−2, 1] (1.3) LEnd (Cn ), n ∈ [−2, 1] (1.4) LEnd (C−1 ) + LEnd (C0 ) +LEnd (C1 ) (1.5) LEnd (C−2 ) + LEnd (C−1 ) +LBegin (C0 ) + LMid (C0 ) (1.6) LEnd (C−2 ) + LEnd (C−1 ) +LBegin (C−1 ) +LBegin (C0 ) + LMid (C0 ) Segmentation Performance F measure OOV Recall CRF-Lex-NR 0.943 0.753 CRF-Lex 0.940 0.729 MT Performance Segmenter MT03 (dev) MT05 (test) CRF-Lex-NR 32.96 31.27 CRF-Lex 32.70 30.95 Linguistic Features (2.1) Cn , n ∈ [−2, 1] (2.2) Cn−1Cn , n ∈"
W08-0336,J03-1002,0,0.0118059,"Missing"
W08-0336,P03-1021,0,0.0842894,") alignments, and using Moses’ grow-diag alignment symmetrization heuristic.1 We set the maximum phrase length to a large value (10), because some segmenters described later in this paper will result in shorter 1 In our experiments, this heuristic consistently performed better than the default, grow-diag-final. words, therefore it is more comparable if we increase the maximum phrase length. During decoding, we incorporate the standard eight feature functions of Moses as well as the lexicalized reordering model. We tuned the parameters of these features with Minimum Error Rate Training (MERT) (Och, 2003) on the NIST MT03 Evaluation data set (919 sentences), and then test the MT performance on NIST MT03 and MT05 Evaluation data (878 and 1082 sentences, respectively). We report the MT performance using the original BLEU metric (Papineni et al., 2001). All BLEU scores in this paper are uncased. The MT training data was subsampled from GALE Year 2 training data using a collection of character 5-grams and smaller n-grams drawn from all segmentations of the test data. Since the MT training data is subsampled with character n-grams, it is not biased towards any particular word segmentation. The MT t"
W08-0336,2001.mtsummit-papers.68,0,0.0232942,"heuristic consistently performed better than the default, grow-diag-final. words, therefore it is more comparable if we increase the maximum phrase length. During decoding, we incorporate the standard eight feature functions of Moses as well as the lexicalized reordering model. We tuned the parameters of these features with Minimum Error Rate Training (MERT) (Och, 2003) on the NIST MT03 Evaluation data set (919 sentences), and then test the MT performance on NIST MT03 and MT05 Evaluation data (878 and 1082 sentences, respectively). We report the MT performance using the original BLEU metric (Papineni et al., 2001). All BLEU scores in this paper are uncased. The MT training data was subsampled from GALE Year 2 training data using a collection of character 5-grams and smaller n-grams drawn from all segmentations of the test data. Since the MT training data is subsampled with character n-grams, it is not biased towards any particular word segmentation. The MT training data contains 1,140,693 sentence pairs; on the Chinese side there are 60,573,223 non-whitespace characters, and the English sentences have 40,629,997 words. Our main source for training our five-gram language model was the English Gigaword c"
W08-0336,C02-1148,0,0.0871509,"flower). Without a standardized notion of a word, traditionally, the task of Chinese word segmentation starts from designing a segmentation standard based on linguistic and task intuitions, and then aiming to building segmenters that output words that conform to the standard. One widely used standard is the Penn Chinese Treebank (CTB) Segmentation Standard (Xue et al., 2005). It has been recognized that different NLP applications have different needs for segmentation. s U Us Chinese information retrieval (IR) systems benefit from a segmentation that breaks compound words into shorter “words” (Peng et al., 2002), paralleling the IR gains from compound splitting in languages like German (Hollink et al., 2004), whereas automatic speech recognition (ASR) systems prefer having longer words in the speech lexicon (Gao et al., 2005). However, despite a decade of very intense work on Chinese to English machine translation (MT), the way in which Chinese word segmentation affects MT performance is very poorly understood. With current statistical phrase-based MT systems, one might hypothesize that segmenting into small chunks, including perhaps even working with individual characters would be optimal. This is b"
W08-0336,C04-1081,0,0.0385509,"Missing"
W08-0336,I05-3027,1,0.133632,"T training lexicon and OOV words in MT test data, and thus causes a problem for MT. To improve a feature-based sequence model for MT, we propose 4 different approaches to deal with named entities, optimal length of word for MT and joint search for segmentation and MT decoding. 5.1 Making Use of External Lexicons One way to improve the consistency of the CRF model is to make use of external lexicons (which are not part of the segmentation training data) to add lexicon-based features. All the features we use are listed in Table 6. Our linguistic features are adopted from (Ng and Low, 2004) and (Tseng et al., 2005). There are three categories of features: Lexicon-based Features (1.1) LBegin (Cn ), n ∈ [−2, 1] (1.2) LMid (Cn ), n ∈ [−2, 1] (1.3) LEnd (Cn ), n ∈ [−2, 1] (1.4) LEnd (C−1 ) + LEnd (C0 ) +LEnd (C1 ) (1.5) LEnd (C−2 ) + LEnd (C−1 ) +LBegin (C0 ) + LMid (C0 ) (1.6) LEnd (C−2 ) + LEnd (C−1 ) +LBegin (C−1 ) +LBegin (C0 ) + LMid (C0 ) Segmentation Performance F measure OOV Recall CRF-Lex-NR 0.943 0.753 CRF-Lex 0.940 0.729 MT Performance Segmenter MT03 (dev) MT05 (test) CRF-Lex-NR 32.96 31.27 CRF-Lex 32.70 30.95 Linguistic Features (2.1) Cn , n ∈ [−2, 1] (2.2) Cn−1Cn , n ∈ [−1, 1] (2.3) Cn−2Cn , n"
W08-0336,W04-1118,0,0.206696,"y understood. With current statistical phrase-based MT systems, one might hypothesize that segmenting into small chunks, including perhaps even working with individual characters would be optimal. This is because the role of a phrase table is to build domain and application appropriate larger chunks that are semantically coherent in the translation process. For example, even if the word for smallpox is treated as two one-character words, they can still appear in a phrase like “ →smallpox”, so that smallpox will still be a candidate translation when the system translates “ ” “ ”. Nevertheless, Xu et al. (2004) show that an MT system with a word segmenter outperforms a system working with individual characters in an alignment template approach. On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. U s U s Beyond this, there has been no finer-grained analysis of what style and size of word segmentation is optimal for MT. Moreover, most discussion of segmentation for other tasks relates to the size units to identify in the segmentation standard: whether to join"
W08-0336,P02-1040,0,\N,Missing
W08-0336,J05-4005,0,\N,Missing
W08-0336,D08-1076,0,\N,Missing
W08-1006,J03-4003,0,0.140266,"Missing"
W08-1006,de-marneffe-etal-2006-generating,1,0.0166189,"Missing"
W08-1006,P03-1013,0,0.0223462,"a-D/Z): (i) Markovization, (ii) lexicalization, and (iii) state splitting. We additionally explore parsing with the inclusion of grammatical function information. Explicit grammatical functions are important to German language understanding, but they are numerous, and na¨ıvely incorporating them into a parser which assumes a small phrasal category inventory causes large performance reductions due to increasing sparsity. 1 Introduction Recent papers provide mixed evidence as to whether techniques that increase statistical parsing performance for English also improve German parsing performance (Dubey and Keller, 2003; K¨ubler et al., 2006). We provide a systematic exploration of this topic to shed light on what techniques might benefit German parsing and show general trends in the relative performance increases for each technique. While these results vary across treebanks, due to differences in annotation schemes as discussed by K¨ubler (2005), we also find similarities and provide explanations for the trend differences based on the annotation schemes. One feature of German that differs markedly from English is substantial free word order. This requires the marking of grammatical functions on phrases to i"
W08-1006,P05-1039,0,0.0311236,"lized and Unlexicalized Baselines Anna N. Rafferty and Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305 {rafferty,manning}@stanford.edu Abstract We address three parsing techniques: (i) Markovization, (ii) lexicalization, and (iii) state splitting (i.e., subcategorization). These techniques are not independent, and we thus examine how lexicalization and Markovization interact, since lexicalization for German has been the most contentious area in the literature. Many of these techniques have been investigated in other work (Schiehlen, 2004; Dubey, 2004; Dubey, 2005), but, we hope that by consolidating, replicating, improving, and clarifying previous results we can contribute to the re-evaluation of German probabilistic parsing after a somewhat confusing start to initial literature in this area. Previous work on German parsing has provided confusing and conflicting results concerning the difficulty of the task and whether techniques that are useful for English, such as lexicalization, are effective for German. This paper aims to provide some understanding and solid baseline numbers for the task. We examine the performance of three techniques on three tree"
W08-1006,P03-1054,1,0.106583,"l Tiger corpus is much larger. Our Negra results are on the test set. not use any morphological analyzer; this should be rectified in future work. A new parsing model could be written to treat separate grammatical functions for nodes as first class objects, rather than just concatenating phrasal categories and functions. Finally, assignment of grammatical functions could be left to a separate post-processing phase, which could exploit not only case information inside noun phrases but joint information across the subcategorization frames of predicates. 2 Methodology We use the Stanford Parser (Klein and Manning, 2003b) for all experiments. An advantage of this parser for baseline experiments is that it provides clean, simple implementations of component models, with many configuration options. We show results in most instances for evaluations both with and without grammatical functions and with and without gold tags. When training and parsing with the inclusion of grammatical functions, we treat each pairing of basic category and grammatical function as one new category. Rules are learned for each such category with a separate orthographic form, with no attempt to learn general rules for nodes with the sa"
W08-1006,W06-1614,0,0.161691,"Missing"
W08-1006,P06-3004,0,0.0471091,"Missing"
W08-1006,P06-1055,0,0.00626075,"adding grammatical functions is not only problematic due to increased categorization but because of sparseness (this task has the same categorization demands as parsing without grammatical functions considered in section 3). The Stanford Parser was initially designed under the assumption of a small phrasal category set, and makes no attempts to smooth grammar rule probabilities (smoothing only probabilities of words having a certain tag and probabilities of dependencies). While this approach is in general not optimal when many category splits are used inside the parser – smoothing helps, cf. Petrov et al. (2006) – it becomes untenable as the category set grows large, multi-faceted, and sparse. This is particularly evident given the results in table 7 that show the precipitous decline in F1 on the Tiger corpus, where the general problems are exacerbated by the flatter annotation style of Tiger. 5 Lexicalization In the tables in section 3, we showed the utility of lexicalization for German parsing when grammatical functions are not required. This contrasts strongly with the results of (Dubey and Keller, 2003; Dubey, 2004) where no performance increases (indeed, performance decreases) are reported from"
W08-1006,D07-1066,0,0.0307545,"Missing"
W08-1006,C04-1056,0,0.68943,"Three German Treebanks: Lexicalized and Unlexicalized Baselines Anna N. Rafferty and Christopher D. Manning Computer Science Department Stanford University Stanford, CA 94305 {rafferty,manning}@stanford.edu Abstract We address three parsing techniques: (i) Markovization, (ii) lexicalization, and (iii) state splitting (i.e., subcategorization). These techniques are not independent, and we thus examine how lexicalization and Markovization interact, since lexicalization for German has been the most contentious area in the literature. Many of these techniques have been investigated in other work (Schiehlen, 2004; Dubey, 2004; Dubey, 2005), but, we hope that by consolidating, replicating, improving, and clarifying previous results we can contribute to the re-evaluation of German probabilistic parsing after a somewhat confusing start to initial literature in this area. Previous work on German parsing has provided confusing and conflicting results concerning the difficulty of the task and whether techniques that are useful for English, such as lexicalization, are effective for German. This paper aims to provide some understanding and solid baseline numbers for the task. We examine the performance of thr"
W08-1301,W08-0601,0,0.00981273,"Missing"
W08-1301,H91-1060,0,0.040933,"e question of the suitability of the Stanford scheme for parser evaluation. 1 Introduction The Stanford typed dependencies representation was designed to provide a simple description of the grammatical relationships in a sentence that could easily be understood and effectively used by people without linguistic expertise who wanted to extract textual relations. The representation was not designed for the purpose of parser evaluation. Nevertheless, we agree with the widespread sentiment that dependency-based evaluation of parsers avoids many of the problems of the traditional Parseval measures (Black et al., 1991), and to the extent that the Stanford dependency representation is an effective representation for the tasks envisioned, it is perhaps closer to an appropriate taskbased evaluation than some of the alternative dependency representations available. In this paper c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 On the other hand, evaluation seems less important; to the best of our knowledge there has never been a convincing and thorough evaluation of either MiniPar o"
W08-1301,de-marneffe-etal-2006-generating,1,0.223848,"Missing"
W08-1301,D07-1024,0,0.011106,"Missing"
W08-1301,P01-1052,0,0.00581586,"onal) linguists and suitability for relation extraction applications led SD to try to adhere to the following design principles (DPs): This paper advocates for the Stanford typed dependencies representation (henceforth SD) being a promising vehicle for bringing the breakthroughs of the last 15 years of parsing research to this broad potential user community. The representation aims to provide a simple, habitable design. All information is represented as binary relations. This maps straightforwardly on to common representations of potential users, including the logic forms of Moldovan and Rus (Moldovan and Rus, 2001),2 semantic web Resource Description Framework (RDF) triples (http://www.w3.org/RDF/), and graph representations (with labeled edges and nodes). Unlike many linguistic formalisms, excessive detail is viewed as a defect: information that users do not understand or wish to process detracts from uptake and usability. The user-centered design process saw the key goal as representing semantically contentful relations suitable for relation extraction and more general information extraction uses. The design supports this use by favoring relations between content words, by maintaining semantically use"
W08-1301,W03-2806,0,0.0424603,"Missing"
W08-1301,W07-1401,0,0.0125064,"Missing"
W08-1301,W07-1004,0,0.0543531,"endency representations for shallow text understanding tasks has become salient, we would argue, following Clegg and Shepherd (2007), that dependency-based evaluation is close to typical user tasks. Moreover, it avoids some of the known deficiencies of other parser evaluation measures such as Parseval (Carroll et al., 1999). Recent work on parser evaluation using dependency graphs in the biomedical domain confirms 6 reviewers for their helpful comments. that researchers regard dependency-based evaluation as a more useful surrogate for extrinsic task-based evaluation (Clegg and Shepherd, 2007; Pyysalo et al., 2007a). In their evaluation, Clegg and Shepherd (2007) aimed at analyzing the capabilities of syntactic parsers with respect to semantically important tasks crucial to biological information extraction systems. To do so, they used the SD scheme, which provides “a de facto standard for comparing a variety of constituent parsers and treebanks at the dependency level,” and they assessed its suitability for evaluation. They found that the SD scheme better illuminates the performance differences between higher ranked parsers (e.g., Charniak-Lease parser (Lease and Charniak, 2005)), and lower ranked par"
W08-1301,P03-1054,1,0.0398737,"d Shepherd (2007) aimed at analyzing the capabilities of syntactic parsers with respect to semantically important tasks crucial to biological information extraction systems. To do so, they used the SD scheme, which provides “a de facto standard for comparing a variety of constituent parsers and treebanks at the dependency level,” and they assessed its suitability for evaluation. They found that the SD scheme better illuminates the performance differences between higher ranked parsers (e.g., Charniak-Lease parser (Lease and Charniak, 2005)), and lower ranked parsers (e.g., the Stanford parser (Klein and Manning, 2003)). Their parser evaluation accommodates user needs: they used the collapsed version of the dependency graphs offered by the SD scheme, arguing that this is the kind of graph one would find most useful in an information extraction project. Although Clegg and Shepherd (2007) also favor dependency graph representations for parser evaluation, they advocate retention of parse trees so information lost in the dependency structures can be accessed. In essence, any existing dependency scheme could be adopted as the gold-standard for evaluation. However if one believes in ultimately valuing extrinsic t"
W08-1301,I05-1006,0,0.00949667,"(Clegg and Shepherd, 2007; Pyysalo et al., 2007a). In their evaluation, Clegg and Shepherd (2007) aimed at analyzing the capabilities of syntactic parsers with respect to semantically important tasks crucial to biological information extraction systems. To do so, they used the SD scheme, which provides “a de facto standard for comparing a variety of constituent parsers and treebanks at the dependency level,” and they assessed its suitability for evaluation. They found that the SD scheme better illuminates the performance differences between higher ranked parsers (e.g., Charniak-Lease parser (Lease and Charniak, 2005)), and lower ranked parsers (e.g., the Stanford parser (Klein and Manning, 2003)). Their parser evaluation accommodates user needs: they used the collapsed version of the dependency graphs offered by the SD scheme, arguing that this is the kind of graph one would find most useful in an information extraction project. Although Clegg and Shepherd (2007) also favor dependency graph representations for parser evaluation, they advocate retention of parse trees so information lost in the dependency structures can be accessed. In essence, any existing dependency scheme could be adopted as the gold-st"
W08-1301,1993.iwpt-1.22,0,0.0293606,", etc. Thinking about this issue, we were struck by two facts. First, we noted how frequently WordNet (Fellbaum, 1998) gets used compared to other resources, such as FrameNet (Fillmore et al., 2003) or the Penn Treebank (Marcus et al., 1993). We believe that much of the explanation for this fact lies in the difference of complexity of the representation used by the resources. It is easy for users not necessarily versed in linguistics to see how to use and to get value from the straightforward structure of WordNet. Second, we noted the widespread use of MiniPar (Lin, 1998) and the Link Parser (Sleator and Temperley, 1993). This clearly shows that (i) it is very easy for a non-linguist thinking in relation extraction terms to see how to make use of a dependency representation (whereas a phrase structure representation seems much more foreign and forbidding), and (ii) the availability of high quality, easy-to-use (and preferably free) tools is essential for driving broader use of NLP tools.1 This paper examines the Stanford typed dependencies representation, which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding. For s"
W08-1301,levy-andrew-2006-tregex,0,0.264916,"ndent of the head of the adverbial 3 Without word position, the representation is deficient if the same word occurs more than once in a sentence. 4 depend on this head. To retrieve adequate heads from a semantic point of view, heuristics are used to inject more structure when the Penn Treebank gives only flat constituents, as is often the case for conjuncts, e.g., (NP the new phone book and tour guide), and QP constituents, e.g., (QP more than 300). Then for each grammatical relation, patterns are defined over the phrase structure parse tree using the tree-expression syntax defined by tregex (Levy and Andrew, 2006). Conceptually, each pattern is matched against every tree node, and the matching pattern with the most specific grammatical relation is taken as the type of the dependency. The automatic extraction of the relations is not infallible. For instance, in the sentence Behind their perimeter walls lie freshly laundered flowers, verdant grass still sparkling from the last shower, yew hedges in an ecstasy of precision clipping (BNC), the system will erroneously retrieve apposition relations between flowers and grass, as well as between flowers and hedges whereas these should be conj and relations. Th"
W08-1301,P04-1042,1,0.0622189,"ubj(think-4, he-3) However in a sentence such as Who the hell does he think he’s kidding? (BNC), the automatic extraction will fail to find that who is the direct object of kidding. Here, it is vital to distinguish between SD as a representation versus the extant conversion tool. Long-distance dependencies are not absent from the formalism, but the tool does not accurately deal with them.5 4 Stanford dependencies in practice SD has been successfully used by researchers in different domains. In the PASCAL Recognizing 5 As possible future work, we have thought of using a tool such as the one of Levy and Manning (2004) to correctly determine long distance dependencies, as input to the current dependency conversion system. This would presumably be effective, but would make the conversion process much heavier weight. 4 http://nlp.stanford.edu/software/lex-parser.shtml 5 traction in the biomedical domain and originally followed the Link Grammar scheme, Pyysalo et al. (2007a) developed a version of the corpus annotated with the SD scheme. They also made available a program and conversion rules that they used to transform Link Grammar relations into SD graphs, which were then hand-corrected (Pyysalo et al., 2007"
W08-1301,J93-2004,0,0.0561261,"has been developed over the last two decades approachable to and usable by everyone who has text understanding needs. That is, usable not only by computational linguists, but also by the computer science community more generally and by all sorts of information professionals including biologists, medical researchers, political scientists, law firms, business and market analysts, etc. Thinking about this issue, we were struck by two facts. First, we noted how frequently WordNet (Fellbaum, 1998) gets used compared to other resources, such as FrameNet (Fillmore et al., 2003) or the Penn Treebank (Marcus et al., 1993). We believe that much of the explanation for this fact lies in the difference of complexity of the representation used by the resources. It is easy for users not necessarily versed in linguistics to see how to use and to get value from the straightforward structure of WordNet. Second, we noted the widespread use of MiniPar (Lin, 1998) and the Link Parser (Sleator and Temperley, 1993). This clearly shows that (i) it is very easy for a non-linguist thinking in relation extraction terms to see how to make use of a dependency representation (whereas a phrase structure representation seems much mo"
W08-1301,W03-2401,0,\N,Missing
W09-0404,W05-0909,0,0.112089,"tion. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which 2 2.1 Textual Entailment for MT Evaluation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between tw"
W09-0404,P03-1021,0,0.00708576,"del. We plan to assess the additional benefit of the full entailment feature set against the T RAD M T feature set extended by a proper lexical similarity metric, such as METEOR. The computation of entailment features is more heavyweight than traditional MT evaluation metrics. We found the speed (about 6 s per hypothesis on a current PC) to be sufficient for easily judging the quality of datasets of the size conventionally used for MT evaluation. However, this may still be too expensive as part of an MT model that directly optimizes some performance measure, e.g., minimum error rate training (Och, 2003). Feature Weights. Finally, we assessed the importance of the different entailment feature groups in the RTE model.1 Since the presence of correlated features makes the weights difficult to interpret, we restrict ourselves to two general observations. First, we find high weights not only for the score of the alignment between hypothesis and reference, but also for a number of syntacto-semantic match and mismatch features. This means that we do get an additional benefit from the presence of these features. For example, features with a negative effect include dropping adjuncts, unaligned root no"
W09-0404,W07-0411,0,0.0306635,"Missing"
W09-0404,E06-1032,0,0.0370947,"quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an M"
W09-0404,P02-1040,0,0.0809369,"combination of lexical and structural features that model the matches and mismatches between system output and reference translation. We use supervised regression models to combine these features and analyze feature weights to obtain further insights into the usefulness of different feature types. Introduction Automatic metrics to assess the quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement"
W09-0404,W08-0309,0,0.108416,"Missing"
W09-0404,2006.amta-papers.25,0,0.157306,"Missing"
W09-0404,P08-1007,0,0.0429762,"of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which 2 2.1 Textual Entailment for MT Evaluation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences ("
W09-0404,P06-1057,0,0.0216052,"valuation Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true. Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006). Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1. Very good MT output should entail the reference translation. In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions. Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over ∗ This paper is based on work funded by the Defense Advanced Research Pro"
W09-0404,W08-0332,0,0.0303371,"Missing"
W09-0404,P06-1114,0,0.0608647,"tailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true. Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006). Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1. Very good MT output should entail the reference translation. In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions. Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over ∗ This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. Th"
W09-0404,C08-1066,1,0.823965,"re are also substantial differences between TE and MT evaluation. Crucially, TE assumes the premise and hypothesis to be well-formed sentences, which is not true in MT evaluation. Thus, a possible criticism to the use of TE methods is that the features could become unreliable for ill-formed MT output. However, there is a second difference between the tasks that works to our advantage. Due to its strict compositional nature, TE requires an accurate semantic analysis of all sentence parts, since, for example, one misanalysed negation or counterfactual embedding can invert the entailment status (MacCartney and Manning, 2008). In contrast, human MT judgments behave more additively: failure of a translation with respect to a single semantic dimension (e.g., polarity or tense) degrades its quality, but usually not crucially so. We therefore expect that even noisy entailment features can be predictive in MT evaluation. 2.2 Table 1: Entailment feature groups provided by the Stanford RTE system, with number of features quadratic in the number of systems. On the other hand, it can be trained on more reliable pairwise preference judgments. In a second step, we combine the individual decisions to compute the highest-likel"
W09-0404,N06-1006,1,\N,Missing
W09-0436,P05-1033,0,0.310081,"lation (MT) from Chinese to English has been a difficult problem: structural differences between Chinese and English, such as the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difficult language pairs like Arabic-English. Many of these structural differences are related to the ubiquitous Chinese d(DE) construction, used for a wide range of noun modification constructions (both single word and clausal) and other uses. Part of the solution to dealing with these ordering issues is hierarchical decoding, such as the Hiero system (Chiang, 2005), a method motivated by d(DE) examples like the one in Figure 1. In this case, the translation goal is to rotate the noun head and the preceding relative clause around d(DE), so that we can translate to “[one of few countries] d [have diplomatic relations with North Korea]”. Hiero can learn this kind of lexicalized synchronous grammar rule. But use of hierarchical decoders has not solved the DE construction translation problem. We analyzed the errors of three state-of-the-art systems • For DNPs (consisting of“XP+DEG”): – Reorder if XP is PP or LCP; – Reorder if XP is a non-pronominal NP • For"
W09-0436,P05-1066,0,0.0926019,"Missing"
W09-0436,D08-1089,1,0.831613,"this example, both systems decide to reorder, but DE-Annotated had the extra information that this d is a drelc . In Figure 4 we can see that in WANG-NP, “d” is being translated as “for”, and the translation afterwards is not grammatically correct. On the other hand, the bottom of Figure 4 shows that with the DE-Annotated preprocessing, now “drelc ” is translated into “which was” and well connected with the later translation. This shows that disambiguating d helps in choosing a better English translation. conduct additional experiments with a hierarchical phrase reordering model introduced by Galley and Manning (2008). The hierarchical phrase reordering model can handle the key examples often used to motivated syntax-based systems; therefore we think it is valuable to see if the DE annotation can still improve on top of that. In Table 5, BASELINE+Hier gives consistent BLEU improvement over BASELINE. Using DE annotation on top of the hierarchical phrase reordering models (DE-Annotated+Hier) provides extra gain over BASELINE+Hier. This shows the DE annotation can help a hierarchical system. We think similar improvements are likely to occur with other hierarchical systems. 5 5.1 Analysis (IP (NP (NN ddd)) (VP"
W09-0436,N03-1017,0,0.00267307,"ng (WANG-NP) showed consistent improvement in Table 5 on all test sets, with BLEU point gains ranging from 0.15 to 0.40. This confirms that having reordering around DEs in NP helps Chinese-English MT. Table 4: The confusion matrix for 5-class DE classification This could be due to the fact that there are some cases where the translation is correct both ways, but also could be because the features we added have not captured the difference well enough. 4 Machine Translation Experiments 4.1 Baseline Experiments Experimental Setting 4.3 For our MT experiments, we used a reimplementation of Moses (Koehn et al., 2003), a state-of-the-art phrase-based system. The alignment is done by the Berkeley word aligner (Liang et al., 2006) and then we symmetrized the word alignment using the grow-diag heuristic. For features, we incorporate Moses’ standard eight features as well as the lexicalized reordering model. Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We evaluate the result with MT02 (878 sentences), MT03 (919 sentences), and MT05 (1082 sentences). Our MT training corpus contains 1,560,071 senten"
W09-0436,P03-1056,1,0.480711,"There are 12,259,997 words on the English side. Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al., 2008). After segmentation, there are 11,061,792 words on the Chinese side. We use a 5-gram language model trained on the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40) and also the English side of all the LDC parallel data permissible under the NIST08 rules. Documents of Gigaword released during the epochs of MT02, MT03, MT05, and MT06 were removed. To run the DE classifier, we also need to parse the Chinese texts. We use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data and the tuning and test sets. Experiments with 5-class DE annotation We use the best setting of the DE classifier described in Section 3 to annotate DEs in NPs in the MT training data as well as the NIST tuning and test sets.10 If a DE is in an NP, we use the annotation of dAB , dAsB , dBprepA , drelc , or dAprepB to replace the original DE character. Once we have the DEs labeled, we preprocess the Chinese sentences by reordering them.11 Note that not all DEs in the Chinese data are in NPs, therefore not all DEs are annotated with the extra la"
W09-0436,N06-1014,0,0.00752865,"o 0.40. This confirms that having reordering around DEs in NP helps Chinese-English MT. Table 4: The confusion matrix for 5-class DE classification This could be due to the fact that there are some cases where the translation is correct both ways, but also could be because the features we added have not captured the difference well enough. 4 Machine Translation Experiments 4.1 Baseline Experiments Experimental Setting 4.3 For our MT experiments, we used a reimplementation of Moses (Koehn et al., 2003), a state-of-the-art phrase-based system. The alignment is done by the Berkeley word aligner (Liang et al., 2006) and then we symmetrized the word alignment using the grow-diag heuristic. For features, we incorporate Moses’ standard eight features as well as the lexicalized reordering model. Parameter tuning is done with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We evaluate the result with MT02 (878 sentences), MT03 (919 sentences), and MT05 (1082 sentences). Our MT training corpus contains 1,560,071 sentence pairs from various parallel corpora from LDC.9 There are 12,259,997 words on the English side. Chinese word se"
W09-0436,W08-0336,1,0.812091,"e, both systems decide to reorder, but DE-Annotated had the extra information that this d is a drelc . In Figure 4 we can see that in WANG-NP, “d” is being translated as “for”, and the translation afterwards is not grammatically correct. On the other hand, the bottom of Figure 4 shows that with the DE-Annotated preprocessing, now “drelc ” is translated into “which was” and well connected with the later translation. This shows that disambiguating d helps in choosing a better English translation. conduct additional experiments with a hierarchical phrase reordering model introduced by Galley and Manning (2008). The hierarchical phrase reordering model can handle the key examples often used to motivated syntax-based systems; therefore we think it is valuable to see if the DE annotation can still improve on top of that. In Table 5, BASELINE+Hier gives consistent BLEU improvement over BASELINE. Using DE annotation on top of the hierarchical phrase reordering models (DE-Annotated+Hier) provides extra gain over BASELINE+Hier. This shows the DE annotation can help a hierarchical system. We think similar improvements are likely to occur with other hierarchical systems. 5 5.1 Analysis (IP (NP (NN ddd)) (VP"
W09-0436,W05-0908,0,0.0133154,"+1.00) 33.75(+1.24) 33.63(+0.88) 32.96 33.10 32.93 33.96(+1.00) 34.33(+1.23) 33.88(+0.95) Translation Error Rate (TER) MT06(tune) MT02 MT03 61.10 63.11 62.09 59.78(−1.32) 62.58(−0.53) 61.36(−0.73) 58.21(−2.89) 61.17(−1.94) 60.27(−1.82) MT05 31.42 31.68(+0.26) 32.91(+1.49) 32.23 33.01(+0.77) MT05 64.06 62.35(−1.71) 60.78(−3.28) Table 5: MT experiments of different settings on various NIST MT evaluation datasets. We used both the BLEU and TER metrics for evaluation. All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005) shows an example that contains a DE construction that translates into a relative clause in English.12 The automatic parse tree of the sentence is listed in Figure 3. The reordered sentences of WANG-NP and DE-Annotated appear on the top and bottom in Figure 4. For this example, both systems decide to reorder, but DE-Annotated had the extra information that this d is a drelc . In Figure 4 we can see that in WANG-NP, “d” is being translated as “for”, and the translation afterwards is not grammatically correct. On the other hand, the bottom of Figure 4 shows that with the DE-Annotated preprocessi"
W09-0436,I05-3005,1,0.751428,"Missing"
W09-0436,D07-1077,0,0.651725,"m 2: ‘the local a bad reputation secondary school’ Team 3: ‘a local stigma secondary schools’ None of the teams reordered “bad reputation” and “middle school” around the d. We argue that this is because it is not sufficient to have a formalism which supports phrasal reordering, but it is also necessary to have sufficient linguistic modeling that the system knows when and how much to rearrange. An alternative way of dealing with structural differences is to reorder source language sentences to minimize structural divergence with the target language, (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007). For example Wang et al. (2007) introduced a set of rules to decide if a d(DE) construction should be reordered or not before translating to English: Introduction Machine translation (MT) from Chinese to English has been a difficult problem: structural differences between Chinese and English, such as the different orderings of head nouns and relative clauses, cause BLEU scores to be consistently lower than for other difficult language pairs like Arabic-English. Many of these structural differences are related to the ubiquitous Chinese d(DE) construction, used for a wide range of noun modifica"
W09-0436,C04-1073,0,0.0387517,"Missing"
W09-0436,D08-1076,0,\N,Missing
W09-2307,W08-0336,1,0.374339,"tion such as in the loc (localizer) relation. For the example in Figure 1, if we look at the sentence structure from the typed dependency parse (bottom of Figure 1), “d d d” is connected to the main verb dd (finish) by a loc (localizer) relation, and the structure is the same for sentences (a) and (b). This suggests that this kind of semantic and syntactic representation could have more benefit than phrase structure parses. Our Chinese typed dependencies are automatically extracted from phrase structure parses. In English, this kind of typed dependencies has been introduced by de Marneffe and Manning (2008) and de Marneffe et al. (2006). Using typed dependencies, it is easier to read out relations between words, and thus the typed dependencies have been used in meaning extraction tasks. We design features over the Chinese typed dependencies and use them in a phrase-based MT system when deciding whether one chunk of Chinese words (MT system statistical phrase) should appear before or after another. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the cla"
W09-2307,W08-1301,1,0.0607658,"Missing"
W09-2307,de-marneffe-etal-2006-generating,1,0.0159854,"Missing"
W09-2307,2006.amta-papers.8,0,0.0161748,"ish are a major factor in the difficulty of machine translation from Chinese to English. The wide variety of such Chinese-English differences include the ordering of head nouns and relative clauses, and the ordering of prepositional phrases and the heads they modify. Previous studies have shown that using syntactic structures from the source side can help MT performance on these constructions. Most of the previous syntactic MT work has used phrase structure parses in various ways, either by doing syntaxdirected translation to directly translate parse trees into strings in the target language (Huang et al., 2006), or by using source-side parses to preprocess the source sentences (Wang et al., 2007). One intuition for using syntax is to capture different Chinese structures that might have the same 51 ࡐ ࡐ (fixed) Figure 1: Sentences (a) and (b) have the same meaning, but different phrase structure parses. Both sentences, however, have the same typed dependencies shown at the bottom of the figure. meaning and hence the same translation in English. But it turns out that phrase structure (and linear order) are not sufficient to capture this meaning relation. Two sentences with the same meaning can have d"
W09-2307,N03-1017,0,0.00387383,"n a phrase-based MT system when deciding whether one chunk of Chinese words (MT system statistical phrase) should appear before or after another. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. We then apply the phrase orientation classifier as a feature in a phrasebased MT system to help reordering. 2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003). The disadvantage of these models is their insensitivity to the content of the words or phrases. More recent work (Tillman, 2004; Och et al., 2004; Koehn et al., 2007) has in52 troduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases. Lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable. Zens and Ney (2006) proposed a discriminatively trained phrase orientation model and evaluated its performance as a classifier and when"
W09-2307,P07-2045,0,0.00675784,"ain a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. We then apply the phrase orientation classifier as a feature in a phrasebased MT system to help reordering. 2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003). The disadvantage of these models is their insensitivity to the content of the words or phrases. More recent work (Tillman, 2004; Och et al., 2004; Koehn et al., 2007) has in52 troduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases. Lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable. Zens and Ney (2006) proposed a discriminatively trained phrase orientation model and evaluated its performance as a classifier and when plugged into a phrase-based MT system. Their framework allows us to easily add in extra features. Therefore we use it as a testbed to see if we can effectively use fea"
W09-2307,N06-1014,0,0.0173938,"Missing"
W09-2307,P03-1021,0,0.0105591,"Missing"
W09-2307,W05-0908,0,0.0493934,"Missing"
W09-2307,N04-4026,0,0.091796,"r another. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. We then apply the phrase orientation classifier as a feature in a phrasebased MT system to help reordering. 2 Discriminative Reordering Model Basic reordering models in phrase-based systems use linear distance as the cost for phrase movements (Koehn et al., 2003). The disadvantage of these models is their insensitivity to the content of the words or phrases. More recent work (Tillman, 2004; Och et al., 2004; Koehn et al., 2007) has in52 troduced lexicalized reordering models which estimate reordering probabilities conditioned on the actual phrases. Lexicalized reordering models have brought significant gains over the baseline reordering models, but one concern is that data sparseness can make estimation less reliable. Zens and Ney (2006) proposed a discriminatively trained phrase orientation model and evaluated its performance as a classifier and when plugged into a phrase-based MT system. Their framework allows us to easily add in extra features. Therefore we use it as a testb"
W09-2307,D07-1077,0,0.0483917,"Missing"
W09-2307,W06-3108,0,0.371343,") (VP (ADVP (AD ี儳)) (VP (VV )ګݙ (NP (NP (ADJP (JJ ࡐ)) (NP (NN 凹䣈))) (NP (NN ދ凹))) (QP (CD ԫۍԲԼ䣐) (CLP (M ց))))) (PU Ζ))) The prevalence in Chinese of grammatical structures that translate into English in different word orders is an important cause of translation difficulty. While previous work has used phrase-structure parses to deal with such ordering problems, we introduce a richer set of Chinese grammatical relations that describes more semantically abstract relations between words. Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. We then apply the log probability of the phrase orientation classifier as an extra feature in a phrase-based MT system, and get significant BLEU point gains on three test sets: MT02 (+0.59), MT03 (+1.00) and MT05 (+0.77). Our Chinese grammatical relations are also likely to be useful for other NLP tasks. ګݙ ګݙ (complete) loc 䝢 (over; 䝢 in) lobj ڣڣ (year) nummod nsubj ৄؑ (city) ৄؑ det 㪤ࠄ 㪤ࠄ (these) advmod ี儳 ี儳 (collectively) dobj range ދ凹 ދ凹 (i"
W09-2307,N04-1021,0,\N,Missing
W09-2307,D08-1076,0,\N,Missing
W09-2501,P05-1074,0,0.0313752,"stitute a major factor in determining entailment. However, we have argued that about half of the true MWEs are decomposable, that is, the part of the alignment that is crucial for entailment can be recovered with a one-to-one alignment link that can be identified even by very limited alignment models. 2 4 We thank Patrick Pantel for granting us access to DIRT. poorly represented represented 0.42 poorly 0.07 rarely 0.06 good 0.05 representatives 0.04 very few 0.04 well 0.02 representative 0.01 Parallel corpora-based paraphrases. An alternative approach to paraphrase acquisition was proposed by Bannard and Callison-Burch (2005). It exploits the variance inherent in translation to extract paraphrases from bilingual parallel corpora. Concretely, it observes translational relationships between a source and a target language and pairs up source language phrases with other source language phrases that translate into the same target language phrases. We applied this method to the large Chinese-English GALE MT evaluation P3/P3.5 corpus (∼2 GB text per language, mostly newswire). The large number of translations makes it impractical to store all observed paraphrases. We therefore filtered the list of paraphrases against the"
W09-2501,W07-1428,0,0.0161426,"of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs and their handling on pr"
W09-2501,W05-1210,0,0.127926,"g et al., 2002). The importance attributed to them is also reflected in a number of workshops (Bond et al., 2003; Tanaka et al., 2004; Moir´on et al., 2006; Gr´egoire et al., 2007). However, there are few detailed breakdowns of the benefits that improved MWE handling provides to applications. This paper investigates the impact of MWEs on the “recognition of textual entailment” (RTE) task (Dagan et al., 2006). Our analysis ties in with the pivotal question of what types of knowledge are beneficial for RTE. A number of papers have suggested that paraphrase knowledge plays a very important role (Bar-Haim et al., 2005; Marsi et al., 2007; Dinu and Wang, 2009). For example, BarHaim et al. (2005) conclude: “Our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task.” (1) PRE: He died. HYP: He kicked the bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedin"
W09-2501,W07-1421,0,0.0212269,"to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs and their handling on practical entailment recognition. 2 C"
W09-2501,P98-2127,0,0.0298403,"→ executive director military → naval forces Before we come to actual experiments on the automatic recognition of MWEs in a practical RTE system, we need to consider the prerequisites for this task. As mentioned in Section 2, if an RTE system is to establish multi-word alignments, it requires a knowledge source that provides accurate semantic similarity judgments for “many-to-many” alignments (capital punishment – death penalty) as well as for “one-to-many” alignments (vote – cast ballots). Such similarities are not present in standard lexical resources like WordNet or Dekang Lin’s thesaurus (Lin, 1998). The best class of candidate resources to provide wide-coverage of multi-word similarities seems to be paraphrase resources. In this section, we examine to what extent two of the most widely used paraphrase resource types provide supporting evidence for the true MWEs in the MSR data. We deliberately use corpus-derived, noisy resources, since we are interested in the real-world (rather than idealized) prospects for accurate MWE alignment. In particular when light verbs are involved (file lawsuits) or when modification adds just minor meaning aspects (executive director), we argue that it is su"
W09-2501,N06-1006,1,0.880616,"Missing"
W09-2501,J93-2003,0,0.00952629,"Word Expressions in Alignment Almost all textual entailment recognition models incorporate an alignment procedure that establishes correspondences between the premise and the hypothesis. The computation of word alignments is usually phrased as an optimization task. The search space is based on lexical similarities, but usually extended with structural biases in order to obtain alignments with desirable properties, such as the contiguous alignment of adjacent words, or the mapping of different source words on to different target words. One prominent constraint of the IBM word alignment models (Brown et al., 1993) is functional alignment, that is each target word is mapped onto at most one source word. Other models produce only one-to-one alignments, where both alignment directions must be functional. MWEs that involve many-to-many or one-tomany alignments like Ex. (1) present a problem for such constrained word alignment models. A functional alignment model can still handle cases like Ex. (1) correctly in one direction (from bottom to top), but not in the other one. One-to-one alignments manage neither. Various workarounds have been proposed in the MT literature, such as computing word alignments in b"
W09-2501,D08-1084,1,0.88192,"Missing"
W09-2501,W07-1402,0,0.0213983,"he bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs"
W09-2501,W07-1414,0,0.0204608,"portance attributed to them is also reflected in a number of workshops (Bond et al., 2003; Tanaka et al., 2004; Moir´on et al., 2006; Gr´egoire et al., 2007). However, there are few detailed breakdowns of the benefits that improved MWE handling provides to applications. This paper investigates the impact of MWEs on the “recognition of textual entailment” (RTE) task (Dagan et al., 2006). Our analysis ties in with the pivotal question of what types of knowledge are beneficial for RTE. A number of papers have suggested that paraphrase knowledge plays a very important role (Bar-Haim et al., 2005; Marsi et al., 2007; Dinu and Wang, 2009). For example, BarHaim et al. (2005) conclude: “Our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task.” (1) PRE: He died. HYP: He kicked the bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Works"
W09-2501,W07-1412,0,0.0230942,"ch can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP and hypothesis and base at least part of their decision on properties of this alignment (Burchardt et al., 2007; Hickl and Bensley, 2007; Iftene and Balahur-Dobrescu, 2007; Zanzotto et al., 2007). We proceed in three steps. First, we analyze the Microsoft Research (MSR) manual word alignments (Brockett, 2007) for the RTE2 dataset (BarHaim et al., 2006), shedding light on the relationship between alignments and multi-word expressions. We provide frequency estimates and a coarse-grained classification scheme for multiword expressions on textual entailment data. Next, we analyze two widely used types of paraphrase resources with respect to their modeling of MWEs. Finally, we investigate the impact of MWEs and their handling on practical entailment recognition. 2 CARDINALITY DECOM - POSAB"
W09-2501,E09-1025,0,0.0138437,"to them is also reflected in a number of workshops (Bond et al., 2003; Tanaka et al., 2004; Moir´on et al., 2006; Gr´egoire et al., 2007). However, there are few detailed breakdowns of the benefits that improved MWE handling provides to applications. This paper investigates the impact of MWEs on the “recognition of textual entailment” (RTE) task (Dagan et al., 2006). Our analysis ties in with the pivotal question of what types of knowledge are beneficial for RTE. A number of papers have suggested that paraphrase knowledge plays a very important role (Bar-Haim et al., 2005; Marsi et al., 2007; Dinu and Wang, 2009). For example, BarHaim et al. (2005) conclude: “Our analysis also shows that paraphrases stand out as a dominant contributor to the entailment task.” (1) PRE: He died. HYP: He kicked the bucket. The exclusion of MWEs that do not lead to multiword alignments (i.e., which can be aligned word by word) is not a significant loss, since these cases are unlikely to cause significant problems for RTE. In addition, an alignment-based approach has the advantage of generality: Almost all existing RTE models align the linguistic material of the premise 1 Proceedings of the 2009 Workshop on Applied Textual"
W09-2501,W02-1001,0,\N,Missing
W09-2501,W07-1401,0,\N,Missing
W09-2501,C98-2122,0,\N,Missing
W09-2510,N06-1006,1,0.888984,"Missing"
W09-2510,W07-1431,1,0.864674,"Missing"
W09-2510,C08-1066,1,0.832814,"equent so the presupposition of the consequent is not projected and it does not entail (1c). then calculate the presuppositions of the entire sentence by projecting the local presuppositions according to Karttunen’s theory. For each operator our system traverses the sentence’s syntactic tree from operator node to root calculating how the local factivity presuppositions project through the various holes, plugs and filters. The result is a set of sentential level presuppositions that can be used to determine inference relations to other sentences. 3 Presupposition in NatLog The NatLog system of MacCartney and Manning (2008; 2009) is a multi-stage NLI system that decomposes the NLI task into 5 stages: (1) linguistic analysis, (2) alignment, (3) lexical entailment classification, (4) entailment projection, and (5) entailment composition. The NatLog architecture and the theory of presupposition projection outlined in section 2 reflect two parallel methods for computing entailment relations between premise and hypothesis. We augment the NatLog system at steps (1), (4) and (5) to compute entailment relations and presuppositions in parallel. The result is two separate entailment relations which are combined to form a"
W09-2510,W09-3714,1,0.891575,"Missing"
W09-3204,E09-1005,0,0.0134883,"lity to assess paraphrase judgments. 2 Related work Semantic relatedness for individual words has been thoroughly investigated in previous work. Budanitsky and Hirst (2006) provide an overview of many of the knowledge-based measures derived from WordNet, although other data sources have been used as well. Hughes and Ramage (2007) is one such measure based on random graph walks. Prior work has considered random walks on various text graphs, with applications to query expansion (Collins-Thompson and Callan, 2005), email address resolution (Minkov and Cohen, 2007), and word-sense disambiguation (Agirre and Soroa, 2009), among others. Measures of similarity have also been proposed for sentence or paragraph length text passages. Mihalcea et al. (2006) present an algorithm for the general problem of deciding the similarity of meaning in two text passages, coining the name “text semantic similarity” for the task. Corley and Mihalcea (2005) apply this algorithm to paraphrase recognition. Previous work has shown that similarity measures can have some success as a measure of textual entailment. Glickman et al. (2005) showed that many entailment problems can be answered using only a bag-of-words representation and"
W09-3204,J06-1003,0,0.0496053,"ments of individual words. We show that walks effectively aggregate information over multiple types of links and multiple input words on an unsupervised paraphrase recognition task. Furthermore, when used as a feature, the walk’s semantic similarity score can improve the performance of an existing, competitive textual entailment system. Finally, we provide empirical results demonstrating that indeed, each step of the random walk contributes to its ability to assess paraphrase judgments. 2 Related work Semantic relatedness for individual words has been thoroughly investigated in previous work. Budanitsky and Hirst (2006) provide an overview of many of the knowledge-based measures derived from WordNet, although other data sources have been used as well. Hughes and Ramage (2007) is one such measure based on random graph walks. Prior work has considered random walks on various text graphs, with applications to query expansion (Collins-Thompson and Callan, 2005), email address resolution (Minkov and Cohen, 2007), and word-sense disambiguation (Agirre and Soroa, 2009), among others. Measures of similarity have also been proposed for sentence or paragraph length text passages. Mihalcea et al. (2006) present an algo"
W09-3204,W07-1427,1,0.85787,"Missing"
W09-3204,W05-1203,0,0.041049,"mage (2007) is one such measure based on random graph walks. Prior work has considered random walks on various text graphs, with applications to query expansion (Collins-Thompson and Callan, 2005), email address resolution (Minkov and Cohen, 2007), and word-sense disambiguation (Agirre and Soroa, 2009), among others. Measures of similarity have also been proposed for sentence or paragraph length text passages. Mihalcea et al. (2006) present an algorithm for the general problem of deciding the similarity of meaning in two text passages, coining the name “text semantic similarity” for the task. Corley and Mihalcea (2005) apply this algorithm to paraphrase recognition. Previous work has shown that similarity measures can have some success as a measure of textual entailment. Glickman et al. (2005) showed that many entailment problems can be answered using only a bag-of-words representation and web co-occurrence statistics. Many systems integrate lexical relatedness and overlap measures with deeper semantic and syntactic features to create improved results upon relatedness alone, as in Montejo-R´aez et al. (2007). A random walk example To provide some intuition about the behavior of the random walk on text passa"
W09-3204,W07-1401,0,0.0157354,"Missing"
W09-3204,C04-1051,0,0.131834,"Missing"
W09-3204,W07-1413,0,0.0225364,"Missing"
W09-3204,P06-1101,0,0.0235179,"part of the graph are reflected in each walk. This means that the meaningfulness of changes in the graph can be evaluated according to how they affect these text similarity scores; this provides a more semantically relevant evaluation of updates to a resource than, for example, counting how many new words or links between words have been added. As shown in Jarmasz and Szpakowicz (2003), an updated resource may have many more links and concepts but still have similar performance on applications as the original. Evaluations of WordNet extensions, such as those in Navigli and Velardi (2005) and Snow et al. (2006), are easily conducted within the framework of the random walk. The presented framework for text semantic similarity with random graph walks is more general than the WordNet-based instantiation explored here. Transition matrices from alternative linguistic resources such as corpus co-occurrence statistics or larger knowledge bases such as Wikipedia may very well add value as a lexical resource underlying the walk. One might also consider tailoring the output of the walk with machine learning techniques like those presented in (Minkov and Cohen, 2007). these words come from the first documents"
W09-3204,D07-1061,1,0.312582,"e recognition task. Furthermore, when used as a feature, the walk’s semantic similarity score can improve the performance of an existing, competitive textual entailment system. Finally, we provide empirical results demonstrating that indeed, each step of the random walk contributes to its ability to assess paraphrase judgments. 2 Related work Semantic relatedness for individual words has been thoroughly investigated in previous work. Budanitsky and Hirst (2006) provide an overview of many of the knowledge-based measures derived from WordNet, although other data sources have been used as well. Hughes and Ramage (2007) is one such measure based on random graph walks. Prior work has considered random walks on various text graphs, with applications to query expansion (Collins-Thompson and Callan, 2005), email address resolution (Minkov and Cohen, 2007), and word-sense disambiguation (Agirre and Soroa, 2009), among others. Measures of similarity have also been proposed for sentence or paragraph length text passages. Mihalcea et al. (2006) present an algorithm for the general problem of deciding the similarity of meaning in two text passages, coining the name “text semantic similarity” for the task. Corley and"
W09-3204,O97-1002,0,0.201568,"ments in a background corpus divided by the number of documents containing the term. maxSim compares only within the same WordNet part-of-speech labeling in order to support evaluation with lexical relatedness measures that cannot cross part-of-speech boundaries. Mihalcea et al. (2006) presents results for several underlying measures of lexical semantic relatedness. These are subdivided into corpus-based measures (using Latent Semantic Analysis (Landauer et al., 1998) and a pointwise-mutual information measure) and knowledge-based resources driven by WordNet. The latter include the methods of Jiang and Conrath (1997), Lesk (1986), Resnik (1999), and others. In this unsupervised experimental setting, we consider using only a thresholded similarity value from our system and from the Mihalcea algorithm to determine the paraphrase or non-paraphrase judgment. For consistency with previous work, we threshold at 0.5. Note that this threshold could be tuned on the training data in a supervised setting. Informally, we observed that on the training data a threshold of near 0.5 was often a good choice for this task. Table 3 shows the results of our system and a representative subset of those reported in (Mihalcea et"
W09-3206,N09-1003,1,0.631483,"Missing"
W09-3206,J06-1003,0,0.41968,"ssful measure, Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), treats each article as its own dimension in a vector space. Texts are compared by first projecting them into the space of Wikipedia articles and then comparing the resulting vectors. In addition to article text, Wikipedia stores a great deal of information about the relationships between the articles in the form of hyperlinks, info boxes, and category pages. Despite a long history of research demonstrating the effectiveness of incorporating link information into relatedness measures based on the WordNet graph (Budanitsky and Hirst, 2006), previous work on Wikipedia has made limited use of this relationship information, using only category links (Bunescu and Pasca, 2006) or just the actual links in a page (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008). In this work, we combine previous approaches by converting Wikipedia into a graph, mapping input texts into the graph, and performing random walks based on Personalized PageRank (Haveliwala, 2002) to obtain stationary distributions that characterize each text. Semantic relatedness between two texts is computed by comparing their distributions. In contrast to previous"
W09-3206,E06-1002,0,0.0140215,"pace. Texts are compared by first projecting them into the space of Wikipedia articles and then comparing the resulting vectors. In addition to article text, Wikipedia stores a great deal of information about the relationships between the articles in the form of hyperlinks, info boxes, and category pages. Despite a long history of research demonstrating the effectiveness of incorporating link information into relatedness measures based on the WordNet graph (Budanitsky and Hirst, 2006), previous work on Wikipedia has made limited use of this relationship information, using only category links (Bunescu and Pasca, 2006) or just the actual links in a page (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008). In this work, we combine previous approaches by converting Wikipedia into a graph, mapping input texts into the graph, and performing random walks based on Personalized PageRank (Haveliwala, 2002) to obtain stationary distributions that characterize each text. Semantic relatedness between two texts is computed by comparing their distributions. In contrast to previous work, we explore the use of all these link types when conComputing semantic relatedness of natural language texts is a key component o"
W09-3206,D07-1061,1,0.852577,"information not present in the page text. 2 PageRank it is chosen from a nonuniform distribution of nodes, specified by a teleport vector. The final weight of node i represents the proportion of time the random particle spends visiting it after a sufficiently long time, and corresponds to that node’s structural importance in the graph. Because the resulting vector is the stationary distribution of a Markov chain, it is unique for a particular walk formulation. As the teleport vector is nonuniform, the stationary distribution will be biased towards specific parts of the graph. In the case of (Hughes and Ramage, 2007) and (Agirre and Soroa, 2009), the teleport vector is used to reflect the input texts to be compared, by biasing the stationary distribution towards the neighborhood of each word’s mapping. The computation of relatedness for a word pair can be summarized in three steps: First, each input word is mapped with to its respective synsets in the graph, creating its teleport vector. In the case words with multiple synsets (senses), the synsets are weighted uniformly. Personalized PageRank is then executed to compute the stationary distribution for each word, using their respective teleport vectors. F"
W09-3206,E09-1005,1,\N,Missing
W09-3714,H05-1079,0,0.0446459,"ponding to each implication signature, but we can describe a few specific cases. For example, implication signature –/◦ seems to project ∧ as |(refuse to stay |refuse to go) and both |and ` as # (refuse to tango # refuse to waltz ). 7 Putting it all together We now have the building blocks of a general method to establish the semantic relation between a premise p and a hypothesis h. The steps are as follows: 1. Find a sequence of atomic edits he1 , . . . , en i which transforms p into h: thus h = (en ◦ . . . ◦ e1 )(p). For convenience, let us define x0 = p, xn = h, and xi = ei (xi−1 ) for i ∈ [1, n]. 2. For each atomic edit ei : 23 Of course, the implicatives may carry presuppositions as well (he managed to escape  it was hard to escape), but these implications are not activated by a simple deletion, as with the factives. 152 (a) Determine the lexical semantic relation β(ei ), as in section 4. (b) Project β(ei ) upward through the semantic composition tree of expression xi−1 to find an atomic semantic relation β(xi−1 , ei ) = β(xi−1 , xi ), as in section 5. 3. Join atomic semantic relations across the sequence of edits, as in section 3: β(p, h) = β(x0 , xn ) = β(x0 , e1 ) ⋊ ⋉ ... ⋊ ⋉ β("
W09-3714,W07-1427,1,0.873086,"Missing"
W09-3714,W03-0906,0,0.325196,"Missing"
W09-3714,W07-1401,0,0.0202381,"Missing"
W09-3714,D08-1084,1,0.420907,"Missing"
W09-3714,N06-1006,1,0.553544,"Missing"
W09-3714,C08-1066,1,0.609192,"Missing"
W09-3714,W06-3907,0,0.585013,"Missing"
W09-3714,D08-1002,0,0.0198397,"Missing"
W10-2902,W95-0102,0,0.564583,"Missing"
W10-2902,A94-1009,0,0.400994,"Missing"
W10-2902,N09-1012,0,0.628626,"Missing"
W10-2902,P04-1061,1,0.926389,"R, T)) (1 − PSTOP (IN, R, T)) PSTOP (VBD, L, F) PSTOP (NNS, L, T) PSTOP (IN, L, T) PSTOP (NN, L, T) PSTOP (⋄, L, F) {z } | POS Tokens 110,760 136,310 163,715 336,555 540,895 730,099 860,053 942,801 986,830 1,028,054 48,201 391,796 PATTACH (⋄, L, VBD) PATTACH (VBD, L, NNS) PATTACH (VBD, R, IN) PATTACH (IN, R, NN) PSTOP (VBD, R, F) PSTOP (NNS, R, T) PSTOP (IN, R, F) PSTOP (NN, R, T) PSTOP (⋄, R, T) . | {z } 3 Standard Data Sets and Evaluation The DMV is traditionally trained and tested on customized subsets of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub-trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic “head-percolation” rules (Collins, 1999) to convert the rest, as is standard practice. We experiment with WSJk (sentences with at most k tokens), for 1 ≤ k ≤ 45, and Section 23 of WSJ∞ (all sentence lengths). We also evaluate on Brown100, similarly derived from the parsed portion of"
W10-2902,P08-1100,0,0.0570334,"Missing"
W10-2902,J93-2004,0,0.0375615,"OP (VBD, L, T)) (1 − PSTOP (VBD, R, T)) (1 − PSTOP (IN, R, T)) PSTOP (VBD, L, F) PSTOP (NNS, L, T) PSTOP (IN, L, T) PSTOP (NN, L, T) PSTOP (⋄, L, F) {z } | POS Tokens 110,760 136,310 163,715 336,555 540,895 730,099 860,053 942,801 986,830 1,028,054 48,201 391,796 PATTACH (⋄, L, VBD) PATTACH (VBD, L, NNS) PATTACH (VBD, R, IN) PATTACH (IN, R, NN) PSTOP (VBD, R, F) PSTOP (NNS, R, T) PSTOP (IN, R, F) PSTOP (NN, R, T) PSTOP (⋄, R, T) . | {z } 3 Standard Data Sets and Evaluation The DMV is traditionally trained and tested on customized subsets of Penn English Treebank’s Wall Street Journal portion (Marcus et al., 1993). Following Klein and Manning (2004), we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub-trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic “head-percolation” rules (Collins, 1999) to convert the rest, as is standard practice. We experiment with WSJk (sentences with at most k tokens), for 1 ≤ k ≤ 45, and Section 23 of WSJ∞ (all sentence lengths). We also evaluate on Brown100, similarly"
W10-2902,N06-1020,0,0.0330881,"performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM would not be doing as badly, compared to Viterbi. Viterbi training is not only faster and more accurate but also free of inside-outside’s recursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featurerich approaches that target conditional likelihoods, essentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such “learning by doing” approaches may be relevant to understanding human language acquisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human probabilistic parsing are massively pruned (Jurafsky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM — or the very limited parallelism of k-best Viterbi — may be more appropriate in modeling this task than the fully-integrated inside-outside solution. ` 16 ´ 1 3 1 5 = = 0.213 2"
W10-2902,P06-1043,0,0.0163891,"performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM would not be doing as badly, compared to Viterbi. Viterbi training is not only faster and more accurate but also free of inside-outside’s recursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featurerich approaches that target conditional likelihoods, essentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such “learning by doing” approaches may be relevant to understanding human language acquisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human probabilistic parsing are massively pruned (Jurafsky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM — or the very limited parallelism of k-best Viterbi — may be more appropriate in modeling this task than the fully-integrated inside-outside solution. ` 16 ´ 1 3 1 5 = = 0.213 2"
W10-2902,J93-2003,0,0.0156613,"ns: (i) initial direction dir ∈ {L, R} in which to attach children, via probability PORDER (ch ); (ii) whether to seal dir, stopping with probability PSTOP (ch , dir, adj), conditioned on adj ∈ {T, F} (true iff considering dir ’s first, i.e., adjacent, child); and (iii) attachments (of class ca ), according to PATTACH (ch , dir, ca ). This produces only projective trees. A root token ♦ generates the head of a sentence as its left (and only) child. Figure 2 displays a simple example. The DMV lends itself to unsupervised learning via inside-outside re-estimation (Baker, 1979). Viterbi training (Brown et al., 1993) re-estimates each next model as if supervised by the previous best parse trees. And supervised learning from 1 The expected number of trials needed to get one Bernoulli(p) success is n ∼ Geometric(p), with n ∈ Z+ , P(n) = (1 − p)n−1 p and E(n) = p−1 ; MoM and MLE agree, pˆ = (# of successes)/(# of trials). 10 60 50 Oracle Ad-Hoc∗ 40 30 20 Uninformed 10 5 10 15 20 25 30 35 40 Ad-Hoc∗ 150 Uninformed 100 50 10 15 20 25 30 35 40 Uninformed 40 30 20 10 5 10 15 20 25 30 35 40 WSJk (training on all WSJ sentences up to k tokens in length) (d) Iterations for Viterbi (Hard EM) 200 150 Ad-Hoc∗ 100 50 (c"
W10-2902,J94-2001,0,0.790451,"Missing"
W10-2902,N03-1023,0,0.0441837,"del gives even better performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM would not be doing as badly, compared to Viterbi. Viterbi training is not only faster and more accurate but also free of inside-outside’s recursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featurerich approaches that target conditional likelihoods, essentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such “learning by doing” approaches may be relevant to understanding human language acquisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human probabilistic parsing are massively pruned (Jurafsky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM — or the very limited parallelism of k-best Viterbi — may be more appropriate in modeling this task than the fully-integrated inside-outside solution. ` 1"
W10-2902,P92-1017,0,0.684899,"Missing"
W10-2902,W03-0407,0,0.0169857,"thod to train the model gives even better performance guarantees than exact training methods. If our task were not parsing but language modeling, where the relevant score is the sum of the probabilities over individual derivations, perhaps classic EM would not be doing as badly, compared to Viterbi. Viterbi training is not only faster and more accurate but also free of inside-outside’s recursion constraints. It therefore invites more flexible modeling techniques, including discriminative, featurerich approaches that target conditional likelihoods, essentially via (unsupervised) self-training (Clark et al., 2003; Ng and Cardie, 2003; McClosky et al., 2006a; McClosky et al., 2006b, inter alia). Such “learning by doing” approaches may be relevant to understanding human language acquisition, as children frequently find themselves forced to interpret a sentence in order to interact with the world. Since most models of human probabilistic parsing are massively pruned (Jurafsky, 1996; Chater et al., 1998; Lewis and Vasishth, 2005, inter alia), the serial nature of Viterbi EM — or the very limited parallelism of k-best Viterbi — may be more appropriate in modeling this task than the fully-integrated inside-"
W10-2902,P10-1130,1,0.698579,"nships beyond local trees, e.g., agreement. Spitkovsky et al. (2009) suggest that classic EM breaks down as sentences get longer precisely because the model makes unwarranted independence assumptions. They hypothesize that the DMV reserves too much probability mass for what should be unlikely productions. Since EM faithfully allocates such re-distributions across the possible parse trees, once sentences grow sufficiently long, this process begins to deplete what began as likelier structures. But medium lengths avoid a flood of exponentially-confusing longer sentences (and 5 In a sister paper, Spitkovsky et al. (2010) improve performance by incorporating parsing constraints harvested from the web into Viterbi training; nevertheless, results presented in this paper remain the best of models trained purely on WSJ. 6 Klein and Manning (2004) originally trained the DMV on WSJ10 and Gillenwater et al. (2009) found it useful to discard data from WSJ3, which is mostly incomplete sentences. 13 7 Proofs (by Construction) This objective function is not convex and in general does not have a unique peak, so in practice one usually settles for θ˜UNS — a fixed point. There is no reason why θˆSUP should agree with θˆUNS"
W10-2902,N09-1009,0,0.394501,"Missing"
W10-2902,P10-1152,0,0.138496,"Missing"
W10-2902,J03-4003,0,\N,Missing
W11-0902,de-marneffe-etal-2006-generating,1,0.182751,"Missing"
W11-0902,P05-1045,1,0.109487,"system. 3 Description of the Generic IE System We illustrate our proposed ideas using a simple IE system that implements a pipeline architecture: entity mention extraction followed by relation mention extraction. Note however that the domain customization discussion in Section 5 is independent of the system architecture or classifiers used for EMD and RMD, and we expect the proposed ideas to apply to other IE approaches as well. We performed all pre-processing (tokenization, part-of-speech (POS) tagging) with the Stanford CoreNLP toolkit.2 For EMD we used the Stanford named entity recognizer (Finkel et al., 2005). In all our experiments we used a generic set of features (“macro”) and the IO notation3 for entity mention labels (e.g., the labels for the tokens “over the Seattle Seahawks on Sunday” (from Figure 1) are encoded as “O O NFLT EAM NFLT EAM O DATE”). 2 http://nlp.stanford.edu/software/ corenlp.shtml 3 The IO notation facilitates faster inference than the IOB or IOB2 notations with minimal impact on performance, when there are fewer adjacent mentions with the same type. Argument Features Syntactic Features Surface Features – Head words of the two arguments and their combination – Entity mention"
W11-0902,D10-1033,0,0.0934157,"Missing"
W11-0902,J02-3001,0,0.206465,"es faster inference than the IOB or IOB2 notations with minimal impact on performance, when there are fewer adjacent mentions with the same type. Argument Features Syntactic Features Surface Features – Head words of the two arguments and their combination – Entity mention labels of the two arguments and their combination – Sequence of dependency labels in the dependency path linking the heads of the two arguments – Lemmas of all words in the dependency path – Syntactic path in the constituent parse tree between the largest constituents headed by the same words as the two arguments (similar to Gildea and Jurafsky (2002)) – Concatenation of POS tags between arguments – Binary indicators set to true if there is an entity mention with a given type between the two arguments Words 110 70,119 Entity Mentions 2,188 Relation Mentions 1,629 Table 2: Summary statistics of the NFL corpus, after our conversion to binary relations. Table 1: Feature set used for RMD. The RMD model was built from scratch as a multi-class classifier that extracts binary relations between entity mentions in the same sentence. During training, known relation mentions become positive examples for the corresponding label and all other possible"
W11-0902,D09-1120,0,0.0229873,"is heuristic is flawed: since most parsers are heavily context dependent, they are likely to not parse correctly arbitrarily short text fragments. For example, the Stanford parser generates the incorrect parse tree: Improving Head Identification for Entity Mentions Table 1 indicates that most RMD features (e.g., lexical information on arguments, dependency paths between arguments) depend on the syntactic heads of entity mentions. This observation applies to other natural language processing (NLP) tasks as well, e.g., semantic role labeling or coreference resolution (Gildea and Jurafsky, 2002; Haghighi and Klein, 2009). It is thus crucial that syntactic heads of mentions be correctly identified. Originally we employed a common heuristic: we first try to find a constituent with the exact same span as the given entity mention in the parse tree of the entire sentence, and extract its head. If no such constituent exists, we parse only the text corresponding to the mention and return the head of the generated tree (Haghighi 7 The syntactic head is “5” for the mention “a 5-yard scoring pass” instead of “pass.”10 This problem is exacerbated out of domain, where the parse tree of the entire sentence is likely to be"
W11-0902,W09-1401,0,0.0604544,"Missing"
W11-0902,P03-1054,1,0.0153456,"e positive examples for the corresponding label and all other possible combinations between entity mentions in the same sentence become negative examples. We used a multiclass logistic regression classifier with L2 regularization. Our feature set is taken from (Yao et al., 2010; Mintz et al., 2009; Roth and Yih, 2007; Surdeanu and Ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations. For syntactic information, we used the Stanford parser (Klein and Manning, 2003) and the Stanford dependency representation (de Marneffe et al., 2006). For RMD, we implemented an additive feature selection algorithm similar to the one in (Surdeanu et al., 2008), which iteratively adds the feature with the highest improvement in F1 score to the current feature set, until no improvement is seen. The algorithm was configured to select features that yielded the best combined performance on the dataset from Roth and Yih (2007) and the training partition of ACE 2007.4 We used ten-fold cross val4 Documents idation on both datasets. We decided to use a standard F1 score to evalua"
W11-0902,E99-1001,0,0.0585842,"(and potentially tune model parameters). In this paper we argue that, even when considerable training data is available, this is not sufficient to maximize performance. We apply several simple ideas that yield a significant performance boost, and can be implemented with minimal effort. In particular: • We show that a combination of a conditional random field model (Lafferty et al., 2001) with a rule-based approach that is recall oriented yields better performance for EMD and for the downstream RMD component. The rulebased approach includes gazetteers, which have been shown to be important by Mikheev et al. (1999), among others. • We improve the unification of the predicted semantic annotations with the syntactic analysis of the corresponding text, i.e., finding the syntactic head of a given semantic constituent. Since many features in an IE system depend on syntactic analysis, this leads to more consistent features and better extraction models. • We add a simple inference engine that generates additional relation mentions based solely on the relation mentions extracted by the RMD model. This engine mitigates some of the limitations of a text-based RMD model, which cannot extract relations not explicit"
W11-0902,N06-2024,0,0.0656665,"Missing"
W11-0902,P09-1113,0,0.1178,"s 1,629 Table 2: Summary statistics of the NFL corpus, after our conversion to binary relations. Table 1: Feature set used for RMD. The RMD model was built from scratch as a multi-class classifier that extracts binary relations between entity mentions in the same sentence. During training, known relation mentions become positive examples for the corresponding label and all other possible combinations between entity mentions in the same sentence become negative examples. We used a multiclass logistic regression classifier with L2 regularization. Our feature set is taken from (Yao et al., 2010; Mintz et al., 2009; Roth and Yih, 2007; Surdeanu and Ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations. For syntactic information, we used the Stanford parser (Klein and Manning, 2003) and the Stanford dependency representation (de Marneffe et al., 2006). For RMD, we implemented an additive feature selection algorithm similar to the one in (Surdeanu et al., 2008), which iteratively adds the feature with the highest improvement in F1 score to the current"
W11-0902,P01-1052,0,0.0542362,"Missing"
W11-0902,N10-1123,0,0.0159955,"ing features built on external knowledge, are more important than the learning model itself. These works are conceptually similar to our paper, but we propose several additional directions to improve robustness, and we investigate their impact in a complete IE system instead of just EMD. Several of our lessons are drawn from the BioCreative challenge1 and the BioNLP shared task (Kim 1 http://biocreative.sourceforge.net/ 3 et al., 2009). These tasks have shown the importance of high quality syntactic annotations and using heuristic fixes to correct systematic errors (Schuman and Bergler, 2006; Poon and Vanderwende, 2010, among others). Systems in the latter task have also shown the importance of high recall in the earlier stages of pipeline system. 3 Description of the Generic IE System We illustrate our proposed ideas using a simple IE system that implements a pipeline architecture: entity mention extraction followed by relation mention extraction. Note however that the domain customization discussion in Section 5 is independent of the system architecture or classifiers used for EMD and RMD, and we expect the proposed ideas to apply to other IE approaches as well. We performed all pre-processing (tokenizati"
W11-0902,W09-1119,0,0.0606409,"ction 2 surveys related work. Section 3 describes the IE system used. We cover the target domain that serves as use case in this paper in Section 4. Section 5 introduces our ideas and evaluates their impact in the target domain. Finally, Section 6 concludes the paper. 2 Related Work Other recent works have analyzed the robustness of information extraction systems. For example, Florian et al. (2010) observed that EMD systems perform badly on noisy inputs, e.g., automatic speech transcripts, and propose system combination (similar to our first proposal) to increase robustness in such scenarios. Ratinov and Roth (2009) also investigate design challenges for named entity recognition, and showed that other design choices, such as the representation of output labels and using features built on external knowledge, are more important than the learning model itself. These works are conceptually similar to our paper, but we propose several additional directions to improve robustness, and we investigate their impact in a complete IE system instead of just EMD. Several of our lessons are drawn from the BioCreative challenge1 and the BioNLP shared task (Kim 1 http://biocreative.sourceforge.net/ 3 et al., 2009). These"
W11-0902,W06-3312,0,0.0253567,"ion of output labels and using features built on external knowledge, are more important than the learning model itself. These works are conceptually similar to our paper, but we propose several additional directions to improve robustness, and we investigate their impact in a complete IE system instead of just EMD. Several of our lessons are drawn from the BioCreative challenge1 and the BioNLP shared task (Kim 1 http://biocreative.sourceforge.net/ 3 et al., 2009). These tasks have shown the importance of high quality syntactic annotations and using heuristic fixes to correct systematic errors (Schuman and Bergler, 2006; Poon and Vanderwende, 2010, among others). Systems in the latter task have also shown the importance of high recall in the earlier stages of pipeline system. 3 Description of the Generic IE System We illustrate our proposed ideas using a simple IE system that implements a pipeline architecture: entity mention extraction followed by relation mention extraction. Note however that the domain customization discussion in Section 5 is independent of the system architecture or classifiers used for EMD and RMD, and we expect the proposed ideas to apply to other IE approaches as well. We performed al"
W11-0902,P08-1082,1,0.718591,"c regression classifier with L2 regularization. Our feature set is taken from (Yao et al., 2010; Mintz et al., 2009; Roth and Yih, 2007; Surdeanu and Ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations. For syntactic information, we used the Stanford parser (Klein and Manning, 2003) and the Stanford dependency representation (de Marneffe et al., 2006). For RMD, we implemented an additive feature selection algorithm similar to the one in (Surdeanu et al., 2008), which iteratively adds the feature with the highest improvement in F1 score to the current feature set, until no improvement is seen. The algorithm was configured to select features that yielded the best combined performance on the dataset from Roth and Yih (2007) and the training partition of ACE 2007.4 We used ten-fold cross val4 Documents idation on both datasets. We decided to use a standard F1 score to evaluate RMD performance rather than the more complex ACE score because we believe that the former is more interpretable. We used gold entity mentions for the feature selection process. T"
W11-0902,D10-1099,0,0.0186421,"8 Relation Mentions 1,629 Table 2: Summary statistics of the NFL corpus, after our conversion to binary relations. Table 1: Feature set used for RMD. The RMD model was built from scratch as a multi-class classifier that extracts binary relations between entity mentions in the same sentence. During training, known relation mentions become positive examples for the corresponding label and all other possible combinations between entity mentions in the same sentence become negative examples. We used a multiclass logistic regression classifier with L2 regularization. Our feature set is taken from (Yao et al., 2010; Mintz et al., 2009; Roth and Yih, 2007; Surdeanu and Ciaramita, 2007) and models the relation arguments, the surface distance between the relation arguments, and the syntactic path between the two arguments, using both constituency and dependency representations. For syntactic information, we used the Stanford parser (Klein and Manning, 2003) and the Stanford dependency representation (de Marneffe et al., 2006). For RMD, we implemented an additive feature selection algorithm similar to the one in (Surdeanu et al., 2008), which iteratively adds the feature with the highest improvement in F1 s"
W11-1806,W09-1402,0,0.297186,"Missing"
W11-1806,E03-1009,0,0.0142277,"(GE) dataset (Kim et al., 2011b) and the new Epigenetics and Post-translational Modifications (EPI) and Infectious Diseases (ID) datasets (Ohta et al., 2011; Pyysalo et al., 2011, respectively). Other changes are relatively minor but documented here as implementation notes. Several improvements were made to anchor detection, improving its accuracy on all three domains. The first is the use of distributional similarity features. Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). We used the Ney-Essen clustering model with morphology to produce 45 clusters. Using these clusters, we extended the feature set for anchor detection from McClosky et al. (2011) as follows: for each lexicalized feature we create an equivalent feature where the corresponding word is replaced by its cluster ID. This yielded consistent improvements of at least 1 percentage point in both anchor detection and event 5 http://github.com/BLLIP/bllip-parser extraction in the development partition of the GE dataset. Additionally, we improved the head percolation rules for selecting the head of each mu"
W11-1806,P07-1033,0,0.025258,"Missing"
W11-1806,N10-1095,0,0.0161104,"our system, receives an n-best list of event structures from each decoder in the event parsing step. The reranker can use any global features of an event structure to rescore it and outputs the highest scoring structure. This is based on parse reranking (Ratnaparkhi, 1999; Collins, 2000) but uses features on event structures instead of syntactic constituency structures. We used Mark Johnson’s cvlm estimator5 (Charniak and Johnson, 2005) when learning weights for the reranking model. Since the reranker can incorporate the outputs from multiple decoders, we use it as an ensemble technique as in Johnson and Ural (2010). 3 Extensions for BioNLP 2011 This section outlines the changes between our BioNLP 2011 shared task submission and the system described in McClosky et al. (2011). The main differences are that all dataset-specific portions of the model have been factored out to handle the expanded Genia (GE) dataset (Kim et al., 2011b) and the new Epigenetics and Post-translational Modifications (EPI) and Infectious Diseases (ID) datasets (Ohta et al., 2011; Pyysalo et al., 2011, respectively). Other changes are relatively minor but documented here as implementation notes. Several improvements were made to an"
W11-1806,W11-1801,0,0.576114,"l event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure. As the full details on our approach are described in McClosky et al. (2011), we will only provide an outline of our methods here along with additional implementation notes. Before running our system, we perform basic preprocessing on the corpora. Sentences need to be segmented, tokenized, and parsed syntactically. We use custom versions of these (except for Infectious Diseases where we use those from Stenetorp et al. (2011)). To ease event parsing, our 2 http://sourceforge.net/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are designed to split off suffixes which are often event anchors. For example, we split the token RelA-induced into the two tokens RelA and induced3 since RelA is a protein and induced an event anchor. If this was a single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph. For syntactic parsing,"
W11-1806,W11-1802,0,0.202989,"instead of syntactic constituency structures. We used Mark Johnson’s cvlm estimator5 (Charniak and Johnson, 2005) when learning weights for the reranking model. Since the reranker can incorporate the outputs from multiple decoders, we use it as an ensemble technique as in Johnson and Ural (2010). 3 Extensions for BioNLP 2011 This section outlines the changes between our BioNLP 2011 shared task submission and the system described in McClosky et al. (2011). The main differences are that all dataset-specific portions of the model have been factored out to handle the expanded Genia (GE) dataset (Kim et al., 2011b) and the new Epigenetics and Post-translational Modifications (EPI) and Infectious Diseases (ID) datasets (Ohta et al., 2011; Pyysalo et al., 2011, respectively). Other changes are relatively minor but documented here as implementation notes. Several improvements were made to anchor detection, improving its accuracy on all three domains. The first is the use of distributional similarity features. Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). We used the Ney-E"
W11-1806,P11-1163,1,0.92818,"is paper, we show that with minimal domain-specific tuning, we are able to achieve competitive performance across the three event extraction domains in the BioNLP 2011 shared task. At the heart of our system1 is an off-the-shelf 1 nlp.stanford.edu/software/eventparser.shtml Event Parsing Our system includes three components: (1) anchor detection to identify and label event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure. As the full details on our approach are described in McClosky et al. (2011), we will only provide an outline of our methods here along with additional implementation notes. Before running our system, we perform basic preprocessing on the corpora. Sentences need to be segmented, tokenized, and parsed syntactically. We use custom versions of these (except for Infectious Diseases where we use those from Stenetorp et al. (2011)). To ease event parsing, our 2 http://sourceforge.net/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are des"
W11-1806,N10-1004,1,0.20041,"/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are designed to split off suffixes which are often event anchors. For example, we split the token RelA-induced into the two tokens RelA and induced3 since RelA is a protein and induced an event anchor. If this was a single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph. For syntactic parsing, we use the self-trained biomedical parsing model from McClosky (2010) with the Charniak and Johnson (2005) reranking parser. We use its actual constituency tree, the dependency graph created by applying head percolation rules, and the Stanford Dependencies (de Marneffe and Manning, 2008) extracted from the tree (collapsed and uncollapsed). Anchor detection uses techniques inspired from named entity recognition to label each token with an event type or none. The features for this stage are primarily drawn from Bj¨orne et al. (2009). We reduce multiword event anchors to their syntactic head.4 We classify each token independently using a logistic regression classi"
W11-1806,E06-1011,0,0.0351838,"Missing"
W11-1806,H05-1066,0,0.0429686,"Missing"
W11-1806,W11-1803,0,0.57546,"l event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure. As the full details on our approach are described in McClosky et al. (2011), we will only provide an outline of our methods here along with additional implementation notes. Before running our system, we perform basic preprocessing on the corpora. Sentences need to be segmented, tokenized, and parsed syntactically. We use custom versions of these (except for Infectious Diseases where we use those from Stenetorp et al. (2011)). To ease event parsing, our 2 http://sourceforge.net/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are designed to split off suffixes which are often event anchors. For example, we split the token RelA-induced into the two tokens RelA and induced3 since RelA is a protein and induced an event anchor. If this was a single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph. For syntactic parsing,"
W11-1806,W11-1804,0,0.559547,"l event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure. As the full details on our approach are described in McClosky et al. (2011), we will only provide an outline of our methods here along with additional implementation notes. Before running our system, we perform basic preprocessing on the corpora. Sentences need to be segmented, tokenized, and parsed syntactically. We use custom versions of these (except for Infectious Diseases where we use those from Stenetorp et al. (2011)). To ease event parsing, our 2 http://sourceforge.net/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are designed to split off suffixes which are often event anchors. For example, we split the token RelA-induced into the two tokens RelA and induced3 since RelA is a protein and induced an event anchor. If this was a single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph. For syntactic parsing,"
W11-1806,W11-1808,1,0.884176,"l event anchors, (2) event parsing to form candidate event structures by linking entities and event anchors, and (3) event reranking to select the best candidate event structure. As the full details on our approach are described in McClosky et al. (2011), we will only provide an outline of our methods here along with additional implementation notes. Before running our system, we perform basic preprocessing on the corpora. Sentences need to be segmented, tokenized, and parsed syntactically. We use custom versions of these (except for Infectious Diseases where we use those from Stenetorp et al. (2011)). To ease event parsing, our 2 http://sourceforge.net/projects/mstparser/ 41 Proceedings of BioNLP Shared Task 2011 Workshop, pages 41–45, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics tokenizations are designed to split off suffixes which are often event anchors. For example, we split the token RelA-induced into the two tokens RelA and induced3 since RelA is a protein and induced an event anchor. If this was a single token, our event parser would be unable to link them since it cannot predict self-loops in the dependency graph. For syntactic parsing,"
W11-1806,W11-1816,0,0.330404,"Missing"
W11-1806,W08-1301,1,\N,Missing
W11-1806,P05-1022,0,\N,Missing
W11-1808,W09-1401,0,0.534253,"Missing"
W11-1808,W11-1801,0,0.421255,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1802,0,0.36214,"ctions on test and development sets we used models learned from the the complete training set. Predictions over training data were produced using crossvalidation. This helps to avoid a scenario where the stacking model learns to rely on high accuracy at training time that cannot be matched at test time. Note that, unlike Stanford’s individual submission in this shared task, the stacked models in this paper do not include the Stanford reranker. This is because it would have required making a reranker model for each crossvalidation fold. We made 19 crossvalidation training folds for Genia (GE) (Kim et al., 2011b), 12 for Epigenetics (EPI), and 17 for Infectious Diseases (ID) (Kim et al., 2011b; Ohta et al., 2011; Pyysalo et al., 2011, respectively). Note that while ID is the smallest and would seem like it would have the fewest folds, we combined the training data of ID with the training and development data from GE. To produce predictions over the test data, we combined the training folds with 6 development folds for GE, 4 for EPI, and 1 for ID. 3 Experiments Table 1 gives an overview of our results on the test sets for all four tasks we submitted to. Note that for the EPI and ID tasks we show the"
W11-1808,P11-1163,1,0.535908,"b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs, but the model does well considering these restrictions. Additionally, this constraint encourages the Stanford model to provide different"
W11-1808,W11-1806,1,0.588093,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,H05-1066,0,0.0337346,") finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs, but the model does well considering these restrictions. Additionally, this constraint encourages the Stanford model to provide different (and thus more useful for stacking) results. Of particular interest to this paper are the four possible decoders in MSTParser. These four decoders come from combinat"
W11-1808,P08-1108,0,0.0519048,"Missing"
W11-1808,W11-1803,0,0.278435,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1804,0,0.36653,"ion of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to dependency trees which are parsed using MSTParser (McDonald et al., 2005).1 Once parsed, the resulting dependency tree is converted back to event structures. Using the Stanford model as the stacked model is helpful since it captures tree structure which is not the focus in the UMass model. Of course, this is also a limitation since actual BioNLP event graphs are DAGs,"
W11-1808,W11-1807,1,0.818921,"edictions from the Stanford system into the UMass system (e.g., as in Nivre and McDonald (2008)). This has the advantage that one model (Umass) determines how to integrate the outputs of the other model (Stanford) into its own structure, whereas in reranking, for example, the combined model is required to output a complete structure produced by only one of the input models. 2 Approach In the following we briefly present both the stacking and the stacked model and some possible ways of integrating the stacked information. 2.1 Stacking Model As our stacking model, we employ the UMass extractor (Riedel and McCallum, 2011). It is based on a discriminatively trained model that jointly predicts trigger labels, event arguments and protein pairs in 51 Proceedings of BioNLP Shared Task 2011 Workshop, pages 51–55, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics binding. We will briefly describe this model but first introduce three types of binary variables that will represent events in a given sentence. Variables ei,t are active if and only if the token at position i has the label t. Variables ai,j,r are active if and only if there is an event with trigger i that has an argument"
W11-1808,D10-1001,0,0.0101169,"(p, q) to include predictions from the systems to be stacked. For example, for every system S to be stacked and every pair of event types (t0 , tS ) we add the features ( 1 hS (i) = tS ∧ t0 = t fS,t0 ,tS (i, t) = 0 otherwise 52 to fT (i, t). Here hS (i) is the event label given to token i according to S. These features allow different weights to be given to each possible combination of type t0 that we want to assign, and type tS that S predicts. Inference in this model amounts to maximizing s (e, a, b) over Y. Our approach to solving this problem is dual decomposition (Komodakis et al., 2007; Rush et al., 2010). We divide the problem into three subproblems: (1) finding the best trigger label and set of outgoing edges for each candidate trigger; (2) finding the best trigger label and set of incoming edges for each candidate trigger; (3) finding the best pairs of entities to appear in the same binding. Due to space limitations we refer the reader to Riedel and McCallum (2011) for further details. 2.2 Stacked Model For the stacked model, we use a system based on an event parsing framework (McClosky et al., 2011a) referred to as the Stanford model in this paper. This model converts event structures to d"
W11-1808,N10-1091,1,0.0608368,"Missing"
W12-3107,P07-1111,0,0.0187035,"and improved robustness from resources like WordNet synonyms (Miller et al., 1990), paraphrasing (Zhou et al., 2006; Snover et al., 2009; Denkowski and Lavie, 2010), and syntactic parse structures (Liu et al., 2005; Owczarzak et al., 2008; He et al., 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al., 2008; Pado et al., 2009). The drawback, however, is they rely on time-consuming 76 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 76–83, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics Figure 1: This diagram illustrates an example translation pair in the Chinese-English portion of OpenMT08 data set (Doc:AFP CMN 20070703.0005, system09, sent 1). The three rows below are the best state transition (edit) sequences that transforms REF to SYS, according to the three proposed models. The correspond"
W12-3107,P07-1038,0,0.0196947,"and improved robustness from resources like WordNet synonyms (Miller et al., 1990), paraphrasing (Zhou et al., 2006; Snover et al., 2009; Denkowski and Lavie, 2010), and syntactic parse structures (Liu et al., 2005; Owczarzak et al., 2008; He et al., 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al., 2008; Pado et al., 2009). The drawback, however, is they rely on time-consuming 76 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 76–83, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics Figure 1: This diagram illustrates an example translation pair in the Chinese-English portion of OpenMT08 data set (Doc:AFP CMN 20070703.0005, system09, sent 1). The three rows below are the best state transition (edit) sequences that transforms REF to SYS, according to the three proposed models. The correspond"
W12-3107,W05-0909,0,0.412573,"ases by adding one-to-many and many-to-many bigram block substitutions. 5 Experiments The goal of our experiments is to test both the accuracy and robustness of the proposed new models. We then show that modeling word swapping and rich linguistics features further improve our results. To better situate our work among past research and to draw meaningful comparison, we use exactly the same standard evaluation data sets and metrics as Pado et al. (2009), which is currently the stateof-the-art result for regression-based MT evaluation. We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and Lavie, 2005) (v0.7), and TER) as our baselines. Since our models are trained to regress human evaluation scores, to make a direct comparison in the same regression setting, we also train a small linear regression model for each baseline metric in the same way as descried in Pado et al. (2009). These regression models are strictly more powerful than the baseline metrics and show higher robustness and better correlation with human increased by 0.5 ∗ max(|s|, |r|) in the worst case, and by and large swapping is rare in comparison to basic edits. Data Set train test A+C U A+U C A C+U MT08 MT06 pFSM 54.6 59.9"
W12-3107,W09-0434,0,0.0250309,"ses synonym information from WordNet; T ER R on the other hand has a disadvantage because it does not use lemmas. Lemma is added later in the TERplus extension (Snover et al., 2009). 3.9 points), suggesting that modeling word swapping is particularly rewarding for Chinese language. On the other hand, pPDA model does not perform better than the pFSM model on Arabic in MT08 and OpenMT06 (which is also Arabic-to-English). This observation is consistent with findings in earlier work that Chinese-English translations exhibit much more medium and long distance reordering than languages like Arabic (Birch et al., 2009). Both the pFSM and pPDA models also significantly outperform the M T R linear regression model that combines the outputs of all four baselines, on all three source languages. This demonstrates that our regression model is more robust and accurate than a state-of-the-art system combination linear-regression model. The RTE R and M T +RTE R linear regression models benefit from the rich linguistic features in the textual entailment system’s output. It has access to all the features in pPDA+f such as paraphrase and dependency parse relations, and many more (e.g., Norm Bank, part-of-speech, negati"
W12-3107,W10-1703,0,0.0833272,"Missing"
W12-3107,W11-2103,0,0.0761939,"Missing"
W12-3107,N10-1031,0,0.0119936,"on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches (Papineni et al., 2002; Doddington, 2002). Despite their simplicity, these measures have shown good correlation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation (CallisonBurch et al. (2009; 2010; 2011), inter alia). Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al., 1990), paraphrasing (Zhou et al., 2006; Snover et al., 2009; Denkowski and Lavie, 2010), and syntactic parse structures (Liu et al., 2005; Owczarzak et al., 2008; He et al., 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al., 2008; Pado et al., 2009). The drawback, however, is they rely on time-consuming 76 Proceedings of the 7"
W12-3107,P02-1001,0,0.0868009,"= pFSMs for MT Regression We start off by framing the problem of machine translation evaluation in terms of weighted edit distance calculated using probabilistic finite state machines (pFSMs). A FSM defines a language by accepting a string of input tokens in the language, and rejecting those that are not. A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM. Commonly used models such as HMMs, n-gram models, Markov Chains, probabilistic finite state transducers and PCFGs all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal et al., 2005). Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations. A string of tokens that our FSM accepts is an edit sequence that transforms a reference translation (denoted as ref ) into a system translation (sys). 77 1 |e| exp θ · f(ei−1 , ei , s, r) Z∏ i=1 (1) We featurize each of the state changes with a loglinear parameterization; f is a set of binary feature functions defined over pairs of neighboring states (by the Markov assumption) and the input sentences, and θ are the associate"
W12-3107,W10-1753,0,0.0300314,"Missing"
W12-3107,knight-al-onaizan-1998-translation,0,0.190628,"lly described as: w(e |s, r) = pFSMs for MT Regression We start off by framing the problem of machine translation evaluation in terms of weighted edit distance calculated using probabilistic finite state machines (pFSMs). A FSM defines a language by accepting a string of input tokens in the language, and rejecting those that are not. A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM. Commonly used models such as HMMs, n-gram models, Markov Chains, probabilistic finite state transducers and PCFGs all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal et al., 2005). Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations. A string of tokens that our FSM accepts is an edit sequence that transforms a reference translation (denoted as ref ) into a system translation (sys). 77 1 |e| exp θ · f(ei−1 , ei , s, r) Z∏ i=1 (1) We featurize each of the state changes with a loglinear parameterization; f is a set of binary feature functions defined over pairs of neighboring states (by the Markov assumption) and the input sentences, and θ are"
W12-3107,W04-3250,0,0.0992285,"y worse are highlighted in bold in each row. judgments. 4 We also compare our models with the state-of-the-art linear regression models reported in Pado et al. (2009) that combine features from multiple MT evaluation metrics (MT), as well as rich linguistic features from a textual entailment system (RTE). In all of our experiments, each reference and system translation sentence pair is tokenized using the PTB (Marcus et al., 1993) tokenization script, and lemmatized by the Porter Stemmer (Porter, 1980). Statistical significance tests are performed using the paired bootstrap resampling method (Koehn, 2004). We divide our experiments into two sections, based on two different prediction tasks — predicting absolute scores and predicting pairwise preference. 5.1 Overall Comparison Exp. 1: Predicting Absolute Scores The first task is to evaluate a system translation on a seven point Likert scale against a single reference. Higher scores indicate translations that are closer to the meaning intended by the reference. Human ratings in the form of absolute scores are available for standard evaluation data sets such as NIST OpenMT06,08.5 Since our model makes predictions at the granularity of a whole sen"
W12-3107,N03-1019,0,0.0943884,"Regression We start off by framing the problem of machine translation evaluation in terms of weighted edit distance calculated using probabilistic finite state machines (pFSMs). A FSM defines a language by accepting a string of input tokens in the language, and rejecting those that are not. A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM. Commonly used models such as HMMs, n-gram models, Markov Chains, probabilistic finite state transducers and PCFGs all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002; Kumar and Byrne, 2003; Vidal et al., 2005). Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations. A string of tokens that our FSM accepts is an edit sequence that transforms a reference translation (denoted as ref ) into a system translation (sys). 77 1 |e| exp θ · f(ei−1 , ei , s, r) Z∏ i=1 (1) We featurize each of the state changes with a loglinear parameterization; f is a set of binary feature functions defined over pairs of neighboring states (by the Markov assumption) and the input sentences, and θ are the associated feature weights; r an"
W12-3107,W05-0904,0,0.227358,"Missing"
W12-3107,J93-2004,0,0.0391735,"limit 5. M ET R is shorthand for M ETEOR R. +f means the model includes synonyms and paraphrase features (cf. Section 4). Best results and scores that are not statistically significantly worse are highlighted in bold in each row. judgments. 4 We also compare our models with the state-of-the-art linear regression models reported in Pado et al. (2009) that combine features from multiple MT evaluation metrics (MT), as well as rich linguistic features from a textual entailment system (RTE). In all of our experiments, each reference and system translation sentence pair is tokenized using the PTB (Marcus et al., 1993) tokenization script, and lemmatized by the Porter Stemmer (Porter, 1980). Statistical significance tests are performed using the paired bootstrap resampling method (Koehn, 2004). We divide our experiments into two sections, based on two different prediction tasks — predicting absolute scores and predicting pairwise preference. 5.1 Overall Comparison Exp. 1: Predicting Absolute Scores The first task is to evaluate a system translation on a seven point Likert scale against a single reference. Higher scores indicate translations that are closer to the meaning intended by the reference. Human rat"
W12-3107,P03-1021,0,0.0992671,"Missing"
W12-3107,P09-1034,1,0.8428,"r et al., 1990), paraphrasing (Zhou et al., 2006; Snover et al., 2009; Denkowski and Lavie, 2010), and syntactic parse structures (Liu et al., 2005; Owczarzak et al., 2008; He et al., 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al., 2008; Pado et al., 2009). The drawback, however, is they rely on time-consuming 76 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 76–83, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics Figure 1: This diagram illustrates an example translation pair in the Chinese-English portion of OpenMT08 data set (Doc:AFP CMN 20070703.0005, system09, sent 1). The three rows below are the best state transition (edit) sequences that transforms REF to SYS, according to the three proposed models. The corresponding alignments generated by the models (pFSM, pPDA, pPDA+f ) are"
W12-3107,P02-1040,0,0.0840518,"n of the pFSM model into a probabilistic Pushdown Automaton (pPDA), which enhances traditional editdistance models with the ability to model phrase shift and word swapping. Furthermore, we give a new loglinear parameterization to the pFSM model, which allows it to easily incorporate rich linguistic features. Related Work Research in automatic machine translation (MT) evaluation metrics has been a key driving force behind the recent advances of statistical machine translation (SMT) systems. The early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches (Papineni et al., 2002; Doddington, 2002). Despite their simplicity, these measures have shown good correlation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation (CallisonBurch et al. (2009; 2010; 2011), inter alia). Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al., 1990), paraphrasing (Zhou et al., 2006; Snover et al., 2009; Denkowski and Lavie, 2010), and syntactic parse structures (Liu et al., 2005; Owczarzak et al., 2008; He"
W12-3107,2006.amta-papers.25,0,0.548654,"set of binary feature functions defined over pairs of neighboring states (by the Markov assumption) and the input sentences, and θ are the associated feature weights; r and s are shorthand for ref and sys; Z is a partition function. In this basic pFSM model, the feature functions are simply identity functions that emit the current state, and the state transition sequence of the previous state and the current state. The feature weights are then automatically learned by training a global regression model where some translational equivalence judgment score (e.g., human assessment score, or HTER (Snover et al., 2006)) for each sys and ref translation pair is the regression target (y). ˆ Since the “gold” edit sequence are not given at training or prediction time, we treat the edit sequences as hidden variables and sum over them in our model. We introduce a new regression variable y ∈ R which is the log-sum of the unnormalized weights (Eqn. (1)) of all edit sequences, formally expressed as: 0 |e | y = log ∑ ∏ exp θ · f(ei−1 , ei , s, r) (2) 0 e ⊆e∗ i=1 The sum over an exponential number of edit sequences in e∗ is solved efficiently using a forwardbackward style dynamic program. Any edit sequence that does n"
W12-3107,W09-0441,0,0.0637754,"Missing"
W12-3107,P08-3005,0,0.0145702,"et synonyms (Miller et al., 1990), paraphrasing (Zhou et al., 2006; Snover et al., 2009; Denkowski and Lavie, 2010), and syntactic parse structures (Liu et al., 2005; Owczarzak et al., 2008; He et al., 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al., 2008; Pado et al., 2009). The drawback, however, is they rely on time-consuming 76 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 76–83, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics Figure 1: This diagram illustrates an example translation pair in the Chinese-English portion of OpenMT08 data set (Doc:AFP CMN 20070703.0005, system09, sent 1). The three rows below are the best state transition (edit) sequences that transforms REF to SYS, according to the three proposed models. The corresponding alignments generated by the models (pFSM"
W12-3107,W06-1610,0,0.0192046,"on (SMT) systems. The early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches (Papineni et al., 2002; Doddington, 2002). Despite their simplicity, these measures have shown good correlation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation (CallisonBurch et al. (2009; 2010; 2011), inter alia). Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al., 1990), paraphrasing (Zhou et al., 2006; Snover et al., 2009; Denkowski and Lavie, 2010), and syntactic parse structures (Liu et al., 2005; Owczarzak et al., 2008; He et al., 2010). But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features. Recent models use linear or SVM regression and train them against human judgments to automatic learn feature weights, and have shown state-of-the-art correlation with human judgments (Albrecht and Hwa, 2007a; Albrecht and Hwa, 2007b; Sun et al., 2008; Pado et al., 2009). The drawback, however, is th"
W12-3107,W09-0401,0,\N,Missing
W12-4403,W11-2103,0,0.0233836,"Missing"
W12-4403,N04-1036,0,0.0286633,", Arabic, and Chinese data. They found that relying on alignment models alone was very poor, even among these high-resource languages, although it was a relatively small corpus (about 1,000 aligned entities). The focus was more on transliteration – an important aspect of translation that we simply aren’t addressing here. Most earlier work used a tagger in one language in combination with machine translation-style alignments models. Among these, Huang et al. is the most closely related to our work as they are translating rare named-entities, and are therefore in a similar low-resource context (Huang et al., 2004). As with the NIST project, most work building on Huang et al. has been in transliteration. Although not cross-linguistic, Piskorski et al.’s work on NER for inflectional languages (2009) also relied on the similarities in edit distance between the intra-language variation of names. In gazetteer-related work, Wang et al. and others since, have looked at edit distance within a language, modeling the distance between observed words and lists of entities (Wang et al., 2009). Similarly, there is a cluster of slightly older work on unsupervised entity detection, also within one language (Pedersen e"
W12-4403,P07-2045,0,0.00312718,"ints out, most parallel texts shouldn’t be alignable, as different contexts mean different translation strategies, most of which will not result in usable input for machine translation (Kay, 2006). This is true of the corpus used here – the translations were made for quick understanding by aid workers, explaining much of the above: it was clearer to break the translation into two sentences; it reduced ambiguity to repeat ‘hospital’ rather than leave it underspecified; the typo simply didn’t matter. We confirmed the ‘unalignability’ of this corpus using the GIZA++ aligner in the Moses toolkit (Koehn et al., 2007); by noting Microsoft Research’s work on the same data where they needed to carefully retranslate the messages for training (Lewis, 2010); and from correspondence with participants in the 2011 Workshop on Machine Translation who reported the need for substantial preprocessing and mixed results. We do not rule out the alignability of the corpus altogether – the system presented here could even be used to create better alignment models – noting only that it is rare that translations can be used straight-ofthe-box, while in our case we can still make use of this data. Even with perfect alignment,"
W12-4403,W11-2164,1,0.832596,"d highly structured resource, that might be considered more supervised than not. In alignment work, the foundational work is Yarowsky et al.’s induction of projections across aligned corpora (Yarowsky et al., 2001), most successfully adapted to cross-linguistic syntactic parsing (Hwa et al., 2005). The machine translation systems used named-entity recognition are too many to list here, but as we say, the system we present could aid translation considerably, especially in the context of low resources languages and humanitarian contexts, a recent focus in the field (Callison-Burch et al., 2011; Lewis et al., 2011). 7 Conclusions We have presented a promising a new strategy for named-entity recognition from unaligned parallel corpora, finding that unsupervised named-entity recognition across languages can be bootstrapped from calculating the local edit distance deviation be28 Unsupervised Precision Edit likelihood deviation Kr`eyol: 0.619 English: 0.633 Language-specific models Kr`eyol: 0.907 English: 0.932 Jointly-learned models Kr`eyol: 0.904 English: 0.915 Recall F-value 0.619 0.633 0.619 0.633 0.687 0.766 0.781 0.840 0.794 0.813 0.846 0.861 0.915 0.206 0.336 Semi-supervised Identification Kr`eyol: 0"
W12-4403,W11-2206,0,0.0172633,"eer-related work, Wang et al. and others since, have looked at edit distance within a language, modeling the distance between observed words and lists of entities (Wang et al., 2009). Similarly, there is a cluster of slightly older work on unsupervised entity detection, also within one language (Pedersen et al., 2006; Nadeau et al., 2006), but all relying on web-scale quantities of unlabeled data. While the implementation is not related, it is also worth highlighting Lin et al.’s very recent work on unsupervised language-independent name translation the mines data from Wikipedia ‘infoboxes’, (Lin et al., 2011) however the infoboxes give a fairly and highly structured resource, that might be considered more supervised than not. In alignment work, the foundational work is Yarowsky et al.’s induction of projections across aligned corpora (Yarowsky et al., 2001), most successfully adapted to cross-linguistic syntactic parsing (Hwa et al., 2005). The machine translation systems used named-entity recognition are too many to list here, but as we say, the system we present could aid translation considerably, especially in the context of low resources languages and humanitarian contexts, a recent focus in t"
W12-4403,N10-1075,1,0.893891,"2 STEP 1: Establish Edit Likelihood Deviation As we state in the introduction, we cannot simply tag in English and then find the least-edit distance word/phrase in the parallel Kr`eyol. We evaluated several different edit distance functions, including the well-known Levenshtein and slightly more complex Jaro-Winkler measures. We also extended the Levenshtein measure by reducing the edit penalty for pairs of letters of phonetic relatedness, such as ‘c’ and ‘k’, following the subword modeling work of Munro and Manning on this corpus and previous subword modeling for short messages (Munro, 2011; Munro and Manning, 2010).2 2 We also attempted a more sophisticated approach to learning weights for edits by extracting edit probabilities from the final model. This also made little improvement, but it could have simply been the result data-sparseness over only 3000 pairs of entities, so no strong conclusions can be drawn. The more sophisticated edit distance functions gave more accurate predictions (which is unsurprising), but the advantages were lost in the following step when calculating the deviation from the norm, with all approaches producing more or less the same seeds. Rather than the String Similarity Esti"
W12-4403,W11-0309,1,0.923163,"w resources. 2 STEP 1: Establish Edit Likelihood Deviation As we state in the introduction, we cannot simply tag in English and then find the least-edit distance word/phrase in the parallel Kr`eyol. We evaluated several different edit distance functions, including the well-known Levenshtein and slightly more complex Jaro-Winkler measures. We also extended the Levenshtein measure by reducing the edit penalty for pairs of letters of phonetic relatedness, such as ‘c’ and ‘k’, following the subword modeling work of Munro and Manning on this corpus and previous subword modeling for short messages (Munro, 2011; Munro and Manning, 2010).2 2 We also attempted a more sophisticated approach to learning weights for edits by extracting edit probabilities from the final model. This also made little improvement, but it could have simply been the result data-sparseness over only 3000 pairs of entities, so no strong conclusions can be drawn. The more sophisticated edit distance functions gave more accurate predictions (which is unsurprising), but the advantages were lost in the following step when calculating the deviation from the norm, with all approaches producing more or less the same seeds. Rather than"
W12-4403,W03-0419,0,0.0524126,"Missing"
W12-4403,W02-2024,0,0.033301,"Missing"
W12-4403,W06-1630,0,0.0297172,"li, Cyrillic, Devanagari (Hindi) and Hanzi (Chinese) scripts, and the methods proposed here would be even richer if they could also identify named entities across scripts. A first pass on cross-script data looks like it is possible to apply our methods across scripts, especially because the seeds only need to be drawn from the most confident matches and across scripts there seem to be some named entities that are more easier to transliterate than others (which is not surprising, of course – most cross-linguistic tasks are heterogeneous in this way). However, with a few notable exceptions like Tao et al. (2006), transliteration is typically a supervised task. As with machine translation it is likely that the methods used here could aid transliteration, providing predictions that can be used within a final, supervised transliteration model (much like the semi-supervised model proposed later on).1 1.1 The limitations of edit-distance and supervised approaches Despite the intuition that named-entities are less likely to change form across translations, it is clearly only a weak trend. Even if we assume oracle knowledge of entities in English (that is, imagining that we have perfect named-entity-recogni"
W12-4403,H01-1035,0,0.0606744,"ised entity detection, also within one language (Pedersen et al., 2006; Nadeau et al., 2006), but all relying on web-scale quantities of unlabeled data. While the implementation is not related, it is also worth highlighting Lin et al.’s very recent work on unsupervised language-independent name translation the mines data from Wikipedia ‘infoboxes’, (Lin et al., 2011) however the infoboxes give a fairly and highly structured resource, that might be considered more supervised than not. In alignment work, the foundational work is Yarowsky et al.’s induction of projections across aligned corpora (Yarowsky et al., 2001), most successfully adapted to cross-linguistic syntactic parsing (Hwa et al., 2005). The machine translation systems used named-entity recognition are too many to list here, but as we say, the system we present could aid translation considerably, especially in the context of low resources languages and humanitarian contexts, a recent focus in the field (Callison-Burch et al., 2011; Lewis et al., 2011). 7 Conclusions We have presented a promising a new strategy for named-entity recognition from unaligned parallel corpora, finding that unsupervised named-entity recognition across languages can"
W12-4403,song-strassel-2008-entity,0,\N,Missing
W12-4403,2010.eamt-1.37,0,\N,Missing
W13-2217,N10-2003,1,0.849554,"ity improvements over MERT (Och, 2003) and PRO (Hopkins and May, 2011) for two languages in a research setting. The purpose of our submission to the 2013 Workshop on Statistical Machine Translation (WMT) Shared Task is to compare the algorithm to more established methods in an evaluation. We submitted English-French (En-Fr) and EnglishGerman (En-De) systems, each with over 100k features tuned on 10k sentences. This paper describes the systems and also includes new feature sets and practical extensions to the original algorithm. Translation Model Our machine translation (MT) system is Phrasal (Cer et al., 2010), a phrase-based system based on alignment templates (Och and Ney, 2004). Like many MT systems, Phrasal models the predictive translation distribution p(e|f ; w) directly as p(e|f ; w) = h i 1 exp w&gt; φ(e, f ) Z(f ) Online, Adaptive Tuning Algorithm ⇐⇒ M (d+ ) &gt; M (d− ) Ensuring pairwise agreement is the same as ensuring w · [φ(d+ ) − φ(d− )] &gt; 0. For learning, we need to select derivation pairs (d+ , d− ) to compute difference vectors x+ = φ(d+ ) − φ(d− ). Then we have a 1-class separation problem trying to ensure w · x+ &gt; 0. The derivation pairs are sampled with the algorithm of Hopkins and M"
W13-2217,P10-4002,0,0.0144529,"the phrase table size by excluding less relevant training examples. 150 4.2 Tokenization We tokenized the English (source) data according to the Penn Treebank standard (Marcus et al., 1993) with Stanford CoreNLP. The French data was tokenized with packages from the Stanford French Parser (Green et al., 2013a), which implements a scheme similar to that used in the French Treebank (Abeillé et al., 2003). German is more complicated due to pervasive compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Aligner (Liang et al., 2006) configured with standard settings. We symmetrized the alignments according to the grow-diag heuristic. 4.4 Language Modeling We estimated unfiltered 5-gram language models using lmplz (Heafield et al., 2013) and loaded them with KenLM (Heafield, 2011). For memory efficiency and faster loading we also used KenLM to convert the LMs to a trie-based, binary format. The German LM included all of the monol"
W13-2217,D11-1125,0,0.204584,"eal-world translation quality for high-dimensional feature maps and associated weight vectors. That case requires a more scalable tuning algorithm. We describe the Stanford University NLP Group submission to the 2013 Workshop on Statistical Machine Translation Shared Task. We demonstrate the effectiveness of a new adaptive, online tuning algorithm that scales to large feature and tuning sets. For both English-French and English-German, the algorithm produces feature-rich models that improve over a dense baseline and compare favorably to models tuned with established methods. 1 2.1 2 Following Hopkins and May (2011) we cast MT tuning as pairwise ranking. Consider a single source sentence f with associated references e1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Define the linear model score M (d) = w · φ(d). For any derivation d+ that is better than d− under a gold metric G, we desire pairwise agreement such that     G e(d+ ), e1:k &gt; G e(d− ), e1:k Introduction Green et al. (2013b) describe an online, adaptive tuning algorithm for feature-rich translation models. They showed considerable translation quality improvements over MERT (Och, 2003"
W13-2217,P07-2045,0,0.00689866,"at this extension yielded a consistent +0.2 BLEU improvement at test time for both languages. Subsequent experiments on the data sets of Green et al. (2013b) showed that standard BLEU+1 works best for multiple references. Extensions to (Green et al., 2013b) Sentence-Level Metric We previously used the gold metric BLEU+1 (Lin and Och, 2004), which smoothes bigram precisions and above. This metric worked well with multiple references, but we found that it is less effective in a single-reference setting 3 3.1 Feature Sets Dense Features The baseline “dense” model has 19 features: the nine Moses (Koehn et al., 2007) baseline features, a hierarchical lexicalized re-ordering model (Galley and Manning, 2008), the (log) bitext count of each translation rule, and an indicator for unique rules. The final dense feature sets for each language differ slightly. The En-Fr system incorporates a second language model. The En-De system adds a future cost component to the linear distortion model (Green et al., 2010).The future cost estimate allows the distortion limit to be raised without a decrease in translation quality. 149 3.2 Sparse Features Bilingual Sparse features do not necessarily fire on each hypothesis exte"
W13-2217,N06-1014,0,0.0551513,"Stanford CoreNLP. The French data was tokenized with packages from the Stanford French Parser (Green et al., 2013a), which implements a scheme similar to that used in the French Treebank (Abeillé et al., 2003). German is more complicated due to pervasive compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Aligner (Liang et al., 2006) configured with standard settings. We symmetrized the alignments according to the grow-diag heuristic. 4.4 Language Modeling We estimated unfiltered 5-gram language models using lmplz (Heafield et al., 2013) and loaded them with KenLM (Heafield, 2011). For memory efficiency and faster loading we also used KenLM to convert the LMs to a trie-based, binary format. The German LM included all of the monolingual data plus the target side of the En-De bitext. We built an analogous model for French. In addition, we estimated a separate French LM from the Gigaword data.1 4.5 French Agreement Correctio"
W13-2217,C04-1072,0,0.0649442,"elize the weight updates. Green et al. (2013b) describe the parallelization technique that is implemented in Phrasal. 2.2 like WMT. To make the metric more robust, Nakov et al. (2012) extended BLEU+1 by smoothing both the unigram precision and the reference length. We found that this extension yielded a consistent +0.2 BLEU improvement at test time for both languages. Subsequent experiments on the data sets of Green et al. (2013b) showed that standard BLEU+1 works best for multiple references. Extensions to (Green et al., 2013b) Sentence-Level Metric We previously used the gold metric BLEU+1 (Lin and Och, 2004), which smoothes bigram precisions and above. This metric worked well with multiple references, but we found that it is less effective in a single-reference setting 3 3.1 Feature Sets Dense Features The baseline “dense” model has 19 features: the nine Moses (Koehn et al., 2007) baseline features, a hierarchical lexicalized re-ordering model (Galley and Manning, 2008), the (log) bitext count of each translation rule, and an indicator for unique rules. The final dense feature sets for each language differ slightly. The En-Fr system incorporates a second language model. The En-De system adds a fu"
W13-2217,P03-1020,0,0.0538024,"Missing"
W13-2217,J93-2004,0,0.0458196,"f the French monolingual data, but sampled a 5M-sentence bitext from the approximately 40M available En-Fr parallel sentences. To select the sentences we first created a “target” corpus by concatenating the tuning and test sets (newstest2008–2013). Then we ran the feature decay algorithm (FDA) (Biçici and Yuret, 2011), which samples sentences that most closely resemble the target corpus. FDA is a principled method for reducing the phrase table size by excluding less relevant training examples. 150 4.2 Tokenization We tokenized the English (source) data according to the Penn Treebank standard (Marcus et al., 1993) with Stanford CoreNLP. The French data was tokenized with packages from the Stanford French Parser (Green et al., 2013a), which implements a scheme similar to that used in the French Treebank (Abeillé et al., 2003). German is more complicated due to pervasive compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Alig"
W13-2217,N09-1046,0,0.0230455,"d method for reducing the phrase table size by excluding less relevant training examples. 150 4.2 Tokenization We tokenized the English (source) data according to the Penn Treebank standard (Marcus et al., 1993) with Stanford CoreNLP. The French data was tokenized with packages from the Stanford French Parser (Green et al., 2013a), which implements a scheme similar to that used in the French Treebank (Abeillé et al., 2003). German is more complicated due to pervasive compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Aligner (Liang et al., 2006) configured with standard settings. We symmetrized the alignments according to the grow-diag heuristic. 4.4 Language Modeling We estimated unfiltered 5-gram language models using lmplz (Heafield et al., 2013) and loaded them with KenLM (Heafield, 2011). For memory efficiency and faster loading we also used KenLM to convert the LMs to a trie-based, binary format. The German"
W13-2217,D08-1089,1,0.870502,"nguages. Subsequent experiments on the data sets of Green et al. (2013b) showed that standard BLEU+1 works best for multiple references. Extensions to (Green et al., 2013b) Sentence-Level Metric We previously used the gold metric BLEU+1 (Lin and Och, 2004), which smoothes bigram precisions and above. This metric worked well with multiple references, but we found that it is less effective in a single-reference setting 3 3.1 Feature Sets Dense Features The baseline “dense” model has 19 features: the nine Moses (Koehn et al., 2007) baseline features, a hierarchical lexicalized re-ordering model (Galley and Manning, 2008), the (log) bitext count of each translation rule, and an indicator for unique rules. The final dense feature sets for each language differ slightly. The En-Fr system incorporates a second language model. The En-De system adds a future cost component to the linear distortion model (Green et al., 2010).The future cost estimate allows the distortion limit to be raised without a decrease in translation quality. 149 3.2 Sparse Features Bilingual Sparse features do not necessarily fire on each hypothesis extension. Unlike prior work on sparse MT features, our feature extractors do not filter featur"
W13-2217,N10-1129,1,0.850586,"etric worked well with multiple references, but we found that it is less effective in a single-reference setting 3 3.1 Feature Sets Dense Features The baseline “dense” model has 19 features: the nine Moses (Koehn et al., 2007) baseline features, a hierarchical lexicalized re-ordering model (Galley and Manning, 2008), the (log) bitext count of each translation rule, and an indicator for unique rules. The final dense feature sets for each language differ slightly. The En-Fr system incorporates a second language model. The En-De system adds a future cost component to the linear distortion model (Green et al., 2010).The future cost estimate allows the distortion limit to be raised without a decrease in translation quality. 149 3.2 Sparse Features Bilingual Sparse features do not necessarily fire on each hypothesis extension. Unlike prior work on sparse MT features, our feature extractors do not filter features based on tuning set counts. We instead rely on the regularizer to select informative features. Several of the feature extractors depend on source-side part of speech (POS) sequences and dependency parses. We created those annotations with the Stanford CoreNLP pipeline. Discriminative Phrase Table A"
W13-2217,J13-1009,1,0.880191,"Missing"
W13-2217,P13-1031,1,0.41077,"the algorithm produces feature-rich models that improve over a dense baseline and compare favorably to models tuned with established methods. 1 2.1 2 Following Hopkins and May (2011) we cast MT tuning as pairwise ranking. Consider a single source sentence f with associated references e1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Define the linear model score M (d) = w · φ(d). For any derivation d+ that is better than d− under a gold metric G, we desire pairwise agreement such that     G e(d+ ), e1:k &gt; G e(d− ), e1:k Introduction Green et al. (2013b) describe an online, adaptive tuning algorithm for feature-rich translation models. They showed considerable translation quality improvements over MERT (Och, 2003) and PRO (Hopkins and May, 2011) for two languages in a research setting. The purpose of our submission to the 2013 Workshop on Statistical Machine Translation (WMT) Shared Task is to compare the algorithm to more established methods in an evaluation. We submitted English-French (En-Fr) and EnglishGerman (En-De) systems, each with over 100k features tuned on 10k sentences. This paper describes the systems and also includes new feat"
W13-2217,P13-2121,0,0.0659945,"2003). German is more complicated due to pervasive compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Aligner (Liang et al., 2006) configured with standard settings. We symmetrized the alignments according to the grow-diag heuristic. 4.4 Language Modeling We estimated unfiltered 5-gram language models using lmplz (Heafield et al., 2013) and loaded them with KenLM (Heafield, 2011). For memory efficiency and faster loading we also used KenLM to convert the LMs to a trie-based, binary format. The German LM included all of the monolingual data plus the target side of the En-De bitext. We built an analogous model for French. In addition, we estimated a separate French LM from the Gigaword data.1 4.5 French Agreement Correction In French verbs must agree in number and person with their subjects, and adjectives (and some past participles) must agree in number and gender with the nouns they modify. On their own, phrasal alignment an"
W13-2217,W11-2123,0,0.0216746,"compounding. We first tokenized the data with the same English tokenizer. Then we split compounds with the lattice-based model (Dyer, 2009) in cdec (Dyer et al., 2010). To simplify post-processing we added segmentation markers to split tokens, e.g., überschritt ⇒ über #schritt. 4.3 Alignment We aligned both bitexts with the Berkeley Aligner (Liang et al., 2006) configured with standard settings. We symmetrized the alignments according to the grow-diag heuristic. 4.4 Language Modeling We estimated unfiltered 5-gram language models using lmplz (Heafield et al., 2013) and loaded them with KenLM (Heafield, 2011). For memory efficiency and faster loading we also used KenLM to convert the LMs to a trie-based, binary format. The German LM included all of the monolingual data plus the target side of the En-De bitext. We built an analogous model for French. In addition, we estimated a separate French LM from the Gigaword data.1 4.5 French Agreement Correction In French verbs must agree in number and person with their subjects, and adjectives (and some past participles) must agree in number and gender with the nouns they modify. On their own, phrasal alignment and target side language modeling yield correc"
W13-2217,C12-1121,0,0.123388,"han the conventional method. Whereas we previously stopped the algorithm after four iterations, we now select the model according to held-out accuracy. + where [x]+ = max(x, 0) is the clipping function that in this case sets a weight to 0 when it falls below the threshold ηt−1 λ. Online algorithms are inherently sequential; this algorithm is no exception. If we want to scale the algorithm to large tuning sets, then we need to parallelize the weight updates. Green et al. (2013b) describe the parallelization technique that is implemented in Phrasal. 2.2 like WMT. To make the metric more robust, Nakov et al. (2012) extended BLEU+1 by smoothing both the unigram precision and the reference length. We found that this extension yielded a consistent +0.2 BLEU improvement at test time for both languages. Subsequent experiments on the data sets of Green et al. (2013b) showed that standard BLEU+1 works best for multiple references. Extensions to (Green et al., 2013b) Sentence-Level Metric We previously used the gold metric BLEU+1 (Lin and Och, 2004), which smoothes bigram precisions and above. This metric worked well with multiple references, but we found that it is less effective in a single-reference setting"
W13-2217,J04-4002,0,0.209079,"for two languages in a research setting. The purpose of our submission to the 2013 Workshop on Statistical Machine Translation (WMT) Shared Task is to compare the algorithm to more established methods in an evaluation. We submitted English-French (En-Fr) and EnglishGerman (En-De) systems, each with over 100k features tuned on 10k sentences. This paper describes the systems and also includes new feature sets and practical extensions to the original algorithm. Translation Model Our machine translation (MT) system is Phrasal (Cer et al., 2010), a phrase-based system based on alignment templates (Och and Ney, 2004). Like many MT systems, Phrasal models the predictive translation distribution p(e|f ; w) directly as p(e|f ; w) = h i 1 exp w&gt; φ(e, f ) Z(f ) Online, Adaptive Tuning Algorithm ⇐⇒ M (d+ ) &gt; M (d− ) Ensuring pairwise agreement is the same as ensuring w · [φ(d+ ) − φ(d− )] &gt; 0. For learning, we need to select derivation pairs (d+ , d− ) to compute difference vectors x+ = φ(d+ ) − φ(d− ). Then we have a 1-class separation problem trying to ensure w · x+ &gt; 0. The derivation pairs are sampled with the algorithm of Hopkins and May (2011). Suppose that we sample s pairs for source sentence ft to comp"
W13-2217,P03-1021,0,0.0396715,"ay (2011) we cast MT tuning as pairwise ranking. Consider a single source sentence f with associated references e1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map φ(d). Define the linear model score M (d) = w · φ(d). For any derivation d+ that is better than d− under a gold metric G, we desire pairwise agreement such that     G e(d+ ), e1:k &gt; G e(d− ), e1:k Introduction Green et al. (2013b) describe an online, adaptive tuning algorithm for feature-rich translation models. They showed considerable translation quality improvements over MERT (Och, 2003) and PRO (Hopkins and May, 2011) for two languages in a research setting. The purpose of our submission to the 2013 Workshop on Statistical Machine Translation (WMT) Shared Task is to compare the algorithm to more established methods in an evaluation. We submitted English-French (En-Fr) and EnglishGerman (En-De) systems, each with over 100k features tuned on 10k sentences. This paper describes the systems and also includes new feature sets and practical extensions to the original algorithm. Translation Model Our machine translation (MT) system is Phrasal (Cer et al., 2010), a phrase-based syst"
W13-2217,P06-1055,0,0.0641403,"ction most of the time. For verbs, we find that the inflections are often accurate: number is encoded in the English verb and subject, and 3rd person is generally correct in the absence of a 1st or 2nd person pronoun. However, since English does not generally encode gender, adjective inflection must rely on language modeling, which is often insufficient. To address this problem we apply an automatic inflection correction post-processing step. First, we generate dependency parses of our system’s output using BONSAI (Candito and Crabbé, 2009), a French-specific extension to the Berkeley Parser (Petrov et al., 2006). Based on these dependencies, we match adjectives with the nouns they modify and past participles with their subjects. Then we use Lefff (Sagot, 2010), a machine-readable French lexicon, to determine the gender and number of the noun and to choose the correct inflection for the adjective or participle. Applied to our 3,000 sentence development set, this correction scheme produced 200 corrections with perfect accuracy. It produces a slight (−0.014) drop in BLEU score. This arises from cases where the reference translation uses a synonymous but differently gendered noun, and consequently has di"
W13-2217,sagot-2010-lefff,0,0.0129899,"lly correct in the absence of a 1st or 2nd person pronoun. However, since English does not generally encode gender, adjective inflection must rely on language modeling, which is often insufficient. To address this problem we apply an automatic inflection correction post-processing step. First, we generate dependency parses of our system’s output using BONSAI (Candito and Crabbé, 2009), a French-specific extension to the Berkeley Parser (Petrov et al., 2006). Based on these dependencies, we match adjectives with the nouns they modify and past participles with their subjects. Then we use Lefff (Sagot, 2010), a machine-readable French lexicon, to determine the gender and number of the noun and to choose the correct inflection for the adjective or participle. Applied to our 3,000 sentence development set, this correction scheme produced 200 corrections with perfect accuracy. It produces a slight (−0.014) drop in BLEU score. This arises from cases where the reference translation uses a synonymous but differently gendered noun, and consequently has different adjective inflection. 4.6 German De-compounding Split German compounds must be merged after translation. This process often requires inserting"
W13-2217,W09-3821,0,\N,Missing
W13-2217,D08-1076,0,\N,Missing
W13-2217,W11-2131,0,\N,Missing
W13-2239,P96-1041,0,0.0262624,"16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003). We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996). Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion.4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a). We used BOLT dev12 dev as a development test set to explore different α parameterizations of the Positive Diversity criteria. 7 equation (3), we measure the diversity of translations produced by an individual system as the negative BLEU score of the translations with respect to the translations from systems built"
W13-2239,N12-1047,0,0.0167,") (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009),"
W13-2239,D08-1024,0,0.0206523,"orrectness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system"
W13-2239,N10-1141,0,0.110527,"uires the construction of multiple systems that are simultaneously diverse and well-performing. If the systems are not distinct enough, they will bring very little value during system combination. However, if some of the systems produce diverse translations but achieve lower overall translation quality, their contributions risk being ignored during system combination. Prior work has approached the need for diverse systems by using different system architectures, model components, system build parameters, decoder hyperparameters, as well as data selection and weighting (Macherey and Och, 2007; DeNero et al., 2010; Xiao et al., 2013). However, during tuning, each individual system is still just trained to maximize its own isolated performance on a tune set, or at best an error-driven reweighting of the tune set, without explicitly taking into account the diversity of the resulting translations. Such tuning does not encourage systems to rigorously explore model variations that achieve both good translation quality and diversity with respect to the other systems. It is reasonable to suspect that this results in individual systems that under exploit the amount of diversity possible, given the characterist"
W13-2239,W11-2107,0,0.0279539,"erates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each individual machine translation system. This mean"
W13-2239,W00-0405,0,0.0118247,"tion with other systems. They constructed chains of systems whereby the output of one decoder is feed as input to the next decoder in the pipeline. The downstream systems are built and tuned to correct errors produced by the preceding system. In this approach, the downstream decoder acts as a machine learning based post editing system. Related Work While the idea of encouraging diversity in individual systems that will be used for system combination has been proven effective in speech recognition and document summarization (Hinton, 2002; Breslin and Gales, 2007; Carbonell and Goldstein, 1998; Goldstein et al., 2000), there has only been a modest amount of prior work exploring such approaches for machine translation. Prior work within machine translation has investigated adapting machine learning techniques for building ensembles of classifiers to translation system tuning, encouraging diversity by varying both the hyperparameters and the data used to build the individual systems, and chaining together individual translation systems. Xiao et al. (2013) explores using boosting to train an ensemble of machine translation systems. Following the standard Adaboost algorithm, each system was trained in sequence"
W13-2239,P13-1031,1,0.839248,"6.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003). We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996). Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion.4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a). We used BOLT dev12 dev as a development test set to explore different α parameterizations of the Positive Diversity criteria. 7 equation (3), we measure the diversity of translations produced by an individual system as the negative BLEU score of the translations with respect to the translations from systems built during prior iterations. For clarity of presentation, these diversity scores are reported as 1.0−BLEU."
W13-2239,2009.mtsummit-posters.1,0,0.1431,"(Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g., using MERT, PRO, MIRA, RAMPION, etc. models [i] ← tune(systems [i"
W13-2239,W10-1744,0,0.0728713,"are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie,"
W13-2239,2010.amta-papers.34,0,0.519175,"are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie,"
W13-2239,D11-1125,0,0.174735,"ness(ref[], sysΘ ) (1) Θ The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their"
W13-2239,N10-2003,1,0.839823,"n be a heterogeneous collection of substantially different systems (e.g., phrase-based, hierarchical, syntactic, or tunable hybrid systems) or even multiple copies of a single machine translation system. In all cases, systems later in the list will be trained to produce translations that both fit the references and are encouraged to be distinct from the systems earlier in the list. During each iteration, the system constructs a 6 Experiments Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010). The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program. The Chinese data was segmented to the Chinese TreeBank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013). The bitext was word aligned using the Berkeley aligner (Liang et al., 2006). Standard phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4"
W13-2239,D07-1029,0,0.019307,"er combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g., using MERT, PRO, MIRA, RAMPION, etc. models [i] ← tune(systems [i], source, PDα,i ()) // Save translations from tuned modeli for use during // the diversity co"
W13-2239,C08-1145,0,0.0181633,"oughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g., using MERT, PRO, MIRA,"
W13-2239,P08-2021,0,0.016728,"uce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He,"
W13-2239,N03-1017,0,0.00817328,"rd phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using grow-diag (Koehn et al., 2003). We made use of a hierarchical reordering model (Galley and Manning, 2008) as well as a 5-gram language model trained on the target side of the bi-text and smoothed using modified Kneeser-Ney (Chen and Goodman, 1996). Individual PDT systems were tuned on the GALE dev10 web tune set using online-PRO (Green et al., 2013; Hopkins and May, 2011) to the Positive Diversity Tuning criterion.4 The Multi-Engine Machine Translation (MEMT) package was used for system combination (Heafield and Lavie, 2010a). We used BOLT dev12 dev as a development test set to explore different α parameterizations of the"
W13-2239,2003.mtsummit-papers.32,0,0.0342342,"ranslation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each"
W13-2239,N06-1014,0,0.0127583,"from the systems earlier in the list. During each iteration, the system constructs a 6 Experiments Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010). The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program. The Chinese data was segmented to the Chinese TreeBank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013). The bitext was word aligned using the Berkeley aligner (Liang et al., 2006). Standard phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics were used to extract a phrase-table over word alignments symmetrized using gro"
W13-2239,D07-1105,0,0.616982,"l system combination requires the construction of multiple systems that are simultaneously diverse and well-performing. If the systems are not distinct enough, they will bring very little value during system combination. However, if some of the systems produce diverse translations but achieve lower overall translation quality, their contributions risk being ignored during system combination. Prior work has approached the need for diverse systems by using different system architectures, model components, system build parameters, decoder hyperparameters, as well as data selection and weighting (Macherey and Och, 2007; DeNero et al., 2010; Xiao et al., 2013). However, during tuning, each individual system is still just trained to maximize its own isolated performance on a tune set, or at best an error-driven reweighting of the tune set, without explicitly taking into account the diversity of the resulting translations. Such tuning does not encourage systems to rigorously explore model variations that achieve both good translation quality and diversity with respect to the other systems. It is reasonable to suspect that this results in individual systems that under exploit the amount of diversity possible, g"
W13-2239,E06-1005,0,0.0255359,"ould not only obtain good translation performance, but also produce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the in"
W13-2239,P04-1063,0,0.028282,"s results in roughly equivalent translation performance (Cherry and Foster, 2012). Even when individual systems are being built to be used in a larger combined system, they are still usually tuned to maximize their isolated individual system performance rather than to maxi1 The exception being Xiao et al. (2013)’s work using boosting for error-driven reweighting of the tuning set 2 Other system combination techniques exist such as candidate selection systems, whereby the combination model attempts to find the best single candidate produced by one of the translation engines (Paul et al., 2005; Nomoto, 2004; Zwarts and Dras, 2008), decoder chaining (Aikawa and Ruopp, 2009), re-decoding informed by the decoding paths taken by other systems (Huang and Papineni, 2007), and decoding model combination (DeNero et al., 2010). 321 Input : systems [], tune(), source, refs [], α, EvalMetric (), SimMetric () Output: models [] // start with an empty set of translations from prior iterations other_sys [] ← [] for i ← 1 to len(systems []) do // new Positive Diversity measure using prior translations PDα,i () ← new PD(α, EvalMetric(), SimMetric(), refs [], other_sys []) // tune a new model to fit PDα,i // e.g."
W13-2239,P03-1021,0,0.0583526,"ing parameter values Θ that produce translations sysΘ that in turn achieve a high score on some correctness measure: arg max Correctness(ref[], sysΘ ) (1) Θ The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization objectives that indirectly attempt to maximize the correctness function by using it to select targets for training discriminative classification models. In practice, either optimizing correctness directly or optimizing a surrogate objective that uses correctness to choose optimization targets results in roughly equivalent translation performance (Cherry and Fo"
W13-2239,P02-1040,0,0.106954,"when combining a good system with poor performing systems even if the systems col3 System Combination Tuning Individual Translation Systems Machine translation systems are tuned toward some measure of the correctness of the translations produced by the system according to one or more manually translated references. As shown in equation (1), this can be written as finding parameter values Θ that produce translations sysΘ that in turn achieve a high score on some correctness measure: arg max Correctness(ref[], sysΘ ) (1) Θ The correctness measure that systems are typically tuned toward is BLEU (Papineni et al., 2002), which measures the fraction of the n-grams that are both present in the reference translations and the translations produced by a system. The BLEU score is computed as the geometric mean of the resulting n-gram precisions scaled by a brevity penalty. The most widely used machine translation tuning algorithm, minimum error rate training (MERT) (Och, 2003), attempts to maximize the correctness objective directly. Popular alternatives such as pairwise ranking objective (PRO) (Hopkins and May, 2011), MIRA (Chiang et al., 2008), and RAMPION (Gimpel and Smith, 2012) use surrogate optimization obje"
W13-2239,N07-1029,0,0.0983508,"ood translation performance, but also produce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system tran"
W13-2239,P07-1040,0,0.021501,"ood translation performance, but also produce translations that are different from those produced by other systems. 2 Similar to speech recognition’s Recognizer Output Voting Error Reduction (ROVER) algorithm (Fiscus, 1997), machine translation system combination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system tran"
W13-2239,W09-0441,0,0.0194723,"bination typically operates by aligning the translations produced by two or more individual translation systems and then using the alignments to construct a search space that allows new translations to be pieced together by picking and choosing parts of the material from the original translations (Bangalore et al., 2001; Matusov et al., 2006; Rosti et al., 2007a; Rosti et al., 2007b; Karakos et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each individual machine t"
W13-2239,P13-1081,0,0.0124211,"translations that both fit the references and are encouraged to be distinct from the systems earlier in the list. During each iteration, the system constructs a 6 Experiments Experiments are performed using a single phrase-based Chinese-to-English translation system, built with the Stanford Phrasal machine translation toolkit (Cer et al., 2010). The system was built using all of the parallel data available for Phase 2 of the DARPA BOLT program. The Chinese data was segmented to the Chinese TreeBank (CTB) standard using a maximum match word segmenter, trained on the output of a CRF segmenter (Xiang et al., 2013). The bitext was word aligned using the Berkeley aligner (Liang et al., 2006). Standard phrase-pair extraction heuris323 PDT System α = 0.95 α = 0.97 α = 0.99 BLEU scores from individual systems tuned during iteration i of PDT 0 1 2 3 4 5 16.2 16.0 15.7 15.9 16.1 16.1 16.4 15.8 15.8 15.9 16.0 16.2 16.3 16.1 16.2 15.9 16.3 16.4 6 15.9 16.1 16.4 7 15.4 16.2 16.3 8 16.1 16.2 16.4 9 15.9 16.4 16.5 10 16.2 16.1 16.3 Table 3: BLEU scores on BOLT dev12 dev achieved by the individual PDT systems tuned on GALE dev10 web tune. Scores report individual system performance before system combination. tics w"
W13-2239,N09-2052,0,0.01801,"et al., 2008; Heafield and Lavie, 2010a).2 The alignment of the individual system translations can be performed using alignment driven evaluation metrics such as invWER, TERp, METEOR (Leusch et al., 2003; Snover et al., 2009; Denkowski and Lavie, 2011). The piecewise selection of material from the original translations is performed using the combination model’s scoring features such as n-gram language models, confidence models over the individual systems, and consensus features that score a combined translation using n-grams matches to the individual system translations (Rosti et al., 2007b; Zhao and He, 2009; Heafield and Lavie, 2010b). Both system confidence model features and ngram consensus features score contributions based in part on how confident the system combination model is in each individual machine translation system. This means that little or no gains will typically be seen when combining a good system with poor performing systems even if the systems col3 System Combination Tuning Individual Translation Systems Machine translation systems are tuned toward some measure of the correctness of the translations produced by the system according to one or more manually translated references"
W13-2239,D08-1089,1,\N,Missing
W13-2239,2005.iwslt-1.5,0,\N,Missing
W13-2239,N12-1023,0,\N,Missing
W13-3512,N06-2001,0,0.651215,"e word structure, and the semantics in grouping related words.2 2 rect morphological inflections was a very central problem in early work in the parallel distributed processing paradigm and criticisms of it (Rumelhart and McClelland, 1986; Plunkett and Marchman, 1991), and later work developed more sophisticated models of morphological structure and meaning (Gasser and Lee, 1990; Gasser, 1994), while not providing a compositional semantics nor working at the scale of what we present. To the best of our knowledge, the work closest to ours in terms of handing unseen words are the factored NLMs (Alexandrescu and Kirchhoff, 2006) and the compositional distributional semantic models (DSMs) (Lazaridou et al., 2013). In the former work, each word is viewed as a vector of features such as stems, morphological tags, and cases, in which a single embedding matrix is used to look up all of these features.3 Though this is a principled way of handling new words in NLMs, the by-product word representations, i.e. the concatenations of factor vectors, do not encode in them the compositional information (they are stored in the NN parameters). Our work does not simply concatenate vectors of morphemes, but rather combines them using"
W13-3512,E03-1009,0,0.0121095,"ds to complement existing ones in an interesting way. 1 Introduction The use of word representations or word clusters pretrained in an unsupervised fashion from lots of text has become a key “secret sauce” for the success of many NLP systems in recent years, across tasks including named entity recognition, part-ofspeech tagging, parsing, and semantic role labeling. This is particularly true in deep neural network models (Collobert et al., 2011), but it is also true in conventional feature-based models (Koo et al., 2008; Ratinov and Roth, 2009). 1 An almost exception is the word clustering of (Clark, 2003), which does have a model of morphology to encourage words ending with the same suffix to appear in the same class, but it still does not capture the relationship between a word and its morphologically derived forms. 104 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104–113, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics an interesting way to learn morphemic semantics and their compositional properties. Our model has the capability of building representations for any new unseen word comprised of known morphemes,"
W13-3512,P12-1092,1,0.666387,"recently such as sentiment analysis at the sentence (Socher et al., 2011c) and document level (Glorot et al., 2011), language modeling (Mnih and Hinton, 2007; Mikolov and Zweig, 2012), paraphrase detection (Socher et al., 2011a), discriminative parsing (Collobert, 2011), and tasks involving semantic relations and compositional meaning of phrases (Socher et al., 2012). Common to many of these works is use of a distributed word representation as the basic input unit. These representations usually capture local cooccurrence statistics but have also been extended to include document-wide context (Huang et al., 2012). Their main advantage is that they can both be learned unsupervisedly as well as be tuned for supervised tasks. In the former training regiment, they are evaluated by how well they can capture human similarity judgments. They have also been shown to perform well as features for supervised tasks, e.g., NER (Turian et al., 2010). While much work has focused on different objective functions for training single and multiword vector representations, very little work has been done to tackle sub-word units and how they can be used to compute syntactic-semantic word vectors. Collobert et al. (2011) e"
W13-3512,P08-1068,0,0.00420093,"ilarity tasks across many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way. 1 Introduction The use of word representations or word clusters pretrained in an unsupervised fashion from lots of text has become a key “secret sauce” for the success of many NLP systems in recent years, across tasks including named entity recognition, part-ofspeech tagging, parsing, and semantic role labeling. This is particularly true in deep neural network models (Collobert et al., 2011), but it is also true in conventional feature-based models (Koo et al., 2008; Ratinov and Roth, 2009). 1 An almost exception is the word clustering of (Clark, 2003), which does have a model of morphology to encourage words ending with the same suffix to appear in the same class, but it still does not capture the relationship between a word and its morphologically derived forms. 104 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104–113, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics an interesting way to learn morphemic semantics and their compositional properties. Our model has the capab"
W13-3512,P13-1149,0,0.609856,"s was a very central problem in early work in the parallel distributed processing paradigm and criticisms of it (Rumelhart and McClelland, 1986; Plunkett and Marchman, 1991), and later work developed more sophisticated models of morphological structure and meaning (Gasser and Lee, 1990; Gasser, 1994), while not providing a compositional semantics nor working at the scale of what we present. To the best of our knowledge, the work closest to ours in terms of handing unseen words are the factored NLMs (Alexandrescu and Kirchhoff, 2006) and the compositional distributional semantic models (DSMs) (Lazaridou et al., 2013). In the former work, each word is viewed as a vector of features such as stems, morphological tags, and cases, in which a single embedding matrix is used to look up all of these features.3 Though this is a principled way of handling new words in NLMs, the by-product word representations, i.e. the concatenations of factor vectors, do not encode in them the compositional information (they are stored in the NN parameters). Our work does not simply concatenate vectors of morphemes, but rather combines them using RNNs, which captures morphological compositionality. The latter work experimented wit"
W13-3512,P05-1044,0,0.00619239,"been built, the NLM assigns a score for each ngram ni consisting of words x1 , . . . , xn as follows: N ∂J(θ) ∑ ∂s (xi ) = + λθ ∂θ ∂θ i=1 In the case of csmRNN, since the objective function in Eq. (3) is not differentiable, we use the subgradient method (Ratliff et al., 2007) to estimate the objective gradient as: ∂J(θ) = ∂θ s (ni ) = υ ⊤ f (W [x1 ; . . . ; xn ] + b) N ∑ i=1 max{0, 1 − s (ni ) + s (ni )} (3) Here, N is the number of all available ngrams in the training corpus, whereas ni is a “corrupted” ngram created from ni by replacing its last word with a random word similar in spirit to (Smith and Eisner, 2005). Our model parameters are θ = {We , Wm , bm , W , b, υ}. Such a ranking criterion influences the model to assign higher scores to valid ngrams than to 4 i:1−s(ni )+s(ni )&gt;0 − ∂s (ni ) ∂s (ni ) + ∂θ ∂θ Back-propagation through structures (Goller and K¨uchler, 1996) is employed to compute the gradient for each individual cost with similar formulae as in (Socher et al., 2010). Unlike their RNN structures over sentences, where each sentence could have an exponential number of derivations, our morphoRNN structure per word is, in general, deterministic. Each word has a single morphological tree str"
W13-3512,D08-1027,0,0.0216219,"Missing"
W13-3512,N13-1090,0,0.380499,"aptures morphological compositionality. The latter work experimented with different compositional DSMs, originally designed to learn meanings of phrases, to derive representations for complex words, in which the base unit is the morpheme similar to ours. However, their models can only combine a stem with an affix and does not support recursive morpheme composition. It is, however, interesting to compare our neural-based representations with their DSM-derived ones and cross test these models on both our rare word similarity dataset and their nearest neighbor one, which we leave as future work. Mikolov et al. (2013) examined existing word embeddings and showed that these representations already captured meaningful syntactic and semantic regularities such as the singular/plural relation that xapple - xapples ≈ xcar - xcars . However, we believe that these nice relationships will not hold for rare and complex words when their vectors are poorly estimated as we analyze in Section 6. Our model, on the other hand, explicitly represents these regularities through morphological structures of words. Related Work Neural network techniques have found success in several NLP tasks recently such as sentiment analysis"
W13-3512,D11-1014,1,0.596159,"t al., 2003) to hierarchical models (Morin, 2005; Mnih and Hinton, 2009) and recently recurrent neural networks (Mikolov et al., 2010; Mikolov et al., 2011), these approaches treat each full-form word as an independent entity and fail to capture the explicit relationship among morphological variants of a word.1 The fact that morphologically complex words are often rare exacerbates the problem. Though existing clusterings and embeddings represent well frequent words, such as “distinct”, they often badly model rare ones, such as “distinctiveness”. In this work, we use recursive neural networks (Socher et al., 2011b), in a novel way to model morphology and its compositionality. Essentially, we treat each morpheme as a basic unit in the RNNs and construct representations for morphologically complex words on the fly from their morphemes. By training a neural language model (NLM) and integrating RNN structures for complex words, we utilize contextual information in Vector-space word representations have been very successful in recent years at improving performance across a variety of NLP tasks. However, common to most existing work, words are regarded as independent entities without any explicit relationsh"
W13-3512,D12-1110,1,0.144032,"vectors are poorly estimated as we analyze in Section 6. Our model, on the other hand, explicitly represents these regularities through morphological structures of words. Related Work Neural network techniques have found success in several NLP tasks recently such as sentiment analysis at the sentence (Socher et al., 2011c) and document level (Glorot et al., 2011), language modeling (Mnih and Hinton, 2007; Mikolov and Zweig, 2012), paraphrase detection (Socher et al., 2011a), discriminative parsing (Collobert, 2011), and tasks involving semantic relations and compositional meaning of phrases (Socher et al., 2012). Common to many of these works is use of a distributed word representation as the basic input unit. These representations usually capture local cooccurrence statistics but have also been extended to include document-wide context (Huang et al., 2012). Their main advantage is that they can both be learned unsupervisedly as well as be tuned for supervised tasks. In the former training regiment, they are evaluated by how well they can capture human similarity judgments. They have also been shown to perform well as features for supervised tasks, e.g., NER (Turian et al., 2010). While much work has"
W13-3512,P10-1040,0,0.632288,"meaning of phrases (Socher et al., 2012). Common to many of these works is use of a distributed word representation as the basic input unit. These representations usually capture local cooccurrence statistics but have also been extended to include document-wide context (Huang et al., 2012). Their main advantage is that they can both be learned unsupervisedly as well as be tuned for supervised tasks. In the former training regiment, they are evaluated by how well they can capture human similarity judgments. They have also been shown to perform well as features for supervised tasks, e.g., NER (Turian et al., 2010). While much work has focused on different objective functions for training single and multiword vector representations, very little work has been done to tackle sub-word units and how they can be used to compute syntactic-semantic word vectors. Collobert et al. (2011) enhanced word vectors with additional character-level features such as capitalization but still can not recover more detailed semantics for very rare or unseen words, which is the focus of this work. This is somewhat ironic, since working out cor2 The rare word dataset and trained word vectors can be found at http://nlp.stanford"
W13-3512,W09-1119,0,0.0407573,"ss many datasets, including a new dataset we introduce focused on rare words to complement existing ones in an interesting way. 1 Introduction The use of word representations or word clusters pretrained in an unsupervised fashion from lots of text has become a key “secret sauce” for the success of many NLP systems in recent years, across tasks including named entity recognition, part-ofspeech tagging, parsing, and semantic role labeling. This is particularly true in deep neural network models (Collobert et al., 2011), but it is also true in conventional feature-based models (Koo et al., 2008; Ratinov and Roth, 2009). 1 An almost exception is the word clustering of (Clark, 2003), which does have a model of morphology to encourage words ending with the same suffix to appear in the same class, but it still does not capture the relationship between a word and its morphologically derived forms. 104 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104–113, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics an interesting way to learn morphemic semantics and their compositional properties. Our model has the capability of building represe"
W13-3512,N10-1013,0,0.196558,"h makes it a challenging dataset. WS353 MC RG SCWS∗ RW All words 0 |0 / 9 / 87 / 341 0 |0 / 1 / 17 / 21 0 |0 / 4 / 22 / 22 26 |2 / 140 / 472 / 1063 801 |41 / 676 / 719 / 714 Complex words 0 |0 / 1 / 6 / 10 0|0/0/0/0 0|0/0/0/0 8 |2 / 22 / 44 / 45 621 |34 / 311 / 238 / 103 Table 4: Word distribution by frequencies – distinct words in each dataset are grouped based on frequencies and counts are reported for the following bins : unknown |[1, 100] / [101, 1000] / [1001, 10000] / [10001, ∞). We report counts for all words in each dataset as well as complex ones. 5.1 Word Similarity Task Similar to (Reisinger and Mooney, 2010) and (Huang et al., 2012), we evaluate the quality of our morphologically-aware embeddings on the popular WordSim-353 dataset (Finkelstein et al., 2002), WS353 for short. In this task, we compare correlations between the similarity scores given by our models and those rated by human. To avoid overfitting our models to a single dataset, we benchmark our models on a variety of others including MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS∗11 (Huang et al., 2012), and our new rare word (RW) dataset (details in §5.1.1). Information about these datasets are summarized in"
W13-3512,P94-1039,0,\N,Missing
W13-3515,P11-1062,0,0.0501137,"ticular presents a framework for making inferences which are not strictly entailed but can be reasonably assumed. Unlike these works, our approach places a greater emphasis on working with large corpora of open-domain predicates. Many NLP applications make use of a knowledge base of facts. These include semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate et al., 2005; Zettlemoyer and Collins, 2007) question answering (Voorhees, 2001), information extraction (Hoffmann et al., 2011; Surdeanu et al., 2012), and recognizing textual entailment (Schoenmackers et al., 2010; Berant et al., 2011). A large body of work has been devoted to creating such knowledge bases. In particular, Open IE systems such as TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011), Ollie (Mausam et al., 2012), and NELL (Carlson et al., 2010) have tackled the task of compiling an open-domain knowledge base. Similarly, the MIT Media Lab’s ConceptNet project (Liu and Singh, 2004) has been working on creating a large database of common sense facts. There have been a number of systems aimed at automatically extending these databases. That is, given an existing database, they propose new relations to be a"
W13-3515,J06-1003,0,0.0199776,"hington’s ReVerb system run over web text. To showcase the system within a cleaner environment, we also build a knowledge base from the MIT Media Lab’s ConceptNet. 4.1 Relation Desires CapableOf Table 3: Example ConceptNet extractions. The top rows correspond to characteristic correct extractions; the bottom rows characterize the types of noise in the data. • Argument Similarity: We define a feature for the similarity between the two arguments in the query fact. Similarity metrics (particularly distributional similarity metrics) often capture a notion more akin to relatedness than similarity (Budanitsky and Hirst, 2006); the subject and object of a relation are, in many cases, related in this sense. 4 Argument 1 cat air ReVerb We created a knowledge base of facts by running ReVerb over ClueWeb09 (Callan et al., 2009). Extractions rated with a confidence under 0.5 were discarded; the first billion undiscarded extractions were used in the final knowledge base. This resulted in approximately 500 million unique facts. Some examples of facts extracted with ReVerb are given in Table 2. Note that our notion of plausibility is far more unclear than in the ConceptNet data; many facts extracted from the internet are e"
W13-3515,N13-1008,0,0.0324867,"at automatically extending these databases. That is, given an existing database, they propose new relations to be added. Snow et al. (2006) present an approach to enriching the WordNet taxonomy; Tandon et al. (2011) extend ConceptNet with new facts; Soderland et al. (2010) use ReVerb extractions to enrich a domain-specific ontology. We differ from these approaches in that we aim to provide an exhaustive completion of the database; we would like to respond to a query with either membership or lack of membership, rather than extending the set of elements which are members. Yao et al. (2012) and Riedel et al. (2013) present a similar task of predicting novel relations between Freebase entities by appealing to a large collection of Open IE extractions. Our work focuses on arguments which are not necessarily named entities, at the expense of leveraging less entityspecific information. Work in classical artificial intelligence has tackled the related task of loosening the closed world assumption and monotonicity of logical reasoning, allowing for modeling of unseen propositions. Reiter (1980) presents an approach to leveraging default propositions in the absence of contradictory evidence; McCarthy (1980) de"
W13-3515,W08-1301,1,0.201595,"Missing"
W13-3515,D11-1142,0,0.0432524,"degree of confidence for each unseen fact. This task can be Our approach generalizes word similarity metrics to a notion of fact similarity, and judges the membership of an unseen fact based on the aggregate similarity between it and existing members of the database. For instance, if we have not seen the fact that philosophers are mortal1 but we know that Greeks are mortal, and that philosophers and Greeks are similar, we would like to infer that the fact is nonetheless plausible. We implement our approach on both a large open-domain database of facts extracted from the Open IE system ReVerb (Fader et al., 2011), and ConceptNet (Liu and Singh, 2004), a hand curated database of common sense facts. 1 This is an unseen fact in http://openie.cs. washington.edu. 133 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 133–142, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics 2 Related Work particular presents a framework for making inferences which are not strictly entailed but can be reasonably assumed. Unlike these works, our approach places a greater emphasis on working with large corpora of open-domain predicates. Many NLP applic"
W13-3515,D10-1106,0,0.0322471,"nguistics 2 Related Work particular presents a framework for making inferences which are not strictly entailed but can be reasonably assumed. Unlike these works, our approach places a greater emphasis on working with large corpora of open-domain predicates. Many NLP applications make use of a knowledge base of facts. These include semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate et al., 2005; Zettlemoyer and Collins, 2007) question answering (Voorhees, 2001), information extraction (Hoffmann et al., 2011; Surdeanu et al., 2012), and recognizing textual entailment (Schoenmackers et al., 2010; Berant et al., 2011). A large body of work has been devoted to creating such knowledge bases. In particular, Open IE systems such as TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011), Ollie (Mausam et al., 2012), and NELL (Carlson et al., 2010) have tackled the task of compiling an open-domain knowledge base. Similarly, the MIT Media Lab’s ConceptNet project (Liu and Singh, 2004) has been working on creating a large database of common sense facts. There have been a number of systems aimed at automatically extending these databases. That is, given an existing database, they propose"
W13-3515,P11-1055,0,0.0287544,"133–142, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics 2 Related Work particular presents a framework for making inferences which are not strictly entailed but can be reasonably assumed. Unlike these works, our approach places a greater emphasis on working with large corpora of open-domain predicates. Many NLP applications make use of a knowledge base of facts. These include semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate et al., 2005; Zettlemoyer and Collins, 2007) question answering (Voorhees, 2001), information extraction (Hoffmann et al., 2011; Surdeanu et al., 2012), and recognizing textual entailment (Schoenmackers et al., 2010; Berant et al., 2011). A large body of work has been devoted to creating such knowledge bases. In particular, Open IE systems such as TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011), Ollie (Mausam et al., 2012), and NELL (Carlson et al., 2010) have tackled the task of compiling an open-domain knowledge base. Similarly, the MIT Media Lab’s ConceptNet project (Liu and Singh, 2004) has been working on creating a large database of common sense facts. There have been a number of systems aimed at au"
W13-3515,P12-1092,1,0.0332401,"is not found a number of simple variants are also tried. These are, in order of preference: a lemmatized version of the phrase, the head word of the phrase, and the head lemma of the phrase. If none of these are found, then the named entities in the sentence were replaced with their types. If that fails as well, acronyms3 were expanded. For words with multiple sense, the maximum similarity for any pair of word senses was used. Lin Jiang-Conrath Wu-Palmer Distributional Cosine Distributional Similarity Based Metrics We define a number of similarity metrics on the 50 dimensional word vectors of Huang et al. (2012). These cover a vocabulary of 100,231 words; a special vector is defined for unknown words. Compound phrases are queried by treating the phrase as a bag of words and averaging the word vectors of each word in the phrase, pruning out unknown words. If the phrase contains no known words, the same relaxation steps are tried as the thesaurus based metrics. 3.3 Name Path Resnik Angle Jensen-Shannon Hellinger Jaccard Dice Formula − log len(w1 , lcs, w2 ) − log P (lcs) log(P (lcs)2 ) log(P (w1 )·P (w2 )) −1 P (lcs)2 P (w1 )·P (w2 ) 2·depth(lcs) 2·depth(lcs)+len(w1 ,lcs,w2 ) log  w1 ·w2 kw1 kkw2 k"
W13-3515,P06-1101,0,0.0197046,"rge body of work has been devoted to creating such knowledge bases. In particular, Open IE systems such as TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011), Ollie (Mausam et al., 2012), and NELL (Carlson et al., 2010) have tackled the task of compiling an open-domain knowledge base. Similarly, the MIT Media Lab’s ConceptNet project (Liu and Singh, 2004) has been working on creating a large database of common sense facts. There have been a number of systems aimed at automatically extending these databases. That is, given an existing database, they propose new relations to be added. Snow et al. (2006) present an approach to enriching the WordNet taxonomy; Tandon et al. (2011) extend ConceptNet with new facts; Soderland et al. (2010) use ReVerb extractions to enrich a domain-specific ontology. We differ from these approaches in that we aim to provide an exhaustive completion of the database; we would like to respond to a query with either membership or lack of membership, rather than extending the set of elements which are members. Yao et al. (2012) and Riedel et al. (2013) present a similar task of predicting novel relations between Freebase entities by appealing to a large collection of O"
W13-3515,P03-1054,1,0.0294095,"Missing"
W13-3515,D12-1042,1,0.830842,"aria, August 8-9 2013. 2013 Association for Computational Linguistics 2 Related Work particular presents a framework for making inferences which are not strictly entailed but can be reasonably assumed. Unlike these works, our approach places a greater emphasis on working with large corpora of open-domain predicates. Many NLP applications make use of a knowledge base of facts. These include semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate et al., 2005; Zettlemoyer and Collins, 2007) question answering (Voorhees, 2001), information extraction (Hoffmann et al., 2011; Surdeanu et al., 2012), and recognizing textual entailment (Schoenmackers et al., 2010; Berant et al., 2011). A large body of work has been devoted to creating such knowledge bases. In particular, Open IE systems such as TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011), Ollie (Mausam et al., 2012), and NELL (Carlson et al., 2010) have tackled the task of compiling an open-domain knowledge base. Similarly, the MIT Media Lab’s ConceptNet project (Liu and Singh, 2004) has been working on creating a large database of common sense facts. There have been a number of systems aimed at automatically extending th"
W13-3515,D12-1048,0,0.0366982,"of open-domain predicates. Many NLP applications make use of a knowledge base of facts. These include semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate et al., 2005; Zettlemoyer and Collins, 2007) question answering (Voorhees, 2001), information extraction (Hoffmann et al., 2011; Surdeanu et al., 2012), and recognizing textual entailment (Schoenmackers et al., 2010; Berant et al., 2011). A large body of work has been devoted to creating such knowledge bases. In particular, Open IE systems such as TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011), Ollie (Mausam et al., 2012), and NELL (Carlson et al., 2010) have tackled the task of compiling an open-domain knowledge base. Similarly, the MIT Media Lab’s ConceptNet project (Liu and Singh, 2004) has been working on creating a large database of common sense facts. There have been a number of systems aimed at automatically extending these databases. That is, given an existing database, they propose new relations to be added. Snow et al. (2006) present an approach to enriching the WordNet taxonomy; Tandon et al. (2011) extend ConceptNet with new facts; Soderland et al. (2010) use ReVerb extractions to enrich a domain-s"
W13-3515,N03-1033,1,0.0395677,"Missing"
W13-3515,W12-3022,0,0.0699048,"mber of systems aimed at automatically extending these databases. That is, given an existing database, they propose new relations to be added. Snow et al. (2006) present an approach to enriching the WordNet taxonomy; Tandon et al. (2011) extend ConceptNet with new facts; Soderland et al. (2010) use ReVerb extractions to enrich a domain-specific ontology. We differ from these approaches in that we aim to provide an exhaustive completion of the database; we would like to respond to a query with either membership or lack of membership, rather than extending the set of elements which are members. Yao et al. (2012) and Riedel et al. (2013) present a similar task of predicting novel relations between Freebase entities by appealing to a large collection of Open IE extractions. Our work focuses on arguments which are not necessarily named entities, at the expense of leveraging less entityspecific information. Work in classical artificial intelligence has tackled the related task of loosening the closed world assumption and monotonicity of logical reasoning, allowing for modeling of unseen propositions. Reiter (1980) presents an approach to leveraging default propositions in the absence of contradictory evi"
W13-3515,N07-4013,0,0.0369953,"h places a greater emphasis on working with large corpora of open-domain predicates. Many NLP applications make use of a knowledge base of facts. These include semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate et al., 2005; Zettlemoyer and Collins, 2007) question answering (Voorhees, 2001), information extraction (Hoffmann et al., 2011; Surdeanu et al., 2012), and recognizing textual entailment (Schoenmackers et al., 2010; Berant et al., 2011). A large body of work has been devoted to creating such knowledge bases. In particular, Open IE systems such as TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011), Ollie (Mausam et al., 2012), and NELL (Carlson et al., 2010) have tackled the task of compiling an open-domain knowledge base. Similarly, the MIT Media Lab’s ConceptNet project (Liu and Singh, 2004) has been working on creating a large database of common sense facts. There have been a number of systems aimed at automatically extending these databases. That is, given an existing database, they propose new relations to be added. Snow et al. (2006) present an approach to enriching the WordNet taxonomy; Tandon et al. (2011) extend ConceptNet with new facts; Soderland"
W13-3515,D07-1071,0,0.0214738,"Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 133–142, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics 2 Related Work particular presents a framework for making inferences which are not strictly entailed but can be reasonably assumed. Unlike these works, our approach places a greater emphasis on working with large corpora of open-domain predicates. Many NLP applications make use of a knowledge base of facts. These include semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate et al., 2005; Zettlemoyer and Collins, 2007) question answering (Voorhees, 2001), information extraction (Hoffmann et al., 2011; Surdeanu et al., 2012), and recognizing textual entailment (Schoenmackers et al., 2010; Berant et al., 2011). A large body of work has been devoted to creating such knowledge bases. In particular, Open IE systems such as TextRunner (Yates et al., 2007), ReVerb (Fader et al., 2011), Ollie (Mausam et al., 2012), and NELL (Carlson et al., 2010) have tackled the task of compiling an open-domain knowledge base. Similarly, the MIT Media Lab’s ConceptNet project (Liu and Singh, 2004) has been working on creating a la"
W13-3721,D11-1037,0,0.0975334,"The SD scheme has been in use for seven years, but still lacks principled analyses of many of the difficult English constructions that have been a staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set o"
W13-3721,W07-2416,0,0.0184095,"staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set of chosen longdistance dependency constructions (Rimell et al. 2009), there are no gold standard Stanford dependency annotations. tactic formalism. For ex"
W13-3721,W06-2920,0,0.0547907,"ructions that have been a staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set of chosen longdistance dependency constructions (Rimell et al. 2009), there are no gold standard Stanford dependency annotatio"
W13-3721,de-marneffe-etal-2006-generating,1,0.178631,"Missing"
W13-3721,P80-1024,0,0.417167,"Missing"
W13-3721,W07-1004,0,0.0560843,"imell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 2007) and a small set of chosen longdistance dependency constructions (Rimell et al. 2009), there are no gold standard Stanford dependency annotations. tactic formalism. For example, in (1a), the object of find can be “raised” to subject position in the main clause to form a tough adjective construction, as in (1b). One of the difficulties for generative grammar in modeling this construction is that the object being raised can be embedded arbitrarily deeply in the sentence, as in (1c). introduces a PP experiencer, the PP can “move” separately (4b), both supporting the hypothesis that the experience"
W13-3721,D09-1085,0,0.405499,"erent constructions The SD scheme has been in use for seven years, but still lacks principled analyses of many of the difficult English constructions that have been a staple of the formal linguistic literature. However, we have found in our annotation work that some of these constructions now arise prominently in terms of cases for which the correct analysis is unclear. Here we try to resolve several of these interesting corner-cases of English grammar. Some of these cases, such as tough adjectives and free relatives, were also discussed in recent evaluations of dependency extraction systems (Rimell et al. 2009, Bender et al. 2011) where the goal was to recover long dependencies. The family of CoNLL dependency schemes for English (Buchholz and Marsi 2006, Johansson and Nugues 2007), another common dependency representation in NLP, largely does not provide satisfying analyses for any of the cases presented here. Small clauses are the one exception, and the CoNLL treatment of small clauses is similar to ours. 4.1 Tough adjectives Tough adjectives, discussed in Bender et al. (2011), have posed challenges to nearly every syn2 To date, except for the BioInfer corpus of biomedical texts (Pyysalo et al. 20"
W13-3721,W08-1301,1,\N,Missing
W14-1611,J92-4003,0,0.246472,"Missing"
W14-1611,D13-1079,0,0.0882977,"fficacy of patterns can be judged by their performance on a fully labeled dataset (Califf and Mooney, 1999; Ciravegna, 2001). In a bootstrapped system, where the data is not fully labeled, existing systems score patterns by either ignoring the unIntroduction This paper considers the problem of building effective entity extractors for custom entity types from specialized domain corpora. We approach the problem by learning rules bootstrapped using seed sets of entities. Though entity extraction using machine learning is common in academic research, rule-based systems dominate in commercial use (Chiticariu et al., 2013), mainly because rules are effective, interpretable, and are easy to customize by non-experts to cope with errors. They also have been shown to perform better than state-of-the-art machine learning methods on some specialized domains (Nallapati and Manning, 2008; Gupta and Manning, 2014a). In addition, building supervised machine learning systems for a reasonably large domain-specific corpus would require hand-labeling sufficient data to 98 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 98–108, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for"
W14-1611,C92-2082,0,0.0970104,"is shown in italics and the extracted entities are shown in bold. train a model, which can be costly and time consuming. Bootstrapped machine-learned rules can make extraction easier and more efficient on such a corpus. In a bootstrapped rule-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific class from unlabeled text (Riloff, 1996; Collins and Singer, 1999). Rules are typically defined by creating patterns around the entities, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). Patterns are scored by their ability to extract more positive entities and less negative entities. Top ranked patterns are used to extract candidate entities from text. High scoring candidate entities are added to the dictionaries and are used to generate more candidate patterns around them. In a supervised setting, the efficacy of patterns can be judged by their performance on a fully labeled dataset (Califf and Mooney, 1999; Ciravegna, 2001). In a bootstrapped system, where the data is not fully labeled, existing systems score patterns"
W14-1611,W99-0613,0,0.0835868,"An example pattern learning system for the class ‘animals’ from the text. Pattern 1 and 2 are candidate patterns. Text matched with the patterns is shown in italics and the extracted entities are shown in bold. train a model, which can be costly and time consuming. Bootstrapped machine-learned rules can make extraction easier and more efficient on such a corpus. In a bootstrapped rule-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific class from unlabeled text (Riloff, 1996; Collins and Singer, 1999). Rules are typically defined by creating patterns around the entities, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). Patterns are scored by their ability to extract more positive entities and less negative entities. Top ranked patterns are used to extract candidate entities from text. High scoring candidate entities are added to the dictionaries and are used to generate more candidate patterns around them. In a supervised setting, the efficacy of patterns can be judged by their performance on a fully labeled dataset (Calif"
W14-1611,D12-1048,0,0.00837428,"rns by their precision, assuming unlaRelated Work Rule based learning has been a topic of interest for many years. Patwardhan (2010) gives a good overview of the research in the field. Rule learn1 www.medhelp.org 99 3. Learning entities: Learned patterns for the class are applied to the text to extract candidate entities. An entity scorer ranks the candidate entities and adds the top entities to C’s dictionary. beled entities to be negative; one of our baselines is similar to their pattern assessment method. Other open information extraction systems like ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012) are mainly geared towards generic, domain-independent relation extractors for web data. We tested learning an entity extractor for a given class using ReVerb. We labeled the binary and unary ReVerb extractions using the class seed entities and retrained its confidence function, with poor results. Poon and Domingos (2010) found a similar result for inducing a probabilistic ontology: an open information extraction system extracted low accuracy relational triples on a small corpus. In this paper, we use features such as distributional similarity and edit distances from learned entities to score"
W14-1611,P09-1045,0,0.0414006,"the binary and unary ReVerb extractions using the class seed entities and retrained its confidence function, with poor results. Poon and Domingos (2010) found a similar result for inducing a probabilistic ontology: an open information extraction system extracted low accuracy relational triples on a small corpus. In this paper, we use features such as distributional similarity and edit distances from learned entities to score patterns. Similar measures have been used before but for learning entities, labeling semantic classes, or for reducing noise in seed sets (Pantel and Ravichandran, 2004; McIntosh and Curran, 2009). Measures for improving entity learning can be used alongside ours since we focus on scoring candidate patterns. 3 The success of bootstrapped pattern learning methods crucially depends on the effectiveness of the pattern scorer and the entity scorer. Here we focus on improving the pattern scoring measure (Step 2 above). 3.1 Creating Patterns Candidate patterns are created using contexts of words or their lemmas in a window of two to four words before and after a positively labeled token. Context words that are labeled with one of the classes are generalized with that class. The target term h"
W14-1611,D08-1046,1,0.800917,"This paper considers the problem of building effective entity extractors for custom entity types from specialized domain corpora. We approach the problem by learning rules bootstrapped using seed sets of entities. Though entity extraction using machine learning is common in academic research, rule-based systems dominate in commercial use (Chiticariu et al., 2013), mainly because rules are effective, interpretable, and are easy to customize by non-experts to cope with errors. They also have been shown to perform better than state-of-the-art machine learning methods on some specialized domains (Nallapati and Manning, 2008; Gupta and Manning, 2014a). In addition, building supervised machine learning systems for a reasonably large domain-specific corpus would require hand-labeling sufficient data to 98 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 98–108, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics labeled entities or assuming them to be negative. However, these scoring schemes cannot differentiate between patterns that extract good versus bad unlabeled entities. The problem is similar to the closed world assumption in distantly"
W14-1611,D11-1142,0,0.0347244,"ning tasks. They assessed patterns by their precision, assuming unlaRelated Work Rule based learning has been a topic of interest for many years. Patwardhan (2010) gives a good overview of the research in the field. Rule learn1 www.medhelp.org 99 3. Learning entities: Learned patterns for the class are applied to the text to extract candidate entities. An entity scorer ranks the candidate entities and adds the top entities to C’s dictionary. beled entities to be negative; one of our baselines is similar to their pattern assessment method. Other open information extraction systems like ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012) are mainly geared towards generic, domain-independent relation extractors for web data. We tested learning an entity extractor for a given class using ReVerb. We labeled the binary and unary ReVerb extractions using the class seed entities and retrained its confidence function, with poor results. Poon and Domingos (2010) found a similar result for inducing a probabilistic ontology: an open information extraction system extracted low accuracy relational triples on a small corpus. In this paper, we use features such as distributional similarity and edit distances"
W14-1611,N04-1041,0,0.0146028,"class using ReVerb. We labeled the binary and unary ReVerb extractions using the class seed entities and retrained its confidence function, with poor results. Poon and Domingos (2010) found a similar result for inducing a probabilistic ontology: an open information extraction system extracted low accuracy relational triples on a small corpus. In this paper, we use features such as distributional similarity and edit distances from learned entities to score patterns. Similar measures have been used before but for learning entities, labeling semantic classes, or for reducing noise in seed sets (Pantel and Ravichandran, 2004; McIntosh and Curran, 2009). Measures for improving entity learning can be used alongside ours since we focus on scoring candidate patterns. 3 The success of bootstrapped pattern learning methods crucially depends on the effectiveness of the pattern scorer and the entity scorer. Here we focus on improving the pattern scoring measure (Step 2 above). 3.1 Creating Patterns Candidate patterns are created using contexts of words or their lemmas in a window of two to four words before and after a positively labeled token. Context words that are labeled with one of the classes are generalized with t"
W14-1611,P98-1067,0,0.0211491,", described in Gupta and Manning (2014b), that visualizes and compares output of multiple pattern-based entity extraction systems. It can be downloaded at http://nlp.stanford.edu/software/ patternviz.shtml. 2 ing systems differ in how they create rules, score them, and score the entities they extract. Here, we mainly discuss the rule scoring part of the previous entity extraction research. The pioneering work by Hearst (1992) used hand written rules to automatically generate more rules that were manually evaluated to extract hypernym-hyponym pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP )2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination o"
W14-1611,P10-1031,0,0.0161002,"ies. An entity scorer ranks the candidate entities and adds the top entities to C’s dictionary. beled entities to be negative; one of our baselines is similar to their pattern assessment method. Other open information extraction systems like ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012) are mainly geared towards generic, domain-independent relation extractors for web data. We tested learning an entity extractor for a given class using ReVerb. We labeled the binary and unary ReVerb extractions using the class seed entities and retrained its confidence function, with poor results. Poon and Domingos (2010) found a similar result for inducing a probabilistic ontology: an open information extraction system extracted low accuracy relational triples on a small corpus. In this paper, we use features such as distributional similarity and edit distances from learned entities to score patterns. Similar measures have been used before but for learning entities, labeling semantic classes, or for reducing noise in seed sets (Pantel and Ravichandran, 2004; McIntosh and Curran, 2009). Measures for improving entity learning can be used alongside ours since we focus on scoring candidate patterns. 3 The success"
W14-1611,W14-3106,1,0.864907,"Missing"
W14-1611,Q13-1030,0,0.0104853,"specific corpus would require hand-labeling sufficient data to 98 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 98–108, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics labeled entities or assuming them to be negative. However, these scoring schemes cannot differentiate between patterns that extract good versus bad unlabeled entities. The problem is similar to the closed world assumption in distantly supervised information extraction systems, when all propositions missing from a knowledge base are considered false (Ritter et al., 2013; Xu et al., 2013). Predicting labels of unlabeled entities can improve scoring patterns. Consider the example shown in Figure 1. Current pattern learning systems would score both patterns equally. However, features like distributional similarity can predict ‘cat’ to be closer to {dog} than ‘car’, and a pattern learning system can use that information to rank ‘Pattern 1’ higher than ‘Pattern 2’. In this paper, we work on bootstrapping entity extraction using seed sets of entities and an unlabeled text corpus. We improve the scoring of patterns for an entity class by defining a pattern’s score"
W14-1611,P05-1047,0,0.0235891,"ted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure ignored unlabeled entities and the confidence measure treated them as negative. Gupta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordnet to assess patterns, which is not feasible for domains that have low coverage in Wordnet, such as medical data. More recently, open information extraction systems have garnered attention. They focus on extracting entities and relations from the web. KnowItAll’s entity extraction from the web (Downey et al., 2004; Etzioni et al., 2005) used components such as list extractors, generic and domain specific pattern learning, and subclass learning. They learned domain-specific patterns using a seed set and scored them by ignoring unlabeled entities. One of our baselines is similar to th"
W14-1611,W02-1028,0,0.329081,"to automatically generate more rules that were manually evaluated to extract hypernym-hyponym pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP )2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure ignored unlabeled entities and the confidence measure treated them as negative. Gupta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordn"
W14-1611,N03-1033,1,0.0532198,"is labeled with the dictionary’s class if the sequence of phrase words or their lemmas match with the sequence of words of a dictionary phrase. Since our corpora are from online discussion forums, they have many spelling mistakes and morphological variations of entities. To deal with the variations, we do fuzzy matching of words – if two words are one edit distance away and are more than 6 characters long, then they are considered a match. We used Stanford TokensRegex (Chang and Manning, 2014) to create and apply surface word patterns to text, and used the Stanford Part-ofSpeech (POS) tagger (Toutanova et al., 2003) to find POS tags of tokens and lemmatize them. When creating patterns, we discarded patterns whose left or right context was 1 or 2 stop words to avoid generating low precision patterns.6 In each iteration, we learned a maximum 20 patterns with ps(r) ≥ θr and maximum 10 words with score ≥ and other classes as negative) and adding the top 50 words extracted by the top 300 patterns to the SC class dictionary. This helps in adding corpus specific SC words to the dictionary. 5 We used top 10,000 words from Google N-grams and top 5,000 words from Twitter (www.twitter.com), accessed from May 19 to"
W14-1611,P13-2117,0,0.0174167,"Missing"
W14-1611,C00-2136,0,0.0269151,"es are shown in bold. train a model, which can be costly and time consuming. Bootstrapped machine-learned rules can make extraction easier and more efficient on such a corpus. In a bootstrapped rule-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific class from unlabeled text (Riloff, 1996; Collins and Singer, 1999). Rules are typically defined by creating patterns around the entities, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). Patterns are scored by their ability to extract more positive entities and less negative entities. Top ranked patterns are used to extract candidate entities from text. High scoring candidate entities are added to the dictionaries and are used to generate more candidate patterns around them. In a supervised setting, the efficacy of patterns can be judged by their performance on a fully labeled dataset (Califf and Mooney, 1999; Ciravegna, 2001). In a bootstrapped system, where the data is not fully labeled, existing systems score patterns by either ignoring the unIntroduction This paper consi"
W14-1611,C02-1154,0,0.0296534,"pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP )2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure ignored unlabeled entities and the confidence measure treated them as negative. Gupta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordnet to assess patterns, which is not feasible for domains that have low coverage in Wordnet, s"
W14-1611,C98-1064,0,\N,Missing
W14-2404,D13-1197,0,0.0146773,"stated by the input text. Text to scene generation offers a rich, interactive environment for grounded language that is familiar to everyone. The entities are common, everyday objects, and the knowledge necessary to address this problem is of general use across many domains. We present a system that leverages user interaction with 3D scenes to generate training data for semantic parsing approaches. Previous semantic parsing work has dealt with grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and with connecting language to spatial relationships (Golland et al., 2010; Artzi and Zettlemoyer, 2013). Semantic parsing methods can also be applied to many aspects of text to scene generation. Furthermore, work on parsing instructions to robots (Matuszek et al., 2013; Tellex et al., 2014) has analogues in the context of discourse about physical scenes. In this extended abstract, we formalize the text to scene generation problem and describe it as a task for semantic parsing methods. To motivate this problem, we present a prototype system that incorporates simple spatial knowledge, and par"
W14-2404,D10-1040,0,0.0681444,"Missing"
W14-2404,Q13-1016,0,0.0304976,"implicit relations that are likely to hold even if they are not explicitly stated by the input text. Text to scene generation offers a rich, interactive environment for grounded language that is familiar to everyone. The entities are common, everyday objects, and the knowledge necessary to address this problem is of general use across many domains. We present a system that leverages user interaction with 3D scenes to generate training data for semantic parsing approaches. Previous semantic parsing work has dealt with grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and with connecting language to spatial relationships (Golland et al., 2010; Artzi and Zettlemoyer, 2013). Semantic parsing methods can also be applied to many aspects of text to scene generation. Furthermore, work on parsing instructions to robots (Matuszek et al., 2013; Tellex et al., 2014) has analogues in the context of discourse about physical scenes. In this extended abstract, we formalize the text to scene generation problem and describe it as a task for semantic parsing methods. To motivate this problem, we present a"
W14-2404,H89-1033,0,0.162744,"Missing"
W14-2404,Q13-1005,0,\N,Missing
W14-3102,C12-1042,0,0.0876032,"spatial relations. Our system then uses this knowledge to generate scenes from natural text inferring implicit constraints. It then leverages user interaction to allow refinement of the scene, and improve the spatial knowledge base. We demonstrate that user interaction is critical in expanding and improving spatial knowledge learned from data. 2 tion of 3D models and on hand crafted rules to map text to object placement decisions, which makes them hard to extend and generalize. More recent work has used crowdsourcing platforms, such as Amazon Mechanical Turk, to collect necessary annotations (Coyne et al., 2012). However, this data collection is treated as a separate pre-process and the user still has no influence on the system’s knowledge base. We address one part of this issue: learning simple spatial knowledge from data and interactively updating it through user feedback. We also infer unstated implicit constraints thus allowing for more natural text input. 2.2 Automatic Scene Layout Prior work on scene layout has focused largely on room interiors and determining good furniture layouts by optimizing energy functions that capture the quality of a proposed layout. These energy functions are encoded"
W14-3102,H89-1033,0,0.442883,"in critically important spatial knowledge. For example, people do not usually mention the presence of the ground or that most objects are supported by it. As a consequence, spatial knowledge is severely lacking in current computational systems. Pioneering work in mapping text to 3D scene representations has taken two approaches to address these challenges. First, by restricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU system demonstrated parsing of natural language input for manipulating the scene, and learning of procedural knowledge through interaction (Winograd, 1972). However, generalization to scenes with more complex objects and spatial relations is very hard to attain. More recently, the WordsEye system has focused on the general text to 3D scene generation task (Coyne and Sproat, 2001), allowing a user to generate a 3D scene directly from a textual description of the objects present, their properties and their spatial arrangement. The authors of WordsEye demonstrated the promise of text to scene generation systems but also pointed out some fundamental issues which restrict the success of their system: a lot of spatial knowledge is required which is ha"
W14-3102,D10-1040,0,\N,Missing
W14-3102,W07-1427,1,\N,Missing
W14-3102,P10-1083,0,\N,Missing
W14-3102,P14-5010,1,\N,Missing
W14-3102,D13-1128,0,\N,Missing
W14-3102,D13-1197,0,\N,Missing
W14-3102,Q13-1016,0,\N,Missing
W14-3102,Q13-1005,0,\N,Missing
W14-3102,P13-2014,0,\N,Missing
W14-3102,E12-1076,0,\N,Missing
W14-3106,P13-4027,0,0.150583,"nd efficient way. SPIED-Learn uses TokensRegex (Chang and Manning, 2014) to create and apply surface word patterns to text. SPIED-Viz takes details of learned entities and patterns as input in a JSON format. It uses Javascript, angular, and jquery to visualize the information in a web browser. 5 Related Work Most interactive IE systems focus on annotation of text, labeling of entities, and manual writing of rules. Some annotation and labeling tools are: MITRE’s Callisto3 , Knowtator4 , SAPIENT (Liakata et al., 2009), brat5 , Melita (Ciravegna et al., 2002), and XConc Suite (Kim et al., 2008). Akbik et al. (2013) interactively helps non-expert users to manually write patterns over dependency trees. GATE6 provides the JAPE language that recognizes regular expressions over annotations. Other systems focus on reducing manual effort for developing extractors (Brauer et al., 2011; Li et al., 2011). In contrast, our tool focuses on visualizing and comparing diagnostic information associated with pattern learning systems. WizIE (Li et al., 2012) is an integrated environment for annotating text and writing pattern extractors for information extraction. It also generates regular expressions around labeled ment"
W14-3106,D13-1079,0,0.0967025,"m. However, it involves time consuming manual inspection of the extracted output. We present a light-weight tool, SPIED, to aid IE system developers in learning entities using patterns with bootstrapping, and visualizing the learned entities and patterns with explanations. SPIED is the first publicly available tool to visualize diagnostic information of multiple pattern learning systems to the best of our knowledge. 1 Introduction Entity extraction using rules dominates commercial industry, mainly because rules are effective, interpretable by humans, and easy to customize to cope with errors (Chiticariu et al., 2013). Rules, which can be hand crafted or learned by a system, are commonly created by looking at the context around already known entities, such as surface word patterns (Hearst, 1992) and dependency patterns (Yangarber et al., 2000). Building a patternbased learning system is usually a repetitive process, usually performed by the system developer, 1 A shorter context size usually extracts entities with higher recall but lower precision. 38 Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 38–44, c Baltimore, Maryland, USA, June 27, 2014. 2014 Asso"
W14-3106,C02-1154,0,0.0602689,"ord.edu/ software/patternslearning.shtml. The visualization code is available at http://nlp.stanford.edu/software/ patternviz.shtml. 2 4. Entity learning: Learned patterns for the class are applied to the text to extract candidate entities. An entity scorer ranks the candidate entities and adds the top entities to DT’s dictionary. The maximum number of entities learned is given as an input to the system by the developer. 5. Repeat steps 1-4 for a given number of iterations. SPIED provides an option to use any of the pattern scoring measures described in (Riloff, 1996; Thelen and Riloff, 2002; Yangarber et al., 2002; Lin et al., 2003; Gupta and Manning, 2014b). A pattern is scored based on the positive, negative, and unlabeled entities it extracts. The positive and negative labels of entities are heuristically determined by the system using the dictionaries and the iterative entity learning process. The oracle labels of learned entities are not available to the learning system. Note that an entity that the system considered positive might actually be incorrect, since the seed dictionaries can be noisy and the system can learn incorrect entities in the previous iterations, and vice-versa. SPIED’s entity s"
W14-3106,W14-1611,1,0.897526,"The maximum number of patterns learned is given as an input to the system by the developer. Viz. SPIED-Viz has pattern-centric and entitycentric views, which visualize learned patterns and entities, respectively, and the explanations for learning them. SPIED-Viz can also contrast two systems by comparing the ranks of learned entities and patterns. In this paper, as a concrete example, we learn and visualize drug-treatment (DT) entities from unlabeled patient-generated medical text, starting with seed dictionaries of entities for multiple classes. The task was proposed and further developed in Gupta and Manning (2014b) and Gupta and Manning (2014a). Our contributions in this paper are: 1. we present a novel diagnostic tool for visualization of output of multiple pattern-based entity learning systems, and 2. we release the code of an end-to-end pattern learning system, which learns entities using patterns in a bootstrapped system and visualizes its diagnostic output. The pattern learning code is available at http://nlp.stanford.edu/ software/patternslearning.shtml. The visualization code is available at http://nlp.stanford.edu/software/ patternviz.shtml. 2 4. Entity learning: Learned patterns for the class"
W14-3106,C92-2082,0,0.041524,"otstrapping, and visualizing the learned entities and patterns with explanations. SPIED is the first publicly available tool to visualize diagnostic information of multiple pattern learning systems to the best of our knowledge. 1 Introduction Entity extraction using rules dominates commercial industry, mainly because rules are effective, interpretable by humans, and easy to customize to cope with errors (Chiticariu et al., 2013). Rules, which can be hand crafted or learned by a system, are commonly created by looking at the context around already known entities, such as surface word patterns (Hearst, 1992) and dependency patterns (Yangarber et al., 2000). Building a patternbased learning system is usually a repetitive process, usually performed by the system developer, 1 A shorter context size usually extracts entities with higher recall but lower precision. 38 Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 38–44, c Baltimore, Maryland, USA, June 27, 2014. 2014 Association for Computational Linguistics 3. Pattern learning: Candidate patterns are scored using a pattern scoring measure and the top ones are added to the list of learned patterns f"
W14-3106,P12-3019,0,0.176585,"Missing"
W14-3106,W09-1325,0,0.0203684,"rs to diagnose errors and tune parameters in their pattern-based entity learning system in an easy and efficient way. SPIED-Learn uses TokensRegex (Chang and Manning, 2014) to create and apply surface word patterns to text. SPIED-Viz takes details of learned entities and patterns as input in a JSON format. It uses Javascript, angular, and jquery to visualize the information in a web browser. 5 Related Work Most interactive IE systems focus on annotation of text, labeling of entities, and manual writing of rules. Some annotation and labeling tools are: MITRE’s Callisto3 , Knowtator4 , SAPIENT (Liakata et al., 2009), brat5 , Melita (Ciravegna et al., 2002), and XConc Suite (Kim et al., 2008). Akbik et al. (2013) interactively helps non-expert users to manually write patterns over dependency trees. GATE6 provides the JAPE language that recognizes regular expressions over annotations. Other systems focus on reducing manual effort for developing extractors (Brauer et al., 2011; Li et al., 2011). In contrast, our tool focuses on visualizing and comparing diagnostic information associated with pattern learning systems. WizIE (Li et al., 2012) is an integrated environment for annotating text and writing patter"
W14-3106,W02-1028,0,0.258406,"lable at http://nlp.stanford.edu/ software/patternslearning.shtml. The visualization code is available at http://nlp.stanford.edu/software/ patternviz.shtml. 2 4. Entity learning: Learned patterns for the class are applied to the text to extract candidate entities. An entity scorer ranks the candidate entities and adds the top entities to DT’s dictionary. The maximum number of entities learned is given as an input to the system by the developer. 5. Repeat steps 1-4 for a given number of iterations. SPIED provides an option to use any of the pattern scoring measures described in (Riloff, 1996; Thelen and Riloff, 2002; Yangarber et al., 2002; Lin et al., 2003; Gupta and Manning, 2014b). A pattern is scored based on the positive, negative, and unlabeled entities it extracts. The positive and negative labels of entities are heuristically determined by the system using the dictionaries and the iterative entity learning process. The oracle labels of learned entities are not available to the learning system. Note that an entity that the system considered positive might actually be incorrect, since the seed dictionaries can be noisy and the system can learn incorrect entities in the previous iterations, and vice"
W14-3106,C00-2136,0,0.29445,"d entities and patterns with explanations. SPIED is the first publicly available tool to visualize diagnostic information of multiple pattern learning systems to the best of our knowledge. 1 Introduction Entity extraction using rules dominates commercial industry, mainly because rules are effective, interpretable by humans, and easy to customize to cope with errors (Chiticariu et al., 2013). Rules, which can be hand crafted or learned by a system, are commonly created by looking at the context around already known entities, such as surface word patterns (Hearst, 1992) and dependency patterns (Yangarber et al., 2000). Building a patternbased learning system is usually a repetitive process, usually performed by the system developer, 1 A shorter context size usually extracts entities with higher recall but lower precision. 38 Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 38–44, c Baltimore, Maryland, USA, June 27, 2014. 2014 Association for Computational Linguistics 3. Pattern learning: Candidate patterns are scored using a pattern scoring measure and the top ones are added to the list of learned patterns for DT. The maximum number of patterns learned is"
W14-3106,W99-0613,0,\N,Missing
W14-3311,W13-2205,0,0.0953026,"Missing"
W14-3311,P13-2121,0,0.0527971,"Missing"
W14-3311,W11-2123,0,0.0545202,"Missing"
W14-3311,D11-1125,0,0.0352263,"elihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. Invoked by prefixing the LM path with the “kenlm:”. 115 2 For simplicity, we assume one reference, but the multireference case is analogous. Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Wei"
W14-3311,P07-1019,0,0.0616148,"), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithreaded, with one decoding instance per thread. Each decoding instance has its own weight vector, so in the programmatic case, it is possible to decode simultaneously under different weight vectors. Two search procedures are included. The default is the phrase-based variant of cube pruning (Huang and Chiang, 2007). The standard multi-stack beam search (Och and Ney, 2004) is also an option. Either procedure can be configured in one of several recombination modes. The “Pharaoh” mode only considers linear distortion, source coverage, and target LM history. The “Exact” mode considers these states in addition to any feature that declares recombination state (see section 3.3). The decoder includes several options for deployment environments such as an unknown word API, pre-/post-processing APIs, and both full and prefixbased force decoding. 2.5 Evaluation and Post-processing All of the error metrics availabl"
W14-3311,N03-1017,0,0.0105437,"cience Department, Stanford University {spenceg,danielcer,manning}@stanford.edu Abstract We present a new version of Phrasal, an open-source toolkit for statistical phrasebased machine translation. This revision includes features that support emerging research trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive machine translation. A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. T"
W14-3311,P07-2045,0,0.0234988,"emerging research trends such as (a) tuning with large feature sets, (b) tuning on large datasets like the bitext, and (c) web-based interactive machine translation. A direct comparison with Moses shows favorable results in terms of decoding speed and tuning time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems"
W14-3311,W08-0304,1,0.816084,"specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. Invoked by prefixing the LM path with the “kenlm:”. 115 2 For simplicity, we assume one reference, but the multireference case is analogous. Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT (Bottou and Bousquet, 2011). Weight updates are performed after each tuning example is decoded, and"
W14-3311,W04-3250,0,0.0231682,"act” mode considers these states in addition to any feature that declares recombination state (see section 3.3). The decoder includes several options for deployment environments such as an unknown word API, pre-/post-processing APIs, and both full and prefixbased force decoding. 2.5 Evaluation and Post-processing All of the error metrics available for tuning can also be invoked for evaluation. For significance testing, the toolkit includes an implementation of the permutation test of Riezler and Maxwell (2005), which was shown to be less susceptible to Type-I error than bootstrap re-sampling (Koehn, 2004). r : s(r,w) r∈R d : w(d) r : s(r,w) d0 : s(d0 ,w) axiom r∈ / cov(d) |cov(d) |= |s| item goal Table 1: Phrase-based MT as deductive inference. This notation can be read as follows: if the antecedents on the top are true, then the consequent on the bottom is true subject to the conditions on the right. The new item d0 is creating by appending r to the ordered sequence of rules that define d. Phrasal also includes two truecasing packages. The LM-based truecaser (Lita et al., 2003) requires an LM estimated from cased, tokenized text. A subsequent detokenization step is thus necessary. A more conv"
W14-3311,N10-2003,1,0.842411,"ase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems. Second, Phrasal provides multi-threaded decoding and online tuning for learning feature-rich models on very large datasets, including the bitext. Third, Phrasal supplies the key ingredients for web-based, interactive MT: an asynchronous RESTful JSON web service implemented as a J2EE servlet, integrated pre- and post"
W14-3311,N12-1047,0,0.182935,"includes native support for the class-based language models that have become popular in recent evaluations (Wuebker et al., 2012; Ammar et al., 2013; Durrani et al., 2013). 2.2 Tuning Once a language model has been estimated and a phrase table has been extracted, the next step is to estimate model weights. Phrasal supports tuning over n-best lists, which permits rapid experimentation with different error metrics and loss functions. Lattice-based tuning, while in principle more powerful, requires metrics and losses that factor over lattices, and in practice works no better than n-best tuning (Cherry and Foster, 2012). Tuning requires a parallel set {(ft , et )}Tt=1 of source sentences ft and target references et .2 Phrasal follows the log-linear approach to phrasebased translation (Och and Ney, 2004) in which the predictive translation distribution p(e|f ; w) is modeled directly as p(e|f ; w) = Rule Extraction The next step in the pipeline is extraction of a phrase table. Phrasal includes a multi-threaded version of the rule extraction algorithm of Och and Ney (2004). Phrase tables can be filtered to a specific data set—as is common in research environments. When filtering, the rule extractor lowers memor"
W14-3311,W11-2107,0,0.0199706,"a reflection. 1 2.3 h i 1 exp w&gt; φ(e, f ) Z(f ) (1) where w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. Invoked by prefixing the LM path with the"
W14-3311,W13-2212,0,0.0302619,"e. Static rule features are useful in two cases. First, if a feature value depends on bitext statistics, which are not accessible during tuning or decoding, then that feature should be stored in the phrase table. Examples are the standard phrase translation probabilities, and the dense rule count and rule uniqueness indicators described by Green et al. (2013). Second, if a feature depends only on the rule and is unlikely to change, then it may be more efficient to store that feature value in the phrase table. An example is a feature template that indicates inclusion in a specific data domain (Durrani et al., 2013). Rule extractor feature templates must implement the FeatureExtractor interface and are loaded via reflection. 1 2.3 h i 1 exp w&gt; φ(e, f ) Z(f ) (1) where w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BL"
W14-3311,P10-4002,0,0.0153594,"ing time. 1 Introduction In the early part of the last decade, phrase-based machine translation (MT) (Koehn et al., 2003) emerged as the preeminent design of statistical MT systems. However, most systems were proprietary or closedsource, so progress was initially constrained by the high engineering barrier to entry into the field. Then Moses (Koehn et al., 2007) was released. What followed was a flowering of work on all aspects of the translation problem, from rule extraction to deployment issues. Other toolkits appeared including Joshua (Post et al., 2013), Jane (Wuebker et al., 2012), cdec (Dyer et al., 2010) and the first version of our package, Phrasal (Cer et al., 2010), a Java-based, open source package. This paper presents a completely re-designed release of Phrasal that lowers the barrier to entry into several exciting areas of MT research. First, Phrasal exposes a simple yet flexible feature API for building large-scale, feature-rich systems. Second, Phrasal provides multi-threaded decoding and online tuning for learning feature-rich models on very large datasets, including the bitext. Third, Phrasal supplies the key ingredients for web-based, interactive MT: an asynchronous RESTful JSON we"
W14-3311,D08-1089,1,0.768952,"Missing"
W14-3311,N06-1014,0,0.0536844,"p://nlp.stanford.edu/software/phrasal/ 2 Standard System Pipeline This section describes the steps required to build a phrase-based MT system from raw text. Each step is implemented as a stand-alone executable. For convenience, the Phrasal distribution includes a script that coordinates the steps. 2.1 Prerequisites Phrasal assumes offline preparation of word alignments and at least one target-side language model. Word Alignment The rule extractor can accommodate either unsymmetrized or symmetrized alignments. Unsymmetrized alignments can be produced with either GIZA++ or the Berkeley Aligner (Liang et al., 2006). Phrasal then applies symmetrization on-the-fly using heuristics such as grow-diag or grow-diag-final. If the alignments are symmetrized separately, then Phrasal accepts align114 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 114–121, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics ments in the i-j Pharaoh format, which indicates that source token i is aligned to target token j. The rule extractor can also create lexicalized reordering tables. The standard phrase orientation model (Tillmann, 2004) and the hierarchical mo"
W14-3311,C04-1072,0,0.0374434,"and n-best lists are not accumulated. Consequently, online tuning is preferable for large tuning sets, or for rapid iteration during development. Phrasal includes the AdaGrad-based (Duchi et al., 2011) tuner of Green et al. (2013). The regularization options are L2 , efficient L1 for feature selection (Duchi and Singer, 2009), or L1 + L2 (elastic net). There are two online loss functions: a pairwise (PRO) objective and a listwise minimum expected error objective (Och, 2003). These online loss functions require sentencelevel error metrics, several of which are available in the toolkit: BLEU+1 (Lin and Och, 2004), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithreaded, with one decoding instance per thread. Each decoding instance has its own weight vector, so in the programmatic case, it is possible to decode simultaneously under different weight vectors. Two search procedures are included. The default is the phrase-based variant of cube pruning (H"
W14-3311,P03-1020,0,0.0652213,"ion test of Riezler and Maxwell (2005), which was shown to be less susceptible to Type-I error than bootstrap re-sampling (Koehn, 2004). r : s(r,w) r∈R d : w(d) r : s(r,w) d0 : s(d0 ,w) axiom r∈ / cov(d) |cov(d) |= |s| item goal Table 1: Phrase-based MT as deductive inference. This notation can be read as follows: if the antecedents on the top are true, then the consequent on the bottom is true subject to the conditions on the right. The new item d0 is creating by appending r to the ordered sequence of rules that define d. Phrasal also includes two truecasing packages. The LM-based truecaser (Lita et al., 2003) requires an LM estimated from cased, tokenized text. A subsequent detokenization step is thus necessary. A more convenient alternative is the CRF-based postprocessor that can be trained to invert an arbitrary pre-processor. This post-processor can perform truecasing and detokenization in one pass. 3 Feature API Phrasal supports dynamic feature extraction during tuning and decoding. In the API, feature templates are called featurizers. There are two types with associated interfaces: RuleFeaturizer and DerivationFeaturizer. One way to illustrate these two featurizers is to consider phrasebased"
W14-3311,C12-1121,0,0.0776569,"ated. Consequently, online tuning is preferable for large tuning sets, or for rapid iteration during development. Phrasal includes the AdaGrad-based (Duchi et al., 2011) tuner of Green et al. (2013). The regularization options are L2 , efficient L1 for feature selection (Duchi and Singer, 2009), or L1 + L2 (elastic net). There are two online loss functions: a pairwise (PRO) objective and a listwise minimum expected error objective (Och, 2003). These online loss functions require sentencelevel error metrics, several of which are available in the toolkit: BLEU+1 (Lin and Och, 2004), Nakov BLEU (Nakov et al., 2012), and TER. 2.4 Decoding The Phrasal decoder can be invoked either programmatically as a Java object or as a standalone application. In both cases the decoder is configured via options that specify the language model, phrase table, weight vector w, etc. The decoder is multithreaded, with one decoding instance per thread. Each decoding instance has its own weight vector, so in the programmatic case, it is possible to decode simultaneously under different weight vectors. Two search procedures are included. The default is the phrase-based variant of cube pruning (Huang and Chiang, 2007). The stand"
W14-3311,W14-3316,1,0.779167,"core code has fewer dependencies in terms of software and expertise. Second, Phrasal makes extensive use of Java interfaces and reflection. This is especially helpful in the feature API. A feature function can be added to the system by simply implementing an interface and specifying the class name on the decoder command line. There is no need to modify or recompile anything other than the new feature function. This paper presents a direct comparison of Phrasal and Moses that shows favorable results in terms of decoding speed and tuning time. An indirect comparison via the WMT2014 shared task (Neidert et al., 2014) showed that Phrasal compares favorably to Moses in an evaluation setting. The source code is freely available at: http://nlp.stanford.edu/software/phrasal/ 2 Standard System Pipeline This section describes the steps required to build a phrase-based MT system from raw text. Each step is implemented as a stand-alone executable. For convenience, the Phrasal distribution includes a script that coordinates the steps. 2.1 Prerequisites Phrasal assumes offline preparation of word alignments and at least one target-side language model. Word Alignment The rule extractor can accommodate either unsymmet"
W14-3311,J04-4002,0,0.927809,"anguage model has been estimated and a phrase table has been extracted, the next step is to estimate model weights. Phrasal supports tuning over n-best lists, which permits rapid experimentation with different error metrics and loss functions. Lattice-based tuning, while in principle more powerful, requires metrics and losses that factor over lattices, and in practice works no better than n-best tuning (Cherry and Foster, 2012). Tuning requires a parallel set {(ft , et )}Tt=1 of source sentences ft and target references et .2 Phrasal follows the log-linear approach to phrasebased translation (Och and Ney, 2004) in which the predictive translation distribution p(e|f ; w) is modeled directly as p(e|f ; w) = Rule Extraction The next step in the pipeline is extraction of a phrase table. Phrasal includes a multi-threaded version of the rule extraction algorithm of Och and Ney (2004). Phrase tables can be filtered to a specific data set—as is common in research environments. When filtering, the rule extractor lowers memory utilization by splitting the data into arbitrary-sized chunks and extracting rules from each chunk. The rule extractor includes a feature API that is independent of the decoder feature"
W14-3311,P03-1021,0,0.0122406,"n to tune to log-likelihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. Invoked by prefixing the LM path with the “kenlm:”. 115 2 For simplicity, we assume one reference, but the multireference case is analogous. Online tuning is faster and more scalable than batch tuning, and sometimes leads to better solutions for non-convex settings like MT"
W14-3311,P02-1040,0,0.0957625,"le extractor feature templates must implement the FeatureExtractor interface and are loaded via reflection. 1 2.3 h i 1 exp w&gt; φ(e, f ) Z(f ) (1) where w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batc"
W14-3311,P13-1031,1,0.894343,"ization by splitting the data into arbitrary-sized chunks and extracting rules from each chunk. The rule extractor includes a feature API that is independent of the decoder feature API. This allows for storage of static rule feature values in the phrase table. Static rule features are useful in two cases. First, if a feature value depends on bitext statistics, which are not accessible during tuning or decoding, then that feature should be stored in the phrase table. Examples are the standard phrase translation probabilities, and the dense rule count and rule uniqueness indicators described by Green et al. (2013). Second, if a feature depends only on the rule and is unlikely to change, then it may be more efficient to store that feature value in the phrase table. An example is a feature template that indicates inclusion in a specific data domain (Durrani et al., 2013). Rule extractor feature templates must implement the FeatureExtractor interface and are loaded via reflection. 1 2.3 h i 1 exp w&gt; φ(e, f ) Z(f ) (1) where w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is no"
W14-3311,W05-0908,0,0.0676322,"ion modes. The “Pharaoh” mode only considers linear distortion, source coverage, and target LM history. The “Exact” mode considers these states in addition to any feature that declares recombination state (see section 3.3). The decoder includes several options for deployment environments such as an unknown word API, pre-/post-processing APIs, and both full and prefixbased force decoding. 2.5 Evaluation and Post-processing All of the error metrics available for tuning can also be invoked for evaluation. For significance testing, the toolkit includes an implementation of the permutation test of Riezler and Maxwell (2005), which was shown to be less susceptible to Type-I error than bootstrap re-sampling (Koehn, 2004). r : s(r,w) r∈R d : w(d) r : s(r,w) d0 : s(d0 ,w) axiom r∈ / cov(d) |cov(d) |= |s| item goal Table 1: Phrase-based MT as deductive inference. This notation can be read as follows: if the antecedents on the top are true, then the consequent on the bottom is true subject to the conditions on the right. The new item d0 is creating by appending r to the ordered sequence of rules that define d. Phrasal also includes two truecasing packages. The LM-based truecaser (Lita et al., 2003) requires an LM esti"
W14-3311,W14-3360,1,0.824384,"source of inefficiency. Second, we observe that the Java parallel garbage collector (GC) runs up to seven threads, which become increasingly active as the number of decoder threads increases. These and other Java overhead threads must be scheduled, limiting gains as the number of decoding threads approaches the number of physical cores. Finally, Figure 3 shows tuning BLEU as a function of wallclock time. For Moses we chose the batch MIRA implementation of Cherry and Foster (2012), which is popular for tuning feature-rich systems. Phrasal uses the online tuner with the expected BLEU objective (Green et al., 2014). Moses achieves a maximum BLEU score of 47.63 after 143 minutes of tuning, while Phrasal reaches this level after just 17 minutes, later reaching a maximum BLEU of 47.75 after 42 minutes. Much of the speedup can be attributed to phrase table and LM loading time: the Phrasal tuner loads these data structures just once, while the Moses tuner loads them every epoch. Of course, this loading time becomes more significant with larger-scale systems. 6 8 ● Conclusion We presented a revised version of Phrasal, an opensource, phrase-based MT toolkit. The revisions support new directions in MT research"
W14-3311,2006.amta-papers.25,0,0.0277386,"actor interface and are loaded via reflection. 1 2.3 h i 1 exp w&gt; φ(e, f ) Z(f ) (1) where w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. MT differs from other machine learning settings in that it is not common to tune to log-likelihood under (1). Instead, a gold error metric G(e0 , e) is chosen that specifies the similarity between a hypothesis e0 and a reference e, and that error is minimized over the tuning set. Phrasal includes Java implementations of BLEU (Papineni et al., 2002), NIST, and WER, and bindings for TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2011). The error metric is incorporated into a loss function ` that returns the loss at either the sentenceor corpus- level. For conventional corpus-level (batch) tuning, Phrasal includes multi-threaded implementations of MERT (Och, 2003) and PRO (Hopkins and May, 2011). The MERT implementation uses the line search of Cer et al. (2008) to directly minimize corpus-level error. The PRO implementation uses a pairwise logistic loss to minimize the number of inversions in the ranked n-best lists. These batch implementations accumulate n-best lists across epochs. In"
W14-3311,N04-4026,0,0.03882,"Missing"
W14-3311,D08-1076,0,\N,Missing
W14-3311,W13-2226,0,\N,Missing
W14-3316,W11-2131,0,0.0272367,"est2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set. https://code.google.com/p/cld2/ 151 2.5 Post-Processing Our post-processor recases and detokenizes system output. For the English-German system, we combined both tasks by using a Conditional Random Field (CRF) model (Lafferty et al., 2001) to learn transformations between the raw output characters and the post-processed versions. For each test dataset, we trained a separate model on 500,000 sentences selected using the Feature Decay Algorithm for bitext selection (Biçici and Yuret, 2011). Features used include the character type of the current and surrounding characters, the token type of the current and surrounding tokens, and the position of the character within its token. The English output was recased using a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-En parallel data using lmplz. 3 Target-class bigram boundary: An indicator feature for the concatenation of the word class of the rightmost word in the left rule and the word class of the leftmost word in the right rule in each adjacent rule pair in a deriva"
W14-3316,buck-etal-2014-n,1,0.877365,"Missing"
W14-3316,W13-2217,1,0.814481,"corpora of parallel text and monolingual text from which our systems were built. 2.1 Pre-Processing We used Stanford CoreNLP to tokenize the English and German data according to the Penn Treebank standard (Marcus et al., 1993). The French source data was tokenized similarly to the French Treebank 1 These authors contributed equally. http://commoncrawl.org 150 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150–156, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics (Abeillé et al., 2003) using the Stanford French tokenizer (Green et al., 2013b). We also lowercased the data and removed any control characters. Further, we filtered out all lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points and standardized white spaces and newlines. We additionally filtered out sentences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our system"
W14-3316,J13-1009,1,0.889149,"Missing"
W14-3316,W14-3360,1,0.882744,"f the German monolingual corpus. Since the test data was also collected from the web, we cannot be sure that the test sentences were not in the language model. However, substantial portions of the test set are translations from other languages, which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest20082012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set. h"
W14-3316,W14-3311,1,0.831976,"f the German monolingual corpus. Since the test data was also collected from the web, we cannot be sure that the test sentences were not in the language model. However, substantial portions of the test set are translations from other languages, which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest20082012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set. h"
W14-3316,P13-2121,1,0.902738,"Missing"
W14-3316,W11-2123,1,0.748178,"entences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our systems used up to three language models. 2.3.1 Constrained Language Models For En-De, we used lmplz (Heafield et al., 2013) to estimate a 5-gram language model on all WMT German monolingual data and the German side of the parallel Common Crawl corpus. To query the model, we used KenLM (Heafield, 2011). For the Fr-En system, we also estimated a 5-gram language model from all the monolingual English data and the English side of the parallel Common Crawl, UN, Giga-FrEn, CzEng and Yandex corpora using the same procedure as above. Additionally, we estimated a second language model from the English Gigaword corpus. All of these language models used interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 2.3.2 Unconstrained Language Model Our unconstrained En-De submission used an additional language model trained on German web text gathered by the Common Crawl"
W14-3316,P07-2045,0,0.00563235,"e of the translation rule and an indicator feature for the concatenation of the two lengths. Rule orientation features: An indicator feature for each translation rule combined with its orientation class (monotone, swap, or discontinuous). This feature also fires only for rules that occur more than 50 times in the parallel data. Again, class-based translation rules were used to extract additional features. Translation System We built our translation systems using Phrasal. 3.1 Features Our translation model has 19 dense features that were computed for all translation hypotheses: the nine Moses (Koehn et al., 2007) baseline features, the eight hierarchical lexicalized reordering model features by Galley and Manning (2008), the log count of each rule, and an indicator for unique rules. On top of that, the model uses the following additional features of Green et al. (2014a). Rule indicator features: An indicator feature for each translation rule. To combat overfitting, this feature fires only for rules that occur more than 50 times in the parallel data. Additional indicator features were constructed by mapping the words in each rule to their corresponding word classes. Alignments: An indicator feature for"
W14-3316,2005.mtsummit-papers.11,0,0.0580071,"Missing"
W14-3316,N04-1022,0,0.145424,"Missing"
W14-3316,C12-1121,0,0.113259,"The signed linear distortion δ for two rules a and b is δ = r(a)−l(b)+1, where r(x) is the rightmost source index of rule x and l(x) is the leftmost source index of rule x. Each adjacent rule pair in a derivation has an indicator feature for the signed linear distortion of this pair. Tuning We used an online, adaptive tuning algorithm (Green et al., 2013c) to learn the feature weights. The loss function is an online variant of expected BLEU (Green et al., 2014a). As a sentence-level metric, we used the extended BLEU+1 metric that smooths the unigram precision as well as the reference length (Nakov et al., 2012). For feature selection, we used L1 regularization. Each tuning epoch produces a different set of weights; we tried all of them on newstest2013, which was held out from the tuning set, then picked the weights that produced the best uncased BLEU score. 3.3 System Parameters We started off with the parameters of our systems for the WMT 2013 Translation Task (Green et al., 2013a) and optimized the L1 -regularization strength. Both systems used the following tuning parameters: a 200-best list, a learning rate of 0.02 and a mini-batch size of 20. The En-De system 152 Track En-De constrained En-De u"
W14-3316,J04-4002,0,0.384373,"Missing"
W14-3316,P13-1135,0,0.025397,"Missing"
W14-3316,D13-1049,0,0.0417255,"Missing"
W14-3316,N06-1014,0,0.0853397,"re, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics (Abeillé et al., 2003) using the Stanford French tokenizer (Green et al., 2013b). We also lowercased the data and removed any control characters. Further, we filtered out all lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points and standardized white spaces and newlines. We additionally filtered out sentences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our systems used up to three language models. 2.3.1 Constrained Language Models For En-De, we used lmplz (Heafield et al., 2013) to estimate a 5-gram language model on all WMT German monolingual data and the German side of the parallel Common Crawl corpus. To query the model, we used KenLM (Heafield, 2011). For the Fr-En system, we also estimated a 5-gram language model from all the monolingual English data and the English side of the parallel Common Crawl, UN, Gi"
W14-3316,P08-1086,0,0.0164417,"23 times as large as the rest of the German monolingual corpus. Since the test data was also collected from the web, we cannot be sure that the test sentences were not in the language model. However, substantial portions of the test set are translations from other languages, which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest20082012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the ne"
W14-3316,P03-1020,0,0.266933,"ned both tasks by using a Conditional Random Field (CRF) model (Lafferty et al., 2001) to learn transformations between the raw output characters and the post-processed versions. For each test dataset, we trained a separate model on 500,000 sentences selected using the Feature Decay Algorithm for bitext selection (Biçici and Yuret, 2011). Features used include the character type of the current and surrounding characters, the token type of the current and surrounding tokens, and the position of the character within its token. The English output was recased using a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-En parallel data using lmplz. 3 Target-class bigram boundary: An indicator feature for the concatenation of the word class of the rightmost word in the left rule and the word class of the leftmost word in the right rule in each adjacent rule pair in a derivation. Length features: Indicator features for the length of the source side and for the length of the target side of the translation rule and an indicator feature for the concatenation of the two lengths. Rule orientation features: An indicator feature for each translation rule"
W14-3316,J93-2004,0,0.0455718,", CzEng, and parallel CommonCrawl corpora. For parallel CommonCrawl, we concatenated the English halves for various language pairs and then deduplicated at the sentence level. In addition, our unconstrained English-German system used German text extracted from the entire 2012, 2013, and winter 2013 CommonCrawl1 corpora by Buck et al. (2014). Tables 1 and 2 show the sizes of the preprocessed corpora of parallel text and monolingual text from which our systems were built. 2.1 Pre-Processing We used Stanford CoreNLP to tokenize the English and German data according to the Penn Treebank standard (Marcus et al., 1993). The French source data was tokenized similarly to the French Treebank 1 These authors contributed equally. http://commoncrawl.org 150 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150–156, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics (Abeillé et al., 2003) using the Stanford French tokenizer (Green et al., 2013b). We also lowercased the data and removed any control characters. Further, we filtered out all lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points a"
W14-3316,de-marneffe-etal-2006-generating,1,\N,Missing
W14-3316,D08-1089,1,\N,Missing
W14-3316,P13-1031,1,\N,Missing
W14-3360,W13-2205,0,0.314914,"from frequency filtering on the tuning data; see section 6.1). The feature is local. Class-based rule indicator Word classes abstract over lexical items. For each rule r, a prototype that abstracts over many rules can be built by concatenating {ϕ(w) : w ∈ f (r)} with {ϕ(w) : w ∈ e(r)}. For example, suppose that Arabic class 492 consists primarily of Arabic present tense verbs and class 59 contains English auxiliaries. Then the model might penalize a rule prototype like 492&gt;59_59, which drops the verb. This template fires an indicator for each rule prototype and is local. Target unigram class (Ammar et al., 2013) Target lexical items with similar syntactic and semantic properties may have very different frequencies in the training data. These frequencies will influence the dense features. For example, in one of our English class mappings the following words map to the same class: word surface-to-surface air-to-air ground-to-air class 0 0 0 freq. 269 98 63 another LM. This template fires a separate indicator for each class {ϕ(w) : w ∈ e(r)} and is local. 3.2 Word Alignments Word alignment features allow the model to recognize fine-grained phrase-internal information that is largely opaque in the dense"
W14-3360,W11-2131,0,0.037778,"sted defaults: Brown, default; Clark, 10 iterations, frequency cutoff τ = 5; Och, 10 iterations. Our implementation: PredictiveFull, 30 iterations, τ = 0; Predictive, 30 iterations, τ = 5. labels. The in-domain rule sets need not be disjoint since some rules might be useful across domains. This paper explores the following approach: we choose one of the M domains as the default. Next, we collect some source sentences for each of the M − 1 remaining domains. Using these examples we then identify in-domain sentence pairs in the bitext via data selection, in our case the feature decay algorithm (Biçici and Yuret, 2011). Finally, our rule extractor adds domain labels to all rules extracted from each selected sentence pair. Crucially, these labels do not influence which rules are extracted or how they are scored. The resulting phrase table contains the same rules, but with a few additional annotations. Our method assumes domain labels for each source input to be decoded. Our experiments utilize gold, document-level labels, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable an"
W14-3360,W08-0336,1,0.807457,"hm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6 Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7 System settings: distortion limit of 5, cube pruning beam size of 1200, maximum phrase length of 7. 8 Other learning settings: 16 threads, mini-batch size of 20; L1 regularization strength λ = 0.001; learning rate η0 = 0.02; initialization of LM to 0.5, word penalty to"
W14-3360,N12-1047,0,0.245585,"e now algorithms for every taste: probabilistic and distribution-free, online and batch, regularized and unregularized. Technical differences aside, the papers that apply these algorithms to phrase-based translation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the s"
W14-3360,W13-2212,0,0.162749,"s, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable and perhaps most common source of error in phrase-based domain adaptation. Our development experiments seemed to confirm this hypothesis as augmentation of the class-based and non-lexical (e.g., Rule shape) features did not reduce error. Therefore, we only augment the lexicalized features: rule indicators and orientations, and word alignments. 4.2 Domain-Specific Feature Templates In-domain Rule Indicator (Durrani et al., 2013) An indicator for each rule that matches the input domain. This template fires a generic in-domain indicator and a domain-specific indicator (e.g., the features might be indomain and indomain-nw). The feature is local. Adjacent Rule Indicator Indicators for adjacent in-domain rules. This template also fires both generic and domain-specific features. The feature is non-local and the state is a boolean indicating if the last rule in a partial derivation is in-domain. 5 Experiments We evaluate and analyze our feature set under a variety of large-scale experimental conditions including multiple do"
W14-3360,N09-1068,1,0.910726,"Missing"
W14-3360,N13-1048,0,0.187988,"meter or an index indicating a categorical value like an n-gram context. For each language, the extended feature templates require unigram counts and a word-to-class mapping ϕ : w 7→ c for word w ∈ V and class c ∈ C. These can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2 Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not 1 http://nlp.stanford.edu/software/phrasal the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ. @ ⇒ reasons -0.022 H. AJ. @ ⇒ reasons for 0.002 H. AJ. @ ⇒ the reasons for 0.016 These translations are all correct depending on con text. When the plural nou"
W14-3360,N12-1023,0,0.17572,"ve algorithm design for machine translation (MT) has lately been a booming enterprise. There are now algorithms for every taste: probabilistic and distribution-free, online and batch, regularized and unregularized. Technical differences aside, the papers that apply these algorithms to phrase-based translation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that"
W14-3360,W13-2217,1,0.90216,"Missing"
W14-3360,P13-1031,1,0.939443,"re a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the standard Moses dense features (Koehn et al., 2007): ease of implementation, language independence, and independence from ancillary corpora like treebanks. In our experiments, they do not overfit and can be extracted effi"
W14-3360,W14-3311,1,0.825114,"orpora. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). training corpora5 come from several Linguistic Data Consortium (LDC) sources from 2012 and earlier (Table 2). The test, development, and tuning corpora6 come from the NIST OpenMT and MetricsMATR evaluations (Table 3). Extended features benefit from more tuning data, so we concatenated five NIST data sets to build one large tuning set. Observe that all test data come from later epochs than the tuning and development data. From these data we built phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results W"
W14-3360,N13-1003,0,0.534621,"nslation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the standard Moses dense features (Koehn et al., 2007): ease of implementation, language independence, and independence from ancillary corpora like treebanks. In our experiments, they do not overfit an"
W14-3360,N09-1025,0,0.0514297,"ptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features are less effective when they do not influence recombination. Liang et al. (2006a) proposed replacing lexical items with supervised part-of-speech (POS)"
W14-3360,P11-2080,0,0.0123227,"small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM) Gruber et al. (2007). Source topic information was then mixed into the translation model dense feature calculations. This work follows Chiang et al. (2011), who present a similar technique but using the same gold NIST labels that we use. Hasler et al. (2012) extended these ideas to a discriminative sparse feature set by augmenting both rule and unigram alignment features with HTMM topic information. 8 Conclusion This paper makes four major contributions. First, we introduced extended features for phrase-based MT that exceeded both dense and feature-rich baselines. Second, we specialized the features to source domains, further extending the gains. Third, we showed that online expected BLEU is faster and more stable than online PRO for extended fe"
W14-3360,D10-1056,0,0.060428,"Missing"
W14-3360,2012.amta-papers.4,0,0.0472446,"es. Yu et al. (2013) used word classes as backoff features to reduce overfitting. Wuebker et al. (2013) replaced all lexical items in the bitext and monolingual data with classes, and estimated the dense feature set. 11 Space limitations preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classif"
W14-3360,P07-1033,0,0.108877,"Missing"
W14-3360,N12-1017,0,0.0217904,"●● ● ● ● ●●● ●● ● ● ● ●● ● ● ●●● ● ● ● ●● ●●● ● ● ● ●● ● ● ●●●●●● ● ● ● ● ●●● ● ● ● ● ●●●● ●● ● ● ● ● ●● ●● ● ● ● ●● ● ● ● ●● ● ●● ● ● ● ●● ● ●● ●●● ● ● ● ● ●● ● ● ●● ● ● ●●● ● ● ●● ●● ● ● ●● ●● ● ●● ● ●● ● ● ●● ● ●● ● ●● ● ● ●● ●●●● ●●● ●● ● ● ● ● ● ●●● ●●●● ● ● ●●● ●● ● ● ●●● ● ●● ●●● ● ●● ●● ● ●● ● ●● ● ● ● ●● ●● ● ● ● ●● ● ● ● ● ● ● ● ●● ● ● ●● References Few MT data sets supply multiple references. Even when they do, those references are but a sample from a larger pool of possible translations. This observation has motivated attempts at generating lattices of translations for evaluation (Dreyer and Marcu, 2012; Bojar et al., 2013). But evaluation is only part of the problem. Table 6c shows that the Dense model, which has only a few features to describe the data, is little affected by the elimination of references. In contrast, the feature-rich model degrades significantly. This may account for the underperformance of features in single-reference settings like WMT (Durrani et al., 2013; Green et al., 2013a). The next section explores the impact of references further. Reference Variance We took the Dense Ar-En output for the dev data, which has four references, and computed the sentence-level BLEU+1"
W14-3360,2012.iwslt-papers.17,0,0.0218206,"tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM) Gruber et al. (2007). Source topic information was then mixed into the translation model dense feature calculations. This work follows Chiang et al. (2011), who present a similar technique but using the same gold NIST labels that we use. Hasler et al. (2012) extended these ideas to a discriminative sparse feature set by augmenting both rule and unigram alignment features with HTMM topic information. 8 Conclusion This paper makes four major contributions. First, we introduced extended features for phrase-based MT that exceeded both dense and feature-rich baselines. Second, we specialized the features to source domains, further extending the gains. Third, we showed that online expected BLEU is faster and more stable than online PRO for extended features. Finally, we released fast, scalable, languageindependent tools for implementing the feature set"
W14-3360,P13-2121,0,0.147076,"ed five NIST data sets to build one large tuning set. Observe that all test data come from later epochs than the tuning and development data. From these data we built phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and t"
W14-3360,D11-1125,0,0.626613,"ng, and release fast, scalable, and language-independent tools for implementing the features. 1 Introduction Scalable discriminative algorithm design for machine translation (MT) has lately been a booming enterprise. There are now algorithms for every taste: probabilistic and distribution-free, online and batch, regularized and unregularized. Technical differences aside, the papers that apply these algorithms to phrase-based translation often share a curious empirical characteristic: the algorithms support extra features, but the features do not significantly improve translation. For example, Hopkins and May (2011) showed that PRO with some simple ad hoc features only exceeds the baseline on one of three language pairs. Gimpel and Smith (2012b) observed a similar result for both PRO and their ramp-loss algorithm. Cherry and Foster (2012) found that, at least in the batch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this p"
W14-3360,Q13-1035,0,0.0707732,"ta selection, in our case the feature decay algorithm (Biçici and Yuret, 2011). Finally, our rule extractor adds domain labels to all rules extracted from each selected sentence pair. Crucially, these labels do not influence which rules are extracted or how they are scored. The resulting phrase table contains the same rules, but with a few additional annotations. Our method assumes domain labels for each source input to be decoded. Our experiments utilize gold, document-level labels, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable and perhaps most common source of error in phrase-based domain adaptation. Our development experiments seemed to confirm this hypothesis as augmentation of the class-based and non-lexical (e.g., Rule shape) features did not reduce error. Therefore, we only augment the lexicalized features: rule indicators and orientations, and word alignments. 4.2 Domain-Specific Feature Templates In-domain Rule Indicator (Durrani et al., 2013) An indicator for each rule that matches the input domain. This template fires a generic in-domain indicator and"
W14-3360,N03-1017,0,0.0213201,"ther a real-valued parameter or an index indicating a categorical value like an n-gram context. For each language, the extended feature templates require unigram counts and a word-to-class mapping ϕ : w 7→ c for word w ∈ V and class c ∈ C. These can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2 Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not 1 http://nlp.stanford.edu/software/phrasal the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ. @ ⇒ reasons -0.022 H. AJ. @ ⇒ reasons for 0.002 H. AJ. @ ⇒ the reasons for 0.016 These translations are all correct depending on con text"
W14-3360,P07-2045,0,0.0160733,"atch case, many algorithms produce similar results, and features only significantly increased quality for one of three language pairs. Only recently did Cherry (2013) and Green et al. (2013b) identify certain features that consistently reduce error. These empirical results suggest that feature design and model fitting, the subjects of this paper, warrant a closer look. We introduce an effective extended feature set for phrase-based MT and identify a loss function that is less prone to overfitting. Extended features share three attractive characteristics with the standard Moses dense features (Koehn et al., 2007): ease of implementation, language independence, and independence from ancillary corpora like treebanks. In our experiments, they do not overfit and can be extracted efficiently during decoding. Because all feature weights are tuned on the development set, the new feature templates are amenable to feature augmentation (Daumé III, 2007), a simple domain adaptation technique that we show works surprisingly well for MT. Extended features are designed according to a principle rather than a rule: they should fire less than standard dense features, which are general, but more than so-called sparse f"
W14-3360,P06-1096,0,0.491875,"e can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2 Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not 1 http://nlp.stanford.edu/software/phrasal the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ. @ ⇒ reasons -0.022 H. AJ. @ ⇒ reasons for 0.002 H. AJ. @ ⇒ the reasons for 0.016 These translations are all correct depending on con text. When the plural noun H AJ  . . @ ‘reasons’ appears in a construct state (iDafa) the preposition for is unrealized. Moreover, depending on the context, the English translation might also require the determiner the, which is also unrealized."
W14-3360,N06-1014,0,0.490338,"e can be extracted from any monolingual data; our experiments simply use both sides of the unaligned parallel training data. The features are language-independent, but we will use Arabic-English as a running example. 3.1 Lexical Choice Lexical choice features make more specific distinctions between target words than the dense translation model features (Koehn et al., 2003). 2 Gao and He (2013) used stochastic gradient descent and expected BLEU to learn phrase table feature weights, but not 1 http://nlp.stanford.edu/software/phrasal the full translation model w. 467 Lexicalized rule indicator (Liang et al., 2006a) Some rules occur frequently enough that we can learn rule-specific weights that augment the dense translation model features. For example, our model learns the following rule indicator features and weights: H. AJ. @ ⇒ reasons -0.022 H. AJ. @ ⇒ reasons for 0.002 H. AJ. @ ⇒ the reasons for 0.016 These translations are all correct depending on con text. When the plural noun H AJ  . . @ ‘reasons’ appears in a construct state (iDafa) the preposition for is unrealized. Moreover, depending on the context, the English translation might also require the determiner the, which is also unrealized."
W14-3360,W05-0908,0,0.354367,"Missing"
W14-3360,C04-1072,0,0.106017,"online variant of PRO tends to produce short translations like its batch counterpart (Nakov et al., 2013). Moreover, PRO requires sampling, making it slow to compute. To address these shortcomings, we explore an online variant of expected error (Och, 2003, Eq.7). Let Et = {ei }ni=1 be a scored n-best list of translations at time step t for source input ft . Let G(e) be a gold error metric that evaluates each candidate translation with respect to a set of one or more (3) To our knowledge, we are the first to experiment with the online version of this loss.2 When G(e) is sentence-level BLEU+1 (Lin and Och, 2004)—the setting in our experiments—this loss is also known as expected BLEU (Cherry and Foster, 2012). However, other metrics are possible. 3 Extended Phrase-based Features We divide our feature templates into five categories, which are well-known sources of error in phrasebased translation. The features are defined over derivations d = {ri }D i=1 , which are ordered sequences of rules r from the translation model. Define functions f (·) to be the source string of a rule or derivation and e(·) to be the target string. Local features can be extracted from individual rules and do not declare any st"
W14-3360,D12-1037,0,0.0333627,"ns preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM"
W14-3360,maamouri-etal-2008-enhancing,0,0.0189658,"lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6 Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7 System settings: distortion limit of 5, cube pruning beam size of 1200, maximum phrase length of 7. 8 Other learning settings: 16 threads, mini-batch size of 20; L1 regularization strength λ = 0.0"
W14-3360,J93-2004,0,0.04582,"lingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6 Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7 System settings: distortion limit of 5, cube pruning bea"
W14-3360,P14-2034,1,0.81695,"For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6 Data sources: tune, MT023568; dev, MT04; dev-dom, domain adaptation dev set is MT04 and all wb and bn data from LDC2007E61; test1, MT09 (Ar-En) and MT12 (Zh-En); test2, Progress0809 which was revealed in the OpenMT 2012 evaluation; test3, MetricsMATR08-10. 7 System settings: distortion limit of 5, cube pruning beam size of 1200, maximum phrase length of 7. 8 Other learning settings"
W14-3360,C12-1121,0,0.261662,"vity penalty, the maximum lowerbounds the multiple-reference score since BLEU aggregates n-grams across references. The multiplereference score is an “easier” target since the model has more opportunities to match n-grams. Consider again the single-reference condition and one of the pathological cases at the top of Figure 1a. Suppose that the low-scoring reference is observed in the single-reference condition. The more expressive feature-rich model has a greater capacity to fit that reference when, under another reference, it would have matched the translation exactly and incurred a low loss. Nakov et al. (2012) suggested extensions to BLEU+1 that were subsequently found to improve accuracy in the single-reference condition (Gimpel and Smith, 2012a). Repeating the min/max calculations with the most effective extensions (according to Gimpel and Smith (2012a)) we observe lower variance (M = 17.32, SD = 10.68). These extensions are very simple, so a more sophisticated noise model is a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended feature"
W14-3360,P12-1048,0,0.0140448,"Wang et al. (2012) augmented the baseline dense feature set with domain labels. They each showed modest improvements for several language pairs. However, neither incorporated a notion of a default prior domain. Liu et al. (2012) investigated local adaption of the log-linear scores by selecting comparable bitext examples for a given source input. After selecting a small local corpus, their algorithm then performs several online update steps—starting from a globally tuned weight vector—prior to decoding the input. The resulting model is effectively a locally weighted, domain-adapted classifier. Su et al. (2012) proposed domain adaptation via monolingual source resources much as we use in-domain monolingual corpora for data selection. They labeled each bitext sentence with a topic using a Hidden Topic Markov Model (HTMM) Gruber et al. (2007). Source topic information was then mixed into the translation model dense feature calculations. This work follows Chiang et al. (2011), who present a similar technique but using the same gold NIST labels that we use. Hasler et al. (2012) extended these ideas to a discriminative sparse feature set by augmenting both rule and unigram alignment features with HTMM to"
W14-3360,P08-1086,0,0.259778,"Missing"
W14-3360,2012.amta-papers.18,0,0.194544,"hen identify in-domain sentence pairs in the bitext via data selection, in our case the feature decay algorithm (Biçici and Yuret, 2011). Finally, our rule extractor adds domain labels to all rules extracted from each selected sentence pair. Crucially, these labels do not influence which rules are extracted or how they are scored. The resulting phrase table contains the same rules, but with a few additional annotations. Our method assumes domain labels for each source input to be decoded. Our experiments utilize gold, document-level labels, but accurate sentencelevel domain classifiers exist (Wang et al., 2012). 4.1 Augmentation of Extended Features Irvine et al. (2013) showed that lexical selection is the most quantifiable and perhaps most common source of error in phrase-based domain adaptation. Our development experiments seemed to confirm this hypothesis as augmentation of the class-based and non-lexical (e.g., Rule shape) features did not reduce error. Therefore, we only augment the lexicalized features: rule indicators and orientations, and word alignments. 4.2 Domain-Specific Feature Templates In-domain Rule Indicator (Durrani et al., 2013) An indicator for each rule that matches the input do"
W14-3360,D07-1080,0,0.0741595,"s a promising future direction. 7 Related Work We review work on phrase-based discriminative feature sets that influence decoder search, and domain adaptation with features.11 7.1 Feature Sets Variants of some extended features are scattered throughout previous work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features ar"
W14-3360,P13-2003,0,0.0431401,"learn w, we follow the online procedure of Green et al. (2013b), who calculate gradient steps with AdaGrad (Duchi et al., 2011) and perform feature selection via L1 regularization in the FOBOS (Duchi and Singer, 2009) framework. This procedure accommodates any loss function for which a subgradient can be computed. Green et al. (2013b) used a PRO objective (Hopkins and May, 2011) with a logistic (surrogate) loss function. However, later results showed overfitting (Green et al., 2013a), and we found that their online variant of PRO tends to produce short translations like its batch counterpart (Nakov et al., 2013). Moreover, PRO requires sampling, making it slow to compute. To address these shortcomings, we explore an online variant of expected error (Och, 2003, Eq.7). Let Et = {ei }ni=1 be a scored n-best list of translations at time step t for source input ft . Let G(e) be a gold error metric that evaluates each candidate translation with respect to a set of one or more (3) To our knowledge, we are the first to experiment with the online version of this loss.2 When G(e) is sentence-level BLEU+1 (Lin and Och, 2004)—the setting in our experiments—this loss is also known as expected BLEU (Cherry and Fos"
W14-3360,J03-1002,0,0.00552048,"y if alignment 1 is a common alignment error. Lexicalized alignment features allow the model to compensate for these events. This feature fires an indicator for each alignment in a rule—including multiword cliques—and is local. Class-based alignments Like the class-based rule indicator, this feature template replaces each lexical item with its word class, resulting in an alignment prototype. This feature fires an indicator for each alignment in a rule after mapping lexical items to classes. It is local. Source class deletion Phrase extraction algorithms often use a “grow” symmetrization step (Och and Ney, 2003) to add alignment points. Sometimes this procedure can produce a rule that deletes important source content words. This feature template allows the model to penalize these rules by firing an indicator for the class of each unaligned source word. The feature is local. Punctuation ratio Languages use different types and ratios of punctuation (Salton, 1958). For example, quotation marks are not commonly used in Arabic, but they are conventional in English. Furthermore, spurious alignments often contain punctuation. To control these two phenomena, this feature template returns the ratio of target"
W14-3360,D13-1138,0,0.112687,"conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features are less effective when they do not influence recombination. Liang et al. (2006a) proposed replacing lexical items with supervised part-of-speech (POS) tags to reduce sparsity. This is a natural idea that lay dormant until recently. Ammar et al. (2013) incorporated unigram and bigram target class features. Yu et al. (2013) used word classes as backoff features to reduce overfitting. Wuebker et al. (2013) replaced all lexical items in the bitext and monolingual data with classes, and estimated the dense feature set. 11 Space limitations preclude discussion of re-ranking features. Then they added these dense class-based features to the baseline lexicalized system. Finally, Cherry (2013) experimented with class-based hierarchical reordering features. However, his features used a bespoke representation rather than the simple full rule string that we use. 7.2 Domain Adaptation with Features Both Clark et al. (2012) and Wang et al. (2012) augmented the baseline dense feature set with domain labels."
W14-3360,J04-4002,0,0.109594,"show that an online variant of expected error (Och, 2003) is significantly faster to compute, less prone to overfitting, and nearly as effective as a pairwise loss. We release all software—feature extractors, and fast word clustering and data selection packages—used in our experiments.1 2 references. The smooth loss function is `t (wt−1 ) = Ep(e|ft ;wt−1 ) [G(e)]   1 X = exp w&gt; φ(e0 , f ) · G(e0 ) Z 0 e ∈Et with P normalization  constant Z = exp w&gt; φ(e0 , f ) . The gradient gt for coordinate j is: Phrase-based Models and Learning e0 ∈Et The log-linear approach to phrase-based translation (Och and Ney, 2004) directly models the predictive translation distribution p(e|f ; w) = 1 Z(f ) h i exp w&gt; φ(e, f ) (2) gt = E[G(e)φj (e, ft )]− E[G(e)]E[φj (e, ft )] (1) where e is the target string, f is the source string, w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. Assume that there is also a function ρ(e, f ) ∈ Rd that produces a recombination map for the features. That is, each coordinate in ρ represents the state of the corresponding coordinate in φ. For example, suppose that φj is the log probability produced by the n-gram langua"
W14-3360,P03-1021,0,0.230152,"ecessary and even detrimental when features follow this principle. We report large-scale translation quality experiments relative to both dense and feature-rich baselines. Our best feature set, which includes domain adaptation features, yields an average +1.05 BLEU improvement for Arabic-English and +0.67 for 466 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 466–476, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Chinese-English. In addition to the extended feature set, we show that an online variant of expected error (Och, 2003) is significantly faster to compute, less prone to overfitting, and nearly as effective as a pairwise loss. We release all software—feature extractors, and fast word clustering and data selection packages—used in our experiments.1 2 references. The smooth loss function is `t (wt−1 ) = Ep(e|ft ;wt−1 ) [G(e)]   1 X = exp w&gt; φ(e0 , f ) · G(e0 ) Z 0 e ∈Et with P normalization  constant Z = exp w&gt; φ(e0 , f ) . The gradient gt for coordinate j is: Phrase-based Models and Learning e0 ∈Et The log-linear approach to phrase-based translation (Och and Ney, 2004) directly models the predictive translat"
W14-3360,P02-1040,0,0.09414,"phrase-based MT systems with Phrasal (Green et al., 2014).7 We aligned the parallel corpora with the Berkeley aligner (Liang et al., 2006b) with standard settings and symmetrized via the grow-diag heuristic. We created separate English LMs for each language pair by concatenating the monolingual Gigaword data with the target-side of the respective bitexts. For each corpus we estimated unfiltered 5-gram language models with lmplz (Heafield et al., 2013). For each condition we ran the learning algorithm for 25 epochs8 and selected the model according to the maximum uncased, corpus-level BLEU-4 (Papineni et al., 2002) score on the dev set. 5.1 Results We evaluate the new feature set relative to two baselines. Dense is the same baseline as Green et al. 5 tune dev dev-dom test1 test2 test3 990M We tokenized the English with Stanford CoreNLP according to the Penn Treebank standard (Marcus et al., 1993), the Arabic with the Stanford Arabic segmenter (Monroe et al., 2014) according to the Penn Arabic Treebank standard (Maamouri et al., 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al., 2008) according to the Penn Chinese Treebank standard (Xue et al., 2005). 6 Data sources: tune, MT023568"
W14-3360,D13-1112,0,0.0509147,"work: unfiltered lexicalized rule indicators and alignments (Liang et al., 2006a); rule shape (Hopkins and May, 2011); rule orientation (Liang et al., 2006b; Cherry, 2013); target unigram class (Ammar et al., 2013). We found that other prior features did not improve translation: higher-order target lexical n-grams (Liang et al., 2006a; Watanabe et al., 2007; Gimpel and Smith, 2012b), higher-order target class n-grams (Ammar et al., 2013), target word insertion (Watanabe et al., 2007; Chiang et al., 2009), and many other unpublished ideas transmitted through received wisdom. To our knowledge, Yu et al. (2013) were the first to experiment with non-local (derivation) features for phrase-based MT. They added discriminative rule features conditioned on target context. This is a good idea that we plan to explore. However, they do not mention if their non-local features declare recombination state. Our empirical experience is that non-local features are less effective when they do not influence recombination. Liang et al. (2006a) proposed replacing lexical items with supervised part-of-speech (POS) tags to reduce sparsity. This is a natural idea that lay dormant until recently. Ammar et al. (2013) incor"
W14-3360,D08-1076,0,\N,Missing
W15-1512,D14-1082,1,0.0959613,"ll in both classification directions in the CLDC task in which past work did not achieve. 1 Introduction Distributed representations of words, also known as word embeddings, are critical components of many neural network based NLP systems. Such representations overcome the sparsity of natural languages by representing words with high-dimensional vectors in a continuous space. These vectors encode semantic information of words, leading to success in a wide range of tasks, such as sequence tagging, sentiment analysis, and parsing (Collobert et al., 2011; Maas et al., 2011; Socher et al., 2013a; Chen and Manning, 2014). As a natural extension, being able to learn representations for larger language structures such as phrases or sentences, has also been of interest to the community lately, for instance (Socher et al., 2013b; Le and Mikolov, 2014). In the multilingual context, most of the recent work in bilingual representation learning such as (Klementiev et al., 2012; Mikolov et al., 2013b; Zou et al., 2013; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2014) only focus on learning embeddings for words and use simple functions, e.g., idf-weighted sum, to synthesize representations for lar"
W15-1512,P12-1092,1,0.659578,"Association for Computational Linguistics grouped into two categories: (a) pseudo-supervised methods which make use of properties in the unannotated training data as supervised signals and (b) task-specific approaches that utilizes annotated data to learn a prediction task. For the former, word embeddings are often part of neural language models that learn to predict next words given contexts by either minimizing the cross-entropy (Bengio et al., 2003; Morin, 2005; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2011) or maximizing the ranking margins (Collobert and Weston, 2008; Huang et al., 2012; Luong et al., 2013). Representatives for the latter include (Collobert and Weston, 2008; Maas et al., 2011; Socher et al., 2013a) which finetune embeddings for various tasks such as sequence labelling, sentiment analysis, and constituent parsing. Larger structure representations – Learning distributed representation for phrases and sentences is harder because one needs to learn both the compositional and non-compositional meanings beyond words. A method that learns distributed representations of sentences, which is closely related to our approach, is the paragraph vector by Le and Mikolov (2"
W15-1512,C12-1089,0,0.659774,"in a continuous space. These vectors encode semantic information of words, leading to success in a wide range of tasks, such as sequence tagging, sentiment analysis, and parsing (Collobert et al., 2011; Maas et al., 2011; Socher et al., 2013a; Chen and Manning, 2014). As a natural extension, being able to learn representations for larger language structures such as phrases or sentences, has also been of interest to the community lately, for instance (Socher et al., 2013b; Le and Mikolov, 2014). In the multilingual context, most of the recent work in bilingual representation learning such as (Klementiev et al., 2012; Mikolov et al., 2013b; Zou et al., 2013; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2014) only focus on learning embeddings for words and use simple functions, e.g., idf-weighted sum, to synthesize representations for larger text sequences from their word members. In contrast, our work aims to learn representations for phrases and sentences as a whole so as to represent non-compositional meanings. In essence, we extend the paragraph vector approach proposed by Le and Mikolov (2014) to the bilingual context to efficiently encode meaningequivalent multi-word sequences in"
W15-1512,2005.mtsummit-papers.11,0,0.0770362,"ur sentence vectors would converge. Moreover, at train time, the model has been trained to minimize the prediction errors of pairs of sentences that share the same sentence vector, its parameters have adapted to this manner. Hence, at test time, although two sentence vectors are learned independently, one can expect that they converge to close points in the shared semantic space. 4 Experiments 4.1 Training data and procedures We attempt to learn the distributed representation for arbitrary sequences of words in English and German. We train our model using the Europarl v7 multilingual corpora (Koehn, 2005), in particular the English-German corpus. The corpus consists of multilingual parliament documents automatically aligned into 1.8M equivalent pairs of sentences. We preprocess the corpus by filtering out the tokens that appear less than 5 times and desegment the German compound words. This leads to the final set of 43K English words and 95K German words. Parameters of our model are updated using a gradient-based method. While for each pair of sentences, the prediction and N -grams and parameter updates are performed in sequence, our training implementation uses the multithreading approach to"
W15-1512,P14-2037,0,0.0759364,"Missing"
W15-1512,W13-3512,1,0.601959,"Missing"
W15-1512,P11-1015,0,0.317614,"as a whole, our model performs equally well in both classification directions in the CLDC task in which past work did not achieve. 1 Introduction Distributed representations of words, also known as word embeddings, are critical components of many neural network based NLP systems. Such representations overcome the sparsity of natural languages by representing words with high-dimensional vectors in a continuous space. These vectors encode semantic information of words, leading to success in a wide range of tasks, such as sequence tagging, sentiment analysis, and parsing (Collobert et al., 2011; Maas et al., 2011; Socher et al., 2013a; Chen and Manning, 2014). As a natural extension, being able to learn representations for larger language structures such as phrases or sentences, has also been of interest to the community lately, for instance (Socher et al., 2013b; Le and Mikolov, 2014). In the multilingual context, most of the recent work in bilingual representation learning such as (Klementiev et al., 2012; Mikolov et al., 2013b; Zou et al., 2013; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2014) only focus on learning embeddings for words and use simple functions, e.g., idf-weig"
W15-1512,P13-1045,1,0.883965,"el performs equally well in both classification directions in the CLDC task in which past work did not achieve. 1 Introduction Distributed representations of words, also known as word embeddings, are critical components of many neural network based NLP systems. Such representations overcome the sparsity of natural languages by representing words with high-dimensional vectors in a continuous space. These vectors encode semantic information of words, leading to success in a wide range of tasks, such as sequence tagging, sentiment analysis, and parsing (Collobert et al., 2011; Maas et al., 2011; Socher et al., 2013a; Chen and Manning, 2014). As a natural extension, being able to learn representations for larger language structures such as phrases or sentences, has also been of interest to the community lately, for instance (Socher et al., 2013b; Le and Mikolov, 2014). In the multilingual context, most of the recent work in bilingual representation learning such as (Klementiev et al., 2012; Mikolov et al., 2013b; Zou et al., 2013; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2014) only focus on learning embeddings for words and use simple functions, e.g., idf-weighted sum, to synthesi"
W15-1512,D13-1170,1,0.0122622,"el performs equally well in both classification directions in the CLDC task in which past work did not achieve. 1 Introduction Distributed representations of words, also known as word embeddings, are critical components of many neural network based NLP systems. Such representations overcome the sparsity of natural languages by representing words with high-dimensional vectors in a continuous space. These vectors encode semantic information of words, leading to success in a wide range of tasks, such as sequence tagging, sentiment analysis, and parsing (Collobert et al., 2011; Maas et al., 2011; Socher et al., 2013a; Chen and Manning, 2014). As a natural extension, being able to learn representations for larger language structures such as phrases or sentences, has also been of interest to the community lately, for instance (Socher et al., 2013b; Le and Mikolov, 2014). In the multilingual context, most of the recent work in bilingual representation learning such as (Klementiev et al., 2012; Mikolov et al., 2013b; Zou et al., 2013; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2014) only focus on learning embeddings for words and use simple functions, e.g., idf-weighted sum, to synthesi"
W15-1512,D13-1141,1,0.651448,"antic information of words, leading to success in a wide range of tasks, such as sequence tagging, sentiment analysis, and parsing (Collobert et al., 2011; Maas et al., 2011; Socher et al., 2013a; Chen and Manning, 2014). As a natural extension, being able to learn representations for larger language structures such as phrases or sentences, has also been of interest to the community lately, for instance (Socher et al., 2013b; Le and Mikolov, 2014). In the multilingual context, most of the recent work in bilingual representation learning such as (Klementiev et al., 2012; Mikolov et al., 2013b; Zou et al., 2013; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Gouws et al., 2014) only focus on learning embeddings for words and use simple functions, e.g., idf-weighted sum, to synthesize representations for larger text sequences from their word members. In contrast, our work aims to learn representations for phrases and sentences as a whole so as to represent non-compositional meanings. In essence, we extend the paragraph vector approach proposed by Le and Mikolov (2014) to the bilingual context to efficiently encode meaningequivalent multi-word sequences in the same semantic space. Our method only"
W15-1512,P14-1006,0,\N,Missing
W15-1521,J92-4003,0,0.134272,"y and monolingually. 7 Related Work We have previously discussed in Section 2 models directly related to our work. In this section, we survey other approaches in learning monolingual and bilingual representations. Current work in dimensionality reduction of word representations can be broadly grouped into 157 three categories (Turian et al., 2010): (a) distributional representations learned from a cooccurrence matrix of words and contexts (documents, neighbor words, etc.) using techniques such as LSA (Dumais et al., 1988) or LDA (Blei et al., 2003), (b) clustering-based representations, e.g., Brown et al. (1992)’s hierarchical clustering algorithm which represents each word as a binary path through the cluster hierarchy, and (c) distributed representations, where each word is explicitly modeled by a dense real-valued vector and directly induced by predicting words from contexts or vice versa as detailed in Section 2.1. Moving beyond monolingual representations, work in constructing bilingual vector-space models divides into two main streams: (a) those that make use of comparable corpora and (b) those that only require unaligned or monolingual text. The former includes various extensions to standard t"
W15-1521,J93-2003,0,0.0912015,"Missing"
W15-1521,D14-1082,1,0.0598101,"he-art accuracy of 80.3 for the German to English CLDC task and a highly competitive performance of 90.7 for the other classification direction. At the same time, our models outperform best embeddings from past bilingual representation work by a large margin in the monolingual word similarity evaluation.1 1 Introduction Distributed word representations have been key to the recent success of many neural network models in tackling various NLP tasks such as tagging, chunking (Collobert et al., 2011), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), and parsing (Socher et al., 2013a; Chen and Manning, 2014). So far, most of the focus has been spent on monolingual problems despite the existence of a wide variety of multilingual NLP tasks, which include not only machine translation (Brown et 1 All our code, data, and embeddings are publicly available at http://stanford.edu/˜lmthang/bivec. al., 1993), but also noun bracketing (Yarowsky and Ngai, 2001), entity clustering (Green et al., 2012), and bilingual NER (Wang et al., 2013). These multilingual applications have motivated recent work in training bilingual representations where similar-meaning words in two languages are embedded close together i"
W15-1521,E14-1049,0,0.668495,"nternehmen is truncated into telekommun*. Alignment effects – It is interesting to observe that the 40- and 80-dimensional MonoAlign models with a simple monotonic alignment assumption can rival the UnsupAlign models, which uses unsupervised alignments, in many metrics. Overall, all our models are superior to the DWA approach (Koˇcisk´y et al., 2014) which learns distributed alignments and embeddings simultaneously. Word similarity results – It is worthwhile to point out that this work does not aim to be best in terms of the word similarity metrics. Past work such as (Pennington et al., 2014; Faruqui and Dyer, 2014) among many others, have demonstrated that higher word similarity scores can be achieved by simply increasing the vocabulary coverage, training corpus size, and the embedding dimension. Rather, we show that our model can learn bilingual embeddings that are naturally better than those of existing approaches monolingually. 6 Analysis Beside the previous quantitative evaluation, we examine our learned embeddings qualitatively in this section through the following methods: (a) nearest not yield consistent improvements across metrics, so we exclude them for clarity. 156 neighbor words and (b) embed"
W15-1521,N12-1007,1,0.727648,"f many neural network models in tackling various NLP tasks such as tagging, chunking (Collobert et al., 2011), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), and parsing (Socher et al., 2013a; Chen and Manning, 2014). So far, most of the focus has been spent on monolingual problems despite the existence of a wide variety of multilingual NLP tasks, which include not only machine translation (Brown et 1 All our code, data, and embeddings are publicly available at http://stanford.edu/˜lmthang/bivec. al., 1993), but also noun bracketing (Yarowsky and Ngai, 2001), entity clustering (Green et al., 2012), and bilingual NER (Wang et al., 2013). These multilingual applications have motivated recent work in training bilingual representations where similar-meaning words in two languages are embedded close together in the same highdimensional space. However, most bilingual representation work tend to focus on learning embeddings that are tailored towards achieving good performance on a bilingual task, often the crosslingual document classification (CLDC) task, but to the detriment of preserving clustering structures of word representations monolingually. In this work, we demonstrate that such a go"
W15-1521,P08-1088,0,0.00701991,"rd techniques such as bilingual latent semantic models (LSA) (Tam and Schultz, 2007; Ruiz and Federico, 2011) or bilingual/multilingual topic models (LDA) (Zhao and Xing, 2007; Ni et al., 2009; Mimno et al., 2009; Vulic et al., 2011). In this work, the general assumption is that aligned documents share identical topic distributions. The latter stream, which eschews the use of comparable data, generally requires a small initial lexicon which is extracted either manually or automatically (e.g., cognates, string edit distances, etc.). Representatives of this strand include work that extends CCA (Haghighi et al., 2008; Boyd-Graber and Blei, 2009), mapping representations of words in different languages into the same space, as well as work that follows a bootstrapping style to iteratively enlarge the initial lexicon (Peirsman and Pad´o, 2010; Vuli´c and Moens, 2013). 8 Conclusion This work proposes a novel approach that jointly learns bilingual representations from scratch by utilizing both the context concurrence information in the monolingual data and the meaningequivalent signals in the parallel data. We advocate a new standard in training bilingual embeddings, that is, to be good in not only gluing repr"
W15-1521,P12-1092,1,0.584719,"onal Linguistics ral probabilistic language models and (b) marginbased ranking models. The former specify either exactly or approximately distributions over all words w in the vocabulary given a context h, and representatives of that approach include (Bengio et al., 2003; Morin, 2005; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2011). The later eschew the goal of training a language model and try to assign high scores for probable words w given contexts h and low scores for unlikely words w ˜ for the same contexts. Work in the later trend includes (Collobert and Weston, 2008; Huang et al., 2012; Luong et al., 2013). Recently, Mikolov et al. (2013a) introduced the skipgram (SG) approach for learning solely word embeddings by reversing the prediction process, that is, to use the current word to infer its surrounding context, as opposed to using preceding contexts to predict subsequent words in traditional language model approaches. SG models greatly simplify the standard neural network-based architecture to only contain a linear projection input layer and an output softmax layer, i.e., there is no non-linear hidden layer. Despite its simplicity, SG models can achieve very good perform"
W15-1521,C12-1089,0,0.778721,"eddings of semantically similar words across languages are close together. In this scheme, the recent work by Zou et al. (2013) considers the unsupervised alignment information derived over a parallel corpus to enforce such a bilingual constraint. Bilingual Training, unlike the previous schemes which fix pretrained representations on either one or both sides, attempts to jointly learn representations from scratch. To us, this is an interesting problem to attest if we can simultaneously learn good vectors for both languages. Despite there has been an active body of work in this scheme such as (Klementiev et al., 2012; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Chandar A P et al., 2014; Gouws et al., 2014), none of these work has carefully examined the quality of their learned bilingual embeddings using monolingual metrics. In fact, we show later in our experiments that while the existing bilingual representations are great for their cross-lingual tasks, they perform poorly monolingually. 3 Our Approach We hypothesize that by allowing the joint model to utilize both the cooccurrence context information within a language and the meaning-equivalent signals across languages, we can obtain better word"
W15-1521,2005.mtsummit-papers.11,0,0.0173639,"uages. For the former, if a word is unaligned but at least one of its immediate neighbors is aligned, we will use either the only neighbor alignment or an average of the two neighbor alignments. For the latter, each source word at position i is aligned to the target word at position [i ∗ T /S] where S and T are the source and target sentence lengths. These two variants are meant to attest how important unsupervised alignment information is in learning bilingual embeddings. 4 Experiments 4.1 Data We train our joint models on the parallel Europarl v7 corpus between German (de) and English (en) (Koehn, 2005), which consists of 1.9M parallel sentences (49.7M English tokens and 52.0M German tokens). After lowercasing and tokenizing we map each digit into 0, i.e. 2013 becomes 0000. Other rare words occurring less than 5 times are mapped to <unk>. The resulting vocabularies are of size 40K for English and 95K for German. 4.2 Training We use the following settings as described in (Mikolov et al., 2013c): stochastic gradient descent with a default learning rate of 0.025, negative sampling with 30 samples, skipgram with context window of size 5, and a subsampling rate4 of value 1e-4. All models are trai"
W15-1521,P14-2037,0,0.222628,"Missing"
W15-1521,N06-1014,0,0.00858623,"vice versa. That has the effect of training a single skipgram model with a joint vocabulary on parallel corpora in which we enrich the training examples with pairs of words coming from both sides instead of just from one language. Alternatively, one can also think of this BiSkip model as training four skipgram models jointly which predict words between the following pairs of languages: l1 → l1 , l2 → l2 , l1 → l2 , and l2 → l1 . In our work, we experiment with two variants of our models: (a) BiSkip-UnsupAlign where we utilize unsupervised alignment information learned by the Berkeley aligner (Liang et al., 2006) and (b) BiSkip-MonoAlign where we simply assume monotonic alignments between words across languages. For the former, if a word is unaligned but at least one of its immediate neighbors is aligned, we will use either the only neighbor alignment or an average of the two neighbor alignments. For the latter, each source word at position i is aligned to the target word at position [i ∗ T /S] where S and T are the source and target sentence lengths. These two variants are meant to attest how important unsupervised alignment information is in learning bilingual embeddings. 4 Experiments 4.1 Data We t"
W15-1521,W13-3512,1,0.34303,"probabilistic language models and (b) marginbased ranking models. The former specify either exactly or approximately distributions over all words w in the vocabulary given a context h, and representatives of that approach include (Bengio et al., 2003; Morin, 2005; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2011). The later eschew the goal of training a language model and try to assign high scores for probable words w given contexts h and low scores for unlikely words w ˜ for the same contexts. Work in the later trend includes (Collobert and Weston, 2008; Huang et al., 2012; Luong et al., 2013). Recently, Mikolov et al. (2013a) introduced the skipgram (SG) approach for learning solely word embeddings by reversing the prediction process, that is, to use the current word to infer its surrounding context, as opposed to using preceding contexts to predict subsequent words in traditional language model approaches. SG models greatly simplify the standard neural network-based architecture to only contain a linear projection input layer and an output softmax layer, i.e., there is no non-linear hidden layer. Despite its simplicity, SG models can achieve very good performances on various sema"
W15-1521,P11-1015,0,0.012151,"representations efficiently. Our learned embeddings achieve a new state-of-the-art accuracy of 80.3 for the German to English CLDC task and a highly competitive performance of 90.7 for the other classification direction. At the same time, our models outperform best embeddings from past bilingual representation work by a large margin in the monolingual word similarity evaluation.1 1 Introduction Distributed word representations have been key to the recent success of many neural network models in tackling various NLP tasks such as tagging, chunking (Collobert et al., 2011), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), and parsing (Socher et al., 2013a; Chen and Manning, 2014). So far, most of the focus has been spent on monolingual problems despite the existence of a wide variety of multilingual NLP tasks, which include not only machine translation (Brown et 1 All our code, data, and embeddings are publicly available at http://stanford.edu/˜lmthang/bivec. al., 1993), but also noun bracketing (Yarowsky and Ngai, 2001), entity clustering (Green et al., 2012), and bilingual NER (Wang et al., 2013). These multilingual applications have motivated recent work in training bilingual represe"
W15-1521,N10-1135,0,0.112632,"Missing"
W15-1521,D14-1162,1,0.121939,"word telekommunikationsunternehmen is truncated into telekommun*. Alignment effects – It is interesting to observe that the 40- and 80-dimensional MonoAlign models with a simple monotonic alignment assumption can rival the UnsupAlign models, which uses unsupervised alignments, in many metrics. Overall, all our models are superior to the DWA approach (Koˇcisk´y et al., 2014) which learns distributed alignments and embeddings simultaneously. Word similarity results – It is worthwhile to point out that this work does not aim to be best in terms of the word similarity metrics. Past work such as (Pennington et al., 2014; Faruqui and Dyer, 2014) among many others, have demonstrated that higher word similarity scores can be achieved by simply increasing the vocabulary coverage, training corpus size, and the embedding dimension. Rather, we show that our model can learn bilingual embeddings that are naturally better than those of existing approaches monolingually. 6 Analysis Beside the previous quantitative evaluation, we examine our learned embeddings qualitatively in this section through the following methods: (a) nearest not yield consistent improvements across metrics, so we exclude them for clarity. 156 nei"
W15-1521,W11-2133,0,0.0124283,"through the cluster hierarchy, and (c) distributed representations, where each word is explicitly modeled by a dense real-valued vector and directly induced by predicting words from contexts or vice versa as detailed in Section 2.1. Moving beyond monolingual representations, work in constructing bilingual vector-space models divides into two main streams: (a) those that make use of comparable corpora and (b) those that only require unaligned or monolingual text. The former includes various extensions to standard techniques such as bilingual latent semantic models (LSA) (Tam and Schultz, 2007; Ruiz and Federico, 2011) or bilingual/multilingual topic models (LDA) (Zhao and Xing, 2007; Ni et al., 2009; Mimno et al., 2009; Vulic et al., 2011). In this work, the general assumption is that aligned documents share identical topic distributions. The latter stream, which eschews the use of comparable data, generally requires a small initial lexicon which is extracted either manually or automatically (e.g., cognates, string edit distances, etc.). Representatives of this strand include work that extends CCA (Haghighi et al., 2008; Boyd-Graber and Blei, 2009), mapping representations of words in different languages i"
W15-1521,P13-1045,1,0.414422,"ficiently. Our learned embeddings achieve a new state-of-the-art accuracy of 80.3 for the German to English CLDC task and a highly competitive performance of 90.7 for the other classification direction. At the same time, our models outperform best embeddings from past bilingual representation work by a large margin in the monolingual word similarity evaluation.1 1 Introduction Distributed word representations have been key to the recent success of many neural network models in tackling various NLP tasks such as tagging, chunking (Collobert et al., 2011), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), and parsing (Socher et al., 2013a; Chen and Manning, 2014). So far, most of the focus has been spent on monolingual problems despite the existence of a wide variety of multilingual NLP tasks, which include not only machine translation (Brown et 1 All our code, data, and embeddings are publicly available at http://stanford.edu/˜lmthang/bivec. al., 1993), but also noun bracketing (Yarowsky and Ngai, 2001), entity clustering (Green et al., 2012), and bilingual NER (Wang et al., 2013). These multilingual applications have motivated recent work in training bilingual representations where simila"
W15-1521,D13-1170,1,0.00671648,"ficiently. Our learned embeddings achieve a new state-of-the-art accuracy of 80.3 for the German to English CLDC task and a highly competitive performance of 90.7 for the other classification direction. At the same time, our models outperform best embeddings from past bilingual representation work by a large margin in the monolingual word similarity evaluation.1 1 Introduction Distributed word representations have been key to the recent success of many neural network models in tackling various NLP tasks such as tagging, chunking (Collobert et al., 2011), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), and parsing (Socher et al., 2013a; Chen and Manning, 2014). So far, most of the focus has been spent on monolingual problems despite the existence of a wide variety of multilingual NLP tasks, which include not only machine translation (Brown et 1 All our code, data, and embeddings are publicly available at http://stanford.edu/˜lmthang/bivec. al., 1993), but also noun bracketing (Yarowsky and Ngai, 2001), entity clustering (Green et al., 2012), and bilingual NER (Wang et al., 2013). These multilingual applications have motivated recent work in training bilingual representations where simila"
W15-1521,P07-1066,0,0.0604641,"word as a binary path through the cluster hierarchy, and (c) distributed representations, where each word is explicitly modeled by a dense real-valued vector and directly induced by predicting words from contexts or vice versa as detailed in Section 2.1. Moving beyond monolingual representations, work in constructing bilingual vector-space models divides into two main streams: (a) those that make use of comparable corpora and (b) those that only require unaligned or monolingual text. The former includes various extensions to standard techniques such as bilingual latent semantic models (LSA) (Tam and Schultz, 2007; Ruiz and Federico, 2011) or bilingual/multilingual topic models (LDA) (Zhao and Xing, 2007; Ni et al., 2009; Mimno et al., 2009; Vulic et al., 2011). In this work, the general assumption is that aligned documents share identical topic distributions. The latter stream, which eschews the use of comparable data, generally requires a small initial lexicon which is extracted either manually or automatically (e.g., cognates, string edit distances, etc.). Representatives of this strand include work that extends CCA (Haghighi et al., 2008; Boyd-Graber and Blei, 2009), mapping representations of word"
W15-1521,P10-1040,0,0.0450258,".g., “managers” and “managern”. Monolingually, German compound words, such as “welthandelsorganisation” and “investitionsbank”, also appears next to each other. These observations further demonstrate the ability of our models to learn representations well both bilingually and monolingually. 7 Related Work We have previously discussed in Section 2 models directly related to our work. In this section, we survey other approaches in learning monolingual and bilingual representations. Current work in dimensionality reduction of word representations can be broadly grouped into 157 three categories (Turian et al., 2010): (a) distributional representations learned from a cooccurrence matrix of words and contexts (documents, neighbor words, etc.) using techniques such as LSA (Dumais et al., 1988) or LDA (Blei et al., 2003), (b) clustering-based representations, e.g., Brown et al. (1992)’s hierarchical clustering algorithm which represents each word as a binary path through the cluster hierarchy, and (c) distributed representations, where each word is explicitly modeled by a dense real-valued vector and directly induced by predicting words from contexts or vice versa as detailed in Section 2.1. Moving beyond mo"
W15-1521,D13-1168,0,0.0275105,"Missing"
W15-1521,P11-2084,0,0.00983087,"Missing"
W15-1521,P13-1106,1,0.813331,"Missing"
W15-1521,N01-1026,0,0.0143646,"tations have been key to the recent success of many neural network models in tackling various NLP tasks such as tagging, chunking (Collobert et al., 2011), sentiment analysis (Maas et al., 2011; Socher et al., 2013b), and parsing (Socher et al., 2013a; Chen and Manning, 2014). So far, most of the focus has been spent on monolingual problems despite the existence of a wide variety of multilingual NLP tasks, which include not only machine translation (Brown et 1 All our code, data, and embeddings are publicly available at http://stanford.edu/˜lmthang/bivec. al., 1993), but also noun bracketing (Yarowsky and Ngai, 2001), entity clustering (Green et al., 2012), and bilingual NER (Wang et al., 2013). These multilingual applications have motivated recent work in training bilingual representations where similar-meaning words in two languages are embedded close together in the same highdimensional space. However, most bilingual representation work tend to focus on learning embeddings that are tailored towards achieving good performance on a bilingual task, often the crosslingual document classification (CLDC) task, but to the detriment of preserving clustering structures of word representations monolingually. In"
W15-1521,D09-1092,0,0.00804927,"a dense real-valued vector and directly induced by predicting words from contexts or vice versa as detailed in Section 2.1. Moving beyond monolingual representations, work in constructing bilingual vector-space models divides into two main streams: (a) those that make use of comparable corpora and (b) those that only require unaligned or monolingual text. The former includes various extensions to standard techniques such as bilingual latent semantic models (LSA) (Tam and Schultz, 2007; Ruiz and Federico, 2011) or bilingual/multilingual topic models (LDA) (Zhao and Xing, 2007; Ni et al., 2009; Mimno et al., 2009; Vulic et al., 2011). In this work, the general assumption is that aligned documents share identical topic distributions. The latter stream, which eschews the use of comparable data, generally requires a small initial lexicon which is extracted either manually or automatically (e.g., cognates, string edit distances, etc.). Representatives of this strand include work that extends CCA (Haghighi et al., 2008; Boyd-Graber and Blei, 2009), mapping representations of words in different languages into the same space, as well as work that follows a bootstrapping style to iteratively enlarge the initi"
W15-1521,D13-1141,1,0.862692,"includes the recent work by Mikolov et al. (2013b) which utilizes a set of meaning-equivalent pairs (translation pairs) obtained from Google Translate to learn the needed linear mapping. Monolingual Adaptation, on the other hand, assumes access to learned representations of a source language. The idea is to bootstrap learning of target representations from well trained embeddings of a source language, usually a resourcerich one like English, with a bilingual constraint to make sure embeddings of semantically similar words across languages are close together. In this scheme, the recent work by Zou et al. (2013) considers the unsupervised alignment information derived over a parallel corpus to enforce such a bilingual constraint. Bilingual Training, unlike the previous schemes which fix pretrained representations on either one or both sides, attempts to jointly learn representations from scratch. To us, this is an interesting problem to attest if we can simultaneously learn good vectors for both languages. Despite there has been an active body of work in this scheme such as (Klementiev et al., 2012; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014; Chandar A P et al., 2014; Gouws et al., 2014), none"
W15-1521,P14-1006,0,\N,Missing
W15-2134,de-marneffe-etal-2014-universal,1,0.908494,"Missing"
W15-2134,W06-2932,0,0.0238086,"Missing"
W15-2134,P06-1033,0,0.627706,"ons favor performance, inverting the transformations to obtain UD for the final product propagates errors, in part due to the nature of lexical-head representations. This prevents the transformations from being profitably used to improve parser performance in that representation. each word depend on the previous one – there certainly exists a variety of choice points in which more than one type of design is defensible. In the dependency tradition, semantic and syntactic criteria have been recognized to motivate headedness, and there are well-known examples of conflicts between those criteria (Nilsson et al., 2006). Here we investigate four syntactic constructions that are loci of such conflicts: verb groups, prepositional phrases, copular clauses and subordinate clauses. The baseline representation is Universal Dependencies (Nivre et al., 2015), a multilingual dependency scheme that strongly prefers lexical heads. For each target construction, structural transformations are defined that demote the lexical head and make it dependent on a functional head. We show experimental results for the performance of MaltParser, a data-driven transitionbased parser, on the product of each transformation. While some"
W15-2134,E12-2012,0,0.0245786,"ft attached to the lexical head. So now for each Xhead transformation, we have Xheads , Xheadf and Xheadp . To provide a comparison with copheads , which was shown in 4a, copheadf and copheadp are illustrated in 11a and 11b, respectively. An important concern with this type of experiment is that the default feature sets for the algorithms may be implicitly biased towards a particular type of representation. Therefore, it was crucial to explore different hyperparameters and feature sets. This was done in two steps. The MaltParser model was obtained via an optimization heuristic: MaltOptimizer (Ballesteros and Nivre, 2012) was used on the different versions of the training set to obtain models optimized for the different transformation. This generates 13 models: one for the baseline, and one for each of the three versions of the four transformations. In a second step, all 13 representations of the dev set data were parsed with all the 13 models that MaltOptimizer produced in the previous step. Note that MaltOptimizer did not use the dev set. The model that performed best on the dev set for each transformation was chosen. Interestingly, it came out that the best-performing model for a representation was never th"
W15-2134,P07-1122,0,0.314956,"Parser, a data-driven transitionbased parser, on the product of each transformation. While some transformed representations are in fact easier to learn, error propagation when inverting the transformations to obtain UD prevents them from being profitably used to improve parser performance in that representation. Introduction 2 There is a considerable amount of research suggesting that the choice of syntactic representation can have an impact on parsing performance, in constituency (Klein and Manning, 2003; Bikel, 2004; Petrov et al., 2006; Bengoetxea and Gojenola, 2009) as well as dependency (Nilsson et al., 2007; Nilsson et al., 2006; Schwartz et al., 2012) parsing. Recently, this has led designers of dependency representations (Marneffe et al., 2014) to suggest the use of an alternative parsing representation to support the performance of statistical learners. While it is clear that, at the limit, trivializing a linguistic representation in order to make it easier to parse is undesirable – for example, by making Related work Schwartz et al. (2012) is a systematic study of how representation choices in dependency annotation schemes affect their learnability for parsing. The choice points investigated"
W15-2134,R09-1007,0,0.0258928,"show experimental results for the performance of MaltParser, a data-driven transitionbased parser, on the product of each transformation. While some transformed representations are in fact easier to learn, error propagation when inverting the transformations to obtain UD prevents them from being profitably used to improve parser performance in that representation. Introduction 2 There is a considerable amount of research suggesting that the choice of syntactic representation can have an impact on parsing performance, in constituency (Klein and Manning, 2003; Bikel, 2004; Petrov et al., 2006; Bengoetxea and Gojenola, 2009) as well as dependency (Nilsson et al., 2007; Nilsson et al., 2006; Schwartz et al., 2012) parsing. Recently, this has led designers of dependency representations (Marneffe et al., 2014) to suggest the use of an alternative parsing representation to support the performance of statistical learners. While it is clear that, at the limit, trivializing a linguistic representation in order to make it easier to parse is undesirable – for example, by making Related work Schwartz et al. (2012) is a systematic study of how representation choices in dependency annotation schemes affect their learnability"
W15-2134,J04-4004,0,0.0616657,"dependent on a functional head. We show experimental results for the performance of MaltParser, a data-driven transitionbased parser, on the product of each transformation. While some transformed representations are in fact easier to learn, error propagation when inverting the transformations to obtain UD prevents them from being profitably used to improve parser performance in that representation. Introduction 2 There is a considerable amount of research suggesting that the choice of syntactic representation can have an impact on parsing performance, in constituency (Klein and Manning, 2003; Bikel, 2004; Petrov et al., 2006; Bengoetxea and Gojenola, 2009) as well as dependency (Nilsson et al., 2007; Nilsson et al., 2006; Schwartz et al., 2012) parsing. Recently, this has led designers of dependency representations (Marneffe et al., 2014) to suggest the use of an alternative parsing representation to support the performance of statistical learners. While it is clear that, at the limit, trivializing a linguistic representation in order to make it easier to parse is undesirable – for example, by making Related work Schwartz et al. (2012) is a systematic study of how representation choices in de"
W15-2134,W07-2416,0,0.110564,"Missing"
W15-2134,P06-1055,0,0.0183531,"a functional head. We show experimental results for the performance of MaltParser, a data-driven transitionbased parser, on the product of each transformation. While some transformed representations are in fact easier to learn, error propagation when inverting the transformations to obtain UD prevents them from being profitably used to improve parser performance in that representation. Introduction 2 There is a considerable amount of research suggesting that the choice of syntactic representation can have an impact on parsing performance, in constituency (Klein and Manning, 2003; Bikel, 2004; Petrov et al., 2006; Bengoetxea and Gojenola, 2009) as well as dependency (Nilsson et al., 2007; Nilsson et al., 2006; Schwartz et al., 2012) parsing. Recently, this has led designers of dependency representations (Marneffe et al., 2014) to suggest the use of an alternative parsing representation to support the performance of statistical learners. While it is clear that, at the limit, trivializing a linguistic representation in order to make it easier to parse is undesirable – for example, by making Related work Schwartz et al. (2012) is a systematic study of how representation choices in dependency annotation s"
W15-2134,P03-1054,1,0.036519,"lexical head and make it dependent on a functional head. We show experimental results for the performance of MaltParser, a data-driven transitionbased parser, on the product of each transformation. While some transformed representations are in fact easier to learn, error propagation when inverting the transformations to obtain UD prevents them from being profitably used to improve parser performance in that representation. Introduction 2 There is a considerable amount of research suggesting that the choice of syntactic representation can have an impact on parsing performance, in constituency (Klein and Manning, 2003; Bikel, 2004; Petrov et al., 2006; Bengoetxea and Gojenola, 2009) as well as dependency (Nilsson et al., 2007; Nilsson et al., 2006; Schwartz et al., 2012) parsing. Recently, this has led designers of dependency representations (Marneffe et al., 2014) to suggest the use of an alternative parsing representation to support the performance of statistical learners. While it is clear that, at the limit, trivializing a linguistic representation in order to make it easier to parse is undesirable – for example, by making Related work Schwartz et al. (2012) is a systematic study of how representation"
W15-2134,C12-1147,0,0.34502,"Missing"
W15-2134,W08-1301,1,0.706822,"ds a conjunct in (1), a noun in (3), and a preposition in (5) in all the parsers. Furthermore, a bias towards the modal heads in (6) and towards the head-initial representation in (4) is seen with some parsers. No significant results are found for (2). In Ivanova et al. (2013), the authors run a set of experiments that provide a comparison of (1) 3 dependency schemes, (2) 3 data-driven dependency parsers and (3) 2 approaches to POS-tagging in a parsing pipeline. The comparison that is relevant here is (1). The dependency representations compared are the basic version of Stanford Dependencies (Marneffe and Manning, 2008), and two versions of the CoNLL Syntactic Dependencies (Johansson and Nugues, 2007). For all parsers and in most experiments (which explore several pipelines with different POS-tagging strategies), SD is easier to label (i.e., label accuracy scores are higher) and CoNLL is easier to structure (i.e., unlabeled attachment scores are higher). In terms of LAS, MaltParser (Nivre et al., 2007) performs best of the 3 parsers with SD, and MSTParser (McDonald et al., 2006) performs best with CoNLL. 3 3.1 Background Universal Dependencies The baseline representation to which transformations are applied"
W15-2134,silveira-etal-2014-gold,1,0.897145,"Missing"
W15-2134,P13-3005,0,\N,Missing
W15-2812,W13-2322,0,0.0242724,"0 based on these ranks3 . We also compute the median rank of the first correct result. We compare these numbers against an oracle system which uses the human-constructed scene graphs as queries instead of the scene graphs generated by the parser. One drawback of evaluating on a downstream task is that evaluation is typically slower compared to using an intrinsic metric. We therefore also compare the parsed scene graphs to the humanconstructed scene graphs. As scene graphs consist of object instances, attributes, and relations and are therefore similar to Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs, we use Smatch F1 (Cai and Knight, 2013) as an additional intrinsic metric. Experiments For our experiments, we split the data into training, development and held-out test sets of size 3,614, 454, and 456 images, respectively. Table 2 shows the aggregated statistics of our training and test sets. We compare our two parsers against the following two baselines. 3 As in Johnson et al. (2015), we observed that the results for recall at 1 were very unstable so we only report recall at 5 and 10 which are typically also more relevant for real-world systems that return multiple results. 75 Dev"
W15-2812,P13-2131,0,0.0128714,"an rank of the first correct result. We compare these numbers against an oracle system which uses the human-constructed scene graphs as queries instead of the scene graphs generated by the parser. One drawback of evaluating on a downstream task is that evaluation is typically slower compared to using an intrinsic metric. We therefore also compare the parsed scene graphs to the humanconstructed scene graphs. As scene graphs consist of object instances, attributes, and relations and are therefore similar to Abstract Meaning Representation (AMR) (Banarescu et al., 2013) graphs, we use Smatch F1 (Cai and Knight, 2013) as an additional intrinsic metric. Experiments For our experiments, we split the data into training, development and held-out test sets of size 3,614, 454, and 456 images, respectively. Table 2 shows the aggregated statistics of our training and test sets. We compare our two parsers against the following two baselines. 3 As in Johnson et al. (2015), we observed that the results for recall at 1 were very unstable so we only report recall at 5 and 10 which are typically also more relevant for real-world systems that return multiple results. 75 Development set Nearest neighbor Object only Rule C"
W15-2812,J82-2003,0,0.561529,"Missing"
W15-2812,P15-2019,0,0.0621348,"trieve images that contain a boy, a t-shirt and a plane but they are unable to interpret the relationships and attributes of these objects which is crucial for retrieving the correct images. As shown in Figure 1, a possible but incorrect combination of these objects is that a boy is wearing a t-shirt and playing with a toy plane. Introduction One proposed solution to these issues is the mapping of image descriptions to multi-modal embeddings of sentences and images and using these embeddings to retrieve images (Plummer et al., 2015; Karpathy et al., 2014; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015). However, one problem of these models is that they are trained on single-sentence captions which are typically unable to capture the rich content of visual scenes in their entirety. Further, the coverage of the description highly depends on the subjectivity of human perception (Rui et al., 1999). Certain details such as whether there is a plane on the boy’s shirt or not might seem irrelevant to the perOne of the big remaining challenges in image retrieval is to be able to search for very specific images. The continuously growing number of images that are available on the web gives users acces"
W15-2812,de-marneffe-etal-2014-universal,1,0.760954,"Missing"
W15-2812,C14-1012,0,0.0464694,"Missing"
W15-2812,P14-1134,0,0.0108811,"LT) program through IBM. The second author is also supported by a Magic Grant from The Brown Institute for Media Innovation. Any opinions, findings, and conclusions or recommendations expressed are those of the author(s) and do not necessarily reflect the view of either DARPA or the US government. Parsing to graph-based representations Representing semantic information with graphs has recently experienced a resurgence caused by the development of the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015). Considering that AMR graphs are, like dependency trees, very similar to scene graphs, we could have also used this representation and transformed it to scene graphs. However, the performance of AMR parsers is still not competitive with the performance of dependency parsers which makes dependency trees are more stable starting point. There also exists some prior work on parsing scene descriptions to semantic representations. As mentioned above, Chang et al. (2014) present a rule-based system to parse natural language descriptions to scene templates, a"
W15-2812,J93-2004,0,0.0535809,"onsider the collective reading and extract only one piano object. A perfect model thus requires a lot of world knowledge. In practice, however, the distributive reading seems to be far more common so we only consider this case. To make the dependency graph more similar to scene graphs, we copy individual nodes of the graph according to the value of their numeric modifier. We limit the number of copies per node to 20 3.1.2 Pronoun resolution Some image descriptions such as “a bed with a pillow on it” contain personal pronouns. To re1 We augment the parser’s training data with the Brown corpus (Marcus et al., 1993) to improve its performance on image descriptions which are often very different from sentences found in newswire corpora. 73 as our data only contains scene graphs with less than 20 objects of the same class. In case a plural noun lacks such a modifier we make exactly one copy of the node. 3.3.1 Object and Attribute Extraction We use the semantic graph to extract all object and attribute candidates. In a first step we extract all nouns, all adjectives and all intransitive verbs from the semantic graph. As this does not guarantee that the extracted objects and attributes belong to known object"
W15-2812,P02-1040,0,0.119817,"al., 2014b) datasets. However, unlike previous work, we split the process into two separate passes with the goal of increasing the number of objects and relations per image. In the first pass, AMT workers were shown an image and asked to write a one sentence description of the entire image or any part of it. To get diverse descriptions, workers were shown the previous descriptions written by other workers for the same image and were asked to describe something about the image which had not been described by anyone else. We ensured diversity in sentence descriptions by a real-time BLEU score (Papineni et al., 2002) threshold between a new sentence and all the previous ones. In the second pass, workers were presented again with an image and with one of its sentences. They were asked to draw bounding boxes around all the objects in the image which were mentioned in the sentence and to describe their attributes and the relations between them. This step was repeated for each sentence of an image and finally the partial scene graphs are combined to one large scene graph for each image. While the main purpose of the two-pass data collection was to increase the number of objects and relations per image, it als"
W15-2812,D14-1162,1,0.131561,"In a first step we extract all nouns, all adjectives and all intransitive verbs from the semantic graph. As this does not guarantee that the extracted objects and attributes belong to known object classes or attribute types and as our image retrieval model can only make use of known classes and types, we predict for each noun the most likely object class and for each adjective and intransitive verb the most likely attribute type. To predict classes and types, we use an L2 -regularized maximum entropy classifier which uses the original word, the lemma and the 100dimensional GloVe word vector (Pennington et al., 2014) as features. Figure 2 shows the original dependency tree and the final semantic graph for the sentence “Both of the men are riding their horses”. 3.2 Rule-Based Parser Our rule-based parser extracts objects, relations and attributes directly from the semantic graph. We define in total nine dependency patterns using Semgrex2 expressions. These patterns capture the following constructions and phenomena: • Adjectival modifiers 3.3.2 Relation Prediction The last step of the parsing pipeline is to determine the attributes of each object and the relations between objects. We consider both of these"
W15-2812,P03-1054,1,0.0804458,"Missing"
W15-2812,J13-4004,1,0.350643,"Missing"
W15-2812,Q14-1017,1,0.67034,"as natural language or our scene graph representation can. 8 Conclusion We presented two parsers which can translate image descriptions to scene graphs. We showed that their output is almost as effective for retrieving images as human-generated scene graphs and that including relations and attributes in queries outperforms a model which only considers objects. We also demonstrated that our parser is well suited for other tasks which require a semantic representation of a visual scene. Multi-modal embeddings Recently, multimodal embeddings of natural language and images got a lot of attention (Socher et al., 2014; Karpathy et al., 2014; Plummer et al., 2015; Kiros et al., 2015; Mao et al., 2015; Chrupala et al., 2015). These embeddings can be used to retrieve images from captions and generating captions from images. As mentioned in the introduction, these models are trained on single-sentence image descriptions which typically cannot capture all the details of a visual scene. Further, unlike our modular system, they cannot be used for other tasks that require an interpretable semantic representation. Acknowledgments We thank the anonymous reviewers for their thoughtful feedback. This work was supporte"
W15-2812,N15-1040,0,0.0149777,". The second author is also supported by a Magic Grant from The Brown Institute for Media Innovation. Any opinions, findings, and conclusions or recommendations expressed are those of the author(s) and do not necessarily reflect the view of either DARPA or the US government. Parsing to graph-based representations Representing semantic information with graphs has recently experienced a resurgence caused by the development of the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) which was followed by several works on parsing natural language sentences to AMR (Flanigan et al., 2014; Wang et al., 2015; Werling et al., 2015). Considering that AMR graphs are, like dependency trees, very similar to scene graphs, we could have also used this representation and transformed it to scene graphs. However, the performance of AMR parsers is still not competitive with the performance of dependency parsers which makes dependency trees are more stable starting point. There also exists some prior work on parsing scene descriptions to semantic representations. As mentioned above, Chang et al. (2014) present a rule-based system to parse natural language descriptions to scene templates, a similar graphbased"
W15-2812,P15-1095,1,0.859708,"Missing"
W15-2812,W14-3102,1,\N,Missing
W15-2812,D14-1217,1,\N,Missing
W15-4002,W11-0114,0,0.0253397,"ilment, and contradiction (Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). However, for robust natural language understanding, it is essential to model these phenomena in their full generality on complex linguistic structures. This paper describes four machine learning experiments that directly evaluate the abilities of these models to learn representations that support specific semantic behaviors. These tasks follow the format of natural language inference (also known as recognizing textual entailment; Dagan et al. 2006), in which the goal is to determine the core inferential relationship between two sentences. We introduce a novel NN archite"
W15-4002,S13-1001,0,0.0608655,"Missing"
W15-4002,W13-3209,0,0.0242407,"x=y x ∩ y = ∅ ∧ x ∪ y 6= D x∩y =∅∧x∪y =D x ∩ y 6= ∅ ∧ x ∪ y = D (else) turtle, reptile reptile, turtle couch, sofa turtle, warthog able, unable animal, non-turtle turtle, pet Table 1: The seven relations of MacCartney and Manning (2009)’s logic are defined abstractly on pairs of sets drawing from the universe D, but can be straightforwardly applied to any pair of natural language words, phrases, or sentences. The relations are defined so as to be mutually exclusive. standard NN layer function (1) and those with the more powerful neural tensor network layer function (2) proposed in Chen et al. (2013). The nonlinearity f (x) = tanh(x) is applied elementwise to the output of either layer function.  (l)  ~x (1) ~yTreeRNN = f (M (r) + ~b ) ~x the tree and into the classifier. For an objective function, we use the negative log likelihood of the correct label with tuned L2 regularization. We initialize parameters uniformly, using the range (−0.05, 0.05) for layer parameters and (−0.01, 0.01) for embeddings, and train the model using stochastic gradient descent (SGD) with learning rates computed using AdaDelta (Zeiler, 2012). The classifier feature vector is fixed at 75 dimensions and the dime"
W15-4002,Q14-1006,0,0.029844,"Missing"
W15-4002,P03-1054,1,0.0236939,"Missing"
W15-4002,J13-2005,0,0.00819476,"edu {∗ Dept. of Linguistics, † NLP Group, ‡ Dept. of Computer Science} Stanford University Stanford, CA 94305, USA Abstract the kind of high-fidelity distributed representations proposed in recent algebraic work on vector space modeling (Coecke et al., 2011; Grefenstette, 2013; Hermann et al., 2013; Rockt¨aschel et al., 2014), and whether any such model can match the performance of grammars based in logical forms in their ability to model core semantic phenomena like quantification, entailment, and contradiction (Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). However, for robust natural language understanding, it is essential to model t"
W15-4002,E12-1004,0,0.0204642,"Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). However, for robust natural language understanding, it is essential to model these phenomena in their full generality on complex linguistic structures. This paper describes four machine learning experiments that directly evaluate the abilities of these models to learn representations that support specific semantic behaviors. These tasks follow the format of natural language inference (also known as recognizing textual entailment; Dagan et al. 2006), in which the goal is to determine the core inferential relationship between two sentences. We introduce a novel NN architecture for natural lang"
W15-4002,W09-3714,1,0.792478,"2011a). These results are encouraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics information in the sentence vectors. Softmax classifier P (@) = 0.8 Much of the theoretical work on natural language inference (and some successful implemented models; MacCartney and Manning 2009; Watanabe et al. 2012) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language. In our first three experiments, we test our models’ ability to learn the foundations of natural language inference by training them to reproduce the behavior of the natural logic of MacCartney and Manning (2009) on artificial data. This logic defines seven mutually-exclusive relations of synonymy, entailment, contradiction, and mutual consistency, as sum"
W15-4002,S14-2001,0,0.0219712,"el restarts, suggesting that there is no fundamental obstacle to learning a perfect model for this problem. 6 The SICK textual entailment challenge The specific model architecture that we use is novel, and though the underlying tree structure approach has been validated elsewhere, our experiments so far do not guarantee that it viable model for handling inference over real natural language data. To investigate our models’ ability to handle the noisy labels and the diverse range of linguistic structures seen in typical natural language data, we use the SICK textual entailment challenge corpus (Marelli et al., 2014b). The corpus consists of about 10k natural language sentence pairs, labeled with entailment, contradiction, or neutral. At only a few thousand distinct sentences (many of them variants on an even smaller set of template sentences), the corpus is not large enough to train a high quality learned model of general natural language, but it is the largest human-labeled entailment corpus that we are aware of, and our results nonetheless show that tree-structured NN models can learn to approximate natural logic-style inference in the real world. Adapting to this task requires us to make a few 4 We t"
W15-4002,marelli-etal-2014-sick,0,0.0210269,"el restarts, suggesting that there is no fundamental obstacle to learning a perfect model for this problem. 6 The SICK textual entailment challenge The specific model architecture that we use is novel, and though the underlying tree structure approach has been validated elsewhere, our experiments so far do not guarantee that it viable model for handling inference over real natural language data. To investigate our models’ ability to handle the noisy labels and the diverse range of linguistic structures seen in typical natural language data, we use the SICK textual entailment challenge corpus (Marelli et al., 2014b). The corpus consists of about 10k natural language sentence pairs, labeled with entailment, contradiction, or neutral. At only a few thousand distinct sentences (many of them variants on an even smaller set of template sentences), the corpus is not large enough to train a high quality learned model of general natural language, but it is the largest human-labeled entailment corpus that we are aware of, and our results nonetheless show that tree-structured NN models can learn to approximate natural logic-style inference in the real world. Adapting to this task requires us to make a few 4 We t"
W15-4002,D14-1162,1,0.108463,"Missing"
W15-4002,W14-2409,0,0.126108,"Missing"
W15-4002,D11-1014,1,0.0289584,". In our first set of experiments, we generate artificial data from a logical grammar and use it to evaluate the models’ ability to learn to handle basic relational reasoning, recursive structures, and quantification. We then evaluate the models on the more natural SICK challenge data. Both models perform competitively on the SICK data and generalize well in all three experiments on simulated data, suggesting that they can learn suitable representations for logical inference in natural language. 1 Introduction Tree-structured recursive neural network models (TreeRNNs; Goller and Kuchler 1996; Socher et al. 2011b) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis (Socher et al., 2011b; Irsoy and Cardie, 2014), image description (Socher et al., 2014), and paraphrase detection (Socher et al., 2011a). These results are encouraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c B"
W15-4002,D12-1110,1,0.0397056,"scard training examples with more than 4 logical operators, yielding 60k short training examples, and 21k test examples across all 12 bins. In addition to the two tree models, we also train a summing NN baseline which is largely identical to the TreeRNN, except that instead of using a learned composition function, it simply sums the term vectors in each expression to compose them before passing them to the comparison layer. Unlike the two tree models, this baseline does not use word order, and is as such guaranteed to ignore some information that it would need in order to succeed perfectly. 3 Socher et al. (2012) show that a matrix-vector TreeRNN model somewhat similar to our TreeRNTN can learn boolean logic, a logic where the atomic symbols are simply the values T and F. While learning the operators of that logic is not trivial, the outputs of each operator can be represented accurately by a single bit. In the much more demanding task presented here, the atomic symbols are variables over these values, and 6 the sentence vectors must thus be able to distinguish up to 22 distinct conditions on valuations. Results Fig. 3 shows the relationship between test accuracy and statement size. While the summing"
W15-4002,D13-1170,1,0.00982266,"x linguistic structures. This paper describes four machine learning experiments that directly evaluate the abilities of these models to learn representations that support specific semantic behaviors. These tasks follow the format of natural language inference (also known as recognizing textual entailment; Dagan et al. 2006), in which the goal is to determine the core inferential relationship between two sentences. We introduce a novel NN architecture for natural language inference which independently computes vector representations for each of two sentences using standard TreeRNN or TreeRNTN (Socher et al., 2013) models, and produces a judgment for the pair using only those representations. This allows us to gauge the abilities of these two models to represent all of the necessary semantic Tree-structured recursive neural networks (TreeRNNs) for sentence meaning have been successful for many applications, but it remains an open question whether the fixed-length representations that they learn can support tasks as demanding as logical deduction. We pursue this question by evaluating whether two such models— plain TreeRNNs and tree-structured neural tensor networks (TreeRNTNs)—can correctly learn to ide"
W15-4002,Q14-1017,1,0.0553835,"antification. We then evaluate the models on the more natural SICK challenge data. Both models perform competitively on the SICK data and generalize well in all three experiments on simulated data, suggesting that they can learn suitable representations for logical inference in natural language. 1 Introduction Tree-structured recursive neural network models (TreeRNNs; Goller and Kuchler 1996; Socher et al. 2011b) for sentence meaning have been successful in an array of sophisticated language tasks, including sentiment analysis (Socher et al., 2011b; Irsoy and Cardie, 2014), image description (Socher et al., 2014), and paraphrase detection (Socher et al., 2011a). These results are encouraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics information in the sentence vectors. Softmax classifier P (@) = 0.8 Much of the theoretical work on natural language inference"
W15-4002,P15-1150,1,0.520584,"s a learned full rank third-order tensor T, of dimension N × N × N , modeling multiplicative interactions between the child vectors. The comparison layer uses the same layer function as the composition layers (either an NN layer or an NTN layer) with independently learned parameters and a separate nonlinearity function. Rather than use a tanh nonlinearity here, we found better results with the leaky rectified linear function (Maas et al., 2013): f (x) = max(x, 0) + 0.01 min(x, 0). Other strong tree-structured models have been proposed in past work (Socher et al., 2014; Irsoy and Cardie, 2014; Tai et al., 2015), but we believe that these two provide a valuable case study, and that positive results on here are likely to generalize well to stronger models. To run the model forward, we assemble the two tree-structured networks so as to match the structures provided for each phrase, which are either included in the source data or given by a parser. The word vectors are then looked up from the vocabulary embedding matrix V (one of the learned model parameters), and the composition and comparison functions are used to pass information up 3 Reasoning about semantic relations The simplest kinds of deduction"
W15-4002,J82-3002,0,0.724179,"stopher D. Manning∗†‡ sbowman@stanford.edu cgpotts@stanford.edu manning@stanford.edu {∗ Dept. of Linguistics, † NLP Group, ‡ Dept. of Computer Science} Stanford University Stanford, CA 94305, USA Abstract the kind of high-fidelity distributed representations proposed in recent algebraic work on vector space modeling (Coecke et al., 2011; Grefenstette, 2013; Hermann et al., 2013; Rockt¨aschel et al., 2014), and whether any such model can match the performance of grammars based in logical forms in their ability to model core semantic phenomena like quantification, entailment, and contradiction (Warren and Pereira, 1982; Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2013). Recent work on the algebraic approach of Coecke et al. (2011) has yielded rich frameworks for computing the meanings of fragments of natural language compositionally from vector or tensor representations, but has not yet yielded effective methods for learning these representations from data in typical machine learning settings. Past experimental work on reasoning with distributed representations have been largely confined to short phrases (Mitchell and Lapata, 2010; Grefenstette et al., 2011; Baroni et al., 2012). Ho"
W15-4002,C12-1171,0,0.0263234,"couraging for the ability of these models to learn to produce and use strong semantic representations for sentences. However, it remains an open question whether any such fully learned model can achieve 12 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 12–21, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics information in the sentence vectors. Softmax classifier P (@) = 0.8 Much of the theoretical work on natural language inference (and some successful implemented models; MacCartney and Manning 2009; Watanabe et al. 2012) involves natural logics, which are formal systems that define rules of inference between natural language words, phrases, and sentences without the need of intermediate representations in an artificial logical language. In our first three experiments, we test our models’ ability to learn the foundations of natural language inference by training them to reproduce the behavior of the natural logic of MacCartney and Manning (2009) on artificial data. This logic defines seven mutually-exclusive relations of synonymy, entailment, contradiction, and mutual consistency, as summarized in Table 1, and"
W15-4002,P11-1060,0,\N,Missing
W16-2504,W00-0726,0,0.38026,"n of tasks to be included in the benchmark suite. These were chosen to be a representative – though certainly not exhaustive – sampling of relevant downstream tasks. Two tasks are included to test syntactic properties of the word embeddings – part-of-speech tagging and chunking. Part-of-speech tagging is carried out on the WSJ dataset described in Toutanova et al. (2003). In order to simplify the task and avoid hand-coded features, we evaluate against the universal part-of-speech tags proposed in Petrov et al. (2012). For chunking, we use the dataset from the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), derived from the Wall Street Journal. Four tasks test the semantic properties of the word embeddings. At the word level, we include named entity recognition. We evaluate on a 4class Named Entity Recognition task: PERSON, LOCATION, ORGANIZATION, and MISC, using the CoNLL 2003 dataset (Tjong Kim Sang and De Meulder, 2003), and an IOB tagging scheme. At the sentence level, we include two tasks – sentiment classification and question classification. We implement binary sentiment classification using the Stanford Sentiment Treebank dataset, and the coarse-grained question classification task desc"
W16-2504,N03-1033,1,0.0587722,"s intrinsic performance did not necessarily correlate with its real-world performance. This finding is a key motivation for this work – we aim to create a metric which does correlate with downstream performance. 3 4 Tasks The following are a selection of tasks to be included in the benchmark suite. These were chosen to be a representative – though certainly not exhaustive – sampling of relevant downstream tasks. Two tasks are included to test syntactic properties of the word embeddings – part-of-speech tagging and chunking. Part-of-speech tagging is carried out on the WSJ dataset described in Toutanova et al. (2003). In order to simplify the task and avoid hand-coded features, we evaluate against the universal part-of-speech tags proposed in Petrov et al. (2012). For chunking, we use the dataset from the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), derived from the Wall Street Journal. Four tasks test the semantic properties of the word embeddings. At the word level, we include named entity recognition. We evaluate on a 4class Named Entity Recognition task: PERSON, LOCATION, ORGANIZATION, and MISC, using the CoNLL 2003 dataset (Tjong Kim Sang and De Meulder, 2003), and an IOB tagging schem"
W16-2504,P14-5004,0,0.0374063,"arity or linear relationships for analogies. These intrinsic evaluations are carried out with little attention paid to how performance correlates with downstream tasks. We propose instead evaluatRelated work Existing work on creating evaluations for word embeddings has focused on lexical semantics tasks. An example of such tasks is WordSim-353 (Finkelstein et al., 2001), in which a series of word pairs are assigned similarity judgments by human annotators, and these are compared to the similarity scores obtained from word embeddings. A thorough such lexical semantics evaluation was created by Faruqui and Dyer (2014)1 . This website allows a user to upload a set of embeddings, and evaluates these embeddings on a series of word similarity benchmarks. We follow the model presented in Faruqui and Dyer (2014), but extend to a series of more realistic downstream tasks. 1 http://www.wordvectors.org 19 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 19–23, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics website where users can upload their embeddings to be evaluated. Schnabel et al. (2015) carried out both a thorough intrinsic evaluatio"
W16-2504,N13-1092,0,0.0237328,"Missing"
W16-2504,Q15-1016,0,0.150489,"Missing"
W16-2504,P14-5010,1,0.00671344,"nce appears on the homepage. It is expected that the evaluation will take a few hours. For example, the best performing hyperparameters on the baseline embeddings result in a running time of 4 hours and 24 minutes. Experimental methodology Training hyperparameters Following Schnabel et al. (2015), we prescribe the use of a fixed snapshot of Wikipedia (dated 200803-01) for training the embeddings to be evaluated. This corpus was selected to be as close in time as possible to the corpus Collobert et al. (2011)’s embeddings were trained on. It was preprocessed by applying the Stanford tokenizer (Manning et al., 2014), and replacing all digits with zeros. 7.2 Fine-tuning Avoiding bias Since this method of evaluation involves training a number of neural network models, there is a significant danger of overfitting to the embeddings used to find the hyperparameters. We attempt to mitigate this in two ways. First, we use simple models with standard neural net layers to limit the number of hyperparameters tuned. We tune only the optimizer type, the l2 coefficient for regularization, and the learning 9 Conclusion We have presented a proposal for a fair and replicable evaluation for word embeddings. We plan to ma"
W16-2504,D14-1162,1,0.127476,"Missing"
W16-2504,petrov-etal-2012-universal,0,0.0112237,"a metric which does correlate with downstream performance. 3 4 Tasks The following are a selection of tasks to be included in the benchmark suite. These were chosen to be a representative – though certainly not exhaustive – sampling of relevant downstream tasks. Two tasks are included to test syntactic properties of the word embeddings – part-of-speech tagging and chunking. Part-of-speech tagging is carried out on the WSJ dataset described in Toutanova et al. (2003). In order to simplify the task and avoid hand-coded features, we evaluate against the universal part-of-speech tags proposed in Petrov et al. (2012). For chunking, we use the dataset from the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), derived from the Wall Street Journal. Four tasks test the semantic properties of the word embeddings. At the word level, we include named entity recognition. We evaluate on a 4class Named Entity Recognition task: PERSON, LOCATION, ORGANIZATION, and MISC, using the CoNLL 2003 dataset (Tjong Kim Sang and De Meulder, 2003), and an IOB tagging scheme. At the sentence level, we include two tasks – sentiment classification and question classification. We implement binary sentiment classification u"
W16-2504,D15-1036,0,0.466211,"ch lexical semantics evaluation was created by Faruqui and Dyer (2014)1 . This website allows a user to upload a set of embeddings, and evaluates these embeddings on a series of word similarity benchmarks. We follow the model presented in Faruqui and Dyer (2014), but extend to a series of more realistic downstream tasks. 1 http://www.wordvectors.org 19 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 19–23, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics website where users can upload their embeddings to be evaluated. Schnabel et al. (2015) carried out both a thorough intrinsic evaluation of word vectors, and a limited extrinsic evaluation showing that an embedding’s intrinsic performance did not necessarily correlate with its real-world performance. This finding is a key motivation for this work – we aim to create a metric which does correlate with downstream performance. 3 4 Tasks The following are a selection of tasks to be included in the benchmark suite. These were chosen to be a representative – though certainly not exhaustive – sampling of relevant downstream tasks. Two tasks are included to test syntactic properties of t"
W17-0416,N16-1181,0,0.0367729,"a novel, and Mary a play.”, we insert one copy node for each elided verb. However, unlike in the previous example, we do not add relations between the copy nodes and the semantically vacuous function word to because it is not required for the interpretation of the sentence. conj cc nsubj and Mary likes0 xcomp The motivation behind these design choices is to have direct and meaningful relations between content words. Many shallow natural language understanding systems, which make use of UD such as open relation extraction systems (Mausam et al., 2012; Angeli et al., 2015) or semantic parsers (Andreas et al., 2016; Reddy et al., 2017), use dependency graph patterns to extract information from sentences. These patterns are typically designed for prototypical clause structures, and by augmenting the dependency graph as described above, many patterns that were designed for canonical clause structures also produce the correct results when applied to sentences with gapping constructions. coffee conj cc nsubj obj xcomp obj Similarly, we insert a copy node as the new root of a sentence in cases in which the leftmost conjunct contains a gap as, for example, in (5). nsubj obj xcomp (24) ... and M. want0 try0 be"
W17-0416,W15-2128,0,0.0236925,"n, to be the head of the relative clause, and we attach the adjective, which modifies the elided noun, to the promoted subject with an orphan relation. 4 This argument based on constituency criteria might seem surprising considering that such evidence was dismissed when deciding on analyses for other constructions in UD, such as prepositional phrases. One of the major criticisms of UD has been that we attach prepositions to their complement instead of treating them as heads of prepositional phrases because this decision appears to be misguided when one considers constituency tests (see, e.g., Osborne (2015)). However, this decision should not be interpreted as UD completely ignoring constituency. It is true that following Tesni`ere (1959), UD treats content words with their function words as dissociated nuclei and thus ignores the results of constituency tests for determining the attachment of function words – an approach that is also taken by some generative grammarians, for example, in the form of the notion of extended projection by Grimshaw (1997). But importantly, UD still respects the constituency of nominals, clauses and other larger units. For this reason, it is important to have an anal"
W17-0416,de-marneffe-etal-2006-generating,1,0.19862,"Missing"
W17-0416,W15-2113,0,0.454357,"S or REMNANTS , and we refer to the dependents of the verb in the clause with the overt verb as the COR RESPONDENTS , as illustrated with the following annotated sentence. John likes tea CORRE - OVERT CORRE SPONDENT VERB SPONDENT 2 and Mary conj obj nsubj conj coffee cc ORPHAN / ORPHAN / REMNANT REMNANT nsubj (3) Sue obj likes pasta drinks nsubj tea and and Peter advmod does , too conj cc John nsubj For the constructions we are mainly concerned with in this paper, i.e., gapping constructions in which the governor of multiple phrases was elided, UD v2 adopts a modified version of a proposal by Gerdes and Kahane (2015). We promote the orphan whose grammatical role dominates all other orphans according to an adaptation of the obliqueness hierarchy,2 to be the head of the conjunct. The motivation behind using such a hierarchy instead of a simpler strategy such as promoting the leftmost phrase is that it leads to a more parallel analysis across languages that differ in word order. We attach all other orphans except for coordinating conjunctions using the special orphan relation. Coordinating conjunctions are attached to the head of the following conjunct with the cc relation. This leads to the analysis in (4)"
W17-0416,D17-1009,0,0.059943,"Missing"
W17-0416,L16-1376,1,0.911598,"Missing"
W17-0416,D12-1048,0,0.0299164,"dded verbs, as in the sentence “I want to try to begin to write a novel, and Mary a play.”, we insert one copy node for each elided verb. However, unlike in the previous example, we do not add relations between the copy nodes and the semantically vacuous function word to because it is not required for the interpretation of the sentence. conj cc nsubj and Mary likes0 xcomp The motivation behind these design choices is to have direct and meaningful relations between content words. Many shallow natural language understanding systems, which make use of UD such as open relation extraction systems (Mausam et al., 2012; Angeli et al., 2015) or semantic parsers (Andreas et al., 2016; Reddy et al., 2017), use dependency graph patterns to extract information from sentences. These patterns are typically designed for prototypical clause structures, and by augmenting the dependency graph as described above, many patterns that were designed for canonical clause structures also produce the correct results when applied to sentences with gapping constructions. coffee conj cc nsubj obj xcomp obj Similarly, we insert a copy node as the new root of a sentence in cases in which the leftmost conjunct contains a gap as, fo"
W17-0416,D07-1013,0,0.056163,"First, it makes it impossible to analyze sentences with orphans that do not have a correspondent such as the sentence with an additional modifier in (8), or sentences whose correspondents appear in a previous sentence as in (7). Second, the remnant relations appear to be an abuse of dependency links as they are clearly not true syntactic dependency relations but rather a kind of co-indexing relation between orphans and correspondents. Further, such an analysis introduces many long-distance dependencies and many non-projective dependencies, both of which are known to lower parsing performance (McDonald and Nivre, 2007). Lastly, as mentioned above, and Mary coffee forms a syntactic unit, which is not captured by this proposal. 6.2 6.3 Composite relations Joakim Nivre and Daniel Zeman developed a third proposal7 as part of the discussion of the second version of the UD guidelines. Their proposal is based on composite relations such as conj>nsubj, which indicates which relations would be present along the dependency path from the first conjunct to the orphan if there was no gap. For example, X conj>nsubj Y indicates that there would have been a conj relation between X and an elided node, and an nsubj relation"
W17-0416,P15-1034,1,\N,Missing
W17-0416,L16-1262,1,\N,Missing
W17-5506,W14-4337,0,0.182077,"hidden current unit. We denote h states of the decoder and y1 , . . . , yn as the output tokens. We extend this decoder with an attentionbased model (Bahdanau et al., 2015; Luong et al., 2015a), where, at every time step t of the decoding, an attention score ati is computed for each hidden state hi of the encoder, using the attention mechanism of (Vinyals et al., 2015). Formally this attention can be described by the following equations: While recent neural dialogue models have explicitly modelled dialogue state through belief and user intent trackers (Wen et al., 2016b; Dhingra et al., 2016; Henderson et al., 2014b), we choose instead to rely on learned neural representations for implicit modelling of dialogue state, forming The data is available for download https://nlp.stanford.edu/blog/a-new-multi-turn-multidomain-task-oriented-dialogue-dataset/ (1) where the recurrence uses a long-short-term memory unit, as described by (Hochreiter and Schmidhuber, 1997). Key-Value Retrieval Networks 1 Encoder ˜ t ]))) uti = wT tanh(W2 tanh(W1 [hi , h ati (3) yt = Softmax(ot ) (6) = ˜0 = h t ot = at 38 (2) Softmax(uti ) m X ati hi i=1 ˜ t, h ˜0 ] U [h t (4) (5) KB entry. For our purposes, the key of an entry corres"
W17-5506,W14-4340,0,0.282484,"hidden current unit. We denote h states of the decoder and y1 , . . . , yn as the output tokens. We extend this decoder with an attentionbased model (Bahdanau et al., 2015; Luong et al., 2015a), where, at every time step t of the decoding, an attention score ati is computed for each hidden state hi of the encoder, using the attention mechanism of (Vinyals et al., 2015). Formally this attention can be described by the following equations: While recent neural dialogue models have explicitly modelled dialogue state through belief and user intent trackers (Wen et al., 2016b; Dhingra et al., 2016; Henderson et al., 2014b), we choose instead to rely on learned neural representations for implicit modelling of dialogue state, forming The data is available for download https://nlp.stanford.edu/blog/a-new-multi-turn-multidomain-task-oriented-dialogue-dataset/ (1) where the recurrence uses a long-short-term memory unit, as described by (Hochreiter and Schmidhuber, 1997). Key-Value Retrieval Networks 1 Encoder ˜ t ]))) uti = wT tanh(W2 tanh(W1 [hi , h ati (3) yt = Softmax(ot ) (6) = ˜0 = h t ot = at 38 (2) Softmax(uti ) m X ati hi i=1 ˜ t, h ˜0 ] U [h t (4) (5) KB entry. For our purposes, the key of an entry corres"
W17-5506,D15-1166,1,0.0241703,"Missing"
W17-5506,D16-1147,0,0.139379,"preserving their endto-end trainability and 2) They often require explicitly modelling user dialogues with belief trackers and dialogue state information, which necessitates additional data annotation and also breaks differentiability. To address some of the modelling issues in previous neural dialogue agents, we introduce a new architecture called the Key-Value Retrieval Network. This model augments existing recurrent network architectures with an attention-based key-value retrieval mechanism over the entries of a knowledge base, which is inspired by recent work on key-value memory networks (Miller et al., 2016). By doing so, it is able to learn how to extract useful information from a knowledge base directly from data in an end-to-end fashion, withIntroduction With the success of new speech-based humancomputer interfaces, there is a great need for effective task-oriented dialogue agents that can handle everyday tasks such as scheduling events and booking hotels. Current commercial dialogue agents are often brittle pattern-matching systems which are unable to maintain the kind of flexible conversations that people desire. Neural dialogue agents present one of the most promising avenues for leveraging"
W17-5506,P16-1002,0,0.00986393,"ading of documents. We store every entry of our KB using a (subject, relation, object) representation. In our representation a KB entry from the dialogue in Figure 1 such as (event=dinner, time=8pm, date=the 13th, party=Ana, agenda=“-”) would be normalized into four separate triples of the form (dinner, time, 8pm). Every KB has at most 230 normalized triples. This formalism is similar to a neo-Davidsonian or RDF-style representation of events. Recent literature has shown that incorporating a copying mechanism into neural architectures improves performance on various sequenceto-sequence tasks (Jia and Liang, 2016; Gu et al., 2016; Ling et al., 2016; Gulcehre et al., 2016; Eric and Manning, 2017). We build off this intuition in the following way: at every timestep of decoding, we take the decoder hidden state and compute an attention score with the key of each normalized ˜ t ]))) utj = rT tanh(W20 tanh(W10 [kj , h ˜ t, h ˜ 0 ] + v¯t ot = U [h (8) yt = Softmax(ot ) (9) t (7) where r, W10 , and W20 are trainable parameters. In (8) above, v¯t is a sparse vector with length |V |+ n. Within v¯t , the entry for the value embedding vj corresponding to the key kj is equal to the logit score utj on kj . Hence,"
W17-5506,P02-1040,0,0.116491,"of a system’s ability to capture user semantics. Because our model does not have provisions for slot-tracking by design, we are unable to report such a metric and hence report our entity F1 . Automatic Evaluation Metrics Though prior work has shown that automatic evaluation metrics often correlate poorly with human assessments of dialogue agents (Liu et al., 2016), we report a number of automatic metrics in Table 3. These metrics are provided for coarse-grained evaluation of dialogue response quality: • BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002), which has also been used in past literature for evaluating dialogue systems both of the chatbot and task-oriented variety (Ritter et al., 2011; Li et al., 2016; Wen et al., 2016b). While work by (Liu et al., 2016) has demonstrated that ngram based evaluation metrics such as BLEU and METEOR do not correlate well with human performance on non-task-oriented dialogue datasets, recently (Sharma et al., 2017) have shown that these metrics can show comparatively stronger correlation with human assessment on task-oriented datasets. We, therefore, calculate average BLEU score over all responses gener"
W17-5506,D11-1054,0,0.0114716,"ch a metric and hence report our entity F1 . Automatic Evaluation Metrics Though prior work has shown that automatic evaluation metrics often correlate poorly with human assessments of dialogue agents (Liu et al., 2016), we report a number of automatic metrics in Table 3. These metrics are provided for coarse-grained evaluation of dialogue response quality: • BLEU: We use the BLEU metric, commonly employed in evaluating machine translation systems (Papineni et al., 2002), which has also been used in past literature for evaluating dialogue systems both of the chatbot and task-oriented variety (Ritter et al., 2011; Li et al., 2016; Wen et al., 2016b). While work by (Liu et al., 2016) has demonstrated that ngram based evaluation metrics such as BLEU and METEOR do not correlate well with human performance on non-task-oriented dialogue datasets, recently (Sharma et al., 2017) have shown that these metrics can show comparatively stronger correlation with human assessment on task-oriented datasets. We, therefore, calculate average BLEU score over all responses generated by the system, and primarily report these scores to gauge our 5.4.2 Results We see that of our baseline models, Copy Net has the lowest agg"
W17-5506,P17-1062,0,0.287791,"rn to explicitly represent user intent through intermediate supervision, which breaks end-to-end trainability. Other work by (Bordes and Weston, 2016; Liu and Perez, 2016) stores dialogue context in a memory module and repeatedly queries and reasons about this context to select an adequate system response from a set of all candidate responses. Another line of recent work has developed taskoriented models which are amenable to both supervised learning and reinforcement learning and are able to incorporate domain-specific knowledge via explicitly-provided features and model-output restrictions (Williams et al., 2017). Our model contrasts with these works in that training is done in a strictly supervised fashion via a per utterance token generative process, and the model does not need dialogue state trackers, relying instead on latent neural embeddings for accurate system response generation. Research in task-oriented dialogue often struggles with a lack of standard, publicly available datasets. Several classical corpora have consisted of moderately-sized collections of dialogues related to travel-booking (Hemphill et al., 1990; Table 2: Statistics of Dataset. in the calendar scheduling domain did not expl"
W17-5506,W13-4065,0,0.0561717,"from [3 · 10−6 , 10−5 ]. We used word embeddings, hidden layer, and cell sizes with size 200. We applied gradient clipping with a clip-value of 10 to avoid gradient explosions during training. The attention, output parameters, word embeddings, and LSTM weights were randomly initialized from a uniform unit-scaled distribution in the style of (Sussillo and Abbott, 2015). We also added a bias of 1 to the LSTM cell forget gate in the style of (Pham et al., 2014). Bennett and Rudnicky, 2002). Another wellknown corpus is derived from a series of competitions on the task of dialogue-state tracking (Williams et al., 2013). While the competitions were designed to test systems for state tracking, recent work has chosen to repurpose this data by only using the transcripts of dialogues without state annotation for developing systems (Bordes and Weston, 2016; Williams et al., 2017). More recently, Maluuba has released a dataset of hotel and travel-booking dialogues collected in a Wizard-ofOz Scheme with elaborate semantic frames annotated (Asri et al., 2017). This dataset aims to encourage research in non-linear decision-making processes that are present in task-oriented dialogues. 5 Experiments 5.3 In this section"
W17-5506,H90-1021,0,\N,Missing
W17-5506,P16-1154,0,\N,Missing
W17-5506,D16-1230,0,\N,Missing
W17-5506,P16-1014,0,\N,Missing
W18-5623,P18-1063,0,0.0317185,"this task. Nallapati et al. (2016) used recurrent neural networks for both the encoder and the decoder. To address the limitation that neural models with a fixed vocabulary cannot handle outof-vocabulary words, a pointer-generator model was proposed which uses an attention mechanism that copies elements directly from the input (Nallapati et al., 2016; Merity et al., 2017; See et al., 2017). See et al. (2017) further proposed a coverage mechanism to address the repetition problem in the generated summaries. Paulus et al. (2018) applied reinforcement learning to summarization and more recently, Chen and Bansal (2018) obtained improved result with a model that first selects sentences and then rewrites them. Summarization of Radiology Reports. Most prior work that attempts to “summarize” radiology reports focused on classifying and extracting information from the report text (Friedman et al., 1995; Hripcsak et al., 1998; Elkins et al., 2000; Hripcsak et al., 2002). More recently, Hassanpour and Langlotz (2016) studied extracting named entities from multi-institutional radiology reports using traditional feature-based classifiers. Goff and Loehfelm (2018) built an NLP pipeline to identify asserted and negate"
W18-5623,W16-6103,0,0.0142593,"of Radiology Reports. Most prior work that attempts to “summarize” radiology reports focused on classifying and extracting information from the report text (Friedman et al., 1995; Hripcsak et al., 1998; Elkins et al., 2000; Hripcsak et al., 2002). More recently, Hassanpour and Langlotz (2016) studied extracting named entities from multi-institutional radiology reports using traditional feature-based classifiers. Goff and Loehfelm (2018) built an NLP pipeline to identify asserted and negated disease entities in the impression section of radiology reports as a step towards report summarization. Cornegruta et al. (2016) proposed to use a recurrent neural network architecture to model radiological language in solving the medical named entity recognition and negation detection tasks on radiology reports. To our knowledge, our work represents the first attempt Related Work Early Summarization Systems. Early work on summarization systems mainly focused on extractive approaches, where the summaries are generated by scoring and selecting sentences from the input. Luhn (1958) proposed to represent the input by topic words and score each sentence by the amount of topic words it contains. Kupiec et al. 205 at automat"
W18-5623,D15-1166,1,0.662923,"Missing"
W18-5623,P14-5010,1,0.0138001,"on 6. 5.1 Pointer-Generator. We also run the baseline pointer-generator model introduced by See et al. (2017). We find the “coverage” mechanism described in the paper did not improve summary quality in our task and therefore did not use it for simplicity. We compare our model with two versions of the pointer-generator model: one with only the findings section as input and another one with the background sections prepended to the findings section as input. Data Collection Reports of all radiographic studies from 2000 to 2014 were collected. We first tokenized all reports with Stanford CoreNLP (Manning et al., 2014), and filtered the dataset by excluding reports where (1) no findings or impression section can be found; (2) multiple findings or impression sections can be found but cannot be aligned; or (3) the findings have fewer than 10 words or the impression has fewer than 2 words. We removed body parts where only a small number of cases are available, and included reports of the top 12 body parts in the PACS system to maintain generalizability. For common body parts with more than 10k reports (e.g., chest), we subsampled 10k reports from them. This results in a dataset with a total of 87,127 reports."
W18-5623,W04-3252,0,0.194661,"Missing"
W18-5623,K16-1028,0,0.0588012,"this direction; (ii) we propose a new customized summarization model to this task that improves over existing methods by better leveraging study background information; (iii) we further show via a radiologist evaluation that the summaries generated by our model have significant clinical validity. 2 Neural Summarization Systems. Summarization systems based on neural network models enable abstractive summarization, where new words and phrases are generated to form the summaries. Rush et al. (2015) first applied an attention-based neural encoder and a neural language model decoder to this task. Nallapati et al. (2016) used recurrent neural networks for both the encoder and the decoder. To address the limitation that neural models with a fixed vocabulary cannot handle outof-vocabulary words, a pointer-generator model was proposed which uses an attention mechanism that copies elements directly from the input (Nallapati et al., 2016; Merity et al., 2017; See et al., 2017). See et al. (2017) further proposed a coverage mechanism to address the repetition problem in the generated summaries. Paulus et al. (2018) applied reinforcement learning to summarization and more recently, Chen and Bansal (2018) obtained im"
W18-5623,D14-1162,1,0.105226,"Missing"
W18-5623,D15-1044,0,0.0928842,"n statements with neural sequence-to-sequence learning, and to our knowledge our work represents the first attempt in this direction; (ii) we propose a new customized summarization model to this task that improves over existing methods by better leveraging study background information; (iii) we further show via a radiologist evaluation that the summaries generated by our model have significant clinical validity. 2 Neural Summarization Systems. Summarization systems based on neural network models enable abstractive summarization, where new words and phrases are generated to form the summaries. Rush et al. (2015) first applied an attention-based neural encoder and a neural language model decoder to this task. Nallapati et al. (2016) used recurrent neural networks for both the encoder and the decoder. To address the limitation that neural models with a fixed vocabulary cannot handle outof-vocabulary words, a pointer-generator model was proposed which uses an attention mechanism that copies elements directly from the input (Nallapati et al., 2016; Merity et al., 2017; See et al., 2017). See et al. (2017) further proposed a coverage mechanism to address the repetition problem in the generated summaries."
W18-5623,P17-1099,1,0.959919,"ural network models enable abstractive summarization, where new words and phrases are generated to form the summaries. Rush et al. (2015) first applied an attention-based neural encoder and a neural language model decoder to this task. Nallapati et al. (2016) used recurrent neural networks for both the encoder and the decoder. To address the limitation that neural models with a fixed vocabulary cannot handle outof-vocabulary words, a pointer-generator model was proposed which uses an attention mechanism that copies elements directly from the input (Nallapati et al., 2016; Merity et al., 2017; See et al., 2017). See et al. (2017) further proposed a coverage mechanism to address the repetition problem in the generated summaries. Paulus et al. (2018) applied reinforcement learning to summarization and more recently, Chen and Bansal (2018) obtained improved result with a model that first selects sentences and then rewrites them. Summarization of Radiology Reports. Most prior work that attempts to “summarize” radiology reports focused on classifying and extracting information from the report text (Friedman et al., 1995; Hripcsak et al., 1998; Elkins et al., 2000; Hripcsak et al., 2002). More recently, H"
W19-4828,P18-1027,1,0.822387,"nding from an analysis perspective even if that head is not always used when making predictions for some downstream task. chine translation. One possibility for the apparent redundancy in BERT’s attention heads is the use of attention dropout, which causes some attention weights to be zeroed-out during training. 7 Related Work There has been substantial recent work performing analysis to better understand what neural networks learn, especially from language model pretraining. One line of research examines the outputs of language models on carefully chosen input sentences (Linzen et al., 2016; Khandelwal et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018). For example, the model’s performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model’s syntactic ability, although it does not reveal how that ability is captured by the network. Another line of work investigates the internal vector representations of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-tra"
W19-4828,W11-1902,0,0.0950967,"Missing"
W19-4828,P17-1080,0,0.0329332,"rained language models achieve very high accuracy when fine-tuned on supervised tasks (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018), but it is not fully understood why. The strong results suggest pre-training teaches the models about the structure of language, but what specific linguistic features do they learn? Recent work has investigated this question by examining the outputs of language models on carefully chosen input sentences (Linzen et al., 2016) or examining the internal vector representations of the model through methods such as probing classifiers (Adi et al., 2017; Belinkov et al., 2017). Complementary to these approaches, we 1 Code will be released at https://github.com/ clarkkev/attention-analysis. 276 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 276–286 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Head 1-1 Attends broadly Head 3-1 Attends to next token Head 8-7 Attends to [SEP] Head 11-6 Attends to periods Figure 1: Examples of heads exhibiting the patterns discussed in Section 3. The darkness of a line indicates the strength of the attention weight (some attention weights a"
W19-4828,Q16-1037,0,0.273616,"ng classifier and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention. 1 Introduction Large pre-trained language models achieve very high accuracy when fine-tuned on supervised tasks (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018), but it is not fully understood why. The strong results suggest pre-training teaches the models about the structure of language, but what specific linguistic features do they learn? Recent work has investigated this question by examining the outputs of language models on carefully chosen input sentences (Linzen et al., 2016) or examining the internal vector representations of the model through methods such as probing classifiers (Adi et al., 2017; Belinkov et al., 2017). Complementary to these approaches, we 1 Code will be released at https://github.com/ clarkkev/attention-analysis. 276 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 276–286 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Head 1-1 Attends broadly Head 3-1 Attends to next token Head 8-7 Attends to [SEP] Head 11-6 Attends to periods Figure 1: Examples of h"
W19-4828,P18-2003,1,0.855456,"di et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. They are trained to do a supervised task (e.g., part-of-speech tagging). If a probing classifier achieves high accuracy, it suggests that the vector representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated neural networks capturing aspects of syntactic structures (Shi et al., 2016; Blevins et al., 2018) or coreference (Tenney et al., 2018, 2019) without explicitly being trained for the tasks. With regards to analyzing attention, Vig (2019) builds a visualization tool for the BERT’s attention and reports observations about some of the heads’ behaviors, but does not perform any quantitative analysis. Burns et al. (2018) analyze the attention of memory networks to understand model performance on a question answering dataset; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention with syntax. Raganato and"
W19-4828,N19-1112,0,0.0443214,"ble 3: Results of attention-based probing tasks on dependency parsing. A simple model taking BERT attention maps and GloVe word embeddings as input performs quite well at dependency parsing. *Not directly comparable to our numbers; see text. Overall, our results from probing both individual and combinations of attention heads suggest that BERT learns some aspects syntax purely as a by-product of self-supervised training. Other work has drawn a similar conclusions from examining BERT’s predictions on agreement tasks (Goldberg, 2019) or internal vector representations (Hewitt and Manning, 2019; Liu et al., 2019). Traditionally, syntax-aware models have been developed through architecture design (e.g., recursive neural networks) or from direct supervision from human-curated treebanks. Our findings are part of a growing body of work indicating that indirect supervision from rich pre-training tasks like language modeling can also produce models sensitive to language’s hierarchical structure. 6 Clustering Attention Heads Figure 6: BERT attention heads embedded in twodimensional space. Distance between points approximately matches the average Jensen-Shannon divergences between the outputs of the correspon"
W19-4828,W18-5454,0,0.0260915,"achieves high accuracy, it suggests that the vector representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated neural networks capturing aspects of syntactic structures (Shi et al., 2016; Blevins et al., 2018) or coreference (Tenney et al., 2018, 2019) without explicitly being trained for the tasks. With regards to analyzing attention, Vig (2019) builds a visualization tool for the BERT’s attention and reports observations about some of the heads’ behaviors, but does not perform any quantitative analysis. Burns et al. (2018) analyze the attention of memory networks to understand model performance on a question answering dataset; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention with syntax. Raganato and Tiedemann (2018) evaluate the attention heads of a machine translation model on dependency parsing, but only report overall UAS scores instead of investigating heads for specific syntactic relations or using probing classifiers. Marecek and Rosa (2018) propose heuristic ways 8 Conclusion We have proposed a series of an"
W19-4828,J93-2004,0,0.0655926,"Missing"
W19-4828,W18-5444,0,0.0530079,"f the heads’ behaviors, but does not perform any quantitative analysis. Burns et al. (2018) analyze the attention of memory networks to understand model performance on a question answering dataset; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention with syntax. Raganato and Tiedemann (2018) evaluate the attention heads of a machine translation model on dependency parsing, but only report overall UAS scores instead of investigating heads for specific syntactic relations or using probing classifiers. Marecek and Rosa (2018) propose heuristic ways 8 Conclusion We have proposed a series of analysis methods for understanding the attention mechanisms of models and applied them to BERT. While most recent work on model analysis for NLP has focused on probing vector representations or model outputs, we have shown that a substantial amount of linguistic knowledge can be found not only in the hidden states, but also in the attention maps. We think probing attention maps complements these other model analysis techniques, and should be part of the toolkit used by researchers to understand what neural networks learn about l"
W19-4828,P18-1198,0,0.0428708,"ence indicating how likely each other word is to be the syntactic head of the current one. • A right-branching baseline that always predicts the head is to the dependent’s right. • A simple one-hidden-layer network that takes as input the GloVe embeddings for the dependent and candidate head as well as a set of features indicating the distances between the two words.4 • Our attention-and-words probe, but with attention maps from a BERT network with pretrained word/positional embeddings but randomly initialized other weights. This kind of baseline is surprisingly strong at other probing tasks (Conneau et al., 2018). Attention-Only Probe. Our first probe learns a simple linear combination of attention weights. p(i|j) ∝ exp X n k=1 k wk αij + k uk αji   Results are shown in Table 3. We find the Attn + GloVe probing classifier substantially outperforms our baselines and achieves a decent UAS of 77, suggesting BERT’s attention maps have a fairly thorough representation of English syntax. As a rough comparison, we also report results from the structural probe from Hewitt and Manning (2019), which builds a probing classifier on top of BERT’s vector representations rather than attention. The scores are not"
W19-4828,D18-1151,0,0.036082,"ead is not always used when making predictions for some downstream task. chine translation. One possibility for the apparent redundancy in BERT’s attention heads is the use of attention dropout, which causes some attention weights to be zeroed-out during training. 7 Related Work There has been substantial recent work performing analysis to better understand what neural networks learn, especially from language model pretraining. One line of research examines the outputs of language models on carefully chosen input sentences (Linzen et al., 2016; Khandelwal et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018). For example, the model’s performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model’s syntactic ability, although it does not reveal how that ability is captured by the network. Another line of work investigates the internal vector representations of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. They are trained to do a supe"
W19-4828,A94-1016,0,0.0236585,"Missing"
W19-4828,N19-1423,0,0.379541,"BERT’s Attention Kevin Clark† Urvashi Khandelwal† Omer Levy‡ Christopher D. Manning† † Computer Science Department, Stanford University ‡ Facebook AI Research {kevclark,urvashik,manning}@cs.stanford.edu omerlevy@fb.com study1 the attention maps of a pre-trained model. Attention (Bahdanau et al., 2015) has been a highly successful neural network component. It is naturally interpretable because an attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word. Our analysis focuses on the 144 attention heads in BERT (Devlin et al., 2019), a large pre-trained Transformer (Vaswani et al., 2017) network that has demonstrated excellent performance on many tasks. We first explore generally how the attention heads behave. We find that there are common patterns in their behavior, such as attending to fixed positional offsets or attending broadly over the whole sentence. A surprisingly large amount of BERT’s attention focuses on the deliminator token [SEP], which we argue is used by the model as a sort of no-op. Generally we find that attention heads in the same layer tend to behave similarly. We next probe each attention head for li"
W19-4828,D14-1162,1,0.113256,"j’s syntactic head, αij from word i to word j produced by head k, and n is the number of attention heads. We include both directions of attention: candidate head to dependent as well as dependent to candidate head. The weight vectors w and u are trained using standard supervised learning on the train set. Attention-and-Words Probe. Given our finding that heads specialize to particular syntactic relations, we believe probing classifiers should benefit from having information about the input words. In particular, we build a model that sets the weights of the attention heads based on the GloVe (Pennington et al., 2014) embeddings for the input words. Intuitively, if the dependent and candidate head are “the” and “cat,” the probing classifier should learn to assign most of the weight to the head 8-11, which achieves excellent performance at the determiner relation. The attentionand-words probing classifier assigns the probabil4 indicator features for short distances as well as continuous distance features, with distance ahead/behind treated separately to capture word order 282 Model Right-branching Distances + GloVe Random Init Attn + GloVe Attn Attn + GloVe Structural probe (Hewitt and Manning, 2019) UAS 26"
W19-4828,P16-1078,0,0.0261099,"sed in Section 3. The darkness of a line indicates the strength of the attention weight (some attention weights are so low they are invisible). Our findings show that particular heads specialize to specific aspects of syntax. To get a more overall measure of the attention heads’ syntactic ability, we propose an attention-based probing classifier that takes attention maps as input. The classifier achieves 77 UAS at dependency parsing, showing BERT’s attention captures a substantial amount about syntax. Several recent works have proposed incorporating syntactic information to improve attention (Eriguchi et al., 2016; Chen et al., 2018; Strubell et al., 2018). Our work suggests that to an extent this kind of syntax-aware attention already exists in BERT, which may be one of the reason for its success. 2 Attention weights can be viewed as governing how “important” every other token is when producing the next representation for the current token. BERT is pre-trained on 3.3 billion tokens of unlabeled text to perform two tasks. In the “masked language modeling” task, the model predicts the identities of words that have been masked-out of the input text. In the “next sentence prediction” task, the model predi"
W19-4828,N18-1202,0,0.117725,"ing similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention. 1 Introduction Large pre-trained language models achieve very high accuracy when fine-tuned on supervised tasks (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018), but it is not fully understood why. The strong results suggest pre-training teaches the models about the structure of language, but what specific linguistic features do they learn? Recent work has investigated this question by examining the outputs of language models on carefully chosen input sentences (Linzen et al., 2016) or examining the internal vector representations of the model through methods such as probing classifiers (Adi et al., 2017; Belinkov et al., 2017). Complementary to these approaches, we 1 Code will be released at https://github.com/ clarkkev/attent"
W19-4828,W18-5426,0,0.0184434,"n, especially from language model pretraining. One line of research examines the outputs of language models on carefully chosen input sentences (Linzen et al., 2016; Khandelwal et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018). For example, the model’s performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model’s syntactic ability, although it does not reveal how that ability is captured by the network. Another line of work investigates the internal vector representations of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. They are trained to do a supervised task (e.g., part-of-speech tagging). If a probing classifier achieves high accuracy, it suggests that the vector representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated neural networks capturing aspects of syntactic structures (Shi et al., 2016; Blevins et al., 2018) or coreference (Ten"
W19-4828,W12-4501,0,0.0425951,".5 (1) poss auxpass ccomp mark prt 7-6 4-10 8-1 8-2 6-7 80.5 82.5 48.8 50.7 99.1 47.7 (1) 40.5 (1) 12.4 (-2) 14.5 (2) 91.4 (-1) 4.3 Accuracy Baseline Coreference Resolution Having shown BERT attention heads reflect certain aspects of syntax, we now explore using attention heads for the more challenging semantic task of coreference resolution. Coreference links are usually longer than syntactic dependencies and state-of-the-art systems generally perform much worse at coreference compared to parsing. Setup. We evaluate the attention heads on coreference resolution using the CoNLL-2012 dataset3 (Pradhan et al., 2012). In particular, we compute antecedent selection accuracy: what percent of the time does the head word of a coreferent mention most attend to the head of one of that mention’s antecedents. We compare against three baselines for selecting an antecedent: • Picking the nearest other mention. Table 1: The best performing attentions heads of BERT on WSJ dependency parsing by dependency type. Numbers after baseline accuracies show the best offset found (e.g., (1) means the word to the right is predicted as the head). We show the 10 most common relations as well as 5 other ones attention heads did we"
W19-4828,N18-1108,0,0.0266004,"rspective even if that head is not always used when making predictions for some downstream task. chine translation. One possibility for the apparent redundancy in BERT’s attention heads is the use of attention dropout, which causes some attention weights to be zeroed-out during training. 7 Related Work There has been substantial recent work performing analysis to better understand what neural networks learn, especially from language model pretraining. One line of research examines the outputs of language models on carefully chosen input sentences (Linzen et al., 2016; Khandelwal et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018). For example, the model’s performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model’s syntactic ability, although it does not reveal how that ability is captured by the network. Another line of work investigates the internal vector representations of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. The"
W19-4828,W18-5431,0,0.0398025,"t al., 2018) or coreference (Tenney et al., 2018, 2019) without explicitly being trained for the tasks. With regards to analyzing attention, Vig (2019) builds a visualization tool for the BERT’s attention and reports observations about some of the heads’ behaviors, but does not perform any quantitative analysis. Burns et al. (2018) analyze the attention of memory networks to understand model performance on a question answering dataset; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention with syntax. Raganato and Tiedemann (2018) evaluate the attention heads of a machine translation model on dependency parsing, but only report overall UAS scores instead of investigating heads for specific syntactic relations or using probing classifiers. Marecek and Rosa (2018) propose heuristic ways 8 Conclusion We have proposed a series of analysis methods for understanding the attention mechanisms of models and applied them to BERT. While most recent work on model analysis for NLP has focused on probing vector representations or model outputs, we have shown that a substantial amount of linguistic knowledge can be found not only in"
W19-4828,P16-1162,0,0.0598324,"Missing"
W19-4828,N19-1357,0,0.0342873,"machine translation models. They also demonstrate that many attention heads can be pruned away without substantially hurting model performance. Interestingly, the important attention heads that remain after pruning tend to be ones with identified behaviors. Michel et al. (2019) similarly show that many of BERT’s attention heads can be pruned. Although our analysis in this paper only found interpretable behaviors in a subset of BERT’s attention heads, these recent works suggest that there might not be much to explain for some attention heads because they have little effect on model perfomance. Jain and Wallace (2019) argue that attention often does not “explain” model predictions. They show that attention weights frequently do not correlate with other measures of feature importance. Furthermore, attention weights can often be substantially changed without altering model predictions. However, our motivation for looking at attention is different: rather than explaining model predictions, we are seeking to understand information learned by the models. For example, if a particular attention head learns a syntactic relation, we consider that an important finding from an analysis perspective even if that head i"
W19-4828,D16-1159,0,0.0214255,"ns of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. They are trained to do a supervised task (e.g., part-of-speech tagging). If a probing classifier achieves high accuracy, it suggests that the vector representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated neural networks capturing aspects of syntactic structures (Shi et al., 2016; Blevins et al., 2018) or coreference (Tenney et al., 2018, 2019) without explicitly being trained for the tasks. With regards to analyzing attention, Vig (2019) builds a visualization tool for the BERT’s attention and reports observations about some of the heads’ behaviors, but does not perform any quantitative analysis. Burns et al. (2018) analyze the attention of memory networks to understand model performance on a question answering dataset; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention wi"
W19-4828,D18-1548,0,0.029955,"ndicates the strength of the attention weight (some attention weights are so low they are invisible). Our findings show that particular heads specialize to specific aspects of syntax. To get a more overall measure of the attention heads’ syntactic ability, we propose an attention-based probing classifier that takes attention maps as input. The classifier achieves 77 UAS at dependency parsing, showing BERT’s attention captures a substantial amount about syntax. Several recent works have proposed incorporating syntactic information to improve attention (Eriguchi et al., 2016; Chen et al., 2018; Strubell et al., 2018). Our work suggests that to an extent this kind of syntax-aware attention already exists in BERT, which may be one of the reason for its success. 2 Attention weights can be viewed as governing how “important” every other token is when producing the next representation for the current token. BERT is pre-trained on 3.3 billion tokens of unlabeled text to perform two tasks. In the “masked language modeling” task, the model predicts the identities of words that have been masked-out of the input text. In the “next sentence prediction” task, the model predicts whether the second half of the input fo"
W19-4828,P19-1452,0,0.0867982,"Missing"
W19-4828,D18-1317,0,0.0219967,"ion heads be clearly grouped by behavior? We investigate these questions by computing the distances between all pairs of attention heads. Formally, we measure the distance between two heads Hi and Hj as: X JS(Hi (token), Hj (token)) Results are shown in Figure 6. We find that there are several clear clusters of heads that behave similarly, often corresponding to behaviors we have already discussed in this paper. Heads within the same layer are often fairly close to each other, meaning that heads within the layer have similar attention distributions. This finding is a bit surprising given that Tu et al. (2018) show that encouraging attention heads to have different behaviors can improve Transformer performance at matoken∈data Where JS is the Jensen-Shannon Divergence between attention distributions. Using these distances, we visualize the attention heads by applying multidimensional scaling (Kruskal, 1964) to embed each head in two dimensions such that the euclidean distance between embeddings reflects the Jensen-Shannon distance between the corresponding heads as closely as possible. 283 of converting attention scores to syntactic trees, but do not quantitatively evaluate their approach. Concurren"
W19-4828,P19-1580,0,0.0757501,"ging attention heads to have different behaviors can improve Transformer performance at matoken∈data Where JS is the Jensen-Shannon Divergence between attention distributions. Using these distances, we visualize the attention heads by applying multidimensional scaling (Kruskal, 1964) to embed each head in two dimensions such that the euclidean distance between embeddings reflects the Jensen-Shannon distance between the corresponding heads as closely as possible. 283 of converting attention scores to syntactic trees, but do not quantitatively evaluate their approach. Concurrently with our work Voita et al. (2019) identify syntactic, positional, and rare-wordsensitive attention heads in machine translation models. They also demonstrate that many attention heads can be pruned away without substantially hurting model performance. Interestingly, the important attention heads that remain after pruning tend to be ones with identified behaviors. Michel et al. (2019) similarly show that many of BERT’s attention heads can be pruned. Although our analysis in this paper only found interpretable behaviors in a subset of BERT’s attention heads, these recent works suggest that there might not be much to explain for"
W19-4828,P15-1137,0,0.0187034,"h dependent has exactly one head but heads have multiple dependents. We also note heads can disagree with standard annotation conventions while still performing syntactic behavior. For example, head 76 marks ’s as the dependent for the poss relation, while gold-standard labels mark the complement of an ’s as the dependent (the accuracy in Table 1 counts ’s as correct). Such disagreements highlight how these syntactic behaviors in BERT are learned as a by-product of self-supervised training, not by copying a human design. We also show the performance of a recent neural coreference system from (Wiseman et al., 2015). Results. Results are shown in Table 2. We find that one of BERT’s attention heads achieves decent coreference resolution performance, improving by over 10 accuracy points on the stringmatching baseline and performing close to the rule-based system. It is particularly good with nominal mentions, perhaps because it is capable of fuzzy matching between synonyms as seen in the bottom right of Figure 5. 5 Probing Attention Head Combinations Since individual attention heads specialize to particular aspects of syntax, the model’s overall “knowledge” about syntax is distributed across multiple atten"
W19-4828,W18-5448,0,0.0223357,"e model pretraining. One line of research examines the outputs of language models on carefully chosen input sentences (Linzen et al., 2016; Khandelwal et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018). For example, the model’s performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model’s syntactic ability, although it does not reveal how that ability is captured by the network. Another line of work investigates the internal vector representations of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. They are trained to do a supervised task (e.g., part-of-speech tagging). If a probing classifier achieves high accuracy, it suggests that the vector representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated neural networks capturing aspects of syntactic structures (Shi et al., 2016; Blevins et al., 2018) or coreference (Tenney et al., 2018, 2019) w"
W19-4828,P18-1117,0,\N,Missing
