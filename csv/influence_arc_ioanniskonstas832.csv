2020.acl-main.137,S17-2097,0,0.0538475,"Missing"
2020.acl-main.137,S17-2091,0,0.0230797,"cytosol perhaps driving’, ’ increased elevation of CuZnSOD’). • is unlikely to help understand a text, e.g., (’DeepBind’, ’ was trained’, ’ on data from RNAcompete , CLIP - RIP - seq [ 10’) and (’microlepidopteran superfamilies’, ’ are heavily entombed’, ’ L:in amber’). We also randomly select representative samples from the 170k unfiltered and 67k filtered sentences from which the OIE extractions are sourced. The reason is that erroneous OIE extractions, e.g., not well-formed tuples, can guide a reader to informative passages in a text. We see similar errors as described by Schneider et al. (2017) and Groth et al. (2018), e.g., long sentences lead to incorrect extractions and errors in argument boundaries. To illustrate the complexity of sentences that an OIE system encounters in scientific texts, consider the following examples: • the arity of relations can be high, e.g., (49 tokens) “A large genome size tends to correlate with delayed mitotic and meiotic division [6–8] decreased plant invasiveness of disturbed sites [9] lower maximum photosynthetic rates in plants [2] and lower metabolic rates in mammals [10] and birds [11, 12].” (Warringer and Blomberg, 2006). • (’transcriptional co"
2020.acl-main.137,D19-5719,0,0.0598643,"Missing"
2020.acl-main.137,H05-1091,0,0.136724,"s for relations per sentence* for FOBIE (and per abstract or paragraph for the other datasets). Narrow Relation Extraction from scientific text Narrow RE entails identifying two or more related entities in a text and classifying the relation that holds between them. Early works on the combined task of Named Entity Recognition and labeling of relations between extracted entities used precomputed dependency features (Liu et al., 2013; Chen et al., 2015; Lin et al., 2016), word position embeddings (Zeng et al., 2014), or considered only the Shortest Dependency Path between two entities as input (Bunescu and Mooney, 2005; Santos et al., 2015; Zeng et al., 2015). Later work aimed to reduce errors propagated by pre-computed dependency features (Nguyen and Grishman, 2015), or by joint modeling of entities and relations (Miwa and Bansal, 2016). Poor performance of these RE systems on scientific texts has led to the development of domain-specific datasets2 . The S CIENCE IE dataset focuses on the extraction of 3 types of key-phrases, rather than Named Entities, and hyponymy and synonymy relations between these (Augenstein et al., 2017). The SemEval 2018 task 7 dataset focuses on 6 narrow relations between 7 entity"
2020.acl-main.137,W16-3001,0,0.0225107,"e RE systems on scientific texts has led to the development of domain-specific datasets2 . The S CIENCE IE dataset focuses on the extraction of 3 types of key-phrases, rather than Named Entities, and hyponymy and synonymy relations between these (Augenstein et al., 2017). The SemEval 2018 task 7 dataset focuses on 6 narrow relations between 7 entity types (G´abor et al., 2018). And the S CI ERC dataset focuses on 7 relation types, including co-reference, between 6 types of entities (Luan et al., 2018a). Top systems developed for both SemEval tasks adapt the LSTMbased approach of Miwa & Bansal (2016), combined with semi-supervised learning and ensembling (Ammar et al., 2018), as well as pre-trained concept embeddings (Luan et al., 2018b). 2.3 # Arg’s Rel’s R/doc S CIENCE IE SemEval 2017: 500 paragraphs from fulltext Computer Science, Material Science, and Physics journal articles, SemEval 2018: 500 abstracts within the domain of Computational Linguistics. S CI ERC: 500 abstracts from Artificial Intelligence conference and workshop proceedings. et al., 2005; Kim et al., 2009; N´edellec et al., 2013; Zhou et al., 2014). Many datasets focus primarily on a predefined set of biomedical relatio"
2020.acl-main.137,P15-1017,0,0.0162067,"8089 4716 9.4 Table 1: Number of arguments, relations and relations per instance for FOBIE, S CIENCE IE, the SemEval 2018 task 7 dataset and S CI ERC. R/doc stands for relations per sentence* for FOBIE (and per abstract or paragraph for the other datasets). Narrow Relation Extraction from scientific text Narrow RE entails identifying two or more related entities in a text and classifying the relation that holds between them. Early works on the combined task of Named Entity Recognition and labeling of relations between extracted entities used precomputed dependency features (Liu et al., 2013; Chen et al., 2015; Lin et al., 2016), word position embeddings (Zeng et al., 2014), or considered only the Shortest Dependency Path between two entities as input (Bunescu and Mooney, 2005; Santos et al., 2015; Zeng et al., 2015). Later work aimed to reduce errors propagated by pre-computed dependency features (Nguyen and Grishman, 2015), or by joint modeling of entities and relations (Miwa and Bansal, 2016). Poor performance of these RE systems on scientific texts has led to the development of domain-specific datasets2 . The S CIENCE IE dataset focuses on the extraction of 3 types of key-phrases, rather than N"
2020.acl-main.137,W16-3002,0,0.0274179,"Missing"
2020.acl-main.137,D11-1142,0,0.144849,"Missing"
2020.acl-main.137,C18-1289,0,0.275905,"-F OR’ and ‘S YNONYMY’, or too broad, i.e., an unbounded number of generic relations extracted from large, heterogeneous corpora (Niklaus et al., 2018), referred to as Open IE (OIE) (Etzioni et al., 2005; Banko et al., 2007). Narrow approaches to IE from scientific text (Augenstein et al., 2017; G´abor et al., 2018; Luan et al., 2018a) cover only a fraction of the information captured in a paper – usually what is within an abstract. It has been shown that scientific texts contain many unique relation types and, therefore, it is not feasible to create separate narrow IE classifiers for these (Groth et al., 2018). On the other hand, OIE systems are primarily developed for the Web and news-wire domain and have been shown to perform poorly on scientific texts. What laymen really need is a bit of both: the accuracy of narrow RE systems to extract central relations from scientific texts and the flexibility of an OIE system to capture a much larger fraction of the possible relations expressed in scientific texts. This work aims to enable rapid comprehension of a large scientific document by identifying a) the central concepts in a text and b) the most significant relations that govern these central concept"
2020.acl-main.137,N19-1370,0,0.0265145,"epts ‘safety’ and ‘efficiency’ involved in a T RADE -O FF relation. Then, by using the argument concepts of the relation as anchor points, we can explore further concepts and relations, e.g., ‘xylem’ in Figure 1. Uncovering these relations can elucidate the meaning of unfamiliar concepts to a layperson (Mausam, 2016). The SORE approach is hypothesized to reduce the number of uninformative extractions without limiting RE to a finite set of relations, which could generally benefit IE from scientific articles, e.g., materials discovery (Kononova et al., 2019) and drug-gene-mutation interactions (Jia et al., 2019). To address SORE we create the Focused Open Biological Information Extraction (FOBIE) dataset. FOBIE includes manually-annotated sentences that express explicit trade-offs, or syntactically similar relations, that capture the central concepts in full-text biology papers. We train a span-based RE model used in a strong scientific IE system (Luan et al., 2018a) to jointly extract these relation structures. We explore SORE and use the output of our model to filter the output of an OIE system (Saha and Mausam; Saha et al., 2017; Pal and Mausam, 2016; Christensen et al., 2011) on a corpus of biolo"
2020.acl-main.137,C16-1139,0,0.0265325,"Missing"
2020.acl-main.137,W09-1401,0,0.144775,"Missing"
2020.acl-main.137,D17-1018,0,0.0176752,"e set of unique trigger words and P ⊂ S. A trade-off is a binary relation, t |= o, with governor ∈ P and dependent ∈ S. A single trigger word p can be in n multiple relations. Def. 2. An argument-modifier is a directed binary relation a ∈ Am , where we omit the classification of a into a set of possible modification types ∈ m. An instance of a is then a tuple <governor, relation, dependent> where one of the arguments is related to a trigger word p, and both arguments ∈ S. 4.2 Baseline system We adapt a span-based approach that has been used previously for the tasks of co-reference resolution (Lee et al., 2017), Semantic Role Labeling (He et al., 2018), and scientific IE (Luan et al., 2018a). The use of span representations as classifier features enables end-to-end learning by propagating information between multiple tasks without increasing the complexity of inference. We train the S CI IE system (Luan et al., 2018a) on FOBIE to extract spans that constitute trigger words and key-phrases, as well as the binary relations between these spans. Figure 3 illustrates the input that we provide to S CI IE. All tokens are embedded using GloVe (Pennington et al., 2014) and ELMo embeddings (original) (Peters"
2020.acl-main.137,P16-1200,0,0.0140742,"e 1: Number of arguments, relations and relations per instance for FOBIE, S CIENCE IE, the SemEval 2018 task 7 dataset and S CI ERC. R/doc stands for relations per sentence* for FOBIE (and per abstract or paragraph for the other datasets). Narrow Relation Extraction from scientific text Narrow RE entails identifying two or more related entities in a text and classifying the relation that holds between them. Early works on the combined task of Named Entity Recognition and labeling of relations between extracted entities used precomputed dependency features (Liu et al., 2013; Chen et al., 2015; Lin et al., 2016), word position embeddings (Zeng et al., 2014), or considered only the Shortest Dependency Path between two entities as input (Bunescu and Mooney, 2005; Santos et al., 2015; Zeng et al., 2015). Later work aimed to reduce errors propagated by pre-computed dependency features (Nguyen and Grishman, 2015), or by joint modeling of entities and relations (Miwa and Bansal, 2016). Poor performance of these RE systems on scientific texts has led to the development of domain-specific datasets2 . The S CIENCE IE dataset focuses on the extraction of 3 types of key-phrases, rather than Named Entities, and"
2020.acl-main.137,D18-1360,0,0.0276407,"Missing"
2020.acl-main.137,S18-1125,0,0.0989163,"uistics, pages 1489–1500 c July 5 - 10, 2020. 2020 Association for Computational Linguistics functional demands that are traded off are usually abstract and domain-independent terms, such as ‘safety’ and ‘efficiency’ in Figure 1. A gap remains in quickly comprehending the central information in a text, e.g., the biological mechanisms that are used to manipulate a trade-off. Information Extraction (IE), and specifically Relation Extraction (RE), can improve the access to central information for downstream tasks (Santos et al., 2015; Zeng et al., 2014; Jiang et al., 2016; Miwa and Bansal, 2016; Luan et al., 2018a). However, the focus of current RE systems and datasets is either too narrow, i.e., a handful of semantic relations, such as ‘U SED -F OR’ and ‘S YNONYMY’, or too broad, i.e., an unbounded number of generic relations extracted from large, heterogeneous corpora (Niklaus et al., 2018), referred to as Open IE (OIE) (Etzioni et al., 2005; Banko et al., 2007). Narrow approaches to IE from scientific text (Augenstein et al., 2017; G´abor et al., 2018; Luan et al., 2018a) cover only a fraction of the information captured in a paper – usually what is within an abstract. It has been shown that scient"
2020.acl-main.137,P16-1105,0,0.260001,"for Computational Linguistics, pages 1489–1500 c July 5 - 10, 2020. 2020 Association for Computational Linguistics functional demands that are traded off are usually abstract and domain-independent terms, such as ‘safety’ and ‘efficiency’ in Figure 1. A gap remains in quickly comprehending the central information in a text, e.g., the biological mechanisms that are used to manipulate a trade-off. Information Extraction (IE), and specifically Relation Extraction (RE), can improve the access to central information for downstream tasks (Santos et al., 2015; Zeng et al., 2014; Jiang et al., 2016; Miwa and Bansal, 2016; Luan et al., 2018a). However, the focus of current RE systems and datasets is either too narrow, i.e., a handful of semantic relations, such as ‘U SED -F OR’ and ‘S YNONYMY’, or too broad, i.e., an unbounded number of generic relations extracted from large, heterogeneous corpora (Niklaus et al., 2018), referred to as Open IE (OIE) (Etzioni et al., 2005; Banko et al., 2007). Narrow approaches to IE from scientific text (Augenstein et al., 2017; G´abor et al., 2018; Luan et al., 2018a) cover only a fraction of the information captured in a paper – usually what is within an abstract. It has bee"
2020.acl-main.137,W15-1506,0,0.0123274,"w RE entails identifying two or more related entities in a text and classifying the relation that holds between them. Early works on the combined task of Named Entity Recognition and labeling of relations between extracted entities used precomputed dependency features (Liu et al., 2013; Chen et al., 2015; Lin et al., 2016), word position embeddings (Zeng et al., 2014), or considered only the Shortest Dependency Path between two entities as input (Bunescu and Mooney, 2005; Santos et al., 2015; Zeng et al., 2015). Later work aimed to reduce errors propagated by pre-computed dependency features (Nguyen and Grishman, 2015), or by joint modeling of entities and relations (Miwa and Bansal, 2016). Poor performance of these RE systems on scientific texts has led to the development of domain-specific datasets2 . The S CIENCE IE dataset focuses on the extraction of 3 types of key-phrases, rather than Named Entities, and hyponymy and synonymy relations between these (Augenstein et al., 2017). The SemEval 2018 task 7 dataset focuses on 6 narrow relations between 7 entity types (G´abor et al., 2018). And the S CI ERC dataset focuses on 7 relation types, including co-reference, between 6 types of entities (Luan et al., 2"
2020.acl-main.137,C18-1326,0,0.0961894,"information in a text, e.g., the biological mechanisms that are used to manipulate a trade-off. Information Extraction (IE), and specifically Relation Extraction (RE), can improve the access to central information for downstream tasks (Santos et al., 2015; Zeng et al., 2014; Jiang et al., 2016; Miwa and Bansal, 2016; Luan et al., 2018a). However, the focus of current RE systems and datasets is either too narrow, i.e., a handful of semantic relations, such as ‘U SED -F OR’ and ‘S YNONYMY’, or too broad, i.e., an unbounded number of generic relations extracted from large, heterogeneous corpora (Niklaus et al., 2018), referred to as Open IE (OIE) (Etzioni et al., 2005; Banko et al., 2007). Narrow approaches to IE from scientific text (Augenstein et al., 2017; G´abor et al., 2018; Luan et al., 2018a) cover only a fraction of the information captured in a paper – usually what is within an abstract. It has been shown that scientific texts contain many unique relation types and, therefore, it is not feasible to create separate narrow IE classifiers for these (Groth et al., 2018). On the other hand, OIE systems are primarily developed for the Web and news-wire domain and have been shown to perform poorly on sc"
2020.acl-main.137,C18-1194,0,0.0560533,"Missing"
2020.acl-main.137,P17-2050,0,0.127192,"iscovery (Kononova et al., 2019) and drug-gene-mutation interactions (Jia et al., 2019). To address SORE we create the Focused Open Biological Information Extraction (FOBIE) dataset. FOBIE includes manually-annotated sentences that express explicit trade-offs, or syntactically similar relations, that capture the central concepts in full-text biology papers. We train a span-based RE model used in a strong scientific IE system (Luan et al., 2018a) to jointly extract these relation structures. We explore SORE and use the output of our model to filter the output of an OIE system (Saha and Mausam; Saha et al., 2017; Pal and Mausam, 2016; Christensen et al., 2011) on a corpus of biology papers. Qualitative analyses show that the output of a narrow RE model can speed up expert analysis of trade-offs in biological texts, and be used to filter out both erroneous and uninformative OIE extractions. 2 2.1 Related work Open Information Extraction OIE systems use a set of handcrafted or learned extraction rules and rely on dependency features to extract open-domain relational tuples from text (Yu et al., 2017; Niklaus et al., 2018). As OIE systems rely on syntactic features they require little fine-tuning when a"
2020.acl-main.137,P15-1061,0,0.081736,"89 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1489–1500 c July 5 - 10, 2020. 2020 Association for Computational Linguistics functional demands that are traded off are usually abstract and domain-independent terms, such as ‘safety’ and ‘efficiency’ in Figure 1. A gap remains in quickly comprehending the central information in a text, e.g., the biological mechanisms that are used to manipulate a trade-off. Information Extraction (IE), and specifically Relation Extraction (RE), can improve the access to central information for downstream tasks (Santos et al., 2015; Zeng et al., 2014; Jiang et al., 2016; Miwa and Bansal, 2016; Luan et al., 2018a). However, the focus of current RE systems and datasets is either too narrow, i.e., a handful of semantic relations, such as ‘U SED -F OR’ and ‘S YNONYMY’, or too broad, i.e., an unbounded number of generic relations extracted from large, heterogeneous corpora (Niklaus et al., 2018), referred to as Open IE (OIE) (Etzioni et al., 2005; Banko et al., 2007). Narrow approaches to IE from scientific text (Augenstein et al., 2017; G´abor et al., 2018; Luan et al., 2018a) cover only a fraction of the information captur"
2020.acl-main.137,W17-5402,0,0.0511226,"Missing"
2020.acl-main.137,E12-2021,0,0.0122781,"rnals on ‘Biology’, ‘Evolutionary Biology, and ‘Systems Biology’. The selection of journals was made only to the extent that the articles focus on the biological domain. We retained the abstract, introduction, results, discussion and conclusion sections. We used spaCy3 to split the texts into sentences and identify POS tags and dependency structure. The FOBIE dataset contains only sentences that the RBS identified as expressing a T RADE - OFF relation. 3.2 Annotation The initial annotations extracted by the RBS were manually corrected and extended by a biology expert using the BRAT interface (Stenetorp et al., 2012). We define three relation types: T RADE - OFF, A RGUMENT-M ODIFIER and N OT- A -T RADE - OFF. The latter denotes phrases that are related to a trigger word, but not by a T RADE - OFF relation. These syntactically similar relations provide useful training signal as negative samples. Negative samples are important because possible trigger words can be contiguous, e.g., the phrase ‘negative correlation’ denotes a T RADE -O FF relation, whereas ‘correlation’ by itself does not. As a result, the annotation of training examples is harder, and lexical and syntactic patterns that correctly signify th"
2020.acl-main.137,W16-1307,0,0.107052,"et al., 2019) and drug-gene-mutation interactions (Jia et al., 2019). To address SORE we create the Focused Open Biological Information Extraction (FOBIE) dataset. FOBIE includes manually-annotated sentences that express explicit trade-offs, or syntactically similar relations, that capture the central concepts in full-text biology papers. We train a span-based RE model used in a strong scientific IE system (Luan et al., 2018a) to jointly extract these relation structures. We explore SORE and use the output of our model to filter the output of an OIE system (Saha and Mausam; Saha et al., 2017; Pal and Mausam, 2016; Christensen et al., 2011) on a corpus of biology papers. Qualitative analyses show that the output of a narrow RE model can speed up expert analysis of trade-offs in biological texts, and be used to filter out both erroneous and uninformative OIE extractions. 2 2.1 Related work Open Information Extraction OIE systems use a set of handcrafted or learned extraction rules and rely on dependency features to extract open-domain relational tuples from text (Yu et al., 2017; Niklaus et al., 2018). As OIE systems rely on syntactic features they require little fine-tuning when applied to different do"
2020.acl-main.137,Q17-1008,0,0.0133101,"ion types: T RADE - OFF, A RGUMENT-M ODIFIER and N OT- A -T RADE - OFF. The latter denotes phrases that are related to a trigger word, but not by a T RADE - OFF relation. These syntactically similar relations provide useful training signal as negative samples. Negative samples are important because possible trigger words can be contiguous, e.g., the phrase ‘negative correlation’ denotes a T RADE -O FF relation, whereas ‘correlation’ by itself does not. As a result, the annotation of training examples is harder, and lexical and syntactic patterns that correctly signify the relation are sparse (Peng et al., 2017). For simplicity’s sake, with some abuse of terminology, we refer to all such relations collectively as trade-offs. We found a substantial amount of arguments to be nested or in a non-projective relationship. In Figure 2 the prepositional phrase ‘in jumping’, conceptually refers to both central concept arguments of the relation, i.e., ‘the need for energy storage’ and ‘the presence of resilin’. We adopt the following annotation heuristic: prepositional phrases are treated as modifying phrases when they apply to multiple arguments (as is the case in Figure 2) or can be distinctly separated from"
2020.acl-main.137,D14-1162,0,0.0837061,"usly for the tasks of co-reference resolution (Lee et al., 2017), Semantic Role Labeling (He et al., 2018), and scientific IE (Luan et al., 2018a). The use of span representations as classifier features enables end-to-end learning by propagating information between multiple tasks without increasing the complexity of inference. We train the S CI IE system (Luan et al., 2018a) on FOBIE to extract spans that constitute trigger words and key-phrases, as well as the binary relations between these spans. Figure 3 illustrates the input that we provide to S CI IE. All tokens are embedded using GloVe (Pennington et al., 2014) and ELMo embeddings (original) (Peters et al., 2018). For a single sentence D = {w1 , ..., wn } all possible spans S = {s1 , ..., sN } are computed, which are withinsentence word sequences. The model deals with O(n4 ) possible combinations of spans, where n is the number of words in a sentence. Therefore, pruning is required to make the classification of span-pairs into relation labels tractable at both training and test time (Lee et al., 2017; He et al., 2018). First, a score φmr of how likely a span is mentioned in a relation is computed. These mention scores enable beam pruning the number"
2020.acl-main.137,N18-1202,0,0.0094206,", 2017), Semantic Role Labeling (He et al., 2018), and scientific IE (Luan et al., 2018a). The use of span representations as classifier features enables end-to-end learning by propagating information between multiple tasks without increasing the complexity of inference. We train the S CI IE system (Luan et al., 2018a) on FOBIE to extract spans that constitute trigger words and key-phrases, as well as the binary relations between these spans. Figure 3 illustrates the input that we provide to S CI IE. All tokens are embedded using GloVe (Pennington et al., 2014) and ELMo embeddings (original) (Peters et al., 2018). For a single sentence D = {w1 , ..., wn } all possible spans S = {s1 , ..., sN } are computed, which are withinsentence word sequences. The model deals with O(n4 ) possible combinations of spans, where n is the number of words in a sentence. Therefore, pruning is required to make the classification of span-pairs into relation labels tractable at both training and test time (Lee et al., 2017; He et al., 2018). First, a score φmr of how likely a span is mentioned in a relation is computed. These mention scores enable beam pruning the number of spans considered for relation classification with"
2020.acl-main.137,I17-1086,0,0.0222382,"e explore SORE and use the output of our model to filter the output of an OIE system (Saha and Mausam; Saha et al., 2017; Pal and Mausam, 2016; Christensen et al., 2011) on a corpus of biology papers. Qualitative analyses show that the output of a narrow RE model can speed up expert analysis of trade-offs in biological texts, and be used to filter out both erroneous and uninformative OIE extractions. 2 2.1 Related work Open Information Extraction OIE systems use a set of handcrafted or learned extraction rules and rely on dependency features to extract open-domain relational tuples from text (Yu et al., 2017; Niklaus et al., 2018). As OIE systems rely on syntactic features they require little fine-tuning when applied to different domains and the extraction rules work for a variety of relation types (Mausam, 2016). These properties can be especially useful on scientific texts where additional knowledge on unknown concepts can ease the textual comprehension for non-experts. Consider the example OIE extractions for ‘xylem’ in the top part of Figure 1. Existing OIE systems have been shown to perform significantly worse on the longer and more complex sentences found in scientific texts than on Wikiped"
2020.acl-main.137,D15-1203,0,0.0149069,"r abstract or paragraph for the other datasets). Narrow Relation Extraction from scientific text Narrow RE entails identifying two or more related entities in a text and classifying the relation that holds between them. Early works on the combined task of Named Entity Recognition and labeling of relations between extracted entities used precomputed dependency features (Liu et al., 2013; Chen et al., 2015; Lin et al., 2016), word position embeddings (Zeng et al., 2014), or considered only the Shortest Dependency Path between two entities as input (Bunescu and Mooney, 2005; Santos et al., 2015; Zeng et al., 2015). Later work aimed to reduce errors propagated by pre-computed dependency features (Nguyen and Grishman, 2015), or by joint modeling of entities and relations (Miwa and Bansal, 2016). Poor performance of these RE systems on scientific texts has led to the development of domain-specific datasets2 . The S CIENCE IE dataset focuses on the extraction of 3 types of key-phrases, rather than Named Entities, and hyponymy and synonymy relations between these (Augenstein et al., 2017). The SemEval 2018 task 7 dataset focuses on 6 narrow relations between 7 entity types (G´abor et al., 2018). And the S C"
2020.acl-main.137,C14-1220,0,0.103567,"58th Annual Meeting of the Association for Computational Linguistics, pages 1489–1500 c July 5 - 10, 2020. 2020 Association for Computational Linguistics functional demands that are traded off are usually abstract and domain-independent terms, such as ‘safety’ and ‘efficiency’ in Figure 1. A gap remains in quickly comprehending the central information in a text, e.g., the biological mechanisms that are used to manipulate a trade-off. Information Extraction (IE), and specifically Relation Extraction (RE), can improve the access to central information for downstream tasks (Santos et al., 2015; Zeng et al., 2014; Jiang et al., 2016; Miwa and Bansal, 2016; Luan et al., 2018a). However, the focus of current RE systems and datasets is either too narrow, i.e., a handful of semantic relations, such as ‘U SED -F OR’ and ‘S YNONYMY’, or too broad, i.e., an unbounded number of generic relations extracted from large, heterogeneous corpora (Niklaus et al., 2018), referred to as Open IE (OIE) (Etzioni et al., 2005; Banko et al., 2007). Narrow approaches to IE from scientific text (Augenstein et al., 2017; G´abor et al., 2018; Luan et al., 2018a) cover only a fraction of the information captured in a paper – usu"
2020.acl-main.137,S18-1111,0,\N,Missing
2020.acl-main.137,W13-2001,0,\N,Missing
2020.acl-main.455,W13-2322,0,0.0294654,"S (1) j=1...M i.e. the average embedding cosine distance to all arguments in the summary. Argument embeddings EiD and EjS are average embeddings of contentword tokens belonging to the arguments:6 Ei∗ = avg e∗k k∈A∗ i ,k6∈stops (2) ∗ ∈ {D, S}, “stops” denotes a list of stopwords. Fact-based weighting: We can represent the summary as of  and the document  two sequences S , facts F1D , F2D , · · · FND0 and F1S , F2S , · · · FM 0 and weight the i-th fact in the document by its average distance to facts in the summary: wif = 3 avg dfij (3) j∈1...M 0 We avoid using sentence-level MRs such as AMR (Banarescu et al., 2013), since current state-of-the-art performance of parsers is far behind compared to the simpler SRL task. 4 By concatenating, the information in each text can be embedded in each other through self-attention. This is useful since the summary sometimes contains additional and/or common-sense knowledge not captured in the document. 5 For example, in Fig. 1, ARG0, V, ARG1 in FACT1, and all the arguments in FACT2 are leaf arguments in the sentence, whereas ARG2 in FACT1 is not. 6 For example, in Fig. 1, “her” and “thanks” are two tokens directly attached to the argument ARG1 of FACT1. Thus, the embe"
2020.acl-main.455,P19-1264,0,0.0182834,"ERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 507"
2020.acl-main.455,J10-3005,0,0.133206,"ssages on social media”. (Teufel and van Halteren, 2004), or via crowdsourcing (Shapira et al., 2019; Hardy et al., 2019). However, evaluation based on a small human-labelled test set is noisy, time consuming, and costly. Xenouleas et al. (2019) propose a referenceless metric, which only checks properties of the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact Representation We represent facts in a sentence by adapting SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors"
2020.acl-main.455,N19-1423,0,0.021835,"and thus tend to downvote paraphrases. We propose a new evaluation metric based on content weighting, where we abstract away from the particular surface form of the target summary, but represent it as facts using Semantic Role Labelling (SRL). In this way, we aim to better capture the semantic correctness of a summary, i.e. be more sensitive to hallucinations and omissions.1 In particular, we weight the facts present in the source document according to the facts selected by a human-written summary. This alignment is conducted using contextual, rather than token-level, embeddings, e.g., BERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Good"
2020.acl-main.455,P19-1330,0,0.0360994,"Missing"
2020.acl-main.455,P18-2058,0,0.0125368,"ng SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors, recipients, time, place, etc (see Fig. 1). Following a simple observation that arguments can function as separate predicates themselves, we construct a hierarchical tree structure for the whole sentence. We create the tree meaning representa2 We use the SRL implementation of He et al. (2018) found in https://allennlp.org with 86.49 test F1 on the Ontonotes 5.0 dataset. Automatic Content Weighting We compute argument and fact weights by measuring the similarity of facts/arguments in the original document and the target summary based on their BERT word embeddings (for content words only) and their distance in the tree MR. We denote tokens its S as  D ofD a document D and  summary D D S S S t = t1 , t2 , · · · tn and t = t1 , t2 , · · · tSm . To get their corresponding contextual embeddings S 4 eD k and ek , we concatenate the two texts, feed them into a pre-trained BERT model (De"
2020.acl-main.455,P17-1044,0,0.0358549,"sed “abdicate” with 13 We computed HROUGE for B ERT S UM A BS using https://github.com/sheffieldnlp/highres. 5074 Model TC ONV S2S P T G EN B ERT S UM A BS CorrF/A Corr-F Corr-A 0.616 0.636 0.596 0.623 0.655 0.683 CorrF/A(L) Corr-F Corr-A 0.700 0.650 0.664 0.620 0.715 0.670 ROUGE R1 R2 RL 31.89 11.54 25.75 29.70 9.21 23.24 38.53 16.09 30.80 BERTScore P R F1 0.613 0.573 0.591 0.577 0.566 0.570 0.628 0.616 0.621 Table 4: Summarisation models evaluated using Corr-F/A on full test set, with ROUGE and BERTScore scores. Note that Corr-F/A(L) is Corr-F/A calculated using a lower-performing SRL tool (He et al., 2017, see Section 6.2). # Source Ground truth 1 B ERT S UM A BS TC ONV S2S Ground truth 2 B ERT S UM A BS P T G EN Ground truth 3 B ERT S UM A BS TC ONV S2S Ground truth 4 B ERT S UM A BS TC ONV S2S Summary Corr-F Corr-A BS-F1 Japan’s emperor Akihito has expressed his desire to abdicate in the next few years, public broadcaster NHK reports. Japan’s emperor Akihito is considering whether to become the next president of the country, reports say. 0.68 0.68 0.67 Japan’s emperor Akihito has announced that he will step down in the Japanese capital, Tokyo. 0.81 0.71 0.67 Dick Advocaat has resigned as Sun"
2020.acl-main.455,D19-1051,0,0.0534601,"Missing"
2020.acl-main.455,W04-1013,0,0.409006,"9), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Lin, 2004), and thus tend to downvote paraphrases. We propose a new evaluation metric based on content weighting, where we abstract away from the particular surface form of the target summary, but represent it as facts using Semantic Role Labelling (SRL). In this way, we aim to better capture the semantic correctness of a summary, i.e. be more sensitive to hallucinations and omissions.1 In particular, we weight the facts present in the source document according to the facts selected by a human-written summary. This alignment is conducted using contextual, rather than token-level, embeddings, e.g., BERT"
2020.acl-main.455,D19-1387,0,0.102108,"summarisation compresses long textual documents into short summaries while retaining the most important information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Li"
2020.acl-main.455,D18-1206,0,0.449005,"). 1 Introduction Text summarisation compresses long textual documents into short summaries while retaining the most important information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using e"
2020.acl-main.455,N04-1019,0,0.198524,"dressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5071–5081 c July 5 - 10, 2020. 2020 Association for Computational Linguistics tion (MR) from the list of facts by choosing the fact with the largest coverage as the root and recursively build sub-trees by replacing arguments with their corresponding sub-facts (ARG2 in FACT1 is replaced by FACT2 in Fig. 1).3 FACT1-tweet: [ARG0: the queen] has [V: tweeted] [ARG1: her tha"
2020.acl-main.455,J05-1004,0,0.01223,"the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact Representation We represent facts in a sentence by adapting SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors, recipients, time, place, etc (see Fig. 1). Following a simple observation that arguments can function as separate predicates themselves, we construct a hierarchical tree structure for the whole sentence. We create the tree meaning representa2 We use the SRL implementation of He et al. (2018) found in ht"
2020.acl-main.455,D18-1437,0,0.0205634,"ng and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Lin, 2004), and thus tend to downvote paraphrases. We propose a new evaluation metric based on content weighting, where we abstract away from the particular surface form of the target summary, but represent it as facts using Semantic Role Labelling (SRL). In this way, we aim to better capture the semantic correctness of a summary,"
2020.acl-main.455,E17-2007,0,0.0468259,"ant information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate against, e.g. ROUGE using exact n-gram overlap (Lin, 2004), and thus tend to downvote paraphrases. We propose a new evaluation metric based on"
2020.acl-main.455,D19-1320,0,0.0733442,"crowdsourcing (Shapira et al., 2019; Hardy et al., 2019). However, evaluation based on a small human-labelled test set is noisy, time consuming, and costly. Xenouleas et al. (2019) propose a referenceless metric, which only checks properties of the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact Representation We represent facts in a sentence by adapting SRL (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. Given a list of parsed propositions for a sentence,2 each predicate-argument structure is considered as one separate fact, where the predicate stands for the event and its arguments are mapped to actors, recipients, time, place, etc (see Fig. 1). Following a simple"
2020.acl-main.455,P17-1099,0,0.349363,"Hardy et al. (2019). 1 Introduction Text summarisation compresses long textual documents into short summaries while retaining the most important information from the source. In contrast to extractive summarisation, which directly copies the most relevant fragments, abstractive summarization retains the most important facts and expresses them via paraphrasing, aggregating and even inferring new facts. Recent advances in neural decoders led to a number of single-document summarisation systems that exhibit some level of abstraction in their outputs, usually in the simplest form of paraphrasing (See et al. (2017); Narayan et al. (2018); Liu and Lapata (2019), inter alia). Evaluating abstractive summarisation remains an open challenge (Schluter, 2017; Kry´sci´nski et al., 2019): First, decoders are amenable to pathogeniessuch as hallucination and/or omission of important information, which are hard to capture using existing evaluation metrics (Cao et al., 2018; Rohrbach et al., 2018; Duˇsek et al., 2020). Second, most datasets used for abstractive summarisation only contain a single reference summary, e.g. (Narayan et al., 2018; V¨olske et al., 2017), which most existing automatic metrics evaluate agai"
2020.acl-main.455,D19-1116,0,0.103063,"using contextual, rather than token-level, embeddings, e.g., BERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting"
2020.acl-main.455,W04-3254,0,0.241663,"Missing"
2020.acl-main.455,W17-4508,0,0.0528854,"Missing"
2020.acl-main.455,D19-1618,0,0.0129665,"M-LOC on social media] SRL Propositions Tree MR FACT1-tweet 3.2 ARG0 V ARG1 ARG2 the queen had tweeted her thanks FACT2-send ARG0 R-ARG0 V ARG1 ARGM-LOC people who sent her 90th birthday messages on social media Figure 1: List of SRL propositions and corresponding tree MR with two facts for the sentence “The queen has tweeted her thanks to people who sent her 90th birthday messages on social media”. (Teufel and van Halteren, 2004), or via crowdsourcing (Shapira et al., 2019; Hardy et al., 2019). However, evaluation based on a small human-labelled test set is noisy, time consuming, and costly. Xenouleas et al. (2019) propose a referenceless metric, which only checks properties of the summary, not its relation to the original document. Sun and Nenkova (2019) compare average token and sentence ELMo embeddings against the document and claim good (system-level) correlations. Another option to avoid reference bias is question-based evaluation, either elicited manually (Clarke and Lapata, 2010; Narayan et al., 2018) or automatically (Scialom et al., 2019). However, it requires reference summaries as base for generating questions, thus only checking the summary contents indirectly. 3 Content Weighting 3.1 Fact R"
2020.acl-main.455,D19-1053,0,0.0162519,"embeddings, e.g., BERT (Devlin et al., 2019). For evaluation, we measure whether an automatically generated summary is able to capture the same facts as the target. We also show that the computed weights correlate well with human perception. Our code is available at https://github. com/XinnuoXu/CorrFA_for_Summarizaion. 2 Related Work The problem of reference bias has been addressed in several ways. First, metrics based on tokenlevel or wider context embedding similarities which aim to better capture paraphrases but remain largely word-oriented, e.g. (Sun and Nenkova, 2019; Zhang et al., 2019; Zhao et al., 2019; Clark et al., 2019). Goodrich et al. (2019) come close to our approach by using entity and relation extraction, but their approach is limited to texts that lend themselves to be represented by RDF triples. An alternative is manual evaluation against the source document. This entails selecting content either using domain experts, e.g., the P YRAMID method (Nenkova and Passonneau, 2004), factoids 1 Note that we do not make any claims about fluency, which we assume is less of a problem for neural text generation. 5071 Proceedings of the 58th Annual Meeting of the Association for Computational L"
2020.acl-main.682,D16-1203,0,0.0262625,"e that is mapped to a scene graph composed of objects represented in terms of abstract and situated attributes. Introduction Several grounded language learning tasks have been proposed to capture perceptual aspects of language (Shekhar et al., 2017; Hudson and Manning, 2019; Suhr et al., 2019; Agrawal et al., 2018). However, the advances in this field have been primarily driven by the final performance measures and less on the grounding capability of the models. In fact, in some cases, high-performance models exploit dataset biases to achieve high scores on the final task (Zhang et al., 2016; Agrawal et al., 2016). In the literature, several methods have been proposed to analyse what kind of information is captured by neural network representations (K´ad´ar et al., 2017; Belinkov and Glass, 2019). Most of these works examine the hidden state representations learned by models trained on only textual data. However, many aspects of human semantic representations are grounded in perceptual experience (Andrews et al., 2009; Riordan and Jones, 2011). This paper explores the idea that visually grounded representations ought to be a result of systematic composition of grounded representations (Harnad, 1990). F"
2020.acl-main.682,J17-4003,0,0.0687773,"Missing"
2020.acl-main.682,W18-5446,0,0.0639334,"Missing"
2020.acl-main.728,D17-1238,1,0.873663,"Missing"
2020.acl-main.728,D14-1162,0,0.0892691,"a relevance score of the options during fine-tuning (casting it as multi-label classification). 3 Implementation 1 We use PyTorch (Paszke et al., 2017) for our exper2 iments . Following Anderson et al. (2018), we use bottom-up features of 36 proposals from images using a Faster-RCNN (Ren et al., 2015) pre-trained on Visual Genome (Krishna et al., 2017) to get a bag of object-level 2048-d image representations. Input question and candidate options are tokenized to a maximum length of 20 while the conversational history to 200. Token embeddings in text are initialized with 300-d GloVe vectors (Pennington et al., 2014) and shared among all text-based encoders. The RNN encodings are implemented using LSTMs (Hochreiter and Schmidhuber, 1997). 1 https://pytorch.org/ Code available at https://github.com/ shubhamagarwal92/visdial_conv 8185 2 We use the Adam optimizer (Kingma and Ba, 2015) both for training and fine-tuning. More training details can be found in Appendix A. where selecting one answer using sparse annotation 4 is an easier task and fine-tuning more difficult. 4.4 4 Task Description 4.1 Dataset We use VisDial v1.0 for our experiments and eval3 uation. The dataset contains 123K/2K/8K dialogs for trai"
2020.acl-main.728,C18-1104,0,0.110062,"Missing"
2020.acl-main.728,D19-1514,0,0.0355175,"ovikova et al., 2017; Reiter, 2018). As a first step, BERT score (Zhang et al., 2019) could be explored to measure ground-truth similarity replacing the noisy NDCG annotations of semantic equivalence. 8 Conclusion and Future Work In sum, this paper shows that we can get SOTA performance on the VisDial task by using transformerbased models with Guided-Attention (Yu et al., 2019b), and by encoding dialog history and finetuning we can improve results even more. Of course, we expect pre-trained visual BERT models to show even more improvements on this task, e.g. Vilbert (Lu et al., 2019), LXMert (Tan and Bansal, 2019), UNITER (Chen et al., 2019) etc. However, we also show the limitations of this shared task in terms of dialog phenomena and evaluation metrics. We, thus, argue that progress needs to be carefully measured by posing the right task in terms of dataset and evaluation procedure. Acknowledgments We thank the anonymous reviewers for their insightful comments. Shubham would like to thank Raghav Goyal for the discussions during ‘Pikabot’ submission to Visual Dialog Challenge 2018. This work received continued support by Adobe Research gift funding for further collaboration. This research also receive"
2020.acl-main.728,D18-1432,1,0.907846,"Missing"
2020.acl-main.728,D17-1137,0,\N,Missing
2020.acl-main.728,J18-3002,0,\N,Missing
2020.acl-main.728,L18-1683,1,\N,Missing
2020.acl-main.728,W18-5033,1,\N,Missing
2020.acl-main.728,N19-1058,0,\N,Missing
2020.acl-main.728,N19-1423,0,\N,Missing
2020.acl-main.728,N16-1014,0,\N,Missing
2020.coling-main.95,I17-1014,0,0.0561196,"Missing"
2020.coling-main.95,D15-1293,0,0.0180715,"the action of sailing, etc) (Barsalou, 2008). This simulation process is called perceptual simulation. Therefore, it is no wonder that recent trends in learning conceptual representations adopt multi-modal and holistic approaches (Bruni et al., 2014) wherein abstract distributional lexical representations (Landauer and Dumais, 1997; Laurence and Margolis, 1999) learned from text corpora are augmented or refined with perceptual information for concrete and context-aware representations built from visual (Kiela et al., 2018; Lazaridou et al., 2015), olfactory (Kiela et al., 2015), or auditory (Kiela and Clark, 2015) modalities. Language games between AI agents, inspired by Wittgenstein’s Language Games among humans (Wittgenstein et al., 1953), are an excellent test bed for such approaches since concepts are expected to emerge when agents are required to communicate to solve specific tasks in specific environments. GuessWhat?! (De Vries et al., 2017) is a prototypical language game of this kind: a Guesser has to identify a target object in a scene represented as an image by asking questions to an Oracle. Learning to ground pixels of the scene into object representations that are relevant for the object ca"
2020.coling-main.95,P15-2038,0,0.0179683,"at” (e.g., what a boat looks like, the action of sailing, etc) (Barsalou, 2008). This simulation process is called perceptual simulation. Therefore, it is no wonder that recent trends in learning conceptual representations adopt multi-modal and holistic approaches (Bruni et al., 2014) wherein abstract distributional lexical representations (Landauer and Dumais, 1997; Laurence and Margolis, 1999) learned from text corpora are augmented or refined with perceptual information for concrete and context-aware representations built from visual (Kiela et al., 2018; Lazaridou et al., 2015), olfactory (Kiela et al., 2015), or auditory (Kiela and Clark, 2015) modalities. Language games between AI agents, inspired by Wittgenstein’s Language Games among humans (Wittgenstein et al., 1953), are an excellent test bed for such approaches since concepts are expected to emerge when agents are required to communicate to solve specific tasks in specific environments. GuessWhat?! (De Vries et al., 2017) is a prototypical language game of this kind: a Guesser has to identify a target object in a scene represented as an image by asking questions to an Oracle. Learning to ground pixels of the scene into object representation"
2020.coling-main.95,N18-1038,0,0.103248,"their memory and are associated with the concept of “boat” (e.g., what a boat looks like, the action of sailing, etc) (Barsalou, 2008). This simulation process is called perceptual simulation. Therefore, it is no wonder that recent trends in learning conceptual representations adopt multi-modal and holistic approaches (Bruni et al., 2014) wherein abstract distributional lexical representations (Landauer and Dumais, 1997; Laurence and Margolis, 1999) learned from text corpora are augmented or refined with perceptual information for concrete and context-aware representations built from visual (Kiela et al., 2018; Lazaridou et al., 2015), olfactory (Kiela et al., 2015), or auditory (Kiela and Clark, 2015) modalities. Language games between AI agents, inspired by Wittgenstein’s Language Games among humans (Wittgenstein et al., 1953), are an excellent test bed for such approaches since concepts are expected to emerge when agents are required to communicate to solve specific tasks in specific environments. GuessWhat?! (De Vries et al., 2017) is a prototypical language game of this kind: a Guesser has to identify a target object in a scene represented as an image by asking questions to an Oracle. Learning"
2020.coling-main.95,P18-1085,0,0.0146813,"zi ))), (2) where η is the minimum margin between two components: i) the distance between the perceptual embedding vi and its reconstruction Dθ (zi ), and ii) the distance between the perceptual embedding vj of a randomly sampled object oj ∈ O¬ci and the reconstruction Dθ (zi ). By doing so, we enforce each object representation to be representative of its category given a specific context by locally contrasting it to another object of a different category in the same scene. Note that this is strikingly different from previous approaches employing a max-margin loss (Elliott and K´ad´ar, 2017; Kiros et al., 2018) where “negative” objects are arbitrarily sampled from other scenes in the same batch. Imagining at inference time. Differently from the category embeddings c employed by all previous work, our imagination embeddings z do not depend on gold category labels at inference time, while still being context-aware and category-aware. In fact, once parameters φ have been learned, the encoder Eφ contains all the information needed to distill embeddings z independently of LIMG , which is necessary only at training time. We consider imagination the ability of the model of generating latent representations"
2020.coling-main.95,N15-1016,0,0.0447796,"Missing"
2020.coling-main.95,N19-1265,0,0.155831,"Missing"
2020.coling-main.95,P19-1646,0,0.133912,"ich the game is played (contextaware). As the scene S is an image, it is natural to associate each object oi ∈ O with a perceptual embedding, i.e., a vector vi ∈ RdO extracted from the penultimate layer of a pretrained vision model (e.g. ResNet-152 (Shekhar et al., 2019)) based on their bounding box.1 However, these representations are not sufficient as they are neither context-aware nor category-aware, i.e., they ignore other objects in the scene and do not leverage their category information. GDSE and other recent approaches (De Vries et al., 2017; Shekhar et al., 2019; Zhuang et al., 2018; Shukla et al., 2019) coped with the second issue by introducing category embeddings as dC -dimensional continuous representations ck ∈ RdC for k = 1, . . . , K. Once learned, a category embedding c is then concatenated to an 8-dimensional feature vector si derived from the object bounding box (cf. De Vries et al. (2017)). While these embeddings partially solve category-awareness, they are not object-aware. For instance, the embedding for the object category “apple” will be the same regardless of a particular object to be a red or green apple, i.e., most likely a centroid representation of the objects seen only du"
2020.coling-main.95,2020.acl-main.682,1,0.869596,"e Bastianelli1 , Andrea Vanzo1 , and Oliver Lemon1 1 Heriot-Watt University, Edinburgh, UK 2 University of California, Los Angeles, USA 3 Carnegie Mellon University, Pittsburgh, USA 1 {as247,i.konstas,a.vanzo,e.bastianelli,o.lemon}@hw.ac.uk 2 aver@cs.ucla.edu, 3 ybisk@cs.cmu.edu Abstract In visual guessing games, a Guesser has to identify a target object in a scene by asking questions to an Oracle. An effective strategy for the players is to learn conceptual representations of objects that are both discriminative and expressive enough to ask questions and guess correctly. However, as shown by Suglia et al. (2020), existing models fail to learn truly multi-modal representations, relying instead on gold category labels for objects in the scene both at training and inference time. This provides an unnatural performance advantage when categories at inference time match those at training time, and it causes models to fail in more realistic “zeroshot” scenarios where out-of-domain object categories are involved. To overcome this issue, we introduce a novel “imagination” module based on Regularized Auto-Encoders, that learns context-aware and category-aware latent embeddings without relying on category label"
2020.lrec-1.255,P17-2054,0,0.0608124,"Missing"
2020.lrec-1.255,C18-1289,0,0.015474,"ntactic patterns, to determine the argument phrases (Etzioni et al., 2011). It is expected that this enables flexibility of extracting trade-offs from a variety of domains (Etzioni et al., 2005; Banko et al., 2007). However, the handcrafted or learned syntactic patterns rely on how well input sentences are handled (L´echelle and Langlais, 2018; Niklaus et al., 2018). Sources of issues may include the length and complexity of sentences, the use of pronouns as subjects, the use of abbreviations as adjectives, handling prepositional phrases, and dealing with co-reference (Schneider et al., 2017; Groth et al., 2018). Machine learning approaches can improve recall for the long tail of patterns that will have to be identified (Mausam, 2016). By manually relabeling the output of our RBS system we rectify errors in argument boundaries and determine the correct relation label. Only few similar datasets exist for scientific IE: S CIENCE IE Semeval 2017 task 10 introduced the S CI ENCE IE dataset, consisting of 500 paragraphs taken from full-text scientific documents in the domains of Computer Science, Material Science and Physics journal articles (Augenstein et al., 2017). It contains annotations of keyphrases"
2020.lrec-1.255,kingsbury-palmer-2002-treebank,0,0.167758,"s while a T RADE - OFF entails a negative correlation. Many argument phrases are found to be nested and can be broken down into an argument and a modifier, e.g., ‘ontogenetic trajectories’ modifies ‘tolerance’ and ‘resistance’ in Figure 2. We do not indicate specific types of modification, such as temporal expressions or phrases that indicate a location, used in Semantic Role Labeling (SRL). The reason is that trigger words can be either nouns or verbs, e.g., ‘balance/NOUN’ and ‘trade-off /VERB’ in Figure 3, hence the classes of modifier labels found in common SRL frameworks such as Propbank (Kingsbury and Palmer, 2002) are not always appropriate. Instead, as a general rule we include the words that indicate the type of modifier during annotation of modifying phrases. We adopt the heuristic that prepositional phrases (PPs) heading a coordinating clause, are treated as modifying phrases when they apply to word-level arguments of a trade-off, as in the top example in Figure 2. In the case of coordination of PPs that contain arguments of a trade-off, we consider each PP as a whole argument, as in the bottom example in Figure 2. Similarly, when nested phrases in an argument can be distinctly separated by punctua"
2020.lrec-1.255,L18-1323,0,0.067731,"Missing"
2020.lrec-1.255,D17-1018,0,0.0394259,"Missing"
2020.lrec-1.255,D18-1360,0,0.270078,"6). Consider again the example of ‘bleaching’: the purely biological term does not carry a notion of teleology, in contrast to the non-biologist interpretations. The active verbs that are associated to engineering functions are not always used in biological texts (Kaiser et al., 2012; Kaiser et al., 2014). As a result, each function of interest requires a separate classifier (Glier, 2013) or its own set of extraction rules that work for a specific domain of texts (Etzioni, 2007; Christensen et al., 2011). Training data for each function would accordingly require annotation by a domain expert (Luan et al., 2018). Instead, this work aims to extract trade-off relations that are central to many biological texts and capture information at an abstraction level that is domain-independent (Vincent, 2016). These trade-offs, and the abstract concepts captured in their arguments, provide an initial filter for information retrieval and support for within-domain search for biological information (Kruiper et al., 2018). 2.2. Scientific Information Extraction Information Extraction (IE) from scientific text can (1) improve access to scientific information, beyond the possibilities of standard search engines (G´abo"
2020.lrec-1.255,C18-1326,0,0.0248064,"(RBS). Like traditional Relation Extraction (RE) systems this RBS relies on matching specific words to find a T RADE -O FF or similar relation (Sarawagi, 2007). Similar to Open IE (OIE) systems it relies on unlexicalized grammatical structures, e.g., syntactic patterns, to determine the argument phrases (Etzioni et al., 2011). It is expected that this enables flexibility of extracting trade-offs from a variety of domains (Etzioni et al., 2005; Banko et al., 2007). However, the handcrafted or learned syntactic patterns rely on how well input sentences are handled (L´echelle and Langlais, 2018; Niklaus et al., 2018). Sources of issues may include the length and complexity of sentences, the use of pronouns as subjects, the use of abbreviations as adjectives, handling prepositional phrases, and dealing with co-reference (Schneider et al., 2017; Groth et al., 2018). Machine learning approaches can improve recall for the long tail of patterns that will have to be identified (Mausam, 2016). By manually relabeling the output of our RBS system we rectify errors in argument boundaries and determine the correct relation label. Only few similar datasets exist for scientific IE: S CIENCE IE Semeval 2017 task 10 int"
2020.lrec-1.255,N18-1202,0,0.0438635,"Missing"
2020.lrec-1.255,L16-1294,0,0.0160016,"uter Science, Material Science and Physics journal articles (Augenstein et al., 2017). It contains annotations of keyphrases that are classified as materials, processes or tasks. Furthermore, hyponym- and synonym-relations between the keyphrases are captured. S EMEVAL 2018 The manually annotated Semeval 2018 task 7 dataset contains 6 relations types that are noted to occur regularly in scientific abstracts; usage, result, model, part-whole, topic and comparison (G´abor et al., 2018). It contains 500 abstracts from the domain of Computational Linguistics and draws on the ACL RD-TEC 2.0 corpus (Qasemizadeh and Schumann, 2016) for entity annotation; technology and method, tool and library, language resource, language resource product, measures and measurements, models and other. Augenstein et al. (2017) found that the S CIENCE IE dataset contains a significantly higher proportion of long keyphrases in comparison to the ACL RD-TEC 2.0 corpus. This is likely due to the different characteristics of sentences taken from abstracts and those in the main body. S CI ERC The S CI ERC dataset consists of 500 abstracts taken from Artificial Intelligence conference and workshop proceedings (Luan et al., 2018). It extends the e"
2020.lrec-1.255,E17-1110,0,0.0294932,"port for within-domain search for biological information (Kruiper et al., 2018). 2.2. Scientific Information Extraction Information Extraction (IE) from scientific text can (1) improve access to scientific information, beyond the possibilities of standard search engines (G´abor et al., 2018; Gupta and Manning, 2011), (2) provide valuable insight into research areas (Tsai et al., 2013; Luan et al., 2018), (3) enable to quickly learn facts on unknown concepts, as extractions can provide a summary view for readers (Mausam, 2016), and (4) augment existing Knowledge Bases (KB) from unlabeled text (Quirk and Poon, 2017). However, annotating scientific text is non-trivial, as it requires domainspecific knowledge from experts. Large-scale crowdsourcing (Tratz, 2019) or human-in-the-loop (He et al., 2016) efforts can be unreliable for such tasks. To alleviate the manual labeling of FOBIE by a domain expert, we developed a simple Rule-Based System (RBS). Like traditional Relation Extraction (RE) systems this RBS relies on matching specific words to find a T RADE -O FF or similar relation (Sarawagi, 2007). Similar to Open IE (OIE) systems it relies on unlexicalized grammatical structures, e.g., syntactic patterns"
2020.lrec-1.255,W17-5402,0,0.0571447,"Missing"
2020.lrec-1.255,E12-2021,0,0.0213942,"ning whether the sentence expresses a trade-off requires additional reasoning and often domainspecific knowledge. Considering the large variety of syntactic structures that do indicate a trade-off, as well as the required expertise in biology, a RBS is not suited for our task. Instead, the output of our simple RBS is used to speed up manual annotation. 3.2. Dataset annotation Out of the 10,000 documents, we retain only the sentences that were annotated with a T RADE -O FF relation by the RBS. Using the BRAT3 interface a biology expert manually corrected argument boundaries and relation types (Stenetorp et al., 2012). During manual annotation we correct the relation label for trigger words, handle negation and identify the boundaries of argument and modifier phrases. We annotate binary relations that constitute non-projective graphs of one or more n-ary relations in a sentence. Each binary relation is a triple <governor, relation, dependent&gt; where: • governor is either a trigger word or a modifying phrase. • relation indicates the type of relation – T RADE - OFF, A RGUMENT-M ODIFIER or N OT- A -T RADE - OFF. • dependent is an argument phrase. Three relation types were used: T RADE - OFF, A RGUMENTM ODIFIE"
2020.lrec-1.255,D19-5901,0,0.0143477,"entific text can (1) improve access to scientific information, beyond the possibilities of standard search engines (G´abor et al., 2018; Gupta and Manning, 2011), (2) provide valuable insight into research areas (Tsai et al., 2013; Luan et al., 2018), (3) enable to quickly learn facts on unknown concepts, as extractions can provide a summary view for readers (Mausam, 2016), and (4) augment existing Knowledge Bases (KB) from unlabeled text (Quirk and Poon, 2017). However, annotating scientific text is non-trivial, as it requires domainspecific knowledge from experts. Large-scale crowdsourcing (Tratz, 2019) or human-in-the-loop (He et al., 2016) efforts can be unreliable for such tasks. To alleviate the manual labeling of FOBIE by a domain expert, we developed a simple Rule-Based System (RBS). Like traditional Relation Extraction (RE) systems this RBS relies on matching specific words to find a T RADE -O FF or similar relation (Sarawagi, 2007). Similar to Open IE (OIE) systems it relies on unlexicalized grammatical structures, e.g., syntactic patterns, to determine the argument phrases (Etzioni et al., 2011). It is expected that this enables flexibility of extracting trade-offs from a variety of"
2020.lrec-1.255,J05-1004,0,\N,Missing
2020.lrec-1.255,I11-1001,0,\N,Missing
2020.lrec-1.255,D16-1258,0,\N,Missing
2020.lrec-1.255,S18-1111,0,\N,Missing
2020.ngt-1.1,W19-5301,0,0.0795298,"Missing"
2020.ngt-1.1,N19-1423,0,0.00776584,"o 15 system submission papers. We elicted two double-blind reviews for each submission, avoiding conflicts of interest. With regards to thematology there were 8 papers with a focus on Natural Language Generation and 8 with the application of Machine Translation 1 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 1–9 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d in mind. The underlying emphasis across submissions was placed this year on capitalizing on the use of pre-training models (e.g., BERT; (Devlin et al., 2019) especially for low-resource datasets. The quality of the accepted publications was very high; there was a significant drop in numbers though in comparison to last year (36 accepted papers from 68 submissions) which is most likely due to the extra overhead on conducting research under lockdown policies sanctioned globally due to COVID19 pandemic. 3 GPU is relatively small compared to the NVIDIA V100 GPU, but the newer Turing architecture introduces support for 4-bit and 8-bit integer operations in Tensor Cores. In practice, however, participants used floating-point operations on the GPU even t"
2020.ngt-1.1,D13-1176,0,0.0312763,"Second, we describe the results of the three shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document-level generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language and 3) STAPLE task: creation of as many possible translations of a given input text. This last shared task was organised by Duolingo. 1 Introduction 2 Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are the workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 4th Workshop on Neural Machine Translation and Generation (WNGT 2020) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization, NLG from structured data, dialog response generation, among others). Overall, the workshop was held with two goals. First, it aimed to synthesize the current state of"
2020.ngt-1.1,W04-1013,0,0.0806851,"of parameters and 8-bit quantization. OpenNMT’s small lower-quality models have low CPU RAM and Docker image size; UEdin is Pareto-optimal for higher-quality models. OpenNMT was the only team to optimize for these metrics in their system description. In their multicore CPU submission, OpenNMT shared memory amongst processes while other participants simply used multiple processes with copies of the model. 4 4.1 Evaluation Measures We employ standard evaluation metrics for the tasks above along two axes following (Hayashi et al., 2019): Textual Accuracy: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for surface-level texutal accuracy compared to reference summaries. Document Generation and Translation Task Following the previous workshop, we continued with the shared task of document-level generation and translation. This task is motivated as the central evaluation testbed for document-level generation systems with different types of inputs by providing parallel dataset consisting of structured tables and text in two languages. We host various tracks within the testbed based on input and output constraints and investigate and contrast the system differences. In particular, we"
2020.ngt-1.1,2020.ngt-1.28,0,0.0265263,"tput, but in certain cases, it is desirable to have many possible translations of a given input text. At Duolingo, the world’s largest online language-learning platform,7 we grade translationbased challenges with sets of human-curated acceptable translation options. Given the many ways of expressing a piece of text, these sets are slow to create, and may be incomplete. This process is ripe for improvement with the aid of rich multi-output translation and paraphrase systems. To this end, we introduce a shared task called STAPLE: Simultaneous Translation and Paraphrasing for Language Education (Mayhew et al., 2020). 4.4 5.1 4.3 Baselines We prepared two baselines for different tracks: FairSeq-19 We use FairSeq (Ng et al., 2019) (WMT’19 single model6 ) for MT and MT+NLG tracks. Submitted Systems One team participated in the task, who focused on the German-English MT track of the task. In this shared task, participants are given a training set consisting of 2500 to 4000 English sentences (or prompts), each of which is paired with a list of comprehensive translations in the target language, weighted and ordered by normalized learner response frequency. At test time, participants are given 500 English promp"
2020.ngt-1.1,D18-1325,0,0.021768,"guage, weighted and ordered by normalized learner response frequency. At test time, participants are given 500 English prompts, and are required to produce the set of comprehensive translations for each prompt. We also provide a high-quality automatic reference translation for each prompt, in the event that a participant wants to work on paraphrase-only approaches. The target languages were Hungarian, Japanese, Korean, Portuguese, and Vietnamese. Team FJWU developed a system around Transformer-based sequence-to-sequence model. Additionally, the model employed hierarchical attention following (Miculicich et al., 2018) for both encoder and decoder to account for the documentlevel context. The system was trained in a twostage process, where a base (sentence-level) NMT model was trained followed by the training of hierarchcal attention networks component. To handle the scarcity of in-domain translation data, they experimented with upsizing the in-domain data up to three times to construct training data. Their ablation experiments showed that this upsizing of in-domain data is effective at increasing the BLEU score. 4.5 Task Description 5.2 Submitted Systems There were 20 participants who submitted to the deve"
2020.ngt-1.1,W19-5333,0,0.0306894,"Missing"
2020.ngt-1.1,P02-1040,0,0.107839,"guage. mostly driven by the number of parameters and 8-bit quantization. OpenNMT’s small lower-quality models have low CPU RAM and Docker image size; UEdin is Pareto-optimal for higher-quality models. OpenNMT was the only team to optimize for these metrics in their system description. In their multicore CPU submission, OpenNMT shared memory amongst processes while other participants simply used multiple processes with copies of the model. 4 4.1 Evaluation Measures We employ standard evaluation metrics for the tasks above along two axes following (Hayashi et al., 2019): Textual Accuracy: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for surface-level texutal accuracy compared to reference summaries. Document Generation and Translation Task Following the previous workshop, we continued with the shared task of document-level generation and translation. This task is motivated as the central evaluation testbed for document-level generation systems with different types of inputs by providing parallel dataset consisting of structured tables and text in two languages. We host various tracks within the testbed based on input and output constraints and investigate and contrast the system differen"
2020.ngt-1.1,W18-6319,0,0.0283845,"Missing"
2021.acl-long.113,D19-1052,0,0.0348151,"Missing"
2021.acl-long.113,P17-1017,0,0.355911,"eedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1419–1434 August 1–6, 2021. ©2021 Association for Computational Linguistics target text, using a separate inference algorithm based on dynamic programming. Crucially, this enables us to directly evaluate and inspect the model’s planning and alignment performance by comparing to manually aligned reference texts. We demonstrate this for two data-to-text generation tasks: the E2E NLG (Novikova et al., 2017) and the WebNLG Challenge (Gardent et al., 2017a). We work with a triple-based semantic representation where a triple consists of a subject, a predicate and an object.2 For instance, in the last triple in Figure 1, Apollo 8, operator and NASA are the subject, predicate and object respectively. Our contributions are as follows: • We present a novel interpretable architecture for jointly learning to plan and generate based on modelling ordering and aggregation by aligning facts in the target text to input representations with an HMM and Transformer encoder-decoder. • We show that our method generates output with higher factual correctness th"
2021.acl-long.113,W19-8652,1,0.900529,"Missing"
2021.acl-long.113,P16-2008,1,0.894371,"Missing"
2021.acl-long.113,N19-1236,0,0.0608247,"s work, we combine advances of both paradigms into a single system by reintroducing sentence planning into neural architectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Assoc"
2021.acl-long.113,D13-1157,1,0.758216,") by learning an implicit mapping between input representations (e.g. RDF triples) and target texts. While this can lead to increased fluency, E2E methods often produce repetitions, hallucination and/or omission of important content for data-to-text (Duˇsek et al., 2020) as well as other natural language generation (NLG) tasks (Cao et al., 2018; Rohrbach et al., 2018). Traditional NLG systems, on the other hand, tightly control which content gets generated, as well as its ordering and aggregation. This process is called sentence planning (Reiter and Dale, 2000; Duboue and McKeown, 2001, 2002; Konstas and Lapata, 2013; Gatt and Krahmer, 2018). Figure 1 shows two different ways to arrange and combine the representations in the input, resulting in widely different generated target texts. In this work, we combine advances of both paradigms into a single system by reintroducing sentence planning into neural architectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input"
2021.acl-long.113,P19-1256,0,0.0180668,"r inspecting sentence planning with a rigorous human evaluation procedure to assess factual correctness in terms of alignment, aggregation and ordering performance. 2 Related Work Factual correctness is one of the main issues for data-to-text generation: How to generate text according to the facts specified in the input triples without adding, deleting or replacing information? The prevailing sequence-to-sequence (seq2seq) architectures typically address this issue via reranking (Wen et al., 2015a; Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018) or some sophisticated training techniques (Nie et al., 2019; Kedzie and McKeown, 2019; Qader et al., 2019). For applications where structured inputs are present, neural graph encoders (Marcheggiani and Perez-Beltrachini, 2018; Rao et al., 2019; Gao et al., 2020) or decoding of explicit graph references (Logan et al., 2019) are applied for higher accuracy. Recently, large-scale pretraining has achieved SoTA results on WebNLG by fine-tuning T5 (Kale and Rastogi, 2020). Several works aim to improve accuracy and controllability by dividing the end-to-end architecture into sentence planning and surface realisation. 2 Note that E2E NLG data and other input"
2021.acl-long.113,W17-5525,1,0.934067,"Missing"
2021.acl-long.113,P02-1040,0,0.109115,"datato-text tasks: the E2E NLG (Novikova et al., 2017) and WebNLG7 (Gardent et al., 2017a). Compared to E2E, WebNLG is smaller, but contains more predicates and has a larger vocabulary. Statistics with examples can be found in Appendix C. We followed the original training-development-test data split for both datasets. 4.2 Evaluation Metrics Generation Evaluation focuses on evaluating the generated text with respect to its similarity to human-authored reference sentences. To compare to previous work, we adopt their associated metrics to evaluate each task. The E2E task is evaluated using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE-L (Lin, 2004), METEOR (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al., 2015). WebNLG is evaluated in terms of BLEU, METEOR, and TER (Snover et al., 2006). Factual Correctness Evaluation tests if the generated text corresponds to the input triples (Wen et al., 2015b; Reed et al., 2018; Duˇsek et al., 2020). We evaluated on the E2E test set using automatic slot error rate (SER),8 i.e., an estimation of the occurrence of the input attributes (predicates) and their values in the outputs, implemented by Duˇsek et al. 7 Since we propose exploring sentence plann"
2021.acl-long.113,W19-8669,0,0.0146065,"us human evaluation procedure to assess factual correctness in terms of alignment, aggregation and ordering performance. 2 Related Work Factual correctness is one of the main issues for data-to-text generation: How to generate text according to the facts specified in the input triples without adding, deleting or replacing information? The prevailing sequence-to-sequence (seq2seq) architectures typically address this issue via reranking (Wen et al., 2015a; Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018) or some sophisticated training techniques (Nie et al., 2019; Kedzie and McKeown, 2019; Qader et al., 2019). For applications where structured inputs are present, neural graph encoders (Marcheggiani and Perez-Beltrachini, 2018; Rao et al., 2019; Gao et al., 2020) or decoding of explicit graph references (Logan et al., 2019) are applied for higher accuracy. Recently, large-scale pretraining has achieved SoTA results on WebNLG by fine-tuning T5 (Kale and Rastogi, 2020). Several works aim to improve accuracy and controllability by dividing the end-to-end architecture into sentence planning and surface realisation. 2 Note that E2E NLG data and other input semantic representations can be converted into"
2021.acl-long.113,W19-8611,0,0.0177222,"ual correctness is one of the main issues for data-to-text generation: How to generate text according to the facts specified in the input triples without adding, deleting or replacing information? The prevailing sequence-to-sequence (seq2seq) architectures typically address this issue via reranking (Wen et al., 2015a; Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018) or some sophisticated training techniques (Nie et al., 2019; Kedzie and McKeown, 2019; Qader et al., 2019). For applications where structured inputs are present, neural graph encoders (Marcheggiani and Perez-Beltrachini, 2018; Rao et al., 2019; Gao et al., 2020) or decoding of explicit graph references (Logan et al., 2019) are applied for higher accuracy. Recently, large-scale pretraining has achieved SoTA results on WebNLG by fine-tuning T5 (Kale and Rastogi, 2020). Several works aim to improve accuracy and controllability by dividing the end-to-end architecture into sentence planning and surface realisation. 2 Note that E2E NLG data and other input semantic representations can be converted into triples, see Section 4.1. Castro Ferreira et al. (2019) feature a pipeline with multiple planning stages and Elder et al. (2019) introduc"
2021.acl-long.113,2020.acl-main.641,0,0.220624,"retable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1419–1434 August 1–6, 2021. ©2021 Association for Computational Linguistics target text, using a separate inference algorithm based on dynamic programming. Cru"
2021.acl-long.113,D18-1356,0,0.341796,"hitectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pag"
2021.acl-long.113,2006.amta-papers.25,0,0.0186005,"cs with examples can be found in Appendix C. We followed the original training-development-test data split for both datasets. 4.2 Evaluation Metrics Generation Evaluation focuses on evaluating the generated text with respect to its similarity to human-authored reference sentences. To compare to previous work, we adopt their associated metrics to evaluate each task. The E2E task is evaluated using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE-L (Lin, 2004), METEOR (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al., 2015). WebNLG is evaluated in terms of BLEU, METEOR, and TER (Snover et al., 2006). Factual Correctness Evaluation tests if the generated text corresponds to the input triples (Wen et al., 2015b; Reed et al., 2018; Duˇsek et al., 2020). We evaluated on the E2E test set using automatic slot error rate (SER),8 i.e., an estimation of the occurrence of the input attributes (predicates) and their values in the outputs, implemented by Duˇsek et al. 7 Since we propose exploring sentence planning and increasing the controllability of the generation model and do not aim for a zero-shot setup, we only focus on the seen category in WebNLG. 8 SER is based on regular expression matching"
2021.acl-long.113,2020.acl-main.455,1,0.785707,"Missing"
2021.acl-long.113,N18-2010,0,0.0165941,"roximate the required planning annotation (entity mentions, their order and sentence splits). Zhao et al. (2020) use a planning stage in a graph-based model – the graph is first reordered into a plan; the decoder conditions on both the input graph encoder and the linearized plan. Similarly, Fan et al. (2019) use a pipeline approach for story generation via SRL-based sketches. However, all of these pipeline-based approaches either require additional manual annotation or depend on a parser for the intermediate steps. Other works, in contrast, learn planning and realisation jointly. For example, Su et al. (2018) introduce a hierarchical decoding model generating different parts of speech at different levels, while filling in slots between previously generated tokens. Puduppully et al. (2019) include a jointly trained content selection and ordering module that is applied before the main text generation step.The model is trained by maximizing the log-likelihood of the gold content plan and the gold output text. Li and Rush (2020) utilize posterior regularization in a structured variational framework to induce which input items are being described by each token of the generated text. Wiseman et al. (201"
2021.acl-long.113,2020.acl-main.224,0,0.440318,"es of both paradigms into a single system by reintroducing sentence planning into neural architectures. We call our system AGG G EN (pronounced ‘again’). AGG G EN jointly learns to generate and plan at the same time. Crucially, our sentence plans are interpretable latent states using semantic facts1 (obtained via Semantic Role Labelling (SRL)) that align the target text with parts of the input representation. In contrast, the plan used in other neural plan-based approaches is usually limited in terms of its interpretability, control, and expressivity. For example, in (Moryossef et al., 2019b; Zhao et al., 2020) the sentence plan is created independently, incurring error propagation; Wiseman et al. (2018) use latent segmentation that limits interpretability; Shao et al. (2019) sample from a latent variable, not allowing for explicit control; and Shen et al. (2020) aggregate multiple input representations which limits expressiveness. AGG G EN explicitly models the two planning processes (ordering and aggregation), but can directly influence the resulting plan and generated 1 Each fact roughly captures “who did what to whom”. 1419 Proceedings of the 59th Annual Meeting of the Association for Computatio"
2021.acl-long.194,P18-5002,0,0.0394662,"Missing"
2021.acl-long.194,2020.emnlp-main.54,0,0.0885915,"ging” strategies, where humans introduce a “missing link” concept, given a source and target topic in the form of two short user utterances (Fig. 1). By grounding the topics on a KG using automatically recognised entities associated with each topic, we can then identify “commonsense” connections which are similar to these missing links. By modelling such topic transitions in the form of Cause-Effect relationships in a KG, we can then perform abductive inference on commonsense knowledge for which we provide a language generation baseline. In particular, we fine-tune a multihop reasoning model (Ji et al., 2020) which was trained on a similar task called Abductive NLG (αNLG) to generate an explanatory hypothesis given two observations. We find that combining a reasoning module over a KG (ConceptNet) with a language model achieves the best performance on our “topic transition” task for both the predicted entity path as well as the generated utterance. In addition, we show that existing multi-topic dialogue datasets, such as PersonaChat (Zhang et al., 2018) and TopicalChat (Gopalakrishnan et al., 2019), cannot be easily adapted to this task, due to the different nature of the tasks they were designed f"
2021.acl-long.194,2020.coling-main.361,0,0.0273848,"ns when asked to complete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data. 1 User A User B User A User B Introduction For a conversation to be truly engaging, we typically assume that both participants take initiative, e.g. by introducing a new topic. We call this a mixed-initiative dialogue. Open-domain systems trained on vast amounts of data (Jiang et al., 2020; Zhang et al., 2020; Gao et al., 2018; Li et al., 2017, 2016; Vinyals and Le, 2015), however, are often purely responsive, make abrupt transitions, or fail to take initiative (see examples in Table 1). In this paper, we consider the case where the system pro-actively introduces a new topic in a conversation by providing a commonsense link of how this new topic relates to what was mentioned previously (see Fig.1). We call this transition strategy “bridging”. Humans deploy a range of strategies 1 https://github.com/karinseve/OTTers Source Topic: I spend a lot of time outside. Transition: I like"
2021.acl-long.194,P16-1094,0,0.0283505,": Yeah and saltwater fish are lucky because they can do that and drink through their mouths. User B: Seems like fresh water fish got the short end of the stick with that one. Have you ever been to a cat cafe? Table 1: Examples of abrupt topic transitions from the PersonaChat and TopicalChat datasets. Current Multi-topic Open-domain Systems. Previous work in open-domain dialogue systems has largely avoided explicitly modelling topic transitions and instead focused on grounding system behaviour in a “persona” (a set of statements about hobbies, demographics, or preferences) (Zhang et al., 2018; Li et al., 2016) or by conditioning conversations on knowledge sources such as newspaper articles, fun facts or Wikipedia articles (Gopalakrishnan et al., 2019; Dinan et al., 2019) to generate engaging responses while avoiding generic replies, improving coherence, and raising new and interesting topics. These approaches often lead to poor topic transitions, as illustrated in Table 1. The PersonaChat example shows neither initiative nor common sense while transitioning to a new topic; it only displays passive acknowledgement from User B. Whereas the TopicalChat example presents a very abrupt topic shift by Use"
2021.acl-long.194,D17-1230,0,0.0309764,"use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data. 1 User A User B User A User B Introduction For a conversation to be truly engaging, we typically assume that both participants take initiative, e.g. by introducing a new topic. We call this a mixed-initiative dialogue. Open-domain systems trained on vast amounts of data (Jiang et al., 2020; Zhang et al., 2020; Gao et al., 2018; Li et al., 2017, 2016; Vinyals and Le, 2015), however, are often purely responsive, make abrupt transitions, or fail to take initiative (see examples in Table 1). In this paper, we consider the case where the system pro-actively introduces a new topic in a conversation by providing a commonsense link of how this new topic relates to what was mentioned previously (see Fig.1). We call this transition strategy “bridging”. Humans deploy a range of strategies 1 https://github.com/karinseve/OTTers Source Topic: I spend a lot of time outside. Transition: I like the outdoors as well, especially gardening. It destres"
2021.acl-long.194,W04-1013,0,0.0727508,"oned in the transition utterance to determine how well they bridge the gap between Topic A and Topic B. We use hits@k ratio as an automatic approximation, which measures the number of relevant entities correctly predicted by the model, out of the k most important entities identified in the target references. This metric shows how well the models ground the concepts introduced in the two dialogue turns and how the reasoning compares to the human standard presented in OTTers. For (2) we adopt the same automated metrics used for evaluating MultiGen on the αNLG dataset for comparability: ROUGE-L (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). However, we report the full BLEU score (Papineni et al., 2002)4 that accounts for the overlap across 1-4 ngrams instead of just 4-grams (BLEU-4). As word-overlap based metrics have been widely criticised due to their lack of correlation with human judgements (Novikova et al., 2017; Reiter, 2018), we also provide an example-based error analysis in Section 4.4. 4.4 Results For each aforementioned split we evaluated three different models to compare performance: the pretrained vGPT2 fine-tuned on each split for OTTers, the Mu"
2021.acl-long.194,D17-1238,1,0.82848,"Missing"
2021.acl-long.194,P02-1040,0,0.109929,"o as an automatic approximation, which measures the number of relevant entities correctly predicted by the model, out of the k most important entities identified in the target references. This metric shows how well the models ground the concepts introduced in the two dialogue turns and how the reasoning compares to the human standard presented in OTTers. For (2) we adopt the same automated metrics used for evaluating MultiGen on the αNLG dataset for comparability: ROUGE-L (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). However, we report the full BLEU score (Papineni et al., 2002)4 that accounts for the overlap across 1-4 ngrams instead of just 4-grams (BLEU-4). As word-overlap based metrics have been widely criticised due to their lack of correlation with human judgements (Novikova et al., 2017; Reiter, 2018), we also provide an example-based error analysis in Section 4.4. 4.4 Results For each aforementioned split we evaluated three different models to compare performance: the pretrained vGPT2 fine-tuned on each split for OTTers, the MultiGen model fine-tuned only on αNLG, and the same model additionally fine-tuned on OTTers (called αNLGft). Overview of Results. Table"
2021.acl-long.194,W18-6319,0,0.0243222,"Missing"
2021.acl-long.194,prasad-etal-2008-penn,0,0.101712,"Missing"
2021.acl-long.194,W11-0144,0,0.0113586,"tes grounding on KG entities. • We collect a crowdsourced dataset, OTTers, and present a rigorous analysis in terms of transition strategies, linguistic properties and entity linking to a KG. • We show that our KG-grounded dataset can effectively leverage the reasoning component of an existing Transformer-based model (Ji et al., 2020) to generate better output compared to a vanilla GPT-2 (Radford et al., 2019) decoder, both in in-domain and out-of-domain data splits. 2 Related Work Topic Transitions in the Linguistic Literature. There is no common definition for the term topic (Goutsos, 1997; Purver et al., 2011); however, there are a number of definitions which are helpful for our purposes. Goutsos (1997) divide a “topic” into two main components: 1) what constitutes a topic (the “what”) and 2) how participants perceive and manage a topic (the “how”). An early work from Brown and Yule (1983) declares that “topics should be described as the most frequently used, unexplained term in the analysis of discourse”. In general, “discourse topics” can be explained as what a portion of the interaction is about, therefore the “aboutness” (Berthoud and Mondada, 1995; Porhiel, 2005). More specifically Chafe (1994"
2021.acl-long.194,J18-3002,0,0.0121531,"ts introduced in the two dialogue turns and how the reasoning compares to the human standard presented in OTTers. For (2) we adopt the same automated metrics used for evaluating MultiGen on the αNLG dataset for comparability: ROUGE-L (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and CIDEr (Vedantam et al., 2015). However, we report the full BLEU score (Papineni et al., 2002)4 that accounts for the overlap across 1-4 ngrams instead of just 4-grams (BLEU-4). As word-overlap based metrics have been widely criticised due to their lack of correlation with human judgements (Novikova et al., 2017; Reiter, 2018), we also provide an example-based error analysis in Section 4.4. 4.4 Results For each aforementioned split we evaluated three different models to compare performance: the pretrained vGPT2 fine-tuned on each split for OTTers, the MultiGen model fine-tuned only on αNLG, and the same model additionally fine-tuned on OTTers (called αNLGft). Overview of Results. Table 7 shows the results of these experiments. vGPT2 performs poorly on the one-turn transition task, regardless of the traindev-test split, which we attribute to the small size of OTTers: with only a few thousand utterances, vGPT2 is una"
2021.acl-long.194,C12-1163,0,0.0664432,"Missing"
2021.acl-long.194,W13-2610,0,0.114083,"Missing"
2021.acl-long.194,P19-1193,0,0.0239353,"ons for the topic of each turn and participants had the freedom to mention their topics (i.e. persona traits) in any order. We use PersonaChat in two different ways: 1) using their persona traits as starting and goal topics for our own data collection, and 2) as a point of comparison for our dataset. Commonsense-Aware Neural Text Generation. Large Language Models still suffer in cases where reasoning over underlying commonsense knowledge is required during generation, including dialogue generation (Zhou et al., 2018), story ending generation (Guan et al., 2019), and topic-to-essay generation (Yang et al., 2019). Recently, Guan et al. (2019); Bhagavatula et al. (2020) attempted to integrate external commonsense knowledge into generative pretrained language models, which we will also attempt in Section 4 using the Abductive NLG (αNLG) dataset (Bhagavatula et al., 2020). Our setup is similar in spirit to αNLG, which is a conditional generation task for explanations given observations in natural language. In particular, the model has to generate an explanatory hypothesis given two observations: the cause (e.g. The Smith family went on a cruise for their summer vacation) and the consequence (e.g. From th"
2021.acl-long.194,P18-1205,0,0.36492,"uctive inference on commonsense knowledge for which we provide a language generation baseline. In particular, we fine-tune a multihop reasoning model (Ji et al., 2020) which was trained on a similar task called Abductive NLG (αNLG) to generate an explanatory hypothesis given two observations. We find that combining a reasoning module over a KG (ConceptNet) with a language model achieves the best performance on our “topic transition” task for both the predicted entity path as well as the generated utterance. In addition, we show that existing multi-topic dialogue datasets, such as PersonaChat (Zhang et al., 2018) and TopicalChat (Gopalakrishnan et al., 2019), cannot be easily adapted to this task, due to the different nature of the tasks they were designed for. Our contributions are as follows: • We propose a new Natural Language Generation task based on one-turn topic transitions for open-domain dialogue based on a “bridging” strategy, which promotes grounding on KG entities. • We collect a crowdsourced dataset, OTTers, and present a rigorous analysis in terms of transition strategies, linguistic properties and entity linking to a KG. • We show that our KG-grounded dataset can effectively leverage th"
2021.acl-long.194,2020.acl-demos.30,0,0.0365194,"plete such a task, and notice that the use of a bridging utterance to connect the two topics is the approach used the most. We finally show how existing state-of-the-art text generation models can be adapted to this task and examine the performance of these baselines on different splits of the OTTers data. 1 User A User B User A User B Introduction For a conversation to be truly engaging, we typically assume that both participants take initiative, e.g. by introducing a new topic. We call this a mixed-initiative dialogue. Open-domain systems trained on vast amounts of data (Jiang et al., 2020; Zhang et al., 2020; Gao et al., 2018; Li et al., 2017, 2016; Vinyals and Le, 2015), however, are often purely responsive, make abrupt transitions, or fail to take initiative (see examples in Table 1). In this paper, we consider the case where the system pro-actively introduces a new topic in a conversation by providing a commonsense link of how this new topic relates to what was mentioned previously (see Fig.1). We call this transition strategy “bridging”. Humans deploy a range of strategies 1 https://github.com/karinseve/OTTers Source Topic: I spend a lot of time outside. Transition: I like the outdoors as wel"
2021.eacl-main.183,2020.emnlp-main.703,1,0.834536,"d 2) a novel way for an agent to play by itself, called Self-play via Iterated Experience Learning (SPIEL). We evaluate the ability of both procedures to generalise: an in-domain evaluation shows an increased accuracy (+7.79) compared with competitors on the evaluation suite CompGuessWhat?!; a transfer evaluation shows improved performance for VQA on the TDIUC dataset in terms of harmonic average accuracy (+5.31) thanks to more fine-grained object representations learned via SPIEL. 1 Background & Related Work Learning a language requires interacting with both the environment and other agents (Bisk et al., 2020). Language games represent one common example of this (Wittgenstein et al., 1953), as seen by the important role of play in L1 child language acquisition (Hainey et al., 2016) as well as L2 learners (Godwin-Jones, 2014). Among the language games defined in the literature (Steels, 2015), guessing games represent the first step in a curriculum for language learning. For example, in GuessWhat?! (de Vries et al., 2017), two agents interact with each other: a Questioner generates questions aimed at finding a hidden object in the scene and an Oracle, aware of the target object, answers the questions"
2021.eacl-main.183,P18-1238,0,0.0243165,"Missing"
2021.eacl-main.183,C18-1104,0,0.0518742,"Missing"
2021.eacl-main.183,N19-1265,0,0.276696,"Missing"
2021.findings-emnlp.133,N19-1423,0,0.00680822,"s”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summary Fact-weights score is then defined as the average weights over all facts in the summary: SFweights = avgj=1···J wjf ∈ [−1, 1] (1) A high SFweights score indicates that the facts in the summaries are well supported by the facts mentioned in the documents. The top section in Table 3 shows SFweights scores reported on M I RA NEWS(S-D), M I RA NEWS(S-A) and M I RA NEWS(S-D&A), which weight facts in the summaries using facts in the main document, assisting documents, and both, respectively. As expected, SFweights on M I RA NEWS(S-D) is higher than on M I RA NEWS(SA), indicating that t"
2021.findings-emnlp.133,P19-1483,0,0.0592215,"Missing"
2021.findings-emnlp.133,P19-1102,0,0.0209277,"4. 7 Related Work Single Document Summarization aims to compress a single textual document while keeping salient information. SDS includes two directions: extractive summarization (Nallapati et al., 2017) which aims at extracting salient sentences from the input document, and abstractive summarization (See et al., 2017; Narayan et al., 2018a; Yang et al., 2019; Liu and Lapata, 2019b; Liu et al., 2020; Rothe et al., 2020; Raffel et al., 2020) which generates a novel short representation of the input. Multi-Document Summarization aims to compress multiple textual documents to a shorter summary (Fabbri et al., 2019). Approaches mainly focus on increasing the capacity of the encoder to process longer inputs (Liu and Lapata, 2019a; Beltagy et al., 2020; Zaheer et al., 2020; Zhang et al., 2020a; Huang et al., 2021), leveraging knowledge graphs (Fan et al., 2019; Li et al., 2020; Jin et al., 2020), and including content selection steps (Nayeem et al., 2018; Wang et al., 2020; Xu and Lapata, 2020; Grenander et al., 2019; Liu et al., 2018). tures, training and decoding, e.g. Cao et al. (2018); Zhang et al. (2020c); Falke et al. (2019); Zhao et al. (2020b). However, we are the first research aiming to reduce th"
2021.findings-emnlp.133,P19-1213,0,0.0501629,"Missing"
2021.findings-emnlp.133,D19-1428,0,0.0547221,"Missing"
2021.findings-emnlp.133,D19-1620,0,0.0414923,"Missing"
2021.findings-emnlp.133,N18-1065,0,0.0602853,"Missing"
2021.findings-emnlp.133,2021.naacl-main.112,0,0.0488549,"Missing"
2021.findings-emnlp.133,2020.acl-main.556,0,0.0528558,"Missing"
2021.findings-emnlp.133,2020.acl-main.703,0,0.0121967,"on), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single document (2020) classified hallucinations into intrinsic that with human authored highlights/description as the mistakenly manipulate information from the source summary. This tas"
2021.findings-emnlp.133,2020.acl-main.555,0,0.0271442,"Missing"
2021.findings-emnlp.133,N03-1020,0,0.691106,"Missing"
2021.findings-emnlp.133,P19-1500,0,0.276332,"oderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single"
2021.findings-emnlp.133,J05-1004,0,0.199508,"support the summary better. • EO M I RA NEWS(S-D&A) in Table 2 contains the best three sentences from the main and assisting documents against the summary. The higher ROUGE scores on M I RA NEWS(S-D&A), as compared to M I RA NEWS(S-D), indicate that assisting documents A contribute additional information to the summaries, which is absent from the main document D. • Summary Fact-weights evaluate the semantic correspondence between a document and its summary using a representation based on “facts”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summary Fact-weights score is then defined as the average weights over all facts in the summary:"
2021.findings-emnlp.133,D19-1387,0,0.25318,"oderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single"
2021.findings-emnlp.133,2020.acl-main.173,1,0.88083,"Missing"
2021.findings-emnlp.133,D18-1206,1,0.844209,"nguage model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single-document Summarization (SDS, middle) and Multi-document Summarization (MDS, right): SDS aims at generating a summary for a"
2021.findings-emnlp.133,N18-1158,1,0.720606,"nguage model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single-document Summarization (SDS, middle) and Multi-document Summarization (MDS, right): SDS aims at generating a summary for a"
2021.findings-emnlp.133,C18-1102,0,0.0553052,"Missing"
2021.findings-emnlp.133,2020.emnlp-main.748,0,0.207296,"s at the end of the main document. Since each document contains around 700 words on average (see Table 1), we truncate the main document to half the size of the model capacity, i.e. 500 words for BART-large and 1000 words for HT, respectively. To include information from all assisting documents, we truncate each of them to fill the 7 Implementation used: https://huggingface.co/ transformers/model_doc/bart.html. 8 We use the implementation from https://github. com/nlpyang/hiersumm. remaining half of the model capacity evenly. • Pipeline (-P): Previous approaches T-DMCA (Liu et al., 2018), TLM (Pilault et al., 2020) and SEAL (Zhao et al., 2020a) show that long input settings for abstractive summarization benefit from a content extraction preprocessing step. We thus introduce a simple weakly supervised content extraction method for the assisting documents, and concatenate the selected content to the end of the main document on the input. Note that the content selection in M I RA NEWS is conditioned on the main document, which is different from content selection in both SDS and MDS that select sentences without additional conditioning. In particular, we first compute a contextual embedding for each sentenc"
2021.findings-emnlp.133,2020.tacl-1.18,1,0.89944,"h data divergence issues between the source and target texts (Dhingra et al., 2019) will function more as an open-ended language model and will be prone to extrinsic hallucinations. In this work, we tackle the problem of extrinsic hallucinations by introducing a new task, MultiResource-Assisted News Summarization and a novel dataset (M I RA NEWS). Following Maynez et al. (2020), we regard the incorporation of background knowledge within a generated summary as the desired property. However, instead of sourcing this knowledge via pretraining on large datasets,2 2 Although they report B ERT S2S (Rothe et al., 2020) to output more factual hallucinations in the summary than their non-pre-trained counterparts on XSum (Narayan et al., 2018a), we base our work on the assumption that articles from alternative news resources covering the same news event can complement the background knowledge in an easier to learn, more direct, and explainable way. Consider the example in Figure 1, where the assisting document (bottom section) from another news resource recounts some facts in the summary (highlighted in pink) in a more explicit way. Note that, as shown in Figure 2 (left), our task is different from both Single"
2021.findings-emnlp.133,2020.emnlp-main.647,0,0.0184006,"vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. datasets are curated by pairing a single document (2020) classified hallucinations into intrinsic that with human authored highlights/description as the mistakenly manipulate information from the source summary. This task is typically approached using document resulting in counterfactual output, and 1 extrinsic that introduce information not grounded Our code and data are available at: https://github.com/XinnuoXu/MiRANews in the document (see Figure 1). Extrinsic halluci1541 1 Introduction Findings of the Association for Computational Linguis"
2021.findings-emnlp.133,P17-1099,0,0.319088,"edian, who and husband matthew broderick were among the early arrivals... Figure 1: An example where the summary (top section) contains information that is not explicitly included in its main document (middle section), but is covered in the related assisting document (bottom section). We highlight the information in the summary that is aligned to its corresponding main and assisting documents with yellow and pink colors, respectively. conditional generation models, including sequenceto-sequence architectures with attention and copy The vast majority of current research on abstrac- mechanisms (See et al., 2017), Transformers (Liu and Lapata, 2019a), and pre-trained language modtive summarization is aimed at single-document news summarization due to the widespread avail- eling (e.g. Radford et al., 2019; Lewis et al., 2020). ability of data, e.g. (NY Times; Sandhaus (2008), While these SotA summarization models reach CNN/DailyMail; Hermann et al. (2015), News- a high level of fluency and coherence, they are also room; Grusky et al. (2018), XSum; Narayan et al. highly prone to hallucinating content that is not (2018a), MLSUM; Scialom et al. 2020). The grounded by the input document. Maynez et al. data"
2021.findings-emnlp.133,2020.emnlp-main.32,0,0.0429564,"Missing"
2021.findings-emnlp.133,2020.acl-main.455,1,0.740794,"han M I RA NEWS(SD). Introducing the assisting documents contributes new information to support the summary better. • EO M I RA NEWS(S-D&A) in Table 2 contains the best three sentences from the main and assisting documents against the summary. The higher ROUGE scores on M I RA NEWS(S-D&A), as compared to M I RA NEWS(S-D), indicate that assisting documents A contribute additional information to the summaries, which is absent from the main document D. • Summary Fact-weights evaluate the semantic correspondence between a document and its summary using a representation based on “facts”. We follow Xu et al. (2020) and represent facts in a sentence by adapting Semantic Role Labelling (Palmer et al., 2005), which roughly captures “who did what to whom” in terms of predicates and their arguments. The facts in  the document and summary are represented as F1D , F2D , · · · FID and  S S F1 , F2 , · · · FJS , respectively. We apply automatic content weighting as defined in (Xu et al., 2020) and weight each fact Fj in the summary using its maximum semantic similarity to the facts in the document wjf = maxi∈I dfij , where dfij is the semantic similarity based on BERT embeddings (Devlin et al., 2019). The Summ"
2021.findings-emnlp.133,2020.emnlp-main.296,0,0.0567443,"Missing"
2021.findings-emnlp.133,2020.acl-main.458,0,0.027433,"42.29 36.18 43.22 36.06 43.11 36.08 43.13 46.72 55.39 43.15 51.02 BertScore P R F1 .701 .674 .684 .701 .666 .679 .701 .677 .685 .685 .682 .680 .690 .682 .682 .684 .686 .681 .769 .745 .755 .716 .731 .721 Table 4: Evaluation on ROUGE and BertScore. 4.2 Evaluation Metrics We evaluate the approaches described in Section 4.1 from four perspectives: • Similarity to Reference focuses on evaluating the generated summary with respect to its similarity to a human-authored ground-truth reference summary. We adopt the exact-matching metric ROUGE (Lin and Hovy, 2003) and the softmatching metric BertScore (Zhang et al., 2020b). • Extractiveness level aims at the bias of each system towards generating extractive summaries. We introduce the n-grams coverage, which equals to 1 − n-gram novelty (see Section 3), to measure the percentage of n-grams in the generated summary that appear in the main and assisting documents. Higher n-gram coverage scores indicate that the system is more extractive. • Support from Assisting Documents measures the proportion of information appearing in the generated summary that originates from assisting documents only. We propose the n-grams coverage over n-grams in the generated summary w"
2021.findings-emnlp.133,2020.findings-emnlp.203,0,0.0149793,"t. Since each document contains around 700 words on average (see Table 1), we truncate the main document to half the size of the model capacity, i.e. 500 words for BART-large and 1000 words for HT, respectively. To include information from all assisting documents, we truncate each of them to fill the 7 Implementation used: https://huggingface.co/ transformers/model_doc/bart.html. 8 We use the implementation from https://github. com/nlpyang/hiersumm. remaining half of the model capacity evenly. • Pipeline (-P): Previous approaches T-DMCA (Liu et al., 2018), TLM (Pilault et al., 2020) and SEAL (Zhao et al., 2020a) show that long input settings for abstractive summarization benefit from a content extraction preprocessing step. We thus introduce a simple weakly supervised content extraction method for the assisting documents, and concatenate the selected content to the end of the main document on the input. Note that the content selection in M I RA NEWS is conditioned on the main document, which is different from content selection in both SDS and MDS that select sentences without additional conditioning. In particular, we first compute a contextual embedding for each sentence in both main and assisting"
2021.nllp-1.14,W06-1620,0,0.0225941,"8.285 6.313 20,86 11,96 127 247 5.368 151.499 8.925 7.293 20,77 12,32 128 292 10.055 283.165 9.837 13.606 20,81 12,16 Table 1: Statistics for the S COT R EG corpus – the number of defined terms, word-level tokens and sentences found in the domestic and non-domestic Scottish Building regulations. This enables representing MWEs as single tokens, which has been shown to improve accuracy of NLP tasks (Green et al., 2011), such as dependency parsing (Nivre and Nilsson, 2004). Supervised approaches are used, amongst which sequence tagging has been found to work well (Constant et al., 2017), e.g., (Blunsom and Baldwin, 2006; Constant et al., 2012). Sequence tagging has also been used for joint MWE identification and Part-of-Speech (POS) tagging (Constant and Sigogne, 2011) and may be amended to handle discontiguous MWEs (Schneider et al., 2014). This study explores sequence tagging for joint MWE processing. However, this study does not aim to handle idiomatic expressions or proverbs. Beyond research on MWE processing, a related task that focuses on identifying technical terms and Named Entities is concept mining, e.g., (Rajagopal et al., 2013; Poria et al., 2014). In contrast to these concept mining studies, we"
2021.nllp-1.14,W11-0809,0,0.0164361,"OT R EG corpus – the number of defined terms, word-level tokens and sentences found in the domestic and non-domestic Scottish Building regulations. This enables representing MWEs as single tokens, which has been shown to improve accuracy of NLP tasks (Green et al., 2011), such as dependency parsing (Nivre and Nilsson, 2004). Supervised approaches are used, amongst which sequence tagging has been found to work well (Constant et al., 2017), e.g., (Blunsom and Baldwin, 2006; Constant et al., 2012). Sequence tagging has also been used for joint MWE identification and Part-of-Speech (POS) tagging (Constant and Sigogne, 2011) and may be amended to handle discontiguous MWEs (Schneider et al., 2014). This study explores sequence tagging for joint MWE processing. However, this study does not aim to handle idiomatic expressions or proverbs. Beyond research on MWE processing, a related task that focuses on identifying technical terms and Named Entities is concept mining, e.g., (Rajagopal et al., 2013; Poria et al., 2014). In contrast to these concept mining studies, we do not rely on dependency parses or external resources, such as ConceptNet (Speer et al., 2013). 3 3.1 MWE tagging for ACC A building regulations corpus"
2021.nllp-1.14,P12-1022,0,0.013388,"7 247 5.368 151.499 8.925 7.293 20,77 12,32 128 292 10.055 283.165 9.837 13.606 20,81 12,16 Table 1: Statistics for the S COT R EG corpus – the number of defined terms, word-level tokens and sentences found in the domestic and non-domestic Scottish Building regulations. This enables representing MWEs as single tokens, which has been shown to improve accuracy of NLP tasks (Green et al., 2011), such as dependency parsing (Nivre and Nilsson, 2004). Supervised approaches are used, amongst which sequence tagging has been found to work well (Constant et al., 2017), e.g., (Blunsom and Baldwin, 2006; Constant et al., 2012). Sequence tagging has also been used for joint MWE identification and Part-of-Speech (POS) tagging (Constant and Sigogne, 2011) and may be amended to handle discontiguous MWEs (Schneider et al., 2014). This study explores sequence tagging for joint MWE processing. However, this study does not aim to handle idiomatic expressions or proverbs. Beyond research on MWE processing, a related task that focuses on identifying technical terms and Named Entities is concept mining, e.g., (Rajagopal et al., 2013; Poria et al., 2014). In contrast to these concept mining studies, we do not rely on dependenc"
2021.nllp-1.14,E12-2021,0,0.0255997,"in surface forms would be reflected by a relatively small vocabulary – which is thought to ease the complexity of various NLP tasks (Church, 2013). As can be seen in Figure 1, S COT R EG has in the order of 10K unique tokens for a total of 283K. In comparison to the more heterogeneous Brown corpus (Francis and Kucera, 1964), which has more than 23K unique tokens for the first 283K tokens, S COT R EG indeed has a small vocabulary. 3.5 Annotating SPAR.txt • “[...], a paved (or equivalent) footpath at least 900mm wide [...]” A domain expert annotated a random selection of 200 sentences in BRAT (Stenetorp et al., 2012). Our assumption is that such a small dataset should suffice for achieving reasonable results on the proposed parsing task. Figure 2 exemplifies how annotations can span single words, multiple words and also indicate that two groups of words belong to a single, discontiguous span. To distinguish between verb-based and noun-based spans, as well as spans that belong to neither of these classes, we annotate the following span types: In this case, while we would like to split ‘a paved (or equivalent) footpath’ into the spans: ‘a paved footpath’, ‘(’, ‘)’, ‘or’ and ‘equivalent’. In a downstream tas"
2021.splurobonlp-1.2,N19-1423,0,0.0222745,"ecast as classification tasks: a landmark, a bearing and a distance. where V is a vocabulary of words and the corresponding geographic map I is represented as a set of M landmark objects oi = (bb, r, n) where bb is a 4-dimensional vector with bounding box coordinates, r is the corresponding Region of Interest (RoI) feature vector produced by an object detector and n =&lt; n1 , n2 . . . nK &gt;, is a multi-token name. We define a function f : V N × R4∗M × R2048∗M × V M ∗K → R × R to predict the GPS destination location yˆ: yˆ = f w, {oi = (bb, r, n)}M  weights are initialized using pretrained BERT (Devlin et al., 2019). hw0 is the hidden state for the special token [CLS]. Metadata Encoder OSM comes with useful metadata in the form of bounding boxes (around the landmark symbols) and names of landmarks on the map. We represent each bounding box as a 4-dimensional vector bbmetak and each name (nk ) using another Transformer initialized with pretrained BERT weights. We treat metadata as a bag of names but since each word can have multiple tokens, we output position embeddings posnk for each name separately; hnk are the resulting hidden states with hnk,0 being the hidden state for [CLS]. (1) Since predicting yˆ"
2021.splurobonlp-1.2,L16-1605,0,0.0684861,"Missing"
2021.splurobonlp-1.2,N16-1088,0,0.0123599,"elligent agent about events happening with respect to a map requires learning to associate natural language with the world representation found within the map. This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al., 2006; Johnson et al., 2017) or 3D photorealistic environments (Kolve et al., 2017; Savva et al., 2019), static images (Ilinykh et al., 2019; Kazemzadeh et al., 2014), and to a lesser extent on synthetic (Thompson et al., 1993) and real geographic maps (Paz-Argaman and Tsarfaty, 2019; Haas and Riezler, 2016; G¨otze and Boye, 2016). The tasks usually relate to navigation (Misra et al., 2018; Thomason et al., 2019) or action execution (Bisk et al., 2018; Shridhar et al., 2019) and as11 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 11–21 August 5–6, 2020. ©2021 Association for Computational Linguistics (visual modality); and iii) a worded instruction. Our approach to the destination prediction task is two-fold. The first stage is a data collection for the “Robot Open Street Map Instructions” (ROSMI) (Katsakioris"
2021.splurobonlp-1.2,D18-1287,0,0.0433435,"Missing"
2021.splurobonlp-1.2,W19-8621,0,0.01909,"log, such as assisting with emergency response and remote robot instruction that require knowledge of maps or building schemas. Effective communication of such an intelligent agent about events happening with respect to a map requires learning to associate natural language with the world representation found within the map. This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al., 2006; Johnson et al., 2017) or 3D photorealistic environments (Kolve et al., 2017; Savva et al., 2019), static images (Ilinykh et al., 2019; Kazemzadeh et al., 2014), and to a lesser extent on synthetic (Thompson et al., 1993) and real geographic maps (Paz-Argaman and Tsarfaty, 2019; Haas and Riezler, 2016; G¨otze and Boye, 2016). The tasks usually relate to navigation (Misra et al., 2018; Thomason et al., 2019) or action execution (Bisk et al., 2018; Shridhar et al., 2019) and as11 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 11–21 August 5–6, 2020. ©2021 Association for Computational Linguistics (visual modality); and iii) a worded instruct"
2021.splurobonlp-1.2,D17-1106,0,0.0623215,"Missing"
2021.splurobonlp-1.2,D19-1681,0,0.0209743,"ive communication of such an intelligent agent about events happening with respect to a map requires learning to associate natural language with the world representation found within the map. This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al., 2006; Johnson et al., 2017) or 3D photorealistic environments (Kolve et al., 2017; Savva et al., 2019), static images (Ilinykh et al., 2019; Kazemzadeh et al., 2014), and to a lesser extent on synthetic (Thompson et al., 1993) and real geographic maps (Paz-Argaman and Tsarfaty, 2019; Haas and Riezler, 2016; G¨otze and Boye, 2016). The tasks usually relate to navigation (Misra et al., 2018; Thomason et al., 2019) or action execution (Bisk et al., 2018; Shridhar et al., 2019) and as11 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 11–21 August 5–6, 2020. ©2021 Association for Computational Linguistics (visual modality); and iii) a worded instruction. Our approach to the destination prediction task is two-fold. The first stage is a data collection for the “Robot Open Street Map Instructio"
2021.splurobonlp-1.2,D14-1086,0,0.0455654,"with emergency response and remote robot instruction that require knowledge of maps or building schemas. Effective communication of such an intelligent agent about events happening with respect to a map requires learning to associate natural language with the world representation found within the map. This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al., 2006; Johnson et al., 2017) or 3D photorealistic environments (Kolve et al., 2017; Savva et al., 2019), static images (Ilinykh et al., 2019; Kazemzadeh et al., 2014), and to a lesser extent on synthetic (Thompson et al., 1993) and real geographic maps (Paz-Argaman and Tsarfaty, 2019; Haas and Riezler, 2016; G¨otze and Boye, 2016). The tasks usually relate to navigation (Misra et al., 2018; Thomason et al., 2019) or action execution (Bisk et al., 2018; Shridhar et al., 2019) and as11 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 11–21 August 5–6, 2020. ©2021 Association for Computational Linguistics (visual modality); and iii) a worded instruction. Our approach to the d"
2021.splurobonlp-1.2,P06-1131,0,0.0600078,"is able to understand instructions referring to previously unseen maps. 2 Related Work Situated dialog encompasses various aspects of interaction. These include: situated Natural Language Processing (Bastianelli et al., 2016); situated reference resolution (Misu, 2018); language grounding (Johnson et al., 2017); visual question answer/visual dialog (Antol et al., 2015); dialog agents for learning visually grounded word meanings and learning from demonstration (Yu et al., 2017); and Natural Language Generation (NLG), e.g. of situated instructions and referring expressions (Byron et al., 2009; Kelleher and Kruijff, 2006). Here, work on instruction processing for destination mapping and navigation are discussed, as well as language grounding and referring expression resolution, with an emphasis on 2D/3D real world and map-based application. Language grounding refers to interpreting language in a situated context and includes collaborative language grounding toward situated humanrobot dialog (Chai et al., 2016), city exploration (Boye et al., 2014), as well as following high-level navigation instructions (Blukis et al., 2018). Mapping instructions to low level actions has been explored in structured environment"
2021.splurobonlp-1.2,D19-1514,0,0.112491,"i = (bb, r, n)}M Visual Encoder Each map image is fed into a pretrained Faster R-CNN detector (Ren et al., 2015), which outputs bounding boxes and RoI feature vectors bbk and rk for k objects. In order to learn better representation for landmarks, we fine-tuned the detector on around 27k images of maps to recognize k objects {o1 , .., ok } and classify landmarks of 213 manually-cleaned classes from OSM; we fixed k to 73 landmarks. Finally, a combined position-aware embedding vk was learned by adding together the vectors bbk and rk as in LXMERT:  (2) 4.2 Model Architecture Inspired by LXMERT (Tan and Bansal, 2019), we present MAPERT, a Transformer-based (Vaswani et al., 2017) model with three separate singlemodality encoders (for NL instructions, metadata and visual features) and a cross-modality encoder that merges them. Fig. 2 depicts the architecture. In the following sections, we describe each component separately. F F (bbk ) + F F (rk ) (3) 2 where F F are feed-forward layers with no bias. vk = 4.3 We describe three different approaches to combining knowledge from maps with the NL instructions: Instructions Encoder The word sequence w is fed to a Transformer encoder and output hidden states hw and"
2021.splurobonlp-1.2,H93-1005,0,0.650364,"ire knowledge of maps or building schemas. Effective communication of such an intelligent agent about events happening with respect to a map requires learning to associate natural language with the world representation found within the map. This symbol grounding problem (Harnad, 1990) has been largely studied in the context of mapping language to objects in a situated simple (MacMahon et al., 2006; Johnson et al., 2017) or 3D photorealistic environments (Kolve et al., 2017; Savva et al., 2019), static images (Ilinykh et al., 2019; Kazemzadeh et al., 2014), and to a lesser extent on synthetic (Thompson et al., 1993) and real geographic maps (Paz-Argaman and Tsarfaty, 2019; Haas and Riezler, 2016; G¨otze and Boye, 2016). The tasks usually relate to navigation (Misra et al., 2018; Thomason et al., 2019) or action execution (Bisk et al., 2018; Shridhar et al., 2019) and as11 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 11–21 August 5–6, 2020. ©2021 Association for Computational Linguistics (visual modality); and iii) a worded instruction. Our approach to the destination prediction task is two-fold. The first stage is a"
2021.splurobonlp-1.2,W17-2802,0,0.0284002,"ree outputs, i.e., reference landmark location on the map, bearing and distance. Our contributions are thus three-fold: • A model that is able to understand instructions referring to previously unseen maps. 2 Related Work Situated dialog encompasses various aspects of interaction. These include: situated Natural Language Processing (Bastianelli et al., 2016); situated reference resolution (Misu, 2018); language grounding (Johnson et al., 2017); visual question answer/visual dialog (Antol et al., 2015); dialog agents for learning visually grounded word meanings and learning from demonstration (Yu et al., 2017); and Natural Language Generation (NLG), e.g. of situated instructions and referring expressions (Byron et al., 2009; Kelleher and Kruijff, 2006). Here, work on instruction processing for destination mapping and navigation are discussed, as well as language grounding and referring expression resolution, with an emphasis on 2D/3D real world and map-based application. Language grounding refers to interpreting language in a situated context and includes collaborative language grounding toward situated humanrobot dialog (Chai et al., 2016), city exploration (Boye et al., 2014), as well as followin"
D13-1101,krestel-etal-2008-minding,0,0.283044,"ns annotated within them. SMHC has a higher density of quotations per document, 8.3 vs. 4.6 in PARC, since articles are fully annotated and 1 The agreement was calculated using the agr metric described in Wiebe and Riloff (2005) as the proportion of commonly annotated ARs with respect to the ARs identified overall by Annotator A and Annotator B respectively 992 Bsay Blist k-NN P 94.4 75.4 88.9 R 43.5 71.1 72.6 F 59.5 73.2 79.9 Table 3: Results for the k-NN verb-cue classifier. Bsay classifies as verb-cue all instances of say while Blist marks as verb-cues all verbs from a pre-compiled list in Krestel et al. (2008). were selected to contain at least one quotation. PARC is instead only partially annotated and comprises articles with no quotations. Excluding null-quotation articles from PARC, the average incidence of annotated quotations per article raises to 7.1. The corpora also differ in quotation type distribution, with direct quotations being largely predominant in SMHC while indirect are more common in PARC. 4 4.1 Experimental Setup Quotation Extraction Quotation extraction is the task of extracting the content span of all of the direct, indirect, and mixed quotations within a given document. More p"
D13-1101,D12-1072,1,0.6309,"Missing"
D13-1101,pareti-2012-database,1,0.483337,"Finally, we use the direct quotation attribution methods described in O’Keefe et al. (2012) and show that they can be successfully applied to indirect and mixed quotations, albeit with lower accuracy. This leads us to conclude that attributing indirect and mixed quotations to speakers is harder than attributing direct quotations. With this work, we set a new state of the art in quotation extraction. We expect that the main contribution of this work will be that future methods can be evaluated in a comparable way, so that the relative merit of various approaches can be determined. 2 Background Pareti (2012) defines an attribution as having a source span, a cue span, and a content span: Source is the span of text that indicates who the content is attributed to, e.g. ‘president Obama’, ‘analysts’, ‘China’, ‘she’. Cue is the lexical anchor of the attribution relation, 990 usually a verb, e.g. ‘say’, ‘add’, ‘quip’. Content is the span of text that is attributed. Based on the type of attitude the source expresses towards a proposition or eventuality, attributions are subcategorised (Prasad et al., 2006) into assertions (Ex.2a) and beliefs (Ex.2b), which imply different degrees of commitment, facts (E"
D13-1101,W06-0305,0,0.180179,"ted in a comparable way, so that the relative merit of various approaches can be determined. 2 Background Pareti (2012) defines an attribution as having a source span, a cue span, and a content span: Source is the span of text that indicates who the content is attributed to, e.g. ‘president Obama’, ‘analysts’, ‘China’, ‘she’. Cue is the lexical anchor of the attribution relation, 990 usually a verb, e.g. ‘say’, ‘add’, ‘quip’. Content is the span of text that is attributed. Based on the type of attitude the source expresses towards a proposition or eventuality, attributions are subcategorised (Prasad et al., 2006) into assertions (Ex.2a) and beliefs (Ex.2b), which imply different degrees of commitment, facts (Ex.2c), expressing evaluation or knowledge, and eventualities (Ex.2d), expressing intention or attitude. (2) a. b. c. d. Mr Abbott said that he will win the election. Mr Abbott thinks he will win the election. Mr Abbott knew that Gillard was in Sydney. Mr Abbott agreed to the public sector cuts. Only assertion attributions necessarily imply a speech act. Their content corresponds to a quotation span and their source is generally referred to in the literature as the speaker. Direct, indirect and mi"
D13-1101,I05-6007,0,0.401373,"Missing"
D13-1157,D10-1049,0,0.187819,"e of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e.,"
D13-1157,H05-1042,1,0.858125,"lecting and ordering the parts of the input to be mentioned in the output text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content plannin"
D13-1157,P09-1010,0,0.0275449,"learns document plans based on Rhetorical Structure Theory (RST; Mann and Thomson, 1988); it therefore has a solid linguistic foundation, but is resource intensive as it assumes access to a text-level discourse parser. We learn document plans automatically using both representations and develop a tractable decoding algorithm for finding the best output, i.e., derivation in our grammar. To the best of our knowledge, this is the first data-driven model to incorporate document planning in a joint end-to-end system. Experimental evaluation on the W EATHER G OV (Liang et al., 2009) and W IN H ELP (Branavan et al., 2009) do1504 mains shows that our approach improves over Konstas and Lapata (2012) by a wide margin. 2 Related Work Content planning is a fundamental component in a natural generation system. Not only does it determine which information-bearing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plan"
D13-1157,W01-1605,0,0.0155037,"l relations augmented with nucleus-satellite information (e.g., Elaboration[N][S] stands for the elaboration relation between the nucleus EDU left-adjoining with the satellite EDU), PRST is the set of production rules of the form PRST ⊆ NRST × {NRST ∪ ΣR } × {NRST ∪ ΣR } associated with a weight for each rule, and D ∈ NRST is the root symbol. Figure 3e gives the discourse tree for the database input of Figure 1b, using GRST . Training In order to obtain the weighted productions of GRST , we use an existing state-of-the-art discourse parser3 (Feng and Hirst, 2012) trained on the RST-DT corpus (Carlson et al., 2001). The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distribution of those in RST-DT, thus empirically supporti"
D13-1157,P01-1023,0,0.111229,"planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998; Karamanis, 2003; Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints relating to discourse coherence and the semantics of the domain. Duboue and McKeown (2001) present perhaps the first empirical approach to content planning. They use techniques from computational biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored outputs using evolutionary algorithms. More recent data-driven work focuses on end-to-end systems rather than individual components, however without taking document planning into account. For example, Kim and Mooney (2010) first define a generative model similar to Liang et al."
D13-1157,W02-2112,0,0.207124,"ents: content planning (selecting and ordering the parts of the input to be mentioned in the output text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by"
D13-1157,P12-1007,0,0.0145589,"is a set of non-terminals corresponding to rhetorical relations augmented with nucleus-satellite information (e.g., Elaboration[N][S] stands for the elaboration relation between the nucleus EDU left-adjoining with the satellite EDU), PRST is the set of production rules of the form PRST ⊆ NRST × {NRST ∪ ΣR } × {NRST ∪ ΣR } associated with a weight for each rule, and D ∈ NRST is the root symbol. Figure 3e gives the discourse tree for the database input of Figure 1b, using GRST . Training In order to obtain the weighted productions of GRST , we use an existing state-of-the-art discourse parser3 (Feng and Hirst, 2012) trained on the RST-DT corpus (Carlson et al., 2001). The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distri"
D13-1157,W13-0113,0,0.0624369,"Missing"
D13-1157,J98-4004,0,0.0212218,"). The W IN H ELP dataset is considerably smaller, and as a result the procedure described in Section 5.1 yields a very sparse grammar. To alleviate this, we horizontally markovized the righthand side of each rule (Collins, 1999; Klein and Manning, 2003).6 After markovization, we obtained a GRSE grammar with 516 rules. On W EATHER G OV, we extracted 434 rules for GRST . On W IN H ELP we could not follow the horizontal markovization procedure, since the discourse trees are already binarized. Instead, we performed vertical markovization, i.e., annotated each non-terminal with their parent node (Johnson, 1998) and obtained a GRST grammar with 419 rules. The model of Konstas and Lapata (2012) has two parameters, namely the number of k-best lists to keep in each derivation, and the order of the language model. We tuned k experimentally on the development set and obtained best results with 60 for W EATHER G OV and 120 for W IN H ELP. We used a trigram model for both domains, trained on each training set. Evaluation We compared two configurations of our system, one with a content planning component based on record type sequences (GRSE ) and 6 When horizontally markovizing, we can encode an arbitrary am"
D13-1157,J04-4001,0,0.0241961,"ing units to talk about, but also arranges them into a structure that creates coherent output. It is therefore not surprising that many content planners have been based on theories of discourse coherence (Hovy, 1993; Scott and de Souza, 1990). Other work has relied on generic planners (Dale, 1988) or schemas (Duboue and McKeown, 2002). In all cases, content plans are created manually, sometimes through corpus analysis. A few researchers recognize that this top-down approach to planning is too inflexible and adopt a generate-and-rank architecture instead (Mellish et al., 1998; Karamanis, 2003; Kibble and Power, 2004). The idea is to produce a large set of candidate plans and select the best one according to a ranking function. The latter is typically developed manually taking into account constraints relating to discourse coherence and the semantics of the domain. Duboue and McKeown (2001) present perhaps the first empirical approach to content planning. They use techniques from computational biology to learn the basic patterns contained within a plan and the ordering among them. Duboue and McKeown (2002) learn a tree-like planner from an aligned corpus of semantic inputs and corresponding human-authored"
D13-1157,C10-2062,0,0.200335,"nt (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of the 2013 Conference on Empirical Methods in"
D13-1157,P03-1054,0,0.0591921,"uk/ikonstas/index.php?page=resources 60 RST-DT W EATHER G OV W IN H ELP 40 20 Topic Change Summary Explanation Temporal Topic-Comment Condition Comparison Evaluation Cause Enablement Background Explanation Contrast Joint Attribution Elaboration 0 Figure 4: Distribution of RST relations on W EATHER G OV, W IN H ELP, and the RST-DT (Williams and Power, 2008). narization). The W IN H ELP dataset is considerably smaller, and as a result the procedure described in Section 5.1 yields a very sparse grammar. To alleviate this, we horizontally markovized the righthand side of each rule (Collins, 1999; Klein and Manning, 2003).6 After markovization, we obtained a GRSE grammar with 516 rules. On W EATHER G OV, we extracted 434 rules for GRST . On W IN H ELP we could not follow the horizontal markovization procedure, since the discourse trees are already binarized. Instead, we performed vertical markovization, i.e., annotated each non-terminal with their parent node (Johnson, 1998) and obtained a GRST grammar with 419 rules. The model of Konstas and Lapata (2012) has two parameters, namely the number of k-best lists to keep in each derivation, and the order of the language model. We tuned k experimentally on the deve"
D13-1157,N12-1093,1,0.271765,"ing which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of"
D13-1157,P09-1011,0,0.0814864,"utput text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper"
D13-1157,W98-1411,0,0.352511,"Missing"
D13-1157,P02-1040,0,0.114646,"e h=1 horizontal siblings plus the mother left-hand side (LHS) non-terminal, in order to uniquely identify the Markov chain. For example, A → B C D becomes A → B hA . . . Bi, hA . . . Bi → C hA . . .Ci, hA . . .Ci → D. 1510 another one based on RST (GRST ). In both cases content plans were extracted from (noisy) unsupervised alignments. As a baseline, we used the original model of Konstas and Lapata (2012). We also compared our model to Angeli et al.’s system (2010), which is state of the art on W EATHER G OV. System output was evaluated automatically, using the BLEU modified precision score (Papineni et al., 2002) with the human-written text as reference. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization and were asked to rate the latter along three dimensions: fluency (is the text grammatical?), semantic correctness (does the meaning conveyed by the text correspond to the database input?) and coherence (is the text comprehensible and logically structured?). Participants used a five point rating scale where a high number indicates better performance. We randomly selected 12 documents from the test s"
D13-1157,W13-2124,0,0.0296492,"Missing"
D13-1157,P04-1011,0,0.0950665,"individual sentences), and surface realization (verbalizing the chosen content in natural language). Traditionally, these components are hand-engineered in order to ensure output of high quality. More recently there has been growing interest in the application of learning methods because of their promise to make generation more robust and adaptable. Examples include learning which content should be present in a document (Duboue and McKeown, 2002; Barzilay and Lapata, 2005), how it should be aligned to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all component"
D13-1157,williams-power-2008-deriving,0,0.0234302,"weighted productions of GRST , we use an existing state-of-the-art discourse parser3 (Feng and Hirst, 2012) trained on the RST-DT corpus (Carlson et al., 2001). The latter contains a selection of 385 Wall Street Journal articles which have been annotated using the framework of RST and an inventory of 78 rhetorical relations, classified into 18 coarse-grained categories (Carlson and Marcu, 2001). Figure 4 gives a comparison of the distribution of relations extracted for the two datasets we used, against the gold-standard annotation of RST-DT. The statistics for the RST-DT corpus are taken from Williams and Power (2008). The relative frequencies of relations on both datasets follow closely the distribution of those in RST-DT, thus empirically supporting the application of the RST framework to our data. We segment each document in our training set into EDUs based on the record-to-text alignments given by the model of Liang et al. (2009) (see Figure 3c). We then run the discourse parser on the resulting EDUs, and retrieve the corresponding discourse tree; the internal nodes are labelled with one of the RST relations. Finally, we replace the leaf EDUs with their respective terminal symbols R(r.t) ∈ ΣR (Figure 3"
D13-1157,N07-1022,0,0.0304879,"ed to utterances (Liang et al., 2009), and how to select a sentence plan among many alternatives (Stent et al., 2004). Beyond isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by treating sentence planning and surface realization as one component (Angeli et al., 2010), by implementing content selection without any document planning (Konstas and Lapata, 2012; Angeli et al., 2010; Kim and Mooney, 2010), or by eliminating content planning entirely (Belz, 2008; Wong and Mooney, 2007). In this paper we present a trainable end-to-end generation system that captures all components of the traditional pipeline, including document planning. Rather than breaking up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Kim and Mooney, 2010), our model performs content planning (i.e., document planning and content selection), sentence planning (i.e., lex1503 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1503–1514, c Seattle, Washington, USA, 18-21 O"
D13-1157,J03-4003,0,\N,Missing
D14-1036,W09-1206,0,0.0606412,"Missing"
D14-1036,W05-0620,0,0.405746,"Missing"
D14-1036,P00-1058,0,0.0769682,"modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefix tree. of non-predictive elementary trees. An example of a PLTAG derivation is given in Figure 2. In step 1, a prediction tree is introduced through substitution, which then allows the adjunction of an adverb in step 2. Step 3 involves the verification of the marker introduced by the prediction tree against the elementary tree for open. In order to efficiently parse PLTAG, Demberg et al. (2013) introduce the concept of fringes. Fringes captu"
D14-1036,J13-4008,1,0.901452,"letions, in any real time application systems, such as dialog processing, and to incrementalize applications such as machine translation (e.g., in speech-tospeech MT). Crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor (Pad´o et al., 2009; Keller, 2010). The present work takes inspiration from the psycholinguistic modeling literature by proposing an iSRL system that is built on top of a cognitively motivated incremental parser, viz., the Psycholinguistically Motivated Tree Adjoining Grammar parser of Demberg et al. (2013). This parser includes a predictive component, i.e., it predicts syntactic structure for upcoming input during incremental processing. This makes PLTAG particularly suitable for iSRL, allowing it to predict incomplete semantic roles as the input string unfolds. Competing approaches, such as iSRL based on an incremental dependency parser, do not share this advantage, as we will discuss in Section 4.3. 2 semantic role labeling is a novel task. Our model builds on an incremental Tree Adjoining Grammar parser (Demberg et al., 2013) which predicts the syntactic structure of upcoming input. This all"
D14-1036,J05-1004,0,0.125834,"ay hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fri"
D14-1036,W07-2416,0,0.0663346,"Missing"
D14-1036,C92-2066,0,0.6767,"edicts the syntactic structure of upcoming input. This allows us to perform incremental parsing and incremental SRL in tandem, exploiting the predictive component of the parser to assign (potentially incomplete) semantic roles on a word-by-word basis. Similar to work on incremental parsing that evaluates incomplete trees (Sangati and Keller, 2013), we evaluate the incomplete semantic structures produced by our model. 3 Psycholinguistically Motivated TAG Demberg et al. (2013) introduce Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG), a grammar formalism that extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with A↓ and are called substitution nodes. Elementary trees that contain a foot node are called auxiliary trees; those that do not are called initial trees. Examples for TAG elementary trees are given in Figure 1a–c. To derive a TAG parse for a sentence, we start with the elementary tree of the head of the sentence and integrate the elementary trees of the"
D14-1036,P10-2012,1,0.843954,"mantic garden paths occur because 301 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 301–312, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics provide semantically informed completions, in any real time application systems, such as dialog processing, and to incrementalize applications such as machine translation (e.g., in speech-tospeech MT). Crucially, any comprehensive model of human language understanding needs to combine an incremental parser with an incremental semantic processor (Pad´o et al., 2009; Keller, 2010). The present work takes inspiration from the psycholinguistic modeling literature by proposing an iSRL system that is built on top of a cognitively motivated incremental parser, viz., the Psycholinguistically Motivated Tree Adjoining Grammar parser of Demberg et al. (2013). This parser includes a predictive component, i.e., it predicts syntactic structure for upcoming input during incremental processing. This makes PLTAG particularly suitable for iSRL, allowing it to predict incomplete semantic roles as the input string unfolds. Competing approaches, such as iSRL based on an incremental depen"
D14-1036,W13-2607,1,0.850665,"sponding nodes in Tv . For simplicity of presentation, we will use a concrete example, see Figure 5. Figure 5a shows the lexicon entries for the words of the sentence Semantic Role Lexicon Recall that Propbank is used to construct the PLTAG treebank, in order to distinguish between arguments and modifiers, which result in elementary trees with substitution nodes, and auxiliary trees, i.e., trees with a foot node, respectively (see Figure 1). Conveniently, we can use the same information to also enrich the extracted lexicon with the semantic role annotations, following the process described by Sayeed and Demberg (2013).1 For arguments, annotations are retained on the substitution node in the parental tree, while for modifiers, the role annotation is displayed on the foot node of the auxiliary tree. Note that we display role annotation on traces that are leaf nodes, 1 Contrary to Sayeed and Demberg (2013) we put role label annotations for PPs on the preposition rather than their NP child, following of the CoNLL 2005 shared task (Carreras and M`arquez, 2005). 2 Prediction tree T in our algorithm is only used during pr verification, so it set to nil for substitution and adjunction operations. 304 Banks refused"
D14-1036,J14-3006,1,0.841387,"nd argument identification. In this respect it is analogous to unlabeled dependency accuracy reported in the parsing literature. We exSystem Comparison We evaluated three configurations of our system. The first configuration (iSRL) uses all semantic roles for each PLTAG lexicon entry, applies the PLTAG parser, IRPA, and both classifiers to perform identification and disambiguation, as described in Section 4. The second one (MajorityBaseline), solves the problem of argument identification and role disambiguation without the classifiers. For the former we employ a set of heuristics according to Lang and Lapata (2014), that rely on gold syntactic dependency information, sourced from CoNLL input. For the latter, we choose the most frequent role given the gold standard dependency relation label for the particular argument. Note that dependencies have been produced in view of the whole sentence and not incrementally. 308 System iSRL-Oracle iSRL Majority-Baseline Malt-Baseline Prec 91.00 81.48 71.05 60.90 Rec 80.26 75.51 58.10 46.14 F1 85.29 78.38 63.92 52.50 prefixes (up to word 10), presumably as it does not benefit from syntactic prediction, and thus cannot generate incomplete triples early in the sentence,"
D14-1036,D07-1062,0,0.165244,"essing as the path from an argument to the predicate can be very informative but is often quite complicated, and depends on the syntactic formalism used. Many paths through the parse tree are likely to occur infrequently (or not at all), resulting in very sparse information for the classifier to learn from. Moreover, as we will discuss in Section 4.4, such path information is not always available when the input is processed incrementally. There is previous SRL work employing Tree Adjoining Grammar, albeit in a non-incremental setting, as a means to reduce the sparsity of syntaxbased features. Liu and Sarkar (2007) extract a rich feature set from TAG derivations and demonstrate that this improves SRL performance. In contrast to incremental parsing, incremental 302 (a) NP (b) S NNS NP↓ Banks (d) S1 (c) VP VP AP VB RB open rarely VP* A1 A0 NP1 ↓ VP11 Banks refused nsbj a B↓ a B C↓ a B↓ b (a) valid open aux today hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) aux"
D14-1036,W08-2121,0,0.0710313,"Missing"
D14-1036,J93-2004,0,0.0497726,"anks (d) S1 (c) VP VP AP VB RB open rarely VP* A1 A0 NP1 ↓ VP11 Banks refused nsbj a B↓ a B C↓ a B↓ b (a) valid open aux today hA0,Banks,refusedi hA1,to,refusedi hA1,Banks,openi hAM-TMP,today,openi tmod Figure 4: Syntactic dependency graph with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a p"
D14-1036,J08-2001,0,0.0889078,"Missing"
D14-1036,W00-1307,0,0.0413792,"h with semantic role annotation and the accompanying semantic triples, for Banks refused to open today. S S C↓ to xcomp Figure 1: PLTAG lexicon entries: (a) and (b) initial trees, (c) auxiliary tree, (d) prediction tree. S AM-TMP A1 fix trees and its new current fringe f 0 and enters it into cell (i + 1, f 0 ). Demberg et al. (2013) convert the Penn Treebank (Marcus et al., 1993) into TAG format by enriching it with head information and argument/modifier information from Propbank (Palmer et al., 2005). This makes it possible to decompose the Treebank trees into elementary trees as proposed by Xia et al. (2000). Prediction trees can be learned from the converted Treebank by calculating the connection path (Mazzei et al., 2007) at each word in a tree. Intuitively, a prediction tree for word wn contains the structure that is necessary to connect wn to the prefix tree w1 . . . wn−1 , but is not part of any of the elementary trees of w1 . . . wn−1 . Using this lexicon, a probabilistic model over PLTAG operations can be estimated following Chiang (2000). C c (b) invalid Figure 3: The current fringe (dashed line) indicates where valid substitutions can occur. Other substitutions result in an invalid prefi"
D14-1036,W09-1201,0,\N,Missing
D14-1036,Q13-1010,1,\N,Missing
D16-1168,P09-1068,0,0.0697666,"essing, pages 1617–1628, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al., 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991). Motivated by this need for thematically diverse, highly coherent stories, we address the problem of story rewriting, or transforming human-authored stories into novel, coherent stories in a new theme. Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) from scratch, we instead begin from an existing story and iteratively edit it towards a thematically novel but –most crucially– semantically compatible story. This approach allows us to reuse much, but not all, of the syntactic and semantic structure of the original text, resulting in the creation of more coherent and solvable math word problems. We define a theme to be a collection of reference texts, such as a movie script or series of books. Given a theme, the rewrite algorithm constructs new texts by substituting thematically a"
D16-1168,P11-1020,0,0.0392888,"l requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automati"
D16-1168,D14-1082,0,0.0399284,"Missing"
D16-1168,W14-3348,0,0.0586977,"Missing"
D16-1168,W08-1105,0,0.0318061,"s method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external parap"
D16-1168,P05-1045,0,0.0718412,"Missing"
D16-1168,D11-1108,0,0.0684546,"Missing"
D16-1168,N13-1092,0,0.0381253,"Missing"
D16-1168,S13-1035,0,0.0126664,"structed from fan-authored scripts of the first 10 episodes of the show (Springfield, 2016) totaling 1370 words. Since our thematic options are taken from arbitrary text, we use the lists of offensive terms published by The Racial Slur database (Database, 2016) and FrontGate Media (Media, 2016) to filter out offensive content. To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b). For syntactic compatibility score Syn (Equation 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011). For SemLex , P airSim and Analogy (Equations 6-8) we use the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity. Finally, we tune the parameters of our model (Equation 2) on the development set S TARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references. Evaluation We compare two ablated configurations of our method against our"
D16-1168,N15-1113,0,0.043938,"Missing"
D16-1168,W11-2123,0,0.00909718,"pringfield, 2016) totaling 1370 words. Since our thematic options are taken from arbitrary text, we use the lists of offensive terms published by The Racial Slur database (Database, 2016) and FrontGate Media (Media, 2016) to filter out offensive content. To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b). For syntactic compatibility score Syn (Equation 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011). For SemLex , P airSim and Analogy (Equations 6-8) we use the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity. Finally, we tune the parameters of our model (Equation 2) on the development set S TARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references. Evaluation We compare two ablated configurations of our method against our full model (F ULL): -S YN that only uses semantic and thematici"
D16-1168,D14-1058,1,0.830597,"iven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad defi"
D16-1168,P16-1084,0,0.0294871,"), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the Project Gutenberg can be a theme, or the script of a single movie, or a sampling of fan fiction from the Internet. This flexibility adds to the utility of our work, a"
D16-1168,N15-1022,1,0.807902,"7) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Si"
D16-1168,P13-1151,0,0.0128537,"o stories through the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven"
D16-1168,Q15-1042,1,0.885046,"Missing"
D16-1168,N16-1136,1,0.804369,"2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the Project Gutenberg can be a theme, or the script of a single movie, or a sampling of fan fiction from the Internet. This flexibility adds to the u"
D16-1168,P14-1026,1,0.822938,"ckrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as them"
D16-1168,A97-1039,0,0.0925939,"tyre and Lapata (2009; 2010) address story generation through the automatic deduction and reassembly of scripts (Schank and Abelson, 1977), or structured representations of events and their participants, and causal relationships involved. Leveraging the automatic script learning methods of Chambers and Jurafsky (2009), McIntyre and Lapata (2010) learn candidate entity-centered plot graphs, or possible events involving the entity and an ordering between these events, with the use of a genetic algorithm. Then plots are compiled into stories through the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hw"
D16-1168,P14-2050,0,0.0415559,"lists of offensive terms published by The Racial Slur database (Database, 2016) and FrontGate Media (Media, 2016) to filter out offensive content. To prohibit overgeneration, we forbid the transformation of stop words or math-specific words (Survivors, 2013; Koncel-Kedziorski et al., 2015b). For syntactic compatibility score Syn (Equation 4) we use the English Fiction subset of the Google Syntactic N-grams corpus (Goldberg and Orwant, 2013) and train a 3-gram language model using KenLM (Heafield, 2011). For SemLex , P airSim and Analogy (Equations 6-8) we use the pretrained word embeddings of Levy and Goldberg (2014). These embeddings are trained using dependency contexts rather than windows of adjacent words, allowing them to capture functional word similarity. Finally, we tune the parameters of our model (Equation 2) on the development set S TARdev and pick those values5 that maximize METEOR score (Denkowski and Lavie, 2014) against 3 human references. Evaluation We compare two ablated configurations of our method against our full model (F ULL): -S YN that only uses semantic and thematicity components and does not incorporate the syntactic compatibility score, -S EM replaces the semantic coher5 We set α"
D16-1168,P14-5010,0,0.00577863,"Missing"
D16-1168,P09-1025,0,0.190599,"the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1617–1628, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al., 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991). Motivated by this need for thematically diverse, highly coherent stories, we address the problem of story rewriting, or transforming human-authored stories into novel, coherent stories in a new theme. Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) from scratch, we instead begin from an existing story and iteratively edit it towards a thematically novel but –most crucially– semantically compatible story. This approach allows us to reuse much, but not all, of the syntactic and semantic structure of the original text, resulting in the creation of more coherent and solvable math word problems. We define a theme to be a collection of reference texts, such as a movie script or series of books. Given a theme, the re"
D16-1168,P10-1158,0,0.294347,"rical Methods in Natural Language Processing, pages 1617–1628, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al., 2002), and personalizing word problems increases student understanding, engagement, and performance in the problem solving process (Hart, 1996; Davis-Dorsey et al., 1991). Motivated by this need for thematically diverse, highly coherent stories, we address the problem of story rewriting, or transforming human-authored stories into novel, coherent stories in a new theme. Rather than synthesizing first a story plot (McIntyre and Lapata, 2009; McIntyre and Lapata, 2010) or script (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016) from scratch, we instead begin from an existing story and iteratively edit it towards a thematically novel but –most crucially– semantically compatible story. This approach allows us to reuse much, but not all, of the syntactic and semantic structure of the original text, resulting in the creation of more coherent and solvable math word problems. We define a theme to be a collection of reference texts, such as a movie script or series of books. Given a theme, the rewrite algorithm constructs n"
D16-1168,D15-1118,0,0.0160036,"ch as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic"
D16-1168,N16-1098,0,0.0219292,", discourse relations, and solvability is essential. Previous work mainly focuses on rewriting single sentences. Second, we build a theme from a text corpus and show how the stories can be adapted to new themes. Third, our method leverages the human-authored story to capture the semantic skeleton and the plot of the current story, rather than synthesizing the story plot. To our knowledge, we are the first to introduce a text rewriting formulation for story generation. Story generation has been of long interest to AI researchers (Meehan, 1976; Lebowitz, 1987; Turner, 1993; Liu and Singh, 2002; Mostafazadeh et al., 2016). Recent methods in story generation first synthesize candidate plots for a story and then compile those plots into text. Li et al. (2013) use crowdsourcing to build plot graphs. McIntyre and Lapata (2009; 2010) address story generation through the automatic deduction and reassembly of scripts (Schank and Abelson, 1977), or structured representations of events and their participants, and causal relationships involved. Leveraging the automatic script learning methods of Chambers and Jurafsky (2009), McIntyre and Lapata (2010) learn candidate entity-centered plot graphs, or possible events invol"
D16-1168,D15-1202,0,0.0216471,"roduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a"
D16-1168,D16-1117,0,0.0127235,"es take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the"
D16-1168,D15-1044,0,0.029105,"highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (X"
D16-1168,D15-1171,1,0.815083,"ques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, the collection of all Science Fiction stories from the Project Gutenberg can be a theme, or the scri"
D16-1168,D15-1135,0,0.0661954,"under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is def"
D16-1168,P08-1040,0,0.0150168,"apata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Z"
D16-1168,D11-1038,0,0.0274259,"izer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and"
D16-1168,P12-1107,0,0.0327745,"Missing"
D16-1168,Q16-1029,0,0.0113683,"), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3"
D16-1168,N10-1056,0,0.0213029,"3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are introduced by Petersen and Ostendorf (2007), Vickrey and Koller (2008), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Konc"
D16-1168,D15-1096,0,0.013681,"), and Siddharthan (2004). Most data-driven approaches take advantage of machine translation techniques, use source-target sentence pairs, and learn rewrite operations (Yatskar et al., 2010; Woodsend and Lapata, 2011a), or use additional external paraphrasing resources (Xu et al., 2016). Finally, this work is related to those on automatically solving math word problems. Specific topics include number word problems (Shi et al., 2015), logic puzzle problems (Mitra and Baral, 2015), arithmetic word problems (Hosseini et al., 2014; Roy and Roth, 2015), algebra word problems (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015a; Roy et al., 2016), and geometry word problems (Seo et al., 2015; Seo et al., 2014). Several datasets of word problems are available (Koncel-Kedziorski et al., 2016; Huang et al., 2016), though none address the need for thematic text. 3 Problem Formulation Our system takes as input a story s and a theme t, and outputs the best rewrite s∗ from generated candidates S. A theme t is defined as a textual corpus that describes a topic or a domain. This is an intentionally broad definition that allows a variety of textual resources to serve as themes. For example, th"
D16-1168,C10-1152,0,0.0280014,"gh the use of a rule-based text surface realizer (Lavoie and Rambow, 1997) and reranked using a language model. Polozov et al. (2015) automatically generate math word problems tailored to a student’s interest using Answer Set Programming to satisfy a collection of pedagogical and narrative requirements. This method naturally produces highly coherent, personalized story problems that meet pedagogical requirements, at the expense of building the thematic ontologies and discourse constraints by hand.2 Additionally, there is related work in text simplification (Wubben et al., 2012; Kauchak, 2013; Zhu et al., 2010; Vanderwende et al., 2007; Woodsend and Lapata, 2011b; Hwang et al., 2015), sentence 2 According to Polozov et al. (2015) building small thematic ontologies of types, relations, and discourse tropes (100-200 entries) for each of only 3 literary settings took 1-2 person months. compression (Filippova and Strube, 2008; Rush et al., 2015), and paraphrasing (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Ganitkevitch et al., 2011). All these tasks are focused on rewriting sentences under a predefined set of constraints, such as simplicity. Different rule-based and data-driven approaches are int"
D18-1192,P16-1223,0,0.0224346,"urce code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamicallydetermined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Dong and Lapata, 2016; Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). Our model also uses a grammar-aware decoder similar to Yin and Neubig (2017) to generate syntactically valid parse trees, augmented with a twostep attention mechanism (Chen et al., 2016), followed by a supervised copying mechanism (Gu et al., 2016a) over the class environment. Recent models for mapping NL to code have been evaluated on datasets containing highly templated code for card games (Hearthstone & MTG; Ling et al., 2016), or manually labeled per-line comments (DJANGO; Oda et al., 2015). These datasets contain ∼20,000 programs with short textual descriptions possibly paired with categorical data, whose values need to be copied onto the resulting code from a single domain. In this work, we collect a new dataset of over 100,000 NL and code 1650 pairs, together with the"
D18-1192,P16-1004,0,0.44428,"enerator, we use a specialized neural encoder-decoder model that (a) encodes the NL together with representations based on subword units for environment identifiers (member variables, methods) and data types, and (b) decodes the resulting code using an attention mechanism with multiple steps, by first attending to the NL, and then to the variables and methods, thus also learning to copy variables and methods. This twostep attention helps the model to match words in the NL with representations of the identifiers in the environment. Rather than directly generating the output source code tokens (Dong and Lapata, 2016; Iyer et al., 2017), the decoder generates production rules from the grammar of the target programming language similar to Rabinovich et al. (2017), Yin and Neubig (2017), and Krishnamurthy et al. (2017) and therefore, guarantees the syntactic wellformedness of the output. To train our model, we collect and release CONCODE, a new dataset comprising over 100,000 (class environment, NL, code) tuples by gathering Java files containing method documentation from public Github repositories. This is an order of magnitude larger than existing datasets that map NL to source code for a general purpose"
D18-1192,P16-1154,0,0.381918,"ion of a(i) given q (i) and the environment (see Figure 2). We evaluate a number of encoder-decoder models that generate source code derivations from NL and the class environment. Our best model encodes all environment components broken down into subword units (Sennrich et al., 2016) separately, using Bi-LSTMs and decodes these contextual representations to produce a sequence of valid production rules that derive syntactically valid source code. The decoder also uses a two-step attention mechanism to match words in the NL with environment components, and then uses a supervised copy mechanism (Gu et al., 2016a) to incorporate environment elements in the resulting code. We describe this architecture below. 3.1 Encoder The encoder computes contextual representations of the NL and each component in the environment. Each word of the NL, qi , is embedded into a high dimensional space using Identifier matrix I (denoted as qi ) followed by the application of a nlayer bidirectional LSTM (Hochreiter and Schmidhuber, 1997). The hidden states of the last layer (h1 , · · · , hz ) are passed on to the attention layer, while the hidden states at the last token are used to initialize the decoder. place to scalar"
D18-1192,P17-1089,1,0.84755,"ialized neural encoder-decoder model that (a) encodes the NL together with representations based on subword units for environment identifiers (member variables, methods) and data types, and (b) decodes the resulting code using an attention mechanism with multiple steps, by first attending to the NL, and then to the variables and methods, thus also learning to copy variables and methods. This twostep attention helps the model to match words in the NL with representations of the identifiers in the environment. Rather than directly generating the output source code tokens (Dong and Lapata, 2016; Iyer et al., 2017), the decoder generates production rules from the grammar of the target programming language similar to Rabinovich et al. (2017), Yin and Neubig (2017), and Krishnamurthy et al. (2017) and therefore, guarantees the syntactic wellformedness of the output. To train our model, we collect and release CONCODE, a new dataset comprising over 100,000 (class environment, NL, code) tuples by gathering Java files containing method documentation from public Github repositories. This is an order of magnitude larger than existing datasets that map NL to source code for a general purpose language (MTG from L"
D18-1192,D17-1160,0,0.0484673,"odels have proved effective in mapping NL to logical forms and also for directly producing general purpose programs. Ling et al. (2016) use a sequence-to-sequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamicallydetermined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Dong and Lapata, 2016; Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). Our model also uses a grammar-aware decoder similar to Yin and Neubig (2017) to generate syntactically valid parse trees, augmented with a twostep attention mechanism (Chen et al., 2016), followed by a supervised copying mechanism (Gu et al., 2016a) over the class environment. Recent models for mapping NL to code have been evaluated on datasets containing highly templated code for card games (Hearthstone & MTG; Ling et al., 2016), or manually labeled per-line comments (DJANGO; Oda et al., 2015). These datasets contain ∼20,000 programs with short textual descriptions po"
D18-1192,N13-1103,0,0.111423,"Missing"
D18-1192,J13-2005,0,0.0408214,"Missing"
D18-1192,P16-1057,0,0.238854,"Missing"
D18-1192,D16-1197,0,0.069616,"Missing"
D18-1192,D15-1166,0,0.0709255,"r(nt ), st−1 , snt ) (2) We use an embedding matrix N to embed nt and matrix A to embed at−1 and par(nt ). If at−1 is a rule that generates a terminal node that represents an identifier or literal, it is represented using a special rule IdentifierOrLiteral to collapse all these rules into a single previous rule. Two-step Attention At time step t, the decoder first attends to every token in the NL representation, hi , using the current decoder state, st , to compute a set of attention weights αt , which are used to combine hi into an NL context vector zt . We use a general attention mechanism (Luong et al., 2015), extended to perform multiple steps. exp(sTt Fhi ) αt,i = P exp(sTt Fhi ) Xi zt = αt,i hi i The context vector zt is used to attend over every type (return type) and variable (method) name in the environment, to produce attention weights βt that are used to combine the entire context x = [t : v : r : m] into an environment context vector et .2 exp(ztT Gxj ) βt,j = P T j exp(zt Gxj ) X et = βt,j xj j Finally, ct is computed using the decoder state and both context vectors zt and et : ˆ [st : zt : et ]) ct = tanh(W 2 “:” denotes concatenation. st−1 FormalParameters (nt ) IdentifierNt—>identifie"
D18-1192,P15-1085,0,0.0242859,"ements is the vector to be augmented, and that the method must take in a scalar parameter as the element to be added. The model also needs to disambiguate between the member variables vecElements and weights. Introduction Natural language can be used to define complex computations that reuse the functionality of rich, existing code bases. However, existing approaches for automatically mapping natural language (NL) to executable code have considered limited language or code environments. They either assume fixed code templates (i.e., generate only parts of a method with a predefined structure; Quirk et al., 2015), a fixed context (i.e., generate the body of the same method within a single fixed class; Ling et al., 2016), or no context at all (i.e., generate code tokens from the text alone; Oda et al., 2015). In this paper, we introduce new data and methods for learning to map language to source code within the context of a real-world programming environment, with application to generating member functions from documentation for automatically collected Java class environments. The presence of rich context provided by an existing code environment better approximates the way programmers capitalize on cod"
D18-1192,P17-1105,0,0.0698702,"Neural encoder-decoder models have proved effective in mapping NL to logical forms and also for directly producing general purpose programs. Ling et al. (2016) use a sequence-to-sequence model with attention and a copy mechanism to generate source code. Instead of directly generating a sequence of code tokens, recent methods focus on constrained decoding mechanisms to generate syntactically correct output using a decoder that is either grammar-aware or has a dynamicallydetermined modular structure paralleling the structure of the abstract syntax tree (AST) of the code (Dong and Lapata, 2016; Rabinovich et al., 2017; Krishnamurthy et al., 2017; Yin and Neubig, 2017). Our model also uses a grammar-aware decoder similar to Yin and Neubig (2017) to generate syntactically valid parse trees, augmented with a twostep attention mechanism (Chen et al., 2016), followed by a supervised copying mechanism (Gu et al., 2016a) over the class environment. Recent models for mapping NL to code have been evaluated on datasets containing highly templated code for card games (Hearthstone & MTG; Ling et al., 2016), or manually labeled per-line comments (DJANGO; Oda et al., 2015). These datasets contain ∼20,000 programs with s"
D18-1192,P16-1162,0,0.024094,"ng source code. The environment comprises a list of variables names v (i) 1..|v(i) | and their corresponding types t(i) 1..|t(i) |, as well as method names m(i) 1..|m(i) |and their return types 1 The method parameters and body can be used as well but we leave this to future work. 1644 r(i) 1..|r(i) |. Our goal is to generate the derivation of a(i) given q (i) and the environment (see Figure 2). We evaluate a number of encoder-decoder models that generate source code derivations from NL and the class environment. Our best model encodes all environment components broken down into subword units (Sennrich et al., 2016) separately, using Bi-LSTMs and decodes these contextual representations to produce a sequence of valid production rules that derive syntactically valid source code. The decoder also uses a two-step attention mechanism to match words in the NL with environment components, and then uses a supervised copy mechanism (Gu et al., 2016a) to incorporate environment elements in the resulting code. We describe this architecture below. 3.1 Encoder The encoder computes contextual representations of the NL and each component in the environment. Each word of the NL, qi , is embedded into a high dimensional"
D18-1192,P17-1041,0,0.40752,"r variables, methods) and data types, and (b) decodes the resulting code using an attention mechanism with multiple steps, by first attending to the NL, and then to the variables and methods, thus also learning to copy variables and methods. This twostep attention helps the model to match words in the NL with representations of the identifiers in the environment. Rather than directly generating the output source code tokens (Dong and Lapata, 2016; Iyer et al., 2017), the decoder generates production rules from the grammar of the target programming language similar to Rabinovich et al. (2017), Yin and Neubig (2017), and Krishnamurthy et al. (2017) and therefore, guarantees the syntactic wellformedness of the output. To train our model, we collect and release CONCODE, a new dataset comprising over 100,000 (class environment, NL, code) tuples by gathering Java files containing method documentation from public Github repositories. This is an order of magnitude larger than existing datasets that map NL to source code for a general purpose language (MTG from Ling et al. (2016) has 13k examples), contains a larger variety of output code templates than existing datasets built for a specific domain, and is the"
D18-1192,P02-1040,0,0.103067,"Missing"
D18-1192,P11-1060,0,\N,Missing
D18-1432,E17-2029,0,0.0275008,"tputs by language-model rescoring and sampling. A lot of recent works explore the use of additional training signals and VAE setups in dialogue generation. In contrast to this paper, they do not focus explicitly on coherence: Asghar et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) use a RL scenario with length as reward signal. Li et al. (2017b) add an adversarial discriminator to provide RL rewards (discriminating between human and machine outputs), Xu et al. (2017) use a full adversarial training setup. The most recent works explore the usage of VAEs: Cao and Clark (2017) explore a vanilla VAE setup conditioned on dual encoder (for contexts and responses) during training, the model of Serban et al. (2017) uses a VAE in a hierarchical E-D model. Shen et al. (2017) use a cVAE conditioned on sentiment and response genericity (based on a handwritten list of phrases). Shen et al. (2018) combine a cVAE with a plain VAE in an adversarial fashion. We also draw on ideas from other areas than dialogue generation to build our models: Tu et al. (2017)’s context gates originate from machine translation and Hu et al. (2017)’s cVAE training stems from free-text generation. 3"
D18-1432,P17-1045,0,0.045199,"02) used by the vast majority of recent dialogue generation works (Zhao et al., 2017; Yao et al., 2017; Li et al., 2017a, 2016c; Sordoni et al., 2015; Li et al., 2016a; Ghazvininejad et al., 2017). BLEU in this paper refers to the default BLEU-4, but we also report on lower n-gram scores (B1, B2, B3).9 • Coh – our novel GloVe-based coherence score calculated using Eq (1) showing the semantic distance of dialogue contexts and generated responses. • D-1, D-2, D-Sent – common metrics used to evaluate the diversity of generated responses (e.g. Li et al., 2016a; Xu et al., 2017; Xing et al., 2017; Dhingra et al., 2017): the proportion of distinct unigrams, bigrams, and sentences in the outputs. 5 Results All model variants described in Section 4 are trained on both OST and fOST datasets. Tables 2 and 3 present the scores of all models tested on the OST and fOST test sets, respectively. Note that in addition to testing the models on the respective test sections of their training datasets, we also test them on the other dataset (OST-trained models on fOST and vice-versa). This way, we can observe the performance of the fOST-trained models in more noisy contexts and see how good the OSTtrained models are when"
D18-1432,S17-1008,0,0.0381174,"thm, similar to Shao et al. (2017) in addition to using a self-attention model. Mou et al. (2016) predict keywords for the output in a preprocessing step while Wu et al. (2018) preselect a vocabulary subset to be used for decoding. Li et al. (2016b) focus specifically on personality generation (using personality embeddings) and Wang et al. (2017) promote topic-specific outputs by language-model rescoring and sampling. A lot of recent works explore the use of additional training signals and VAE setups in dialogue generation. In contrast to this paper, they do not focus explicitly on coherence: Asghar et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) use a RL scenario with length as reward signal. Li et al. (2017b) add an adversarial discriminator to provide RL rewards (discriminating between human and machine outputs), Xu et al. (2017) use a full adversarial training setup. The most recent works explore the usage of VAEs: Cao and Clark (2017) explore a vanilla VAE setup conditioned on dual encoder (for contexts and responses) during training, the model of Serban et al. (2017) uses a VAE in a hierarchical E-D model. Shen et al. (2017) use a cVAE conditioned on sent"
D18-1432,K16-1002,0,0.0755824,"is, quantifying the contributions that come from effective modeling of coherence into our models. All our experimental code is freely available on GitHub.1 2 Coherence-based Dialogue Generation Our model aims to generate responses given a dialogue context, incorporating measures of coherence estimated purely from the training data. We propose the following enhancements to the attention-based E-D architecture (Bahdanau et al., 2015; Luong et al., 2015): • We introduce a stochastic latent variable z conditioned on previous dialogue context to store the global information about the conversation (Bowman et al., 2016; Chung et al., 2015; Li and Jurafsky, 2017; Hu et al., 2017). 1 https://github.com/XinnuoXu/CVAE_Dial • We force the model to condition on the measure of coherence explicitly by encoding a latent variable (code) c learned from data. • We incorporate a context gate (Tu et al., 2017) that dynamically controls the ratio at which the generated words in the response derive directly from the coherence-enhanced dialogue context or the previously generated parts of the response. In the rest of this section, we introduce the measure of coherence (Section 2.1), we present an overview of our model (Sect"
D18-1432,P17-4012,0,0.0244374,"ence and diversity metrics (cf. Section 4.2) between OST and fOST. Unsurprisingly, coherence for fOST is much higher than OST, with a slightly higher diversity. We list dialogue examples for different coherence scores in Supplemental Material B. Dataset for Coherence Measure In order to accurately measure coherence on our domain using the semantic distance as defined in Section 2.1, we train GloVe embeddings on the full OpenSubtitles corpus (i.e. 100K movies). 4 Experiments Our generator model, ablative variants, and baselines are implemented using the publicly available OpenNMT-py framework (Klein et al., 2017) based on Bahdanau et al. (2015) and Luong et al. (2015). We used the publicly available glovepython package8 to implement our coherence measure. We experiment on two versions of our model: (1) cVAE with the coherence context gate as described in Section 2.3 (cVAE-XGate), (2) cVAE with the original context gate implementation of 5 The coherence score is calculated as shown in Eq (1). We observed that the scores on the training set follow a normal distribution with a slight tail on the negatively correlated side, so we fit a normal distribution to the data with parameters N (0.25, 0.22) and set"
D18-1432,W04-3250,0,0.0670261,"Missing"
D18-1432,N16-1014,0,0.675039,"End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous attempts to model diversity and coherence by enhancing all three aspects of the learning process: the data, the model, and the objective function. While previous research has addressed these aspects individually, this paper is the first to address all three in a unified framework. Instead of using existing linguistic knowledge or labeled datasets, we aim to control for coherence by learning directly from data"
D18-1432,P16-1094,0,0.570159,"End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous attempts to model diversity and coherence by enhancing all three aspects of the learning process: the data, the model, and the objective function. While previous research has addressed these aspects individually, this paper is the first to address all three in a unified framework. Instead of using existing linguistic knowledge or labeled datasets, we aim to control for coherence by learning directly from data"
D18-1432,D17-1019,0,0.118219,"sponse B-Coh: Well, I got water. B-Incoh: I don’t know. B-Coh: Specifically the stove. B-Incoh: Let’s go for a walk. Figure 1: Examples of conversational history (left) with two alternative responses to follow it (right): (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address t"
D18-1432,L16-1147,0,0.0334699,"s of the generative model. Given a dialogue context x and an expected coherence value c, the context encoder first encodes the dialogue context into a hidden state h. The prior network then generates a sample z0 conditioned on the dialogue context. The decoder is initialized with s, i.e., the concatenation of h, z and c. During decoding, the next word is generated via the context gate modulating between the attention-reweighted context and the previously generated words of the response. 3 Dataset and Filtering Dataset for Generator We train and evaluate our models on the OpenSubtitles corpus (Lison and Tiedemann, 2016) with automatic dialogue turn segmentation (Lison and Meena, 2016).4 A training pair consists of a dialogue context and a corresponding response. We consider three consecutive turns as the dialogue context and the following turn as the response. From a total of 65M instances, we select those that have context and response lengths of less than 120 and 30 words, respectively. We create two datasets: 1. OST (plain OpenSubtitles) consists of 2M/4K/4K instances as our training/development/test sets, selected randomly from the whole corpus; 2. fOST (filtered OpenSubtitles) contains the same amount o"
D18-1432,D15-1166,0,0.122553,"Missing"
D18-1432,C16-1316,0,0.0565902,"herent (8). 6 Related Work Our work fits into the context of the very active area of end-to-end generative conversation models, where neural E-D approaches have been first applied by Vinyals and Le (2015) and extended by many others since. Many works address the lack of diversity and coherence in E-D outputs (Sountsov and Sarawagi, 2016; Wei et al., 2017) but do not attempt to model coherence directly, unlike our work: Li et al. (2016a) use anti-LM reranking; Li et al. (2016c) modify the beam search decoding algorithm, similar to Shao et al. (2017) in addition to using a self-attention model. Mou et al. (2016) predict keywords for the output in a preprocessing step while Wu et al. (2018) preselect a vocabulary subset to be used for decoding. Li et al. (2016b) focus specifically on personality generation (using personality embeddings) and Wang et al. (2017) promote topic-specific outputs by language-model rescoring and sampling. A lot of recent works explore the use of additional training signals and VAE setups in dialogue generation. In contrast to this paper, they do not focus explicitly on coherence: Asghar et al. (2017) use reinforcement learning with human-provided feedback, Li et al. (2017a) u"
D18-1432,P02-1040,0,0.101228,"Missing"
D18-1432,D14-1162,0,0.0888307,"ference on Empirical Methods in Natural Language Processing, pages 3981–3991 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics the previous two utterances and containing rich thematic words, whereas the response “Let’s go for a walk.” is unrelated and uninteresting. In order to obtain coherent responses, we present three generic enhancements to existing encoder-decoder (E-D) models: 1. We define a measure of coherence simply as the averaged word embedding similarity between the words of the context and the response computed using GloVe vectors (Pennington et al., 2014). 2. We filter a corpus of conversations based on our measure of coherence, which leaves us with context-response pairs that are both topically coherent and lexically diverse. 3. We train an E-D generator recast as a conditional Variational Autoencoder (cVAE; Zhao et al., 2017) model that incorporates two latent variables, one for encoding the context and another for conditioning on the measure of coherence, trained jointly as in Hu et al. (2017). We then decode using a context gate (Tu et al., 2017) to control the generation of words that directly relate to the most topical words of the conte"
D18-1432,P15-1152,0,0.0877581,"to follow it (right): (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This p"
D18-1432,N15-1020,0,0.212013,": (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous"
D18-1432,D16-1158,0,0.0534187,"lstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring high-frequency word sequences (Sountsov and Sarawagi, 2016; Wei et al., 2017). To address these problems, Li et al. (2016a) and Li et al. (2017a) attempt to promote diversity by improving the objective function, but do not model diversity explicitly. Serban et al. (2017) focus on This paper extends previous attempts to model diversity and coherence by enhancing all three aspects of the learning process: the data, the model, and the objective function. While previous research has addressed these aspects individually, this paper is the first to address all three in a unified framework. Instead of using existing linguistic knowledge or labeled datasets,"
D18-1432,D17-1228,0,0.0292964,"Missing"
D18-1432,D17-1065,0,0.182531,"ndard responses (Papineni et al., 2002) used by the vast majority of recent dialogue generation works (Zhao et al., 2017; Yao et al., 2017; Li et al., 2017a, 2016c; Sordoni et al., 2015; Li et al., 2016a; Ghazvininejad et al., 2017). BLEU in this paper refers to the default BLEU-4, but we also report on lower n-gram scores (B1, B2, B3).9 • Coh – our novel GloVe-based coherence score calculated using Eq (1) showing the semantic distance of dialogue contexts and generated responses. • D-1, D-2, D-Sent – common metrics used to evaluate the diversity of generated responses (e.g. Li et al., 2016a; Xu et al., 2017; Xing et al., 2017; Dhingra et al., 2017): the proportion of distinct unigrams, bigrams, and sentences in the outputs. 5 Results All model variants described in Section 4 are trained on both OST and fOST datasets. Tables 2 and 3 present the scores of all models tested on the OST and fOST test sets, respectively. Note that in addition to testing the models on the respective test sections of their training datasets, we also test them on the other dataset (OST-trained models on fOST and vice-versa). This way, we can observe the performance of the fOST-trained models in more noisy contexts and se"
D18-1432,D17-1233,0,0.0335803,"Missing"
D18-1432,P17-1061,0,0.395973,"neural models in terms of BLEU score as well as metrics of coherence and diversity. 1 Response B-Coh: Well, I got water. B-Incoh: I don’t know. B-Coh: Specifically the stove. B-Incoh: Let’s go for a walk. Figure 1: Examples of conversational history (left) with two alternative responses to follow it (right): (BCoh) a more coherent, topical utterance, and (B-Incoh) a generic, inconsistent response. model structure without any upgrades to the objective function. Other works control the style of the output by leveraging external resources (Hu et al. (2017): sentiment classifier, time annotation; Zhao et al. (2017): dialogue acts) or focus on wellstructured input such as paragraphs (Li and Jurafsky, 2017). Introduction End-to-end neural response generation methods are promising for developing open domain dialogue systems as they allow to learn from very large unlabeled datasets (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015). However, these models have also been shown to generate generic, uninformative, and non-coherent replies (e.g., “I don’t know.” in Figure 1), mainly due to the fact that neural systems tend to settle for the most frequent options, thus penalizing length and favoring"
D19-5601,W18-2716,0,0.0593982,"Missing"
D19-5601,W04-1013,0,0.0851055,"-text NLG and MT along two axes: • MT+NLG: RotoWire, WMT19, Monolingual RotoWire refers to the RotoWire dataset (Wiseman et al., 2017) (train/valid), WMT19 refers to the set of parallel corpora allowable by the WMT 2019 English-German task, and Monolingual refers to monolingual data allowable by the same WMT 2019 task, pre-trained embeddings (e.g., GloVe (Pennington et al., 2014)), pre-trained contextualized embeddings (e.g., BERT (Devlin et al., 2019)), pre-trained language models (e.g., GPT-2 (Radford et al., 2019)). Textual Accuracy Measures: We used BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for texutal accuracy compared to reference summaries. Content Accuracy Measures: We evaluate the fidelity of the generated content to the input data using relation generation (RG), content selection (CS), and content ordering (CO) metrics (Wiseman et al., 2017). 2 model for both languages together, using a shared BPE vocabulary obtained from target game summaries and by prefixing the target text with the target language indicator. For MT and MT+NLG tracks, they mined the in-domain data by extracting basketball-related texts from Newscrawl when one of the following conditions are m"
D19-5601,D14-1162,0,0.0824718,"Missing"
D19-5601,D15-1044,0,0.0588225,"types of inputs. The results of the shared task are summarized in Sections 3 and 4. Introduction 2 Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are now a workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 3rd Workshop on Neural Machine Translation and Generation (WNGT 2019) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization (Rush et al., 2015), NLG from structured data (Wen et al., 2015), dialog response generation (Vinyals and Le, 2015), among others). Overall, the workshop was held with two goals. First, it aimed to synthesize the current state of knowledge in neural machine translation and generation: this year we continued to encourage submissions that not only advance the state of the art through algorithmic advances, but also analyze and understand the current state of the art, pointing to future research directions. Towards this Summary of Research Contributions We published a call for long papers, extended abstracts for pre"
D19-5601,W18-6502,0,0.0159592,"ocument-level Generation and Translation # documents Avg. # tokens (En) Avg. # tokens (De) Vocabulary size (En) Vocabulary size (De) The first shared task at the workshop focused on document-level generation and translation. Many recent attempts at NLG have focused on sentencelevel generation (Lebret et al., 2016; Gardent et al., 2017). However, real world language generation applications tend to involve generation of much larger amount of text such as dialogues or multisentence summaries. The inputs to NLG systems also vary from structured data such as tables (Lebret et al., 2016) or graphs (Wang et al., 2018), to textual data (Nallapati et al., 2016). Because of such difference in data and domain, comparison between different methods has been nontrivial. This task aims to (1) push forward such document-level generation technology by providing a testbed, and (2) examine the differences between generation based on different types of inputs including both structured data and translations in another language. In particular, we provided the following 6 tracks which focus on different input/output requirements: Valid Test 242 323 320 4163 5425 240 328 324 - 241 329 325 - Table 1: Data statistics of Roto"
D19-5601,D13-1176,0,\N,Missing
D19-5601,D15-1199,0,\N,Missing
D19-5601,P02-1040,0,\N,Missing
D19-5601,P10-2041,0,\N,Missing
D19-5601,W14-3302,0,\N,Missing
D19-5601,D17-1239,0,\N,Missing
D19-5601,W14-7001,0,\N,Missing
D19-5601,W17-4717,0,\N,Missing
D19-5601,W17-3518,0,\N,Missing
D19-5601,N19-1423,0,\N,Missing
E09-1058,W03-2111,0,0.426616,"Missing"
E09-1058,P04-1044,1,0.736088,"e European Chapter of the ACL, pages 505–513, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 505 alogue acts in the dialogue history, that the US is sensitive to (see section 3). The paper is organized as follows. After a short relation to previous work, we describe the data (Section 5) and derive baseline results (Section 6). Section 3 describes the User Simulations that we use for re-ranking hypotheses. Section 7 describes our learning experiments for classifying and selecting from n-best recognition hypotheses and Section 9 reports our results. 2 (Gabsdil and Lemon, 2004) similarly perform reordering of n-best lists by combining acoustic and pragmatic features. Their study shows that dialogue features such as the previous system question and whether a hypothesis is the correct answer to a particular question contributed more to classification accuracy than the other attributes. (Jonson, 2006) classifies recognition hypotheses with labels denoting acceptance, clarification, confirmation and rejection. These labels were learned in a similar way to (Gabsdil and Lemon, 2004) and correspond to varying levels of confidence, being essentially potential directives to"
E09-1058,P03-2004,0,0.0465568,"Missing"
E09-1058,A00-2029,0,0.0222918,"ate Update (ISU) approach to dialogue management (Traum et al., 1999) and therefore expect it to be applicable to a range of related multimodal dialogue systems. Relation to Previous Work In psycholinguistics, the idea that human dialogue participants simulate each other to some extent is gaining currency. (Pickering and Garrod, 2007) write: “if B overtly imitates A, then A’s comprehension of B’s utterance is facilitated by A’s memory for A’s previous utterance.” We explore aspects of this idea in a computational manner. Similar work in the area of spoken dialogue systems is described below. (Litman et al., 2000) use acoustic-prosodic information extracted from speech waveforms, together with information derived from their speech recognizer, to automatically predict misrecognized turns in a corpus of train-timetable information dialogues. In our experiments, we also use recognizer confidence scores and a limited number of acoustic-prosodic features (e.g. amplitude in the speech signal) for hypothesis classification, but we also use User Simulation predictions. (Walker et al., 2000) use a combination of features from the speech recognizer, natural language understanding, and dialogue manager/discourse"
E09-1058,N07-2038,0,0.321225,"ctions. We evaluate this approach in comparison with a baseline system that works in the standard way: always choosing the topmost hypothesis in the n-best list. In such systems, complex repair strategies are required when the top hypothesis is incorrect. The main novelty of this work is that we explore the use of predictions from simple statistical User Simulations to re-rank n-best lists of ASR hypotheses. These User Simulations are now commonly used in statistical learning approaches to dialogue management (Williams and Young, 2003; Schatzmann et al., 2006; Young, 2006; Young et al., 2007; Schatzmann et al., 2007), but they have not been used for context-sensitive ASR before. In our model, the system’s “belief” b(h) in a recognition hypothesis h is factored in two parts: the observation probability P (o|h) (approximated by the ASR confidence score) and the User Simulation probability P (h|us, C) of the hypothesis: We use a machine learner trained on a combination of acoustic and contextual features to predict the accuracy of incoming n-best automatic speech recognition (ASR) hypotheses to a spoken dialogue system (SDS). Our novel approach is to use a simple statistical User Simulation (US) for this tas"
K17-1004,D14-1155,1,0.38453,"ons reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatically affect the way people write.1 1 Ending She feels flattered and asks John on a date. The girl found this charming, and gave him a second chance. John was happy about being rejected. Table 1: Examples of stories from the story cloze task. The table shows a story prefix with three contrastive endings: The original ending, a coherent ending and a incoherent one. since different tasks likely engage different cognitive processes (Campbell and Pennebaker, 2003; Banerjee et al., 2014).2 We show that similar writing tasks with different constraints on the author can lead to measurable differences in her writing style. As a case study, we present experiments based on the recently introduced ROC story cloze task (Mostafazadeh et al., 2016a). In this task, authors were asked to write five-sentence self-contained stories, henceforth original stories. Then, each original story was given to a different author, who was shown only the first four sentences as a story context, and asked to write two contrasting story endings: a right (coherent) ending, and a wrong (incoherent) ending"
K17-1004,P82-1020,0,0.851485,"Missing"
K17-1004,N12-1033,0,0.00560131,"yle has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state, and also her concrete response. They also provide valuable lessons for designing new NLP datasets. 10 style and physical health. Psychological Science 14(1):60–65. Danqi Chen, Jason Bol"
K17-1004,D15-1075,0,0.02051,"ples compared to 3,742 in the story cloze task), this indicates that simple instructions may help alleviate the effects of writing style found in this paper. Another way to avoid such effects is to have people rate naturally occurring sentences by parameters such as coherence (or, conversely, the level of surprise), rather than asking them to generate new text. 8 Machine reading. The story cloze task, which is the focus of this paper, is part of a wide set of machine reading/comprehension challenges published in the last few years. These include datasets like bAbI (Weston et al., 2016), SNLI (Bowman et al., 2015), CNN/DailyMail (Hermann et al., 2015), LAMBADA (Paperno et al., 2016) and SQuAD (Rajpurkar et al., 2016). While these works have presented resources for researchers, it is often the case that these datasets suffer from methodological problems caused by applying noisy automatic tools to generate them (Chen et al., 2016).11 In this paper, we have pointed to another methodological challenge in designing machine reading tasks: different writing tasks used to generated the data affect writing style, confounding classification problems. 9 Conclusion Different writing tasks assigned to an author res"
K17-1004,P17-2097,0,0.437985,"Missing"
K17-1004,P14-2072,0,0.0173871,"Missing"
K17-1004,N16-1098,0,0.194848,"Washington, Seattle, WA, USA 2 Allen Institute for Artificial Intelligence, Seattle, WA, USA {roysch,msap,ikonstas,lzilles,yejin,nasmith}@cs.washington.edu Story Prefix John liked a girl at his work. He tried to get her attention by acting silly. She told him to grow up. John confesses he was trying to make her like him more. Abstract A writer’s style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatical"
K17-1004,W17-0906,0,0.0269252,"Missing"
K17-1004,W16-2505,0,0.160323,"Washington, Seattle, WA, USA 2 Allen Institute for Artificial Intelligence, Seattle, WA, USA {roysch,msap,ikonstas,lzilles,yejin,nasmith}@cs.washington.edu Story Prefix John liked a girl at his work. He tried to get her attention by acting silly. She told him to grow up. John confesses he was trying to make her like him more. Abstract A writer’s style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatical"
K17-1004,D16-1264,0,0.0242504,"15), LAMBADA Background: The Story Cloze Task To understand how different writing tasks affect writing style, we focus on the story cloze task (Mostafazadeh et al., 2016a). While this task was developed to facilitate representation and learning of commonsense story understanding, its design included a few key choices which make it ideal for our study. We describe the task below. ROC stories. The ROC story corpus consists of 49,255 five-sentence stories, collected on Ama3 Recently, additional 53K stories were released, which results in roughly 100K stories. 16 (Paperno et al., 2016) and SQuAD (Rajpurkar et al., 2016), for which results improved dramatically over similar or much shorter periods of time. This suggests that this task is challenging and that high performance is hard to achieve. In addition, Mostafazadeh et al. (2016a) made substantial efforts to ensure the quality of this dataset. First, each pair of endings was written by the same author, which ensured that style differences between authors could not be used to solve the task. Furthermore, Mostafazadeh et al. implemented nine baselines for the task, using surface level features as well as narrative-informed ones, and showed that each of them"
K17-1004,P11-1077,0,0.0110115,"eve state of the art results on the story cloze task. The findings presented in this paper have cognitive implications, as they motivate further research Related Work Writing style. Writing style has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state,"
K17-1004,W11-1515,1,0.178023,"Missing"
K17-1004,P11-1032,1,0.0248668,"Writing tasks can even have a long-term effect, as writing emotional texts was observed to benefit both physical and mental health (LepDesign of NLP tasks. Our study also provides important insights for the future design of NLP tasks. The story cloze task was very carefully designed. Many factors, such as topic diversity and 21 and Wallace, 1963; Pennebaker and King, 1999; Schwartz et al., 2013b). The line of work that most resembles our work is the detection of deceptive text. Several researchers have used stylometric features to predict deception (Newman et al., 2003; Hancock et al., 2007; Ott et al., 2011; Feng et al., 2012). Some works even showed that gender affects a person’s writing style when lying (P´erez-Rosas and Mihalcea, 2014a,b). In this work, we have shown that an even more subtle writing task—writing coherent and incoherent story endings—imposes different styles on the author. temporal and causal relation diversity, were controlled for (Mostafazadeh et al., 2016a). The authors also made sure each pair of endings was written by the same author, partly in order to avoid author-specific style effects. Nonetheless, despite these efforts, several significant style differences can be fo"
K17-1004,P16-1144,0,0.0591898,"Missing"
K17-1004,W17-0907,1,0.671362,"Missing"
K17-1004,D13-1193,1,0.264354,"periment 2 Table 5: The top 5 most heavily weighted features for predicting right vs. wrong endings (5a) and original vs. new (right) endings (5b). length is the sentence length feature (see Section 4). ore and Smyth, 2002; Frattaroli, 2006). Campbell and Pennebaker (2003) also showed that the health benefits of writing emotional text are accompanied by changes in writing style, mostly in the use of pronouns. Another line of work has shown that writing style is affected by mental state. First, an author’s personality traits (e.g., depression, neuroticism, narcissism) affect her writing style (Schwartz et al., 2013a; Ireland and Mehl, 2014). Second, temporary changes, such as a romantic relationship (Ireland et al., 2011; Bowen et al., 2016), work collaboration (Tausczik, 2009; Gonzales et al., 2009), or negotiation (Ireland and Henderson, 2014) may also affect writing style. Finally, writing style can also change from one sentence to another, for instance between positive and negative text (Davidov et al., 2010) or when writing sarcastic text (Tsur et al., 2010). This large body of work indicates a tight connection between writing tasks, mental states, and variation in writing style. This connection hi"
K17-1004,W07-0602,0,0.0529888,"Writing style. Writing style has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state, and also her concrete response. They also provide valuable lessons for designing new NLP datasets. 10 style and physical health. Psychological Science 14(1):60–65"
K17-1004,P13-1093,0,0.0486373,"Missing"
K17-1004,N12-1097,1,0.0996929,"Missing"
K17-1004,P12-2034,1,\N,Missing
K17-1004,P16-1223,0,\N,Missing
N12-1093,D10-1049,0,0.546232,"balized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010). In this paper we describe an end-to-end generation model that performs content selection and surface realization jointly. Given a corpus of database records and textual descriptions (for some of them), we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be rendered into natural 752 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics (a) (b)"
N12-1093,H05-1042,1,0.888973,"text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (rendering the specification chosen by the sentence planner into a surface string). Traditionally, these components are hand-engineered in order to generate high quality text, however at the expense of portability and scalability. It is thus no surprise that recent years have witnessed a growing interest in automatic methods for creating trainable generation components. Examples include learning which database records should be present in a text (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2"
N12-1093,J07-2003,0,0.381389,"eneration in the air travel domain, (b) weather forecast generation, and (c) sportscasting. language. This grammar represents a set of trees which we encode compactly using a weighted hypergraph (or packed forest), a data structure that defines a probability (or weight) for each tree. Generation then boils down to finding the best derivation tree in the hypergraph which can be done efficiently using the Viterbi algorithm. In order to ensure that our generation output is fluent, we intersect our grammar with a language model and perform decoding using a dynamic programming algorithm (Huang and Chiang, 2007). Our model is conceptually simpler than previous approaches and encodes information about the domain and its structure globally, by considering the input space simultaneously during generation. Our only assumption is that the input must be a set of records essentially corresponding to database-like tables whose columns describe fields of a certain type. Experimental evaluation on three domains obtains results competitive to the state of the art without using any domain specific constraints, explicit feature engineering or labeled data. 2 Related Work Our work is situated within the broader cl"
N12-1093,H94-1010,0,0.562578,"ary in this domain (henceforth W EATHER G OV) is comparable to ROBO C UP (345 words), however, the texts are longer (|w |= 29.3) and more varied. On average, each forecast has 4 sentences and the content selection problem is more challenging; only 5.8 out of the 36 records per scenario are mentioned in the text which roughly corresponds to 1.4 records per sentence. We used 25,000 scenarios from W EATHER G OV for training, 1,000 scenarios for development and 3,528 scenarios for testing. This is the same partition used in Angeli et al. (2010). For the air travel domain we used the ATIS dataset (Dahl et al., 1994), consisting of 5,426 scenarios. These are transcriptions of spontaneous utterances of users interacting with a hypothetical on1-B EST A NGELI k-B EST H UMAN W EATHER G OV Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. Near 57. South wind. As high as 23 mph. Chance of precipitation is 20. Breezy, with a chance of showers. Mostly cloudy, with a high near 57. South wind between 3 and 9 mph. ATIS What what what what flights from Denver Phoenix ROBOCUP Pink9 to to Pink7 kicks Show me the flights from Denver to Phoenix Pink9 passes back to Pink7 A chance o"
N12-1093,W02-2112,0,0.136869,"nd structure of the target text), sentence planning (determining the structure and lexical content of individual sentences), and surface realization (rendering the specification chosen by the sentence planner into a surface string). Traditionally, these components are hand-engineered in order to generate high quality text, however at the expense of portability and scalability. It is thus no surprise that recent years have witnessed a growing interest in automatic methods for creating trainable generation components. Examples include learning which database records should be present in a text (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately ("
N12-1093,W05-1506,0,0.010118,"over1 .t) FS2,2 (temp1 ,max) with FS0,1 (temp1 ,start) ··· FS1,2 (skyCover1 ,start) ··· Figure 2: Partial hypergraph representation for the sentence “Sunny with a low around 30 .” For the sake of readability, we show a partial span on the first two words without weights on the hyperarcs. words) as well as the optimal segmentation of the text, provided we have a trained set of weights. The inside-outside algorithm is commonly used for estimating the weights of a PCFG. However, we first transform the CYK parser and our grammar into a hypergraph and then compute the weights using inside-outside. Huang and Chiang (2005) define a weighted directed hypergraph as follows: Definition 1 An ordered hypergraph H is a tuple hN, E,t, Ri, where N is a finite set of nodes, E is a finite set of hyperarcs and R is the set of weights. Each hyperarc e ∈ E is a triple e = hT (e), h(e), f (e)i, where h(e) ∈ N is its head node, T (e) ∈ N ∗ is a set of tail nodes and f (e) is a monotonic weight function R|T (e) |to R and t ∈ N is a target node. Definition 2 We impose the arity of a hyperarc to be |e |= |T (e) |= 2, in other words, each head node is connected with at most two tail nodes. Given a context-free grammar G = hN, T,"
N12-1093,P07-1019,0,0.0841923,"a) query generation in the air travel domain, (b) weather forecast generation, and (c) sportscasting. language. This grammar represents a set of trees which we encode compactly using a weighted hypergraph (or packed forest), a data structure that defines a probability (or weight) for each tree. Generation then boils down to finding the best derivation tree in the hypergraph which can be done efficiently using the Viterbi algorithm. In order to ensure that our generation output is fluent, we intersect our grammar with a language model and perform decoding using a dynamic programming algorithm (Huang and Chiang, 2007). Our model is conceptually simpler than previous approaches and encodes information about the domain and its structure globally, by considering the input space simultaneously during generation. Our only assumption is that the input must be a set of records essentially corresponding to database-like tables whose columns describe fields of a certain type. Experimental evaluation on three domains obtains results competitive to the state of the art without using any domain specific constraints, explicit feature engineering or labeled data. 2 Related Work Our work is situated within the broader cl"
N12-1093,P08-1067,0,0.0245233,"Missing"
N12-1093,C10-2062,0,0.510524,", 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010). In this paper we describe an end-to-end generation model that performs content selection and surface realization jointly. Given a corpus of database records and textual descriptions (for some of them), we define a probabilistic context-free grammar (PCFG) that captures the structure of the database and how it can be rendered into natural 752 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 752–761, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics (a) (b) Flight Search Day Temp"
N12-1093,W01-1812,0,0.0101706,"graph (Gallo et al., 1993). Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyperarcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). During testing, we are given a set of database records without the corresponding text. Using the trained grammar we compile a hypergraph specific to this test input and decode it approximately via cube pruning (Chiang, 2007). The choice of the hypergraph framework is motivated by at least three reasons. Firstly, hypergraphs can be used to represent the search space of most parsers (Klein and Manning, 2001). Secondly, they are more efficient and faster than the common CYK parser-based representation for PCFGs by a factor of more than ten (Huang and Chiang, 2007). And thirdly, the hypergraph representation allows us to integrate an n-gram language model and perform decoding efficiently using k-best Viterbi search, optimizing what to say and how to say at the same time. 3.1 Grammar Definition Our model captures the inherent structure of the database with a number of CFG rewrite rules, in a similar way to how Liang et al. (2009) define Markov chains in the different levels of their hierarchical mod"
N12-1093,D09-1005,0,0.0332863,"destination, day, time). Our goal then is to reduce the tasks of content selection and surface realization into a common probabilistic pars754 ing problem. We do this by abstracting the structure of the database (and accompanying texts) into a PCFG whose probabilities are learned from training data.1 Specifically, we convert the database into rewrite rules and represent them as a weighted directed hypergraph (Gallo et al., 1993). Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyperarcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). During testing, we are given a set of database records without the corresponding text. Using the trained grammar we compile a hypergraph specific to this test input and decode it approximately via cube pruning (Chiang, 2007). The choice of the hypergraph framework is motivated by at least three reasons. Firstly, hypergraphs can be used to represent the search space of most parsers (Klein and Manning, 2001). Secondly, they are more efficient and faster than the common CYK parser-based representation for PCFGs by a factor of more than ten (Huang and Chiang, 2007). And thirdly, the hypergraph r"
N12-1093,P09-1011,0,0.299928,"l content of individual sentences), and surface realization (rendering the specification chosen by the sentence planner into a surface string). Traditionally, these components are hand-engineered in order to generate high quality text, however at the expense of portability and scalability. It is thus no surprise that recent years have witnessed a growing interest in automatic methods for creating trainable generation components. Examples include learning which database records should be present in a text (Duboue and McKeown, 2002; Barzilay and Lapata, 2005) and how these should be verbalized (Liang et al., 2009). Besides concentrating on isolated components, a few approaches have emerged that tackle concept-to-text generation end-to-end. Due to the complexity of the task, most models simplify the generation process, e.g., by creating output that consists of a few sentences, thus obviating the need for document planning, or by treating sentence planning and surface realization as one component. A common modeling strategy is to break up the generation process into a sequence of local decisions, each learned separately (Reiter et al., 2005; Belz, 2008; Chen and Mooney, 2008; Angeli et al., 2010; Kim and"
N12-1093,D11-1149,0,0.104573,"roduced by Liang et al. (2009). Their model decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component is based on templates that are automatically extracted and smoothed with domainspecific constraints in order to guarantee fluent output. Other related work (Wong and Mooney, 2007; Lu and Ng, 2011). has focused on generating natural language sentences from logical form (i.e., lambdaexpressions) using mostly synchronous context-free grammars (SCFGs). Similar to Angeli et al. (2010), we also present an end-to-end system that performs content selection and surface realization. However, rather than breaking up the generation task into a sequence of local decisions, we optimize what to say and how to say simultaneously. We do not learn mappings from a logical form, but rather focus on input which is less constrained, possibly more noisy and with a looser structure. Our key insight is to conv"
N12-1093,P02-1040,0,0.0868619,"two configurations of our system. A baseline that uses the top scoring derivation in each subgeneration (1- BEST) and another version which makes better use of our decoding algorithm and considers the best k derivations (i.e., 15 for W EATHER G OV, 40 for ATIS, and 25 for ROBO C UP). We compared our output to Angeli et al. (2010) whose approach is closest to ours and state-of-the-art on the W EATHER G OV domain. For ROBO C UP, we also compare against the bestpublished results (Kim and Mooney, 2010). Evaluation We evaluated system output automatically, using the BLEU modified precision score (Papineni et al., 2002) with the human-written text as reference. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization and were asked to rate the latter along two dimensions: fluency (is the text grammatical and overall understandable?) and semantic correctness (does the meaning conveyed by the text correspond to the database input?). The subjects used a five point rating scale where a high number indicates better performance. We randomly selected 12 docSystem 1-B EST k-B EST A NGELI K IM -M OONEY ROBO C UP W EATHER"
N12-1093,N07-1022,0,0.307467,"ocal label assignments and their pairwise relations. Building on this work, Liang et al. (2009) present a hierarchical hidden semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. A few approaches have emerged more recently that combine content selection and surface realization. Kim and Mooney (2010) adopt a two-stage approach: using a generative model similar to Liang et al. (2009), they first decide what to say and then verbalize the selected input with WASP−1 , an existing generation system (Wong and Mooney, 2007). In contrast, Angeli et al. (2010) propose a unified content selection and surface realization model which also operates over the alignment output produced by Liang et al. (2009). Their model decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component is based on templates that are"
N12-1093,D07-1071,0,0.0360778,"Missing"
N19-1071,P82-1020,0,0.808165,"Missing"
N19-1071,K16-1002,0,0.104267,"Missing"
N19-1071,N16-1012,0,0.115087,"Missing"
N19-1071,D16-1140,0,0.0298938,"on of SEQ3 to unsupervised abstractive sentence compression, with additional task-specific loss functions; (3) state of the art performance in unsupervised abstractive sentence compression. This work is a step towards exploring the potential of SEQ3 in other tasks, such as machine translation. 2 N ∑ Wv oct + bv softmax(uct ) (3) Wo , bo , Wv , bv are learned. ct is also used when updating the state hct of the decoder, along with the embedding ect of yt and a countdown argument M − t (scaled by a learnable wd ) indicating the number of the remaining words of the summary (Fevry and Phang, 2018; Kikuchi et al., 2016). −−→ hct+1 = RNNc (hct , ect , ct , wd (M − t)) (4) For each input x = ⟨x1 , . . . , xN ⟩, we obtain a target length M for the summary y = ⟨y1 , . . . , yM ⟩ by sampling (and rounding) from a uniform distribution U (αN, βN); α, β are hyper-parameters (α < β < 1); we set M = 5, if the sampled M is smaller. Sampling M, instead of using a static compression ratio, allows us to train a model capable of producing summaries with varying (e.g., user-specified) compression ratios. Controlling the output length in encoder-decoder architectures has been explored in machine translation (Kikuchi et al.,"
N19-1071,W18-2706,0,0.0200439,"ect , ct , wd (M − t)) (4) For each input x = ⟨x1 , . . . , xN ⟩, we obtain a target length M for the summary y = ⟨y1 , . . . , yM ⟩ by sampling (and rounding) from a uniform distribution U (αN, βN); α, β are hyper-parameters (α < β < 1); we set M = 5, if the sampled M is smaller. Sampling M, instead of using a static compression ratio, allows us to train a model capable of producing summaries with varying (e.g., user-specified) compression ratios. Controlling the output length in encoder-decoder architectures has been explored in machine translation (Kikuchi et al., 2016) and summarization (Fan et al., 2018). Proposed Model 2.1 Compressor 2.2 Differentiable Word Sampling The bottom left part of Fig. 2 illustrates the internals of the compressor C. An embedding layer projects the source sequence x to the word embeddings es = ⟨es1 , . . . , esN ⟩, which are then enTo generate the summary, we need to sample its words yt from the categorical distributions p(yt |y<t , x), which is a non-differentiable process. 674 Soft-Argmax Instead of sampling yt , a simple workaround during training is to pass as input to the next timestep of C’s decoder and to the corresponding timestep of R’s encoder a weighted s"
N19-1071,K18-1040,0,0.62226,"equire parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets. 1 Compressor (encoder-decoder) Topic Loss ?1 , ?2 , … , ?? ?ො1 , ?ො2 , … , ?ො? Reconstructor (encoder-decoder) LM Prior Loss Figure 1: Overview of the proposed SEQ3 autoencoder. posed (Artetxe et al., 2018; Lample et al., 2018b). Unsupervised (or semi-supervised) SEQ 2 SEQ models have also been proposed for summarization tasks with no (or small) parallel text-summary sets, including unsupervised sentence compression. Current models, however, barely reach leadN baselines (Fevry and Phang, 2018; Wang and Lee, 2018), and/or are non-differentiable (Wang and Lee, 2018; Miao and Blunsom, 2016), thus relying on reinforcement learning, which is unstable and inefficient. By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ3 , that can be trained end-to-end via gradient-based optimization. SEQ3 employs differentiable approximations for sampling from categorical distributions (Maddison et al., 2017; Jang et al., 2017), which have been shown to outperform reinforcement learning (Havrylov and Titov, 2017). Therefore it is a generic framework which can be easily ex"
N19-1071,W17-3204,0,0.0356547,", . . . , xN ⟩ of N words, and generates a summary y = ⟨y1 , . . . , yM ⟩ of M words (M<N), y being a latent variable. R and C communicate only through the discrete words of the summary y (§2.2). R (§2.3) produces a seˆ = ⟨ˆ quence x x1 , . . . , x ˆN ⟩ of N words from y, tryIntroduction Neural sequence-to-sequence models (SEQ 2 SEQ) perform impressively well in several natural language processing tasks, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) or syntactic constituency parsing (Vinyals et al., 2015). However, they require massive parallel training datasets (Koehn and Knowles, 2017). Consequently there has been extensive work on utilizing non-parallel corpora to boost the performance of SEQ 2 SEQ models (Sennrich et al., 2016; G¨ulc¸ehre et al., 2015), mostly in neural machine translation where models that require absolutely no parallel corpora have also been pro673 Proceedings of NAACL-HLT 2019, pages 673–681 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ?ො? ?ො2 ? ℎ?−1 ℎ1? … ℎ0? ℎ1? ℎ2? … ? ??−1 ?1? … ?0? ?1? ?2? … ?1 ?1 ?ො1 coded by a bidirectional RNN, producing hs = ⟨hs1 , . . . , hsN ⟩. Each hst is the concatenation"
N19-1071,N18-2081,0,0.0329404,"Missing"
N19-1071,D15-1044,0,0.821236,"s the input text. ai hsi i=1 The matrix Wa is learned. We obtain a probability distribution for yt over the vocabulary V by combining ct and the current state hct of the decoder. oct = tanh(Wo [ct ; hct ] + bo ) (1) uct (2) = p(yt |y<t , x) = ˆ) ing to minimize a reconstruction loss LR = (x, x (§2.5). A pretrained language model acts as a prior on y, introducing an additional loss LP (x, y) that encourages SEQ3 to produce human-readable summaries. A third loss LT (x, y) rewards summaries y with similar topic-indicating words as x. Experiments (§3) on the Gigaword sentence compression dataset (Rush et al., 2015) and the DUC -2003 and DUC -2004 shared tasks (Over et al., 2007) produce promising results. Our contributions are: (1) a fully differentiable sequence-to-sequence-to-sequence (SEQ3 ) autoencoder that can be trained without parallel data via gradient optimization; (2) an application of SEQ3 to unsupervised abstractive sentence compression, with additional task-specific loss functions; (3) state of the art performance in unsupervised abstractive sentence compression. This work is a step towards exploring the potential of SEQ3 in other tasks, such as machine translation. 2 N ∑ Wv oct + bv softma"
N19-1071,D15-1166,0,0.063871,"rallel corpora have also been pro673 Proceedings of NAACL-HLT 2019, pages 673–681 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ?ො? ?ො2 ? ℎ?−1 ℎ1? … ℎ0? ℎ1? ℎ2? … ? ??−1 ?1? … ?0? ?1? ?2? … ?1 ?1 ?ො1 coded by a bidirectional RNN, producing hs = ⟨hs1 , . . . , hsN ⟩. Each hst is the concatenation of the corresponding left-to-right and right-to-left states (outputs in LSTMs) of the bi-RNN. ← − − → −−→ ←−− hst = [RNNs (est , h st−1 ); RNNs (est , h st+1 )] Reconstructor ? ℎ? ? ?? To generate the summary y, we employ the attentional RNN decoder of Luong et al. (2015), with their global attention and input feeding. Concretely, at each timestep (t ∈ {1, . . . , M}) we compute a probability distribution ai over all the states hs1 , . . . , hsN of the source encoder conditioned on the current state hct of the compressor’s decoder to produce a context vector ct . ?Μ LM prior Loss Compressor ℎ1? ℎ2? … ? ℎ? ℎ0? ℎ1? … ? ℎ?−1 ?1? ?2? … ??? ?0? ?1? … ? ??−1 ?1 ?2 ?? Topic Loss ai = softmax(hsi ⊺ Wa hct ), ct = 3 Figure 2: More detailed illustration of SEQ . The compressor (C) produces a summary from the input text, and the reconstructor (R) tries to reproduce the i"
N19-1071,P17-1099,0,0.260948,"Missing"
N19-1071,P16-1009,0,0.0512056,"gh the discrete words of the summary y (§2.2). R (§2.3) produces a seˆ = ⟨ˆ quence x x1 , . . . , x ˆN ⟩ of N words from y, tryIntroduction Neural sequence-to-sequence models (SEQ 2 SEQ) perform impressively well in several natural language processing tasks, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) or syntactic constituency parsing (Vinyals et al., 2015). However, they require massive parallel training datasets (Koehn and Knowles, 2017). Consequently there has been extensive work on utilizing non-parallel corpora to boost the performance of SEQ 2 SEQ models (Sennrich et al., 2016; G¨ulc¸ehre et al., 2015), mostly in neural machine translation where models that require absolutely no parallel corpora have also been pro673 Proceedings of NAACL-HLT 2019, pages 673–681 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ?ො? ?ො2 ? ℎ?−1 ℎ1? … ℎ0? ℎ1? ℎ2? … ? ??−1 ?1? … ?0? ?1? ?2? … ?1 ?1 ?ො1 coded by a bidirectional RNN, producing hs = ⟨hs1 , . . . , hsN ⟩. Each hst is the concatenation of the corresponding left-to-right and right-to-left states (outputs in LSTMs) of the bi-RNN. ← − − → −−→ ←−− hst = [RNNs (est , h st−1 ); RNNs (e"
N19-1071,D18-1267,0,0.0184866,"of the sampled words yt in the forward pass, approximate differentiable embeddings in the backward pass). (5) i where uct is the unnormalized score in Eq. 2 (i.e., the logit) of each word wi and τ ∈ (0, ∞) is the temperature. As τ → 0 most of the probability mass in Eq. 5 goes to the most probable word, hence the operation approaches the arg max. 2.4 Decoder Initialization We initialize the hidden state of each decoder us−→ ← − ing a transformation of the concatenation [hsN ; hs1 ] of the last hidden states (from the two directions) of its bidirectional encoder and a length vector, following Mallinson et al. (2018). The length vector for the decoder of the compressor C consists of the target summary length M, scaled by a learnable parameter wv , and the compression ratio M N. Gumbel-Softmax We still want to be able to perform sampling, though, as it has the benefit of adding stochasticity and facilitating exploration of the parameter space. Hence, we use the GumbelSoftmax (GS) reparametrization trick (Maddison et al., 2017; Jang et al., 2017) as a low variance approximation of sampling from categorical distributions. Sampling a specific word yt from the softmax (Eq. 3) is equivalent to adding (element-w"
N19-1071,D16-1031,0,0.606807,"ssion on benchmark datasets. 1 Compressor (encoder-decoder) Topic Loss ?1 , ?2 , … , ?? ?ො1 , ?ො2 , … , ?ො? Reconstructor (encoder-decoder) LM Prior Loss Figure 1: Overview of the proposed SEQ3 autoencoder. posed (Artetxe et al., 2018; Lample et al., 2018b). Unsupervised (or semi-supervised) SEQ 2 SEQ models have also been proposed for summarization tasks with no (or small) parallel text-summary sets, including unsupervised sentence compression. Current models, however, barely reach leadN baselines (Fevry and Phang, 2018; Wang and Lee, 2018), and/or are non-differentiable (Wang and Lee, 2018; Miao and Blunsom, 2016), thus relying on reinforcement learning, which is unstable and inefficient. By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ3 , that can be trained end-to-end via gradient-based optimization. SEQ3 employs differentiable approximations for sampling from categorical distributions (Maddison et al., 2017; Jang et al., 2017), which have been shown to outperform reinforcement learning (Havrylov and Titov, 2017). Therefore it is a generic framework which can be easily extended to other tasks, e.g., machine translation and semantic parsing via task-specific losses. I"
N19-1071,D18-1451,0,0.715263,"mmary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets. 1 Compressor (encoder-decoder) Topic Loss ?1 , ?2 , … , ?? ?ො1 , ?ො2 , … , ?ො? Reconstructor (encoder-decoder) LM Prior Loss Figure 1: Overview of the proposed SEQ3 autoencoder. posed (Artetxe et al., 2018; Lample et al., 2018b). Unsupervised (or semi-supervised) SEQ 2 SEQ models have also been proposed for summarization tasks with no (or small) parallel text-summary sets, including unsupervised sentence compression. Current models, however, barely reach leadN baselines (Fevry and Phang, 2018; Wang and Lee, 2018), and/or are non-differentiable (Wang and Lee, 2018; Miao and Blunsom, 2016), thus relying on reinforcement learning, which is unstable and inefficient. By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ3 , that can be trained end-to-end via gradient-based optimization. SEQ3 employs differentiable approximations for sampling from categorical distributions (Maddison et al., 2017; Jang et al., 2017), which have been shown to outperform reinforcement learning (Havrylov and Titov, 2017). Therefore it is a generic framework which can be easily extended to other tasks"
N19-1071,K16-1028,0,0.103221,"Missing"
N19-1071,D10-1050,0,0.0733195,"Missing"
N19-1071,D14-1162,0,0.0937538,"et. Length Penalty A fourth loss LL (not shown in Fig. 1) helps the (decoder of the) compressor to predict the end-of-sequence (EOS) token at the target summary length M. LL is the cross-entropy between the distributions p(yt |y<t , x) (Eq. 3) of the compressor at t = M + 1 and onward, with the one-hot distribution of the EOS token. 2.6 Modeling Details Parameter Sharing We tie the weights of layers encoding similar information, to reduce the number of trainable parameters. First, we use a shared embedding layer for the encoders and decoders, initialized with 100-dimensional GloVe embeddings (Pennington et al., 2014). Additionally, we tie the shared embedding layer with the output layers of both decoders (Press and Wolf, 2017; Inan et al., 2017). Finally, we tie the encoders of the compressor and reconstructor (see Appendix). OOVs Out-of-vocabulary words are handled as in Fevry and Phang (2018) (see Appendix). 3 Results Table 1 reports the Gigaword results. SEQ 3 outperforms the unsupervised Pretrained Generator across all metrics by a large margin. It also surpasses LEAD -8. If we remove the LM prior, performance drops, esp. in ROUGE -2 and ROUGE L . This makes sense, since the pretrained LM rewards corr"
N19-1071,E17-2025,0,0.0312917,"of-sequence (EOS) token at the target summary length M. LL is the cross-entropy between the distributions p(yt |y<t , x) (Eq. 3) of the compressor at t = M + 1 and onward, with the one-hot distribution of the EOS token. 2.6 Modeling Details Parameter Sharing We tie the weights of layers encoding similar information, to reduce the number of trainable parameters. First, we use a shared embedding layer for the encoders and decoders, initialized with 100-dimensional GloVe embeddings (Pennington et al., 2014). Additionally, we tie the shared embedding layer with the output layers of both decoders (Press and Wolf, 2017; Inan et al., 2017). Finally, we tie the encoders of the compressor and reconstructor (see Appendix). OOVs Out-of-vocabulary words are handled as in Fevry and Phang (2018) (see Appendix). 3 Results Table 1 reports the Gigaword results. SEQ 3 outperforms the unsupervised Pretrained Generator across all metrics by a large margin. It also surpasses LEAD -8. If we remove the LM prior, performance drops, esp. in ROUGE -2 and ROUGE L . This makes sense, since the pretrained LM rewards correct word order. We also tried removing the topic loss, but the model failed to converge and results were extrem"
N19-1071,P17-1101,0,0.0405545,"Missing"
P12-1039,D10-1049,0,0.187718,"antly, in this framework non-local features are computed at all internal hypergraph nodes, allowing the decoder to take advantage of them continuously at all stages of the generation process. We incorporate features that are local with respect to a span of a sub-derivation in the packed forest; we also (approximately) include features that arbitrarily exceed span boundaries, thus capturing more global knowledge. Experimental results on the ATIS domain (Dahl et al., 1994) demonstrate that our model outperforms a baseline based on the best derivation and a stateof-the-art discriminative system (Angeli et al., 2010) by a wide margin. Our contributions in this paper are threefold: we recast concept-to-text generation in a probabilistic parsing framework that allows to jointly optimize content selection and surface realization; we represent parse derivations compactly using hypergraphs and illustrate the use of an algorithm for generating (rather than parsing) in this framework; finally, the application of discriminative reranking to conceptto-text generation is novel to our knowledge and as our experiments show beneficial. 2 Related Work Early discriminative approaches to text generation were introduced i"
P12-1039,W05-0909,0,0.019038,"cal features, hence the use of k-best derivation lists.5 We compared our model to Angeli et al. (2010) whose approach is closest to ours.6 We evaluated system output automatically, using the BLEU-4 modified precision score (Papineni et 5 Since the addition of these features, essentially incurs reranking, it follows that the systems would exhibit the exact same performance as the baseline system with 1-best lists. 6 We are grateful to Gabor Angeli for providing us with the code of his system. 375 al., 2002) with the human-written text as reference. We also report results with the METEOR score (Banerjee and Lavie, 2005), which takes into account word re-ordering and has been shown to correlate better with human judgments at the sentence level. In addition, we evaluated the generated text by eliciting human judgments. Participants were presented with a scenario and its corresponding verbalization (see Figure 3) and were asked to rate the latter along two dimensions: fluency (is the text grammatical and overall understandable?) and semantic correctness (does the meaning conveyed by the text correspond to the database input?). The subjects used a five point rating scale where a high number indicates better perf"
P12-1039,P05-1022,0,0.0670049,"a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component performs decisions based on templates that are automatically extracted and smoothed with domain-specific knowledge in order to guarantee fluent output. Discriminative reranking has been employed in many NLP tasks such as syntactic parsing (Charniak and Johnson, 2005; Huang, 2008), machine translation (Shen et al., 2004; Li and Khudanpur, 2009) and semantic parsing (Ge and Mooney, 2006). Our model is closest to Huang (2008) who also performs forest reranking on a hypergraph, using both local and non-local features, whose weights are tuned with the averaged perceptron algorithm (Collins, 2002). We adapt forest reranking to generation and introduce several task-specific features that boost performance. Although conceptually related to Angeli et al. (2010), our model optimizes content selection and surface realization simultaneously, rather than as a sequenc"
P12-1039,J07-2003,0,0.202914,"F). Note, that in order to estimate the trigram feature at the FS node, we need to carry word information in the derivations of its antecedents, as we go bottom-up.2 Given these two types of features, we can then adapt Huang’s (2008) approximate decoding algoˆ Essentially, we perform bottomˆ h). rithm to find (w, up Viterbi search, visiting the nodes in reverse topological order, and keeping the k-best derivations for each. The score of each derivation is a linear combination of local and non-local features weights. In machine translation, a decoder that implements forest rescoring (Huang and Chiang, 2007) uses the language model as an external criterion of the goodness of sub-translations on account of their grammaticality. Analogously here, non-local features influence the selection of the best combinations, by introducing knowledge that exceeds the confines of the node under consideration and thus depend on the sub-derivations generated so far. (e.g., word trigrams spanning a field node rely on evidence from antecedent nodes that may be arbitrarily deeper than the field’s immediate children). Our treatment of leaf nodes (see rules (8) and (9)) differs from the way these are usually handled i"
P12-1039,W02-1001,0,0.0970964,"ation component performs decisions based on templates that are automatically extracted and smoothed with domain-specific knowledge in order to guarantee fluent output. Discriminative reranking has been employed in many NLP tasks such as syntactic parsing (Charniak and Johnson, 2005; Huang, 2008), machine translation (Shen et al., 2004; Li and Khudanpur, 2009) and semantic parsing (Ge and Mooney, 2006). Our model is closest to Huang (2008) who also performs forest reranking on a hypergraph, using both local and non-local features, whose weights are tuned with the averaged perceptron algorithm (Collins, 2002). We adapt forest reranking to generation and introduce several task-specific features that boost performance. Although conceptually related to Angeli et al. (2010), our model optimizes content selection and surface realization simultaneously, rather than as a sequence. The discriminative aspect of two models is also fundamentally different. We have a single reranking component that applies throughout, whereas they train different discriminative models for each local decision. 1. S → R(start) 2. R(ri .t) → FS(r j , start) R(r j .t) [P(r j .t |ri .t) · λ] 3 3. R(ri .t) → FS(r j , start) [P(r j"
P12-1039,H94-1010,0,0.291299,"encodes exponentially many derivations, we can explore a much larger hypothesis space than would have been possible with an n-best list. Importantly, in this framework non-local features are computed at all internal hypergraph nodes, allowing the decoder to take advantage of them continuously at all stages of the generation process. We incorporate features that are local with respect to a span of a sub-derivation in the packed forest; we also (approximately) include features that arbitrarily exceed span boundaries, thus capturing more global knowledge. Experimental results on the ATIS domain (Dahl et al., 1994) demonstrate that our model outperforms a baseline based on the best derivation and a stateof-the-art discriminative system (Angeli et al., 2010) by a wide margin. Our contributions in this paper are threefold: we recast concept-to-text generation in a probabilistic parsing framework that allows to jointly optimize content selection and surface realization; we represent parse derivations compactly using hypergraphs and illustrate the use of an algorithm for generating (rather than parsing) in this framework; finally, the application of discriminative reranking to conceptto-text generation is n"
P12-1039,P06-2034,0,0.0608386,"s of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component performs decisions based on templates that are automatically extracted and smoothed with domain-specific knowledge in order to guarantee fluent output. Discriminative reranking has been employed in many NLP tasks such as syntactic parsing (Charniak and Johnson, 2005; Huang, 2008), machine translation (Shen et al., 2004; Li and Khudanpur, 2009) and semantic parsing (Ge and Mooney, 2006). Our model is closest to Huang (2008) who also performs forest reranking on a hypergraph, using both local and non-local features, whose weights are tuned with the averaged perceptron algorithm (Collins, 2002). We adapt forest reranking to generation and introduce several task-specific features that boost performance. Although conceptually related to Angeli et al. (2010), our model optimizes content selection and surface realization simultaneously, rather than as a sequence. The discriminative aspect of two models is also fundamentally different. We have a single reranking component that appl"
P12-1039,W06-1417,0,0.0111444,"ion and generating in this setting. Experimental evaluation on the ATIS domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study. 1 Introduction Concept-to-text generation broadly refers to the task of automatically producing textual output from non-linguistic input such as databases of records, logical form, and expert system knowledge bases (Reiter and Dale, 2000). A variety of concept-totext generation systems have been engineered over the years, with considerable success (e.g., Dale et al. (2003), Reiter et al. (2005), Green (2006), Turner et al. (2009)). Unfortunately, it is often difficult to adapt them across different domains as they rely mostly on handcrafted components. Following a generative approach, we could first learn the weights of the PCFG by maximising the joint likelihood of the model and then perform generation by finding the best derivation tree in the hypergraph. The performance of this baseline system could be potentially further improved using discriminative reranking (Collins, 2000). Typically, this method first creates a list of n-best candidates from a generative model, and then reranks them with"
P12-1039,P07-1019,0,0.014614,"l fields (F). Note, that in order to estimate the trigram feature at the FS node, we need to carry word information in the derivations of its antecedents, as we go bottom-up.2 Given these two types of features, we can then adapt Huang’s (2008) approximate decoding algoˆ Essentially, we perform bottomˆ h). rithm to find (w, up Viterbi search, visiting the nodes in reverse topological order, and keeping the k-best derivations for each. The score of each derivation is a linear combination of local and non-local features weights. In machine translation, a decoder that implements forest rescoring (Huang and Chiang, 2007) uses the language model as an external criterion of the goodness of sub-translations on account of their grammaticality. Analogously here, non-local features influence the selection of the best combinations, by introducing knowledge that exceeds the confines of the node under consideration and thus depend on the sub-derivations generated so far. (e.g., word trigrams spanning a field node rely on evidence from antecedent nodes that may be arbitrarily deeper than the field’s immediate children). Our treatment of leaf nodes (see rules (8) and (9)) differs from the way these are usually handled i"
P12-1039,P08-1067,0,0.0873507,"from to denver boston number dep/ar 9 departure month dep/ar august departure arg1 arg2 type arrival time 1600 < type what query flight λx. f light(x) ∧ f rom(x, denver) ∧ to(x, boston) ∧ day number(x, 9) ∧ month(x, august)∧ less than(arrival time(x), 1600) Give me the flights leaving Denver August ninth coming back to Boston before 4pm. Figure 1: Example of non-linguistic input as a structured database and logical form and its corresponding text. We omit record fields that have no value, for the sake of brevity. baseline system. An appealing alternative is to rerank the hypergraph directly (Huang, 2008). As it compactly encodes exponentially many derivations, we can explore a much larger hypothesis space than would have been possible with an n-best list. Importantly, in this framework non-local features are computed at all internal hypergraph nodes, allowing the decoder to take advantage of them continuously at all stages of the generation process. We incorporate features that are local with respect to a span of a sub-derivation in the packed forest; we also (approximately) include features that arbitrarily exceed span boundaries, thus capturing more global knowledge. Experimental results on"
P12-1039,W01-1812,0,0.0400883,"ds whose type is integer. Function g( f .v) generates an integer number given the field value, using either of the following six ways (Liang et al., 2009): identical to the field value, rounding up or rounding down to a multiple of 5, rounding off to the closest multiple of 5 and finally adding or subtracting some unexplained noise.1 The weight is a multinomial over the six generation function modes, given the record field f . The CFG in Table 1 will produce many derivations for a given input (i.e., a set of database records) which we represent compactly using a hypergraph or a packed forest (Klein and Manning, 2001; Huang, 2008). Simplified examples of this representation are shown in Figure 2. 3.2 Hypergraph Reranking For our generation task, we are given a set of database records d, and our goal is to find the best corresponding text w. This corresponds to the best grammar derivation among a set of candidate derivations represented implicitly in the hypergraph structure. As shown in Table 1, the mapping from d to w is unknown. Therefore, all the intermediate multinomial distributions, described in the previous section, define a hidden correspondence structure h, between records, fields, and their valu"
P12-1039,N12-1093,1,0.649676,"rds. Baseline Feature This is the log score of a generative decoder trained on the PCFG from Table 1. We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). Decoding was performed approx4 The resulting dataset and a technical report describing the mapping procedure in detail are available from http://homepages.inf.ed.ac.uk/s0793019/index.php? page=resources 374 imately via cube pruning (Chiang, 2007), by integrating a trigram language model extracted from the training set (see Konstas and Lapata (2012) for details). Intuitively, the feature refers to the overall goodness of a specific derivation, applied locally in every hyperedge. Alignment Features Instances of this feature family refer to the count of each PCFG rule from Table 1. For example, the number of times rule R(search1 .t) → FS( f light1 , start)R( f light1 .t) is included in a derivation (see Figure 2(a)) Lexical Features These features encourage grammatical coherence and inform lexical selection over and above the limited horizon of the language model captured by Rules (6)–(9). They also tackle anomalies in the generated output"
P12-1039,D09-1005,0,0.0249437,"s (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. 4.2 Features Broadly speaking, we defined two types of features, namely lexical and structural ones. In addition, we used a generatively trained PCFG as a baseline feature and an alignment feature based on the cooccurrence of records (or fields) with words. Baseline Feature This is the log score of a generative decoder trained on the PCFG from Table 1. We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). Decoding was performed approx4 The resulting dataset and a technical report describing the mapping procedure in detail are available from http://homepages.inf.ed.ac.uk/s0793019/index.php? page=resources 374 imately via cube pruning (Chiang, 2007), by integrating a trigram language model extracted from the training set (see Konstas and Lapata (2012) for details). Intuitively, the feature refers to the overall goodness of a specific derivation, applied locally in every hyperedge. Alignment Features Instances of this feature family refer to the count of each PCFG rule from Table 1. For example,"
P12-1039,P06-1096,0,0.0626103,"Missing"
P12-1039,P09-1011,0,0.138535,"ion task. Local and non-local information (e.g., word n-grams, long370 range dependencies) was taken into account with the use of features in a maximum entropy probability model. More recently, Wong and Mooney (2007) describe an approach to surface realization based on synchronous context-free grammars. The latter are learned using a log-linear model with minimum error rate training (Och, 2003). Angeli et al. (2010) were the first to propose a unified approach to content selection and surface realization. Their model operates over automatically induced alignments of words to database records (Liang et al., 2009) and decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization component performs decisions based on templates that are automatically extracted and smoothed with domain-specific knowledge in order to guarantee fluent output. Discriminative reranking has been employed in many NLP tasks such a"
P12-1039,P03-1021,0,0.0199151,"ed in spoken dialogue systems, and usually tackled content selection and surface realization separately. Ratnaparkhi (2002) conceptualized surface realization (from a fixed meaning representation) as a classification task. Local and non-local information (e.g., word n-grams, long370 range dependencies) was taken into account with the use of features in a maximum entropy probability model. More recently, Wong and Mooney (2007) describe an approach to surface realization based on synchronous context-free grammars. The latter are learned using a log-linear model with minimum error rate training (Och, 2003). Angeli et al. (2010) were the first to propose a unified approach to content selection and surface realization. Their model operates over automatically induced alignments of words to database records (Liang et al., 2009) and decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which words to use to describe the chosen fields. Each of these decisions is implemented as a log-linear model with features learned from training data. Their surface realization compone"
P12-1039,P02-1040,0,0.0942865,"Missing"
P12-1039,N04-1023,0,0.200359,"Missing"
P12-1039,W09-0607,0,0.0148359,"ting in this setting. Experimental evaluation on the ATIS domain shows that our model outperforms a competitive discriminative system both using BLEU and in a judgment elicitation study. 1 Introduction Concept-to-text generation broadly refers to the task of automatically producing textual output from non-linguistic input such as databases of records, logical form, and expert system knowledge bases (Reiter and Dale, 2000). A variety of concept-totext generation systems have been engineered over the years, with considerable success (e.g., Dale et al. (2003), Reiter et al. (2005), Green (2006), Turner et al. (2009)). Unfortunately, it is often difficult to adapt them across different domains as they rely mostly on handcrafted components. Following a generative approach, we could first learn the weights of the PCFG by maximising the joint likelihood of the model and then perform generation by finding the best derivation tree in the hypergraph. The performance of this baseline system could be potentially further improved using discriminative reranking (Collins, 2000). Typically, this method first creates a list of n-best candidates from a generative model, and then reranks them with arbitrary features (bo"
P12-1039,N07-1022,0,0.0984533,"minative reranking to conceptto-text generation is novel to our knowledge and as our experiments show beneficial. 2 Related Work Early discriminative approaches to text generation were introduced in spoken dialogue systems, and usually tackled content selection and surface realization separately. Ratnaparkhi (2002) conceptualized surface realization (from a fixed meaning representation) as a classification task. Local and non-local information (e.g., word n-grams, long370 range dependencies) was taken into account with the use of features in a maximum entropy probability model. More recently, Wong and Mooney (2007) describe an approach to surface realization based on synchronous context-free grammars. The latter are learned using a log-linear model with minimum error rate training (Och, 2003). Angeli et al. (2010) were the first to propose a unified approach to content selection and surface realization. Their model operates over automatically induced alignments of words to database records (Liang et al., 2009) and decomposes into a sequence of discriminative local decisions. They first determine which records in the database to talk about, then which fields of those records to mention, and finally which"
P12-1039,D07-1071,0,0.0185589,"and the second argument denotes the value. We also defined special record types, such as condition and search. The latter is introduced for every lambda operator and assigned the categorical field what with the value flight which refers to the record type of variable x. Contrary to datasets used in previous generation studies (e.g., ROBO C UP (Chen and Mooney, 2008) and W EATHER G OV (Liang et al., 2009)), ATIS has a much richer vocabulary (927 words); each scenario corresponds to a single sentence (average length is 11.2 words) with 2.65 out of 19 record types mentioned on average. Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. 4.2 Features Broadly speaking, we defined two types of features, namely lexical and structural ones. In addition, we used a generatively trained PCFG as a baseline feature and an alignment feature based on the cooccurrence of records (or fields) with words. Baseline Feature This is the log score of a generative decoder trained on the PCFG from Table 1. We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Li and E"
P12-1039,N10-1069,0,\N,Missing
P15-1115,P05-1022,0,0.381588,"m. These include arbitrarily longrange dependencies contained in a parse tree, and more importantly non-isomorphic representations of the input sentence such as its semantic frame, i.e., the set of all semantic roles tripes that pertain to the same predicate. In order to accommodate these, we decode via beam search over candidate parses. We keep a list of the k-best analyses and prune those whose score scr(x) = Φ(x, y) · w¯ falls below a threshold. 3.2 Incremental k-best Parsing What we described in the previous section could equally apply to k-best re-ranking for full-sentence parsing (e.g., Charniak and Johnson, 2005). For incremental parsing, in addition to outputting yˆ for the full sentence, we need to output prefix trees yˆn for every prefix of length n ∈ {1 . . . N} of sentence x = a1 . . . aN with length N. Let hxn , yˆn , ni, be the state of our model after we have parsed the first n words of sentence x, resulting in analysis yˆn . / 0i, where 0/ is The initial state is defined as hx0 , 0, the empty analysis, and the final state is hx, y, ˆ Ni, which represents a full analysis for the input sentence. We need a function ADV that transitions from a state at word an to a set of states at word 1193 3 4"
P15-1115,W02-1001,0,0.254765,"les are generated by removing either the argument, or the predicate, or the role label, from a complete triple. This provides a way of generalizing between triples that share some information without being completely identical. Predicate/Argument/Role encodes the elements of a complete SRL triple individually (argument, predicate, or role). This allows for further generalization and reduces sparsity. 1195 5 5 Feature Weight Estimation Algorithm 1: Averaged Structured Perceptron 1 We estimate the vector of feature weights w¯ in Equation (2) using the averaged structured perceptron algorithm of Collins (2002); we give the pseudocode in Algorithm 1. The perceptron makes T passes over L training examples. In each iteration, for each sentence prefix/prefix tree pair (xn , yn ), it computes the best scoring prefix tree yˆn among the candidate prefix trees, given the current feature weights w. ¯ In line 7, the algorithm updates w¯ with the difference (if any) between the feature representations of the best scoring prefix tree yˆn and the approximate gold-standard prefix tree y+ n (see Section 3.2). Note that since we use a constant beam during decoding with the PLTAG parser in order to enumerate the se"
P15-1115,J05-1003,0,0.311093,"can be re-constructed by following backpointers in the chart. This is done only for evaluation at the end of the sentence or incrementally on demand. Reranking Features This section describes the features used for reranking the prefix trees generated by the incremental parser. We include three different classes of features, based on local information from PLTAG elementary trees, based on global and structural information from prefix trees, and based on semantic information provided by iSRL triples. In contrast to work on discriminative full-sentence parsing (e.g., Charniak and Johnson, 2005; Collins and Koo, 2005), we can only use features extracted from the prefix trees being constructed incrementally as the sentence is parsed. The right context of the current word cannot be used, as this would violate incrementality. Every feature combination we try also includes the following baseline features: Prefix Tree Probability is the log probability of the prefix tree as scored by the probability model of the baseline parser. The score is normalized by prefix length, to avoid getting larger negative log probability scores for longer prefixes. Elementary Tree Probability is the log probability of the elementa"
P15-1115,P04-1015,0,0.316872,"incomplete triples: the first one is predicateincomplete, with the argument goals assigned an A0, waiting to be attached to a predicate. The second one is argument-incomplete with predicate realized assigned an A1, waiting for an argument to follow. (see Section 1). Note the use of incomplete semantic role triples in Figure 2b. 3 Model We use a discriminative model in order to re-rank the output of the baseline PLTAG parser based on semantic roles assigned by the iSRL system. 3.1 Problem Formulation Our overall approach is closely related to the discriminative incremental parsing framework of Collins and Roark (2004). The goal is to learn a mapping from input sentences x ∈ X to parse trees y ∈ Y . For a given set of training pairs of sentences and gold-standard parse trees (x, y) ∈ X × Y , the output yˆ can be defined as: yˆ = argmax Φ(x, y) · w¯ y∈GEN(x) (1) where GEN(x) is a function that enumerates candidate parse trees for a given input x, Φ is a representation that maps each training example (x, y) to a feature vector Φ(x, y) ∈ Rd , and w¯ ∈ Rd is a vector of feature weights. During training, the task is to estimate w¯ given the training examples. In terms of efficiency, a crucial part of Equation (1"
P15-1115,J13-4008,1,0.636189,"ine an incremental TAG parser with an incremental semantic role labeling (iSRL) system. The iSRL system takes prefix trees and computes their most likely semantic role assignments. We show that these role assignments can be used to re-rank the output of the incremental parser, leading to substantial improvements in parsing performance compared to the baseline parser, both in full-sentence F-score and in incremental F-score. 2 Incremental Semantic Role Labeling The current work builds on an existing incremental parser, the Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG) parser of Demberg et al. (2013). The distinguishing feature of this parser is that it builds fully connected structures (no words are left unattached during incremental parsing); this requires it to make predictions about the right context, which are verified as more of the input becomes available. Konstas et al. (2014) show that semantic information can be attached to PLTAG structures, making it possible to assign semantic roles incrementally. In the present paper, we use these semantic roles to re-rank the output of the PLTAG parser. 2.1 Psycholinguistically Motivated TAG PLTAG extends standard TAG (Joshi and Schabes, 199"
P15-1115,W09-1205,0,0.0314195,"st of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and previous content to output partial dependency trees; then they output role labels given the full parser output. In contrast to all the joint approaches, we perform both parsing and semantic role labeling stric"
P15-1115,P08-1067,0,0.178743,"here GEN(x) is a function that enumerates candidate parse trees for a given input x, Φ is a representation that maps each training example (x, y) to a feature vector Φ(x, y) ∈ Rd , and w¯ ∈ Rd is a vector of feature weights. During training, the task is to estimate w¯ given the training examples. In terms of efficiency, a crucial part of Equation (1) is the search strategy over parses produced by GEN and, to a smaller degree, the dimensionality of w. ¯ One common decoding technique is to implement a dynamic program, thus avoiding the explicit enumeration of all analyses for a given timestamp (Huang, 2008). However, central to the discriminative approach is the exploration of features that cannot be straightforwardly embedded into the parser using a dynamic program. These include arbitrarily longrange dependencies contained in a parse tree, and more importantly non-isomorphic representations of the input sentence such as its semantic frame, i.e., the set of all semantic roles tripes that pertain to the same predicate. In order to accommodate these, we decode via beam search over candidate parses. We keep a list of the k-best analyses and prune those whose score scr(x) = Φ(x, y) · w¯ falls below"
P15-1115,P10-1110,0,0.0611814,"Missing"
P15-1115,C92-2066,0,0.66769,"Demberg et al. (2013). The distinguishing feature of this parser is that it builds fully connected structures (no words are left unattached during incremental parsing); this requires it to make predictions about the right context, which are verified as more of the input becomes available. Konstas et al. (2014) show that semantic information can be attached to PLTAG structures, making it possible to assign semantic roles incrementally. In the present paper, we use these semantic roles to re-rank the output of the PLTAG parser. 2.1 Psycholinguistically Motivated TAG PLTAG extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with A↓ and are called substitution nodes. To derive a TAG parse for a sentence, we start with the elementary tree of the head of the sentence and integrate the elementary trees of the other lexical items of the sentence using two operations: adjunction at an internal node and substitution at a substitution node (the node at which the operation applies is t"
P15-1115,D14-1036,1,0.669336,"ubstantial improvements in parsing performance compared to the baseline parser, both in full-sentence F-score and in incremental F-score. 2 Incremental Semantic Role Labeling The current work builds on an existing incremental parser, the Psycholinguistically Motivated Tree Adjoining Grammar (PLTAG) parser of Demberg et al. (2013). The distinguishing feature of this parser is that it builds fully connected structures (no words are left unattached during incremental parsing); this requires it to make predictions about the right context, which are verified as more of the input becomes available. Konstas et al. (2014) show that semantic information can be attached to PLTAG structures, making it possible to assign semantic roles incrementally. In the present paper, we use these semantic roles to re-rank the output of the PLTAG parser. 2.1 Psycholinguistically Motivated TAG PLTAG extends standard TAG (Joshi and Schabes, 1992) in order to enable incremental parsing. Standard TAG assumes a lexicon of elementary trees, each of which contains at least one lexical item as an anchor and at most one leaf node as a foot node, marked with A∗. All other leaves are marked with A↓ and are called substitution nodes. To d"
P15-1115,P10-1113,0,0.0231158,"ncremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and prev"
P15-1115,J93-2004,0,0.0488098,". T do for i ← 1 . . . L do for n ← 1 . . . N do yˆn = argmaxyn ∈πn Φ(xn , yn ) · w¯ if y+ n 6= yˆn then w¯ ← w¯ + Φ(xn , y+ n ) − Φ(xn , yn ) 1 T 1 L 1 return T ∑t=1 L ∑i=1 ∑N n=1 N wt,i,n 6 6.1 Experiments Setup We use the PLTAG parser of Demberg et al. (2013) to enumerate prefix trees yn and to compute the prefix tree and word probability scores which we use as features. We also use the iSRL system of Konstas et al. (2014) to generate incremental SRL triples. Their system includes a semanticallyenriched lexicon extracted from the Wall Street Journal (WSJ) part of the Penn Treebank corpus (Marcus et al., 1993), converted to PLTAG format. Semantic role annotation is sourced from Propbank. We trained the probability model of the parser and the identification and labeling classifiers of the iSRL system using the intersection of Sections 2–21 of WSJ and the English portion of the CoNLL 2009 Shared Task (Hajiˇc et al., 2009). We learn the weight vector w¯ by training the perceptron algorithm also on Sections 2–21 of WSJ (see Section 5 for details). We use the PoS tags predicted by the parser, rather than gold standard PoS tags. Testing is performed on section 23 of WSJ, for sentences up to 40 words. 6.2"
P15-1115,N07-1050,0,0.0171619,", these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the current word and previous content to output partial dependency trees; then they output role labels given the full parser output. In contrast to all the joint approaches, we perform both parsing and semantic role labeling strictly incrementally, without having access to the whole sentence, outputting prefix trees and iSRL triples for every sentence prefix. Our appro"
P15-1115,J01-2004,0,0.115552,"ints attained by the combination of all features in T REE +P LTAG +S RL. We also report combined SRL F-score computed on the re-ranked syntactic trees (rightmost column of Table 1). We find that compared to the baseline, only a small improvement of 0.55 points is achieved by T REE +P LTAG +S RL, while T REE +P LTAG improves by 0.84 points. The syntax-only variant therefore outperforms the full model, but only by a small margin. 7 Related Work The most similar approach in the literature is Collins and Roark’s (2004) re-ranking model for incremental parsing. They learn the syntactic features of Roark (2001) using the perceptron model of Collins (2002). Similar to us, they use the incremental parser to search over candidate parses. However, they limited themselves to local derivation features (akin to our PLTAG features), and do not explore global syntactic feature (tree features) or SRL features. Even though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collin"
P15-1115,J10-1001,0,0.0515722,"Missing"
P15-1115,P11-1063,0,0.0505551,"Missing"
P15-1115,W04-0304,0,0.0394096,"nnis Konstas and Frank Keller Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh {ikonstas,keller}@inf.ed.ac.uk Abstract 2001; Schuler et al., 2010), dependency grammar (Chelba and Jelinek, 2000; Nivre, 2007; Huang and Sagae, 2010), or tree-substitution grammars (Sangati and Keller, 2013). Typical applications of incremental parsers include speech recognition (Chelba and Jelinek, 2000; Roark, 2001; Xu et al., 2002), machine translation (Schwartz et al., 2011; Tan et al., 2011), reading time modeling (Demberg and Keller, 2008), or dialogue systems (Stoness et al., 2004). Incremental parsing, however, is considerably harder than full-sentence parsing: when processing the n-th word in a sentence, an , the parser only has access to the left context (words a1 . . . an−1 ); the right context (words an+1 . . . aN ) is not known yet. This can lead to local ambiguity, i.e., produce additional syntactic analyses that are valid for the sentence prefix, but become invalid as the right context is processed. As an example consider the sentence prefix in (1): Incremental parsing is the task of assigning a syntactic structure to an input sentence as it unfolds word by word"
P15-1115,W05-0636,0,0.0410017,"they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce parser and rely only on the"
P15-1115,P11-1021,0,0.0502082,"Missing"
P15-1115,J08-2002,0,0.0339219,"RL features. Even though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce"
P15-1115,P07-1031,0,0.0201983,"s and syntactic features. Finally, our baseline is the PLTAG parser of Demberg et al. (2013), using the original probability model without any re-ranking. A comparison with other incremental parsers would be desirable, but is not trivial to achieve. This is because the PLTAG parser is trained and evaluated on a version of the Penn Treebank that was converted to PLTAG format. This renders our results not directly comparable to parsers that reproduce the Penn Treebank bracketing. For example, the PLTAG parser produces deeper tree structures informed by Propbank and the noun phrase annotation of Vadas and Curran (2007). BASELINE T REE S RL T REE +P LTAG T REE +P LTAG +S RL 0.85 F-score System BASELINE T REE S RL T REE +P LTAG T REE +P LTAG +S RL 10 20 Prefix Length 30 Figure 3: Incremental parsing F-score for increasing sentence prefixes, up to 40 words. 6.3 Results Figure 3 gives the results of evaluating incremental parsing performance. The x-axis shows prefix length, and the y-axis shows incremental F-score computed as suggested by Sangati and Keller (2013). Each point is averaged over all prefixes of a given length in the test set. To quantify the trends shown in this figure, we also compute the area un"
P15-1115,W08-2129,0,0.0238105,"though they re-rank the output of an incremental parser, they only evaluate full sentence parsing performance. Other reranking approaches to syntactic parsing make use of an extensive set of global features, but apply it on the k-best list of full sentence parses (Charniak and Johnson, 2005; Collins and Koo, 2005) or the k-best list of derivations of a packed forest (Huang, 2008), i.e., these approaches are not incremental. Based on the CoNLL Shared Tasks (e.g., Hajiˇc et al., 2009), a number of systems exist that perform syntactic parsing and semantic role labeling jointly. Toutanova et al. (2008), Sutton and McCallum (2005) and Li et al. (2010) combine the scores of two separate models, i.e., a syntactic parser and a semantic role labeler, and re-rank the combination using features from each domain. Titov et al. (2009) and Gesmundo et al. (2009), instead of combining models, create a common search space for syntactic parsing and SRL, using a shift reduce-style technique (Nivre, 2007) and learn a latent variable model (Incremental Sigmoid Belief Networks) that optimizes over both tasks at the same time. Volokh and Neumann (2008) use a variant of Nivre’s (2007) incremental shift-reduce"
P15-1115,P02-1025,0,0.0738327,"Missing"
P15-1115,W09-1201,0,\N,Missing
P15-1115,Q13-1010,1,\N,Missing
P16-1195,P15-2017,0,0.017322,"using rules to create dependency trees for each section of the query, followed by a transformation step to make the output more natural (Ngonga Ngomo et al., 2013). These approaches are not learning based, and require significant manual template-engineering efforts. We use recurrent neural networks (RNN) based on LSTMs and neural attention to jointly model source code and NL. Recently, RNN-based approaches have gained popularity for text generation and have been used in machine translation (Sutskever et al., 2011), image and video description (Karpathy and Li, 2015; Venugopalan et al., 2015; Devlin et al., 2015), sentence summarization (Rush et al., 2015), and Chinese poetry generation (Zhang and Lapata, 2014). Perhaps most closely related, Wen et al. (2015) generate text for spoken dialogue systems with a two-stage approach, comprising an LSTM decoder semantically conditioned on the logical representation of speech acts, and a reranker to generate the final output. In contrast, we design an end-to-end attention-based model for source code. For code retrieval, Allamanis et al. (2015b) proposed a system that uses Stackoverflow data and web search logs to create models for retrieving C# code snippets g"
P16-1195,C12-2040,0,0.0433296,"Missing"
P16-1195,W11-2123,0,0.00870051,"nerate text from source code, and hence we adapt them slightly for this task, as explained below. IR is an information retrieval baseline that outputs the title associated with the code cj in the training set that is closest to the input code c in terms of token Levenshtein distance. In this case s from Eq.1 becomes, s(c, nj ) = −1 × lev(cj , c), 1 ≤ j ≤ J MOSES (Koehn et al., 2007) is a popular phrase-based machine translation system. We perform generation by treating the tokenized code snippet as the source language, and the title as the target. We train a 3-gram language model using KenLM (Heafield, 2011) to use with MOSES, and perform MIRA-based tuning (Cherry and Foster, 2012) of hyper-parameters using DEV. SUM-NN is the neural attention-based abstractive summarization model of Rush et al. (2015). 7 http://torch.ch It uses an encoder-decoder architecture with an attention mechanism based on a fixed context window of previously generated words. The decoder is a feed-forward neural language model that generates the next word based on previous words in a context window of size k. In contrast, we decode using an LSTM network that can model long range dependencies and our attention weights are ti"
P16-1195,P07-2045,0,0.0437632,"beam size to 10, and the maximum summary length to 20 words. 6 Experimental Setup 6.1 GEN Task 6.1.1 Baselines For the GEN task, we compare CODE-NN with a number of competitive systems, none of which had been previously applied to generate text from source code, and hence we adapt them slightly for this task, as explained below. IR is an information retrieval baseline that outputs the title associated with the code cj in the training set that is closest to the input code c in terms of token Levenshtein distance. In this case s from Eq.1 becomes, s(c, nj ) = −1 × lev(cj , c), 1 ≤ j ≤ J MOSES (Koehn et al., 2007) is a popular phrase-based machine translation system. We perform generation by treating the tokenized code snippet as the source language, and the title as the target. We train a 3-gram language model using KenLM (Heafield, 2011) to use with MOSES, and perform MIRA-based tuning (Cherry and Foster, 2012) of hyper-parameters using DEV. SUM-NN is the neural attention-based abstractive summarization model of Rush et al. (2015). 7 http://torch.ch It uses an encoder-decoder architecture with an attention mechanism based on a fixed context window of previously generated words. The decoder is a feed-"
P16-1195,N13-1103,0,0.0216271,"k well for retrieval but cannot be used for generation. We learn a neural generation model without using search logs and show that it can also be used to score code for retrieval, with much higher accuracy. Synthesizing code from language is an alternative to code retrieval and has been studied in both the Systems and NLP research communities. Giordani and Moschitti (2012), Li and Jagadish (2014), and Gulwani and Marron (2014) synthesize source code from NL queries for database and spreadsheet applications. Similarly, Lei et al. (2013) interpret NL instructions to machine-executable code, and Kushman and Barzilay (2013) convert language to regular expressions. Unlike most synthesis methods, CODE-NN is domain agnostic, as we demonstrate its applications on both C# and SQL. 4 Dataset We collected data from StackOverflow (SO), a popular website for posting programming-related questions. Anonymized versions of all the posts can be freely downloaded.3 Each post can have multiple tags. Using the C# tag for C# and the sql, database and oracle tags for SQL, we were able to collect 934,464 and 977,623 posts respectively.4 Each post comprises a short title, a detailed question, and one or more responses, of which one"
P16-1195,P13-1127,0,0.00791456,"plicative models to score (code, language) pairs, an approach that could work well for retrieval but cannot be used for generation. We learn a neural generation model without using search logs and show that it can also be used to score code for retrieval, with much higher accuracy. Synthesizing code from language is an alternative to code retrieval and has been studied in both the Systems and NLP research communities. Giordani and Moschitti (2012), Li and Jagadish (2014), and Gulwani and Marron (2014) synthesize source code from NL queries for database and spreadsheet applications. Similarly, Lei et al. (2013) interpret NL instructions to machine-executable code, and Kushman and Barzilay (2013) convert language to regular expressions. Unlike most synthesis methods, CODE-NN is domain agnostic, as we demonstrate its applications on both C# and SQL. 4 Dataset We collected data from StackOverflow (SO), a popular website for posting programming-related questions. Anonymized versions of all the posts can be freely downloaded.3 Each post can have multiple tags. Using the C# tag for C# and the sql, database and oracle tags for SQL, we were able to collect 934,464 and 977,623 posts respectively.4 Each post"
P16-1195,C04-1072,0,0.0258451,"g automatic metrics, and also perform a human study. Automatic Evaluation We report METEOR (Banerjee and Lavie, 2005) and sentence level BLEU-4 (Papineni et al., 2002) scores. METEOR is recall-oriented and measures how well our model captures content from the references in our output. BLEU-4 measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences. Since the generated summaries are short and there are multiple alternate summaries for a given code snippet, higher order n-grams may not overlap. We remedy this problem by using +1 smoothing (Lin and Och, 2004). We compute these metrics on the tuning set DEV and the held-out evaluation set EVAL. Human Evaluation Since automatic metrics do not always agree with the actual quality of the results (Stent et al., 2005), we perform human evaluation studies to measure the output of our system and baselines across two modalities, namely naturalness and informativeness. For the former, we asked 5 native English speakers to rate each title against grammaticality and fluency, on a scale between 1 and 5. For informativeness (i.e., the amount of content carried over from the input code to the NL summary, ignorin"
P16-1195,D11-1149,0,0.00588652,"he code (e.g., consider the phrase ‘second largest’ in Example 3 in Figure 1). In addition to being directly useful for interpreting uncommented code, high-quality generation models can also be used for code retrieval, and in turn, for natural language programming by applying nearest neighbor techniques to a large corpus of automatically summarized code. Natural language generation has traditionally been addressed as a pipeline of modules that decide ‘what to say’ (content selection) and ‘how to say it’ (realization) separately (Reiter and Dale, 2000; Wong and Mooney, 2007; Chen et al., 2010; Lu and Ng, 2011). Such approaches require supervision at each stage and do not scale well to large domains. We instead propose an end-to-end neural network called CODE-NN that jointly performs content selection using an attention mechanism, and surface realization using Long Short Term Memory (LSTM) networks. The system generates a summary one word at a time, guided by an attention mechanism over embeddings of the source code, and by context from previously generated words provided by a LSTM network (Hochreiter and Schmidhuber, 1997). The simplicity of the model allows it to be learned from the training data"
P16-1195,D15-1166,0,0.0285122,"q. 1) as a product of the conditional next-word probabilities l Y k X αi,j · cj F j=1 Figure 2: Generation of a title n = n1 , . . . , END given code snippet c1 , ..., ck . The attention cell computes a distributional representation ti of the code snippet based on the current LSTM hidden state hi . A combination of ti and hi is used to generate the next word, ni , which feeds back into the next LSTM cell. This is repeated until a fixed number of words or END is generated. ∝ blocks denote softmax operations. s(c, n) = Attention The generation of each word is guided by a global attention model (Luong et al., 2015), which computes a weighted sum of the embeddings of the code snippet tokens based on the current LSTM state (see right part in Figure 2). Formally, we represent c as a set of 1-hot vectors c1 , . . . , ck ∈ {0, 1}|C |for each source code token; C is the vocabulary of all tokens in our code snippets. Our attention model computes, p(ni |n1 , . . . , ni−1 ) i=1 with, p(ni |n1 , . . . , ni−1 ) ∝ W tanh(W1 hi + W2 ti ) where, W ∈ R|N |×H and W1 , W2 ∈ RH×H , H being the embedding dimensionality of the summaries. ti is the contribution from the attention model on the source code (see below). hi rep"
P16-1195,N16-1086,0,0.00817074,"cribe them. There is also work on improving program comprehension (Haiduc et al., 2074 2010), identifying cross-cutting source code concerns (Rastkar et al., 2011), and summarizing software bug reports (Rastkar et al., 2010). To the best of our knowledge, we are the first to use learning techniques to construct completely new sentences from arbitrary code snippets. Source code summarization is also related to generation from formal meaning representations. Wong and Mooney (2007) present a system that learns to generate sentences from lambda calculus expressions by inverting a semantic parser. Mei et al. (2016), Konstas and Lapata (2013), and Angeli et al. (2010) create learning algorithms for text generation from database records, again assuming data that pairs sentences with formal meaning representations. In contrast, we present algorithms for learning from easily gathered web data. In the database community, Simitsis and Ioannidis (2009) recognize the need for SQL database systems to talk back to users. Koutrika et al. (2010) built an interactive system (LOGOS) that translates SQL queries to text using NL templates and database schemas. Similarly there has been work on translating SPARQL queries"
P16-1195,P13-2007,0,0.152214,"snippet c ∈ UC , the goal is to produce a NL sentence n∗ ∈ UN that maximizes some scoring function s ∈ (UC × UN → R): n∗ = argmax s(c, n) (1) n RET We also use the scoring function s to retrieve the highest scoring code snippet c∗j from our training corpus, given a NL question n ∈ UN : c∗j = argmax s(cj , n), 1 ≤ j ≤ J cj (2) In this work, s is computed using an LSTM neural attention model, to be described in Section 5. 3 Related Work Although we focus on generating high-level summaries of source code snippets, there has been work on producing code descriptions at other levels of abstraction. Movshovitz-Attias and Cohen (2013) study the task of predicting class-level comments by learning n-gram and topic models from open source Java projects and testing it using a character-saving metric on existing comments. Allamanis et al. (2015a) create models for suggesting method and class names by embedding them in a high dimensional continuous space. Sridhara et al. (2010) present a pipeline that generates summaries of Java methods by selecting relevant content and generating phrases using templates to describe them. There is also work on improving program comprehension (Haiduc et al., 2074 2010), identifying cross-cutting"
P16-1195,P02-1040,0,0.118383,"attention weights are tied to the LSTM hidden states. We set the embedding and hidden state dimensions and context window size by tuning on our validation set. We found this model to generate overly short titles like ‘sql server 2008’ when a length restriction was not imposed on the output text. Therefore, we fix the output length to be the average title length in the training set while decoding. 6.1.2 Evaluation Metrics We evaluate the GEN task using automatic metrics, and also perform a human study. Automatic Evaluation We report METEOR (Banerjee and Lavie, 2005) and sentence level BLEU-4 (Papineni et al., 2002) scores. METEOR is recall-oriented and measures how well our model captures content from the references in our output. BLEU-4 measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences. Since the generated summaries are short and there are multiple alternate summaries for a given code snippet, higher order n-grams may not overlap. We remedy this problem by using +1 smoothing (Lin and Och, 2004). We compute these metrics on the tuning set DEV and the held-out evaluation set EVAL. Human Evaluation Since automatic metrics do not always agree w"
P16-1195,D15-1044,0,0.201488,"ch section of the query, followed by a transformation step to make the output more natural (Ngonga Ngomo et al., 2013). These approaches are not learning based, and require significant manual template-engineering efforts. We use recurrent neural networks (RNN) based on LSTMs and neural attention to jointly model source code and NL. Recently, RNN-based approaches have gained popularity for text generation and have been used in machine translation (Sutskever et al., 2011), image and video description (Karpathy and Li, 2015; Venugopalan et al., 2015; Devlin et al., 2015), sentence summarization (Rush et al., 2015), and Chinese poetry generation (Zhang and Lapata, 2014). Perhaps most closely related, Wen et al. (2015) generate text for spoken dialogue systems with a two-stage approach, comprising an LSTM decoder semantically conditioned on the logical representation of speech acts, and a reranker to generate the final output. In contrast, we design an end-to-end attention-based model for source code. For code retrieval, Allamanis et al. (2015b) proposed a system that uses Stackoverflow data and web search logs to create models for retrieving C# code snippets given NL questions and vice versa. They const"
P16-1195,P15-1150,0,0.0230533,"Missing"
P16-1195,N15-1173,0,0.0185772,"ueries to natural language using rules to create dependency trees for each section of the query, followed by a transformation step to make the output more natural (Ngonga Ngomo et al., 2013). These approaches are not learning based, and require significant manual template-engineering efforts. We use recurrent neural networks (RNN) based on LSTMs and neural attention to jointly model source code and NL. Recently, RNN-based approaches have gained popularity for text generation and have been used in machine translation (Sutskever et al., 2011), image and video description (Karpathy and Li, 2015; Venugopalan et al., 2015; Devlin et al., 2015), sentence summarization (Rush et al., 2015), and Chinese poetry generation (Zhang and Lapata, 2014). Perhaps most closely related, Wen et al. (2015) generate text for spoken dialogue systems with a two-stage approach, comprising an LSTM decoder semantically conditioned on the logical representation of speech acts, and a reranker to generate the final output. In contrast, we design an end-to-end attention-based model for source code. For code retrieval, Allamanis et al. (2015b) proposed a system that uses Stackoverflow data and web search logs to create models for retriev"
P16-1195,D15-1199,0,0.012087,"Missing"
P16-1195,N07-1022,0,0.023521,"an include complex, non-local aspects of the code (e.g., consider the phrase ‘second largest’ in Example 3 in Figure 1). In addition to being directly useful for interpreting uncommented code, high-quality generation models can also be used for code retrieval, and in turn, for natural language programming by applying nearest neighbor techniques to a large corpus of automatically summarized code. Natural language generation has traditionally been addressed as a pipeline of modules that decide ‘what to say’ (content selection) and ‘how to say it’ (realization) separately (Reiter and Dale, 2000; Wong and Mooney, 2007; Chen et al., 2010; Lu and Ng, 2011). Such approaches require supervision at each stage and do not scale well to large domains. We instead propose an end-to-end neural network called CODE-NN that jointly performs content selection using an attention mechanism, and surface realization using Long Short Term Memory (LSTM) networks. The system generates a summary one word at a time, guided by an attention mechanism over embeddings of the source code, and by context from previously generated words provided by a LSTM network (Hochreiter and Schmidhuber, 1997). The simplicity of the model allows it"
P16-1195,D14-1074,0,0.0139263,"on step to make the output more natural (Ngonga Ngomo et al., 2013). These approaches are not learning based, and require significant manual template-engineering efforts. We use recurrent neural networks (RNN) based on LSTMs and neural attention to jointly model source code and NL. Recently, RNN-based approaches have gained popularity for text generation and have been used in machine translation (Sutskever et al., 2011), image and video description (Karpathy and Li, 2015; Venugopalan et al., 2015; Devlin et al., 2015), sentence summarization (Rush et al., 2015), and Chinese poetry generation (Zhang and Lapata, 2014). Perhaps most closely related, Wen et al. (2015) generate text for spoken dialogue systems with a two-stage approach, comprising an LSTM decoder semantically conditioned on the logical representation of speech acts, and a reranker to generate the final output. In contrast, we design an end-to-end attention-based model for source code. For code retrieval, Allamanis et al. (2015b) proposed a system that uses Stackoverflow data and web search logs to create models for retrieving C# code snippets given NL questions and vice versa. They construct distributional representations of code structure an"
P16-1195,W05-0909,0,\N,Missing
P16-1195,N12-1047,0,\N,Missing
P17-1014,D15-1198,1,0.896171,"Missing"
P17-1014,P05-1045,0,0.0446321,"c) named entity clustering, and (d) insertion of scope markers. group, :ARG2-of, :ARG1-of, :ARG0.4 The order traverses children in the sequence they are presented in the AMR. We consider alternative orderings of children in Section 7 but always follow the pattern demonstrated above. ation, we render the corresponding format when predicted. Figure 2(b) contains an example of all preprocessing up to this stage. Named Entity Clusters When performing AMR generation, each of the AMR fine-grained entity types is manually mapped to one of the four coarse entity types used in the Stanford NER system (Finkel et al., 2005): person, location, organization and misc. This reduces the sparsity associated with many rarely occurring entity types. Figure 2 (c) contains an example with named entity clusters. Rendering Function Our rendering function marks scope, and generates tokens following the pre-order traversal of the graph: (1) if the element is a node, it emits the type of the node. (2) if the element is an edge, it emits the type of the edge and then recursively emits a bracketed string for the (concept) node immediately after it. In case the node has only one child we omit the scope markers (denoted with left"
P17-1014,N16-1087,0,0.360548,"ifying the contributions that come from preprocessing and the paired training procedure. 2 Neural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). Similar to our approach, Peng et al. (2017) deal with sparsity by anonymizing named entities and typing low frequency words, resulting in a very compact vocabulary (2k tokens). However, we avoid reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6). AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order. Our models do not need to rely on a particular linearizat"
P17-1014,S16-1176,0,0.461842,"mance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions. 1 and * op1 elect.01 op2 celebrate.01 ARG0 ARG0 poss name name person op1 person ARG0-of Obama vote.01 Figure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can be easily overcome, by demonstrating that seq2s"
P17-1014,P14-1134,0,0.507344,"Missing"
P17-1014,bender-2014-language,0,0.0121142,"ucially, we avoid relying on resources such as knowledge bases and externally trained parsers. We achieve competitive results for the parsing task (SMATCH 62.1) and state-of-theart performance for generation (BLEU 33.8). For future work, we would like to extend our work to different meaning representations such as the Minimal Recursion Semantics (MRS; Copestake et al. (2005)). This formalism tackles certain linguistic phenomena differently from AMR (e.g., negation, and co-reference), contains explicit annotation on concepts for number, tense and case, and finally handles multiple languages10 (Bender, 2014). Taking a step further, we would like to apply our models on Semantics-Based Machine Translation using MRS as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as English and Japanese (Siegel, 2000). state :arg0 report :arg1 ( obligate :arg1 ( government-organization :arg0-of ( govern :arg1 loc_0 ) ) :arg2 ( help :arg1 ( and :op1 ( stabilize :arg1 ( state :mod weak ) ) :op2 ( push :arg1 ( regulate :mod international :arg0-of ( stop :arg1 terror"
P17-1014,S16-1182,0,0.0387204,"et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences (Napoles et al., 2012) and then use the automatically parsed AMR graphs to pre-train an AMR generator. This paired training allows both the parser and generator to learn hi"
P17-1014,S16-1180,0,0.0589977,"ications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences ("
P17-1014,S16-1179,0,0.037075,"ermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR pars"
P17-1014,P16-1025,0,0.0126843,"t enhances both the text-to-AMR parser and AMR-to-text generator. More concretely, first we use self-training to Introduction Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text. As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text. AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al."
P17-1014,P13-2131,0,0.267935,"M (Barzdins and Gosko, 2016) Prec 72.3 67.2 62.2 61.9 59.7 54.9 - Dev Rec 61.4 65.1 66.0 64.8 62.9 60.0 - F1 69.0 66.6 66.1 64.4 63.3 61.3 57.4 - Prec 70.4 66.8 64.0 59.7 60.2 57.8 53.1 55.0 - Test Rec 63.1 65.7 53.0 64.7 63.6 60.9 58.1 50.0 - F1 67.1 66.5 66.3 58.0 62.1 61.9 59.3 55.5 52.0 43.0 Table 1: SMATCH scores for AMR Parsing. *Reported numbers are on the newswire portion of a previous release of the corpus (LDC2014T12). Corpus AMR G IGA-200k G IGA-2M G IGA-20M summarizes statistics about the original dataset and the extracted portions of Gigaword. We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al., 2002)5 . We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 – line 1). We searched over the set {128, 256, 500, 1024} for the best combinations of sizes and set both to 500. Models were trained by optimizing cross-entropy loss with stochastic gradient descent, using a batch size of 100 and dropout rate of 0.5. Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8. For the initial parser trained on the AMR corpus, (Al"
P17-1014,C12-1083,0,0.153954,"Missing"
P17-1014,N15-1114,0,0.0887244,". Our approach is two-fold. First, we introduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator. More concretely, first we use self-training to Introduction Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text. As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text. AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categ"
P17-1014,W16-6603,0,0.309947,"(Barzdins and Gosko, 2016; Peng et al., 2017). Similar to our approach, Peng et al. (2017) deal with sparsity by anonymizing named entities and typing low frequency words, resulting in a very compact vocabulary (2k tokens). However, we avoid reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6). AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large language model t"
P17-1014,D15-1136,0,0.106718,"MR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences (Napoles et al., 2012) and then use the automatically parsed AMR graphs to pre-train an AMR generator. This paired training allows both the parser and generator to learn high quality represent"
P17-1014,D15-1166,0,0.261978,"igure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can be easily overcome, by demonstrating that seq2seq models can be trained using any graph-isomorphic linearization and that unlabeled text can be used to significantly reduce sparsity. Our approach is two-fold. First, we introduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator. More concretely,"
P17-1014,D16-1183,0,0.0137661,"establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions. 1 and * op1 elect.01 op2 celebrate.01 ARG0 ARG0 poss name name person op1 person ARG0-of Obama vote.01 Figure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can b"
P17-1014,S16-1178,0,0.0388043,"esentation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities. bootstrap a high quality AMR parser from millions of unl"
P17-1014,W12-3018,0,0.0220269,"Missing"
P17-1014,P16-1009,0,0.0228418,"external corpus. Related Work Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm. Zhou et al. (2016) extend JAMR by performing the concept and relation identification tasks jointly with an incremental model. Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules. In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities. Data Augmentation Our paired training procedure is largely inspired by Sennrich et al. (2016). They improve neural MT performance for low resource language pairs by using a back-translation MT system for a large monolingual corpus of the target language in order to create synthetic output, Grammar-based Parsing Wang et al. (2016) (CAMR) perform a series of shift-reduce transformations on the output of an externally-trained dependency parser, similar to Damonte et al. (2017), 147 et al., 2016).1 Our model uses a global attention decoder and unknown word replacement with small modifications (Luong et al., 2015). The model uses a stacked bidirectional-LSTM encoder to encode an input sequ"
P17-1014,J05-1004,0,0.015856,"e use (section 3.2), graph-to-sequence conversion (section 3.3), and our paired training procedure (section 3.4). 3.1 Tasks We assume access to a training dataset D where each example pairs a natural language sentence s with an AMR a. The AMR is a rooted directed acylical graph. It contains nodes whose names correspond to sense-identified verbs, nouns, or AMR specific concepts, for example elect.01, Obama, and person in Figure 1. One of these nodes is a distinguished root, for example, the node and in Figure 1. Furthermore, the graph contains labeled edges, which correspond to PropBank-style (Palmer et al., 2005) semantic roles for verbs or other relations introduced for AMR, for example, arg0 or op1 in Figure 1. The set of node and edge names in an AMR graph is drawn from a set of tokens C, and every word in a sentence is drawn from a vocabulary W . We study the task of training an AMR parser, i.e., finding a set of parameters θP for model f , that predicts an AMR graph a ˆ, given a sentence s: a ˆ = argmax f a|s; θP a  3.3 Our seq2seq models require that both the input and target be presented as a linear sequence of tokens. We define a linearization order for an AMR graph as any sequence of its nod"
P17-1014,D16-1224,0,0.150248,"d reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6). AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large language model trained on Gigaword. We show that our seq2seq model has the capacity to learn the same information as a language model, especially after pretraining on the external corpus. Related Work Alignment-based Parsing Flanigan et al. (2014) ("
P17-1014,P02-1040,0,0.101398,"59.7 54.9 - Dev Rec 61.4 65.1 66.0 64.8 62.9 60.0 - F1 69.0 66.6 66.1 64.4 63.3 61.3 57.4 - Prec 70.4 66.8 64.0 59.7 60.2 57.8 53.1 55.0 - Test Rec 63.1 65.7 53.0 64.7 63.6 60.9 58.1 50.0 - F1 67.1 66.5 66.3 58.0 62.1 61.9 59.3 55.5 52.0 43.0 Table 1: SMATCH scores for AMR Parsing. *Reported numbers are on the newswire portion of a previous release of the corpus (LDC2014T12). Corpus AMR G IGA-200k G IGA-2M G IGA-20M summarizes statistics about the original dataset and the extracted portions of Gigaword. We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al., 2002)5 . We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 – line 1). We searched over the set {128, 256, 500, 1024} for the best combinations of sizes and set both to 500. Models were trained by optimizing cross-entropy loss with stochastic gradient descent, using a batch size of 100 and dropout rate of 0.5. Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8. For the initial parser trained on the AMR corpus, (Algorithm 1 – line 1), we use a single stack version of o"
P17-1014,D16-1112,0,0.02967,"roduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator. More concretely, first we use self-training to Introduction Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text. As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text. AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use 146 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1014 Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretra"
P17-1014,E17-1035,0,0.732145,"e-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions. 1 and * op1 elect.01 op2 celebrate.01 ARG0 ARG0 poss name name person op1 person ARG0-of Obama vote.01 Figure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as “Obama” being the “arg0” of the verb “elected”. of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016). In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can be easily overcome,"
P17-1014,D16-1065,0,0.0687239,"into fragments and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large language model trained on Gigaword. We show that our seq2seq model has the capacity to learn the same information as a language model, especially after pretraining on the external corpus. Related Work Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm. Zhou et al. (2016) extend JAMR by performing the concept and relation identification tasks jointly with an incremental model. Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules. In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities. Data Augmentation Our paired training procedure is largely inspired by Sennrich et al. (2016). They improve neural MT performance for low resource language pairs by using a back-translation MT system for a large monolingual corpus of the target lang"
P17-1014,P07-2045,0,\N,Missing
P17-1014,D14-1048,0,\N,Missing
P17-1014,S16-1181,0,\N,Missing
P17-1014,E17-1051,0,\N,Missing
P17-1089,Q13-1005,1,0.947688,"Missing"
P17-1089,D13-1160,0,0.510211,"Missing"
P17-1089,P15-1127,1,0.43684,"Missing"
P17-1089,N13-1103,0,0.0606504,"Missing"
P17-1089,W10-2903,0,0.0282341,"Missing"
P17-1089,D13-1161,1,0.8402,"Missing"
P17-1089,P16-1004,0,0.623508,"Missing"
P17-1089,D11-1140,1,0.458728,"Missing"
P17-1089,J13-2005,0,0.537802,"short regular expressions (Locascio et al., 2016). Our work extends these results to the task of SQL generation. Finally, Ling et al. (2016) generate Java/Python code for trading cards given a natural language description; however, this system suffers from low overall accuracy. A final direction of related work studies methods for reducing the annotation effort required to train a semantic parser. Semantic parsers have been trained from various kinds of annotations, including labeled queries (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005), question/answer pairs (Liang et al., 2013; Kwiatkowski et al., 2013; Berant et al., cluding batch learning of models that directly produce programs (e.g., regular expressions (Locascio et al., 2016)), learning from paraphrases (often gathered through crowdsourcing (Wang et al., 2015)), data augmentation (e.g. based on manually engineered semantic grammars (Jia and Liang, 2016)) and learning through direct interaction with users (e.g., where a single user teaches the model new concepts (Wang et al., 2016)). However, there are unique advantages to our approach, including showing (1) that non-linguists can write SQL to encode complex, c"
P17-1089,C12-2040,0,0.0994407,"Missing"
P17-1089,P16-1057,0,0.0840081,"Missing"
P17-1089,P16-1002,0,0.201283,"Missing"
P17-1089,D16-1197,0,0.0742209,"Missing"
P17-1089,D16-1032,1,0.290482,"Missing"
P17-1089,D15-1166,0,0.0825084,"(R) if f = correct then T ← T ∪ (n, q) else if f = wrong then qˆ ← annotate(n) T ← T ∪ (n, qˆ) end end end end Algorithm 1: Feedback-based learning. 4 Semantic Parsing to SQL We use a neural sequence-to-sequence model for mapping natural language questions directly to SQL queries and this allows us to scale our feedback-based learning approach, by easily crowdsourcing labels when necessary. We further present two data augmentation techniques which use content from the database schema and external paraphrase resources. 4.1 Model We use an encoder-decoder model with global attention, similar to Luong et al. (2015), where the anonymized utterance (see Section 4.2) is encoded using a bidirectional LSTM network, then decoded to directly predict SQL query tokens. Fixed pre-trained word embeddings from word2vec (Mikolov et al., 2013) are concatenated to the embeddings that are learned for source tokens from the training data. The decoder predicts a conditional probability distribution over possible values for the next SQL token given the previous tokens using a combination of the previous SQL token embedding, attention over the hidden states of the encoder network, and an attention signal from the previous"
P17-1089,D12-1069,1,0.486156,"Missing"
P17-1089,P14-1026,1,0.73715,"Missing"
P17-1089,D16-1183,0,0.0453999,"Missing"
P17-1089,P16-2033,0,0.0171502,"Missing"
P17-1089,P13-1092,0,0.0289636,"Missing"
P17-1089,D07-1071,1,0.537961,"Missing"
P17-1089,D16-1117,0,0.0112453,"ly, we do a small scale online experiment for a new domain, academic paper metadata search, demonstrating that actual users can provide useful feedback and our full approach is an effective method for learning a high quality parser that continues to improve over time as it is used. 2 Related Work Although diverse meaning representation languages have been used with semantic parsers – such as regular expressions (Kushman and Barzilay, 2013; Locascio et al., 2016), Abstract Meaning Representations (AMR) (Artzi et al., 2015; Misra and Artzi, 2016), and systems of equations (Kushman et al., 2014; Roy et al., 2016) – parsers for querying databases have typically used either logic programs (Zelle and Mooney, 1996), lambda calculus (Zettlemoyer and Collins, 2005), or λDCS (Liang et al., 2013) as the meaning represen1 964 http://www.upwork.com 2013), distant supervision (Krishnamurthy and Mitchell, 2012; Choi et al., 2015), and binary correct/incorrect feedback signals (Clarke et al., 2010; Artzi and Zettlemoyer, 2013). Each of these schemes presents a particular trade-off between annotation effort and parser accuracy; however, recent work has suggested that labeled queries are the most effective (Yih et a"
P17-1089,D14-1135,1,0.697797,"Missing"
P17-1089,P16-1224,0,0.03279,"Missing"
P17-1089,P15-1129,0,0.488314,"Missing"
P17-1089,N07-1022,0,0.0186852,"Missing"
P17-1089,P11-1060,0,\N,Missing
P17-1089,D15-1198,1,\N,Missing
P17-1089,P16-1073,0,\N,Missing
W17-0907,P82-1020,0,0.813873,"Missing"
W17-0907,W07-0602,0,0.0823614,". (2017). 2 System Description We design a system that predicts, given a pair of story endings, which is the right one and which is the wrong one. Our system applies a linear classifier guided by several types of features to solve the task. We describe the system in detail below. 52 Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 52–55, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics 2.1 Model (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). We add the following classification features to capture style differences between the two endings. These features are computed on the story endings alone (right or wrong), and do not consider, either at train or at test time, the first four (shared) sentences of each story. We train a binary logistic regression classifier to distinguish between right and wrong stories. We use the set of right stories as positive samples and the set of wrong stories as negative samples. At test time, for a given pair, we consider the classification results of both candidates. If our cla"
W17-0907,N16-1098,0,0.0403179,"Missing"
W17-0907,W11-1515,1,0.892048,"Missing"
W17-0907,P11-1077,0,0.0656157,"rring less than 3 times by a special out-of-vocabulary character, yielding a vocabulary size of 21,582. Only during training, we apply a (1) The intuition is that a correct ending should be unsurprising (to the model) given the four preceding sentences of the story (the numerator), controlling for the inherent surprise of the words in that ending (the denominator).1 Stylistic features. We hypothesize that right and wrong endings might be distinguishable using style features. We adopt style features that have been shown useful in the past in tasks such as detection of age (Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender 2 www.nltk.org/api/nltk.tokenize.html www.tensorflow.org 4 We train on both the Spring 2016 and the Winter 2017 datasets, a total of roughly 100K stories. 3 1 Note that taking the logarithm of the expression in Equation 1 gives the pointwise mutual information between the story and the ending, under the language model. 53 Model DSSM (Mostafazadeh et al., 2016) LexVec (Salle et al., 2016) RNNLM features Stylistic features Combined (Style + RNNLM) Human judgment Acc. 0.585 0.599 0.677 0.724 0.752 1.000 style on the authors, which is expressed in the different style"
W17-0907,D13-1193,1,0.0470041,", we keep them. If not, the label whose posterior probability is lower is reversed. We describe the classification features below. 2.2 • Length. The number of words in the sentence. Features We use two types of features, designed to capture different aspects of the problem. We use neural language model features to leverage corpus level word distributions, specifically longer term sequence probabilities. We use stylistic features to capture differences in writing between coherent story endings and incoherent ones. • Word n-grams. We use sequences of 1– 5 words. Following Tsur et al. (2010) and Schwartz et al. (2013), we distinguish between high frequency and low frequency words. Specifically, we replace content words, which are often low frequency, with their part-of-speech tags (Nouns, Verbs, Adjectives, and Adverbs). Language model features. We experiment with state-of-the-art text comprehension models, specifically an LSTM (Hochreiter and Schmidhuber, 1997) recurrent neural network language model (RNNLM; Mikolov et al., 2010). Our RNNLM is used to generate two different probabilities: pθ (ending), which is the language model probability of the fifth sentence alone and pθ (ending |story), which is the"
W17-0907,K17-1004,1,0.671994,"Missing"
W18-5709,W18-6514,1,0.717195,"Missing"
W18-5709,W07-0734,0,0.0774496,"M-HRED–attn) for a given context size. Adding KB input boosts performance more for a shorter context compared to longer context. It can be conjectured that the longer context contains some of the information that is in the KB queries and so there is less impact of the KB input when we include the longer context. Compare the difference for M-HRED–attn–kb vs. M-HRED–attn for a context of 2 (3 BLEU points) vs. 5 (2 BLEU points) in Table 1. Conversely, longer context improves more the models without KB queries. We evaluate our response generation using the B LEU (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) automatic metrics.6 We reproduce the baseline results from Saha et al. (2017) using their code and data-generation scripts.7 Model Saha et al. M-HRED* T-HRED M-HRED T-HRED–attn M-HRED–attn T-HRED–attn M-HRED–attn M-HRED–kb T-HRED–attn–kb M-HRED–attn–kb T-HRED–attn–kb M-HRED–attn–kb Cxt 2 2 2 2 2 5 5 2 2 2 5 5 B LEU -4 0.3767 0.4292 0.4308 0.4331 0.4345 0.4442 0.4451 0.4573 0.4601 0.4624 0.4612 0.4634 M ETEOR 0.2847 0.3269 0.3288 0.3298 0.3315 0.3374 0.3371 0.3436 0.3456 0.3476 0.3461 0.3480 ROUGE -L 0.6235 0.6692 0.6700 0.6710 0.6712 0.6797 0.6799 0.6872 0.690"
W18-5709,D16-1203,0,0.0307088,"rounding in KB. We show textual context as well as relevant knowledge base input (and omit image context) for brevity’s sake. While our model uses a context of 5, for simplicity, we show only 2 previous turns. probe found that the orientations for retrieved images may not directly follow the description in the query (KB). There are other intents for which even KB does not help, such as those requiring user modelling. 5 model outputs showed a substantial improvement (over 3 B LEU points) on incorporating KB information, integrating visual context still remains a bottleneck, as also observed by Agrawal et al. (2016); Qian et al. (2018). This suggests the need for a better mechanism to encode visual context. Since our KB-grounded model assumes user intent annotation and KB queries as additional inputs, we plan to build a model to provide them automatically. Conclusion and Future Work This work focuses on the task of textual response generation in multimodal task-oriented dialogue system. We used the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017) for experiments and introduced a novel conversational model grounded in language, vision and Knowledge Base (KB). Our best performing mod"
W18-5709,E06-2009,0,0.0825572,"Missing"
W18-5709,P04-1077,0,0.0794841,"Adding KB input boosts performance more for a shorter context compared to longer context. It can be conjectured that the longer context contains some of the information that is in the KB queries and so there is less impact of the KB input when we include the longer context. Compare the difference for M-HRED–attn–kb vs. M-HRED–attn for a context of 2 (3 BLEU points) vs. 5 (2 BLEU points) in Table 1. Conversely, longer context improves more the models without KB queries. We evaluate our response generation using the B LEU (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) automatic metrics.6 We reproduce the baseline results from Saha et al. (2017) using their code and data-generation scripts.7 Model Saha et al. M-HRED* T-HRED M-HRED T-HRED–attn M-HRED–attn T-HRED–attn M-HRED–attn M-HRED–kb T-HRED–attn–kb M-HRED–attn–kb T-HRED–attn–kb M-HRED–attn–kb Cxt 2 2 2 2 2 5 5 2 2 2 5 5 B LEU -4 0.3767 0.4292 0.4308 0.4331 0.4345 0.4442 0.4451 0.4573 0.4601 0.4624 0.4612 0.4634 M ETEOR 0.2847 0.3269 0.3288 0.3298 0.3315 0.3374 0.3371 0.3436 0.3456 0.3476 0.3461 0.3480 ROUGE -L 0.6235 0.6692 0.6700 0.6710 0.6712 0.6797 0.6799 0.6872 0.6909 0.6917 0.6913 0.6923 In summary"
W18-5709,D15-1166,0,0.092393,"elebrity profiles using basic pattern matching over the user utterance. For each of the celebrities in the user query, we order the corresponding synsets by their probability of endorsement. If no celebrity is found, we use synset information from the query to extract celebrities which endorse the corresponding synset. 3.3 Experiments and Results 4.2 Implementation We used PyTorch3 (Paszke et al., 2017) for our experiments.4 We did not use any kind of delexicalisation5 and rely on our model to directly learn Input feeding decoder We use an input feeding decoder with the attention mechanism of Luong et al. (2015). We concatenate the KB input hkb n with the decoder input cxt (cf. Eq. (10), where hdec n,0 = hN ). The rationale behind this late fusion of KB representation is that KB input remains the same for a given context and 2 We used the same training-development-test split as provided by the dataset authors. 3 https://pytorch.org/ 4 Code can be found at: https://github.com/shubhamagarwal92/mmd 5 Replacing specific values with placeholders (Henderson et al., 2014). 62 Intent from the conversational history and KB. All encoders and decoders are based on 1-layer GRU cells (Cho et al., 2014) with 512 a"
W18-5709,P02-1040,0,0.102215,"stark uplift (M-HRED– attn–kb vs. M-HRED–attn) for a given context size. Adding KB input boosts performance more for a shorter context compared to longer context. It can be conjectured that the longer context contains some of the information that is in the KB queries and so there is less impact of the KB input when we include the longer context. Compare the difference for M-HRED–attn–kb vs. M-HRED–attn for a context of 2 (3 BLEU points) vs. 5 (2 BLEU points) in Table 1. Conversely, longer context improves more the models without KB queries. We evaluate our response generation using the B LEU (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) automatic metrics.6 We reproduce the baseline results from Saha et al. (2017) using their code and data-generation scripts.7 Model Saha et al. M-HRED* T-HRED M-HRED T-HRED–attn M-HRED–attn T-HRED–attn M-HRED–attn M-HRED–kb T-HRED–attn–kb M-HRED–attn–kb T-HRED–attn–kb M-HRED–attn–kb Cxt 2 2 2 2 2 5 5 2 2 2 5 5 B LEU -4 0.3767 0.4292 0.4308 0.4331 0.4345 0.4442 0.4451 0.4573 0.4601 0.4624 0.4612 0.4634 M ETEOR 0.2847 0.3269 0.3288 0.3298 0.3315 0.3374 0.3371 0.3436 0.3456 0.3476 0.3461 0.3480 ROUGE -L 0.6235 0.6692 0.6700 0.671"
W18-5709,W13-4067,0,0.0783197,"Missing"
W18-5709,E17-1042,0,0.0341701,"al model where an encoded knowledge base (KB) representation is appended to the decoder input. Our model substantially outperforms strong baselines in terms of text-based similarity measures (over 9 BLEU points, 3 of which are solely due to the use of additional information from the KB). 1 Introduction Conversational agents have become ubiquitous, with variants ranging from open-domain conversational chit-chat bots (Ram et al., 2018; Papaioannou et al., 2017; Fang et al., 2017) to domainspecific task-based dialogue systems (Singh et al., 2000; Rieser and Lemon, 2010, 2011; Young et al., 2013; Wen et al., 2017). Our work builds upon the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017), which contains dialogue sessions in the ecommerce (fashion) domain. Figure 1 illustrates an example chat session with multimodal interaction between the user and the system. We focus on the task of generating textual responses conditioned on the previous conversational history. Traditional goal-oriented dialogue systems relied on slot-filling approach to this task, i.e. explicit modelling of all attributes in the domain (Lemon 2 Related Work With recent progress in deep learning, there is contin"
W18-5709,N15-1173,0,0.0187718,"h multimodal interaction between the user and the system. We focus on the task of generating textual responses conditioned on the previous conversational history. Traditional goal-oriented dialogue systems relied on slot-filling approach to this task, i.e. explicit modelling of all attributes in the domain (Lemon 2 Related Work With recent progress in deep learning, there is continued interest in the tasks involving both vision and language, such as image captioning (Xu et al., 2015; Vinyals et al., 2015; Karpathy and FeiFei, 2015), visual storytelling (Huang et al., 2016), video description (Venugopalan et al., 2015b,a) or dialogue grounded in visual context (Antol et al., 2015; Das et al., 2017; Tapaswi et al., 2016). Bordes et al. (2016) and Ghazvininejad et al. (2017) presented knowledge-grounded neural models; however, these are uni-modal in nature, involve only textual interaction and do not take into account the conversational history in a dia59 Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 59–66 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ISBN 978-1-948087-75-9 Figure 2: Schematic diagram of"
W18-5709,E09-1078,1,\N,Missing
W18-6514,D16-1203,0,0.0559846,"arch-based multimodal dialogue by learning from the recently released Multimodal Dialogue (MMD) dataset (Saha et al., 2017). We introduce a novel extension to the Hierarchical Recurrent Encoder-Decoder (HRED) model (Serban et al., 2016) and show that our implementation significantly outperforms the model of Saha et al. (2017) by modelling the full multimodal context. Contrary to their results, our generation outputs improved by adding attention and increasing context size. However, we also show that multimodal HRED does not improve significantly over text-only HRED, similar to observations by Agrawal et al. (2016) and Qian et al. (2018). Our model learns to handle textual correspondence between the questions and answers, while mostly ignoring the visual context. This indicates that we need better visual models to enTable 1 provides results for different configurations of our model (“T” stands for text-only in the encoder, “M” for multimodal, and “attn” for using attention in the decoder). We experimented with different context sizes and found that output quality improved with increased context size (models with 5-turn context perform better than those with a 2-turn context), confirming the observation"
W18-6514,P02-1040,0,0.103838,"ge and long term context: “Will the 5th result go well with a large sized messenger bag?”, inference over aggregate of images: “List more in the upper material of the 5th image and style as the 3rd and the 5th”, co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of the dataset for the model. This is done since the authors originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3). 3.3 Analysis and Results We report sentence-level B LEU -4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017). 1 https://pytorch.org/ Our code is freely available at: https://github.com/shubhamagarwal92/mmd 3 In future, we plan to exploit state-of-the-art frameworks such as ResNet or DenseNet and fine tune the image encoder jointly, during the training of the model. 2 131 Figure 4: Examples of predictions using M-HRED–attn (5). Recall, we are focusing on generating textual responses. Our model predictions are shown in blue while the true gold target in red. We are showing"
W18-6514,W04-3250,0,0.0306892,"Missing"
W18-6514,W07-0734,0,0.289957,"he 5th result go well with a large sized messenger bag?”, inference over aggregate of images: “List more in the upper material of the 5th image and style as the 3rd and the 5th”, co-reference resolution. Note that we started with the raw transcripts of dialogue sessions to create our own version of the dataset for the model. This is done since the authors originally consider each image as a different context, while we consider all the images in a single turn as one concatenated context (cf. Figure 3). 3.3 Analysis and Results We report sentence-level B LEU -4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007) and ROUGE -L (Lin and Och, 2004) using the evaluation scripts provided by (Sharma et al., 2017). 1 https://pytorch.org/ Our code is freely available at: https://github.com/shubhamagarwal92/mmd 3 In future, we plan to exploit state-of-the-art frameworks such as ResNet or DenseNet and fine tune the image encoder jointly, during the training of the model. 2 131 Figure 4: Examples of predictions using M-HRED–attn (5). Recall, we are focusing on generating textual responses. Our model predictions are shown in blue while the true gold target in red. We are showing only the previous user utterance f"
W18-6514,E09-1078,1,\N,Missing
W18-6514,P04-1077,0,\N,Missing
W19-1601,D18-1287,0,0.0642789,"Missing"
W19-1601,D13-1038,0,0.0319802,"l Linguistics on a nautical chart rather than passively following instructions. In addition, our environment is dynamic. New objects are being created and the user with the agent, together, come up with the desired referring expressions (see Figure 3). A similar interactive method is described in (Schlangen, 2016), where they ground non-linguistic visual information through conversation. In situated dialog, each user can perceive the environment in a different way, meaning that referring expressions need to be carefully selected and verified, especially if the shared environment is ambiguous (Fang et al., 2013). Our contributions include: 1) a generic dialog framework and the implemented software to conduct multiple wizard WoZ experiments for multimodal collaborative planning interaction; 2) available on request, a corpus of 22 dialogs on 2 missions with varying complexities and 3) a corpus analysis (Section 4) indicating that incorporating an extra modality in conjunction with spatial referencing in a chatting interface is crucial for successfully planning missions. 3 Figure 1: Experimental Set-Up, where a) SeeTrack Wizard, b) Chatting Wizard, and c) Subject console. Figure contains images from See"
W19-1601,P97-1035,0,0.73387,"Missing"
W19-1601,P19-1651,0,0.0319595,"d spatial requirements, such as situated robot planning (Misra et al., 2018), developing accurate goal-oriented dialog systems can be extremely challenging, especially in dynamic environments, such as underwater. The ultimate goal of this work is to learn a dialog strategy that optimizes interaction for quality and speed of plan creation, thus linking interaction style with extrinsic task success metrics. Therefore, we conducted a Wizard of Oz (WoZ) study for data collection that can be used to derive reward functions for Reinforcement Learning, as in (Rieser, 2008). Similar work is shown in (Kitaev et al., 2019), where the task involves two humans collaboratively drawing objects with one being the teller and the other the person who draws. The agents must be able to adapt and hold a dialog about novel scenes that will be dynamically constructed. However, in our scenario the agent must be capable of not only adapting but also identifying and editing specific attributes of the dynamic objects that are being created in the process. Previous data collection on situated dialog, such as the Map Task Corpus (Anderson et al., 1991), tackle the importance of referencing objects while giving instructions on a"
W19-1601,E17-1042,0,0.111885,"Missing"
W19-1601,P17-1062,0,0.0662446,"sed in the development of a mixed-initiative datadriven multimodal conversational agent, for planning missions collaboratively with a human operator. With the collected WoZ data, we can capture the main strategies of how to plan a mission and make data-driven simulations possible. Therefore, we can train a Reinforcement Learning agent on simulated dialogs that are fully data-driven with the reward function being derived from our subjects’ preferences, optimizing for plan quality and speed. Moreover, supervised approaches that require less data to learn, such as the Hybrid Code Networks (HCN) (Williams et al., 2017), could be used for the creation of such a system. Finally, the system will be compared to a baseline in a further human evaluation study. Theme 1 Suggestions for extra functionality: Due to delays some subjects were not sure if the program crashed. We had a dialog act “wait” but feedback indicated it would be better to have a visual indicator as well. Note, in the actual future working system, we will not have the same delays as in the WoZ experiment. Theme 2 Chart meta-data: Some subjects (P5 most specifically) desired more meta-data on the plan images they were receiving when referring to a"
W19-1601,P16-1094,0,0.0610886,"Missing"
W19-8644,C12-1008,0,0.0214154,"ble 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that th"
W19-8644,K16-1002,0,0.0500592,"Missing"
W19-8644,K18-1031,0,0.0234145,"NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar to adversarial evalua"
W19-8644,W17-5534,0,0.0278203,"Missing"
W19-8644,W17-3518,0,0.0514265,"tic instances. We use a partial delexicalisation (replacing names with placeholders).5 Datasets 5.2 We experiment on the following two datasets, both in the restaurant/hotel information domain: • NEM3 (Novikova et al., 2017) – Likert-scale rated outputs (scores 1–6) of 3 NLG systems over 3 datasets, totalling 2,460 instances. • E2E system rankings (Duˇsek et al., 2019) – outputs of 21 systems on a single NLG dataset with 2,979 5-way relative rankings. We choose these two datasets because they contain human-assessed outputs from a variety of NLG systems. Another candidate is the WebNLG corpus (Gardent et al., 2017), which we leave for future work due to MR format differences. Although both selected datasets contain ratings for multiple criteria (informativeness, naturalness and quality for NEM and the latter two for E2E), we follow Duˇsek et al. (2017) and focus on the overall quality criterion in our experiments as it takes both semantic accuracy and fluency into account. Model Settings We evaluate our model in several configurations, with increasing amounts of synthetic training data. Note that even setups using training human references (i.e. additional in-domain data) are still “referenceless”—they"
W19-8644,W18-6505,0,0.0177058,"subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar to adversarial evaluation—distinguishing between human- and machine-generated outputs (Goodfellow et al., 2014). This approach is employed in generators for random text (Bowman et al., 2016) and dialogue responses (Kannan and Vinyals, 2016; Li et al., 2017; Bruni and Fernandez, 2017). We argue that our approach is more explainable with users being able to reason with the ord"
W19-8644,D14-1020,0,0.0407575,"Missing"
W19-8644,W04-3250,0,0.0259493,"baseline metrics and the original RatPred system are taken over from Duˇsek et al. (2017). Configurations marked with “*” use human references for test instances (this includes word-overlap-based metrics such as BLEU). System Our base system + generated pairs based on training system outputs + generated pairs based on training human references Training insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza e"
W19-8644,C16-1105,0,0.0411811,"Missing"
W19-8644,W07-0734,0,0.069244,"ilable, and we release our experimental code on GitHub.1 2 The Task(s) The task of NLG QE for ratings is to assign a numerical score to a single NLG output, given its input MR, such as a dialogue act (consisting of the main intent, attributes and values). The score can be e.g. on a Likert scale in the 1-6 range (Novikova et al., 2017). In a pairwise ranking task, the QE system is given two outputs of different NLG systems for the same MR, and decides which one has better quality (see Figure 1). As opposed to automatic word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), no human reference texts for the given MR are required. This widens the scope of possible applications – QE systems can be used for previously unseen MRs. 3 Jointly learning to rank and rate was first introduced by Sculley (2010) for support vector machines and similar approaches have been applied for image classification (Park et al., 2017; Liu et al., 2018) as well as audio classification (Lee et al., 2016), However, we argue that the application for text classification/QE is novel, as is the implementation as a single neural network with two parts that share parameters, capable of trainin"
W19-8644,D17-1230,0,0.0685314,"Missing"
W19-8644,W04-1013,0,0.0575487,"Missing"
W19-8644,P17-1103,0,0.0220179,"on training system outputs + generated pairs based on training human references Training insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE sy"
W19-8644,P10-1157,0,0.0931245,"Missing"
W19-8644,L16-1575,0,0.0313741,"Missing"
W19-8644,W18-7005,0,0.0266463,"ng insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utteranc"
W19-8644,D16-1228,0,0.0189365,"airs based on training human references Training insts 11,921 50,324 428,873 Accuracy 0.708 0.732† 0.740 Avg. loss 0.173 0.158 0.153 Table 2: Results on the E2E rankings dataset. Boldface denotes significant improvements over previous configurations according to pairwise bootstrap resampling (Koehn, 2004) (p < 0.05; † = p < 0.01). clude requests, confirmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predic"
W19-8644,P02-1040,0,0.110835,"king. Both datasets are freely available, and we release our experimental code on GitHub.1 2 The Task(s) The task of NLG QE for ratings is to assign a numerical score to a single NLG output, given its input MR, such as a dialogue act (consisting of the main intent, attributes and values). The score can be e.g. on a Likert scale in the 1-6 range (Novikova et al., 2017). In a pairwise ranking task, the QE system is given two outputs of different NLG systems for the same MR, and decides which one has better quality (see Figure 1). As opposed to automatic word-overlap-based metrics, such as BLEU (Papineni et al., 2002) or METEOR (Lavie and Agarwal, 2007), no human reference texts for the given MR are required. This widens the scope of possible applications – QE systems can be used for previously unseen MRs. 3 Jointly learning to rank and rate was first introduced by Sculley (2010) for support vector machines and similar approaches have been applied for image classification (Park et al., 2017; Liu et al., 2018) as well as audio classification (Lee et al., 2016), However, we argue that the application for text classification/QE is novel, as is the implementation as a single neural network with two parts that"
W19-8644,J18-3002,0,0.0127542,"as of NLP, such as machine translation (MT) (Specia et al., 2010, 2018), research on QE in natural language generation (NLG) from structured meaning representations (MR) such as dialogue acts is relatively recent (Duˇsek et al., 2017; Ueffing et al., 2018) and often focuses on output fluency only (Tian et al., 2018; Kann et al., 2018). In contrast to traditional metrics, QE does not rely on gold-standard human reference texts (Specia et al., 2010), which are expensive to obtain, do not cover the full output space, and are not accurate on the level of individual outputs (Novikova et al., 2017; Reiter, 2018). Automatic QE for NLG has several possible use cases that can improve NLG quality and reliability. For example, rating individual NLG outputs allows to ensure a minimum output quality and engage a backup, e.g., template-based NLG system, if a certain threshold is not met. Relative ranking of multiple NLG outputs can be used directly within a system to rerank n-best outputs or to guide system development, selecting optimal system parameters or comparing to state of the art. In this paper, we present a novel model that jointly learns to perform both tasks—rating individual outputs as well as pa"
W19-8644,W14-3301,0,0.0608728,"Missing"
W19-8644,W18-6530,0,0.0526372,"Missing"
W19-8644,W18-6512,0,0.0144683,"tive topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar t"
W19-8644,P19-1256,0,0.0197181,"et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG outputs and includes better synthetic training data creation methods. Our approach to QE is similar to adversarial evaluation—distinguishing between human- and machine-generated outputs (Goodfellow et al., 2014). This approach is employed in generators for random text (Bowman et al., 2016) and dialogue responses (Kannan and Vinyals, 2016; Li et al., 2017; Bruni and Fernandez, 2017). We argue that our approach is more explainable with users being able to reason with the ordinal output score. Acknowledgments This"
W19-8644,N18-3007,0,0.0665249,"Missing"
W19-8644,D17-1238,1,0.880678,"Missing"
W19-8644,D15-1199,0,0.067814,"Missing"
W19-8644,D17-1239,0,0.0306122,"rmations, etc., while the E2E data only contain informative statements. 7 Related Work QE has been an active topic in many NLP tasks— image captioning (Anderson et al., 2016), dialogue response generation (Lowe et al., 2017), grammar correction (Napoles et al., 2016) or text simplification (Martin et al., 2018)—with MT being perhaps the most prominent area (Specia et al., 2010; Avramidis, 2012; Specia et al., 2018). QE for NLG recently saw an increase of focus in various subtasks, such as title generation (Ueffing et al., 2018; Camargo de Souza et al., 2018) or content selection and ordering (Wiseman et al., 2017). Furthermore, several recent studies focus on predicting NLG fluency only, e.g., (Tian et al., 2018; Kann et al., 2018). However, apart from our work, (Duˇsek et al., 2017) is the only general NLG QE system to our knowledge, which aims to predict the overall quality of a generated utterance, where quality includes both fluency and semantic coverage of the MR. Note that the correct semantic coverage of MRs is a problem for many neural NLG approaches (Gehrmann et al., 2018; Duˇsek et al., 2019; Nie et al., 2019). Compared to Duˇsek et al. (2017), our model is able to jointly rate and rank NLG o"
W19-8644,E17-1019,0,\N,Missing
