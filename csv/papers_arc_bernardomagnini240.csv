2021.sigdial-1.25,Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey,2021,-1,-1,3,0,1499,vevake balaraman,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"This paper aims at providing a comprehensive overview of recent developments in dialogue state tracking (DST) for task-oriented conversational systems. We introduce the task, the main datasets that have been exploited as well as their evaluation metrics, and we analyze several proposed approaches. We distinguish between static ontology DST models, which predict a fixed set of dialogue states, and dynamic ontology models, which can predict dialogue states even when the ontology changes. We also discuss the model{'}s ability to track either single or multiple domains and to scale to new domains, both in terms of knowledge transfer and zero-shot learning. We cover a period from 2013 to 2020, showing a significant increase of multiple domain methods, most of them utilizing pre-trained language models."
2020.paclic-1.20,Simple is Better! Lightweight Data Augmentation for Low Resource Slot Filling and Intent Classification,2020,-1,-1,2,1,10248,samuel louvan,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.lrec-1.259,Comparing Machine Learning and Deep Learning Approaches on {NLP} Tasks for the {I}talian Language,2020,-1,-1,1,1,1501,bernardo magnini,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present a comparison between deep learning and traditional machine learning methods for various NLP tasks in Italian. We carried on experiments using available datasets (e.g., from the Evalita shared tasks) on two sequence tagging tasks (i.e., named entities recognition and nominal entities recognition) and four classification tasks (i.e., lexical relations among words, semantic relations among sentences, sentiment analysis and text classification). We show that deep learning approaches outperform traditional machine learning algorithms in sequence tagging, while for classification tasks that heavily rely on semantics approaches based on feature engineering are still competitive. We think that a similar analysis could be carried out for other languages to provide an assessment of machine learning / deep learning models across different languages."
2020.lrec-1.407,The {E}uropean Language Technology Landscape in 2020: Language-Centric and Human-Centric {AI} for Cross-Cultural Communication in Multilingual {E}urope,2020,4,1,35,0.137101,60,georg rehm,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe{'}s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI {--} including many opportunities, synergies but also misconceptions {--} has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions."
2020.insights-1.3,How Far Can We Go with Data Selection? A Case Study on Semantic Sequence Tagging Tasks,2020,-1,-1,2,1,10248,samuel louvan,Proceedings of the First Workshop on Insights from Negative Results in NLP,0,"Although several works have addressed the role of data selection to improve transfer learning for various NLP tasks, there is no consensus about its real benefits and, more generally, there is a lack of shared practices on how it can be best applied. We propose a systematic approach aimed at evaluating data selection in scenarios of increasing complexity. Specifically, we compare the case in which source and target tasks are the same while source and target domains are different, against the more challenging scenario where both tasks and domains are different. We run a number of experiments on semantic sequence tagging tasks, which are relatively less investigated in data selection, and conclude that data selection has more benefit on the scenario when the tasks are the same, while in case of different (although related) tasks from distant domains, a combination of data selection and multi-task learning is ineffective for most cases."
2020.coling-main.42,Recent Neural Methods on Slot Filling and Intent Classification for Task-Oriented Dialogue Systems: A Survey,2020,-1,-1,2,1,10248,samuel louvan,Proceedings of the 28th International Conference on Computational Linguistics,0,"In recent years, fostered by deep learning technologies and by the high demand for conversational AI, various approaches have been proposed that address the capacity to elicit and understand user{'}s needs in task-oriented dialogue systems. We focus on two core tasks, slot filling (SF) and intent classification (IC), and survey how neural based models have rapidly evolved to address natural language understanding in dialogue systems. We introduce three neural architectures: independent models, which model SF and IC separately, joint models, which exploit the mutual benefit of the two tasks simultaneously, and transfer learning models, that scale the model to new domains. We discuss the current state of the research in SF and IC, and highlight challenges that still require attention."
W19-5911,Leveraging Non-Conversational Tasks for Low Resource Slot Filling: Does it help?,2019,0,0,2,1,10248,samuel louvan,Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue,0,"Slot filling is a core operation for utterance understanding in task-oriented dialogue systems. Slots are typically domain-specific, and adding new domains to a dialogue system involves data and time-intensive processes. A popular technique to address the problem is transfer learning, where it is assumed the availability of a large slot filling dataset for the source domain, to be used to help slot filling on the target domain, with fewer data. In this work, instead, we propose to leverage source tasks based on semantically related non-conversational resources (e.g., semantic sequence tagging datasets), as they are both cheaper to obtain and reusable to several slot filling domains. We show that using auxiliary non-conversational tasks in a multi-task learning setup consistently improves low resource slot filling performance."
W19-5807,How to Use Gazetteers for Entity Recognition with Neural Models,2019,0,2,5,1,17170,simone magnolini,Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5),0,None
P19-3013,{FASTD}ial: Abstracting Dialogue Policies for Fast Development of Task Oriented Agents,2019,0,0,2,0,25321,serra tekiroglu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present a novel abstraction framework called FASTDial for designing task oriented dialogue agents, built on top of the OpenDial toolkit. This framework is meant to facilitate prototyping and development of dialogue systems from scratch also by non tech savvy especially when limited training data is available. To this end, we use a generic and simple frame-slots data-structure with pre-defined dialogue policies that allows for fast design and implementation at the price of some flexibility reduction. Moreover, it allows for minimizing programming effort and domain expert training time, by hiding away many implementation details. We provide a system demonstration screencast video in the following link: https://vimeo.com/329840716"
W18-5704,A Methodology for Evaluating Interaction Strategies of Task-Oriented Conversational Agents,2018,0,0,3,0,7695,marco guerini,Proceedings of the 2018 {EMNLP} Workshop {SCAI}: The 2nd International Workshop on Search-Oriented Conversational {AI},0,"In task-oriented conversational agents, more attention has been usually devoted to assessing task effectiveness, rather than to \textit{how} the task is achieved. However, conversational agents are moving towards more complex and human-like interaction capabilities (e.g. the ability to use a formal/informal register, to show an empathetic behavior), for which standard evaluation methodologies may not suffice. In this paper, we provide a novel methodology to assess - in a completely controlled way - the impact on the quality of experience of agent{'}s interaction strategies. The methodology is based on a within subject design, where two slightly different transcripts of the same interaction with a conversational agent are presented to the user. Through a series of pilot experiments we prove that this methodology allows fast and cheap experimentation/evaluation, focusing on aspects that are overlooked by current methods."
W18-5711,Exploring Named Entity Recognition As an Auxiliary Task for Slot Filling in Conversational Language Understanding,2018,0,0,2,1,10248,samuel louvan,Proceedings of the 2018 {EMNLP} Workshop {SCAI}: The 2nd International Workshop on Search-Oriented Conversational {AI},0,"Slot filling is a crucial task in the Natural Language Understanding (NLU) component of a dialogue system. Most approaches for this task rely solely on the domain-specific datasets for training. We propose a joint model of slot filling and Named Entity Recognition (NER) in a multi-task learning (MTL) setup. Our experiments on three slot filling datasets show that using NER as an auxiliary task improves slot filling performance and achieve competitive performance compared with state-of-the-art. In particular, NER is effective when supervised at the lower layer of the model. For low-resource scenarios, we found that MTL is effective for one dataset."
W18-5036,Toward zero-shot Entity Recognition in Task-oriented Conversational Agents,2018,0,2,4,0,7695,marco guerini,Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue,0,"We present a domain portable zero-shot learning approach for entity recognition in task-oriented conversational agents, which does not assume any annotated sentences at training time. Rather, we derive a neural model of the entity names based only on available gazetteers, and then apply the model to recognize new entities in the context of user utterances. In order to evaluate our working hypothesis we focus on nominal entities that are largely used in e-commerce to name products. Through a set of experiments in two languages (English and Italian) and three different domains (furniture, food, clothing), we show that the neural gazetteer-based approach outperforms several competitive baselines, with minimal requirements of linguistic features."
L18-1085,{KRAUTS}: A {G}erman Temporally Annotated News Corpus,2018,0,1,5,0,3877,jannik strotgen,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1684,Enriching a Lexicon of Discourse Connectives with Corpus-based Data,2018,0,1,3,1,30291,anna feltracco,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
S16-1121,"{FBK}-{HLT}-{NLP} at {S}em{E}val-2016 Task 2: A Multitask, Deep Learning Approach for Interpretable Semantic Textual Similarity",2016,8,4,3,1,17170,simone magnolini,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1339,Acquiring Opposition Relations among {I}talian Verb Senses using Crowdsourcing,2016,0,0,4,1,30291,anna feltracco,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We describe an experiment for the acquisition of opposition relations among Italian verb senses, based on a crowdsourcing methodology. The goal of the experiment is to discuss whether the types of opposition we distinguish (i.e. complementarity, antonymy, converseness and reversiveness) are actually perceived by the crowd. In particular, we collect data for Italian by using the crowdsourcing platform CrowdFlower. We ask annotators to judge the type of opposition existing among pairs of sentences -previously judged as opposite- that differ only for a verb: the verb in the first sentence is opposite of the verb in second sentence. Data corroborate the hypothesis that some opposition relations exclude each other, while others interact, being recognized as compatible by the contributors."
C16-2028,{T}ext{P}ro-{AL}: An Active Learning Platform for Flexible and Efficient Production of Training Data for {NLP} Tasks,2016,8,1,1,1,1501,bernardo magnini,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"This paper presents TextPro-AL (Active Learning for Text Processing), a platform where human annotators can efficiently work to produce high quality training data for new domains and new languages exploiting Active Learning methodologies. TextPro-AL is a web-based application integrating four components: a machine learning based NLP pipeline, an annotation editor for task definition and text annotations, an incremental re-training procedure based on active learning selection from a large pool of unannotated data, and a graphical visualization of the learning status of the system."
2016.gwc-1.16,Using {W}ord{N}et to Build Lexical Sets for {I}talian Verbs,2016,0,1,4,1,30291,anna feltracco,Proceedings of the 8th Global WordNet Conference (GWC),0,We present a methodology for building lexical sets for argument slots of Italian verbs. We start from an inventory of semantically typed Italian verb frames and through a mapping to WordNet we automatically annotate the sets of fillers for the argument positions in a corpus of sentences. We evaluate both a baseline algorithm and a syntax driven algorithm and show that the latter performs significantly better in terms of precision.
W15-0803,Opposition Relations among Verb Frames,2015,19,2,3,1,30291,anna feltracco,"Proceedings of the The 3rd Workshop on {EVENTS}: Definition, Detection, Coreference, and Representation",0,"In this paper we propose a scheme for annotating opposition relations among verb frames in lexical resources. The scheme is tested on the T-PAS resource, an inventory of typed predicate argument structures for Italian, conceived for both linguistic research and computational tasks. After discussing opposition relations from a linguistic point of view and listing the tags we decided to use, we report the results of the experiment we performed to test the annotation scheme, in terms of interannotation agreement and linguistic analysis of annotated data."
S15-2132,{S}em{E}val-2015 Task 4: {T}ime{L}ine: Cross-Document Event Ordering,2015,14,25,6,0.388014,5690,annelyse minard,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the outcomes of the TimeLine task (Cross-Document Event Ordering), that was organised within the Time and Space track of SemEval-2015. Given a set of documents and a set of target entities, the task consisted of building a timeline for each entity, by detecting, anchoring in time and ordering the events involving that entity. The TimeLine task goes a step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents. Four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs, the best of which obtained an F1-score of 7.85 in the main track (timeline creation from raw text)."
R15-1051,Predicting Correlations Between Lexical Alignments and Semantic Inferences,2015,13,0,2,0.952381,17170,simone magnolini,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"While there is a strong intuition that word alignments (e.g. synonymy, hyperonymy) play a relevant role in recognizing textto-text semantic inferences (e.g. textual entailment, semantic similarity), this intuition is often not reflected in the system performances and there is a general need of a deeper comprehension of the role of lexical resources. This paper provides an empirical analysis of the dependencies between data-sets, lexical resources and algorithms that are commonly used in text-to-text inference tasks. We define a resource impact index, based on lexical alignments between pairs of texts, and show that such index is significantly correlated with the performance of different textual entailment algorithms. The result is an operational, algorithm-independent, procedure for predicting the performance of a class of available RTE algorithms."
J15-1008,"Book Reviews: Recognizing Textual Entailment: Models and Applications by {I}do {D}agan, {D}an {R}oth, Mark Sammons and Fabio Massimo Zanzotto",2015,-1,-1,1,1,1501,bernardo magnini,Computational Linguistics,0,None
P14-5008,The Excitement Open Platform for Textual Inferences,2014,13,42,1,1,1501,bernardo magnini,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper presents the Excitement Open Platform (EOP), a generic architecture and a comprehensive implementation for textual inference in multiple languages. The platform includes state-of-art algorithms, a large number of knowledge resources, and facilities for experimenting and testing innovative approaches. The EOP is distributed as an open source software."
rehm-etal-2014-strategic,"The Strategic Impact of {META}-{NET} on the Regional, National and International Level",2014,47,2,22,0.137101,60,georg rehm,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article provides an overview of the dissemination work carried out in META-NET from 2010 until early 2014; we describe its impact on the regional, national and international level, mainly with regard to politics and the situation of funding for LT topics. This paper documents the initiativeÂs work throughout Europe in order to boost progress and innovation in our field."
jezek-etal-2014-pas,{T}-{PAS}; A resource of Typed Predicate Argument Structures for linguistic analysis and semantic processing,2014,15,4,2,1,18933,elisabetta jezek,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The goal of this paper is to introduce T-PAS, a resource of typed predicate argument structures for Italian, acquired from corpora by manual clustering of distributional information about Italian verbs, to be used for linguistic analysis and semantic processing tasks. T-PAS is the first resource for Italian in which semantic selection properties and sense-in-context distinctions of verbs are characterized fully on empirical ground. In the paper, we first describe the process of pattern acquisition and corpus annotation (section 2) and its ongoing evaluation (section 3). We then demonstrate the benefits of pattern tagging for NLP purposes (section 4), and discuss current effort to improve the annotation of the corpus (section 5). We conclude by reporting on ongoing experiments using semiautomatic techniques for extending coverage (section 6)."
piperidis-etal-2014-meta,{META}-{SHARE}: One year after,2014,9,9,9,0,11075,stelios piperidis,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents META-SHARE (www.meta-share.eu), an open language resource infrastructure, and its usage since its Europe-wide deployment in early 2013. META-SHARE is a network of repositories that store language resources (data, tools and processing services) documented with high-quality metadata, aggregated in central inventories allowing for uniform search and access. META-SHARE was developed by META-NET (www.meta-net.eu) and aims to serve as an important component of a language technology marketplace for researchers, developers, professionals and industrial players, catering for the full development cycle of language technology, from research through to innovative products and services. The observed usage in its initial steps, the steadily increasing number of network nodes, resources, users, queries, views and downloads are all encouraging and considered as supportive of the choices made so far. In tandem, take-up activities like direct linking and processing of datasets by language processing services as well as metadata transformation to RDF are expected to open new avenues for data and resources linking and boost the organic growth of the infrastructure while facilitating language technology deployment by much wider research communities and industrial sectors."
2014.lilt-9.4,Decomposing Semantic Inference,2014,0,3,2,0,40339,elana cabria,"Linguistic Issues in Language Technology, Volume 9, 2014 - Perspectives on Semantic Representations for Textual Inference",0,"Beside formal approaches to semantic inference that rely on logical representation of meaning, the notion of Textual Entailment (TE) has been proposed as an applied framework to capture major semantic inference needs across applications in Computational Linguistics. Although several approaches have been tried and evaluation campaigns have shown improvements in TE, a renewed interest is rising in the research community towards a deeper and better understanding of the core phenomena involved in textual inference. Pursuing this direction, we are convinced that crucial progress will derive from a focus on decomposing the complexity of the TE task into basic phenomena and on their combination. In this paper, we carry out a deep analysis on TE data sets, investigating the relations among two relevant aspects of semantic inferences: the logical dimension, i.e. the capacity of the inference to prove the conclusion from its premises, and the linguistic dimension, i.e. the linguistic devices used to accomplish the goal of the inference. We propose a decomposition approach over TE pairs, where single linguistic phenomena are isolated in what we have called atomic inference pairs, and we show that at this granularity level the actual correlation between the linguistic and the logical dimensions of semantic inferences emerges and can be empirically observed."
W13-3805,Entailment graphs for text exploration,2013,0,2,2,0,955,ido dagan,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,None
2013.mtsummit-european.5,Bridges Across the Language Divide {---} {EU}-{BRIDGE} Excitement: Exploring Customer Interactions through Textual {E}ntail{MENT},2013,-1,-1,2,0,955,ido dagan,Proceedings of Machine Translation Summit XIV: European projects,0,None
2013.mtsummit-european.6,Excitement: Exploring Customer Interactions through Textual {E}ntail{MENT},2013,-1,-1,2,0,955,ido dagan,Proceedings of Machine Translation Summit XIV: European projects,0,None
W12-4006,Extracting Context-Rich Entailment Rules from {W}ikipedia Revision History,2012,24,3,2,1,5629,elena cabrio,Proceedings of the 3rd Workshop on the People{'}s Web Meets {NLP}: Collaboratively Constructed Semantic Resources and their Applications to {NLP},0,"Recent work on Textual Entailment has shown a crucial role of knowledge to support entailment inferences. However, it has also been demonstrated that currently available entailment rules are still far from being optimal. We propose a methodology for the automatic acquisition of large scale context-rich entailment rules from Wikipedia revisions, taking advantage of the syntactic structure of entailment pairs to define the more appropriate linguistic constraints for the rule to be successfully applicable. We report on rule acquisition experiments on Wikipedia, showing that it enables the creation of an innovative (i.e. acquired rules are not present in other available resources) and good quality rule repository."
bongelli-etal-2012-corpus,A Corpus of Scientific Biomedical Texts Spanning over 168 Years Annotated for Uncertainty,2012,35,3,8,0,43214,ramona bongelli,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Uncertainty language permeates biomedical research and is fundamental for the computer interpretation of unstructured text. And yet, a coherent, cognitive-based theory to interpret Uncertainty language and guide Natural Language Processing is, to our knowledge, non-existing. The aim of our project was therefore to detect and annotate Uncertainty markers â which play a significant role in building knowledge or beliefs in readers' minds â in a biomedical research corpus. Our corpus includes 80 manually annotated articles from the British Medical Journal randomly sampled from a 168-year period. Uncertainty markers have been classified according to a theoretical framework based on a combined linguistic and cognitive theory. The corpus was manually annotated according to such principles. We performed preliminary experiments to assess the manually annotated corpus and establish a baseline for the automatic detection of Uncertainty markers. The results of the experiments show that most of the Uncertainty markers can be recognized with good accuracy."
cattoni-etal-2012-knowledgestore,The {K}nowledge{S}tore: an Entity-Based Storage System,2012,13,13,4,0,5711,roldano cattoni,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the KnowledgeStore, a large-scale infrastructure for the combined storage and interlinking of multimedia resources and ontological knowledge. Information in the KnowledgeStore is organized around entities, such as persons, organizations and locations. The system allows (i) to import background knowledge about entities, in form of annotated RDF triples; (ii) to associate resources to entities by automatically recognizing, coreferring and linking mentions of named entities; and (iii) to derive new entities based on knowledge extracted from mentions. The KnowledgeStore builds on state of art technologies for language processing, including document tagging, named entity extraction and cross-document coreference. Its design provides for a tight integration of linguistic and semantic features, and eases the further processing of information by explicitly representing the contexts where knowledge and mentions are valid or relevant. We describe the system and report about the creation of a large-scale KnowledgeStore instance for storing and integrating multimedia contents and background knowledge relevant to the Italian Trentino region."
W11-0135,Towards Component-Based Textual Entailment,2011,15,6,2,1,5629,elena cabrio,Proceedings of the Ninth International Conference on Computational Semantics ({IWCS} 2011),0,"In the Textual Entailment community, a shared effort towards a deeper understanding of the core phenomena involved in textual inference is recently arose. To analyse how the common intuition that decomposing TE would allow a better comprehension of the problem from both a linguistic and a computational viewpoint, we propose a definition for strong component-based TE, where each component is in itself a complete TE system, able to address a TE task on a specific phenomenon in isolation. We review the literature according to our definition, trying to position relevant work as more or less close to our idea of strong component-based TE. Several dimensions of the problem are discussed: i) the implementation of system components to address specific inference types, ii) the analysis of the phenomena relevant to component-based TE, and iii) the development of evaluation methodologies to assess TE systems capabilities to address single phenomena in a pair."
W10-3114,Contradiction-focused qualitative evaluation of textual entailment,2010,22,4,1,1,1501,bernardo magnini,Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,0,"In this paper we investigate the relation between positive and negative pairs in Textual Entailment (TE), in order to highlight the role of contradiction in TE datasets. We base our analysis on the decomposition of Text-Hypothesis pairs into monothematic pairs, i.e. pairs where only one linguistic phenomenon at a time is responsible for entailment judgment and we argue that such a deeper inspection of the linguistic phenomena behind textual entailment is necessary in order to highlight the role of contradiction. We support our analysis with a number of empirical experiments, which use current available TE systems."
forner-etal-2010-evaluating,Evaluating Multilingual Question Answering Systems at {CLEF},2010,28,13,3,1,42942,pamela forner,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The paper offers an overview of the key issues raised during the seven yearsÂ activity of the Multilingual Question Answering Track at the Cross Language Evaluation Forum (CLEF). The general aim of the Multilingual Question Answering Track has been to test both monolingual and cross-language Question Answering (QA) systems that process queries and documents in several European languages, also drawing attention to a number of challenging issues for research in multilingual QA. The paper gives a brief description of how the task has evolved over the years and of the way in which the data sets have been created, presenting also a brief summary of the different types of questions developed. The document collections adopted in the competitions are sketched as well, and some data about the participation are provided. Moreover, the main evaluation measures used to evaluate system performances are explained and an overall analysis of the results achieved is presented."
bentivogli-etal-2010-building,Building Textual Entailment Specialized Data Sets: a Methodology for Isolating Linguistic Phenomena Relevant to Inference,2010,9,34,6,0.833333,8246,luisa bentivogli,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper proposes a methodology for the creation of specialized data sets for Textual Entailment, made of monothematic Text-Hypothesis pairs (i.e. pairs in which only one linguistic phenomenon relevant to the entailment relation is highlighted and isolated). The expected benefits derive from the intuition that investigating the linguistic phenomena separately, i.e. decomposing the complexity of the TE problem, would yield an improvement in the development of specific strategies to cope with them. The annotation procedure assumes that humans have knowledge about the linguistic phenomena relevant to inference, and a classification of such phenomena both into fine grained and macro categories is suggested. We experimented with the proposed methodology over a sample of pairs taken from the RTE-5 data set, and investigated critical issues arising when entailment, contradiction or unknown pairs are considered. The result is a new resource, which can be profitably used both to advance the comprehension of the linguistic phenomena relevant to entailment judgments and to make a first step towards the creation of large-scale specialized data sets."
C10-2012,Toward Qualitative Evaluation of Textual Entailment Systems,2010,11,4,2,1,5629,elena cabrio,Coling 2010: Posters,0,"This paper presents a methodology for a quantitative and qualitative evaluation of Textual Entailment systems. We take advantage of the decomposition of Text Hypothesis pairs into monothematic pairs, i.e. pairs where only one linguistic phenomenon at a time is responsible for entailment judgment, and propose to run TE systems over such datasets. We show that several behaviours of a system can be explained in terms of the correlation between the accuracy on monothematic pairs and the accuracy on the corresponding original pairs."
W09-2505,Optimizing Textual Entailment Recognition Using Particle Swarm Optimization,2009,14,12,2,0,3127,yashar mehdad,Proceedings of the 2009 Workshop on Applied Textual Inference ({T}ext{I}nfer),0,"This paper introduces a new method to improve tree edit distance approach to textual entailment recognition, using particle swarm optimization. Currently, one of the main constraints of recognizing textual entailment using tree edit distance is to tune the cost of edit operations, which is a difficult and challenging task in dealing with the entailment problem and datasets. We tried to estimate the cost of edit operations in tree edit distance algorithm automatically, in order to improve the results for textual entailment. Automatically estimating the optimal values of the cost operations over all RTE development datasets, we proved a significant enhancement in accuracy obtained on the test sets."
cabrio-etal-2008-qall,The {QALL}-{ME} Benchmark: a Multilingual Resource of Annotated Spoken Requests for Question Answering,2008,7,9,3,1,5629,elena cabrio,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents the QALL-ME benchmark, a multilingual resource of annotated spoken requests in the tourism domain, freely available for research purposes. The languages currently involved in the project are Italian, English, Spanish and German. It introduces a semantic annotation scheme for spoken information access requests, specifically derived from Question Answering (QA) research. In addition to pragmatic and semantic annotations, we propose three QA-based annotation levels: the Expected Answer Type, the Expected Answer Quantifier and the Question Topical Target of a request, to fully capture the content of a request and extract the sought-after information. The QALL-ME benchmark is developed under the EU-FP6 QALL-ME project which aims at the realization of a shared and distributed infrastructure for Question Answering (QA) systems on mobile devices (e.g. mobile phones). Questions are formulated by the users in free natural language input, and the system returns the actual sequence of words which constitutes the answer from a collection of information sources (e.g. documents, databases). Within this framework, the benchmark has the twofold purpose of training machine learning based applications for QA, and testing their actual performance with a rapid turnaround in controlled laboratory setting."
magnini-etal-2008-evaluation,Evaluation of Natural Language Tools for {I}talian: {EVALITA} 2007,2008,20,12,1,1,1501,bernardo magnini,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"EVALITA 2007, the first edition of the initiative devoted to the evaluation of Natural Language Processing tools for Italian, provided a shared framework where participantsÂ systems had the possibility to be evaluated on five different tasks, namely Part of Speech Tagging (organised by the University of Bologna), Parsing (organised by the University of Torino), Word Sense Disambiguation (organised by CNR-ILC, Pisa), Temporal Expression Recognition and Normalization (organised by CELCT, Trento), and Named Entity Recognition (organised by FBK, Trento). We believe that the diffusion of shared tasks and shared evaluation practices is a crucial step towards the development of resources and tools for Natural Language Processing. Experiences of this kind, in fact, are a valuable contribution to the validation of existing models and data, allowing for consistent comparisons among approaches and among representation schemes. The good response obtained by EVALITA, both in the number of participants and in the quality of results, showed that pursuing such goals is feasible not only for English, but also for other languages."
W07-1401,The Third {PASCAL} Recognizing Textual Entailment Challenge,2007,28,623,2,1,41190,danilo giampiccolo,Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing,0,"This paper presents the Third PASCAL Recognising Textual Entailment Challenge (RTE-3), providing an overview of the dataset creating methodology and the submitted systems. In creating this year's dataset, a number of longer texts were introduced to make the challenge more oriented to realistic scenarios. Additionally, a pool of resources was offered so that the participants could share common tools. A pilot task was also set up, aimed at differentiating unknown entailments from identified contradictions and providing justifications for overall system decisions. 26 participants submitted 44 runs, using different approaches and generally presenting new entailment models and achieving higher scores than in the previous challenges."
S07-1001,{S}em{E}val-2007 Task 01: Evaluating {WSD} on Cross-Language Information Retrieval,2007,17,19,2,0.0529895,8824,eneko agirre,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper presents a first attempt of an application-driven evaluation exercise of WSD. We used a CLIR testbed from the Cross Lingual Evaluation Forum. The expansion, indexing and retrieval strategies where fixed by the organizers. The participants had to return both the topics and documents tagged with WordNet 1.6 word senses. The organization provided training data in the form of a pre-processed Semcor which could be readily used by participants. The task had two participants, and the organizer also provide an in-house WSD system for comparison."
S07-1041,{IRST}-{BP}: Web People Search Using Name Entities,2007,4,24,2,0,21558,octavian popescu,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"In this paper we describe a person clustering system for web pages and report the results we have obtained on the test set of the Semeval 2007 Web Person Search task. Deciding which particular person a name refers to within a text document depends mainly on the capacity to extract the relevant information out of texts when it is present. We consider relevant here to stand primarily for two properties: (1) uniqueness and (2) appropriateness. In order to address both (1) and (2) our method gives primary importance to Name Entities (NEs), defined according to the ACE specifications. The common nouns not referring to entities are considered further as coreference clues only if they are found within already coreferred documents."
W06-2713,Representing and Accessing Multilevel Linguistic Annotation using the {MEANING} Format,2006,3,3,4,0,42361,emanuele pianta,Proceedings of the 5th Workshop on {NLP} and {XML} ({NLPXML}-2006): Multi-Dimensional Markup in Natural Language Processing,0,"We present an XML annotation format (MEANING Annotation Format, MAF) specifically designed to represent and integrate different levels of linguistic annotations and a tool that provides flexible access to them (MEANING Browser). We describe our experience in integrating linguistic annotations coming from different sources, and the solutions we adopted to implement efficient access to corpora annotated with the Meaning Format."
W06-0504,Ontology Population from Textual Mentions: Task Definition and Benchmark,2006,10,20,1,1,1501,bernardo magnini,Proceedings of the 2nd Workshop on Ontology Learning and Population: Bridging the Gap between Text and Knowledge,0,"In this paper we propose and investigate Ontology Population from Textual Mentions (OPTM), a sub-task of Ontology Population from text where we assume that mentions for several kinds of entities (e.g. PERSON, ORGANIZATION , LOCATION , GEOPOLITICAL_ ENTITY) are already extracted from a document collection. On the one hand, OPTM simplifies the general Ontology Population task, limiting the input textual material; on the other hand, it introduces challenging extensions to Ontology Population restricted to named entities, being open to a wider spectrum of linguistic phenomena. We describe a manually created benchmark for OPTM and discuss several factors which determine the difficulty of the task."
kouylekov-magnini-2006-building,Building a Large-Scale Repository of Textual Entailment Rules,2006,7,5,2,1,29816,milen kouylekov,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Entailment rules are rules where the left hand side (LHS) specifies some knowledge which entails the knowledge expressed n the RHS of the rule, with some degree of confidence. Simple entailment rules can be combined in complex entailment chains, which n turn are at the basis of entailment-based reasoning, which has been recently proposed as a pervasive and application independent approach to Natural Language Understanding. We present the first elease of a large-scale repository of entailment rules at the lexical level, which have been derived from a number of available resources, including WordNet and a word similarity database. Experiments on the PASCAL-RTE dataset show that this resource plays a crucial role in recognizing textual entailment."
magnini-etal-2006-multilingual,The Multilingual Question Answering Track at {CLEF},2006,3,6,1,1,1501,bernardo magnini,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents an overview of the Multilingual Question Answering evaluation campaigns which have been organized at CLEF (Cross Language Evaluation Forum) since 2003. Over the years, the competition has registered a steady increment in the number of participants and languages involved. In fact, from the original eight groups which participated in 2003 QA track, the number of competitors in 2005 rose to twenty-four. Also, the performances of the systems have steadily improved, and the average of the best performances in the 2005 saw an increase of 10{\%} with respect to the previous year."
E06-1003,Weakly Supervised Approaches for Ontology Population,2006,11,51,2,1,11964,hristo tanev,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
W04-3249,Unsupervised Domain Relevance Estimation for Word Sense Disambiguation,2004,9,16,2,0,3532,alfio gliozzo,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents Domain Relevance Estimation (DRE), a fully unsupervised text categorization technique based on the statistical estimation of the relevance of a text with respect to a certain category. We use a pre-defined set of categories (we call them domains) which have been previously associated to WORDNET word senses. Given a certain domain, DRE distinguishes between relevant and non-relevant texts by means of a Gaussian Mixture model that describes the frequency distribution of domain words inside a large-scale corpus. Then, an Expectation Maximization algorithm computes the parameters that maximize the likelihood of the model on the empirical data. The correct identification of the domain of the text is a crucial point for Domain Driven Disambiguation, an unsupervised Word Sense Disambiguation (WSD) methodology that makes use of only domain information. Therefore, DRE has been exploited and evaluated in the context of a WSD task. Results are comparable to those of state-ofthe-art unsupervised WSD systems and show that DRE provides an important contribution."
W04-2214,"Revising the {W}ordnet Domains Hierarchy: semantics, coverage and balancing",2004,8,142,3,0.833333,8246,luisa bentivogli,Proceedings of the Workshop on Multilingual Linguistic Resources,0,"The continuous expansion of the multilingual information society has led in recent years to a pressing demand for multilingual linguistic resources suitable to be used for different applications.n n In this paper we present the WordNet Domains Hierarchy (WDH), a language-independent resource composed of 164, hierarchically organized, domain labels (e.g. Architecture, Sport, Medicine). Although WDH has been successfully applied to various Natural Language Processing tasks, the first available version presented some problems, mostly related to the lack of a clear semantics of the domain labels. Other correlated issues were the coverage and the balancing of the domains. We illustrate a new version of WDH addressing these problems by an explicit and systematic reference to the Dewey Decimal Classification. The new version of WDH has a better defined semantics and is applicable to a wider range of tasks."
W04-0805,The {I}talian lexical sample task at Senseval-3,2004,5,2,1,1,1501,bernardo magnini,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"The Italian lexical sample task at SENSEVAL-3 provided a framework to evaluate supervised and semi-supervised WSD systems. This paper reports on the task preparation xe2x80x93 which offered the opportunity to review and refine the Italian MultiWordNet xe2x80x93 and on the results of the six participants, focussing on both the manual and automatic tagging procedures."
W04-0861,The {``}Meaning{''} system on the {E}nglish all-words task,2004,0,4,5,0,49095,luis villarejo,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
tanev-etal-2004-multilingual,Multilingual Pattern Libraries for Question Answering: a Case Study for Definition Questions,2004,4,4,5,1,11964,hristo tanev,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we investigate the effectiveness of a novel resource for Multilingual Question Answering (QA). Such a resource consists of a set of multilingual pattern libraries for answer extraction and validation. In the spirit of the ongoing attempts to develop freely available resources for QA, we argue that the distribution and use of pattern libraries will contribute to make Multilingual QA a more feasible task."
atserias-etal-2004-cross,Cross-Language Acquisition of Semantic Models for Verbal Predicates,2004,11,2,2,0,36866,jordi atserias,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents a semantic-driven methodology for the automatic acquisition of verbal models. Our approach relies strongly on the semantic generalizations allowed by already existing resources (e.g. Domain labels, Named Entity categories, concepts in the SUMO ontology, etc). Several experiments have been carried out using comparable corpora in four languages (Italian, Spanish, Basque and English) and two domains (FINANCE and SPORT) showing that the semantic patterns acquired can be general enough to be ported from one language to the other language."
C04-1163,A Semantic-based Approach to Interoperabiltity of Classification Hierarchies: Evaluation of Linguistic Techniques,2004,7,18,1,1,1501,bernardo magnini,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Classification Hierarchies (CHs) are widely used to organize documents in a way that makes their retrieval casier. Common examples of CHs are Web directories, marketplace catalogs, and file systems. In this paper we discuss and evaluate CtxMatch, an approach to interoperability that discovers mappings among CHs considering the semantic interpretation of their nodes. CtxMatch performs a linguistic processing of the labels attached to the nodes, including tokenization, Part of Speech tagging, multiword recognition and word sense disambiguation. We present an evaluation of the overall performance of the approach over Web directories as well as a systematic analysis of the linguistic modules involved."
W02-1304,{MEANING}: a Roadmap to Knowledge Technologies,2002,30,30,2,0,6129,german rigau,{COLING}-02: A Roadmap for Computational Linguistics,0,"Knowledge Technologies need to extract knowledge from existing texts, which calls for advanced Human Language Technologies (HLT). Progress is being made in Natural Language Processing but there is still a long way towards Natural Language Understanding. An important step towards this goal is the development of technologies and resources that deal with concepts rather than words. The MEANING project argues that we need to solve two complementary and intermediate tasks to enable the next generation of intelligent open domain HLT application systems: Word Sense Disambiguation and large-scale enrichment of Lexical Knowledge Bases. Innovations in this area will lead to HLT with deeper understanding of texts, and immediate progress in real applications of Knowledge Technologies."
W02-1109,A {W}ord{N}et-Based Approach to Named Entites Recognition,2002,6,40,1,1,1501,bernardo magnini,{COLING}-02: {SEMANET}: Building and Using Semantic Networks,0,"This paper presents a Named Entities (NE) recognition system for the English written language, which combines the wealth of the WordNet taxonomy and the effectiveness of traditional rule-based approaches. The core of the system relies on the combination of approximately 200 language-dependent rules with a set of predicates, defined on the WordNet hierarchy, for the identification of both proper nouns and trigger words. The strengths of this approach are twofold. First, the use of a semantic network allows it to cope with the difficulty of building and maintaining extensive gazetteers. Second, considering the recent spread of WordNet-like semantic networks for languages other than English and aligned with the English version, the use of language-independent predicates offers a useful basis for achieving multilinguality."
P02-1054,Is It the Right Answer? Exploiting Web Redundancy for Answer Validation,2002,13,174,1,1,1501,bernardo magnini,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"Answer Validation is an emerging topic in Question Answering, where open domain systems are often required to rank huge amounts of candidate answers. We present a novel approach to answer validation based on the intuition that the amount of implicit knowledge which connects an answer to a question can be quantitatively estimated by exploiting the redundancy of Web information. Experiments carried out on the TREC-2001 judged-answer collection show that the approach achieves a high level of performance (i.e. 81% success rate). The simplicity and the efficiency of this approach make it suitable to be used as a module in Question Answering systems."
magnini-etal-2002-towards,Towards Automatic Evaluation of Question/Answering Systems,2002,13,7,1,1,1501,bernardo magnini,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper presents an innovative approach to the automatic evaluation of Question Answering systems. The methodology relies on the use of the Web, considered as an xe2x80x9coraclexe2x80x9d containing all the information needed to check the relevance of a candidate answer with respect to a given question. The procedure is completely automatic (i.e. no human intervention is required) and it is based on the assumption that the answersxe2x80x99 relevance can be assessed from a purely quantitative perspective. The methodology is based on a Web search using patterns derived both from the question and from the answer. Different kinds of patterns have been identified, ranging from xe2x80x9clenientxe2x80x9d (i.e. boolean combinations of single words), to xe2x80x9cstrictxe2x80x9d patterns (i.e. whole sentences or combinations of phrases). A statistically-based algorithm has been developed which considers both the kinds of patterns used in the search and the number of documents returned from the Web. Experiments carried out on the TREC-10 corpus show that the approach achieves a high level of performance (i.e. 80% success"
S01-1027,Using Domain Information for Word Sense Disambiguation,2001,5,51,1,1,1501,bernardo magnini,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"The major goal in ITC-irst's participation at Senseval-2 was to test the role of domain information in word sense disambiguation. The underlying working hypothesis is that domain labels, such as Medicine, Architecture and Sport provide a natural way to establish semantic relations among word senses, which can be profitably used during the disambiguation process. For each task in which we participated (i.e. English all words, English 'lexical sample' and Italian 'lexical sample') a different mix of knowledge based and statistical techniques were implemented."
W00-1102,Exploiting Lexical Expansions and {B}oolean Compositions for Web Querying,2000,11,9,1,1,1501,bernardo magnini,{ACL}-2000 Workshop on Recent Advances in Natural Language Processing and Information Retrieval,0,"This paper describes an experiment aiming at evaluating the role of NLP based optimizations (i.e. morphological derivation and synonymy expansion) in web search strategies. Keywords and their expansions are composed in two different Boolean expressions (i.e. expansion insertion and Cartesian combination) and then compared with a keyword conjunctive composition, considered as the baseline. Results confirm the hypothesis that linguistic optimizations significantly improve the search engine performances."
W00-0804,Experiments in Word Domain Disambiguation for Parallel Texts,2000,11,44,1,1,1501,bernardo magnini,{ACL}-2000 Workshop on Word Senses and Multi-linguality,0,"This paper describes some preliminary results about Word Domain Disambiguation, a variant of Word Sense Disambiguation where words in a text are tagged with a domain label in place of a sense label. The English WORDNET and its aligned Italian version, MULTIWORDNET, both augmented with domain labels, are used as the main information repositories. A baseline algorithm for Word Domain Disambiguation is presented and then compared with a mutual help disambiguation strategy, which takes advantages of the shared senses of parallel bilingual texts."
roventini-etal-2000-italwordnet,{I}tal{W}ord{N}et: a Large Semantic Database for {I}talian,2000,12,36,4,0,45899,adriana roventini,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The focus of this paper is on the work we are carrying out to develop a large semantic database within an Italian national project, SITAL, aiming at realizing a set of integrated (compatible) resources and tools for the automatic processing of the Italian language. Within SI-TAL, ItalWordNet is the reference lexical resource which will contain information related to about 130,000 word senses grouped into synsets. This lexical database is not being created ex novo, but extending and revising the Italian lexical wordnet built in the framework of the EuroWordNet project. In this paper we firstly describe how the lexical coverage of our wordnet is being extended by adding adjectives, adverbs and proper nouns, plus a terminological subset belonging to the economic and financial domain. The relevant changes involved by these extensions both in the linguistic model and in the data structure are then illustrated. In particular we discuss i) the new semantic relations identified to encode information on adjectives and adverbs ii) the new architecture including the terminological subset."
magnini-cavaglia-2000-integrating,Integrating Subject Field Codes into {W}ord{N}et,2000,10,318,1,1,1501,bernardo magnini,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In this paper, we present a lexical resource where WordNet synsets are annotated with Subject Field Codes. We discuss both the methodological issues we dealt with and the annotation techniques used. A quantitative analysis of the resource coverage, as well as a qualitative evaluation of the proposed annotations, are reported."
W97-0805,Lexical Discrimination with the {I}talian Version of {W}ord{N}et,1997,6,10,2,0,55546,alessandro artale,Automatic Information Extraction and Building of Lexical Semantic Resources for {NLP} Applications,0,None
A92-1003,An Approach to Multilevel Semantics for Applied Systems,1992,20,9,2,0,14633,alberto lavelli,Third Conference on Applied Natural Language Processing,0,"Multilevel semantics has been proposed as a powerful architecture for semantic analysis. We propose a methodology that, while maintaining the generality of the multilevel approach, is able to establish formal constraints over the possible ways to organize the level hierarchy. More precisely, we propose a strong version of the multilevel approach in which a level can be defined if and only if it is possible to characterize a meaningfulness notion peculiar to that level. Within such an architecture each level reached during the analysis computes its meaningfulness value; this result is then handled according to modalities that are peculiar to that level.The component described in this paper was designed to be portable with respect to the application domain and so far has been tested as the semantic analysis component of two multimedial dialog systems, ALFresco and MAIA."
