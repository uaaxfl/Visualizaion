2019.icon-1.28,J95-3006,0,0.639725,"urse Collection is populated for the next episode and return the value in ‘NEW’ slot of the Discourse Collection as the answer. The answer to the question rAmA nE kyA dIyA? [Kya Question Type] answer will be kalama (refer Figure 4). Kiske: From Table 2, we observe that subject whose entity is asked is known. We consider this subject as Main Noun(MN), we check the children of MN who have relation ‘k7’ and return it as the answer. If this relation doesn’t exist, we check for children with ‘r6’ relation and return it as the answer. ‘r6’ and ‘k7’ are called SambandhRelations in Panninian Grammar (Bharati et al., 1995). The answer to the question kauvA kiske talAza meM udA? [Kiske Question Type] answer will be pAnI (refer Figure 7). Kiska: We extract the Main Verb(MV) from the Parser_Output of the Discourse Collection 3 Experiments Panchatantra is a collection of fables. It has five parts, Mitra-Bheda (The loss of friends), Mitra-laabha (The winning of friends), Kakolukiyam (on crows and owls), Labdhapranasam(Losing what you have gained) and Apariksitakarakam (IllConsidered actions). We have chosen a corpus of 65 stories from the tales across all parts of Panchantantra to test our system. We collected the s"
2020.eamt-1.9,C12-2008,0,0.0310288,"use of crosslingual resources to bridge the language barrier and induce inter-language correspondence. Bel et al. (2003) used a bilingual dictionary to translate documents in the target language to the source language and trained a classifier in the source language for text classification. Mihalcea et al. (2007) used a bilingual lexicon to translate subjective words and phrases in the source language into the target language. Shi et al. (2010) utilizes a bilingual dictionary to translate the classification model from a source language to a target language rather than the documents themselves. Balamurali et al. (2012) used WordNet senses as features for CLSA in Indian languages (Hindi and Marathi). The CLMM model (Meng et al., 2012) treated the source language and the target language words in an unlabeled bilingual parallel dataset as generated simultaneously by a set of mixture components. The CR-RL approach (Xiao and Guo, 2013) learned word embeddings by using a set of bilingual word pairs where one part of the word vector contains language specific features and the other part contains language independent features. CL-SCL model (Prettenhofer and Stein, 2010) leveraged structural correspondence learning"
2020.eamt-1.9,C10-1004,0,0.0383042,"vides a detailed comparison of our approach with prior work in both the low-resource and no-resource setting. Section 8 addresses the advantages and shortcomings of the proposed approach and state our concluding remarks. 2 Background and Related Work CLSC using Machine Translation Systems : The most straightforward approach in CLSC involves using machine translation systems to translate sentences, words, phrases or documents in the target language to the source language and then learning a classifier in the source language to predict the sentiment (Kanayama et al., 2004; Wan, 2008; Wan, 2009; Banea et al., 2010; Lu et al., 2011; Can et al., 2018). The baseline CL-MT (Prettenhofer and Stein, 2010) method uses this technique by using Google Translate1 to translate documents in the target language to the source language and learns a classifier in the source language using the bag-of-words features. Similarly, the BiDRL model (Zhou et al., 2016) used Google Translate and employed a joint learning approach to simultaneously learn both word and document representations in both source and target language which are then used for sentiment classification. However, these methods are overly reliant on the perf"
2020.eamt-1.9,Q17-1010,0,0.0171714,"VD Music Books DVD Music Train 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 Test 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 Unlabelled 50000 30000 25220 165470 91516 60392 32870 9358 15940 169780 68326 55892 Table 1: Multilingual Amazon Review Text Classification dataset statistics. 3.2 Sentiraama Dataset Positive Negative Total Movies 136 131 267 Train Test Books +ve -ve 80 80 20 20 Movies +ve -ve 108 105 28 26 Table 3: Subset of the Sentiraama corpus used in our experiments. 4 Multilingual Word Representation For our experiments, we train fastText embeddings (Bojanowski et al., 2017) to project each word to a monolingual semantic space for each language in the datasets described in Section 3. We then employ the unsupervised MUSE approach (Conneau et al., 2017) to align the monolingual spaces of each language in an adversarial manner to a common multilingual semantic space. While training MUSE we use English as the target semantic space and align all the other monolingual semantic spaces to this space. Let X = {x1 , x2 , . . . xa } and Y = {y1 , y2 , . . . yb } be the source and target fastText word embeddings respectively. Let W be a linear mapping from X to Y. A discrimi"
2020.eamt-1.9,Q18-1039,0,0.186547,"data in most languages makes it a challenging task to develop deep learning based solutions for them. Hence there is a pressing need to pay special attention to developing solutions capable of sentiment analysis in a low resource setting. Some of the initial methods that attempt to tackle this problem of data scarcity using transfer learning (training a neural model on one language and applying the trained model on another language via weight sharing) do not perform well due to the limited overlap between the vocabularies of the different languages and difference in their syntactic structure (Chen et al., 2018b). Cross-lingual sentiment classification (CLSC) methods try to alleviate this problem by leveraging labeled data from one language to improve the performance on another language (Bel et al., 2003). However, these methods typically rely on auxiliary cross-lingual resources such as a parallel corpora (Yarowsky et al., 2001; Xu and Yang, 2017), bilingual lexicons (Mihalcea et al., 2007) or the use of machine translation systems (Kanayama et al., 2004; Wan, 2009; Prettenhofer and Stein, 2010; Can et al., 2018). Unfortunately, the curation of such cross-lingual resources is both a time and a labo"
2020.eamt-1.9,L18-1100,1,0.932484,"eraging various monolingual datasets for training without any kind of crosslingual supervision. The proposed architecture attempts to learn language agnostic sentiment features via adversarial training on multiple resource-rich languages which can then be leveraged for inferring sentiment information at a sentence level on a low resource language. Our model outperforms the current state-of-the-art methods on the Multilingual Amazon Review Text Classification dataset (Prettenhofer and Stein, 2010) and achieves significant performance gains over prior work on the low resource Sentiraama corpus (Gangula and Mamidi, 2018). A detailed analysis of our research highlights the ability of our architecture to perform significantly well in the presence of minimal amounts of training data for low resource languages. c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 1 Introduction Sentiment analysis refers to a series of methods, techniques, and tools aimed at extracting the intended sentiment from a written opinion. Traditional sentiment analysis techniques have relied on using supervised term weighting methods including terms’ distribution of"
2020.eamt-1.9,P15-1162,0,0.0684682,"Missing"
2020.eamt-1.9,C04-1071,0,0.379663,"aring) do not perform well due to the limited overlap between the vocabularies of the different languages and difference in their syntactic structure (Chen et al., 2018b). Cross-lingual sentiment classification (CLSC) methods try to alleviate this problem by leveraging labeled data from one language to improve the performance on another language (Bel et al., 2003). However, these methods typically rely on auxiliary cross-lingual resources such as a parallel corpora (Yarowsky et al., 2001; Xu and Yang, 2017), bilingual lexicons (Mihalcea et al., 2007) or the use of machine translation systems (Kanayama et al., 2004; Wan, 2009; Prettenhofer and Stein, 2010; Can et al., 2018). Unfortunately, the curation of such cross-lingual resources is both a time and a labour intensive task. Hence, there is a need for architectures that can perform well in the absence of such cross-lingual resources. In this paper, we address this problem by presenting a neural Language Invariant Sentiment Analyzer (LISA) architecture that is capable of training on multiple monolingual sentiment labelled datasets to learn language agnostic sentiment features that can be transferred to perform sentiment analysis in low-resource languag"
2020.eamt-1.9,D14-1181,0,0.0106397,"Missing"
2020.eamt-1.9,P11-1033,0,0.0266952,"parison of our approach with prior work in both the low-resource and no-resource setting. Section 8 addresses the advantages and shortcomings of the proposed approach and state our concluding remarks. 2 Background and Related Work CLSC using Machine Translation Systems : The most straightforward approach in CLSC involves using machine translation systems to translate sentences, words, phrases or documents in the target language to the source language and then learning a classifier in the source language to predict the sentiment (Kanayama et al., 2004; Wan, 2008; Wan, 2009; Banea et al., 2010; Lu et al., 2011; Can et al., 2018). The baseline CL-MT (Prettenhofer and Stein, 2010) method uses this technique by using Google Translate1 to translate documents in the target language to the source language and learns a classifier in the source language using the bag-of-words features. Similarly, the BiDRL model (Zhou et al., 2016) used Google Translate and employed a joint learning approach to simultaneously learn both word and document representations in both source and target language which are then used for sentiment classification. However, these methods are overly reliant on the performance of the ma"
2020.eamt-1.9,P12-1060,0,0.0194431,"ed a bilingual dictionary to translate documents in the target language to the source language and trained a classifier in the source language for text classification. Mihalcea et al. (2007) used a bilingual lexicon to translate subjective words and phrases in the source language into the target language. Shi et al. (2010) utilizes a bilingual dictionary to translate the classification model from a source language to a target language rather than the documents themselves. Balamurali et al. (2012) used WordNet senses as features for CLSA in Indian languages (Hindi and Marathi). The CLMM model (Meng et al., 2012) treated the source language and the target language words in an unlabeled bilingual parallel dataset as generated simultaneously by a set of mixture components. The CR-RL approach (Xiao and Guo, 2013) learned word embeddings by using a set of bilingual word pairs where one part of the word vector contains language specific features and the other part contains language independent features. CL-SCL model (Prettenhofer and Stein, 2010) leveraged structural correspondence learning with the help of a bilingual dictionary to learn a source-target feature space. Pham et al. (2015) used a parallel co"
2020.eamt-1.9,P07-1123,0,0.334958,"e and applying the trained model on another language via weight sharing) do not perform well due to the limited overlap between the vocabularies of the different languages and difference in their syntactic structure (Chen et al., 2018b). Cross-lingual sentiment classification (CLSC) methods try to alleviate this problem by leveraging labeled data from one language to improve the performance on another language (Bel et al., 2003). However, these methods typically rely on auxiliary cross-lingual resources such as a parallel corpora (Yarowsky et al., 2001; Xu and Yang, 2017), bilingual lexicons (Mihalcea et al., 2007) or the use of machine translation systems (Kanayama et al., 2004; Wan, 2009; Prettenhofer and Stein, 2010; Can et al., 2018). Unfortunately, the curation of such cross-lingual resources is both a time and a labour intensive task. Hence, there is a need for architectures that can perform well in the absence of such cross-lingual resources. In this paper, we address this problem by presenting a neural Language Invariant Sentiment Analyzer (LISA) architecture that is capable of training on multiple monolingual sentiment labelled datasets to learn language agnostic sentiment features that can be"
2020.eamt-1.9,D16-1058,0,0.0785573,"Missing"
2020.eamt-1.9,W15-1512,0,0.0254908,"). The CLMM model (Meng et al., 2012) treated the source language and the target language words in an unlabeled bilingual parallel dataset as generated simultaneously by a set of mixture components. The CR-RL approach (Xiao and Guo, 2013) learned word embeddings by using a set of bilingual word pairs where one part of the word vector contains language specific features and the other part contains language independent features. CL-SCL model (Prettenhofer and Stein, 2010) leveraged structural correspondence learning with the help of a bilingual dictionary to learn a source-target feature space. Pham et al. (2015) used a parallel corpus between the source language and the target language to learn bilingual paragraph vectors (Bi-PV). UMM (Xu and Wan, 2017) learned multilingual sentimentaware word representations based on unlabeled parallel data and used pivot languages to transfer sentiment information in the absence of parallel data . The CLDFA approach (Xu and Yang, 1 https://translate.google.com/ 2017) adopted cross-lingual distillation and adversarial techniques on parallel corpora for CLSC. Our work draws inspiration from the ADAN-GRL model (Chen et al., 2018b) which employed language adversarial t"
2020.eamt-1.9,D13-1153,0,0.0209821,"lingual lexicon to translate subjective words and phrases in the source language into the target language. Shi et al. (2010) utilizes a bilingual dictionary to translate the classification model from a source language to a target language rather than the documents themselves. Balamurali et al. (2012) used WordNet senses as features for CLSA in Indian languages (Hindi and Marathi). The CLMM model (Meng et al., 2012) treated the source language and the target language words in an unlabeled bilingual parallel dataset as generated simultaneously by a set of mixture components. The CR-RL approach (Xiao and Guo, 2013) learned word embeddings by using a set of bilingual word pairs where one part of the word vector contains language specific features and the other part contains language independent features. CL-SCL model (Prettenhofer and Stein, 2010) leveraged structural correspondence learning with the help of a bilingual dictionary to learn a source-target feature space. Pham et al. (2015) used a parallel corpus between the source language and the target language to learn bilingual paragraph vectors (Bi-PV). UMM (Xu and Wan, 2017) learned multilingual sentimentaware word representations based on unlabeled"
2020.eamt-1.9,D17-1053,0,0.0147441,"erated simultaneously by a set of mixture components. The CR-RL approach (Xiao and Guo, 2013) learned word embeddings by using a set of bilingual word pairs where one part of the word vector contains language specific features and the other part contains language independent features. CL-SCL model (Prettenhofer and Stein, 2010) leveraged structural correspondence learning with the help of a bilingual dictionary to learn a source-target feature space. Pham et al. (2015) used a parallel corpus between the source language and the target language to learn bilingual paragraph vectors (Bi-PV). UMM (Xu and Wan, 2017) learned multilingual sentimentaware word representations based on unlabeled parallel data and used pivot languages to transfer sentiment information in the absence of parallel data . The CLDFA approach (Xu and Yang, 1 https://translate.google.com/ 2017) adopted cross-lingual distillation and adversarial techniques on parallel corpora for CLSC. Our work draws inspiration from the ADAN-GRL model (Chen et al., 2018b) which employed language adversarial training to learn language invariant features from bilingual word embeddings (BWE) which were created using a parallel corpus. In fact, our propo"
2020.eamt-1.9,P10-1114,0,0.305165,"to tackle this data scarcity issue by introducing a neural architecture for language invariant sentiment analysis capable of leveraging various monolingual datasets for training without any kind of crosslingual supervision. The proposed architecture attempts to learn language agnostic sentiment features via adversarial training on multiple resource-rich languages which can then be leveraged for inferring sentiment information at a sentence level on a low resource language. Our model outperforms the current state-of-the-art methods on the Multilingual Amazon Review Text Classification dataset (Prettenhofer and Stein, 2010) and achieves significant performance gains over prior work on the low resource Sentiraama corpus (Gangula and Mamidi, 2018). A detailed analysis of our research highlights the ability of our architecture to perform significantly well in the presence of minimal amounts of training data for low resource languages. c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 1 Introduction Sentiment analysis refers to a series of methods, techniques, and tools aimed at extracting the intended sentiment from a written opinion. Tradi"
2020.eamt-1.9,D10-1103,0,0.089853,"Missing"
2020.eamt-1.9,D13-1170,0,0.0121488,"ting methods including terms’ distribution of classes, word-level polarity scoring and using SVMs (Durant and Smith, 2006) and Naive Bayes classifiers (Prasad, 2010) for pattern extraction using hand-crafted features. The advent of deep learning techniques for sentiment analysis has now enabled the extraction of high quality sentiment data from written texts. One majorly overlooked factor in the performance of these neoteric approaches is their dependency on large annotated datasets compiled from multiple data sources related to or sourced from newspapers, tweets, photos and product reviews. (Socher et al., 2013; Kim, 2014; Tai et al., 2015; Iyyer et al., 2015; Wang et al., 2016). Given global nature of the current information sharing infrastructure, most data generated belongs to one of the three languages : English, Mandarin or Spanish. This abundance of raw data aids and motivates the creation of annotated resources in these languages. Conversely, the paucity of annotated data in most languages makes it a challenging task to develop deep learning based solutions for them. Hence there is a pressing need to pay special attention to developing solutions capable of sentiment analysis in a low resource"
2020.eamt-1.9,P15-1150,0,0.0313069,"stribution of classes, word-level polarity scoring and using SVMs (Durant and Smith, 2006) and Naive Bayes classifiers (Prasad, 2010) for pattern extraction using hand-crafted features. The advent of deep learning techniques for sentiment analysis has now enabled the extraction of high quality sentiment data from written texts. One majorly overlooked factor in the performance of these neoteric approaches is their dependency on large annotated datasets compiled from multiple data sources related to or sourced from newspapers, tweets, photos and product reviews. (Socher et al., 2013; Kim, 2014; Tai et al., 2015; Iyyer et al., 2015; Wang et al., 2016). Given global nature of the current information sharing infrastructure, most data generated belongs to one of the three languages : English, Mandarin or Spanish. This abundance of raw data aids and motivates the creation of annotated resources in these languages. Conversely, the paucity of annotated data in most languages makes it a challenging task to develop deep learning based solutions for them. Hence there is a pressing need to pay special attention to developing solutions capable of sentiment analysis in a low resource setting. Some of the initial"
2020.eamt-1.9,D08-1058,0,0.0577035,"rimental setup and provides a detailed comparison of our approach with prior work in both the low-resource and no-resource setting. Section 8 addresses the advantages and shortcomings of the proposed approach and state our concluding remarks. 2 Background and Related Work CLSC using Machine Translation Systems : The most straightforward approach in CLSC involves using machine translation systems to translate sentences, words, phrases or documents in the target language to the source language and then learning a classifier in the source language to predict the sentiment (Kanayama et al., 2004; Wan, 2008; Wan, 2009; Banea et al., 2010; Lu et al., 2011; Can et al., 2018). The baseline CL-MT (Prettenhofer and Stein, 2010) method uses this technique by using Google Translate1 to translate documents in the target language to the source language and learns a classifier in the source language using the bag-of-words features. Similarly, the BiDRL model (Zhou et al., 2016) used Google Translate and employed a joint learning approach to simultaneously learn both word and document representations in both source and target language which are then used for sentiment classification. However, these methods"
2020.eamt-1.9,P09-1027,0,0.225425,"ell due to the limited overlap between the vocabularies of the different languages and difference in their syntactic structure (Chen et al., 2018b). Cross-lingual sentiment classification (CLSC) methods try to alleviate this problem by leveraging labeled data from one language to improve the performance on another language (Bel et al., 2003). However, these methods typically rely on auxiliary cross-lingual resources such as a parallel corpora (Yarowsky et al., 2001; Xu and Yang, 2017), bilingual lexicons (Mihalcea et al., 2007) or the use of machine translation systems (Kanayama et al., 2004; Wan, 2009; Prettenhofer and Stein, 2010; Can et al., 2018). Unfortunately, the curation of such cross-lingual resources is both a time and a labour intensive task. Hence, there is a need for architectures that can perform well in the absence of such cross-lingual resources. In this paper, we address this problem by presenting a neural Language Invariant Sentiment Analyzer (LISA) architecture that is capable of training on multiple monolingual sentiment labelled datasets to learn language agnostic sentiment features that can be transferred to perform sentiment analysis in low-resource languages without"
2020.eamt-1.9,P17-1130,0,0.014073,"(training a neural model on one language and applying the trained model on another language via weight sharing) do not perform well due to the limited overlap between the vocabularies of the different languages and difference in their syntactic structure (Chen et al., 2018b). Cross-lingual sentiment classification (CLSC) methods try to alleviate this problem by leveraging labeled data from one language to improve the performance on another language (Bel et al., 2003). However, these methods typically rely on auxiliary cross-lingual resources such as a parallel corpora (Yarowsky et al., 2001; Xu and Yang, 2017), bilingual lexicons (Mihalcea et al., 2007) or the use of machine translation systems (Kanayama et al., 2004; Wan, 2009; Prettenhofer and Stein, 2010; Can et al., 2018). Unfortunately, the curation of such cross-lingual resources is both a time and a labour intensive task. Hence, there is a need for architectures that can perform well in the absence of such cross-lingual resources. In this paper, we address this problem by presenting a neural Language Invariant Sentiment Analyzer (LISA) architecture that is capable of training on multiple monolingual sentiment labelled datasets to learn langu"
2020.eamt-1.9,H01-1035,0,0.0854526,"using transfer learning (training a neural model on one language and applying the trained model on another language via weight sharing) do not perform well due to the limited overlap between the vocabularies of the different languages and difference in their syntactic structure (Chen et al., 2018b). Cross-lingual sentiment classification (CLSC) methods try to alleviate this problem by leveraging labeled data from one language to improve the performance on another language (Bel et al., 2003). However, these methods typically rely on auxiliary cross-lingual resources such as a parallel corpora (Yarowsky et al., 2001; Xu and Yang, 2017), bilingual lexicons (Mihalcea et al., 2007) or the use of machine translation systems (Kanayama et al., 2004; Wan, 2009; Prettenhofer and Stein, 2010; Can et al., 2018). Unfortunately, the curation of such cross-lingual resources is both a time and a labour intensive task. Hence, there is a need for architectures that can perform well in the absence of such cross-lingual resources. In this paper, we address this problem by presenting a neural Language Invariant Sentiment Analyzer (LISA) architecture that is capable of training on multiple monolingual sentiment labelled dat"
2020.eamt-1.9,P16-1133,0,0.0359856,"Missing"
2020.figlang-1.15,P16-3016,0,0.0383394,"Missing"
2020.figlang-1.15,K16-1017,0,0.123224,"that the data is balanced with the same number of sarcastic and nonsarcastic samples (Abercrombie and Hovy, 2016). Table 1: Fields used in the training data different ways by the research community. Sarcasm detection is not wholly a linguistic problem but extra-lingual features like author and audience information, communication environment etc., also play a significant role in sarcasm identification (Bamman and Smith, 2015). Davoodi and Kosseim (2017) used semi-supervised approaches to detect sarcasm. Another approach is automatic learning and exploiting word embeddings to recognize sarcasm (Amir et al., 2016). Emojis also have a significant impact on the sarcastic nature of the text, which might help in detecting sarcasm better (Felbo et al., 2017). Other approaches to detect sarcasm include Bi-Directional Gated Recurrent Neural Network (Bi-Directional GRNU) (Zhang et al., 2016). Sarcasm detection in speech is also gaining importance (Castro et al., 2019). Some of the earlier works involving conversation contexts in detecting sarcasm are trying to model conversation contexts and understand what part of conversation sentence was involved in triggering sarcasm (Ghosh et al., 2017, 2018) and identify"
2020.figlang-1.15,W10-2914,0,0.241469,"Teradata India Pvt. Ltd, India 1 adithya.avvaru@students.iiit.ac.in 1 radhika.mamidi@iiit.ac.in 2 sanath.vobilisetty@teradata.com Abstract Sarcasm is used to criticize people, to provide political or apolitical views, to make fun of ideas, etc., and the most common form of sarcasm usage is through text. Some major sources of the sarcastic text are social media platforms like Twitter, Instagram, Facebook, Quora, WhatsApp etc. Out of these, Twitter forms the major source of sarcastic content drawing attention from researchers across the globe (Bamman and Smith, 2015; Rajadesingan et al., 2015; Davidov et al., 2010). Due to its inherent nature of flipping the context of the sentence, sarcasm in a sentence is difficult to detect even for humans (Chaudhari and Chandankhede, 2017). Here, the context is considered only in one sentence. How do we deal with situations where the sarcastic sentence depends on a conversation context and the context spans over multiple sentences preceding the response sarcastic sentence? Addressing this problem may help in identifying the root cause of sarcasm in a larger context, which is even tougher because conversation sentences differ in number, some conversation sentences th"
2020.figlang-1.15,davoodi-kosseim-2017-automatic,0,0.0134479,"Twitter tweets and Reddit posts were organized into train and test sets. The number of samples in each of these datasets is shown in Table 3. It is clear from the table that the data is balanced with the same number of sarcastic and nonsarcastic samples (Abercrombie and Hovy, 2016). Table 1: Fields used in the training data different ways by the research community. Sarcasm detection is not wholly a linguistic problem but extra-lingual features like author and audience information, communication environment etc., also play a significant role in sarcasm identification (Bamman and Smith, 2015). Davoodi and Kosseim (2017) used semi-supervised approaches to detect sarcasm. Another approach is automatic learning and exploiting word embeddings to recognize sarcasm (Amir et al., 2016). Emojis also have a significant impact on the sarcastic nature of the text, which might help in detecting sarcasm better (Felbo et al., 2017). Other approaches to detect sarcasm include Bi-Directional Gated Recurrent Neural Network (Bi-Directional GRNU) (Zhang et al., 2016). Sarcasm detection in speech is also gaining importance (Castro et al., 2019). Some of the earlier works involving conversation contexts in detecting sarcasm are"
2020.figlang-1.15,D17-1169,0,0.0801391,"e training data different ways by the research community. Sarcasm detection is not wholly a linguistic problem but extra-lingual features like author and audience information, communication environment etc., also play a significant role in sarcasm identification (Bamman and Smith, 2015). Davoodi and Kosseim (2017) used semi-supervised approaches to detect sarcasm. Another approach is automatic learning and exploiting word embeddings to recognize sarcasm (Amir et al., 2016). Emojis also have a significant impact on the sarcastic nature of the text, which might help in detecting sarcasm better (Felbo et al., 2017). Other approaches to detect sarcasm include Bi-Directional Gated Recurrent Neural Network (Bi-Directional GRNU) (Zhang et al., 2016). Sarcasm detection in speech is also gaining importance (Castro et al., 2019). Some of the earlier works involving conversation contexts in detecting sarcasm are trying to model conversation contexts and understand what part of conversation sentence was involved in triggering sarcasm (Ghosh et al., 2017, 2018) and identify the specific sentence that is sarcastic given a sarcastic post that contains multiple sentences (Ghosh et al., 2018). Humans could infer sarc"
2020.figlang-1.15,J18-4009,0,0.163888,"sation context and the context spans over multiple sentences preceding the response sarcastic sentence? Addressing this problem may help in identifying the root cause of sarcasm in a larger context, which is even tougher because conversation sentences differ in number, some conversation sentences themselves may be sarcastic and response text may depend on more than one conversation sentences. This is the research problem that we are trying to address and are largely successful in building better models which outperformed the baseline F-measures of 0.6 for Reddit and 0.67 for Twitter datasets (Ghosh et al., 2018). We have achieved Fmeasures of 0.752 for Twitter and 0.621 for Reddit datasets. Sarcasm detection, regarded as one of the subproblems of sentiment analysis, is a very typical task because the introduction of sarcastic words can flip the sentiment of the sentence itself. To date, many research works revolve around detecting sarcasm in one single sentence and there is very limited research to detect sarcasm resulting from multiple sentences. Current models used Long Short Term Memory (Hochreiter and Schmidhuber, 1997) (LSTM) variants with or without attention to detect sarcasm in conversations."
2020.figlang-1.15,W17-5523,0,0.114997,"recognize sarcasm (Amir et al., 2016). Emojis also have a significant impact on the sarcastic nature of the text, which might help in detecting sarcasm better (Felbo et al., 2017). Other approaches to detect sarcasm include Bi-Directional Gated Recurrent Neural Network (Bi-Directional GRNU) (Zhang et al., 2016). Sarcasm detection in speech is also gaining importance (Castro et al., 2019). Some of the earlier works involving conversation contexts in detecting sarcasm are trying to model conversation contexts and understand what part of conversation sentence was involved in triggering sarcasm (Ghosh et al., 2017, 2018) and identify the specific sentence that is sarcastic given a sarcastic post that contains multiple sentences (Ghosh et al., 2018). Humans could infer sarcasm better with conversation context which emphasises the importance of conversation context (Wallace et al., 2014). The structure of the paper is as follows. In Section 3, we describe the dataset (fields provided in the train and the test data and an example data along with its explanation). Section 4 describes the feature extraction where the emphasis is on data preprocessing and the procedure to select conversation sentences. Secti"
2020.figlang-1.15,P19-2052,0,0.0254343,"ated that the number of sentences in the conversation that can contribute to the sarcasm and the results agrees to this estimation. We also perform a comparative study of our different versions of BERT-based model with other variants of LSTM model and XLNet (Yang et al., 2019) (both using the estimated number of conversation sentences) and find out that BERT-based models outperformed them. 1 Introduction For many NLP researchers from both academia and industry, sarcasm detection has been one of the most focused areas of research among many research problems like code-mixed sentiment analysis (Lal et al., 2019), detection of offensive or hate speeches (Liu et al., 2019), questionanswering(Soares and Parreiras, 2018), etc. One of the main reasons why sarcasm finds a significant portion of research work is because of its nature that the addition of a sarcastic clause or a word can alter the sentiment of the sentence. 2 Related work Sarcasm is a form of figurative language where the meaning of a sentence does not hold and the interpretation is quite contrary. A quick survey about sarcasm detection and some of the earlier approaches is compiled by Joshi et al. (2017). The problem of sarcasm detection is"
2020.figlang-1.15,P19-1455,0,0.248116,"Missing"
2020.figlang-1.15,S19-2011,0,0.0268536,"an contribute to the sarcasm and the results agrees to this estimation. We also perform a comparative study of our different versions of BERT-based model with other variants of LSTM model and XLNet (Yang et al., 2019) (both using the estimated number of conversation sentences) and find out that BERT-based models outperformed them. 1 Introduction For many NLP researchers from both academia and industry, sarcasm detection has been one of the most focused areas of research among many research problems like code-mixed sentiment analysis (Lal et al., 2019), detection of offensive or hate speeches (Liu et al., 2019), questionanswering(Soares and Parreiras, 2018), etc. One of the main reasons why sarcasm finds a significant portion of research work is because of its nature that the addition of a sarcastic clause or a word can alter the sentiment of the sentence. 2 Related work Sarcasm is a form of figurative language where the meaning of a sentence does not hold and the interpretation is quite contrary. A quick survey about sarcasm detection and some of the earlier approaches is compiled by Joshi et al. (2017). The problem of sarcasm detection is targeted in 98 Proceedings of the Second Workshop on Figura"
2020.figlang-1.15,P14-2084,0,0.211902,"-Directional GRNU) (Zhang et al., 2016). Sarcasm detection in speech is also gaining importance (Castro et al., 2019). Some of the earlier works involving conversation contexts in detecting sarcasm are trying to model conversation contexts and understand what part of conversation sentence was involved in triggering sarcasm (Ghosh et al., 2017, 2018) and identify the specific sentence that is sarcastic given a sarcastic post that contains multiple sentences (Ghosh et al., 2018). Humans could infer sarcasm better with conversation context which emphasises the importance of conversation context (Wallace et al., 2014). The structure of the paper is as follows. In Section 3, we describe the dataset (fields provided in the train and the test data and an example data along with its explanation). Section 4 describes the feature extraction where the emphasis is on data preprocessing and the procedure to select conversation sentences. Section 5 describes the systems used in training the data whereas section 6 discusses the comparative results of various models. Section 7 presents concluding remarks and future direction of research. 3 Field id response context Table 2: Fields used in the testing data Datasets Twi"
2020.figlang-1.15,C16-1231,0,0.228073,"ures like author and audience information, communication environment etc., also play a significant role in sarcasm identification (Bamman and Smith, 2015). Davoodi and Kosseim (2017) used semi-supervised approaches to detect sarcasm. Another approach is automatic learning and exploiting word embeddings to recognize sarcasm (Amir et al., 2016). Emojis also have a significant impact on the sarcastic nature of the text, which might help in detecting sarcasm better (Felbo et al., 2017). Other approaches to detect sarcasm include Bi-Directional Gated Recurrent Neural Network (Bi-Directional GRNU) (Zhang et al., 2016). Sarcasm detection in speech is also gaining importance (Castro et al., 2019). Some of the earlier works involving conversation contexts in detecting sarcasm are trying to model conversation contexts and understand what part of conversation sentence was involved in triggering sarcasm (Ghosh et al., 2017, 2018) and identify the specific sentence that is sarcastic given a sarcastic post that contains multiple sentences (Ghosh et al., 2018). Humans could infer sarcasm better with conversation context which emphasises the importance of conversation context (Wallace et al., 2014). The structure of"
2020.icon-main.29,W15-3703,0,0.0282551,"mood classification of Telugu (Abburi et al., 2016) and Hindi songs (Patra et al., 2016) Nowadays, research mainly focuses on social media and very little attention is given to the traditional literature of which shayari is an important part. Automatic analysis of poetry is done for poems written in various languages like English, Chinese, Arabic, Malay, and Spanish. Barros et al. (2013) tried to categorize poems based on their emotional content. In the case of traditional literary works such as poetry, a lexicon creation methodology has been discussed for analyzing classical Chinese poetry (Hou and Frank, 2015). Hamidi et al. (2009) has also proposed a meter classification system for Persian poems based on features extracted from uttered poem. Alsharif et al. (2013) tried to classify Arabic poetry according to emotion associated with it. 3 Dataset Due to the unavailability of annotated Hindi Shayari corpus with sentiment polarity information, the dataset was constructed manually. The advent of UTF-8 encoding has led to text in Indian scripts increasing day by day on the web. We collected shayaris from numerous online sources such as: https://poetrytadka.com, https://shayarifm.com/ https://shayarilov"
2020.icon-main.29,P02-1053,0,0.0311408,"task at hand and therefore is not used in baseline experiments. Table 1 provides details on the initial statistics of the dataset before annotation. 2 4.1 Annotation Principles of Annotation Three levels of granularity are described for existing methods of sentiment analysis. So, the task of sentiment analysis can be carried out at three different levels (Liu, 2012). On the basis of the level defined, the task is to identify if positive or negative sentiment is expressed at that level. It can be done at an aspect level (Hu and Liu, 2004), sentence level or at the level of the whole document (Turney, 2002). In the case of shayaris, it is possible that the different parts of the shayaris elicit different emotions. Since the task is to identify sentiment of the shayari as a whole, annotation is carried out only at an overall document level. The annotators were asked to go through the whole shayari before tagging them. This results in the tag corresponding to the polarity of the general mood evoked by it. 4.2 Annotation Process We hired 5 annotators from different parts of India for the process of annotating the shayaris. These annotators were chosen from different regions in order to eliminate th"
2020.icon-main.36,L18-1550,0,0.0466232,"ature. The relevance of a feature xi is given by, Relevancy(xi ) = χ2 (xi ) + AN OV A(xi ) + M I(xi ) (1) Instead of an LR classifier given in the HYBRID model, we used the fastText classifier (Joulin et al., 2016) for the feature selection. We used the fastText classifier as it is often on par with deep learning classifiers in terms of accuracy and performs faster computations. The fastText classifier treats the average of word embeddings as document embeddings, then feeds document embeddings into a feed-forward NN or a multinomial LR classifier. We used pre-trained fastText word embeddings (Grave et al., 2018) while training a classifier. To get the final features list, we sorted the normalized, aggregated value in descending order and divided the entire feature space into k sets. In our model, we divided the sorted feature space into 20 sets. The value of k is fixed to 20 using a trial and error basis. We take the first set as the vocabulary of the classifier. We then trained the classifier and noted its accuracy. In the second iteration, we considered the vocabulary as the combination of first 274 1 A detailed explanation and a simple example of χ is given at https://www.mathsisfun.com/data/chi-s"
2020.icon-main.36,D14-1181,0,0.0069483,"Missing"
2020.icon-main.36,P18-1007,0,0.0228969,"2017) proposes the use of neural network-based feature selection and text classification. Our work comes under this category. 3 Proposed Pipeline In this section, we present our feature selection and neural network pipeline. The feature selection and neural network pipeline start with selecting a good tokenizer to tokenize the data and create a feature set. The tokenizer used for our feature selection is the Sentencepiece tokenizer (Kudo and Richardson, 2018). Sentencepiece tokenizer implements subword units by using byte-pair-encoding (BPE) (Sennrich et al., 2015) and unigram language model (Kudo, 2018). In the feature subset generation, we considered a hybrid feature selection method known as HYBRID (G¨unal, 2012). It has proved that a combination of the features selected by various methods is more effective and computationally faster than the features selected by individual filter and wrapper methods. Similar to the HYBRID model, we used three filters to obtain the relevancy score. The filters we considered were CHI2, ANOVA-F, and MI. These filters calculate the relevancy between the word and the class labels. Before feature selection, we used the Bag-ofWords(BoW) model to vectorize the da"
2020.icon-main.36,D18-2012,0,0.0126038,"ore applying the neural network classifiers. The paper uses a multi-layer perceptron (MLP) classifier in combination with filter-based FS method. Alkhatib et al. (2017) proposes the use of neural network-based feature selection and text classification. Our work comes under this category. 3 Proposed Pipeline In this section, we present our feature selection and neural network pipeline. The feature selection and neural network pipeline start with selecting a good tokenizer to tokenize the data and create a feature set. The tokenizer used for our feature selection is the Sentencepiece tokenizer (Kudo and Richardson, 2018). Sentencepiece tokenizer implements subword units by using byte-pair-encoding (BPE) (Sennrich et al., 2015) and unigram language model (Kudo, 2018). In the feature subset generation, we considered a hybrid feature selection method known as HYBRID (G¨unal, 2012). It has proved that a combination of the features selected by various methods is more effective and computationally faster than the features selected by individual filter and wrapper methods. Similar to the HYBRID model, we used three filters to obtain the relevancy score. The filters we considered were CHI2, ANOVA-F, and MI. These fil"
2020.icon-main.36,N16-1174,0,0.0549481,"N, RNN, BERT, and Text GCN achieve state-of-the-art results on text classification. CNN uses 1d convolutions (Zhang et al., 2015) and character level convolutions (Conneau et al., 2016) to learn the semantic similarity of words or characters, which helps in classifying the text. RNN models such as GRU, LSTM, and BiLSTM (Liu et al., 2016) take word to word sequences to learn a better textual representation of a document that helps in text classification. Attention mechanisms have been introduced in these LSTM models, which increased the representativeness of the text for better classification (Yang et al., 2016). Transformer models such as BERT (Devlin et al., 2018) uses the attention mechanism that learns contextual relations between words or sub-words in a text (Adhikari et al., 2019). Text GCN (Yao et al., 2019) uses a graph-convolutional network to learn a heterogeneous word document graph on the whole corpus. Text GCN can capture global word co-occurrence information and use graph convolutions to learn a global representation, which helps classify the documents. 2.2 Feature Selection on Text Data The text classification often involves extensive data with thousands of features. Although tens of t"
2020.icon-main.36,N16-1000,0,0.223369,"Missing"
2020.icon-main.48,W04-0502,0,0.0556762,"Missing"
2020.icon-techdofication.3,L18-1550,0,0.0163235,"odule in architecture is Self-Attention based BiLSTM classifier. We employed this selfattention (Kelvin Xu and Bengion, 2015) based BiLSTM model to extract the semantic and sentiment information from the input text data. Selfattention is an intra-attention mechanism in which a softmax function gives each subword’s weights in the sentence. The outcome of this module is a weighted sum of hidden representations at each subword. The self-attention mechanism is built on BiLSTMs architecture (See figure 3), and it takes input as pre-trained embeddings of the subwords. We passed the Telugu fasttext (Grave et al., 2018) subword embeddings to a BiLSTM layer to get hidden representation at each timestep, which is the input to the self-attention component. Suppose the input sentence S is given by the → − subwords (w1 , w2 , ..., wn ). Let h represents the ← − forward hidden state and h represents the backward hidden state at ith position in BiLSTM. The merged representation ki is obtained by combining exp(ei ) ai = Pn j=1 exp(ej ) (3) Finally, we compute the sentence S latent representation vector h using below equation h= aX i ×ki (4) i=1 The latent representation vector h is fed to a fully connected layer fol"
2020.icon-techdofication.4,D14-1181,0,0.0134749,"Missing"
2020.lrec-1.339,bhattacharyya-2010-indowordnet,0,0.0356016,"in News domain. News data may have opinionated references along with factual data. Hence at a sentence level these can be classified into positive, negative and neutral categories. Moreover, sentence level sentiment analysis provides room for usage of a sentiment lexicon for identifying affective words. We built an Odia sentiment lexicon for the task of sentiment classification previously (Mohanty et al., 2017). This lexicon has been built by using resources available for three other Indian languages: Bengali, Telugu, Tamil (Das and Bandyopadhyay, 2010) which are similar to Odia. IndoWordNet (Bhattacharyya, 2010) was used for establishing language pairs between Odia and each of the three aforementioned Indian languages. Classification performance using the Odia sentiment lexicon should provide valuable insight on the usability of this sentiment lexicon. We have created an annotated corpus of Odia sentences from the abundantly available data in news domain for the language. This has further been made publicly available to promote research in the field. Secondly, in order to test the usability the already present Odia sentiment lexicon, we experimented with various classifiers by training and testing on"
2020.lrec-1.339,W10-3208,0,0.0426052,"this paper, we create a sentiment annotated corpus of Odia sentences in News domain. News data may have opinionated references along with factual data. Hence at a sentence level these can be classified into positive, negative and neutral categories. Moreover, sentence level sentiment analysis provides room for usage of a sentiment lexicon for identifying affective words. We built an Odia sentiment lexicon for the task of sentiment classification previously (Mohanty et al., 2017). This lexicon has been built by using resources available for three other Indian languages: Bengali, Telugu, Tamil (Das and Bandyopadhyay, 2010) which are similar to Odia. IndoWordNet (Bhattacharyya, 2010) was used for establishing language pairs between Odia and each of the three aforementioned Indian languages. Classification performance using the Odia sentiment lexicon should provide valuable insight on the usability of this sentiment lexicon. We have created an annotated corpus of Odia sentences from the abundantly available data in news domain for the language. This has further been made publicly available to promote research in the field. Secondly, in order to test the usability the already present Odia sentiment lexicon, we exp"
2020.lrec-1.339,W06-2915,0,0.0495786,"ree and two wickets respectively. • ଏଥିସହିତ ଏହି ଘଟଣାେର ବୟବହୃତ ହିେରାହୁ∞ା ପୟାସନ େମାଟରସାଇେକଲକୁ ମଧୟ ଜବତ କରାଯାଇଛି Transliteration: Ethi sahitha ehi ghatanaare byabahrutha herohondaa passion motorcycle ku madhya jabath karaa jaayichi. Affective Words: ଜବତ English: Alongside this, the used hero-honda passion motorcycle has also been ceased. • Author’s point of view - It is important focus on the language used by the author of the article. The language used, gives insight on the point of view of the writer of the sentence. This further contributes to the sentiment associated with a given sentence (Lin et al., 2006). Understanding the sentence from the author’s perspective based on the language (e.g - usage of affective words) used by the author should help determine the sentiment of a given sentence. • ଅେ∞ରଲି ଅା ୬୫େର ଅଲ ଅାଉଟ ଭାରତ ୨୪୩ ରନ େର ବିଜୟୀ Transliteration: Affective Words: ବିଜୟ English: Australia : All-out in 65 runs. India win by 24 runs. • Annotator’s point of view - Every annotator has their own pre-conditioned biases associated with certain sentences based on social, cultural and economic conditions. For example, a sentence describing the English cricket team’s victory over India could invo"
2020.lrec-1.339,W17-5219,1,0.856071,"public use. However, we have attempted to create an annotated corpus for Odia poetry, previously in literature (Mohanty et al., 2018). In this paper, we create a sentiment annotated corpus of Odia sentences in News domain. News data may have opinionated references along with factual data. Hence at a sentence level these can be classified into positive, negative and neutral categories. Moreover, sentence level sentiment analysis provides room for usage of a sentiment lexicon for identifying affective words. We built an Odia sentiment lexicon for the task of sentiment classification previously (Mohanty et al., 2017). This lexicon has been built by using resources available for three other Indian languages: Bengali, Telugu, Tamil (Das and Bandyopadhyay, 2010) which are similar to Odia. IndoWordNet (Bhattacharyya, 2010) was used for establishing language pairs between Odia and each of the three aforementioned Indian languages. Classification performance using the Odia sentiment lexicon should provide valuable insight on the usability of this sentiment lexicon. We have created an annotated corpus of Odia sentences from the abundantly available data in news domain for the language. This has further been made"
2020.lrec-1.617,L16-1429,0,0.0914985,"Missing"
2020.lrec-1.617,Q17-1010,0,0.0325884,"trained word embeddings available for Telugu which were obtained by running word2vec on large corpus 1 . These embeddings are used to initialize the embedding layer. 4.2.7. Interactive Attention Networks (IAN) IAN (Ma et al., 2017) models aspect and context interactively. It uses aspects’ hidden states and context’s hidden states to generate supervised attention vectors and captures important information from aspect and context. With this design, aspect and context influence the prediction interactively. • Fasttext Embeddings: We initialize the embedding layer using fasttext word embeddings. (Bojanowski et al., 2017) 5.1. 4.2.8. Deep Memory Networks (Tang et al., 2016) Deep Memory Networks capture the importance of each context word when referring to the classification of an aspect. Text representation and degree of importance are calculated using the neural attention model over an external memory. Figure 9 shows the architecture of the Deep Memory Network model. Softmax hop 1 ∑ Attention Linear Model configuration and training 5.1.1. Aspect Term Identification We tokenized each sentence into a list of words. We only retain those words appearing more than 5 times in building vocabulary. The hyperparameter"
2020.lrec-1.617,L18-1100,1,0.818418,"earning methods. But resource creation tasks like (Pontiki et al., 2014) and (Pontiki et al., 2016) are key for the progress of ABSA. Lack of reliable resources of this kind restricts Indian Languages from research developments in ABSA. There have been some developments recently in Hindi. In (Akhtar et 5017 al., 2016), a dataset for Hindi was created and in (Akhtar et al., 2018), an approach was developed for ABSA in Hindi using the created dataset. However, in Telugu, there is no dataset available for ABSA. In Telugu, (Abburi et al., 2017) has a corpus of song lyrics for sentiment analysis. (Gangula and Mamidi, 2018) has several product, book and movie reviews annotated at document level. (Mukku et al., 2016) and (Mukku and Mamidi, 2017) have annotated data for sentiment analysis at sentence level. But sentiment analysis at aspect level remains unexplored in Telugu. In this paper, we created a dataset for aspect based sentiment analysis in Telugu. 3. Data Creation and Annotation In this section, we gave a detailed description of dataset creation and the challenges faced. In this section, we use the transliterated form of Telugu along with their English translations to list out the examples. It is to ensur"
2020.lrec-1.617,L18-1473,0,0.0654091,"Missing"
2020.lrec-1.617,W17-5408,1,0.646695,"of ABSA. Lack of reliable resources of this kind restricts Indian Languages from research developments in ABSA. There have been some developments recently in Hindi. In (Akhtar et 5017 al., 2016), a dataset for Hindi was created and in (Akhtar et al., 2018), an approach was developed for ABSA in Hindi using the created dataset. However, in Telugu, there is no dataset available for ABSA. In Telugu, (Abburi et al., 2017) has a corpus of song lyrics for sentiment analysis. (Gangula and Mamidi, 2018) has several product, book and movie reviews annotated at document level. (Mukku et al., 2016) and (Mukku and Mamidi, 2017) have annotated data for sentiment analysis at sentence level. But sentiment analysis at aspect level remains unexplored in Telugu. In this paper, we created a dataset for aspect based sentiment analysis in Telugu. 3. Data Creation and Annotation In this section, we gave a detailed description of dataset creation and the challenges faced. In this section, we use the transliterated form of Telugu along with their English translations to list out the examples. It is to ensure better readability. However, the original dataset is in Telugu Script. 3.1. Data Scraping and Cleaning We crawled several"
2020.lrec-1.617,S14-2004,0,0.149925,"Missing"
2020.lrec-1.617,S16-1002,0,0.1463,"Missing"
2020.lrec-1.617,J11-1002,0,0.0510964,"nvolved in it. In section 4, we discussed about the methods we used to tackle ABSA and in section 5, we gave the details of experiments performed and analysed the results obtained and we finally concluded in section 6. 2. Related Work Research in sentiment analysis has evolved a lot over the years. Many developments have been made since customer reviews were analysed in (Hu and Liu, 2004). ABSA is one of its branches which gained momentum in recent years due to its analysis at a finer level. Deep learning advancements led to improvements in ABSA. Some of the works in ABSA in recent times are (Qiu et al., 2011), (Tang et al., 2016), (Cheng et al., 2017), (Xue and Li, 2018). Most of these works use deep learning methods. But resource creation tasks like (Pontiki et al., 2014) and (Pontiki et al., 2016) are key for the progress of ABSA. Lack of reliable resources of this kind restricts Indian Languages from research developments in ABSA. There have been some developments recently in Hindi. In (Akhtar et 5017 al., 2016), a dataset for Hindi was created and in (Akhtar et al., 2018), an approach was developed for ABSA in Hindi using the created dataset. However, in Telugu, there is no dataset available f"
2020.lrec-1.617,D16-1021,0,0.144099,"ction 4, we discussed about the methods we used to tackle ABSA and in section 5, we gave the details of experiments performed and analysed the results obtained and we finally concluded in section 6. 2. Related Work Research in sentiment analysis has evolved a lot over the years. Many developments have been made since customer reviews were analysed in (Hu and Liu, 2004). ABSA is one of its branches which gained momentum in recent years due to its analysis at a finer level. Deep learning advancements led to improvements in ABSA. Some of the works in ABSA in recent times are (Qiu et al., 2011), (Tang et al., 2016), (Cheng et al., 2017), (Xue and Li, 2018). Most of these works use deep learning methods. But resource creation tasks like (Pontiki et al., 2014) and (Pontiki et al., 2016) are key for the progress of ABSA. Lack of reliable resources of this kind restricts Indian Languages from research developments in ABSA. There have been some developments recently in Hindi. In (Akhtar et 5017 al., 2016), a dataset for Hindi was created and in (Akhtar et al., 2018), an approach was developed for ABSA in Hindi using the created dataset. However, in Telugu, there is no dataset available for ABSA. In Telugu, ("
2020.lrec-1.617,D16-1058,0,0.0870505,"Missing"
2020.lrec-1.617,P18-1234,0,0.0135954,"used to tackle ABSA and in section 5, we gave the details of experiments performed and analysed the results obtained and we finally concluded in section 6. 2. Related Work Research in sentiment analysis has evolved a lot over the years. Many developments have been made since customer reviews were analysed in (Hu and Liu, 2004). ABSA is one of its branches which gained momentum in recent years due to its analysis at a finer level. Deep learning advancements led to improvements in ABSA. Some of the works in ABSA in recent times are (Qiu et al., 2011), (Tang et al., 2016), (Cheng et al., 2017), (Xue and Li, 2018). Most of these works use deep learning methods. But resource creation tasks like (Pontiki et al., 2014) and (Pontiki et al., 2016) are key for the progress of ABSA. Lack of reliable resources of this kind restricts Indian Languages from research developments in ABSA. There have been some developments recently in Hindi. In (Akhtar et 5017 al., 2016), a dataset for Hindi was created and in (Akhtar et al., 2018), an approach was developed for ABSA in Hindi using the created dataset. However, in Telugu, there is no dataset available for ABSA. In Telugu, (Abburi et al., 2017) has a corpus of song"
2020.semeval-1.147,C10-2028,0,0.164131,"Missing"
2020.semeval-1.147,D14-1162,0,0.0822986,"Missing"
2020.semeval-1.147,P14-1109,0,0.0720043,"Missing"
2020.semeval-1.147,P14-1146,0,0.113187,"Missing"
2020.semeval-1.166,esuli-sebastiani-2006-sentiwordnet,0,0.0126194,"Missing"
2020.semeval-1.166,W14-5124,0,0.0660044,"Missing"
2020.semeval-1.166,C16-1234,0,0.0201736,"entiment analysis of English as well as Hinglish using deep neural networks. M.G. Jhanwar, A. Das (2018) proposed an ensemble of character-trigrams based LSTM model and a n-grams based Multinomial Naive Bayes model to classify the sentiments of Hinglish code-mixed data. Shalini K, Barathi Ganesh HB et al. (2018) addressed the performance of distributed representation methods in sentiment analysis and reported comparisons among different machine learning and deep learning techniques. Other attempts include using sub-word level compositions with LSTMs to capture sentiment at the morpheme level (Joshi et al., 2016). We attempted to perform SemEval 2020 task-9 (Patwa et al., 2020) with various classification and deep learning models to analyze the results and also how such models contributed to a great advance in this task. 3 Dataset In this paper, we used the dataset provided by Task 3: SentiMix in SemEval 2020. The corpus contains a total of 20000 tweets and it is sub-divided into three sets (train, validation and test). Each corpus except test contains code-mixed tweets along with their corresponding sentiment labels. These code-mixed tweets are tokenized into tokens. And the tokens of each tweet are"
2020.semeval-1.166,D14-1162,0,0.08745,"with tf-idf weighted averaging. The code-mixed tweet vector construction scheme is described below: PN f eatureV ectortweet = × Glove(tokeni ) total no. of tokens in tweet (N ) n=1 tf-idf(tokeni ) Empirically, we found that standard averaging of Glove and tf-idf gave better results than normal tf-idf weighted averaging. 4.3 Deep Neural Networks In this subsection, we describe the character and word embedding based deep neural network called “Syntactic and Semantic LSTM (SS-LSTM)” that gave better predictions on our corpus. Initially, We tried with Word2Vec (T. Mikolov et al., 2013), GloVe (J. Pennington et al., 2014), FastText (Bojanowski et al., 2016), Character embeddings for each word in the input code-mixed tweet. We train a simple LSTM model using each of these embeddings to test the effectiveness of these embeddings for sentiment classification. FastText and Character level embeddings gave slightly better results than other embeddings. By considering these results we modeled the SS-LSTM architecture given below. 4.3.1 Character Level Embeddings Character level embeddings use a one-dimensional convolutional neural network (1D-CNN) to find the numeric representation of words by looking at their charac"
2020.semeval-1.166,D14-1181,0,0.010875,"Missing"
2020.socialnlp-1.1,W19-4809,1,0.920812,"edings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1–6 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 presupposition.” On the other hand, there can be some presuppositions which do not contain any lexical triggers. Stalnaker (1974) defines them as Conversational Presuppositions. He suggests that they are the “inferences which are licensed by general conversational principles, in combination with the truth conditions of the presupposing utterance”. 2 election campaign speeches. Gangula et al. (2019) proposed an attention mechanism to detection of bias in Telugu news articles. To our knowledge there has been no work on presupposition in Telugu till date. Our research is the first of its kind which proposes guidelines to identify presuppositions in political news and use that information to enhance the computational methods to detect bias in political news articles. Related Work Though there have been several speculations in the linguistic research community about the extra linguistic information provided by the use of presuppositions, very few of them are backed up with proper surveys and"
2020.socialnlp-1.1,Y18-1028,1,0.636968,"cles. He manually analysed how presuppositions can contribute in differentiating between the styles of reporting in the pro-government and anti-government stance of the newspaper. However, none of these studies have tested the validity of their claims on a large corpus and no computational work has been done in this domain so far. Moreover, all of the above research was carried out for English news, and there has been little work on Politics and News discourse in Telugu, which is a low resource language. Mukku et al. (2016) applied ML techniques for Sentiment Analysis of Telugu news articles. Kameswari and Mamidi (2018) carried out a case study on political influence through linguistic choices on a corpus of 3 Corpus Creation and Annotation To validate our idea computationally, we need a large dataset of news articles which have been annotated for their bias and magnitude of presupposition. There is no such dataset which captures both the features, so we took the corpus1 created by Gangula et al. (2019) and modified it. It consists of 1329 articles collected from various newspapers in Telugu, a Dravidian language spoken widely in Telangana and Andhra Pradesh in India. Each article was annotated with a label"
2021.acl-srw.12,W17-0811,0,0.0270017,"n Figure 1. Also in computer science, string similarity is an important family of algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string. Researchers have already put the efforts and showed that these algorithms effectively calculate the similarity between two strings (Levenshtein, 1965; Yujian and Bo, 2007; Masek and Paterson, 1980; Larsen, 1992; Kondrak, 2005). Some studies have also been done on calculating similarity particularly for Indian languages (Singh and Surana, 2007; Wagner and Fischer, 1974; Islam and Inkpen, 2008; Akhtar et al., 2017; Sengupta and Saha, 2015). In this work, we will consider sentences as a string and use some of the above algorithms for calculating the similarity between two languages. 2.1 Token Overlap This is the most general approach that works by converting strings into sets of their tokens and then counting the number of tokens which are shared between the both sets. Similarity between two languages using token overlap is calculated as follows: • Collect parallel data of languages of which we want to calculate similarity. • Transliterate those languages to a common 113 |T okens1 ∩T okens2 | 1 max(|T o"
2021.acl-srw.12,N16-4006,0,0.0287455,"tacharyya (2020) also presents an impressive case study for utilizing language relatedness for Machine translation but that study was more inclined toward exploring statistical approaches to MT. Prasanna (2018) in his work has explored efficient ways of exploiting relatedness in multilingualism and transfer learning for low resource machine translation. India is one of the most linguistically diverse nations of the world and is culturally very rich. Most of these languages are somewhat similar to each other on account of sharing a common ancestry or being in contact for a long period of time (Bhattacharyya et al., 2016). Nowadays, researchers are constantly putting efforts in utilizing the language relatedness to improve the performance of various NLP systems such as cross lingual semantic search, machine translation (Kunchukuttan and Bhattacharyya, 2020), sentiment analysis systems, etc. So in this paper, we performed an extensive case study on similarity involving languages of the Indian subcontinent. Language similarity prediction is defined as the task of measuring how similar the two languages are on the basis of their lexical, morphological and syntactic features. In this study, we concentrate only on"
2021.acl-srw.12,2020.sustainlp-1.4,0,0.0163521,"ate, NLP tasks become challenging for low resource languages like Indian languages. India is a multicultural country, a country with highly religious and ethnically diverse people. People of different races and classes live in different parts of the country, and they speak a variety of languages. Most of the Indian languages are divided into two main language families namely Indo-Aryan1 and Dravidian2 . Underlying the vast diversity in Indian languages are many commonalities. Because of contact over thousands of years, most of the Indian languages have undergone convergence to a large extent (Shridhar et al., 2020). Therefore, exploiting language relatedness becomes very crucial in NLP related tasks for Indian languages. Kunchukuttan and Bhattacharyya (2020) also presents an impressive case study for utilizing language relatedness for Machine translation but that study was more inclined toward exploring statistical approaches to MT. Prasanna (2018) in his work has explored efficient ways of exploiting relatedness in multilingualism and transfer learning for low resource machine translation. India is one of the most linguistically diverse nations of the world and is culturally very rich. Most of these la"
2021.acl-srw.12,D07-1003,0,0.00776808,"ogical and syntactic features. In this study, we concentrate only on the approach to calculate lexical similarity between Indian languages by looking at various factors such as size and type of corpus, similarity algorithms, subword segmentation, etc. The main takeaways from our work are: (i) Relative order of the language similarities largely remain the same, regardless of the factors mentioned above, (ii) Similarity within the same language family is higher, (iii) Languages share more lexical features at the subword level. 1 Introduction Recently, there has been an explosion in information (Wang et al., 2007) and a massive amount of natural language data is added daily on the Internet. Moreover, the human literature in different cultures is digitalized and became available in digital libraries (Farouk, 2019). A very large amount of this data is formatted in natural language. This makes NLP techniques crucial to make the use of this high amount of data. Since most of the NLP techniques either require linguistic knowledge But no such large scale study has been done on exploring different factors that may affect the process of calculating similarity among Indian languages. This could really help the"
2021.acl-srw.12,jha-2010-tdil,0,0.0339313,"am) Similarity This works by converting strings into sets of qgrams (sequences of q characters, also sometimes called k-shingles ) Kondrak (2005). The similarity or distance between the two strings is then the similarity or distance between the sets. Here we are using Jaccard index as our similarity technique which is a special case of shingle based algorithms. We can compute similarity using Jaccard between two languages as follows: Pn simqgram = 3 1 qgram(s1, s2) ∗ 100 n Experiments For our case study, we are performing all the experiments using the ILCI (Indian Language Corpora Initiative) Jha (2010) and PMI (Prime Minister of 114 Case 1: In this case, we are evaluating the effect of algorithm used for calculating sentence similarity on the similarity among the language pairs. We are computing the similarity for every language pair present in our ILCI corpora using each algorithm mentioned in subsections 2. Also, as per the requirement of our pipeline, we are also mapping each language to Devanagari script to share the same surface form. Case 2: Here, we are performing the experiments to confirm whether the choice of script selection matters in transliteration step of our pipeline for cal"
2021.calcs-1.1,P19-2025,0,0.0270539,"in English-Hindi. They tried to differentiate between borrowing and code-mixing based on the frequency of co-occurrence of words in tweets. 1. Public Meetings in Telangana: KCR’s speech was during the Telangana movement, meant for the creation of a new state. He addressed the pathetic situation of Telangana residents and also discussed the plan and policies for the new state.CBN’s speech is during the Telangana elections in 2018. The audience were residents of Telangana. We will refer to this as communicative event 1. In Dravidian languages, there is very little work done in this area so far. Srirangam et al. (2019) created a corpus for Named Entity recognition in English-Telugu code mixed tweets,Jitta et al. (2017) created a English-Telugu code mixed conversational data for Dialog Act recognition. There has been very less work on analysing the Telangana dialect. Bhaskar wrote a book named Telanagana Padakosam with Telangana words and their corresponding words in Standard dialect. Also, he has drawn few observations that are common in Telangana dialect.Chakravarthy (2016), An Annotated Translation of Kalarekhalu A Historical Novel by Ampasayya Naveen, describes the important phases that lead to the Telan"
2021.calcs-1.1,Y18-1028,1,0.744097,"r Chandra Babu Naidu(CBN), former Chief Minister of Andhra Pradesh. KCR is the founder of the Telangana Rashtra Samiti (TRS) party and is widely regarded as the face of the Telangana movement for a separate state in 2014. CBN is the leader of the Telugu Desam Party. They use a variety of dialects and languages such as Telangana Telugu, Modern Standard Telugu, Urdu, English and Hindi in their speeches. We chose a total of 6 speeches of both the speakers in three different social settings and communicative contexts to analyse the levels of code-mixing and code-switching as follows: Related Work Kameswari and Mamidi (2018) conducted a study about various interpersonal speech choices in election campaign speeches, including the usage patterns of nouns, pronouns, kinship terms, rhetorical questions, etc. There are a few more studies (Martinez Guillem (2009), Ilic and Radulovic (2015), Kampf and Katriel (2016)) which analyse the deeper intention behind the choice of words and phrases using the famous Speech Act theory by Searle et al. (1980) and the Sociocognitive model by Van Dijk (2014). There has been some work recently on CM and CS involving Indian languages. But most of the work is done in the social media do"
2021.calcs-1.4,W18-3219,0,0.0218979,"Missing"
2021.calcs-1.4,W14-3914,0,0.0122148,"nce to Sequence Based Learning for English-Hingilsh Code-Switched Machine Translation. Radhika Mamidi LTRC IIIT-Hyderabad radhika.mamidi @iiit.ac.in Suman Dowlagar LTRC IIIT-Hyderabad suman.dowlagar @research.iiit.ac.in Abstract There are no standard grammar rules that are meant to be practiced in the code-switched text. The code-switched data often contain variations of spellings and grammar. The computational processing of code-mixed or code-switched data is challenging due to the nature of the mixing and the presence of non-standard variations in spellings and grammar, and transliteration (Bali et al., 2014). Because of such linguistic complexities, code-switching poses several unseen difficulties in fundamental fields of natural language processing (NLP) tasks such as language identification, part-of-speech tagging, shallow parsing, Named entity recognition, sentiment analysis, offensive language identification etc. Code-Switching is the embedding of linguistic units or phrases from two or more languages in a single sentence. This phenomenon is practiced in all multilingual communities and is prominent in social media. Consequently, there is a growing need to understand codeswitched translations"
2021.calcs-1.4,W18-3817,0,0.0342683,"nslation of the codemixed text a complex task. There is relatively less research in the field of the machine translation of the code-switched text, partially due to the relative lack of structured corpora and also potentially because it also poses significant linguistic challenges such as ambiguity in language identification, spelling variations, informal style of writing, Misplaced/skipped punctuation, etc. Nonetheless, some researchers have provided datasets to enable research in code-mixed machine translation, specifically in Hindi-English codeswitched scenario (Srivastava and Singh, 2020; Dhar et al., 2018). Dhar et al. (2018) presented a parallel corpus of the 13,738 code-mixed HindiEnglish sentences and their corresponding human translation in English. In addition, they also provided a translation pipeline built on top of Google Translate. The pipeline fragments the input sentence into multiple chunks and identifies the language of each word in the chunk before feeding it to google-translate. The pipeline gives a BLEU1 metric of 0.153 on the given English dataset. Dhar et al. (2018) translated the 6,096 code-mixed English-Hindi sentences into English and presented a translation augmentation pi"
2021.calcs-1.4,2021.dravidianlangtech-1.8,1,0.702151,"of language interaction/contact is considered to be an essential phenomenon, especially in multilingual societies. In bilingual or multilingual communities, speakers use their native tongue and their second language during interactions. This form of alternation of two or more languages is called Code-Switching (CS) (Muysken et al., 2000). Through the advent of social media, people from around the world can connect and exchange information instantly. Users from a Multilingual community often express their thoughts or opinions on social media by mixing different languages in the same utterance (Dowlagar and Mamidi, 2021). This mixing or alteration of two or more languages is known as code-mixing or code-switching (Wardhaugh, 2011). This paper presents a gated convolutional sequence to sequence encoder and decoder models (Gehring et al., 2017) for machine translation on the code-mixed text. We have used the convolutional model because of its sliding window concept to deal with contextual words and the convolutions to extract rich representations. The paper is organized as follows. Section 2 provides related work on the code-switched text for machine translation. Section 3 provides information on the task and d"
2021.calcs-1.4,P02-1040,0,0.111673,"all three consecutive tokens at a time. These filters will help extract different features in the given text and aid the machine translation model. The description of the encoder and decoder convolutional models is given in the subsequent subsections. 4.1 Here, we demonstrate the performance of the machine translation systems on the code-mixed text. We experiment with the popular RNN based encoder-decoder machine translation and vanilla transformer models and evaluate their performance on the given English-Hinglish machine translation task. We use BLEU metrics to evaluate system performance (Papineni et al., 2002). Encoder In the encoder model, each token in the source sentence is passed through an embedding layer. As the convolutional model has no recurrent connections, the model has no idea about the order of the tokens within a sequence. So it is necessary to add the positional embedding layer. In the positional embedding, the position of the tokens, including the start of the sequence and the end of the sequence, are encoded. Next, the token and positional embeddings are combined by elementwise sum. The obtained embedding vector contains the token and also its position within the sequence. The give"
2021.calcs-1.4,2020.wnut-1.7,0,0.0131491,"making the research and translation of the codemixed text a complex task. There is relatively less research in the field of the machine translation of the code-switched text, partially due to the relative lack of structured corpora and also potentially because it also poses significant linguistic challenges such as ambiguity in language identification, spelling variations, informal style of writing, Misplaced/skipped punctuation, etc. Nonetheless, some researchers have provided datasets to enable research in code-mixed machine translation, specifically in Hindi-English codeswitched scenario (Srivastava and Singh, 2020; Dhar et al., 2018). Dhar et al. (2018) presented a parallel corpus of the 13,738 code-mixed HindiEnglish sentences and their corresponding human translation in English. In addition, they also provided a translation pipeline built on top of Google Translate. The pipeline fragments the input sentence into multiple chunks and identifies the language of each word in the chunk before feeding it to google-translate. The pipeline gives a BLEU1 metric of 0.153 on the given English dataset. Dhar et al. (2018) translated the 6,096 code-mixed English-Hindi sentences into English and presented a transla"
2021.codi-main.2,Q17-1010,0,0.00625773,"orhood in its understanding. (2019) experimented with 2 layers of GCN and observed that further stacking of layers did not improve performance. After experimenting, the first layer’s embedding size was limited to 200, whereas the window size was set to 20, along with learning rate as 0.02 and dropout rate as 0.5. The training and test data were split into 80:20 ratio and the number of training epochs was stopped early if after 10 successive epochs the validation loss did not decrease. 307 6 321 6.1 FastText Word Embeddings FastText uses sub-word information to capture a word’s representation (Bojanowski et al., 2017). A word is split into character n-grams, and these collectively make up the embedding for a word. FastText is highly advantageous for an agglutinative language such as Telugu (Srinivasu and Manivannan, 2018). In an agglutinative language, a lexeme is attached with suffixes which carry information such as gender, number (singular/plural) or tense. For instance, the word below is made up of smaller morphemes. intikochindhannamaata Morphemes that make up the word: illu + ki + ochuta + i + anamata + emphasis 15 295 308 309 310 311 312 313 314 315 316 317 318 319 320 322 323 324 325 326 327 328 32"
2021.codi-main.2,P18-2093,0,0.0140763,"(such as light-bulb jokes, knock-knock jokes) are a part of verbal humor, where they are not contextdependent, i.e., they can be removed from a conversation and still perceived as humorous. On the other hand, CH is heavily dependent on various factors including speakers’ personalities, the relevant culture that is referenced, and current events. Numerous studies that focus on the detection of humor 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 Introduction in short jokes/tweets rely on the contrastive discourse relation present in humorous instances (Liu et al., 2018). However, in conversations, participants’ personalities, their sense of humor, and the relationship between the participants, add unique complexities to the task of detection of CH. The following example (translated) from a Telugu stage play, Kanyasulkam. 042 Puta: Where is he? Madhu: You don’t listen to me, I have already told you that the person you’re looking for is not here! 050 Context: Giri and Rama are hiding under Madhu’s bed. Puta is looking for Giri and the latter is hiding because he is scared of Puta. The audience, Giri and Rama know that Madhu is lying about Giri’s whereabouts. T"
2021.codi-main.2,H05-1067,0,0.132368,"Every tweet is assigned several attributes such as tweet ID, user ID, timestamp, etc. Using a tweet’s ID, conversation ID and timestamp, tweets and their replies were compiled to form a conversation. A combination of Twint4 , an advanced Twitter scraping tool, and Twitter’s API v2 endpoint5 were used to assemble the data. To avoid the non-canonical forms of words obtained from scraping Telugu tweets written in roman script, ‘lang: te’ (aiming to fetch tweets written in Telugu script) was used as a filter when scraping tweets. In addition, in order to build a multidomain non-humorous dataset (Mihalcea and Strapparava, 2005), hashtags such as ‘#cinema’, ‘#politics’, ‘#cooking’, etc. were used as filters when collecting tweets. Usernames were not used as speakers in the conversation for two reasons. Firstly, for the sake of anonymity, and secondly, usernames and names contain numbers and special characters. Consequently, to generate a natural conversation, the usernames were replaced by common Telugu names. Using the tweet’s author ID, speaker identity was preserved. For instance, User123: Hello, how is today’s weather? User456: It seems to be very sunny. User123: Oh, wonderful! 212 In both instances, ‘User123’ is"
2021.codi-main.2,2020.law-1.4,1,0.725369,"the relationship between the participants, add unique complexities to the task of detection of CH. The following example (translated) from a Telugu stage play, Kanyasulkam. 042 Puta: Where is he? Madhu: You don’t listen to me, I have already told you that the person you’re looking for is not here! 050 Context: Giri and Rama are hiding under Madhu’s bed. Puta is looking for Giri and the latter is hiding because he is scared of Puta. The audience, Giri and Rama know that Madhu is lying about Giri’s whereabouts. This irony is recognized by the audience and causes Dramatic Irony (Dempster, 1932)(Pamulapati et al., 2020). This depicts how the participants contribute to CH. There have been attempts at formulating a typology for CH. Dynel (2009) defined CH and enlisted the different types of verbal discourses that cause CH. Similarly, Pamulapati et. al (2020) developed a hierarchical framework that considers whether it is a monologue/dialogue, the benignity of the utterance, the type, and techniques used to cause CH. Despite the recent interest in CH, there is a need for more computational work in this field as researchers are striving to make virtual assistants like Siri and Alexa produce more human-like disco"
2021.codi-main.2,P18-1041,0,0.0168253,"does not capture word relations well. In comparison, FastText’s nearest neighbors defined in the pre-trained model for the word ‘aNxulO’ (translates to ‘in that’) are shown in Table 2. By inspecting the nearest neighbors of the queried word, FastText captures syntactic (plural of ‘in that’ is ‘in those’) and semantic relations (antonym of ‘in that’ is ‘in this’). This highlights the importance of word embeddings for the model’s overall performance for the task to be carried out. Word representations is a key aspect that contributes to the effectiveness and performance of text classification (Shen et al., 2018)(Wang et al., 2018). As mentioned in Section 3.3, conversational text written in Roman script was transliterated to Telugu script using Google Transliterate API. The API produces an array of most probable transliterations of the input given, after which the first element is considered by default. However, at times, the API does not produce accurate results with a slight margin of error. Thus, potentially producing word(s) that do not exist in the language’s vocabulary. For instance, if the API produces the word ‘rANiMcina’, when it should correctly be ‘rAniMcina’ (an error difference of one ch"
2021.codi-main.2,L18-1193,0,0.0606884,"Missing"
2021.codi-main.2,P18-1216,0,0.0119322,"ord relations well. In comparison, FastText’s nearest neighbors defined in the pre-trained model for the word ‘aNxulO’ (translates to ‘in that’) are shown in Table 2. By inspecting the nearest neighbors of the queried word, FastText captures syntactic (plural of ‘in that’ is ‘in those’) and semantic relations (antonym of ‘in that’ is ‘in this’). This highlights the importance of word embeddings for the model’s overall performance for the task to be carried out. Word representations is a key aspect that contributes to the effectiveness and performance of text classification (Shen et al., 2018)(Wang et al., 2018). As mentioned in Section 3.3, conversational text written in Roman script was transliterated to Telugu script using Google Transliterate API. The API produces an array of most probable transliterations of the input given, after which the first element is considered by default. However, at times, the API does not produce accurate results with a slight margin of error. Thus, potentially producing word(s) that do not exist in the language’s vocabulary. For instance, if the API produces the word ‘rANiMcina’, when it should correctly be ‘rAniMcina’ (an error difference of one character) and Text G"
2021.dialdoc-1.4,W17-5038,0,0.0598317,"Missing"
2021.dialdoc-1.4,W06-1908,0,0.186196,"Missing"
2021.dravidianlangtech-1.19,W14-3914,0,0.0133423,"already begun to address hate speech and offensive content and how it affects society’s functioning (Chakravarthi, 2020). Research and technologies worldwide are utilizing natural language and machine learning tools to detect and curb the use of offensive content on social media. In a multilingual society, code-mixing has become a norm. The hateful and offensive content is delivered in a code-mixed form (Jose et al., 2020; Priyadharshini et al., 2020). Automatic hate speech detection on code-mixed data has faced quite many challenges due to the non-standard variations in spelling and grammar (Bali et al., 2014). The typical hate-speech and offensive language tools developed for monolingual data will not work for codemixed data. So there is a need for more research and analysis to be done to identify the offensive content in code-mixed social media data. To encourage research on code-mixing and restrain the use of offensive texts on social media, the NLP community has organized several workshops such as Workshops on Computational Approaches to Linguistic Code-Switching, SentiMix (Patwa et al., 2020), Dravidian CodeMix (Chakravarthi et al., 2020d), HASOC Dravidian CodeMix 1 (Chakravarthi et al., 2020b"
2021.dravidianlangtech-1.19,W18-1105,0,0.0131876,"a pre-trained BERT model with the class balanced loss for offensive content identification on the Dravidian code-mixed text. The paper is organized as follows. Section 2 provides related work on offensive content identification on CM social media text. Section 3 provides information on the task and datasets. Section 4 describes the proposed work. Section 5 presents the experimental setup and the performance of the model. Section 6 concludes our work. 2 Related Work This section describes the related work on hate speech detection and offensive content identification in the code-mixed scenario. Bohra et al. (2018) created a dataset for hate speech detection from Hindi-English tweets. They collected around 4575 Hindi-English tweets and used traditional machine learning models with feature engineering for hateful and offensive content identification. Mandl et al. (2019) created a HindiEnglish dataset for a hate speech and offensive content identification Task (HASOC), organized at FIRE 2019. It consists of 4665 Hindi-English annotated posts collected from social media sites. They used Twitter API for crawling an unbiased dataset. The top models used the Long Short Term Memory (LSTM) (Schmidhuber and Hoch"
2021.dravidianlangtech-1.19,2020.peoples-1.5,0,0.0158474,"gue and their second language in different domains. This form of alternation of two or more languages is called ’code-mixing’ (CM) (Muysken et al., 2000). With the increase in social media access, Offensive content and hateful material on the internet has increased in the recent past (Thavareesan and Mahesan, 2019, 2020a,b). The internet harbors a variety of hateful and offensive statements, and nowadays, social media is a hotbed of such conversations. Recently, countries across the world have already begun to address hate speech and offensive content and how it affects society’s functioning (Chakravarthi, 2020). Research and technologies worldwide are utilizing natural language and machine learning tools to detect and curb the use of offensive content on social media. In a multilingual society, code-mixing has become a norm. The hateful and offensive content is delivered in a code-mixed form (Jose et al., 2020; Priyadharshini et al., 2020). Automatic hate speech detection on code-mixed data has faced quite many challenges due to the non-standard variations in spelling and grammar (Bali et al., 2014). The typical hate-speech and offensive language tools developed for monolingual data will not work fo"
2021.dravidianlangtech-1.19,2020.sltu-1.25,0,0.601058,"ges due to the non-standard variations in spelling and grammar (Bali et al., 2014). The typical hate-speech and offensive language tools developed for monolingual data will not work for codemixed data. So there is a need for more research and analysis to be done to identify the offensive content in code-mixed social media data. To encourage research on code-mixing and restrain the use of offensive texts on social media, the NLP community has organized several workshops such as Workshops on Computational Approaches to Linguistic Code-Switching, SentiMix (Patwa et al., 2020), Dravidian CodeMix (Chakravarthi et al., 2020d), HASOC Dravidian CodeMix 1 (Chakravarthi et al., 2020b; Mandl et al., 2020). Similarly, the European Association of Computational Linguistics 2021’s DravidianLangTech (Chakravarthi et al., 2021) was also devoted to identifying offensive content on Kannada-English, Tamil-English, and Malayalam-English code-mixed languages. This task aims to classify the given code-mixed com1 https://sites.google.com/view/ dravidian-codemix-fire2020/overview 154 Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, pages 154–159 April 20, 2021 ©2021 Association for Com"
2021.dravidianlangtech-1.19,2020.sltu-1.28,0,0.534872,"ges due to the non-standard variations in spelling and grammar (Bali et al., 2014). The typical hate-speech and offensive language tools developed for monolingual data will not work for codemixed data. So there is a need for more research and analysis to be done to identify the offensive content in code-mixed social media data. To encourage research on code-mixing and restrain the use of offensive texts on social media, the NLP community has organized several workshops such as Workshops on Computational Approaches to Linguistic Code-Switching, SentiMix (Patwa et al., 2020), Dravidian CodeMix (Chakravarthi et al., 2020d), HASOC Dravidian CodeMix 1 (Chakravarthi et al., 2020b; Mandl et al., 2020). Similarly, the European Association of Computational Linguistics 2021’s DravidianLangTech (Chakravarthi et al., 2021) was also devoted to identifying offensive content on Kannada-English, Tamil-English, and Malayalam-English code-mixed languages. This task aims to classify the given code-mixed com1 https://sites.google.com/view/ dravidian-codemix-fire2020/overview 154 Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, pages 154–159 April 20, 2021 ©2021 Association for Com"
2021.dravidianlangtech-1.19,2021.dravidianlangtech-1.17,0,0.0206022,"data. So there is a need for more research and analysis to be done to identify the offensive content in code-mixed social media data. To encourage research on code-mixing and restrain the use of offensive texts on social media, the NLP community has organized several workshops such as Workshops on Computational Approaches to Linguistic Code-Switching, SentiMix (Patwa et al., 2020), Dravidian CodeMix (Chakravarthi et al., 2020d), HASOC Dravidian CodeMix 1 (Chakravarthi et al., 2020b; Mandl et al., 2020). Similarly, the European Association of Computational Linguistics 2021’s DravidianLangTech (Chakravarthi et al., 2021) was also devoted to identifying offensive content on Kannada-English, Tamil-English, and Malayalam-English code-mixed languages. This task aims to classify the given code-mixed com1 https://sites.google.com/view/ dravidian-codemix-fire2020/overview 154 Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, pages 154–159 April 20, 2021 ©2021 Association for Computational Linguistics ments into one of the six predefined categories: Not-offensive, offensive-untargeted, OffensiveTargeted-Insult-Individual, Offensive- TargetedInsult-Group, Offensive-Targeted"
2021.dravidianlangtech-1.19,2020.peoples-1.6,0,0.270473,"omments and Tweets. The dataset contains 4000 annotated Youtube comments and 4952 annotated tweets for the Malayalam language, and 4940 annotated tweets for the Tamil language. The top models used deep learning models like LSTM, CNN, and BERT for hate speech and offensive content detection on the given Dravidian data. 3 Task and Dataset information The goal of offensive language identification is to identify the offensive language content of the code-mixed dataset of comments/posts in TamilEnglish (Chakravarthi et al., 2020c), MalayalamEnglish (Chakravarthi et al., 2020a), and KannadaEnglish (Hande et al., 2020) Dravidian languages collected from social media. Each comment or post is annotated with offensive, not-offensive, and notin-intended-language labels. Where the offensive label is fine-grained into further categories. The description of each label is given below 155 • Offensive-targeted-individual: offensive text delivered to an individual or person • Offensive-targeted-group: offensive text delivered to a group of people • Offensive-targeted-other: offensive text delivered to topics such as films, elections, sports, and so on. • Offensive-untargeted: the offensive text is delivered but withou"
2021.dravidianlangtech-1.19,2020.trac-1.7,0,0.0133346,"ized at FIRE 2019. It consists of 4665 Hindi-English annotated posts collected from social media sites. They used Twitter API for crawling an unbiased dataset. The top models used the Long Short Term Memory (LSTM) (Schmidhuber and Hochreiter, 1997) with attention mechanism, pre-trained Bidirectional encoder representations with transformers (BERT) (Devlin et al., 2018) models, and convolutional neural networks (CNN) for hate speech and offensive content identification. Kumar et al. (2018) created the dataset for an aggression detection task. The dataset was annotated for a comparison task by (Rani et al., 2020). This data set consists of 3367 posts and tweets collected from social media sites. The authors used traditional machine learning classification models for hate speech detection. A shared task called Dravidian Code-Mix (Chakravarthi et al., 2020b) was organized to identify the offensive language from comments/posts in code-mixed Dravidian Languages (Tamil-English and Malayalam-English) collected from social media. Each comment/post is annotated with the offensive language label at the comment/post level. The data set has been collected from YouTube comments and Tweets. The dataset contains 40"
2021.dravidianlangtech-1.8,2020.peoples-1.5,0,0.0223149,"2020. Our experimental results on multiple CMSA datasets demonstrate that the GCN with multi-headed attention model has shown an improvement in classification metrics. 1 Introduction Through the advent of social media, people from around the world can connect and exchange information instantly. The evolution of social media texts from blogs and messaging websites has created many new opportunities for information access worldwide (Thavareesan and Mahesan, 2019, 2020a,b). Multilingual communities often express their thoughts on social media by mixing different languages in the same utterance (Chakravarthi, 2020a). This mixing or alteration of two or more languages is known as code-mixing or code-switching (Wardhaugh, 2011; Chakravarthi, 2020b). The computational modeling of code-mixed text 65 Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, pages 65–72 April 20, 2021 ©2021 Association for Computational Linguistics Graph convolutional networks have been effective at tasks for knowledge representation and can preserve global structure information of a graph in graph embeddings. In this work, we propose the use of the graph neural network for code-mixed sen"
2021.dravidianlangtech-1.8,2020.sltu-1.25,0,0.49812,"f such non-standard variations, CM poses several unseen difficulties in fundamental fields of natural language processing (NLP) tasks such as language identification, part-ofspeech tagging, shallow parsing, Natural language understanding, sentiment analysis, etc. To encourage research on code-mixing and to solve the problems related to code-mixed text, the NLP community organizes several tasks and workshops such as Task9: SentiMix, SemEval 2020 (Patwa et al., 2020), and 4th Workshop on Computational Approaches for Linguistic Code-Switching (Solorio et al., 2020), Dravidian Code-Mix FIRE 2020 (Chakravarthi et al., 2020c; Mandl et al., 2020). All of these tasks focus on sentiment analysis on code-mixed text. Some researchers used traditional methods, such as support vector machines (SVM’s) (Vapnik, 2013) and Logistic Regression (LR), where the code-mixed text is represented with sparse linguistic features (e.g., bag-of-words and n-grams). Others used deep learning models such as convolutional neural networks (CNN) (Kim, 2014), recurrent neural networks (RNN) (Hochreiter and Schmidhuber, 1997), transformer models (Vaswani et al., 2017), etc. The above deep learning models can capture semantic and syntactic in"
2021.dravidianlangtech-1.8,2020.sltu-1.28,0,0.489163,"f such non-standard variations, CM poses several unseen difficulties in fundamental fields of natural language processing (NLP) tasks such as language identification, part-ofspeech tagging, shallow parsing, Natural language understanding, sentiment analysis, etc. To encourage research on code-mixing and to solve the problems related to code-mixed text, the NLP community organizes several tasks and workshops such as Task9: SentiMix, SemEval 2020 (Patwa et al., 2020), and 4th Workshop on Computational Approaches for Linguistic Code-Switching (Solorio et al., 2020), Dravidian Code-Mix FIRE 2020 (Chakravarthi et al., 2020c; Mandl et al., 2020). All of these tasks focus on sentiment analysis on code-mixed text. Some researchers used traditional methods, such as support vector machines (SVM’s) (Vapnik, 2013) and Logistic Regression (LR), where the code-mixed text is represented with sparse linguistic features (e.g., bag-of-words and n-grams). Others used deep learning models such as convolutional neural networks (CNN) (Kim, 2014), recurrent neural networks (RNN) (Hochreiter and Schmidhuber, 1997), transformer models (Vaswani et al., 2017), etc. The above deep learning models can capture semantic and syntactic in"
2021.dravidianlangtech-1.8,2021.dravidianlangtech-1.4,0,0.0702146,"Missing"
2021.dravidianlangtech-1.8,N18-2078,0,0.0158765,"nd do not explicitly model global word co-occurrence information in a corpus. 2.3 Recently, Graph Neural Networks has received growing attention in the field of natural language processing. The authors used Text GCN (Yao et al., 2019) to construct a heterogeneous graph that helped them in text classification and achieved great results. Graph Neural Networks have also been used in intent slot labeling (Zhang et al., 2020), semantic role labeling (Marcheggiani and Titov, 2017), knowledge base construction (Wang et al., 2019), relation classification (Sahu et al., 2019), and machine translation (Marcheggiani et al., 2018). The use of graph convolutional networks with undirected graphs to model the free word order languages for CMSA has never been explored before. 3 Proposed Work In this section, we describe our proposed work. First, we present the pre-processing steps to filter the data, and then we describe graph-convolutional networks with multi-headed attention for codemixed sentiment analysis. The data we have used for CMSA consists of code-mixed Tamil-English and Malayalam-English youtube comments, and it has many irregularities. Such as the Roman script is used to write Tamil and Malayalam text with vari"
2021.dravidianlangtech-1.8,D17-1159,0,0.0284593,"ce of the model. Section 5 concludes our work. 2 2020c). These methods mainly focus on the position of the word and local consecutive word sequences and do not explicitly model global word co-occurrence information in a corpus. 2.3 Recently, Graph Neural Networks has received growing attention in the field of natural language processing. The authors used Text GCN (Yao et al., 2019) to construct a heterogeneous graph that helped them in text classification and achieved great results. Graph Neural Networks have also been used in intent slot labeling (Zhang et al., 2020), semantic role labeling (Marcheggiani and Titov, 2017), knowledge base construction (Wang et al., 2019), relation classification (Sahu et al., 2019), and machine translation (Marcheggiani et al., 2018). The use of graph convolutional networks with undirected graphs to model the free word order languages for CMSA has never been explored before. 3 Proposed Work In this section, we describe our proposed work. First, we present the pre-processing steps to filter the data, and then we describe graph-convolutional networks with multi-headed attention for codemixed sentiment analysis. The data we have used for CMSA consists of code-mixed Tamil-English a"
2021.dravidianlangtech-1.8,L18-1550,0,0.0236097,"on-linear activation function. The weight matrix has dimensions (F i , F i+1 ). Equation 1 can be explained as a feature transformation that occurs with information borrowed from the neighborhood nodes. But the transformation in the equation 1 focuses only on neighborhood information and ignores its own node features, so we add the identity matrix to the adjacency matrix. Later we normalize the feature representations w.r.t degree of each node. So the initial propagation rule is modified as given in equation 2 Word embeddings After transliteration, we have used the pre-trained fastText model (Grave et al., 2018) to obtain the word embeddings. We didn’t rely on extracting word-embeddings from the same corpora as its size is small, and we need a large code mixed corpora with cross-lingual mapping or cross-lingual dictionary to obtain better word-embeddings. Even Khanuja et al. (2020) has suggested that we can use pre-trained models to learn representations of words. Their paper has shown an improved performance in accuracy when pre-trained models are used on the code-mixed datasets. 3.3 (1) ˜ iW i) f (H i , A) = σ(AH (2) where A˜ = D−1/2 (A + I)D−1/2 is the normalized symmetric adjacency matrix. The mu"
2021.dravidianlangtech-1.8,N18-1202,0,0.0347356,"We use point-wise mutual information (PMI), to calculate weights between two nodes. A positive PMI value indicates a high semantic correlation of words in a corpus, whereas a negative PMI value indicates little or no semantic correlation in the corpus. As suggested in the paper, we only add edges between word pairs with positive PMI values. Note: There are no direct document-to-document relations in the graph. • LSTM: The LSTM model uses the last hidden state to represent the whole sentence. We used fastText pre-trained word embeddings. • ELMO + SVM: Deep Contextualised Word Representations (Peters et al., 2018) uses a deep, bi-directional LSTM model to create word representations. ELMo obtained the word representations by analyzing words within the context that they are used. It also uses character-based embeddings to model the representations for out-of-vocabulary words. Adjacency matrix,   P M I(i, j) Aij = T F − IDFi,j   0 i, j are words i is document, j is word otherwise (4) where P M I(i, j) > 0 and PMI value of a word pair i, j is computed as P M I(i, j) = log p(i, j) p(i)p(j) • BERT(pre-trained multilingual): We used pre-trained multilingual bi-directional encoder representations using t"
2021.dravidianlangtech-1.8,D14-1181,0,0.0164496,"Missing"
2021.dravidianlangtech-1.8,P19-1423,0,0.0386978,"Missing"
2021.dravidianlangtech-1.8,P19-1191,0,0.0322102,"Missing"
2021.ltedi-1.11,W14-3914,0,0.0494369,"Missing"
2021.ltedi-1.11,2020.peoples-1.5,0,0.240525,"uage contact is known as code-mixing (Di Sciullo et al., 1986). Code-mixed data is real-world unprocessed data that has non-standard variations of spelling and does not follow a grammatical structure (Bali et al., 2 Related Work Hope speech detection is a novel topic with a significantly limited amount of research done in this field. (Palakodety et al., 2019) propose a novel task to automatically detect hope speech on web content that may play a positive role in diffusing hostility on social media triggered by heightened political tensions during a conflict between two nuclear power nations. (Chakravarthi, 2020) created a multilingual, hostility-diffusing hope speech dataset for equality, diversity, and inclusion. It is a new large-scale English, Tamil (code-switched) dataset, and Malay86 Proceedings of the First Workshop on Language Technology for Equality, Diversity and Inclusion, pages 86–91 April 19, 2021. ©2020 Association for Computational Linguistics Dataset English Malayalam-English Tamil-English #Train 22762 8564 16160 #Dev 2843 1070 2018 #Test 2846 1071 2020 #Total 28451 10705 20198 Table 1: Data Statistics alam (code-switched) YouTube comments. They have experimented on the dataset by usin"
2021.ltedi-1.11,2021.ltedi-1.8,0,0.204533,"Missing"
2021.ltedi-1.11,W17-1101,0,0.020117,"perform the experiments by taking advantage of pre-processing and transfer-learning models. We observed that the pre-trained multilingual-BERT model with convolution neural networks gave the best results. Our model ranked 1st , 3rd , and 4th ranks on English, Malayalam-English, and Tamil-English code-mixed datasets. 1 Introduction Nowadays, people often use social media websites to share their views and thoughts. The thoughts might be positive or negative. Much work has been done towards identifying the negative thoughts, i.e., hate speech and offensive content identification on social media (Schmidt and Wiegand, 2017; Davidson et al., 2017). Research is now shifting to the field of analyzing positivity via hope speech detection on social media. Hope is a positive state of mind, an expectation of positive outcomes concerning events and circumstances in one’s life (Youssef and Luthans, 2007). Hope drives an individual to move forward. Hope can be a useful tool for each individual to maintain a stable and optimistic attitude towards life. In a multilingual society, people usually express their thoughts by mixing two or more languages in a single utterance. This form of language contact is known as code-mixin"
2021.ltedi-1.21,2020.peoples-1.5,0,0.0383271,"be comment dataset to model the polarity of comments based on the kernel models. Recently, to support the Rohingyas refugees, Palakodety et al. (2019) analyze how the hope speech from the social media comments can be used to reduce tensions between India and Pakistan during the Puluma attack. In this work, the authors intend to find the hope speech in SMNs can reduce the strain and violence between two nations. The authors created a corpus of 921,235 English Youtube comments posted by 392,460 users to accomplish this work. One of the notable research work for hope speech detection is HopeEDI (Chakravarthi, 2020) corpus creation. In this work, authors created a corpus from user-generated YouTube comments for English, Tamil, and Malayalam. Then they developed various machine learning models for benchmark results on the dataset. 4 Figure 1: Distribution of samples per class 5 5.1 • All the comments in the dataset are converted to lowercase. • Converted the online chatting abbreviations like ‘ASAP’, ‘YOLO’ into their original form by creating a slang words mapping dictionary. This paper used the corpus (HopeEDI) provided by the “Hope Speech Detection” organizers to train and tune the models. The HopeEDI"
2021.ltedi-1.21,2021.ltedi-1.8,0,0.0315207,"ic minorities. And some of the research studies (Ganda, 2014) have shown that SMNs and OSGs significantly influence people’s self-identification and self-understanding. So it’s necessary to detect the positive content from online sources. In this paper, we discussed the identification of Hope Speech for Equality, Diversity, and Inclusion (EDI) from the YouTube comments. Hope can be defined as a state of mind that brings fortitude, support, and reassurance to life. Hope can also come 1 Shared Task Description We are attempting this shared task for English language only. The LT-EDI-2021 Task 2 (Chakravarthi and Muralidaran, 2021) is as follows: Given a YouTube comment or post, the system should identify its class. We have three classes ‘Hope speech’, ‘Not hope speech’ and ‘Other language.’ We classify the comment into the ‘Hope speech’ category when it promotes hope, optimism, faith, support, reassurance, suggestions, or it offers EDI values. If a comment doesn’t have attributes mentioned in the hope speech class, classify it into the ‘Not hope speech’ class. We categorize the comment into the ‘Other language’ class when the comment not in the English language. Table 1 shows few sample YouTube comments from HopeEDI. h"
2021.ltedi-1.21,N19-1423,0,0.0362541,"he transformer based BERT model and then explain how we fine-tune this model to our problem. 5.3.1 Input Data Format The input token sequence for the BERT model must be given in a certain format. Every input sequence must start with a [CLS] (classification token) token, and every sequence should be separated from other sequences by using a [SEP] (separation token) token. According to this BERT input data format, we added a [CLS] token to every input sequence and appended a [SEP] token after every sentence. 5.3.2 BERT Architecture Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) is a “deep bidirectional” model designed to learn deep bidirectional representations from a large unsupervised 145 Unlike traditional directional models, which reads the input sequence either from left to right or right to left, the BERT encoder attention mechanism processes the input sequence simultaneously, allowing all input tokens in the sequence to be processed in parallel. This feature enables the model to know the context of a token based on the tokens around it. We can see all the layers of BERT architecture in Figure 3. This pre-trained BERT model fine-tuned to our problem by adding"
2021.ltedi-1.21,D14-1162,0,0.0948038,"Missing"
2021.ltedi-1.21,P14-1118,0,0.0589712,"Missing"
2021.ltedi-1.23,2020.peoples-1.5,0,0.272501,"ACL2021: Hope Speech Detection Using Indic Transliteration and Transformers Ishan Sanjeev Upadhyay*1 , Nikhil E*1 , Anshul Wadhawan2 , and Radhika Mamidi3 1,3 International Institute of Information Technology, Hyderabad 2 Flipkart Private Limited 1 {ishan.sanjeev, nikhil.e}@research.iiit.ac.in 2 anshul.wadhwan@flipkart.com 3 radhika.mamidi@iiit.ac.in Abstract work done on hope speech or help speech detection before that has used logistic regression and active learning techniques (Palakodety et al., 2020a,b). In our paper, we will be doing the hope speech detection task on the HopeEDI dataset (Chakravarthi, 2020; Chakravarthi and Muralidaran, 2021) which consists of user comments from Youtube in English, Tamil and Malayalam. In this paper, we will first look at the task definition, followed by the methodology used. We will then look at the experiments and results followed by conclusion and future work. This paper aims to describe the approach we used to detect hope speech in the HopeEDI dataset. We experimented with two approaches. In the first approach, we used contextual embeddings to train classifiers using logistic regression, random forest, SVM, and LSTM based models.The second approach involved"
2021.ltedi-1.23,2021.ltedi-1.8,0,0.1603,"Detection Using Indic Transliteration and Transformers Ishan Sanjeev Upadhyay*1 , Nikhil E*1 , Anshul Wadhawan2 , and Radhika Mamidi3 1,3 International Institute of Information Technology, Hyderabad 2 Flipkart Private Limited 1 {ishan.sanjeev, nikhil.e}@research.iiit.ac.in 2 anshul.wadhwan@flipkart.com 3 radhika.mamidi@iiit.ac.in Abstract work done on hope speech or help speech detection before that has used logistic regression and active learning techniques (Palakodety et al., 2020a,b). In our paper, we will be doing the hope speech detection task on the HopeEDI dataset (Chakravarthi, 2020; Chakravarthi and Muralidaran, 2021) which consists of user comments from Youtube in English, Tamil and Malayalam. In this paper, we will first look at the task definition, followed by the methodology used. We will then look at the experiments and results followed by conclusion and future work. This paper aims to describe the approach we used to detect hope speech in the HopeEDI dataset. We experimented with two approaches. In the first approach, we used contextual embeddings to train classifiers using logistic regression, random forest, SVM, and LSTM based models.The second approach involved using a majority voting ensemble of"
2021.ltedi-1.23,N19-1423,0,0.0271956,"ence, they do not suffer from long dependency issues. Query, Key and Value are three different ways in which input vectors are used in the self-attention mechanism. The attention score for every input vector is calculated using a compatibility function which takes as input the query vector and all the keys. The final output is a weighted sum of values where the weights are the attention scores calculated by the compatibility function. We have used the following transformers in our experiment. We chose RoBERTa for our final model in English and ALBERT (IndicBERT) for Tamil and Malayalam. BERT (Devlin et al., 2019) is based on the transformer architecture. Using its multi-layer encode module,It is able to jointly utilize both left and right contexts across all layers to pre-train its bidirectional representations. BERT is trained on two unsupervised prediction tasks, next sentence prediction and masked language modelling. We have fine tuned “bert-base-uncased” model on the dataset for one of our experiments. RoBERTa (Liu et al., 2019) is a transformer architecture which is based on optimizations made to 158 https://github.com/sanskrit-coders/ indic_transliteration the BERT approach. It trains on more da"
2021.ltedi-1.23,2020.findings-emnlp.445,0,0.0994198,"Missing"
2021.ltedi-1.23,W18-5113,0,0.0174086,"e of 11 models which were obtained by fine-tuning pre-trained transformer models (BERT, ALBERT, RoBERTa, IndicBERT) after adding an output layer. We found that the second approach was superior for English, Tamil and Malayalam. Our solution got a weighted F1 score of 0.93, 0.75 and 0.49 for English,Malayalam and Tamil respectively. Our solution ranked first in English, eighth in Malayalam and eleventh in Tamil. 1 2 Introduction The spread of hate speech on social media is a problem that still exists today. While there have been attempts made at hate speech detection (Schmidt and Wiegand, 2017; Lee et al., 2018) to stop the spread of negativity, this form of censorship can also be misused to obstruct rights and freedom of speech. Furthermore, hate speech tends to spread faster than non-hate speech (Mathew et al., 2019).While there has been a growing amount of marginalized people looking for support online (Gowen et al., 2012; Wang and Jurgens, 2018), there has been a substantial amount of hate towards them too (Mondal et al., 2017). Therefore, detecting and promoting content that reduces hostility and increases hope is important. Hope speech detection can be seen as a rare positive mining task becaus"
2021.ltedi-1.23,W17-1101,0,0.0254752,"g a majority voting ensemble of 11 models which were obtained by fine-tuning pre-trained transformer models (BERT, ALBERT, RoBERTa, IndicBERT) after adding an output layer. We found that the second approach was superior for English, Tamil and Malayalam. Our solution got a weighted F1 score of 0.93, 0.75 and 0.49 for English,Malayalam and Tamil respectively. Our solution ranked first in English, eighth in Malayalam and eleventh in Tamil. 1 2 Introduction The spread of hate speech on social media is a problem that still exists today. While there have been attempts made at hate speech detection (Schmidt and Wiegand, 2017; Lee et al., 2018) to stop the spread of negativity, this form of censorship can also be misused to obstruct rights and freedom of speech. Furthermore, hate speech tends to spread faster than non-hate speech (Mathew et al., 2019).While there has been a growing amount of marginalized people looking for support online (Gowen et al., 2012; Wang and Jurgens, 2018), there has been a substantial amount of hate towards them too (Mondal et al., 2017). Therefore, detecting and promoting content that reduces hostility and increases hope is important. Hope speech detection can be seen as a rare positive"
2021.ltedi-1.23,D18-1004,0,0.0189947,"olution ranked first in English, eighth in Malayalam and eleventh in Tamil. 1 2 Introduction The spread of hate speech on social media is a problem that still exists today. While there have been attempts made at hate speech detection (Schmidt and Wiegand, 2017; Lee et al., 2018) to stop the spread of negativity, this form of censorship can also be misused to obstruct rights and freedom of speech. Furthermore, hate speech tends to spread faster than non-hate speech (Mathew et al., 2019).While there has been a growing amount of marginalized people looking for support online (Gowen et al., 2012; Wang and Jurgens, 2018), there has been a substantial amount of hate towards them too (Mondal et al., 2017). Therefore, detecting and promoting content that reduces hostility and increases hope is important. Hope speech detection can be seen as a rare positive mining task because hope speech constitutes a low percentage of overall content (Palakodety et al., 2020a). There has been Task Definition The given problem is a comment level classification task for the identification of ”hope speech” within YouTube comments, wherein they are to be classified as ”Hope speech”, ”Not hope speech” and ”Not in intended language”."
2021.semeval-1.149,2020.semeval-1.186,0,0.0856775,"Missing"
2021.semeval-1.149,N19-1423,0,0.0194898,"each modality. Only the English textual cues are used in tasks A and B, while the visual cues are also used in task C. Task B is a modification of task A which further requires predicting the spans for each identified technique as well. ∗ The authors have contributed equally. Meme classification is a multimodal problem that often requires visual and textual cues to convey a message. Memes can convey very different meanings if either of the cues is removed. A few samples are shown in figure 1 to demonstrate the importance of visual and textual cues for classification. We experiment with BERT (Devlin et al., 2019) based unimodal models for tasks A and B. Since they are state-of-the-art models for natural language understanding, they are a good choice for understanding the complex propaganda techniques in texts. Transformers (Vaswani et al., 2017) have limited sequence length, which limits their performance on longer data, but in the case of memes, the textual content is also very limited. For task C, we experiment with VisualLinguistic (VL) models like UNITER (Chen et al., 2020), VisualBERT (Li et al., 2019), LXMERT (Tan and Bansal, 2019) for the crossmodal understanding of memes. We further explore th"
2021.semeval-1.149,2021.semeval-1.7,0,0.135486,"e of 57.0, 48.2, and 52.1 in the corresponding subtasks. 1 Figure 1: Sample memes demonstrating the multimodal setting Introduction Memes are text superimposed on graphics that convey messages in the form of jokes, sarcasm, etc. In the current era of the internet and social media, they are very quick to spread. If used as a part of a disinformation campaign, it can be quite tricky to notice the agenda behind them and has the potential to influence a large mass of people without them realizing it (Muller, 2018; Tard´aguila et al., 2018; Glowacki et al., 2018). To this end, SemEval 2021 Task 6 (Dimitrov et al., 2021) focuses on identifying such persuasive techniques (Miller, 1939) in a multimodal (visuallinguistic) setting. It consists of three subtasks that enable the participants to study the problem in each modality. Only the English textual cues are used in tasks A and B, while the visual cues are also used in task C. Task B is a modification of task A which further requires predicting the spans for each identified technique as well. ∗ The authors have contributed equally. Meme classification is a multimodal problem that often requires visual and textual cues to convey a message. Memes can convey very"
2021.semeval-1.149,D17-1317,0,0.054804,"hind the success of such campaigns is the boom of the internet and social media in recent years. Another factor being the difficulty to spot such techniques manually because of the high volume of text produced and the unnoticeable nature of such content. With the recent interest of the research community in ”fake news,” the detection of persuasive techniques or highly biased texts has emerged as an active research area. Some of the previous work in this direction analyzes the general pattern of propaganda (Garimella et al., 2018; Chatfield et al., 2015), performs analysis at a document level (Rashkin et al., 2017; Barr´onCede˜no et al., 2019) and a fine-grained analysis of the text (Da San Martino et al., 2019, 2020). However, most previous work analyses the techniques in a textual unimodal setting only. This work studies propaganda techniques in a new age domain like memes. Meme classification task can be considered a combined VL multimodal problem. It is similar to the currently popular VL problems like Visual Question Answering (Antol et al., 2015), Visual Commonsense Reasoning (Zellers et al., 2019) and Visual Entailment (Xie et al., 2019), as we have to classify semantically correlated text with"
2021.semeval-1.149,2020.trac-1.6,0,0.0360074,"rrelated text with that of the visual content in the image. Hence a crossmodal approach under vision and text should perform better than unimodal architectures. Basic VL approaches are based on simple fusion techniques in the form of early or late fusion to correlate unimodally trained visual and textual models. However, in an ideal scenario, a multimodally trained model should be more effective in detecting persuasive techniques in memes. With the rising interest in VL problems, recent work attempts to study similar problems in a VL multimodal setting (Gomez et al., 2020; Kiela et al., 2020; Suryawanshi et al., 2020). Data Description The dataset consists of 951 memes in total, which is further divided into train/ Figure 2: Data Distribution of the labels in the training set dev/ test splits. All the tasks have the same set of memes in their training sets, but the labeling differs for each of them. For task A, only the textual cues were used to identify the techniques. For task B, the spans of each technique were further detected. For task C, more techniques were identified using visual cues. The distribution of the labels is illustrated in figure 2. Detailed information of the dataset can be found in the"
2021.semeval-1.149,D19-1514,0,0.0125355,"ual and textual cues for classification. We experiment with BERT (Devlin et al., 2019) based unimodal models for tasks A and B. Since they are state-of-the-art models for natural language understanding, they are a good choice for understanding the complex propaganda techniques in texts. Transformers (Vaswani et al., 2017) have limited sequence length, which limits their performance on longer data, but in the case of memes, the textual content is also very limited. For task C, we experiment with VisualLinguistic (VL) models like UNITER (Chen et al., 2020), VisualBERT (Li et al., 2019), LXMERT (Tan and Bansal, 2019) for the crossmodal understanding of memes. We further explore the effectiveness of ensembling models trained in 1075 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 1075–1081 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics different modalities. The code for all subtasks is available at http://github.com/kshitij98/ multimodal-propaganda.git 2 Background Propaganda aims to push biased agendas to influence people’s mindsets. It is successful in achieving its goal by hiding its way through any of the numerou"
2021.semeval-1.173,P19-1394,0,0.0266794,") have been used for this task (Zhang and Luo, 2018). Recurrent neural networks combined with user-related information have also been used for hate speech detection in Twitter Data (Pitsilis et al., 2018) whereas multilingual transformer architectures were leveraged by (Ghosh Roy et al., 2021) to detect hostile content in English, Hindi and German. 3 Task and dataset overview The task(Meaney et al., 2021) is divided into 4 sub-tasks. Related work There have been many attempts made at computational humour detection. In this section, we briefly describe other work in this area. In this approach(Blinov et al., 2019), the authors have used universal language model fine-tuning method 1. Humour detection: This is a binary classification task where the model needs to predict if the text is humorous or not where the values are either 0 and 1. 2. Humour Rating: This is a regression task where the model needs to rate how humorous the text is where the value can vary between 0 to 5. 3. Controversy detection: This is a binary classification task where the model needs to classify text as controversial or not if it has been classified as humorous. It can be either 0 or 1. 1221 Proceedings of the 15th International"
2021.semeval-1.173,2020.acl-main.740,0,0.0553973,"Missing"
2021.semeval-1.173,2020.semeval-1.98,0,0.0279923,"We achieved an F1 score of 0.959 in the humor classification task and 0.592 in the humor controversy task. For the regression tasks, we achieved a RMSE score of 0.541 and 0.488 in the humor regression and offense regression task respectively. 2 for humour recognition. Convolutional neural networks (CNN) have also been used for this task by (Chen and Soo, 2018) whereas (Weller and Seppi, 2019) used transformers to classify humour. There has also been a lot of shared tasks and workshops related to computational humour. One of them is SemEval-2020 Task 7: Assessing Humor in Edited News Headlines(Hossain et al., 2020) where Zhang(Zhang et al., 2020) used bidirectional neural networks with an attention mechanism and incorporated lexical features to assess humour in edited news headlines. There has been a lot of work done on hate speech and offensive speech detection as well. CNN’s and gated recurrent units (GRU) have been used for this task (Zhang and Luo, 2018). Recurrent neural networks combined with user-related information have also been used for hate speech detection in Twitter Data (Pitsilis et al., 2018) whereas multilingual transformer architectures were leveraged by (Ghosh Roy et al., 2021) to dete"
2021.semeval-1.173,2020.findings-emnlp.445,0,0.0573444,"Missing"
2021.semeval-1.173,2021.ccl-1.108,0,0.0871954,"Missing"
2021.semeval-1.173,2021.semeval-1.9,0,0.0352315,"mechanism and incorporated lexical features to assess humour in edited news headlines. There has been a lot of work done on hate speech and offensive speech detection as well. CNN’s and gated recurrent units (GRU) have been used for this task (Zhang and Luo, 2018). Recurrent neural networks combined with user-related information have also been used for hate speech detection in Twitter Data (Pitsilis et al., 2018) whereas multilingual transformer architectures were leveraged by (Ghosh Roy et al., 2021) to detect hostile content in English, Hindi and German. 3 Task and dataset overview The task(Meaney et al., 2021) is divided into 4 sub-tasks. Related work There have been many attempts made at computational humour detection. In this section, we briefly describe other work in this area. In this approach(Blinov et al., 2019), the authors have used universal language model fine-tuning method 1. Humour detection: This is a binary classification task where the model needs to predict if the text is humorous or not where the values are either 0 and 1. 2. Humour Rating: This is a regression task where the model needs to rate how humorous the text is where the value can vary between 0 to 5. 3. Controversy detect"
2021.semeval-1.173,P17-1161,0,0.0310636,"and regression task. 4.2 Lexical features The structure of humorous and offensive texts can be a bit different from normal texts. We have leveraged a lexical feature set that would help us capture that information and distinguish humorous and offensive texts. The set of lexical features are: • Counting the total number of letters, punctuation, upper case letters and numbers within the text. • Identifying the presence of any named entity. For detecting named entities, we have used the AllenNLP named entity recogniser1 which uses pretrained GloVe vectors for token embeddings and a GRU encoder.(Peters et al., 2017) • Detecting the presence of interrogation by identifying ’?’ symbol or any WH-word • Detecting the number of personal pronouns and what kind of personal pronouns they are: first-person, second-person or third-person. 1 https://demo.allennlp.org/ named-entity-recognition/ named-entity-recognition For detecting the personal pronouns, we have used a pre-defined list of personal pronouns. 4.3 Sentence embeddings For generating the sentence embeddings, we have experimented with 4 different pre-trained transformer models: bert-base-uncased(Devlin et al., 2018), roberta-base(Liu et al., 2019), googl"
2021.semeval-1.173,D19-1372,0,0.0242777,"offensive or controversial humour being posted online. (Weitz, 2017) In this task, we have presented a transformer based approach combined with lexical and hurtlex feature sets to quantify humour and offense of a piece of text. We achieved an F1 score of 0.959 in the humor classification task and 0.592 in the humor controversy task. For the regression tasks, we achieved a RMSE score of 0.541 and 0.488 in the humor regression and offense regression task respectively. 2 for humour recognition. Convolutional neural networks (CNN) have also been used for this task by (Chen and Soo, 2018) whereas (Weller and Seppi, 2019) used transformers to classify humour. There has also been a lot of shared tasks and workshops related to computational humour. One of them is SemEval-2020 Task 7: Assessing Humor in Edited News Headlines(Hossain et al., 2020) where Zhang(Zhang et al., 2020) used bidirectional neural networks with an attention mechanism and incorporated lexical features to assess humour in edited news headlines. There has been a lot of work done on hate speech and offensive speech detection as well. CNN’s and gated recurrent units (GRU) have been used for this task (Zhang and Luo, 2018). Recurrent neural netwo"
2021.semeval-1.173,2020.semeval-1.129,0,0.0583144,"Missing"
2021.wassa-1.13,W19-4828,0,0.140201,"that when learning without curriculum, in the case of harder tasks, the model is confounded and overwhelmed by the presence of hard samples. Hence, laying out the training where the model observes easier samples first is natural to improve performance. However, this reasoning does not explain well why there is an apparent decrement of performance for CL on high performing tasks. 4 Axis III: Attention Movement Visualization Visualization of Attention (Vaswani et al., 2017) is an important and popular visualization method for interpreting the prediction in these models. Previous works such as (Clark et al., 2019) have visualized attentions to identify where does BERT look. (Vaswani et al., 2017) also show in attention visualization that various heads of the transformer look at various linguistic details. In this section we use attention visualization to qualitatively analyze and explain how the model’s focus on the sentence changes as the various stages (when model encounters new and harder data) of CL progresses. It is important to note that BERT has N layers and H heads. Within each of these N xH heads there lies a T xT self attention from each input time stamp to every other time stamp. Where T is"
2021.wassa-1.13,N19-1423,0,0.656781,"ifficulty” quotient of sample si . Furthermore, One Pass makes distinct, mutually exclusive sets of the training data and trains on each one of these sets one by one. This makes it faster as compared to Baby Steps, where data cumulatively increases in each pass. This implies that the model is trained on previous data and the additional harder data. To analyze the two methods for executing CL we choose two curriculum strategies (difficulty scoring function). Furthermore, we also experiment with an individual setting explained in following sections. 2.1 We use the popular transformer model BERT(Devlin et al., 2019) for our experiments due to how ubiquitous it is across natural language processing tasks. Bidirectional Encoder Representations from Transformers (BERT) is a masked language model trained on large corpora. A sentence is added with a special token (CLS) at the beginning and is passed into the pre-trained BERT model. It tokenizes the sentence with a maximum length of 512 and outputs a contextual representation for each of the tokenized words. There are variants of pre-trained BERT depending upon the hyper-parameters of the model. BERT-Base Uncased consists of 12 transformer encoders, and output"
2021.wassa-1.13,W18-5446,0,0.0188068,"zed. In the above images, the model performance on each individual test set sample is illustrated across all curriculum phases. These phases correspond with how much data the model has observed so far. in performance on CIFAR-100 is almost twice the increment in performance on CIFAR-10 while using the same network VGG-net for training. They argue that this might be because in easier tasks such as CIFAR-10, there are already enough easy samples observed during training without CL, and hence improvement caused by CL is subdued. (Xu et al., 2020) enable their CL across the range of tasks in GLUE(Wang et al., 2018). GLUE encompasses a wide range of tasks in natural language understanding, with varying performances. For example a task such as RTE or CoLA is considered harder than SST-2 or QNLI. The BERT(Devlin et al., 2019) model’s performance for the same are 70.1, 60.5, 94.9, 91.1 respectively . We built our experiments upon these works. Proposed work is different from (Hacohen and Weinshall, 2019) in the way that our work focuses on sentiment analysis in NLP, whereas (Hacohen and Weinshall, 2019) experimented strictly with image processing tasks. Furthermore, since the apparent relation between task d"
2021.wassa-1.13,2021.ccl-1.108,0,0.0348567,"Missing"
2021.wassa-1.13,Q19-1040,0,0.013539,"ent analysis in NLP, whereas (Hacohen and Weinshall, 2019) experimented strictly with image processing tasks. Furthermore, since the apparent relation between task difficulty and improvement due to CL wasn’t the main focus of the work, the experimentation wasn’t enough to fully establish the correlation. While (Xu et al., 2020) perform experiments within the purview of NLU and Text Classification, the experiments themselves are not consistent across dataset and the nature of task. Specifically, we believe it’s hard to conclude from experiments on tasks as disparate as Linguistic Acceptability(Warstadt et al., 2019) and Sentiment Analysis(Socher et al., 2013) to establish the relationship between CL and task difficulty. Such an experiment inadvertently raises more questions whether the difference of improvement due to CL is due to the nature of tasks (Sentiment Identification as opposed to Linguistical soundness detection) or the nature of the dataset being different (sentence lengths or vocabulary). We eschew this concern by performing experiment within the same task (Sentiment Analysis), and the same dataset (SST5), instead we introduce change in task difficulty by using the fine grained labels provide"
2021.wassa-1.13,P16-1043,0,0.0261703,"previous works as well. (Hacohen and Weinshall, 2019; Weinshall et al., 2018). (Cirik et al., 2016) proposed Baby Steps and One Pass curriculum techniques using sentence length as a curriculum strategy for training LSTM (Hochreiter and Schmidhuber, 1997) on Sentiment Analysis. A tree-structured curriculum ordering based on semantic similarity is proposed by (Han and Myaeng, 2017). (Rao et al., 2020) propose an auxiliary network that is first trained on the dataset and used to calculate difficulty scores for the curriculum order. CL is also used in NLP within tasks such as Question Answering (Sachan and Xing, 2016, 2018) and NLG for Answer Generation (Liu et al., 2018). For Sentiment Analysis, (Cirik et al., 2016) propose a strategy derived from sentence length, where smaller sentences are considered easier and are provided first. (Han and Myaeng, 2017) provide a tree-structured curriculum based on semantic similarity between new samples and samples already trained. (Tsvetkov et al., 2016) suggest a curriculum based on handcrafted semantic, linguistic, syntactic features for word representation learning. Some of these works (Cirik et al., 2016; Han and Myaeng, 2017; Rao et al., 2020) have suggested tha"
2021.wassa-1.13,2020.acl-main.542,0,0.243921,"is observed every time the color of a sample As discussed earlier, we take k = 5; hence we end up with 5 phases on the x-axis. • Visualizing Test Performance: This visualization is done on the unseen test set. Our above hypotheses are still natural to understand if visualized on the train set. However, the samples in the visualization are always unseen during the training. This implies that the model forgets or remembers training samples in the corresponding phases and the associated concepts as well. 3 Axis II: Curriculum Learning only helps Difficult tasks (Hacohen and Weinshall, 2019) and (Xu et al., 2020) propose that CL might help tasks which are harder than easier. They observe that for tasks which have low performance, the difference in performance caused due to introducing a curriculum training is more than the tasks which have a higher performance without curriculum. We call this the Task Difficulty Hypothesis. (Hacohen and Weinshall, 2019) perform an image classification experiment where they experiment with enabling curriculum on the CIFAR dataset(Krizhevsky et al., 2009) with 100 classes(CIFAR-100) and 10 classes(CIFAR-10) as two separate experiments. They use VGG network(Simonyan and"
2021.wassa-1.13,N18-1058,0,0.0571633,"Missing"
2021.wassa-1.13,D13-1170,0,0.0370958,"is on the same dataset. This Aux model architecture will be the same as the model finally used for CL. This allows us to find out which training samples are actually difficult. We learn what samples are the most difficult to classify and what are the easiest from this model. For all training samples of D, we define the curriculum score as follows: S(si ) = c X (Aux(si )j − yij )2 (2) j Dataset Following previous works in curriculum-driven sentiment analysis (Cirik et al., 2016; Han and Myaeng, 2017; Tsvetkov et al., 2016; Rao et al., 2020) We use the Stanford Sentiment Treebank (SST) dataset (Socher et al., 2013). Unlike most sentiment analysis datasets with binary labels, SST is for a 5-class classification consisting of 8544/1101/2210 samples in train, development, Our dataset has 5 labels. https://nlp.stanford.edu/sentiment/ Model Details: BERT where Aux(si )j is the prediction of auxiliary model Aux on sentence si , j is the iterator over the number of classes c = 5. In essence, we find the mean squared error between the prediction and the sentence’s true labels. If S(si ) is high, it implies the sentence is hard to classify, and if less, then the sentence is easy. Because the features were traine"
2021.wassa-1.13,N10-1116,0,0.2103,"ntiment and Social Media Analysis, pages 117–128 April 19, 2021. ©2021 Association for Computational Linguistics perform experiments using both techniques. While the idea of curriculum remains the same across these works, the strategy itself to decide sample ordering is often diverse. 2 and test set. We use this standard split with reported results averaged over 5 turns. 2.2 Axis I: Curriculum Learning: One Pass and Baby Steps While Curriculum Learning as defined by (Bengio et al., 2009) is not constrained by a strict description, later related works (Cirik et al., 2016; Han and Myaeng, 2017; Spitkovsky et al., 2010; Rao et al., 2020) make distinctions between Baby Steps curriculum and One-Pass curriculum. Most of these previous works have also shown the dominance of Baby Steps over One-Pass. Baby Steps and One Pass curriculum can be defined as follows. For every sentence si ∈ D, its sentiment is described as yi ∈ {0, 1, 2, 3, 4}, where i ∈ {1, 2...n} for n data points in D. For a model fw , its prediction based on si will be fw (si ). Loss L is defined on the model prediction and actual output as L(yi , fw (si )) and Cost defining the task as C(D, fw ) as X1 L(yi , fw (si )) n (1) Here, curriculum strat"
2021.wassa-1.13,P16-1013,0,0.117644,"). (Rao et al., 2020) propose an auxiliary network that is first trained on the dataset and used to calculate difficulty scores for the curriculum order. CL is also used in NLP within tasks such as Question Answering (Sachan and Xing, 2016, 2018) and NLG for Answer Generation (Liu et al., 2018). For Sentiment Analysis, (Cirik et al., 2016) propose a strategy derived from sentence length, where smaller sentences are considered easier and are provided first. (Han and Myaeng, 2017) provide a tree-structured curriculum based on semantic similarity between new samples and samples already trained. (Tsvetkov et al., 2016) suggest a curriculum based on handcrafted semantic, linguistic, syntactic features for word representation learning. Some of these works (Cirik et al., 2016; Han and Myaeng, 2017; Rao et al., 2020) have suggested that Baby Steps performs better than One Pass. We 117 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 117–128 April 19, 2021. ©2021 Association for Computational Linguistics perform experiments using both techniques. While the idea of curriculum remains the same across these works, the strategy itself to decide"
2021.wat-1.19,P11-2027,0,0.0131273,"rning rate scheduling. We validate the models every epoch and select the best checkpoint after each training based on the best validation BLEU score. To train our systems efficiently, we prune the vocabulary of our model by removing the tokens which do not appear in any of the datasets mentioned in the previous section. While decoding, we use beam search with a beam size of 5. 4 Results and Discussion The BLEU score (Papineni et al., 2002) is the official metric for evaluating the performance of the models in the leaderboard. The leaderboard further uses RIBES (Isozaki et al., 2010) and AMFM (Banchs and Li, 2011) metrics for the evaluations. We report the performance of our models after tokenizing the Hindi outputs using indic-tokenizer4 in Table 2. It can be seen that our model is able to generalize well on the challenge set as well and performs better than other systems by a large margin. To 2 We use the implementation available in Detectron2 (https://github.com/facebookresearch/ detectron2). 168 3 https://github.com/pytorch/fairseq https://github.com/ltrc/ indic-tokenizer 4 English Sentence A large pipe extending from the wall of the court. English Sentence A person riding a motorcycle. Hindi Trans"
2021.wat-1.19,W18-6402,0,0.0178486,"arning model in this direction, and later works utilize transformer-based approaches (Vaswani et al., 2017; Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020) for the problem. Multimodal translation aims to use the visual modality with the source text to help create a better context of the source text. Specia et al. (2016) first conducted a shared task on the problem and released the dataset, Multi30K (Elliott et al., 2016). It is an extended German version of Flickr30K (Young et al., 2014), which was further extended to French and Czech (Elliott et al., 2017; Barrault et al., 2018). For multimodal translation between English and Hindi, Parida et al. (2019) propose a subset of Visual Genome dataset (Krishna et al., 2017) and provide parallel sentences for each of the captions. Although both English and Hindi are spoken by a large number of people around the world, there has been limited research in this direction. Dutta Chowdhury et al. (2018) created a synthetic dataset for multimodal translation of the language pair and further used the system proposed by Calixto and Liu (2017). Later, Sanayai Meetei et al. (2019) work with the same architecture on the multimodal trans"
2021.wat-1.19,N19-1422,0,0.0319721,"Missing"
2021.wat-1.19,D17-1105,0,0.0157558,"(Young et al., 2014), which was further extended to French and Czech (Elliott et al., 2017; Barrault et al., 2018). For multimodal translation between English and Hindi, Parida et al. (2019) propose a subset of Visual Genome dataset (Krishna et al., 2017) and provide parallel sentences for each of the captions. Although both English and Hindi are spoken by a large number of people around the world, there has been limited research in this direction. Dutta Chowdhury et al. (2018) created a synthetic dataset for multimodal translation of the language pair and further used the system proposed by Calixto and Liu (2017). Later, Sanayai Meetei et al. (2019) work with the same architecture on the multimodal translation task in WAT 2019. Laskar et al. (2019) used a doubly attentive RNN-based encoder and decoder architecture (Calixto and Liu, 2017; Calixto et al., 2017). Laskar et al. (2020) also proposed a similar architecture and pretrained on a large textual parallel dataset (Kunchukuttan et al., 2018) in their system. Train 28,930 4.95 5.03 Dataset Description We use the dataset provided by the shared task organizers (Parida et al., 2019), which consists of images and their associated English captions from V"
2021.wat-1.19,P17-1175,0,0.0412598,"Missing"
2021.wat-1.19,W18-3405,0,0.0163897,"ducted a shared task on the problem and released the dataset, Multi30K (Elliott et al., 2016). It is an extended German version of Flickr30K (Young et al., 2014), which was further extended to French and Czech (Elliott et al., 2017; Barrault et al., 2018). For multimodal translation between English and Hindi, Parida et al. (2019) propose a subset of Visual Genome dataset (Krishna et al., 2017) and provide parallel sentences for each of the captions. Although both English and Hindi are spoken by a large number of people around the world, there has been limited research in this direction. Dutta Chowdhury et al. (2018) created a synthetic dataset for multimodal translation of the language pair and further used the system proposed by Calixto and Liu (2017). Later, Sanayai Meetei et al. (2019) work with the same architecture on the multimodal translation task in WAT 2019. Laskar et al. (2019) used a doubly attentive RNN-based encoder and decoder architecture (Calixto and Liu, 2017; Calixto et al., 2017). Laskar et al. (2020) also proposed a similar architecture and pretrained on a large textual parallel dataset (Kunchukuttan et al., 2018) in their system. Train 28,930 4.95 5.03 Dataset Description We use the"
2021.wat-1.19,N19-1409,0,0.0119168,"ed dataset. The average number of tokens in the source and target language are reported for all the sentence pairs. 3 System Overview In this section, we describe the systems we use for the task. Related Work 3.1 Earlier works in the field of machine translation largely used statistical or rule-based approaches, while neural machine translation has gained popularity in the recent past. Kalchbrenner and Blunsom (2013) released the first deep learning model in this direction, and later works utilize transformer-based approaches (Vaswani et al., 2017; Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020) for the problem. Multimodal translation aims to use the visual modality with the source text to help create a better context of the source text. Specia et al. (2016) first conducted a shared task on the problem and released the dataset, Multi30K (Elliott et al., 2016). It is an extended German version of Flickr30K (Young et al., 2014), which was further extended to French and Czech (Elliott et al., 2017; Barrault et al., 2018). For multimodal translation between English and Hindi, Parida et al. (2019) propose a subset of Visual Genome dataset (Krishna et al., 2017) and prov"
2021.wat-1.19,W17-4718,0,0.0169135,"ased the first deep learning model in this direction, and later works utilize transformer-based approaches (Vaswani et al., 2017; Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020) for the problem. Multimodal translation aims to use the visual modality with the source text to help create a better context of the source text. Specia et al. (2016) first conducted a shared task on the problem and released the dataset, Multi30K (Elliott et al., 2016). It is an extended German version of Flickr30K (Young et al., 2014), which was further extended to French and Czech (Elliott et al., 2017; Barrault et al., 2018). For multimodal translation between English and Hindi, Parida et al. (2019) propose a subset of Visual Genome dataset (Krishna et al., 2017) and provide parallel sentences for each of the captions. Although both English and Hindi are spoken by a large number of people around the world, there has been limited research in this direction. Dutta Chowdhury et al. (2018) created a synthetic dataset for multimodal translation of the language pair and further used the system proposed by Calixto and Liu (2017). Later, Sanayai Meetei et al. (2019) work with the same architecture"
2021.wat-1.19,W16-3210,0,0.0546075,"Missing"
2021.wat-1.19,D10-1092,0,0.0328364,"oothing and polynomial decay learning rate scheduling. We validate the models every epoch and select the best checkpoint after each training based on the best validation BLEU score. To train our systems efficiently, we prune the vocabulary of our model by removing the tokens which do not appear in any of the datasets mentioned in the previous section. While decoding, we use beam search with a beam size of 5. 4 Results and Discussion The BLEU score (Papineni et al., 2002) is the official metric for evaluating the performance of the models in the leaderboard. The leaderboard further uses RIBES (Isozaki et al., 2010) and AMFM (Banchs and Li, 2011) metrics for the evaluations. We report the performance of our models after tokenizing the Hindi outputs using indic-tokenizer4 in Table 2. It can be seen that our model is able to generalize well on the challenge set as well and performs better than other systems by a large margin. To 2 We use the implementation available in Detectron2 (https://github.com/facebookresearch/ detectron2). 168 3 https://github.com/pytorch/fairseq https://github.com/ltrc/ indic-tokenizer 4 English Sentence A large pipe extending from the wall of the court. English Sentence A person r"
2021.wat-1.19,D13-1176,0,0.011605,"stly, we conduct a thorough error analysis of our systems and conclude with a direction for future work. 2 Valid Test Challenge 998 4.93 4.99 1,595 4.92 4.92 1,400 5.85 6.17 Table 1: The statistics of the provided dataset. The average number of tokens in the source and target language are reported for all the sentence pairs. 3 System Overview In this section, we describe the systems we use for the task. Related Work 3.1 Earlier works in the field of machine translation largely used statistical or rule-based approaches, while neural machine translation has gained popularity in the recent past. Kalchbrenner and Blunsom (2013) released the first deep learning model in this direction, and later works utilize transformer-based approaches (Vaswani et al., 2017; Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020) for the problem. Multimodal translation aims to use the visual modality with the source text to help create a better context of the source text. Specia et al. (2016) first conducted a shared task on the problem and released the dataset, Multi30K (Elliott et al., 2016). It is an extended German version of Flickr30K (Young et al., 2014), which was further extended to French and Cz"
2021.wat-1.19,L18-1548,0,0.483982,"ity of such datasets hinders building robust systems for multimodal translation. To address these issues, we propose to bring the visual information to a textual domain and fine-tune a high resource unimodal translation system to incorporate the added information in the input. We add the visual information by extracting the object classes by using an object detector and add them as tags to the source text. Further, we use mBART, a pretrained multilingual sequenceto-sequence model, as the base architecture for our translation system. We fine-tune the model on a textual-only dataset released by Kunchukuttan et al. (2018) consisting of 1,609,682 parallel sentences in English and Hindi. Further, we finetune it on the training set enriched with the object tags extracted from the images. We achieve state-of-the-art performance on the given dataset. The code for our proposed system is available at https://github.com/kshitij98/vita. The main contributions of our work are as follows: • We explore the effectiveness of fine-tuning mBART to translate English sentences to Hindi in the text-only domain. • We further propose a multimodal system for translation by enriching the input with the object tags extracted from the"
2021.wat-1.19,D19-5205,0,0.0167538,"ion between English and Hindi, Parida et al. (2019) propose a subset of Visual Genome dataset (Krishna et al., 2017) and provide parallel sentences for each of the captions. Although both English and Hindi are spoken by a large number of people around the world, there has been limited research in this direction. Dutta Chowdhury et al. (2018) created a synthetic dataset for multimodal translation of the language pair and further used the system proposed by Calixto and Liu (2017). Later, Sanayai Meetei et al. (2019) work with the same architecture on the multimodal translation task in WAT 2019. Laskar et al. (2019) used a doubly attentive RNN-based encoder and decoder architecture (Calixto and Liu, 2017; Calixto et al., 2017). Laskar et al. (2020) also proposed a similar architecture and pretrained on a large textual parallel dataset (Kunchukuttan et al., 2018) in their system. Train 28,930 4.95 5.03 Dataset Description We use the dataset provided by the shared task organizers (Parida et al., 2019), which consists of images and their associated English captions from Visual Genome (Krishna et al., 2017) along with the Hindi translations of the captions. The dataset also provides a challenge test which co"
2021.wat-1.19,2020.acl-main.703,0,0.0277582,"s a challenge test which consists of sentences where there are ambiguous English words, and the image can help in resolving the ambiguity. The statistics of the dataset are shown in Table 1. We use the provided dataset splits for training our models. We also use the dataset released by Kunchukuttan et al. (2018) which consists of parallel sentences in English and Hindi. We use the training set, which contains 1,609,682 sentences, for training our systems. 3.2 Model We fine-tune mBART, which is a multilingual sequence-to-sequence denoising auto-encoder that has been pre-trained using the BART (Lewis et al., 2020) objective on large-scale monolingual corpora of 25 languages, including both English and Hindi. The pre-training corpus consists of 55,608 million English tokens (300.8 GB) and 1,715 million Hindi tokens (20.2 GB). Its architecture is a standard sequence-to-sequence Transformer (Vaswani et al., 2017), with 12 encoder and decoder layers each and a model dimension of 1024 on 16 heads resulting in ∼680 million parameters. To train our systems efficiently, we prune mBART’s vocabulary by removing the tokens which are not present in the provided dataset or the dataset released by Kunchukuttan et al"
2021.wat-1.19,2020.tacl-1.47,0,0.016356,"ge number of tokens in the source and target language are reported for all the sentence pairs. 3 System Overview In this section, we describe the systems we use for the task. Related Work 3.1 Earlier works in the field of machine translation largely used statistical or rule-based approaches, while neural machine translation has gained popularity in the recent past. Kalchbrenner and Blunsom (2013) released the first deep learning model in this direction, and later works utilize transformer-based approaches (Vaswani et al., 2017; Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020) for the problem. Multimodal translation aims to use the visual modality with the source text to help create a better context of the source text. Specia et al. (2016) first conducted a shared task on the problem and released the dataset, Multi30K (Elliott et al., 2016). It is an extended German version of Flickr30K (Young et al., 2014), which was further extended to French and Czech (Elliott et al., 2017; Barrault et al., 2018). For multimodal translation between English and Hindi, Parida et al. (2019) propose a subset of Visual Genome dataset (Krishna et al., 2017) and provide parallel senten"
2021.wat-1.19,N19-4009,0,0.032164,"Missing"
2021.wat-1.19,P02-1040,0,0.109208,"ens per GPU. We use the Adam optimizer ( = 10−6 , β1 = 0.9, β2 = 0.98) (Kingma and Ba, 2015) with 0.1 attention dropout, 0.3 dropout, 0.2 label smoothing and polynomial decay learning rate scheduling. We validate the models every epoch and select the best checkpoint after each training based on the best validation BLEU score. To train our systems efficiently, we prune the vocabulary of our model by removing the tokens which do not appear in any of the datasets mentioned in the previous section. While decoding, we use beam search with a beam size of 5. 4 Results and Discussion The BLEU score (Papineni et al., 2002) is the official metric for evaluating the performance of the models in the leaderboard. The leaderboard further uses RIBES (Isozaki et al., 2010) and AMFM (Banchs and Li, 2011) metrics for the evaluations. We report the performance of our models after tokenizing the Hindi outputs using indic-tokenizer4 in Table 2. It can be seen that our model is able to generalize well on the challenge set as well and performs better than other systems by a large margin. To 2 We use the implementation available in Detectron2 (https://github.com/facebookresearch/ detectron2). 168 3 https://github.com/pytorch/"
2021.wat-1.19,D19-5224,0,0.025241,"r extended to French and Czech (Elliott et al., 2017; Barrault et al., 2018). For multimodal translation between English and Hindi, Parida et al. (2019) propose a subset of Visual Genome dataset (Krishna et al., 2017) and provide parallel sentences for each of the captions. Although both English and Hindi are spoken by a large number of people around the world, there has been limited research in this direction. Dutta Chowdhury et al. (2018) created a synthetic dataset for multimodal translation of the language pair and further used the system proposed by Calixto and Liu (2017). Later, Sanayai Meetei et al. (2019) work with the same architecture on the multimodal translation task in WAT 2019. Laskar et al. (2019) used a doubly attentive RNN-based encoder and decoder architecture (Calixto and Liu, 2017; Calixto et al., 2017). Laskar et al. (2020) also proposed a similar architecture and pretrained on a large textual parallel dataset (Kunchukuttan et al., 2018) in their system. Train 28,930 4.95 5.03 Dataset Description We use the dataset provided by the shared task organizers (Parida et al., 2019), which consists of images and their associated English captions from Visual Genome (Krishna et al., 2017) a"
2021.wat-1.19,W16-2346,0,0.0557614,"Missing"
2021.wat-1.19,Q14-1006,0,0.0130134,"d popularity in the recent past. Kalchbrenner and Blunsom (2013) released the first deep learning model in this direction, and later works utilize transformer-based approaches (Vaswani et al., 2017; Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020) for the problem. Multimodal translation aims to use the visual modality with the source text to help create a better context of the source text. Specia et al. (2016) first conducted a shared task on the problem and released the dataset, Multi30K (Elliott et al., 2016). It is an extended German version of Flickr30K (Young et al., 2014), which was further extended to French and Czech (Elliott et al., 2017; Barrault et al., 2018). For multimodal translation between English and Hindi, Parida et al. (2019) propose a subset of Visual Genome dataset (Krishna et al., 2017) and provide parallel sentences for each of the captions. Although both English and Hindi are spoken by a large number of people around the world, there has been limited research in this direction. Dutta Chowdhury et al. (2018) created a synthetic dataset for multimodal translation of the language pair and further used the system proposed by Calixto and Liu (2017"
2021.woah-1.14,P19-1041,0,0.245477,"ces such as anxiety and isolation from the community, which can, in turn, lead to suicidal behaviour.1 Various attempts have been made to detect such harassment (Dadvar et al., 2013; Chatzakou et al., 2019) and hate speech (Davidson et al., 2017; Badjatiya et al., 2017) in the past, but very few have attempted to transfer the negative aspect of such speech. Recently, many new tasks have been introduced in the domain of text style transfer. However, since parallel corpora is usually not available, most style transfer approaches adopt an unsupervised manner (Li et al., 2018; Zhang et al., 2018; John et al., 2019; Wang et al., 2019) . We contribute a dataset of non-parallel sentences, each sentence being either an insult or a compliment, collected from Reddit, more specifically, from three subreddits r/RoastMe, 1 Figure 1: Examples of indirect insults and compliments with attributes highlighted in bold Source: https://www.stopbullying.gov/resources/facts r/ToastMe, and r/FreeCompliments. Some examples of such sentences can be seen in Figure 1. With a diverse range of online communication platforms being introduced across the world, and existing platforms’ user-bases growing at a fast pace, moderation"
2021.woah-1.14,2020.semeval-1.290,1,0.720341,"r face, hair, and eyes2 . We also propose to use classification models to detect and style transfer models to convert such targeted negative comments, often associated with online harassment, in which menacing or insulting messages are sent by direct messages or posted on social media. We also perform benchmarking experiments using existing state-of-the-art models on both fronts and analyse its results. 2 Related Work Existing work primarily focuses on the detection of offensive language or hate speech on social media using classification models (Davidson et al., 2017; Badjatiya et al., 2017; Dadu and Pant, 2020), and not on transferring the negative aspect of such speech into a positive counterpart. Detection usually involves either lexical or rule-based approach (P´erez et al., 2012; Serra and Venter, 2011), or more recently, a supervised learning approach (Yin et al., 2009; Dinakar et al., 2011). Many attempts on detection of specific types of toxic speech have also been attempted (Basile et al., 2019; Zampieri et al., 2020). Previous work on text style transfer has largely focused on transferring attributes of sentiment in reviews (Li et al., 2018; Hu et al., 2017; Pant et al., 2020) or converting"
2021.woah-1.14,N19-1423,0,0.0347953,"We also discuss about the challenges faced and the metrics used for evaluation. 4.1 Models For classification experiments, we experiment using the following models: 1. Logistic Regression: One of the most common classification algorithms used, Logistic Regression (LR) uses the logistic (sigmoid) function to return a probability value that can be further mapped to multiple classes. 2. SVM: Support Vector Machines (SVM) use an objective function that finds a hyperplane in an N dimensional space, where N is the number of features, which distinctly separates the data points into classes. 3. BERT (Devlin et al., 2019): Bidirectional Encoder Representations from Transformers or BERT is a relatively recent transformer-based model, which leverages transfer learning. At the time of release, BERT outperformed several other models in language modeling tasks. 4. RoBERTa (Liu et al., 2019): RoBERTa improves on BERT by modifying several hyperparameters and performs pretraining on larger amounts of data for a longer amount of time. 135 5. XLNet (Yang et al., 2019): While BERT and RoBERTa are categorized as autoencoder language models, XLNet is a generalized autoregressive pretraining method. Instead of using Masked"
2021.woah-1.14,W11-2123,0,0.0107261,"ing the well-known metrics of Accuracy, Precision, Recall and F1-Score. For the style transfer experiments, we evaluate the performance on three different aspects, following previous works: 1. Style Transfer Intensity: We train a separate fastText model (Joulin et al., 2017) on the training data and evaluate the different model outputs to determine the accuracy of style transfer. 2. Content Preservation: We use BLEU as an evaluation metric and utilize the SacreBLEU (Post, 2018) implementation. 3. Fluency: We calculate fluency using a language model from the KenLM library for our experiments. (Heafield, 2011) after training the language model on the target domain (compliment). A lower perplexity indicates a more fluent sentence and vice-versa. Apart from automatic evaluation, we also do human evaluation on 280 sentences randomly selected from the test set. The evaluators were asked to rank sentences on basis of their fluency and degree of being a compliment (DOC) on a scale of 1 to 5. Two annotators were shown a list of sentences, with no indication of the source of the sentence. The Cohen’s Kappa metric (Cohen, 1960) was used to measure the agreement between the two annotators. The value of kappa"
2021.woah-1.14,E17-2068,0,0.0462745,"0 0.979 0.967 0.971 0.970 Recall 0.897 0.851 0.974 0.973 0.973 F1 0.883 0.818 0.970 0.971 0.972 Model StyleEmb RetrieveOnly DeleteRetrieve LingST Tag&Gen Model Input StyleEmb RetrieveOnly DeleteRetrieve LingST Tag&Gen This has recently been utilized for the politeness transfer task. Evaluation For the classification experiments, we evaluate using the well-known metrics of Accuracy, Precision, Recall and F1-Score. For the style transfer experiments, we evaluate the performance on three different aspects, following previous works: 1. Style Transfer Intensity: We train a separate fastText model (Joulin et al., 2017) on the training data and evaluate the different model outputs to determine the accuracy of style transfer. 2. Content Preservation: We use BLEU as an evaluation metric and utilize the SacreBLEU (Post, 2018) implementation. 3. Fluency: We calculate fluency using a language model from the KenLM library for our experiments. (Heafield, 2011) after training the language model on the target domain (compliment). A lower perplexity indicates a more fluent sentence and vice-versa. Apart from automatic evaluation, we also do human evaluation on 280 sentences randomly selected from the test set. The eva"
2021.woah-1.14,L18-1102,0,0.0311553,"t (Li et al., 2018; Sudhakar et al., 2019) which separates content from style attributes in an explicit manner and then combines the separated content with the target attribute and pass it through a generator. 2) Disentanglement in latent space (John et al., 2019) which tries to separate style from content within the embedding space by using suitable objective functions. 3) Adversarial or reinforcement learning based (Luo et al., 2019) approaches in which disentanglement may not be even required. Reddit has been widely used in multiple natural language processing tasks as a data source. While Khodak et al. (2018) use Reddit to create a large corpus for sarcasm, Nogueira dos Santos et al. (2018) source their data from r/Politics on Reddit along with Twitter. Many controversial subreddits such r/The Donald have been used for detection of hate speech in the past (Qian et al., 2019). Although Nogueira dos Santos et al. (2018) proposed the task of translating offensive sentences to non-offensive ones using style transfer, in our work, we go one step further and propose to convert offensive sentences into positive compliments. Prior work on r/RoastMe has mostly been on a sociopragmatic perspective (Dynel an"
2021.woah-1.14,N18-1169,0,0.406596,"emotions leading to adverse consequences such as anxiety and isolation from the community, which can, in turn, lead to suicidal behaviour.1 Various attempts have been made to detect such harassment (Dadvar et al., 2013; Chatzakou et al., 2019) and hate speech (Davidson et al., 2017; Badjatiya et al., 2017) in the past, but very few have attempted to transfer the negative aspect of such speech. Recently, many new tasks have been introduced in the domain of text style transfer. However, since parallel corpora is usually not available, most style transfer approaches adopt an unsupervised manner (Li et al., 2018; Zhang et al., 2018; John et al., 2019; Wang et al., 2019) . We contribute a dataset of non-parallel sentences, each sentence being either an insult or a compliment, collected from Reddit, more specifically, from three subreddits r/RoastMe, 1 Figure 1: Examples of indirect insults and compliments with attributes highlighted in bold Source: https://www.stopbullying.gov/resources/facts r/ToastMe, and r/FreeCompliments. Some examples of such sentences can be seen in Figure 1. With a diverse range of online communication platforms being introduced across the world, and existing platforms’ user-ba"
2021.woah-1.14,2020.acl-main.169,0,0.0304284,"Missing"
2021.woah-1.14,P18-2031,0,0.0267628,"butes in an explicit manner and then combines the separated content with the target attribute and pass it through a generator. 2) Disentanglement in latent space (John et al., 2019) which tries to separate style from content within the embedding space by using suitable objective functions. 3) Adversarial or reinforcement learning based (Luo et al., 2019) approaches in which disentanglement may not be even required. Reddit has been widely used in multiple natural language processing tasks as a data source. While Khodak et al. (2018) use Reddit to create a large corpus for sarcasm, Nogueira dos Santos et al. (2018) source their data from r/Politics on Reddit along with Twitter. Many controversial subreddits such r/The Donald have been used for detection of hate speech in the past (Qian et al., 2019). Although Nogueira dos Santos et al. (2018) proposed the task of translating offensive sentences to non-offensive ones using style transfer, in our work, we go one step further and propose to convert offensive sentences into positive compliments. Prior work on r/RoastMe has mostly been on a sociopragmatic perspective (Dynel and Poppi, 2019; Kasunic and Kaufman, 2018). However, there is no previous work that"
2021.woah-1.14,W18-6319,0,0.0121582,"n This has recently been utilized for the politeness transfer task. Evaluation For the classification experiments, we evaluate using the well-known metrics of Accuracy, Precision, Recall and F1-Score. For the style transfer experiments, we evaluate the performance on three different aspects, following previous works: 1. Style Transfer Intensity: We train a separate fastText model (Joulin et al., 2017) on the training data and evaluate the different model outputs to determine the accuracy of style transfer. 2. Content Preservation: We use BLEU as an evaluation metric and utilize the SacreBLEU (Post, 2018) implementation. 3. Fluency: We calculate fluency using a language model from the KenLM library for our experiments. (Heafield, 2011) after training the language model on the target domain (compliment). A lower perplexity indicates a more fluent sentence and vice-versa. Apart from automatic evaluation, we also do human evaluation on 280 sentences randomly selected from the test set. The evaluators were asked to rank sentences on basis of their fluency and degree of being a compliment (DOC) on a scale of 1 to 5. Two annotators were shown a list of sentences, with no indication of the source of"
2021.woah-1.14,D19-1482,0,0.0258941,"ies to separate style from content within the embedding space by using suitable objective functions. 3) Adversarial or reinforcement learning based (Luo et al., 2019) approaches in which disentanglement may not be even required. Reddit has been widely used in multiple natural language processing tasks as a data source. While Khodak et al. (2018) use Reddit to create a large corpus for sarcasm, Nogueira dos Santos et al. (2018) source their data from r/Politics on Reddit along with Twitter. Many controversial subreddits such r/The Donald have been used for detection of hate speech in the past (Qian et al., 2019). Although Nogueira dos Santos et al. (2018) proposed the task of translating offensive sentences to non-offensive ones using style transfer, in our work, we go one step further and propose to convert offensive sentences into positive compliments. Prior work on r/RoastMe has mostly been on a sociopragmatic perspective (Dynel and Poppi, 2019; Kasunic and Kaufman, 2018). However, there is no previous work that uses r/RoastMe as a data source in a style transfer task to the best of our knowledge. 3 The JDC Dataset We contribute the Jibe and Delight Corpus (JDC), a new non-parallel style transfer"
2021.woah-1.14,W16-5603,0,0.0297133,"´erez et al., 2012; Serra and Venter, 2011), or more recently, a supervised learning approach (Yin et al., 2009; Dinakar et al., 2011). Many attempts on detection of specific types of toxic speech have also been attempted (Basile et al., 2019; Zampieri et al., 2020). Previous work on text style transfer has largely focused on transferring attributes of sentiment in reviews (Li et al., 2018; Hu et al., 2017; Pant et al., 2020) or converting factual captions to humorous or romantic ones (Li et al., 2018). Other tasks include transferring formality (Xu et al., 2019) or gender or political style (Reddy and Knight, 2016). Recently, transferring politeness has also been proposed by Madaan et al. (2020). Most approaches use unsupervised methods 2 Made available at https://github.com/ravsimar-sodhi/jibesand-delights since parallel data is usually not available. These approaches can be broadly divided into three groups: 1) Explicit disentanglement (Li et al., 2018; Sudhakar et al., 2019) which separates content from style attributes in an explicit manner and then combines the separated content with the target attribute and pass it through a generator. 2) Disentanglement in latent space (John et al., 2019) which t"
2021.woah-1.14,D19-1322,0,0.016386,"iews (Li et al., 2018; Hu et al., 2017; Pant et al., 2020) or converting factual captions to humorous or romantic ones (Li et al., 2018). Other tasks include transferring formality (Xu et al., 2019) or gender or political style (Reddy and Knight, 2016). Recently, transferring politeness has also been proposed by Madaan et al. (2020). Most approaches use unsupervised methods 2 Made available at https://github.com/ravsimar-sodhi/jibesand-delights since parallel data is usually not available. These approaches can be broadly divided into three groups: 1) Explicit disentanglement (Li et al., 2018; Sudhakar et al., 2019) which separates content from style attributes in an explicit manner and then combines the separated content with the target attribute and pass it through a generator. 2) Disentanglement in latent space (John et al., 2019) which tries to separate style from content within the embedding space by using suitable objective functions. 3) Adversarial or reinforcement learning based (Luo et al., 2019) approaches in which disentanglement may not be even required. Reddit has been widely used in multiple natural language processing tasks as a data source. While Khodak et al. (2018) use Reddit to create"
2021.woah-1.14,2020.emnlp-demos.6,0,0.0798983,"Missing"
D19-5524,D11-1145,0,0.0511195,"ysis on the consumer side, for female smokers on Instagram, targeting the same age group, but based entirely on feature extraction on images, particularly selfies. More recently, Malik et al. (2019) explored patterns of communication of e-cigarette company Juul use on Twitter. They categorized 1008 randomly selected tweets across four dimensions, namely, user type, sentiment, genre, theme. However, they explore the effects of only Juul, and not other cigarettes or e-cigarettes, further limiting their experiment to only Juul-based analysis and inferences. In the domain of Disease Surveillance, Aramaki et al. (2011) explored the problem of identifying influenza epidemics using machine-learning based tweet classifiers along with search engine trends Related Work Mysl´ın et al. (2013) explored content and senti3 https://github.com/kartikeypant/ smokeng-tobacco-classification 182 Name Label Mention of Non-Tobacco Drugs Unrelated or Ambiguous Mention Personal or Anecdotal Mention Informative or Advisory Mention Advertisements OD UM PM IM AD Annotation Class -1 0 1 2 3 Table 1: Label and ID associated with each class. for medical keywords and medical records for the disease in a local environment. For doing s"
D19-5524,N19-1423,0,0.0169258,"Missing"
D19-5524,P19-1194,0,0.0463918,"Missing"
D19-5524,D14-1162,0,0.0836118,"of tobacco and e-cigarette use or consumption – mention associated health risks or benefits – portray the use of tobacco products or ecigarettes by a public figure – emphasize social campaigns for antismoking, smoking cessation and related products such as patches 4 In this section we describe the classifiers designed for this task of fine grained classification. The classifier architecture is based upon a combination of choosing word representations, along with a discriminator that is compatible with that representation. We use the TF-IDF for the suport vector machines and GloVe embeddings (Pennington et al., 2014) with our convolutional neural network architecture and recurrent architectures (LSTM and Bi-LSTM). We also used FastText and BERT embeddings (both base and large) with their native classifiers to note the change in the accuracies. • Advertisements: All tweets written with the intent of the sale of tobacco products, ecigarettes and associated products or services are marked advertisements. In this classification, intent is considered using the mention of price as an objective measure. • Mention of Non-Tobacco Drugs: Tweets which mention the use, sale, anecdotes and information about drugs othe"
D19-5524,Q17-1010,0,\N,Missing
D19-5524,E17-2068,0,\N,Missing
D19-6126,P17-2090,0,0.0136697,"stion X is obtained. There is a lack of dialogue conversational data in Telugu. But, any deep learning technique requires some considerable amount of data for training. And due to this, 388 natural language questions were created initially. Since the categories of the questions asked are finite, the questions posed are also limited. But the 388 questions are not sufficient for training a question classifier of 11 classes. Therefore, we performed Data Augmentation which led to a considerable amount of question data that could be used for training the classifier. This idea has been inspired by (Fadaee et al., 2017) and has been modified according to our requirement. 4.1 Experiments Multiple experiments using various deep learning models and machine learning approaches were performed for the question classification task. The results are shown in table 1. 4.1.1 Support Vector Machine SVM(Cortes and Vapnik, 1995) is one of the most popular machine learning classifier. The question representation X is used as the input for SVM. The final question representations are the main features on which SVM is trained. Data augmentation is done by making slight changes in the already present data to create more data."
D19-6126,W18-5048,0,0.0144493,"-based system. It was one of the first systems to facilitate conversation between man and computer in natural language. Another such early rule-based dialogue system was PARRY (Colby et al., 1971). It was the first dialogue system to pass the Turing Test. There are other systems like (Chung, 2004), (Zue et al., 2000) and (Ferguson and Allen, 1998) which are mixed-initiative and domain-specific systems. They operate and deliver information only related to a particular domain. In contrast, there are also generic dialogue system architectures which can adapt to domains. (ALLEN et al., 2000) and (Galescu et al., 2018) propose such architectures. Another kind of dialogue systems is data-driven dialogue systems. They mine conversations from the already available dialogue-corpus. (Serban et al., 2015a), (Jafarpour and Burges, 2010), (Ritter et al., 2011) and (Leuski and Traum, 2011) are some of the systems which are data-driven. They mainly extract the relevant required response using Information Retrieval techniques. 235 4 • • • • • • Timings of availability of the doctor Specialization of the doctor Qualification of the doctor Experience of the doctor Consultancy fees of the doctor Checking the availability"
D19-6126,W06-1908,0,0.0322321,"eepLo), pages 234–242 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 There is another kind of dialogue systems like (Fujie et al., 2019) which mainly work with the user feedback combined with any other technique. This helps in the evolution and learning of the dialogue system. There are also some notable dialogue system like (Vinyals and Le, 2015), (Ritter et al., 2010), (Serban et al., 2015b) and (Mutiwokuziva et al., 2017) which are based on neural networks and deep learning. In Telugu, the first dialogue system is (Nandi Reddy and Bandyopadhyay, 2006) and it uses computational rules and frames for answer generation. Another dialogue system in Telugu is (Ch. Sravanthi et al., 2015). The authors use various complex linguistic properties of the question to understand the meaning of the query and then process it accordingly. question is known, we process the question using Named Entity Recognition(NER) and extract all the relevant details which are required to answer the question belonging to the particular category. If the information is sufficient to answer the question, then using it, an SQL query is built for retrieving the data which is r"
D19-6126,D11-1054,0,0.0221529,"the Turing Test. There are other systems like (Chung, 2004), (Zue et al., 2000) and (Ferguson and Allen, 1998) which are mixed-initiative and domain-specific systems. They operate and deliver information only related to a particular domain. In contrast, there are also generic dialogue system architectures which can adapt to domains. (ALLEN et al., 2000) and (Galescu et al., 2018) propose such architectures. Another kind of dialogue systems is data-driven dialogue systems. They mine conversations from the already available dialogue-corpus. (Serban et al., 2015a), (Jafarpour and Burges, 2010), (Ritter et al., 2011) and (Leuski and Traum, 2011) are some of the systems which are data-driven. They mainly extract the relevant required response using Information Retrieval techniques. 235 4 • • • • • • Timings of availability of the doctor Specialization of the doctor Qualification of the doctor Experience of the doctor Consultancy fees of the doctor Checking the availability of a doctor – At a particular time of the day – On a particular day of the week • Information about which hospital a doctor works in 3.2 Question Classification In this phase, the question posed by the user is classified into one of the"
D19-6126,N10-1020,0,0.0143602,"ystems is to provide the users with any information or help about that particular chosen do234 Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo), pages 234–242 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 There is another kind of dialogue systems like (Fujie et al., 2019) which mainly work with the user feedback combined with any other technique. This helps in the evolution and learning of the dialogue system. There are also some notable dialogue system like (Vinyals and Le, 2015), (Ritter et al., 2010), (Serban et al., 2015b) and (Mutiwokuziva et al., 2017) which are based on neural networks and deep learning. In Telugu, the first dialogue system is (Nandi Reddy and Bandyopadhyay, 2006) and it uses computational rules and frames for answer generation. Another dialogue system in Telugu is (Ch. Sravanthi et al., 2015). The authors use various complex linguistic properties of the question to understand the meaning of the query and then process it accordingly. question is known, we process the question using Named Entity Recognition(NER) and extract all the relevant details which are required t"
D19-6126,D14-1181,0,0.00251106,"used on the language. For example, mostly when the case marker ’ki’ occurs, it is preceded by time in the question. In another instance, whenever ’lo’ occurs, it is a case marker which is associated with location. We also have some definitive rules like ’gAaru’ is always followed by Random Forest(Svetnik et al., 2003) is used for question classification with the following features: 1. Bag of words of Unigrams 2. Bag of words of Bigrams 3. TFIDF Values of Bigrams 4. TFIDF Values of both Unigrams and Bigrams combined 4.1.5 Convolutional Neural Network Word embedding based model is used for CNN (Kim, 2014), the X matrix is given as input to the CNN model followed by the fully connected layer and finally a softmax layer. The filter of size 4 is used for the convolutions. 4.1.6 Bidirectional LSTM A single-layer Bidirectional LSTM (Schuster and Paliwal, 1997) is used for question classification. The representation of the question X is given as an input to the Bidirectional LSTM layer. The output of this is then fed into a Dense Layer and then finally softmax is performed. The hidden dimension of Bi-LSTM is 64. The dropout rate is set 0.4 for avoiding overfitting. 4.1.7 Long Short Term Memory A sin"
I13-1173,W08-1604,0,0.510128,"for one of the models proposed in section 5. Finally, we present our experimental results in section 8 and conclude in Section 9. 3 Problem Figure 3: Context based NLIDB system 2 Related Work Chai and Jin (2004) and Sun and Chai (2007) investigate the role of discourse modeling to track contextual information in interactive Question Answering systems. They analyzed the relations between user’s responses and proposed models based on centering theory to identify the contextual information. However, the above mentioned models fail to utilize system’s responses. Kirschner and Bernardi (2007) and Bernardi and Kirschner (2008) proposed models which utilize both user’s responses and system’s responses. But, in all these approaches, no attempt was made to understand the structure of user-system interactions. We believe that understanding the structure of user-system interactions is the key to identifying an effective model to track contextual information. Responses by both user and system in a usersystem interaction can be grouped into a set based on the information shared among them. Each individual group is called ‘local contextual group’ (LCG) and the corresponding information (i.e. information present in every us"
I13-1173,W06-3001,0,0.575944,"Missing"
I13-1173,W04-2504,0,0.864735,"09; Gupta et al., 2012). Although NLIDB systems are able to answer a wide range of natural language queries (NL queries), they are not used much in commercial applications. One of the main reasons for the less acceptance of these systems in realtime applications is that they lack robust context processing capabilities (Bertomeu et al., 2006). Currently there is very little work which explicitly aims to investigate the role of context processing capabilities in NLIDB systems. However, the importance of context processing capabilities has been explored extensively in Question Answering systems (Chai and Jin, 2004; Kato et al., 2004; Kirschner and Bernardi, 2007; Negri and Kouylekov, 2007; Kirschner and Bernardi, 2010). Users often fail to express their intention (information need) in a single NL query (user reFigure 1: An example of context based usersystem interaction For example, let us consider a user-system interaction shown in Figure 1. User responses are represented as U1, U2, etc. and system responses are represented as S1, S2, etc. In this example, to interpret U2, information present in the preceding query U1 is needed. That means information present in U1 is the contextual information for U2"
I13-1173,J82-2002,0,0.855288,"Missing"
I13-1173,2007.sigdial-1.8,0,0.0837427,"systems are able to answer a wide range of natural language queries (NL queries), they are not used much in commercial applications. One of the main reasons for the less acceptance of these systems in realtime applications is that they lack robust context processing capabilities (Bertomeu et al., 2006). Currently there is very little work which explicitly aims to investigate the role of context processing capabilities in NLIDB systems. However, the importance of context processing capabilities has been explored extensively in Question Answering systems (Chai and Jin, 2004; Kato et al., 2004; Kirschner and Bernardi, 2007; Negri and Kouylekov, 2007; Kirschner and Bernardi, 2010). Users often fail to express their intention (information need) in a single NL query (user reFigure 1: An example of context based usersystem interaction For example, let us consider a user-system interaction shown in Figure 1. User responses are represented as U1, U2, etc. and system responses are represented as S1, S2, etc. In this example, to interpret U2, information present in the preceding query U1 is needed. That means information present in U1 is the contextual information for U2. Query U3 does not depend on the information pre"
I13-1173,W10-4359,0,0.0153337,"age queries (NL queries), they are not used much in commercial applications. One of the main reasons for the less acceptance of these systems in realtime applications is that they lack robust context processing capabilities (Bertomeu et al., 2006). Currently there is very little work which explicitly aims to investigate the role of context processing capabilities in NLIDB systems. However, the importance of context processing capabilities has been explored extensively in Question Answering systems (Chai and Jin, 2004; Kato et al., 2004; Kirschner and Bernardi, 2007; Negri and Kouylekov, 2007; Kirschner and Bernardi, 2010). Users often fail to express their intention (information need) in a single NL query (user reFigure 1: An example of context based usersystem interaction For example, let us consider a user-system interaction shown in Figure 1. User responses are represented as U1, U2, etc. and system responses are represented as S1, S2, etc. In this example, to interpret U2, information present in the preceding query U1 is needed. That means information present in U1 is the contextual information for U2. Query U3 does not depend on the information present in preceding queries. Semester name ‘Monsoon 2011’ pr"
N16-1159,W14-3914,0,0.126279,"Missing"
N16-1159,W14-3902,0,0.286734,"alizer. The POS tagger uses the output of the normalizer to assign each word a POS tag. Finally, the Shallow Parser assigns a chunk label with boundary. The functionality and performance of each module is described in greater detail in the following subsections. 4.1 While language identification at the document level is a well-established task (McNamee, 2005), identifying language in social media posts has certain challenges associated to it. Spelling errors, phonetic typing, use of transliterated alphabets and abbreviations combined with code-mixing make this problem interesting. Similar to (Barman et al., 2014), we performed two experiments treating language identification as a three class (‘hi’, ‘en’, ‘rest’) classification problem. The feature set comprised of BNC: normalized frequency of the word in British National Corpus (BNC)3 . LEXNORM: binary feature indicating presence of the word in the lexical normalization dataset released by Han et al. (2011). HINDI DICT: binary feature indicating presence of the word in a dictionary of 30,823 transliterated Hindi words as released by Gupta (2012). NGRAM: word n-grams. AFFIXES: prefixes and suffixes of the word. Using these features and introducing a co"
N16-1159,P11-2008,0,0.0243967,"Missing"
N16-1159,gupta-etal-2012-mining,0,0.0191516,"d below. Both subnormalizers generated normalized candidates which were then ranked, as explained later in this subsection. 1. Noisy Channel Framework: A generative model was trained to produce noisy (unnormalized) tokens from a given normalized word. Using the model’s confidence score and the probability of the normalized word in the background corpus, n-best normalizations were chosen. First, we obtained character alignments between noisy Hindi words in Roman script (Hr ) to normalized Hindi wordsformat(Hw ) using GIZA++ (Och and Ney, 2003) on 30,823 Hindi word pairs of the form (Hw - Hr ) (Gupta et al., 2012). Next, a CRF classifier was trained over these alignments, enabling it to convert a character sequence from Roman to Devanagari using learnt letter transformations. Using this model, noisy Hr words were created for Hw words obtained from a dictionary of 1,17,789 Hindi words (Biemann et al., 2007). Finally, using the formula below, we computed the most probable Hw for a given Hr . Hw = argmaxHwi p(Hwi |Hr ) = argmaxHwi p(Hr |Hwi )p(Hwi ) where p(Hwi ) is the probability of word Hwi in the background corpus. 1343 Accuracy 69.27 70.44 72.61 73.18 73.55 75.07 Table 4: Feature Ablation for POS Tag"
N16-1159,P11-1038,0,0.0207668,"Missing"
N16-1159,D12-1039,0,0.0215036,"Missing"
N16-1159,R15-1033,0,0.122373,"Missing"
N16-1159,W01-0706,0,0.0975929,"ual speakers. Two other annotators reviewed and cleaned it. To measure interannotator agreement, another annotator read the guidelines and annotated 25 sentences (334 tokens) from scratch. The inter-annotator agreement calculated using Cohen’s κ (Cohen, 1960) came out to be 0.97, 0.83 and 0.89 for language identification, POS tagging and shallow parsing respectively. 4 Shallow Parsing Pipeline Shallow parsing is the task of identifying and segmenting text into syntactically correlated word groups (Abney, 1992; Harris, 1957). Shallow parsing is a viable alternative to full parsing as shown by (Li and Roth, 2001). Our shallow parsing pipeline is composed of four main modules, as shown in Figure 1. These modules, in the order of their usage, are Language Identification, Normalization, POS Tagger and Shallow Parser. Our pipeline takes a raw utterance in Roman script as input on which each module runs sequentially. Twokenizer2 (Owoputi et al., 2013) which 2 performs well on Hindi-English CSMT (Jamatia et al., 2015) was used to tokenize the utterance into words. The Language Identification module assigns each token a language label. Based on the language label assigned, the Normalizer runs the Hindi norma"
N16-1159,P12-1109,0,0.0291547,"Missing"
N16-1159,J03-1002,0,0.0453549,"i and other for English/Rest, had two subnormalizers each, as described below. Both subnormalizers generated normalized candidates which were then ranked, as explained later in this subsection. 1. Noisy Channel Framework: A generative model was trained to produce noisy (unnormalized) tokens from a given normalized word. Using the model’s confidence score and the probability of the normalized word in the background corpus, n-best normalizations were chosen. First, we obtained character alignments between noisy Hindi words in Roman script (Hr ) to normalized Hindi wordsformat(Hw ) using GIZA++ (Och and Ney, 2003) on 30,823 Hindi word pairs of the form (Hw - Hr ) (Gupta et al., 2012). Next, a CRF classifier was trained over these alignments, enabling it to convert a character sequence from Roman to Devanagari using learnt letter transformations. Using this model, noisy Hr words were created for Hw words obtained from a dictionary of 1,17,789 Hindi words (Biemann et al., 2007). Finally, using the formula below, we computed the most probable Hw for a given Hr . Hw = argmaxHwi p(Hwi |Hr ) = argmaxHwi p(Hr |Hwi )p(Hwi ) where p(Hwi ) is the probability of word Hwi in the background corpus. 1343 Accuracy 69"
N16-1159,N13-1039,0,0.0428979,"Missing"
N16-1159,petrov-etal-2012-universal,0,0.0251756,"Missing"
N16-1159,D08-1110,0,0.0483601,"Missing"
N16-1159,D14-1105,0,0.227709,"xt analysis of Hindi English CSMT. The pipeline is accessible at 1 . 1 2 Introduction Multilingual speakers tend to exhibit code-mixing and code-switching in their use of language on social media platforms. Code-Mixing is the embedding of linguistic units such as phrases, words or morphemes of one language into an utterance of another language whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systems (Gumperz., 1982). Here we use code-mixing to refer to both the scenarios. Hindi-English bilingual speakers produce huge amounts of CSMT. Vyas et al. (2014) noted that the complexity in analyzing CSMT stems from nonadherence to a formal grammar, spelling variations, lack of annotated data, inherent conversational nature of the text and of course, code-mixing. Therefore, there is a need to create datasets and Natural 1 http://bit.ly/csmt-parser-api Background Bali et al. (2014) gathered data from Facebook generated by English-Hindi bilingual users which on analysis, showed a significant amount of codemixing. Barman et al. (2014) investigated language identification at word level on Bengali-HindiEnglish CSMT. They annotated a corpus with more than"
P17-3012,P13-2041,0,0.0378613,"Missing"
P18-3014,D14-1125,0,0.0385282,"Missing"
P18-3014,H05-1044,0,0.0939136,"he top level of granularity, it is 99 Proceedings of ACL 2018, Student Research Workshop, pages 99–104 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics often impossible to infer the sentiment expressed about any particular entity, because a document may convey different opinions for different entities. Hence, when we consider the tasks of opinion mining where the sole aim is to capture the sentiment polarities about entities, such as products in product reviews, it has been shown that sentencelevel and phrase-level analysis lead to a performance gain (Wilson et al., 2005; Choi and Wiebe, 2014). In the context of Indian languages, (Das et al., 2012) proposes an alternate way to build the resources for multilingual affect analysis where translations into Telugu are done using WordNet. works for Telugu sentiment analysis using sentence-level annotations who developed annotated corpora. Ours is the first of it’s kind NLP research which uses sentiment annotation of bi-grams for sentiment analysis (opinion mining). 3 Building the Benchmark Corpus Lexicons play an important role in sentiment analysis. Having annotated lexicon is key to carry out sentiment analysis e"
P18-3014,W10-3208,0,0.711205,"at annotated lexicons are of immense importance in any language for sentiment analysis (a.k.a opinion mining). For our experiments, we utilize the reviews dataset from Sentiraama 1 corpus. It contains 668 reviews in total for 267 movies, 201 products and 200 books. Product reviews has 101 positive and 100 negative entries; movie reviews has 136 positive and 132 negative reviews; book reviews data has 100 positive and 100 negative entries. Since the obtained corpus is only annotated with document-level sentiment labels, we perform the word-level sentiment annotation manually. • SentiWordNet : (Das and Bandyopadhyay, 2010) proposes multiple computational techniques like, WordNet based, dictionary based, corpus based and generative approaches to generate Telugu SentiWordNet. (Das and Bandyopadhyay, 2011) proposes a tool Dr Sentiment where it automatically creates the PsychoSentiWordNet which is an extension of SentiWordNet that presently holds human psychological knowledge on a few aspects along with sentiment knowledge. • Advances in Telugu: (Naidu et al., 2017) utilizes Telugu SentiWordNet on the news corpus to perform the task of Sentiment Analysis. (Mukku and Mamidi, 2017) developed a polarity annotated corp"
P18-3014,P11-4009,0,0.724537,"nter (LTRC) International Institute of Information Technology, Hyderabad {sreekavitha.parupalli, vijjinianvesh.rao}@research.iiit.ac.in radhika.mamidi@iiit.ac.in Abstract polarity value considering the whole document, sentence-wise polarity, word-wise in some given text respectively (Naidu et al., 2017). Despite extensive research, the existing solutions and systems have a lot of scope for improvement, to meet the standards of the end users. The main problem arises while cataloging the possibly infinite set of conceptual rules that operate behind the analyzing the hidden polarity of the text (Das and Bandyopadhyay, 2011). In this paper, we perform a word-level sentiment annotation to validate the usage of such techniques for improving sentiment analysis task. Furthermore, we use word embeddings of the word-level sentiment annotated lexicon to predict the sentiment label of a document. We experiment with various machine learning algorithms to analyze the affect of word-level sentiment annotations on (document-level) sentiment analysis. The paper is organized as follows. In section 2 we discuss the previous works in the field of sentiment analysis, existing resources for Telugu and specific advances that are ma"
P18-3014,L18-1100,1,0.465709,"d negative labels are given in case of positive and negative sentiments in the word respectively. Ambiguous label is given to words which acquire sentiment based on the words it is used along with or it’s position in a sentence. Neutral label is given when the word has no sentiment in it. However, neutral and ambiguous sentiment labels are of no significant use for the task of sentiment analysis. Henceforth, those labels are ignored in our experiments. Sentiment annotations are performed on two different kinds of data. Table 1 showcases the distribution of sentiment labels at the word-level. (Gangula and Mamidi, 2018) and (Mukku and Mamidi, 2017) are the only reported 1 https://ltrc.iiit.ac.in/showfile.php? filename=downloads/sentiraama/ 100 • Unigrams: We obtain 7,663 words from Telugu SentiWordNet 2 resource to calculate the base-line accuracy of any word-level sentiment annotated model. These words are already annotated for sentiment/polarity. However, it doesn’t provide extensive coverage of Telugu. Later on, we discover a newly developed large resource of Telugu words by (Parupalli and Singh, 2018), OntoSenseNet, which has a collection of 21,000 words (adjectives+verbs+adverbs). We perform the task of"
P18-3014,W17-5408,1,0.542615,"on manually. • SentiWordNet : (Das and Bandyopadhyay, 2010) proposes multiple computational techniques like, WordNet based, dictionary based, corpus based and generative approaches to generate Telugu SentiWordNet. (Das and Bandyopadhyay, 2011) proposes a tool Dr Sentiment where it automatically creates the PsychoSentiWordNet which is an extension of SentiWordNet that presently holds human psychological knowledge on a few aspects along with sentiment knowledge. • Advances in Telugu: (Naidu et al., 2017) utilizes Telugu SentiWordNet on the news corpus to perform the task of Sentiment Analysis. (Mukku and Mamidi, 2017) developed a polarity annotated corpus where positive, negative, neutral polarities are assigned to 5410 sentences in the corpus collected from several sources. They developed a gold standard annotated corpus of Telugu sentences aimed at improving sentiment analysis in Telugu. To minimize the dependence of machine learning(ML) approaches for sentiment analysis on abundance of corpus, this paper proposes a novel method to learn representations of resource-poor languages by training them jointly with resource-rich languages using a siamese network (Choudhary et al., 2018a). A novel approach to c"
P18-3014,W02-1011,0,0.0190749,"focusing on explicit opinion expressions to addressing a type of opinion inference which is a result of opinions expressed towards events having positive or negative effects on entities. There are three ways in which one can perform sentiment analysis : document-level, sentencelevel, entity or word-level. These determine the 2 Related Work • Sentiment Analysis: Several approaches have been proposed to capture the sentiment in the text where each approach addresses the issue at different levels of granularity. Some researchers have proposed methods for document-level sentiment classification (Pang et al., 2002; Turney and Littman, 2003). At the top level of granularity, it is 99 Proceedings of ACL 2018, Student Research Workshop, pages 99–104 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics often impossible to infer the sentiment expressed about any particular entity, because a document may convey different opinions for different entities. Hence, when we consider the tasks of opinion mining where the sole aim is to capture the sentiment polarities about entities, such as products in product reviews, it has been shown that sentencelevel and phrase-level anal"
P18-3017,P13-1138,0,0.0482769,"Missing"
P18-3021,D17-1151,0,0.0214283,"e outputs is known ahead of time. In fact, recurrent neural networks, long short-term memory networks (Hochreiter and Schmidhuber, 1997), and gated recurrent neural networks (Chung et al., 2014) have become standard approaches in sequence modelling and transduction problems such as language modelling and machine translation. RNNs struggle to cope with long-term dependency in the data due to vanishing gradient problem (Hochreiter, 1998). This problem is solved using Long Short Term Memory (LSTM) recurrent neural networks. Training details All the code is written in Python 2.7 using tf-seq2seq (Britz et al., 2017), a generalpurpose encoder-decoder framework for Tensorflow (Abadi et al., 2016) deep learning library version 1.0.1. Both the encoder and the decoder are jointly trained end-to-end on the synthetic datasets we created. SCMIL has a learning rate of 0.001, batch size of 100, sequence length of 50 (characters) and number of training steps 10,000. The size of the encoder LSTM cell is 256 with one layer. The size of the decoder LSTM cell is 256 with two layers. We use Adam optimization (Kingma and Ba, 2014) for training SCMIL. The character embedding dimension are fixed to 256 and the dropout rate"
P18-3021,D15-1051,0,0.316588,"Missing"
P18-3021,P07-2045,0,0.0041137,"tch size of 100, sequence length of 50 (characters) and number of training steps 10,000. The size of the encoder LSTM cell is 256 with one layer. The size of the decoder LSTM cell is 256 with two layers. We use Adam optimization (Kingma and Ba, 2014) for training SCMIL. The character embedding dimension are fixed to 256 and the dropout rate to 0.8. 148 4 Experiments and Results randomly in the ratio 80:10:10 respectively. In all our results, the models learn over the train partition, get tuned over the dev partition, and are evaluated over the test partition. We train a SMT model using Moses (Koehn et al., 2007) on Hindi and Telugu synthetic datasets. This is our main baseline model. The standard set of features is used, including a phrase model, length penalty, jump penalty and a language model. This SMT model is trained at the character-level. Hence, the system learns mappings between character-level phrases. Moses framework allows us to easily conduct experiments with several settings and compare with SCMIL. Other baselines are character based sequenceto-sequence attention models: CNN-GRU and GRU-GRU. All the models compared in this set of experiments look at batch sizes of 100 inputs and a maximu"
R15-1042,P11-1140,0,0.0258949,"al., (2015) built a statistical sandhi splitter (SSS) which identifies and generates meaningful words in a compound word using conditional random fields (CRFs).” “Natarajan and Charniak (2011) used statistical methods like Dirichlet Process and Gibbs Sampling for Sanskrit sandhi splitting.” In the recent years, the use of hybrid systems is increasing. Hybrid systems combine both statistical and rule based techniques. “Devadath, (2014) identifies split point statistically and uses character level rules specific to language to split the compound word accordingly.” “Popovi c et al., (2006) and Macherey et al., (2011) have discussed the challenges faced in machine translation due to compound words and handled compound words within the machine translation task.” To the best of our knowledge, no one has shown the effect of sandhi splitting on various NLP applications. In this paper, we discuss the effect of SSS (which gives better performance than the existing systems in Telugu language) on three different NLP applications i.e. Machine Translation, Anaphora Resolution and Dialogue System in Telugu. The results show that the performance of these systems is better after adopting SSS. 3 Statistical Sandhi Split"
R15-1065,giordani-moschitti-2010-corpora,0,0.0358655,"Missing"
R15-1065,W08-1604,0,0.0453229,"Missing"
R15-1065,W06-3001,0,0.0505821,"Missing"
R15-1065,C12-2040,0,0.032402,"Missing"
R15-1065,W04-2504,0,0.0904815,"Missing"
R15-1065,W10-2903,0,0.026831,"Missing"
R15-1065,P06-1115,0,0.0775308,"Missing"
R15-1065,J82-2002,0,0.809079,"Missing"
R15-1065,N03-1033,0,0.048171,"Missing"
R15-1065,N06-1056,0,0.0189243,"Missing"
R15-1065,de-marneffe-etal-2006-generating,0,0.0259844,"Missing"
R15-1065,I13-1173,1,\N,Missing
S16-1158,P14-2075,0,0.0168183,"ation into exploring word difficulty for non-native English speakers. We developed two systems using Nearest Centroid Classification technique to distinguish complex words from simple words. Optimized over G-score, the presented solution obtained a G-score of 0.67, while the winner achieved a G-score of 0.77 and the average G-score of all the submitted systems in the task was 0.56. 1 Introduction Lexical Simplification aims at improving the readability and comprehensibility of text by transforming complex text into simple text. Lexical Simplification (Specia et al., 2012; Belder et al., 2010; Horn et al., 2014) is the process of replacing a word in a given context with its simplest substitute to enhance the readability of the text. The process should make sure that while replacing words with other variants, the meaning of the text is preserved. Lexical Simplification (Siddharthan, 2014) is useful to a wide variety of target audience like people with aphasia, children and also non-native speakers. Complex Word Identification (Shardlow 2013; Paetzold 2015) is considered to be the first step in the pipeline of Lexical Simplification. The overall performance of a Lexical Simplification system is thus cr"
S16-1158,N15-2002,0,0.0116469,"and comprehensibility of text by transforming complex text into simple text. Lexical Simplification (Specia et al., 2012; Belder et al., 2010; Horn et al., 2014) is the process of replacing a word in a given context with its simplest substitute to enhance the readability of the text. The process should make sure that while replacing words with other variants, the meaning of the text is preserved. Lexical Simplification (Siddharthan, 2014) is useful to a wide variety of target audience like people with aphasia, children and also non-native speakers. Complex Word Identification (Shardlow 2013; Paetzold 2015) is considered to be the first step in the pipeline of Lexical Simplification. The overall performance of a Lexical Simplification system is thus crucially dependent upon Complex Word Identification. The problem of Complex Word Identification is relatively new in the field of Natural Language Processing. However, a few approaches have been previously proposed for this task. The simplicity score (Bott et al., 2012) of a word is computed by integrating both, frequency and length of a word. They consider a threshold value and simplify words only if the word’s frequency is lower than the fixed thr"
S16-1158,P13-3015,0,0.521482,"the readability and comprehensibility of text by transforming complex text into simple text. Lexical Simplification (Specia et al., 2012; Belder et al., 2010; Horn et al., 2014) is the process of replacing a word in a given context with its simplest substitute to enhance the readability of the text. The process should make sure that while replacing words with other variants, the meaning of the text is preserved. Lexical Simplification (Siddharthan, 2014) is useful to a wide variety of target audience like people with aphasia, children and also non-native speakers. Complex Word Identification (Shardlow 2013; Paetzold 2015) is considered to be the first step in the pipeline of Lexical Simplification. The overall performance of a Lexical Simplification system is thus crucially dependent upon Complex Word Identification. The problem of Complex Word Identification is relatively new in the field of Natural Language Processing. However, a few approaches have been previously proposed for this task. The simplicity score (Bott et al., 2012) of a word is computed by integrating both, frequency and length of a word. They consider a threshold value and simplify words only if the word’s frequency is lower th"
S16-1158,S12-1046,0,0.0538598,"cation. It presents a preliminary investigation into exploring word difficulty for non-native English speakers. We developed two systems using Nearest Centroid Classification technique to distinguish complex words from simple words. Optimized over G-score, the presented solution obtained a G-score of 0.67, while the winner achieved a G-score of 0.77 and the average G-score of all the submitted systems in the task was 0.56. 1 Introduction Lexical Simplification aims at improving the readability and comprehensibility of text by transforming complex text into simple text. Lexical Simplification (Specia et al., 2012; Belder et al., 2010; Horn et al., 2014) is the process of replacing a word in a given context with its simplest substitute to enhance the readability of the text. The process should make sure that while replacing words with other variants, the meaning of the text is preserved. Lexical Simplification (Siddharthan, 2014) is useful to a wide variety of target audience like people with aphasia, children and also non-native speakers. Complex Word Identification (Shardlow 2013; Paetzold 2015) is considered to be the first step in the pipeline of Lexical Simplification. The overall performance of a"
W13-4008,P04-1035,0,0.0161375,"h function with respect to current node is defined in line 8. This continues (line 1 − 15) untill there is no such node which improves the graph’s health or till the number of iterations reach epoch. These refined post sentiment scores along with post features (topic Count and intention type count) are used to classify posts’ stance. We discuss the results in Subsection 6.2. 66 Algorithm 5 Gradient Ascent Approach Our first baseline is a Unigram system which uses unigram content information of the utterances. Unigram systems are proved reliable in sentiment analysis (Mullen and Collier, 2004; Pang and Lee, 2004). The second baseline system LexFeatures uses the lexical features (Table 5). This baseline system is a strong baseline for the evaluation because it captures sentiment as well as pragmatic information of the utterances. We construct two systems to capture intentions: a TopicScore system which uses the topic directed sentiment scores (described in Subsection 3.3) and topic occurrence counts to capture utterance intentions, and a TopicScore+LexFeatures system which uses topic sentiment scores (described in Subsection 3.3) along with lexical features in Table 5. All systems are implemented using"
W13-4008,W11-0705,0,0.11185,"Missing"
W13-4008,W11-1701,0,0.0196991,"ntire post. We explain our stance classification method using post content features and post intention structure in this section. Section 5 describes the use of the dialogue structure of the debate and presents a gradient ascent method for re-evaluating posts’ stance. We present experiments and results on capturing users’ intentions and stance classification in Section 6. This is followed by conclusions in Section 7. 2 Related Work To classify posts’ stance in dual-sided debates, previous approaches have used probabilistic (Somasundaran and Wiebe, 2009) as well as machine learning techniques (Anand et al., 2011; Somasundaran and Wiebe, 2010). Some approaches extensively used the dialogue structure to identify posts’ stance (Walker et al., 2012) whereas others considered opinion expressions and their targets essential to capture sentiment in the posts towards debate topics (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). Pure machine learning approaches (Anand et al., 2011) have extracted lexical and contextual features of debate posts to classify their stance. Walker et al. (2012) partitioned the debate posts based on the dialogue structure of the debate and assigned stance to a partiti"
W13-4008,prasad-etal-2008-penn,0,0.0183084,"on at the utterance level plays a vital role in overall stance taking. We define a set of intentions each utterance can hold. The proposed topic directed sentiment analysis based approach will automatically identify users’ intention behind each utterance. Because of the unstructured and noisy nature of social media, we need to pre-process the debate 62 Type Contrast data before analyzing it further for users’ intentions. 3.1 Preprocessing The posts data is split into utterances, i.e. smallest discourse units, based on sentence ending markers and a few specific Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008) discourse markers listed in Table 2. Merged words like ‘mindboggling’, ‘super-awesome’, etc. are split based on the default Unix dictionary and special character delimiters. Once the debate posts are broken into utterances, we identify the intention behind each utterance in the post to compute entire post’s stance. Table 1 presents the statistics of the debate data collected from ‘convinceme.net’. Debates 28 Posts 2040 Author 1333 P/A 1.53 Reason Result Elaboration Conjunction Table 2: PDTB Discourse Markers List Utterances 12438 Evaluation data was created by five linguists who were provided"
W13-4008,baccianella-etal-2010-sentiwordnet,0,0.0361699,"Missing"
W13-4008,P09-1026,0,0.263881,"ssification, we define the health of the debate structure and show that maximizing its value leads to better stance classification accuracies. 1 Introduction Online debate forums provide Internet users a platform to discuss popular ideological debates. Debate in essence is a method of interactive and representational arguments. In an online debate, users make assertions with superior content to support their stance. Factual accuracy and emotional appeal are important features used to persuade the readers. It is easy to observe that personal opinions are important in ideological stance taking (Somasundaran and Wiebe, 2009). Because of the availability of Internet resources and time, people intelligently use the factual data to support their opinions. Online debates differ from public debates in terms of logical consistency. In online debates, users assert their opinion towards either side, sometimes ignoring discourse coherence required for logical soundness of the post. Generally they use strong degree of sentiment words including insulting or sarcastic remarks for greater emphasis 61 Proceedings of the SIGDIAL 2013 Conference, pages 61–69, c Metz, France, 22-24 August 2013. 2013 Association for Computational"
W13-4008,de-marneffe-etal-2006-generating,0,0.012717,"Missing"
W13-4008,W10-0214,0,0.686904,"differ from public debates in terms of logical consistency. In online debates, users assert their opinion towards either side, sometimes ignoring discourse coherence required for logical soundness of the post. Generally they use strong degree of sentiment words including insulting or sarcastic remarks for greater emphasis 61 Proceedings of the SIGDIAL 2013 Conference, pages 61–69, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics sions to the debate topic closest to them. Probabilistic association learning of target-opinion pair and the debate topic was used by Somasundaran and Wiebe (2010) as an integer linear programming problem to classify posts’ stance. Even though opinions might not be directed towards debate topics, these approaches attach the opinions to debate topics based only on their context cooccurrence. Our approach finds the target word for an opinion expression by analyzing the full dependency parse of the utterance. There has also been a lot of work done in social media on target directed sentiment analysis (Agarwal et al., 2011; O’Hare et al., 2009; Mukherjee and Bhattacharyya, 2012) which we incorporate for capturing users’ intentions. Agarwal et al. (2011) use"
W13-4008,P05-1045,0,0.00523211,"es of its synset member words is stored in the word’s tree node, otherwise a zero sentiment score is stored. If words are modified by negation words like {’never’,’not’,’nonetheless’,etc.}, their sentiment scores are negated. Extended targets (extendedTargets) are the entities closely related to debate topics. For example, ‘Joker’,‘Clarke Kent’ are related to ‘Batman’ and ‘Darth Vader’, ‘Yoda’ to ‘Star Wars’. To extract the extended targets, we capture named entities (NE) from the Wikipedia page of the debate topic (fetched using jsoup java library) using the Stanford Named Entity Recognizer (Finkel et al., 2005) and sort them based on their page occurrence count. Out of top-k (k = 20) NEs, some can belong to both of the debate topics. For example, ‘DC Comics’ is common between ‘Superman’ and ‘Batman’. We remove these NEs from individual lists and the remaining NEs are treated as extended targets (extendedTargets) of the debate topics. Debate topic directed sentiment scores are calculated by adding the sentiment scores of the utterance words which belong to the extended targets list of each debate topic. We refer to these scores as AScore and BScore representing scores directed towards topics A and B."
W13-4008,N12-1072,0,0.0915932,"ion 5 describes the use of the dialogue structure of the debate and presents a gradient ascent method for re-evaluating posts’ stance. We present experiments and results on capturing users’ intentions and stance classification in Section 6. This is followed by conclusions in Section 7. 2 Related Work To classify posts’ stance in dual-sided debates, previous approaches have used probabilistic (Somasundaran and Wiebe, 2009) as well as machine learning techniques (Anand et al., 2011; Somasundaran and Wiebe, 2010). Some approaches extensively used the dialogue structure to identify posts’ stance (Walker et al., 2012) whereas others considered opinion expressions and their targets essential to capture sentiment in the posts towards debate topics (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010). Pure machine learning approaches (Anand et al., 2011) have extracted lexical and contextual features of debate posts to classify their stance. Walker et al. (2012) partitioned the debate posts based on the dialogue structure of the debate and assigned stance to a partition using lexical features of candidate posts. This approach has a disadvantage that it loses post’s individuality because it assigns sta"
W13-4008,U06-1007,0,0.0814614,"Missing"
W13-4008,W11-1902,0,0.0404049,"between Superman and Batman. 3.3 Topic Directed Sentiment Analysis To identify intetion behind each utterance, we calculate debate topic directed sentiment. In topic directed sentiment analysis, the sentiment score is calculated using dependency parses of utterances and the sentiment lexicon sentiWordNet (Baccianella et al., 2010). sentiWordNet is a lexical corpus used for opinion mining. It stores positive and negative sentiment scores for every sense of the word present in the wordNet (Fellbaum, 2010). First, pronoun referencing is resolved using the Stanford co-reference resolution system (Lee et al., 2011). Using the Stanford dependency parser (De Marneffe et al., 2006), utterances are represented in a tree format where each node represents an utterance word storing its sentiment score and the edges represents dependency relations. Each 2. A− and B− : These tags capture users’ intent to oppose topic A or B. For example, “Superman is a stupid person who has an obvious weakness, like cyclops.” the user is opposing Superman by pointing out his weakness. 3. N I: This category includes utterances which hold no sentiment towards the debate topics or can utter about non-debate topic entities, In utter"
W13-4008,W04-3253,0,0.0396305,"al derivative of the Health function with respect to current node is defined in line 8. This continues (line 1 − 15) untill there is no such node which improves the graph’s health or till the number of iterations reach epoch. These refined post sentiment scores along with post features (topic Count and intention type count) are used to classify posts’ stance. We discuss the results in Subsection 6.2. 66 Algorithm 5 Gradient Ascent Approach Our first baseline is a Unigram system which uses unigram content information of the utterances. Unigram systems are proved reliable in sentiment analysis (Mullen and Collier, 2004; Pang and Lee, 2004). The second baseline system LexFeatures uses the lexical features (Table 5). This baseline system is a strong baseline for the evaluation because it captures sentiment as well as pragmatic information of the utterances. We construct two systems to capture intentions: a TopicScore system which uses the topic directed sentiment scores (described in Subsection 3.3) and topic occurrence counts to capture utterance intentions, and a TopicScore+LexFeatures system which uses topic sentiment scores (described in Subsection 3.3) along with lexical features in Table 5. All systems"
W13-4008,miltsakaki-etal-2004-penn,0,\N,Missing
W14-5312,J95-3006,0,0.38712,"uages are lexically and grammatically similar. Lexical borrowing4 occurs between languages. Gramatically, there are many similarities. Indian languages are synthetic5 ; derivational and inflectional morphologies result in the formation of complex words by stringing two or more morphemes. ILs predominantly have subject-object-verb (SOV) word order. They show agreement6 among words. We captured such type of characteristics, by building a robust feature set. 2 Related Work Traditionally, morphological analysis for Indian languages has been done using the rule based approach. For Hindi, the MA by Bharati et al. (1995) is most widely used among the NLP researchers in the Indian Community. Goyal and Lehal (2008) and Kanuparthi et al. (2012) MAs are advanced versions of the Bharati et al. (1995)’s analyzer. Kanuparthi et al. (2012) built a derivational MA for Hindi by introducing a layer over the Bharati et al. (1995)’s MA .It identifies 22 derivational suffixes which help in providing derivational analysis for the word whose suffix matches with one of these 22 suffixes. 1 The Indic languages are the dominant language family of the Indian subcontinent, generally spoken in the regions of northern India and Pak"
W14-5312,W12-3623,0,0.0277284,"Missing"
W14-5312,W12-2302,0,0.0213123,"similarities. Indian languages are synthetic5 ; derivational and inflectional morphologies result in the formation of complex words by stringing two or more morphemes. ILs predominantly have subject-object-verb (SOV) word order. They show agreement6 among words. We captured such type of characteristics, by building a robust feature set. 2 Related Work Traditionally, morphological analysis for Indian languages has been done using the rule based approach. For Hindi, the MA by Bharati et al. (1995) is most widely used among the NLP researchers in the Indian Community. Goyal and Lehal (2008) and Kanuparthi et al. (2012) MAs are advanced versions of the Bharati et al. (1995)’s analyzer. Kanuparthi et al. (2012) built a derivational MA for Hindi by introducing a layer over the Bharati et al. (1995)’s MA .It identifies 22 derivational suffixes which help in providing derivational analysis for the word whose suffix matches with one of these 22 suffixes. 1 The Indic languages are the dominant language family of the Indian subcontinent, generally spoken in the regions of northern India and Pakistan 2 The Dravidian languages are spoken mainly in southern India 3 Vibhakti is a Sanskrit grammatical term that encompas"
W14-5312,W13-4914,0,0.0745604,"nguages Saikrishna Srirampur IIIT Hyderabad saikrishna.srirampur @research.iiit.ac.in Ravi Chandibhamar IIIT Hyderabad chandibhamar.ravi @students.iiit.ac.in Radhika Mamidi IIIT Hyderabad radhika.mamidi @iiit.ac.in Abstract Statistical morph analyzers have proved to be highly accurate while being comparatively easier to maintain than rule based approaches. Our morph analyzer (SMA++) is an improvement over the statistical morph analyzer (SMA) described in Malladi and Mannem (2013). SMA++ predicts the gender, number, person, case (GNPC) and the lemma (L) of a given token. We modified the SMA in Malladi and Mannem (2013), by adding some rich machine learning features. The feature set was chosen specifically to suit the characteristics of Indian Languages. In this paper we apply SMA++ to four Indian languages viz. Hindi, Urdu, Telugu and Tamil. Hindi and Urdu belong to the Indic1 language family. Telugu and Tamil belong to the Dravidian2 language family. We compare SMA++ with some state-of-art statistical morph analyzers viz. Morfette in Chrupała et al. (2008) and SMA in Malladi and Mannem (2013). In all four languages, our system performs better than the above mentioned state-of-art SMAs. 1 Introduction Morph"
W14-5312,chrupala-etal-2008-learning,0,\N,Missing
W15-5928,J94-4002,0,0.592541,"Missing"
W15-5928,P98-2143,0,0.381982,"Missing"
W15-5928,C14-1172,0,0.0221508,"Missing"
W15-5928,I13-1130,0,0.0628874,"Missing"
W15-5928,C98-2138,0,\N,Missing
W15-5953,C10-2065,0,0.0272888,"gging was limited to linguistic domain, but now with the help of statistics, machine learning and pattern matching, automated DA tagging with various DA recognition approaches (Král and Cerisara, 2012) have come into existence. Some of the DA tagging methods include word based DA tagging (Garner et al., 1996), which shows that individual words are the potential source for tagging utterances in dialogs. On the other hand, (Webb et al., 2005) used n-grams with predictivity criterion for DA tagging which shows that instead of considering all n-grams, take only those which surpass the threshold. (Klüwer et al., 2010), proved that n-grams obtained from dependency parsing are powerfull enough for DA tagging. (Liu et al., 2013) and (Rotaru, 2002), proved that memory based learning techniques can be used for DA tagging. Other methods for DA tagging include Naive Bayesian interpretation (Reithinger and Klesen, 1997), Hidden Markov Models (Stolcke et al., 2000). Telugu is a free word order language. Existing n-gram cue based methods are mainly D S Sharma, R Sangal and E Sherly. Proc. of the 12th Intl. Conference on Natural Language Processing, pages 376–383, c Trivandrum, India. December 2015. 2015 NLP Associat"
W16-6305,baccianella-etal-2010-sentiwordnet,0,0.183737,"in nouns and verbs also. Various approaches have been proposed for building a SentiWordNet in the past. Turney worked on sentiment analysis for customer reviews dataset, using an unsupervised learning algorithm (Turney, 2002). Wiebe proposed methods to generate a resource, with subjective information, for a given target language from resources present in English (Wiebe and Riloff, 2005). Translation methods included using a bilingual dictionary and a parallel corpora based approach. For English, SentiWordNet was developed by Esuli (Esuli and Sebastiani, 2006) with improvements over the years (Baccianella et al., 2010). English SentiWordNet 3.0 is based off the Princeton English WordNet (Miller, 1995). Expansion strategies were suggested to increase the coverage of English SentiWordNet by assigning scores to antonym and synonym synsets. Less resourced languages depend on resources present in English to build such lexical tools. Whalley and Medagoda propose a method to build a sentiment lexicon for Sinhala using Sentiwordnet 3.0 (Whalley and Medagoda, 2015). The Sentiwordnet is mapped to an online Sinhala dictionary. Scores for each lexicon and its synonyms is assigned based on English Sentiwordnet scores. S"
W16-6305,bakliwal-etal-2012-hindi,0,0.222915,"n. Such words had to be avoided because they could be either positive or negative, depending on the context. A total of 4,954 words were present in more than one list. If a given word was present in a majority of the lists it appeared in, the majority opinion (positive or negative) was considered. The final list only contained strongly subjective words which served as the source lexicon before translation. 4.2 Target Lexicon Each of the words from the final list was then translated using Google translate. Bilingual dictionaries may not account for all the words because of language variations (Bakliwal et al., 2012). This method of translation is also labour intensive. Context dependent word mapping between two languages is a tough task in general. Though Google Translate has its own challenges, this method was used for faster translation of words and better translation performance. Some words were not translated into Tamil because the target language lacks such words. Multiword entries in the source lexicon were challeng33 ing to translate as sometimes the first word would get translated but not the rest. In a few cases a multi-word entry would get translated to an accurate single word and in some cases"
W16-6305,P11-4009,0,0.26371,"ntiwordnet 3.0 (Whalley and Medagoda, 2015). The Sentiwordnet is mapped to an online Sinhala dictionary. Scores for each lexicon and its synonyms is assigned based on English Sentiwordnet scores. Similar work is prevalent in literature for many Indian languages as well. Joshi built a SentiWordNet for Hindi using English SentiWordNet and linking English and Hindi WordNets (Joshi et al., 2010). Polarity scores were copied from the words in English SentiWordNet to the corresponding translated words in Hindi SentiWordNet. Another approach was proposed by Amitava Das (Das and Bandyopadhyay, 2010) (Das and Bandyopadhyay, 2011) (Das and Gamb¨ack, 2012) 31 in order to build SentiWordNet for three Indian languages (Bengali, Hindi and Telugu). This approach used two resources available in English which provided subjectivity information: SentiWordNet 3.0 and Subjectivity Lexicon. A bilingual dictionary based translation was carried out in order to obtain the target lexicon. A Wordnet based approach, to assign scores to synsets, and an automatic corpus based approach were also suggested. 3 Resources Used For the creation of Tamil SentiWordnet, English SentiWordNet 3.0 and Subjectivity Lexicon were the two most reliable r"
W16-6305,esuli-sebastiani-2006-sentiwordnet,0,0.537641,"using various lexicons in English. Each of these lexicons comprises of English words with certain polarity. After several levels of preprocessing, a final set of English words was obtained. These words were then translated into Tamil using Google Translate2 . The final set of words were annotated with either positive or negative polarity based on its prior polarity in English. The final lexicon was checked by Tamil annotators to remove any ambiguous entries and also for accuracy of translation. The various tools used for the construction of SentiWordNet for Tamil include English SentiWordNet (Esuli and Sebastiani, 2006), AFINN111 lexicon (Nielsen, 2011), Subjectivity Lexicon (Wilson et al., 2005), Opinion Lexicon (Liu et al., 2005) and Google Translate. The rest of the paper is organized into various sections. Section 2 deals with related work and progress towards building SentiWordNets for Indian languages followed by Section 3 describing the resources and tools used. Section 4 contains a detailed explanation of the approach followed to build the Tamil SentiWordNet. Section 5 defines the evaluation scheme for verification of resource 1 http://www.sas.com/enu s/home.html https://translate.google.co.in/ D S S"
W16-6305,S16-1001,0,0.0581485,"Missing"
W16-6305,W06-1652,0,0.0453288,"SentiWordNet polarity scores are obtained from learning through large English corpora. A threshold of 0.4 was considered (Das and Bandyopadhyay, 2010), as those words which have a score lower than the threshold may lose subjectivity upon translation to the target language. Words which have scores above 0.4 are assumed to be strongly subjective. Upon filtering words from English SentiWordNet based on the above criteria, a total of 16,791 tokens were obtained. The Subjectivity Lexicon contains 8,222 words in total. From this set, all words which were annotated as weakly subjective were removed (Riloff et al., 2006). A total of 2,652 weakly subjective words were discarded resulting in a new set of only strongly subjective words. As mentioned before, this list also contains Part-of-Speech tags. Those words which were tagged ’anypos’ were also removed to prevent context related ambiguities. Since the main aim was only to capture positive or negative sentiment, words tagged as neutral were also removed. The final list of words from Subjectivity Lexicon comprised of 4,526 tokens. On merging the two filtered lists it was found that 2,199 tokens were common between the both. Among these duplicates only words w"
W16-6305,P02-1053,0,0.009592,"6. 2 Related Work Sentiment analysis has been an age-old task and has been improving steadily over the past few decades. ”It is one of the most active research areas in natural language processing and is also widely studied in data mining, web mining, and text mining” (Liu, 2012). Initially the analysis was only restricted to adjectives and adverbs but now many lexical resources contain nouns and verbs also. Various approaches have been proposed for building a SentiWordNet in the past. Turney worked on sentiment analysis for customer reviews dataset, using an unsupervised learning algorithm (Turney, 2002). Wiebe proposed methods to generate a resource, with subjective information, for a given target language from resources present in English (Wiebe and Riloff, 2005). Translation methods included using a bilingual dictionary and a parallel corpora based approach. For English, SentiWordNet was developed by Esuli (Esuli and Sebastiani, 2006) with improvements over the years (Baccianella et al., 2010). English SentiWordNet 3.0 is based off the Princeton English WordNet (Miller, 1995). Expansion strategies were suggested to increase the coverage of English SentiWordNet by assigning scores to antony"
W16-6305,H05-2018,0,0.860504,"with certain polarity. After several levels of preprocessing, a final set of English words was obtained. These words were then translated into Tamil using Google Translate2 . The final set of words were annotated with either positive or negative polarity based on its prior polarity in English. The final lexicon was checked by Tamil annotators to remove any ambiguous entries and also for accuracy of translation. The various tools used for the construction of SentiWordNet for Tamil include English SentiWordNet (Esuli and Sebastiani, 2006), AFINN111 lexicon (Nielsen, 2011), Subjectivity Lexicon (Wilson et al., 2005), Opinion Lexicon (Liu et al., 2005) and Google Translate. The rest of the paper is organized into various sections. Section 2 deals with related work and progress towards building SentiWordNets for Indian languages followed by Section 3 describing the resources and tools used. Section 4 contains a detailed explanation of the approach followed to build the Tamil SentiWordNet. Section 5 defines the evaluation scheme for verification of resource 1 http://www.sas.com/enu s/home.html https://translate.google.co.in/ D S Sharma, R Sangal and A K Singh. Proc. of the 13th Intl. Conference on Natural L"
W16-6305,W10-3208,0,\N,Missing
W17-2902,D17-1151,0,0.0224618,"Missing"
W17-2902,W11-0705,0,0.0619173,"e in social psychology for identification of sexist content and its impact. Research has provided 8 To better understand the nature of sexism, sentiment analysis can be done. In recent times, sentiment analysis of Twitter data has received a lot of attention (Pak and Paroubek, 2010). Some of the early works by Go et al. (2009) and Bermingham and Smeaton (2010) use distant learning to acquire sentiment data. They show that using unigrams, bigrams and part-of-speech (POS) tags as features, SVM outperforms other classifiers like Naive Bayes and MaxEnt. To remove the need for feature engineering, Agarwal et al. (2011) use POS-specific prior polarity features and tree kernel for sentiment analysis. To detect contextual polarity using phrase-level sentiment analysis, Wilson et al. (2005) identify whether a phrase is neutral or polar. If the phrase is polar, they then disambiguate the polarity of the polar expression. State-of-the-art sentiment analyzers use deep learning techniques like Convolutional Neural Network (CNN) (Dos Santos and Gatti, 2014) and Recursive Neural Network (Tang et al., 2015) based approach to learn features automatically from the input text. 3 However, the total number of unique tweets"
W17-2902,baccianella-etal-2010-sentiwordnet,0,0.0168944,"f words and bag of n-grams as features for text classification. Bag of n-grams feature captures partial information about the local word order. FastText allows update of word vectors through back-propagation during training allowing the model to fine-tune word representations according to the task at hand (Bojanowski et al., 2016). The model is trained using stochastic gradient descent and a linearly decaying learning rate. 5 (Pradhan et al., 2004) to chunk tweets and get all the phrases. We calculated the positive score and the negative score for each phrase in the tweet, using SentiWordNet (Baccianella et al., 2010) and subjectivity lexicon (Taboada et al., 2011). The overall sentiment score of a tweet was calculated by summing up the individual score of the phrases in the tweet. If this overall sentiment score of the tweet was greater than 0, then the tweet was marked as positive; if the overall sentiment score was less than 0, it was marked as negative; else the tweet was marked as neutral. Table 6 shows the results of the basic sentiment analysis of tweets. 5.4 Polarity Detection FastText The training set and the test set were split in 7 : 3 ratio for FastText. Table 8 reports precision at 1 of runnin"
W17-2902,C14-1008,0,0.0455715,"rams, bigrams and part-of-speech (POS) tags as features, SVM outperforms other classifiers like Naive Bayes and MaxEnt. To remove the need for feature engineering, Agarwal et al. (2011) use POS-specific prior polarity features and tree kernel for sentiment analysis. To detect contextual polarity using phrase-level sentiment analysis, Wilson et al. (2005) identify whether a phrase is neutral or polar. If the phrase is polar, they then disambiguate the polarity of the polar expression. State-of-the-art sentiment analyzers use deep learning techniques like Convolutional Neural Network (CNN) (Dos Santos and Gatti, 2014) and Recursive Neural Network (Tang et al., 2015) based approach to learn features automatically from the input text. 3 However, the total number of unique tweets identified, after removing retweets, were only 712 in number. The total number of tokens in the created dataset is 74,874. The mean length of BS tweets is 80.95, with a standard deviation of 25.75. The dataset also contains the metadata of each tweet, like username, time of creation of the tweet, it’s geographic location, number of retweets and number of likes. 3.1 We collected data using the public Twitter Search API. The terms quer"
W17-2902,J93-2004,0,0.0582791,"al. Table 6 shows the results of the basic sentiment analysis of tweets. 5.4 Polarity Detection FastText The training set and the test set were split in 7 : 3 ratio for FastText. Table 8 reports precision at 1 of running FastText, using 100 dimension word vectors, for 5, 8, 10 and 15 epochs with a learning rate of 0.1 and the size of context window as 5. It is observed that there is no improvement in the F1-score after 10 epochs. To detect the polarity of each tweet, we experimented with rule-based sentiment analysis techniques using linguistic features. First, using the Penn Treebank tagset (Marcus et al., 1993), all tweets were tagged for part-of-speech (POS). After this, we used the Stanford Shallow Parser 12 Benevolent Hostile Others P 0.97 0.89 0.80 SVM R F1 0.69 0.80 0.33 0.48 0.99 0.89 P 0.69 0.57 0.91 Seq2Seq R F1 0.77 0.73 0.65 0.61 0.87 0.88 Table 7: Comparision of Precision (P), Recall (R) and F1 score (F1) of classification of tweets into HS, BS and Others class using SVM and Seq2seq models. Epochs 5 8 10 15 Precision 0.81 0.84 0.87 0.87 Recall 0.81 0.84 0.87 0.87 F1-Score 0.81 0.84 0.87 0.87 training data increases. This is further reflected in the high precision, recall and the comparabl"
W17-2902,pak-paroubek-2010-twitter,0,0.0479888,"cribes the available dataset of HS tweets that we used for our experiments. Section 4 and 5 describe the technical aspects of the experiments conducted for the classification of tweets. We discuss the results of the experiments in Section 6 before concluding the paper in Section 7. 2 Related Work A considerable amount of work has been done in social psychology for identification of sexist content and its impact. Research has provided 8 To better understand the nature of sexism, sentiment analysis can be done. In recent times, sentiment analysis of Twitter data has received a lot of attention (Pak and Paroubek, 2010). Some of the early works by Go et al. (2009) and Bermingham and Smeaton (2010) use distant learning to acquire sentiment data. They show that using unigrams, bigrams and part-of-speech (POS) tags as features, SVM outperforms other classifiers like Naive Bayes and MaxEnt. To remove the need for feature engineering, Agarwal et al. (2011) use POS-specific prior polarity features and tree kernel for sentiment analysis. To detect contextual polarity using phrase-level sentiment analysis, Wilson et al. (2005) identify whether a phrase is neutral or polar. If the phrase is polar, they then disambigu"
W17-2902,N04-1030,0,0.090379,"ification (Joulin et al., 2016). It is often at par with deep learning classifiers in terms of accuracy, and much faster for training and evaluation. FastText uses bag of words and bag of n-grams as features for text classification. Bag of n-grams feature captures partial information about the local word order. FastText allows update of word vectors through back-propagation during training allowing the model to fine-tune word representations according to the task at hand (Bojanowski et al., 2016). The model is trained using stochastic gradient descent and a linearly decaying learning rate. 5 (Pradhan et al., 2004) to chunk tweets and get all the phrases. We calculated the positive score and the negative score for each phrase in the tweet, using SentiWordNet (Baccianella et al., 2010) and subjectivity lexicon (Taboada et al., 2011). The overall sentiment score of a tweet was calculated by summing up the individual score of the phrases in the tweet. If this overall sentiment score of the tweet was greater than 0, then the tweet was marked as positive; if the overall sentiment score was less than 0, it was marked as negative; else the tweet was marked as neutral. Table 6 shows the results of the basic sen"
W17-2902,J11-2001,0,0.0282051,"sification. Bag of n-grams feature captures partial information about the local word order. FastText allows update of word vectors through back-propagation during training allowing the model to fine-tune word representations according to the task at hand (Bojanowski et al., 2016). The model is trained using stochastic gradient descent and a linearly decaying learning rate. 5 (Pradhan et al., 2004) to chunk tweets and get all the phrases. We calculated the positive score and the negative score for each phrase in the tweet, using SentiWordNet (Baccianella et al., 2010) and subjectivity lexicon (Taboada et al., 2011). The overall sentiment score of a tweet was calculated by summing up the individual score of the phrases in the tweet. If this overall sentiment score of the tweet was greater than 0, then the tweet was marked as positive; if the overall sentiment score was less than 0, it was marked as negative; else the tweet was marked as neutral. Table 6 shows the results of the basic sentiment analysis of tweets. 5.4 Polarity Detection FastText The training set and the test set were split in 7 : 3 ratio for FastText. Table 8 reports precision at 1 of running FastText, using 100 dimension word vectors, fo"
W17-2902,D15-1167,0,0.0159837,"s, SVM outperforms other classifiers like Naive Bayes and MaxEnt. To remove the need for feature engineering, Agarwal et al. (2011) use POS-specific prior polarity features and tree kernel for sentiment analysis. To detect contextual polarity using phrase-level sentiment analysis, Wilson et al. (2005) identify whether a phrase is neutral or polar. If the phrase is polar, they then disambiguate the polarity of the polar expression. State-of-the-art sentiment analyzers use deep learning techniques like Convolutional Neural Network (CNN) (Dos Santos and Gatti, 2014) and Recursive Neural Network (Tang et al., 2015) based approach to learn features automatically from the input text. 3 However, the total number of unique tweets identified, after removing retweets, were only 712 in number. The total number of tokens in the created dataset is 74,874. The mean length of BS tweets is 80.95, with a standard deviation of 25.75. The dataset also contains the metadata of each tweet, like username, time of creation of the tweet, it’s geographic location, number of retweets and number of likes. 3.1 We collected data using the public Twitter Search API. The terms queried were common phrases and hashtags that are gen"
W17-2902,N16-2013,0,0.342974,"al networks, they learn distributed lowdimensional text representations, where semantically similar comments and words reside in the similar part of vector space. They, then, feed this to a linear classifier to identify hateful and clean comments. Davidson et al. (2017) use hate speech lexicon to collect tweets containing hate speech keywords. They train a multi-class classifier to separate these tweets into one of the three classes: those containing hate speech, only offensive language, and those with neither. Hate speech dataset, containing sexist tweets, has been made publicly available by Waseem and Hovy (2016). This dataset contains 16k tweets that fall into one of the three classes: sexist, racist or neither. They list a set of criteria based on critical race theory to annotate the data and then use Support Vector Machines (SVM) with handcrafted features to classify tweets. However, one of the major drawbacks of the decsribed approaches and dataset is that it takes into account only hostile sexist tweets. • Others: if the tweet is not sexist To the best of our knowledge, there has not been any previous study in computationally identifying benevolent sexism and classifying sexist content into two d"
W17-2902,H05-1044,0,0.0401564,"e. In recent times, sentiment analysis of Twitter data has received a lot of attention (Pak and Paroubek, 2010). Some of the early works by Go et al. (2009) and Bermingham and Smeaton (2010) use distant learning to acquire sentiment data. They show that using unigrams, bigrams and part-of-speech (POS) tags as features, SVM outperforms other classifiers like Naive Bayes and MaxEnt. To remove the need for feature engineering, Agarwal et al. (2011) use POS-specific prior polarity features and tree kernel for sentiment analysis. To detect contextual polarity using phrase-level sentiment analysis, Wilson et al. (2005) identify whether a phrase is neutral or polar. If the phrase is polar, they then disambiguate the polarity of the polar expression. State-of-the-art sentiment analyzers use deep learning techniques like Convolutional Neural Network (CNN) (Dos Santos and Gatti, 2014) and Recursive Neural Network (Tang et al., 2015) based approach to learn features automatically from the input text. 3 However, the total number of unique tweets identified, after removing retweets, were only 712 in number. The total number of tokens in the created dataset is 74,874. The mean length of BS tweets is 80.95, with a s"
W17-5219,baccianella-etal-2010-sentiwordnet,0,0.510386,"es. The approach mainly uses adjectives for Sentiment Analysis. However, sufficient pre-processing was carried out using available tools for English before the phrases were successfully classified. Even though sentiment depends on context, lexical resources have proven to give a good baseline for further studies. The English language has several lexical resources such as the SentiWordNet as described by Esuli (Esuli and Sebastiani, 2006). It contains over 3 million tokens assigned with polarity and objectivity score. The resource has been improved over the years as demonstrated in literature (Baccianella et al., 2010). Another such important resource is the Subjectivity Lexicon (Wilson et al., 2005) which is a part of OpinionFinder2 . Languages which have a scarcity of readily available data depend on resource rich languages to build such lexicons. Whalley (Whalley and Medagoda, 2015) describes how the Sinhalese sentiment lexicon was created using the English SentiWordNet 3.0. The SentiWordNet in English was mapped to a Sinhalese dictionary and the scores were copied from one language to another. Another way to achieve this is by linking the WordNets of the source and target language. Joshi proposed a meth"
W17-5219,P97-1023,0,0.520273,"Missing"
W17-5219,bakliwal-etal-2012-hindi,0,0.0284145,"-Odia language pair have not yet been developed. In this paper, we discuss a method to create a SentiWordNet for Odia, which is resource-poor, by only using resources which are currently available for Indian languages. The lexicon created, would serve as a tool for Sentiment Analysis related task specific to Odia data. 1 Introduction For resource-poor languages, one popular approach is to use readily available resources in English to generate a source lexicon. The source lexicon is then translated using a Machine Translation system or a bilingual dictionary to create the final target lexicon (Bakliwal et al., 2012). In case of the English-Odia language pair, a good 2 Previous Work Since its introduction in 1961 by IBM, Sentiment Analysis has been a fast growing area in computer 143 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 143–148 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics science. Research on Sentiment Analysis began in English. However with increasing demand, several researchers have developed various tools and resources for many other languages. Odia (ISO 639 language code: o"
W17-5219,W16-6305,1,0.727688,"ngual dictionary to translate words from English to Bengali. Amitava Das (Das and Bandyopadhyay, 2010) (Das and Gamb¨ack, 2012) (Das and Bandyopadhyay, 2011) proposes several ways to generate such lexical resources for other Indian languages. One approach suggests the usage of both English SentiWordNet 3.0 and Subjectivity Lexicon and adopting a translation based approach in order to build the lexicon in three Indian languages (Das and Bandyopadhyay, 2010). A SentiWordNet for Tamil has also been developed using a similar translation based approach for currently available resources in English (Kannan et al., 2016). Due to lack of a sufficiently large parallel corpus or a bilingual dictionary, direct translation techniques from English to Odia could not be applied in-order to build the SentiWordNet in Odia. 3 Prerequisites For creating Odia SentiWordNet, SentiWordNets of three Indian languages, namely Bengali, Tamil and Telugu are used. Polarity of words for these resources has proved to be reliable (Das and Bandyopadhyay, 2010). Multiple SentiWordNets are used for a better estimate of sentiment for each word and reduction of ambiguities while building the resource. For creation of lexicon for Odia, Wor"
W17-5219,P02-1053,0,0.039071,"inguistics science. Research on Sentiment Analysis began in English. However with increasing demand, several researchers have developed various tools and resources for many other languages. Odia (ISO 639 language code: ori)1 , being a resource-poor language, lacks necessary tools to perform Sentiment Analysis. Since opinion mining has proved extremely useful in online review and survey systems and since data is more readily available than ever, Sentiment Analysis serves as an effective method to achieve automated scoring of products, movies, etc. Turney worked on classifying customer reviews (Turney, 2002). They adopt an unsupervised learning technique to predict the semantic orientation of phrases. Hatzivassiloglou (Hatzivassiloglou and R. McKeown, 1997) and Turney (Turney and Littman, 2003) describe methods of using a set of words gathered a priori as a seed list to classify the semantic orientation of phrases. The former method (Hatzivassiloglou and R. McKeown, 1997) was the first to deal with opinion classification in phrases. The approach mainly uses adjectives for Sentiment Analysis. However, sufficient pre-processing was carried out using available tools for English before the phrases we"
W17-5219,P11-4009,0,0.060438,"Missing"
W17-5219,H05-2018,0,0.0339359,"processing was carried out using available tools for English before the phrases were successfully classified. Even though sentiment depends on context, lexical resources have proven to give a good baseline for further studies. The English language has several lexical resources such as the SentiWordNet as described by Esuli (Esuli and Sebastiani, 2006). It contains over 3 million tokens assigned with polarity and objectivity score. The resource has been improved over the years as demonstrated in literature (Baccianella et al., 2010). Another such important resource is the Subjectivity Lexicon (Wilson et al., 2005) which is a part of OpinionFinder2 . Languages which have a scarcity of readily available data depend on resource rich languages to build such lexicons. Whalley (Whalley and Medagoda, 2015) describes how the Sinhalese sentiment lexicon was created using the English SentiWordNet 3.0. The SentiWordNet in English was mapped to a Sinhalese dictionary and the scores were copied from one language to another. Another way to achieve this is by linking the WordNets of the source and target language. Joshi proposed a method to create a SentiWordNet 1 2 for Hindi by linking the English and Hindi WordNets"
W17-5219,W12-3707,0,0.0324164,"Missing"
W17-5219,W10-3208,0,\N,Missing
W17-5219,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
W17-5408,W10-3207,0,0.0240142,"ble. (Wiebe et al., 2005) describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. This was the first attempt to manually annotate the 10,000 sentence corpus of articles from the news. (Alm et al., 2005) have manually annotated 1580 sentences extracted from 22 Grimms’ tales for the task of emotion annotation at the sentence level. (Arora, 2013) performed sentiment analysis task for the Hindi Language with limited corpus made manually annotated by the native Hindi speakers. (Das and Bandyopadhyay, 2010b) aims to manually annotate the sentences in a web-based Bengali blog corpus with the emotional components such as emotional expression (word/phrase), intensity, associated holder and topic(s). (Das and Bandyopadhyay, 2010a) built a lexicon of words to support the task of Telugu sentiment analysis and is made available to the public. (Das and Bandyopadhay, 2010) created an interactive gaming to technology (Dr. Sentiment) to create and validate SentiWordNet for Telugu. sites like Twitter and Facebook. Although the news genre has received much less attention within the Sentiment Analysis commun"
W17-5408,H05-1073,0,0.0156912,"gs of the First Workshop on Building Linguistically Generalizable NLP Systems, pages 54–58 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Figure 1: Process of building the resource text using various ML techniques, but no data was publicly made available. (Wiebe et al., 2005) describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. This was the first attempt to manually annotate the 10,000 sentence corpus of articles from the news. (Alm et al., 2005) have manually annotated 1580 sentences extracted from 22 Grimms’ tales for the task of emotion annotation at the sentence level. (Arora, 2013) performed sentiment analysis task for the Hindi Language with limited corpus made manually annotated by the native Hindi speakers. (Das and Bandyopadhyay, 2010b) aims to manually annotate the sentences in a web-based Bengali blog corpus with the emotional components such as emotional expression (word/phrase), intensity, associated holder and topic(s). (Das and Bandyopadhyay, 2010a) built a lexicon of words to support the task of Telugu sentiment analys"
W18-3511,P13-3009,0,0.0226408,"of annotated resources available is considerably low. This deters the novelty of research possible in the language. Additionally, the properties of Telugu are significantly different compared to major languages such as English. 3.2 Sense-type classification of Verbs Verbs provide relational and semantic framework for its sentences and are considered as the most important lexical and syntactic category of language. In a single verb many verbal sense-types are present and different verbs share same verbal sense-types. These sense-types are inspired from different schools of Indian philosophies (Rajan, 2013). The seven sense-types of verbs along with their primitive sense along with Telugu examples are given by (Parupalli and Singh, 2018). In this paper, we adopt 8483 verbs of OntoSenseNet as our gold-standard annotated resource. This resource is utilized for learning the sense-identification by classifiers developed in our paper. In this paper, we adopt the lexical resource OntoSenseNet for Telugu. The resource consists of 21,000 root words alongside their meanings. The primary and secondary sense of each extracted word is identified manually by the native speakers of language. The paper tries t"
W18-4005,P11-4009,0,0.0560484,"Missing"
W18-4005,W10-3208,0,0.0617155,"Missing"
W18-4005,W12-3707,0,0.052688,"Missing"
W18-4005,L18-1100,1,0.864635,"Missing"
W18-4005,C00-1044,0,0.553717,"Missing"
W18-4005,D07-1115,0,0.120987,"Missing"
W18-4005,W17-5408,1,0.887148,"Missing"
W18-4005,P18-3014,1,0.832137,"Missing"
W18-4005,I08-7012,0,0.068173,"Missing"
W19-1301,W11-1701,0,0.0332076,"yo et al., 2018) and improving named entity recognition tasks (Pham et al., 2019) and target dependent sentiment analysis (Gupta et al., 2019). Related Work Stance Detection problem is widely discussed and studied for the past few years in opinion mining. One of the initial work on stance classification (Somasundaran and Wiebe, 2010) explores the use of sentiment and arguing features for classifying stances in ideological debates by constructing an arguing lexicon from a manually annotated corpus. The combination of opinion target pair features was employed for the classification task. Later, Anand et al. (2011) identifies that for a particular topic, classification results using 2 3 Method Description feature representations that generalize well across tasks. The following loss function is comprised of loss of the main task and the auxiliary task. We use a lambda parameter to control the effect of loss of the auxiliary task on the total loss. The following subsections explain the preprocessing of the corpus and the deep learning architecture proposed for stance detection. 3.1 Preprocessing • Loss function: Preprocessing is done on the tweets by removing twitter handles starting with “@” or words tha"
W19-1301,P17-2054,0,0.0288638,"rpus of Spanish tweets (Anta et al., 2013), topic detection, and sentiment analysis approaches are used. Multi-task learning approach (MTL) jointly trains multiple tasks in parallel, which acts as additional regularization, to improve the underlying network’s generalization across all the tasks. It has proven to be a novel and effective learning schema in many NLP problems. Recently, multitask learning approaches have been used for sentiment and sarcasm detection in (Majumder et al., 2019) , implicit discourse relationship identification (Lan et al., 2017), key-phrase boundary classification (Augenstein and Søgaard, 2017), improving sequence tagging tasks (Changpinyo et al., 2018) and improving named entity recognition tasks (Pham et al., 2019) and target dependent sentiment analysis (Gupta et al., 2019). Related Work Stance Detection problem is widely discussed and studied for the past few years in opinion mining. One of the initial work on stance classification (Somasundaran and Wiebe, 2010) explores the use of sentiment and arguing features for classifying stances in ideological debates by constructing an arguing lexicon from a manually annotated corpus. The combination of opinion target pair features was e"
W19-1301,S16-1003,0,0.0604041,"Missing"
W19-1301,C18-1251,0,0.0624128,"Missing"
W19-1301,W10-0214,0,0.0447165,"cently, multitask learning approaches have been used for sentiment and sarcasm detection in (Majumder et al., 2019) , implicit discourse relationship identification (Lan et al., 2017), key-phrase boundary classification (Augenstein and Søgaard, 2017), improving sequence tagging tasks (Changpinyo et al., 2018) and improving named entity recognition tasks (Pham et al., 2019) and target dependent sentiment analysis (Gupta et al., 2019). Related Work Stance Detection problem is widely discussed and studied for the past few years in opinion mining. One of the initial work on stance classification (Somasundaran and Wiebe, 2010) explores the use of sentiment and arguing features for classifying stances in ideological debates by constructing an arguing lexicon from a manually annotated corpus. The combination of opinion target pair features was employed for the classification task. Later, Anand et al. (2011) identifies that for a particular topic, classification results using 2 3 Method Description feature representations that generalize well across tasks. The following loss function is comprised of loss of the main task and the auxiliary task. We use a lambda parameter to control the effect of loss of the auxiliary t"
W19-1301,I13-1191,0,0.0165789,"onsideration of 2 Stance Detection Example: “Demonetisation is a step towards the development and betterment of society.” * These authors contributed equally to this work. http://www.internetlivestats.com/twitter-statistics/ 1 Proceedings of the 10th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 1–5 c Minneapolis, June 6, 2019. 2019 Association for Computational Linguistics lexical and contextual features are far better than the best feature set without any contextual features analyzing the dialogic structure of debates. Walker et al. (2012); Hasan and Ng (2013) studied stance detection in two-side online debate data, and Faulkner (2014) examined document-level argument stance in student essays where the language of the texts are structured, monolingual and grammatically correct. And lately, a shared task for stance detection research focused on Twitter data (Mohammad et al., 2016). In this tweet, we can observe that the user most likely is in favor of the move. Our model for stance detection determines the stance taken by the tweeter automatically. An example of a tweet in the code-mixed Hindi-English corpus is Example: “Notebandi ne foreigners ko b"
W19-1301,N12-1072,0,0.0267356,"ific issue, based on consideration of 2 Stance Detection Example: “Demonetisation is a step towards the development and betterment of society.” * These authors contributed equally to this work. http://www.internetlivestats.com/twitter-statistics/ 1 Proceedings of the 10th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 1–5 c Minneapolis, June 6, 2019. 2019 Association for Computational Linguistics lexical and contextual features are far better than the best feature set without any contextual features analyzing the dialogic structure of debates. Walker et al. (2012); Hasan and Ng (2013) studied stance detection in two-side online debate data, and Faulkner (2014) examined document-level argument stance in student essays where the language of the texts are structured, monolingual and grammatically correct. And lately, a shared task for stance detection research focused on Twitter data (Mohammad et al., 2016). In this tweet, we can observe that the user most likely is in favor of the move. Our model for stance detection determines the stance taken by the tweeter automatically. An example of a tweet in the code-mixed Hindi-English corpus is Example: “Noteban"
W19-1301,D17-1134,0,0.0302805,"entiment and domain-specific features. Also, for the corpus of Spanish tweets (Anta et al., 2013), topic detection, and sentiment analysis approaches are used. Multi-task learning approach (MTL) jointly trains multiple tasks in parallel, which acts as additional regularization, to improve the underlying network’s generalization across all the tasks. It has proven to be a novel and effective learning schema in many NLP problems. Recently, multitask learning approaches have been used for sentiment and sarcasm detection in (Majumder et al., 2019) , implicit discourse relationship identification (Lan et al., 2017), key-phrase boundary classification (Augenstein and Søgaard, 2017), improving sequence tagging tasks (Changpinyo et al., 2018) and improving named entity recognition tasks (Pham et al., 2019) and target dependent sentiment analysis (Gupta et al., 2019). Related Work Stance Detection problem is widely discussed and studied for the past few years in opinion mining. One of the initial work on stance classification (Somasundaran and Wiebe, 2010) explores the use of sentiment and arguing features for classifying stances in ideological debates by constructing an arguing lexicon from a manually anno"
W19-1307,S17-2126,0,0.0336617,"zed the following set of values for our main task humor detection. have also been studied in recent years. Deep learning techniques (LeCun et al., 2015) have contributed to significant progress in various areas of research, including natural language understanding. Convolutional neural network based networks have been used for sentence classification (Kim, 2014), bidirectional LSTM networks (biLSTM) were used for sequence tagging (Huang et al., 2015), and attention based bidirectional LSTM networks were used for relational classification (Zhou et al., 2016) and topic-based sentiment analysis (Baziotis et al., 2017). In this work, we propose three deep learning networks using bilingual word embeddings as input and compare it against the classification models presented in (Khandelwal et al., 2018) using their annotated corpus to detect one of the playful domains of language: Humor. An example from the corpus: “Subha ka bhula agar sham ko wapas ghar aa jaye then we must thank GPS technology..” “(If someone is lost in the morning and returns home in the evening then we must thank GPS technology.) This tweet is annotated as humorous. In particular, we are focused on code-mixed data as it lacks the presence o"
W19-1307,Q17-1010,0,0.0263268,"behavior, events, reviews, studying trends as well as linguistic analysis (Vyas et al., 2014). 1.1 • Embedding size: 300, Window length: 10, Negative sampling 1.3 One of the limitations of word2vec model is the inability to handle words with very low frequency in the training corpus and out-of-vocabulary words which might be present in the unseen text instances. Example: people on social media write words like “happppyyyyy”, “lolll”, etc. These kinds of new words can’t have pre-trained word embeddings. To address this problem in the bilingual scenario, we analyzed the performance of fastText (Bojanowski et al., 2017) word embedding model, which considers subword information, for generating word embeddings. FastText learns character n-gram (Joulin et al., 2016) representations and represents words as the sum of the n-gram vectors, where n is a hyperparameter. We kept hyperparameters like embedding size, window length, etc., same as in word2vec model to compare their results. 1.4 Model Architecture We propose three different deep learning architectures for the task of humor detection based on CNN and biLSTM networks which take bilingual word embeddings as input. We used cross-entropy loss function and Adam"
W19-1307,D13-1084,0,0.0453111,"Missing"
W19-1307,D08-1102,0,0.0792768,"Missing"
W19-1307,W10-2914,0,0.156367,"logy for different purposes like entertainment, learning and sharing their experiences. This led to a tremendous increase in content generated by users on social networking and microblogging sites. Websites like Facebook, Twitter, and Reddit (Danet and Herring, 2007) act as a platform for users to reach large masses in real-time and express their thoughts freely and sometimes anonymously amongst communities and virtual networks. These natural language texts depict various linguistic elements such as aggression, irony, humor, and sarcasm. In recent years, automatic detection of these elements (Davidov et al., 2010) has become a research interest for both organizations and research communities. * These authors contributed equally to this work. https://en.wikipedia.org/wiki/Multilingualism in India 57 Proceedings of the 10th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 57–61 c Minneapolis, June 6, 2019. 2019 Association for Computational Linguistics negative sampling. Based on the results, we finalized the following set of values for our main task humor detection. have also been studied in recent years. Deep learning techniques (LeCun et al., 2015) have"
W19-1307,D08-1110,0,0.115352,"Missing"
W19-1307,D14-1105,0,0.0195632,"r annotated corpus to detect one of the playful domains of language: Humor. An example from the corpus: “Subha ka bhula agar sham ko wapas ghar aa jaye then we must thank GPS technology..” “(If someone is lost in the morning and returns home in the evening then we must thank GPS technology.) This tweet is annotated as humorous. In particular, we are focused on code-mixed data as it lacks the presence of bilingual word embeddings, commonly used, to train any deep learning model which is essential for understanding human behavior, events, reviews, studying trends as well as linguistic analysis (Vyas et al., 2014). 1.1 • Embedding size: 300, Window length: 10, Negative sampling 1.3 One of the limitations of word2vec model is the inability to handle words with very low frequency in the training corpus and out-of-vocabulary words which might be present in the unseen text instances. Example: people on social media write words like “happppyyyyy”, “lolll”, etc. These kinds of new words can’t have pre-trained word embeddings. To address this problem in the bilingual scenario, we analyzed the performance of fastText (Bojanowski et al., 2017) word embedding model, which considers subword information, for gener"
W19-1307,D16-1058,0,0.0833751,"Missing"
W19-1307,L18-1193,0,0.0430902,"Missing"
W19-1307,P16-2034,0,0.0394865,"tics negative sampling. Based on the results, we finalized the following set of values for our main task humor detection. have also been studied in recent years. Deep learning techniques (LeCun et al., 2015) have contributed to significant progress in various areas of research, including natural language understanding. Convolutional neural network based networks have been used for sentence classification (Kim, 2014), bidirectional LSTM networks (biLSTM) were used for sequence tagging (Huang et al., 2015), and attention based bidirectional LSTM networks were used for relational classification (Zhou et al., 2016) and topic-based sentiment analysis (Baziotis et al., 2017). In this work, we propose three deep learning networks using bilingual word embeddings as input and compare it against the classification models presented in (Khandelwal et al., 2018) using their annotated corpus to detect one of the playful domains of language: Humor. An example from the corpus: “Subha ka bhula agar sham ko wapas ghar aa jaye then we must thank GPS technology..” “(If someone is lost in the morning and returns home in the evening then we must thank GPS technology.) This tweet is annotated as humorous. In particular, w"
W19-1307,D14-1181,0,0.00730251,"Missing"
W19-4809,P14-1062,0,0.0453728,"s a lot of effort, concentration, attention to detail and is also time taking. Thus automating this bias de1 https://www.ethnologue.com/statistics/size 77 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 77–84 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics documents with sparse lexical features, such as ngrams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Kalchbrenner et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network based approaches have been quite effective, classification based only on articles or only on headlines may not give better results as articles may contain unnecessary extra information and headlines being short may not capture required information. So a combination of article and headline is required for better classification. In this paper, we test our hypothesis that classification can be improved by focusing on essential parts of new"
W19-4809,D14-1181,0,0.00668653,"of ui with U, the hidden representation of encoded headline representation Q and get a normalized importance αi through a softmax function. After that we compute the representation of the news article as a weighted sum of the word annotations based on the weights. All of the above are learned during the training process. 4.1.4 Bias detection The vector v is used to detect towards which political party the article is biased to as: p = Sof tmax(Wc v + bc ) 5.1.3 Neural Network methods We experimented with multiple neural network architectures like: CNNs Word based neural network model like in (Kim, 2014) are used. Branched CNNs Figure 3 shows the branched CNN architecture. (10) Training loss is the negative log likelihood of the correct labels: L=− X Log(pdi ) Baselines (11) d where i is the label of document d. 5 Experiments All the experiments are carried out in a 5-fold cross validation scenario. As headlines express the ideological view of the news stories, in some cases only the headline would be sufficient to detect bias. So except for Headline Attention Networks, for all other baselines we divided dataset into three parts: Figure 3: Branched CNN architecture LSTMs and GRU based models"
W19-4809,O18-1021,0,0.0225778,"Missing"
W19-4809,P12-2018,0,0.0171617,"using a process called coding or theoretical frameworks like discourse analysis and content analysis. This analysis requires a lot of effort, concentration, attention to detail and is also time taking. Thus automating this bias de1 https://www.ethnologue.com/statistics/size 77 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 77–84 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics documents with sparse lexical features, such as ngrams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Kalchbrenner et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network based approaches have been quite effective, classification based only on articles or only on headlines may not give better results as articles may contain unnecessary extra information and headlines being short may not capture required information. So a combination of article and headline is required for better"
W19-4809,N16-1174,0,0.126046,"Missing"
W19-4809,D17-1317,0,0.0535959,"re relevant rather than simply filtering out. Our model outperforms various common classification architectures by a significant margin. Figure 1: News article from the dataset. Bias towards ”TDP” 2 Related Work Identification and analysis of bias in news articles has led to extensive research in the fields of anthropology, discourse analysis, and media studies. (Sivandi and Dowlatabadi, 2015) used the headlines and leads of newspaper articles to detect bias in their complete linguistic approach to the problem. (Iyyer et al., 2014) used recursive neural networks to detect political ideology. (Rashkin et al., 2017) introduced a propagandists dataset focused propaganda news and presented a study on the language of news media in context of political fact checking. (Recasens et al., 2013) conducted a study related to bias in the Wikipedia articles using logisitc regression. Many industrial organizations are working in this space worldwide to fight disinformation. First Draft News is a project ”to fight mis- and disinformation online” founded by 9 organizations brought together by the Google News Lab. Full Fact is a charity based in London to check and correct facts reported in the news. CrossCheck is a new"
Y18-1007,Y18-1000,0,0.173887,"Missing"
Y18-1021,R15-1033,0,0.0371766,"Missing"
Y18-1021,W14-3908,0,0.255069,"Missing"
Y18-1021,W11-3603,0,0.0503485,"Missing"
Y18-1021,W14-3915,0,0.0429203,"Missing"
Y18-1021,N13-1131,0,0.0597845,"Missing"
Y18-1021,Y14-1041,0,0.0447559,"Missing"
Y18-1021,D13-1084,0,0.0262447,"Missing"
Y18-1021,W14-3907,0,0.0403628,"Missing"
Y18-1059,baccianella-etal-2010-sentiwordnet,0,0.255393,"ion category to emotional expression. However, such manual categorization requires an understanding of the emotional content of each expression, which is time-consuming and an arduous task. In (Warriner et al., 2013), emotions are projected as points in 3-dimensional space of valence (positivenessnegativeness), arousal (active-passive), and dominance (dominant-submissive). Using this theory, there is a huge effort on creating valence lexicons like MPQA (Wilson et al., 2005), Norms Lexicon (Warriner et al., 2013), NRC Emotion Lexicon (Mohammad and Bravo-Marquez, 2017a), WordNet Affect Lexicon (Baccianella et al., 2010) and many others. However, these lexicon based approaches usually ignore the intensity of emotions and sentiment, which provides important information for fine-grained sentiment analysis. The current research shifts towards automatic emotion classification which has been proposed for many different kinds of text, including tweets (Mohammad and Kiritchenko, 2015; Mohammad and Bravo-Marquez, 2017a). Existing approaches to analyze intensity are based simply on lexicons, word-embeddings, combinational features and supervised learning. (Nielsen, 2011) introduced lexicon based methods which rely on"
Y18-1059,Q17-1010,0,0.0112601,"sm detection. We can use the architecture of Deep-Emoji and train the model using millions of tweets from social media to get a better representation of new data. Using the pre-trained Deep-Emoji model, we extracted two different set of features - one, 64dimensional vector from the softmax layer and the other, 2304-dimensional vector from attention layer. 5.2 Word-Embedding Features In this paper, we tried four different pretrained word-embedding approaches such as Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), Edinburgh Twitter Corpus (Petrovi´c et al., 2010) and FastText (Bojanowski et al., 2017) for generating word vectors. We used the GloVe model of 300 dimensions. 516 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 5.3 Skip-Thought Features Skip-Thoughts vectors (Kiros et al., 2015) model is in the framework of encoder-decoder models. Here, an encoder maps words to sentence vector and a decoder is used to generate the surrounding sentences. The main advantage of Skip-Thought vectors is that it can produce highly generic sentence representations from an encoder that share both semantic and syn"
Y18-1059,C14-1008,0,0.0202423,"as been proposed for many different kinds of text, including tweets (Mohammad and Kiritchenko, 2015; Mohammad and Bravo-Marquez, 2017a). Existing approaches to analyze intensity are based simply on lexicons, word-embeddings, combinational features and supervised learning. (Nielsen, 2011) introduced lexicon based methods which rely on lexicons to assign the intensity score of each word in the tweet. However, this method did not consider the semantic information from the text. Some supervised methods like deep neural networks were applied to tweet sentiment analysis to predict the polarity (dos Santos and Gatti, 2014). Although deep learning methods outperform lexicon based methods as shown in (dos Santos and Gatti, 2014), but could not capture the fine-grained property of the sentiment in a text. To capture this finegrained aspect of a sentiment, (Mohammad, 2016) proposed to identify the intensity of emotion in texts. To further expand the scope of emotion analysis, (Mohammad and Bravo-Marquez, 2017b; Mohammad et al., 2018) introduced EmoInt-2017 and SemEval-2018 shared tasks where the top performing teams use deep learning models such as CNN, RNN, LSTMs (Goel et al., 2017; K¨oper et al., 2017) and classi"
Y18-1059,W17-5228,0,0.0173242,"based methods as shown in (dos Santos and Gatti, 2014), but could not capture the fine-grained property of the sentiment in a text. To capture this finegrained aspect of a sentiment, (Mohammad, 2016) proposed to identify the intensity of emotion in texts. To further expand the scope of emotion analysis, (Mohammad and Bravo-Marquez, 2017b; Mohammad et al., 2018) introduced EmoInt-2017 and SemEval-2018 shared tasks where the top performing teams use deep learning models such as CNN, RNN, LSTMs (Goel et al., 2017; K¨oper et al., 2017) and classifiers like Support Vector Machine or Random Forest (Duppada and Hiray, 2017; K¨oper et al., 2017). In the above two tasks, some participants use an ensemble-based approach by simply averaging the outputs of two top performing models (Duppada and Hiray, 2017; Duppada et al., 2018) and the weighted average of predicted outputs of three different deep neural network based models (Goel et al., 2017). The subtasks of SemEval-2018 Task-1, AIT (Mohammad et al., 2018) are detailed in Section 3. The structure of the paper is as follows. In section 3, we describe the dataset. Section 4 describes the approach we are using to build the model, while section 5 discusses the approa"
Y18-1059,S18-1002,0,0.0111634,"d to identify the intensity of emotion in texts. To further expand the scope of emotion analysis, (Mohammad and Bravo-Marquez, 2017b; Mohammad et al., 2018) introduced EmoInt-2017 and SemEval-2018 shared tasks where the top performing teams use deep learning models such as CNN, RNN, LSTMs (Goel et al., 2017; K¨oper et al., 2017) and classifiers like Support Vector Machine or Random Forest (Duppada and Hiray, 2017; K¨oper et al., 2017). In the above two tasks, some participants use an ensemble-based approach by simply averaging the outputs of two top performing models (Duppada and Hiray, 2017; Duppada et al., 2018) and the weighted average of predicted outputs of three different deep neural network based models (Goel et al., 2017). The subtasks of SemEval-2018 Task-1, AIT (Mohammad et al., 2018) are detailed in Section 3. The structure of the paper is as follows. In section 3, we describe the dataset. Section 4 describes the approach we are using to build the model, while section 5 discusses the approaches of preprocessing and feature extraction. Section 6 presents comparative results of various models along with the analysis of the results. Section 7 presents concluding remarks and future work. 3 Datas"
Y18-1059,D17-1169,0,0.0269181,"ng & Feature Extraction To preprocess each tweet, first we break all the contractions (like “can’t” to “cannot”, “I’m” to “I am” etc,.) followed by spelling corrections, decoding special words and acronyms (like “e g” to “eg”, “fb” to “facebook” etc,.) and symbol replacements (like “$” to “dollar”, “=” to “is equal to” etc). Later, we tokenized each tweet using NLTK tweet tokenizer 2 . The basic idea of using different experts and eclectic features is from the intuition that each expert learn from different aspects of the concatenated 2 https://www.nltk.org/api/nltk.tokenize. html Deep-Emoji (Felbo et al., 2017) performs prediction using the model trained on a dataset of 1246 million tweets and achieves state-of-the-art performance within sentiment, emotion and sarcasm detection. We can use the architecture of Deep-Emoji and train the model using millions of tweets from social media to get a better representation of new data. Using the pre-trained Deep-Emoji model, we extracted two different set of features - one, 64dimensional vector from the softmax layer and the other, 2304-dimensional vector from attention layer. 5.2 Word-Embedding Features In this paper, we tried four different pretrained word-e"
Y18-1059,W17-5207,0,0.0185948,"predict the polarity (dos Santos and Gatti, 2014). Although deep learning methods outperform lexicon based methods as shown in (dos Santos and Gatti, 2014), but could not capture the fine-grained property of the sentiment in a text. To capture this finegrained aspect of a sentiment, (Mohammad, 2016) proposed to identify the intensity of emotion in texts. To further expand the scope of emotion analysis, (Mohammad and Bravo-Marquez, 2017b; Mohammad et al., 2018) introduced EmoInt-2017 and SemEval-2018 shared tasks where the top performing teams use deep learning models such as CNN, RNN, LSTMs (Goel et al., 2017; K¨oper et al., 2017) and classifiers like Support Vector Machine or Random Forest (Duppada and Hiray, 2017; K¨oper et al., 2017). In the above two tasks, some participants use an ensemble-based approach by simply averaging the outputs of two top performing models (Duppada and Hiray, 2017; Duppada et al., 2018) and the weighted average of predicted outputs of three different deep neural network based models (Goel et al., 2017). The subtasks of SemEval-2018 Task-1, AIT (Mohammad et al., 2018) are detailed in Section 3. The structure of the paper is as follows. In section 3, we describe the dat"
Y18-1059,W17-5206,0,0.0383407,"Missing"
Y18-1059,S17-1007,0,0.0386988,"Missing"
Y18-1059,W17-5205,0,0.0285696,"Missing"
Y18-1059,S18-1001,0,0.0189676,"method did not consider the semantic information from the text. Some supervised methods like deep neural networks were applied to tweet sentiment analysis to predict the polarity (dos Santos and Gatti, 2014). Although deep learning methods outperform lexicon based methods as shown in (dos Santos and Gatti, 2014), but could not capture the fine-grained property of the sentiment in a text. To capture this finegrained aspect of a sentiment, (Mohammad, 2016) proposed to identify the intensity of emotion in texts. To further expand the scope of emotion analysis, (Mohammad and Bravo-Marquez, 2017b; Mohammad et al., 2018) introduced EmoInt-2017 and SemEval-2018 shared tasks where the top performing teams use deep learning models such as CNN, RNN, LSTMs (Goel et al., 2017; K¨oper et al., 2017) and classifiers like Support Vector Machine or Random Forest (Duppada and Hiray, 2017; K¨oper et al., 2017). In the above two tasks, some participants use an ensemble-based approach by simply averaging the outputs of two top performing models (Duppada and Hiray, 2017; Duppada et al., 2018) and the weighted average of predicted outputs of three different deep neural network based models (Goel et al., 2017). The subtasks of"
Y18-1059,D14-1162,0,0.0797588,"Missing"
Y18-1059,W10-0513,0,0.0971362,"Missing"
Y18-1059,P14-2070,0,0.0188167,"ient Boosting, Random Forest, and Neural Network(NN) for subtasks EI-reg and V-reg. While for the subtasks EIoc and V-oc, we consider all the models except NN model. For subtask E-c, we consider all the models except Light Gradient Boosting model. Model 5.5 Hash-tag Intensity Features The work by (Mohammad and Bravo-Marquez, 2017a) describes that removal of the emotion word hashtags causes the emotional intensity of the tweet to drop. This indicates that emotion word hashtags are not redundant with the rest of the tweet in terms of the overall intensity. Here, we used Depeche mood dictionary (Staiano and Guerini, 2014) to get the intensities of hashtag words. We average the intensities of all hashtags of a single tweet to get the total intensity score. 5.6 Stylometric Features Tweets and other electronic messages (e-mails, posts, etc.) are written far shorter, way more informal and much richer in terms of expressive elements Gradient Boosting XGBoosting Neural Network Random Forest Light Gradient Boosting Parameters n estimators: 3000, Learning rate: 0.05 Max depth: 4 n estimators: 100 learning rate: 0.1 max depth: 3 Optimizer: adam Activation : relu n estimators: 250 max depth: 4 n estimators: 720 learning"
Y18-1059,H05-1116,0,0.0629778,"n from the tweet. We compared the results of our Experts Model with both baseline results and top five performers of SemEval-2018 Task-1, Affect in Tweets (AIT). The experimental results show that our proposed approach deals with the emotion detection problem and stands at top-5 results. 1 Introduction Sentiment analysis is one of the most famous Natural Language Processing (NLP) tasks. This task was used in social network services (Pang et al., 2008; Asur and Huberman, 2010), e-retailing, advertising (Qiu et al., 2010; Jin et al., 2007), question answering systems (Somasundaran et al., 2007; Stoyanov et al., 2005) and many other domains. It focuses on the automatic prediction of polarity or sentiment on tweets or reviews. While most computer science research in this field has focused on strict positive/negative sentiment analysis, the three dominant theories (Marsella et al., 2010; Stelmack and Stalikas, 1991) of emotion agree that humans express or operate with much more nuanced emotion representations. In other words, tweets or reviews, in recent times, include non-standard representations of emotion like emoticons, emojis etc. This task of sentiment analysis became increasingly complex due to an add"
Y18-1059,H05-1044,0,0.118247,"et al., 2010; Wilson et al., 2004; Liu and Zhang, 2012). Many prior works of emotion detection have always used manual strategies to map emotion category to emotional expression. However, such manual categorization requires an understanding of the emotional content of each expression, which is time-consuming and an arduous task. In (Warriner et al., 2013), emotions are projected as points in 3-dimensional space of valence (positivenessnegativeness), arousal (active-passive), and dominance (dominant-submissive). Using this theory, there is a huge effort on creating valence lexicons like MPQA (Wilson et al., 2005), Norms Lexicon (Warriner et al., 2013), NRC Emotion Lexicon (Mohammad and Bravo-Marquez, 2017a), WordNet Affect Lexicon (Baccianella et al., 2010) and many others. However, these lexicon based approaches usually ignore the intensity of emotions and sentiment, which provides important information for fine-grained sentiment analysis. The current research shifts towards automatic emotion classification which has been proposed for many different kinds of text, including tweets (Mohammad and Kiritchenko, 2015; Mohammad and Bravo-Marquez, 2017a). Existing approaches to analyze intensity are based s"
Y18-1080,D17-1254,0,0.0116573,"ur n-grams shown in the n-gram split of the word, only two ([खा] ([KA]) and [गा] ([gA])) carry grammatical information. The other two are meaningless. While the morpheme - [ए] ([e]) - is not even generated as its size is 1. A lot of work has been done in task-independent Sentence Classification in the last few years. Scholars have used a number of different neural network architectures for the purpose. Kim (2014) and Kalchbrenner et al. (2014) used CNNs for the task. These works were amongst the earliest on taskindependent classification, and were later extended and modified by several works (Gan et al., 2017; Zhang et al., 2017; Li et al., 2017). Zhang et al. (2015), Zhang and LeCun (2015) and Becker et al. (2017) used very deep CNNs to extract information from character level input. Wehrmann et al. (2017) on the other hand, using a model similar to that of Kim (2014), showed that a single convolution layer with character embeddings as input is good enough for classification purposes. These works only used character unigrams as input. Tummalapalli et al. (2018) showed that character-ngram input can lead to much better performance in morphologically rich languages as compared to character unigrams"
Y18-1080,D08-1097,0,0.0429306,"uages - Hindi and Telugu. 1 Introduction Sentence Classification is one of the most fundamental tasks in Natural Language Processing, where a given sentence is required to be classified into a pre-defined set of classes. This pre-defined set depends upon the specific task under consideration, which could be any of Sentiment Analysis, Question Classification, Subjectivity Analysis etc. There has been a lot of work done on each of these tasks separately, where the features and techniques chosen for classification are specific to the particular radhika.mamidi@iiit.ac.in task (Silva et al., 2011; Huang et al., 2008; Pang and Lee, 2005a; Kouloumpis et al., 2011). However, the recent popularity and developments in Neural Networks have enabled researchers to build classifier models that are more generic in nature (Kim, 2014; Kalchbrenner et al., 2014). Given sufficient training data, these networks have the ability to learn task specific features for classification. Since these models do not require hand-crafted features, they make an attractive choice for the construction of task-independent or language-independent systems. There are a huge number of languages used in different parts of the world and it i"
Y18-1080,P14-1062,0,0.329857,"defined set depends upon the specific task under consideration, which could be any of Sentiment Analysis, Question Classification, Subjectivity Analysis etc. There has been a lot of work done on each of these tasks separately, where the features and techniques chosen for classification are specific to the particular radhika.mamidi@iiit.ac.in task (Silva et al., 2011; Huang et al., 2008; Pang and Lee, 2005a; Kouloumpis et al., 2011). However, the recent popularity and developments in Neural Networks have enabled researchers to build classifier models that are more generic in nature (Kim, 2014; Kalchbrenner et al., 2014). Given sufficient training data, these networks have the ability to learn task specific features for classification. Since these models do not require hand-crafted features, they make an attractive choice for the construction of task-independent or language-independent systems. There are a huge number of languages used in different parts of the world and it is difficult and expensive to build resources for each language. Also, building an end-to-end task-specific languagespecific system for every major language is a very tedious and expensive task. We, thus need to build systems which are gen"
Y18-1080,D14-1181,0,0.342406,". This pre-defined set depends upon the specific task under consideration, which could be any of Sentiment Analysis, Question Classification, Subjectivity Analysis etc. There has been a lot of work done on each of these tasks separately, where the features and techniques chosen for classification are specific to the particular radhika.mamidi@iiit.ac.in task (Silva et al., 2011; Huang et al., 2008; Pang and Lee, 2005a; Kouloumpis et al., 2011). However, the recent popularity and developments in Neural Networks have enabled researchers to build classifier models that are more generic in nature (Kim, 2014; Kalchbrenner et al., 2014). Given sufficient training data, these networks have the ability to learn task specific features for classification. Since these models do not require hand-crafted features, they make an attractive choice for the construction of task-independent or language-independent systems. There are a huge number of languages used in different parts of the world and it is difficult and expensive to build resources for each language. Also, building an end-to-end task-specific languagespecific system for every major language is a very tedious and expensive task. We, thus need to"
Y18-1080,C02-1150,0,0.144427,"tated sentences. Some statistics for the sentiment analysis datasets have been listed in Table 1. Since none of the Sentiment Analysis datasets consist of a train-test split, we perform 10-fold cross-validation to obtain results on them. Dataset MR Senti-Hi Senti-Te Positive 5331 2240 1491 Neutral 3408 2477 Negative 5331 932 1441 Table 1: Statistics for Sentiment Analysis datasets. The table shows the number of sentences of each sentiment in different datasets. The other two datasets are for Question Classification in English and Hindi. The English dataset, TREC-En, is the TREC-UIUC3 dataset (Li and Roth, 2002). It has 6 core classes, namely Abbreviation, Description, Entity, Human, Location, Numeric, and 50 fine classes. This dataset was released with a train-test split, with 5452 questions in the train set and 500 in the test set. The Hindi dataset, TREC-Hi (Tummalapalli et al., 2018) was prepared by translating the TREC-En dataset into Hindi. The classes are the same in both the datasets. This dataset consists of 5444 questions in the train set and 499 in test set. We consider classification into the 6 core classes for this work. Statistics for the question classification datasets have been given"
Y18-1080,D17-1201,0,0.0167617,"f the word, only two ([खा] ([KA]) and [गा] ([gA])) carry grammatical information. The other two are meaningless. While the morpheme - [ए] ([e]) - is not even generated as its size is 1. A lot of work has been done in task-independent Sentence Classification in the last few years. Scholars have used a number of different neural network architectures for the purpose. Kim (2014) and Kalchbrenner et al. (2014) used CNNs for the task. These works were amongst the earliest on taskindependent classification, and were later extended and modified by several works (Gan et al., 2017; Zhang et al., 2017; Li et al., 2017). Zhang et al. (2015), Zhang and LeCun (2015) and Becker et al. (2017) used very deep CNNs to extract information from character level input. Wehrmann et al. (2017) on the other hand, using a model similar to that of Kim (2014), showed that a single convolution layer with character embeddings as input is good enough for classification purposes. These works only used character unigrams as input. Tummalapalli et al. (2018) showed that character-ngram input can lead to much better performance in morphologically rich languages as compared to character unigrams. Recurrent and recursive networks wer"
Y18-1080,W17-5408,1,0.869377,"rchical attention networks for classification, while Zhang et al. (2016a) combined LSTMs and CNNs for a dependency sensitive model. In this paper, we experiment using models described by Tummalapalli et al. (2018) which was an extension of Kim (2014). Little work has been done in Hindi and Telugu 699 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 sentence classification. Although, significant work has been done in Hindi and Telugu sentiment analysis (Singhal and Bhattacharyya, 2016; Mukku et al., 2016; Mukku and Mamidi, 2017). Tummalapalli et al. (2018) compared various deep architectures for morphologically rich languages and proposed an ensemble based model for sentence classification, where multiple CNN models trained on different inputs were combined by taking the average of the final layer outputs. CNN models combining multiple inputs were also proposed by Yin and Sch¨utze (2016), who combined different types of word embeddings at the convolution stage, and Zhang et al. (2016b), who performed independent convolution operations on all inputs and combined them at the penultimate layer. Syllables have been used"
Y18-1080,P05-1015,0,0.753957,"lugu. 1 Introduction Sentence Classification is one of the most fundamental tasks in Natural Language Processing, where a given sentence is required to be classified into a pre-defined set of classes. This pre-defined set depends upon the specific task under consideration, which could be any of Sentiment Analysis, Question Classification, Subjectivity Analysis etc. There has been a lot of work done on each of these tasks separately, where the features and techniques chosen for classification are specific to the particular radhika.mamidi@iiit.ac.in task (Silva et al., 2011; Huang et al., 2008; Pang and Lee, 2005a; Kouloumpis et al., 2011). However, the recent popularity and developments in Neural Networks have enabled researchers to build classifier models that are more generic in nature (Kim, 2014; Kalchbrenner et al., 2014). Given sufficient training data, these networks have the ability to learn task specific features for classification. Since these models do not require hand-crafted features, they make an attractive choice for the construction of task-independent or language-independent systems. There are a huge number of languages used in different parts of the world and it is difficult and expe"
Y18-1080,C16-1287,0,0.0123805,"alli et al., 2018). Yang et al. (2016) explored hierarchical attention networks for classification, while Zhang et al. (2016a) combined LSTMs and CNNs for a dependency sensitive model. In this paper, we experiment using models described by Tummalapalli et al. (2018) which was an extension of Kim (2014). Little work has been done in Hindi and Telugu 699 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 sentence classification. Although, significant work has been done in Hindi and Telugu sentiment analysis (Singhal and Bhattacharyya, 2016; Mukku et al., 2016; Mukku and Mamidi, 2017). Tummalapalli et al. (2018) compared various deep architectures for morphologically rich languages and proposed an ensemble based model for sentence classification, where multiple CNN models trained on different inputs were combined by taking the average of the final layer outputs. CNN models combining multiple inputs were also proposed by Yin and Sch¨utze (2016), who combined different types of word embeddings at the convolution stage, and Zhang et al. (2016b), who performed independent convolution operations on all inputs and combined them at the"
Y18-1080,D13-1170,0,0.00687716,". (2015), Zhang and LeCun (2015) and Becker et al. (2017) used very deep CNNs to extract information from character level input. Wehrmann et al. (2017) on the other hand, using a model similar to that of Kim (2014), showed that a single convolution layer with character embeddings as input is good enough for classification purposes. These works only used character unigrams as input. Tummalapalli et al. (2018) showed that character-ngram input can lead to much better performance in morphologically rich languages as compared to character unigrams. Recurrent and recursive networks were also used (Socher et al., 2013; Dong et al., 2014; Irsoy and Cardie, 2014), some of them depend upon parse trees, which makes it difficult to adapt them to languages with scarce resources. It has also been noted that CNNs often give better performance than LSTMs in sentence classification (Kim, 2014; Tummalapalli et al., 2018). Yang et al. (2016) explored hierarchical attention networks for classification, while Zhang et al. (2016a) combined LSTMs and CNNs for a dependency sensitive model. In this paper, we experiment using models described by Tummalapalli et al. (2018) which was an extension of Kim (2014). Little work has"
Y18-1080,W10-1401,0,0.0200639,"Missing"
Y18-1080,N16-1174,0,0.052277,"tion purposes. These works only used character unigrams as input. Tummalapalli et al. (2018) showed that character-ngram input can lead to much better performance in morphologically rich languages as compared to character unigrams. Recurrent and recursive networks were also used (Socher et al., 2013; Dong et al., 2014; Irsoy and Cardie, 2014), some of them depend upon parse trees, which makes it difficult to adapt them to languages with scarce resources. It has also been noted that CNNs often give better performance than LSTMs in sentence classification (Kim, 2014; Tummalapalli et al., 2018). Yang et al. (2016) explored hierarchical attention networks for classification, while Zhang et al. (2016a) combined LSTMs and CNNs for a dependency sensitive model. In this paper, we experiment using models described by Tummalapalli et al. (2018) which was an extension of Kim (2014). Little work has been done in Hindi and Telugu 699 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 sentence classification. Although, significant work has been done in Hindi and Telugu sentiment analysis (Singhal and Bhattacharyya, 2016; Mukku"
Y18-1080,N16-1177,0,0.0273235,"Missing"
Y18-1080,N16-1178,0,0.0309833,"Missing"
