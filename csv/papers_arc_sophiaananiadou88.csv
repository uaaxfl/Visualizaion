2021.naacl-main.2,Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors,2021,-1,-1,3,1,3221,fenia christopoulou,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a multi-task, probabilistic approach to facilitate distantly supervised relation extraction by bringing closer the representations of sentences that contain the same Knowledge Base pairs. To achieve this, we bias the latent space of sentences via a Variational Autoencoder (VAE) that is trained jointly with a relation classifier. The latent code guides the pair representations and influences sentence reconstruction. Experimental results on two datasets created via distant supervision indicate that multi-task learning results in performance benefits. Additional exploration of employing Knowledge Base priors into theVAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results."
2021.findings-emnlp.182,{G}enerative{RE}: Incorporating a Novel Copy Mechanism and Pretrained Model for Joint Entity and Relation Extraction,2021,-1,-1,2,0,2082,jiarun cao,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Previous neural Seq2Seq models have shown the effectiveness for jointly extracting relation triplets. However, most of these models suffer from incompletion and disorder problems when they extract multi-token entities from input sentences. To tackle these problems, we propose a generative, multi-task learning framework, named GenerativeRE. We firstly propose a special entity labelling method on both input and output sequences. During the training stage, GenerativeRE fine-tunes the pre-trained generative model and learns the special entity labels simultaneously. During the inference stage, we propose a novel copy mechanism equipped with three mask strategies, to generate the most probable tokens by diminishing the scope of the model decoder. Experimental results show that our model achieves 4.6{\%} and 0.9{\%} F1 score improvements over the current state-of-the-art methods in the NYT24 and NYT29 benchmark datasets respectively."
2021.findings-acl.77,Investigating Text Simplification Evaluation,2021,-1,-1,4,0,7690,laura vasquezrodriguez,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.eacl-main.135,{S}pan{E}mo: Casting Multi-label Emotion Classification as Span-prediction,2021,-1,-1,2,0.740741,10719,hassan alhuzali,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Emotion recognition (ER) is an important task in Natural Language Processing (NLP), due to its high impact in real-world applications from health and well-being to author profiling, consumer analysis and security. Current approaches to ER, mainly classify emotions independently without considering that emotions can co-exist. Such approaches overlook potential ambiguities, in which multiple emotions overlap. We propose a new model {``}SpanEmo{''} casting multi-label emotion classification as span-prediction, which can aid ER models to learn associations between labels and words in a sentence. Furthermore, we introduce a loss function focused on modelling multiple co-existing emotions in the input sentence. Experiments performed on the SemEval2018 multi-label emotion data over three language sets (i.e., English, Arabic and Spanish) demonstrate our method{'}s effectiveness. Finally, we present different analyses that illustrate the benefits of our method in terms of improving the model performance and learning meaningful associations between emotion classes and words in the sentence."
2021.eacl-demos.28,Paladin: an annotation tool based on active and proactive learning,2021,-1,-1,3,1,11105,minhquoc nghiem,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,0,"In this paper, we present Paladin, an open-source web-based annotation tool for creating high-quality multi-label document-level datasets. By integrating active learning and proactive learning to the annotation task, Paladin makes the task less time-consuming and requiring less human effort. Although Paladin is designed for multi-label settings, the system is flexible and can be adapted to other tasks in single-label settings."
2020.lrec-1.245,Semantic Annotation for Improved Safety in Construction Work,2020,0,0,4,0,17111,paul thompson,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Risk management is a vital activity to ensure employee safety in construction projects. Various documents provide important supporting evidence, including details of previous incidents, consequences and mitigation strategies. Potential hazards may depend on a complex set of project-specific attributes, including activities undertaken, location, equipment used, etc. However, finding evidence about previous projects with similar attributes can be problematic, since information about risks and mitigations is usually hidden within and may be dispersed across a range of different free text documents. Automatic named entity recognition (NER), which identifies mentions of concepts in free text documents, is the first stage in structuring knowledge contained within them. While developing NER methods generally relies on annotated corpora, we are not aware of any such corpus targeted at concepts relevant to construction safety. In response, we have designed a novel named entity annotation scheme and associated guidelines for this domain, which covers hazards, consequences, mitigation strategies and project attributes. Four health and safety experts used the guidelines to annotate a total of 600 sentences from accident reports; an average inter-annotator agreement rate of 0.79 F-Score shows that our work constitutes an important first step towards developing tools for detailed semantic analysis of construction safety documents."
2020.coling-main.507,A Neural Model for Aggregating Coreference Annotation in Crowdsourcing,2020,-1,-1,3,1,21605,maolin li,Proceedings of the 28th International Conference on Computational Linguistics,0,"Coreference resolution is the task of identifying all mentions in a text that refer to the same real-world entity. Collecting sufficient labelled data from expert annotators to train a high-performance coreference resolution system is time-consuming and expensive. Crowdsourcing makes it possible to obtain the required amounts of data rapidly and cost-effectively. However, crowd-sourced labels can be noisy. To ensure high-quality data, it is crucial to infer the correct labels by aggregating the noisy labels. In this paper, we split the aggregation into two subtasks, i.e, mention classification and coreference chain inference. Firstly, we predict the general class of each mention using an autoencoder, which incorporates contextual information about each mention, while at the same time taking into account the mention{'}s annotation complexity and annotators{'} reliability at different levels. Secondly, to determine the coreference chain of each mention, we use weighted voting which takes into account the learned reliability in the first subtask. Experimental results demonstrate the effectiveness of our method in predicting the correct labels. We also illustrate our model{'}s interpretability through a comprehensive analysis of experimental results."
2020.acl-main.669,Revisiting Unsupervised Relation Extraction,2020,19,0,3,0,23070,thy tran,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Unsupervised relation extraction (URE) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (KBs). URE methods can be categorised into generative and discriminative approaches, which rely either on hand-crafted features or surface form. However, we demonstrate that by using only named entities to induce relation types, we can outperform existing methods on two popular datasets. We conduct a comparison and evaluation of our findings with other URE techniques, to ascertain the important features in URE. We conclude that entity types provide a strong inductive bias for URE."
W19-5036,Improving classification of Adverse Drug Reactions through Using Sentiment Analysis and Transfer Learning,2019,0,0,2,0.740741,10719,hassan alhuzali,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"The availability of large-scale and real-time data on social media has motivated research into adverse drug reactions (ADRs). ADR classification helps to identify negative effects of drugs, which can guide health professionals and pharmaceutical companies in making medications safer and advocating patients{'} safety. Based on the observation that in social media, negative sentiment is frequently expressed towards ADRs, this study presents a neural model that combines sentiment analysis with transfer learning techniques to improve ADR detection in social media postings. Our system is firstly trained to classify sentiment in tweets concerning current affairs, using the SemEval17-task4A corpus. We then apply transfer learning to adapt the model to the task of detecting ADRs in social media postings. We show that, in combination with rich representations of words and their contexts, transfer learning is beneficial, especially given the large degree of vocabulary overlap between the current affairs posts in the SemEval17-task4A corpus and posts about ADRs. We compare our results with previous approaches, and show that our model can outperform them by up to 3{\%} F-score."
P19-1423,Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network,2019,26,2,4,0,22858,sunil sahu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction."
N19-1295,Modelling Instance-Level Annotator Reliability for Natural Language Labelling Tasks,2019,0,0,4,1,21605,maolin li,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"When constructing models that learn from noisy labels produced by multiple annotators, it is important to accurately estimate the reliability of annotators. Annotators may provide labels of inconsistent quality due to their varying expertise and reliability in a domain. Previous studies have mostly focused on estimating each annotator{'}s overall reliability on the entire annotation task. However, in practice, the reliability of an annotator may depend on each specific instance. Only a limited number of studies have investigated modelling per-instance reliability and these only considered binary labels. In this paper, we propose an unsupervised model which can handle both binary and multi-class labels. It can automatically estimate the per-instance reliability of each annotator and the correct label for each instance. We specify our model as a probabilistic model which incorporates neural networks to model the dependency between latent variables and instances. For evaluation, the proposed method is applied to both synthetic and real data, including two labelling tasks: text classification and textual entailment. Experimental results demonstrate our novel method can not only accurately estimate the reliability of annotators across different instances, but also achieve superior performance in predicting the correct labels and detecting the least reliable annotators compared to state-of-the-art baselines."
D19-5727,Coreference Resolution in Full Text Articles with {BERT} and Syntax-based Mention Filtering,2019,0,0,6,1,26538,hailong trieu,Proceedings of The 5th Workshop on BioNLP Open Shared Tasks,0,"This paper describes our system developed for the coreference resolution task of the CRAFT Shared Tasks 2019. The CRAFT corpus is more challenging than other existing corpora because it contains full text articles. We have employed an existing span-based state-of-theart neural coreference resolution system as a baseline system. We enhance the system with two different techniques to capture longdistance coreferent pairs. Firstly, we filter noisy mentions based on parse trees with increasing the number of antecedent candidates. Secondly, instead of relying on the LSTMs, we integrate the highly expressive language model{--}BERT into our model. Experimental results show that our proposed systems significantly outperform the baseline. The best performing system obtained F-scores of 44{\%}, 48{\%}, 39{\%}, 49{\%}, 40{\%}, and 57{\%} on the test set with B3, BLANC, CEAFE, CEAFM, LEA, and MUC metrics, respectively. Additionally, the proposed model is able to detect coreferent pairs in long distances, even with a distance of more than 200 sentences."
D19-1381,A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection,2019,0,0,3,1,26991,kurt espinosa,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"We tackle the nested and overlapping event detection task and propose a novel search-based neural network (SBNN) structured prediction model that treats the task as a search problem on a relation graph of trigger-argument structures. Unlike existing structured prediction tasks such as dependency parsing, the task targets to detect DAG structures, which constitute events, from the relation graph. We define actions to construct events and use all the beams in a beam search to detect all event structures that may be overlapping and nested. The search process constructs events in a bottom-up manner while modelling the global properties for nested and overlapping structures simultaneously using neural networks. We show that the model achieves performance comparable to the state-of-the-art model Turku Event Extraction System (TEES) on the BioNLP Cancer Genetics (CG) Shared Task 2013 without the use of any syntactic and hand-engineered features. Further analyses on the development set show that our model is more computationally efficient while yielding higher F1-score performance."
D19-1498,Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs,2019,0,1,3,1,3221,fenia christopoulou,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach."
W18-2324,Investigating Domain-Specific Information for Neural Coreference Resolution on Biomedical Texts,2018,0,1,4,1,26538,hailong trieu,Proceedings of the {B}io{NLP} 2018 workshop,0,"Existing biomedical coreference resolution systems depend on features and/or rules based on syntactic parsers. In this paper, we investigate the utility of the state-of-the-art general domain neural coreference resolution system on biomedical texts. The system is an end-to-end system without depending on any syntactic parsers. We also investigate the domain specific features to enhance the system for biomedical texts. Experimental results on the BioNLP Protein Coreference dataset and the CRAFT corpus show that, with no parser information, the adapted system compared favorably with the systems that depend on parser information on these datasets, achieving 51.23{\%} on the BioNLP dataset and 36.33{\%} on the CRAFT corpus in F1 score. In-domain embeddings and domain-specific features helped improve the performance on the BioNLP dataset, but they did not on the CRAFT corpus."
W18-1302,Paths for uncertainty: Exploring the intricacies of uncertainty identification for news,2018,0,0,2,1,7229,chrysoula zerva,Proceedings of the Workshop on Computational Semantics beyond Events and Roles,0,"Currently, news articles are produced, shared and consumed at an extremely rapid rate. Although their quantity is increasing, at the same time, their quality and trustworthiness is becoming fuzzier. Hence, it is important not only to automate information extraction but also to quantify the certainty of this information. Automated identification of certainty has been studied both in the scientific and newswire domains, but performance is considerably higher in tasks focusing on scientific text. We compare the differences in the definition and expression of uncertainty between a scientific domain, i.e., biomedicine, and newswire. We delve into the different aspects that affect the certainty of an extracted event in a news article and examine whether they can be easily identified by techniques already validated in the biomedical domain. Finally, we present a comparison of the syntactic and lexical differences between the the expression of certainty in the biomedical and newswire domains, using two annotated corpora."
P18-2014,A Walk-based Model on Entity Graphs for Relation Extraction,2018,0,25,3,1,3221,fenia christopoulou,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present a novel graph-based neural network model for relation extraction. Our model treats multiple pairs in a sentence simultaneously and considers interactions among them. All the entities in a sentence are placed as nodes in a fully-connected graph structure. The edges are represented with position-aware contexts around the entity pairs. In order to consider different relation paths between two entities, we construct up to $l$-length walks between each pair. The resulting walks are merged and iteratively used to update the edge representations into longer walks representations. We show that the model achieves performance comparable to the state-of-the-art systems on the ACE 2005 dataset without using any external tools."
N18-1131,A Neural Layered Model for Nested Named Entity Recognition,2018,0,38,3,0,29451,meizhi ju,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Entity mentions embedded in longer entity mentions are referred to as nested entities. Most named entity recognition (NER) systems deal only with the flat entities and ignore the inner nested ones, which fails to capture finer-grained semantic information in underlying texts. To address this issue, we propose a novel neural model to identify nested entities by dynamically stacking flat NER layers. Each flat NER layer is based on the state-of-the-art flat NER model that captures sequential context representation with bidirectional Long Short-Term Memory (LSTM) layer and feeds it to the cascaded CRF layer. Our model merges the output of the LSTM layer in the current flat NER layer to build new representation for detected entities and subsequently feeds them into the next flat NER layer. This allows our model to extract outer entities by taking full advantage of information encoded in their corresponding inner entities, in an inside-to-outside way. Our model dynamically stacks the flat NER layers until no outer entities are extracted. Extensive evaluation shows that our dynamic model outperforms state-of-the-art feature-based systems on nested NER, achieving 74.7{\%} and 72.2{\%} on GENIA and ACE2005 datasets, respectively, in terms of F-score."
L18-1042,A New Corpus to Support Text Mining for the Curation of Metabolites in the {C}h{EBI} Database,2018,0,0,8,0.784314,1608,matthew shardlow,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We present a new corpus of 200 abstracts and 100 full text papers which have been annotated with named entities and relations in the biomedical domain as part of the OpenMinTeD project. This corpus facilitates the goal in OpenMinTeD of making text and data mining accessible to the users who need it most. We describe the process we took to annotate the corpus with entities (Metabolite, Chemical, Protein, Species, Biological Activity and Spectral Data) and relations (Isolated From, Associated With, Binds With and Metabolite Of ). We report inter-annotator agreement (using F-score) for entities of between 0.796 and 0.892 using a strict matching protocol and between 0.875 and 0.963 using a relaxed matching protocol. For relations we report inter annotator agreement of between 0.591 and 0.693 using a strict matching protocol and between 0.744 and 0.793 using a relaxed matching protocol. We describe how this corpus can be used within ChEBI to facilitate text and data mining and how the integration of this work with the OpenMinTeD text and data mining platform will aid curation of ChEBI and other biomedical databases."
D18-2019,{APL}enty: annotation tool for creating high-quality datasets using active and proactive learning,2018,0,3,2,1,11105,minhquoc nghiem,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"In this paper, we present APLenty, an annotation tool for creating high-quality sequence labeling datasets using active and proactive learning. A major innovation of our tool is the integration of automatic annotation with active learning and proactive learning. This makes the task of creating labeled datasets easier, less time-consuming and requiring less human effort. APLenty is highly flexible and can be adapted to various other tasks."
W17-2314,Proactive Learning for Named Entity Recognition,2017,14,3,3,1,21605,maolin li,{B}io{NLP} 2017,0,"The goal of active learning is to minimise the cost of producing an annotated dataset, in which annotators are assumed to be perfect, i.e., they always choose the correct labels. However, in practice, annotators are not infallible, and they are likely to assign incorrect labels to some instances. Proactive learning is a generalisation of active learning that can model different kinds of annotators. Although proactive learning has been applied to certain labelling tasks, such as text classification, there is little work on its application to named entity (NE) tagging. In this paper, we propose a proactive learning method for producing NE annotated corpora, using two annotators with different levels of expertise, and who charge different amounts based on their levels of experience. To optimise both cost and annotation quality, we also propose a mechanism to present multiple sentences to annotators at each iteration. Experimental results for several corpora show that our method facilitates the construction of high-quality NE labelled datasets at minimal cost."
E17-1093,Distributed Document and Phrase Co-embeddings for Descriptive Clustering,2017,22,2,7,0,25550,motoki sato,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"Descriptive document clustering aims to automatically discover groups of semantically related documents and to assign a meaningful label to characterise the content of each cluster. In this paper, we present a descriptive clustering approach that employs a distributed representation model, namely the paragraph vector model, to capture semantic similarities between documents and phrases. The proposed method uses a joint representation of phrases and documents (i.e., a co-embedding) to automatically select a descriptive phrase that best represents each document cluster. We evaluate our method by comparing its performance to an existing state-of-the-art descriptive clustering method that also uses co-embedding but relies on a bag-of-words representation. Results obtained on benchmark datasets demonstrate that the paragraph vector-based method obtains superior performance over the existing approach in both identifying clusters and assigning appropriate descriptive labels to them."
W16-3921,Learning to recognise named entities in tweets by exploiting weakly labelled data,2016,25,1,3,1,26991,kurt espinosa,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"Named entity recognition (NER) in social media (e.g., Twitter) is a challenging task due to the noisy nature of text. As part of our participation in the W-NUT 2016 Named Entity Recognition Shared Task, we proposed an unsupervised learning approach using deep neural networks and leverage a knowledge base (i.e., DBpedia) to bootstrap sparse entity types with weakly labelled data. To further boost the performance, we employed a more sophisticated tagging scheme and applied dropout as a regularisation technique in order to reduce overfitting. Even without hand-crafting linguistic features nor leveraging any of the W-NUT-provided gazetteers, we obtained robust performance with our approach, which ranked third amongst all shared task participants according to the official evaluation on a gold standard named entity-annotated corpus of 3,856 tweets."
S16-1093,{N}a{CT}e{M} at {S}em{E}val-2016 Task 1: Inferring sentence-level semantic similarity from an ensemble of complementary lexical and sentence-level features,2016,17,2,5,0.784314,2016,piotr przybyla,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1205,Ensemble Classification of Grants using {LDA}-based Features,2016,0,0,4,0,34924,yannis korkontzelos,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Classifying research grants into useful categories is a vital task for a funding body to give structure to the portfolio for analysis, informing strategic planning and decision-making. Automating this classification process would save time and effort, providing the accuracy of the classifications is maintained. We employ five classification models to classify a set of BBSRC-funded research grants in 21 research topics based on unigrams, technical terms and Latent Dirichlet Allocation models. To boost precision, we investigate methods for combining their predictions into five aggregate classifiers. Evaluation confirmed that ensemble classification models lead to higher precision.It was observed that there is not a single best-performing aggregate method for all research topics. Instead, the best-performing method for a research topic depends on the number of positive training instances available for this topic. Subject matter experts considered the predictions of aggregate models to correct erroneous or incomplete manual assignments."
L16-1290,Identifying Content Types of Messages Related to Open Source Software Projects,2016,17,1,3,0,34924,yannis korkontzelos,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Assessing the suitability of an Open Source Software project for adoption requires not only an analysis of aspects related to the code, such as code quality, frequency of updates and new version releases, but also an evaluation of the quality of support offered in related online forums and issue trackers. Understanding the content types of forum messages and issue trackers can provide information about the extent to which requests are being addressed and issues are being resolved, the percentage of issues that are not being fixed, the cases where the user acknowledged that the issue was successfully resolved, etc. These indicators can provide potential adopters of the OSS with estimates about the level of available support. We present a detailed hierarchy of content types of online forum messages and issue tracker comments and a corpus of messages annotated accordingly. We discuss our experiments to classify forum messages and issue tracker comments into content-related classes, i.e.{\textasciitilde}to assign them to nodes of the hierarchy. The results are very encouraging."
W15-3804,Event Extraction in pieces:Tackling the partial event identification problem on unseen corpora,2015,29,1,2,1,7229,chrysoula zerva,Proceedings of {B}io{NLP} 15,0,"Biomedical event extraction systems have the potential to provide a reliable means of enhancing knowledge resources and mining the scientific literature. However, toachieve this goal, it is necessary that current event extraction models are improved, such that they can be applied confidently to unseen data with a minimal rate of error. Motivated by this requirement, this work targets a particular type of error, namely partial events, where an event is missing one or more arguments. Specifically, we attempt to improve the performance of a state-of-the-art event extraction tool, EventMine, when applied to a new cancer pathway curation corpus. We propose a post-processing ranking approach based on relaxed constraints, in order to reconsider the candidate arguments for each event trigger, and suggest possible new arguments.The proposed methodology, applicable to the output of any event extractionsystem, achieves an improvement in argument recall of 2%-4% when appliedto EventMine output, and thus constitutes a promising direction for further developments."
W14-1101,{K}eynote: Supporting evidence-based medicine using text mining,2014,0,0,1,1,3223,sophia ananiadou,Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi),0,"Evidence-based medicine uses systematic reviews to identify relevant studies to answer specific research questions. An underlying principle of the approach is the importance of specifying a priori the research question to drive the review process. Such reviews have a central role in health technology assessments, development of clinical guidelines and public health guidance, and evidence-informed policy and practice. However, public health questions are complex and often need to be described using abstract, fuzzy terminology. Understanding the scope of evidence often emerges during a review and cannot be defined a priori. Can text mining support a dynamic and multidimensional definition of relevance using interactive, exploratory searching under uncertainty? Can text mining help reviewers to explore evidence of interconnections between different factors, diseases and human behaviour?"
W14-1110,Building a semantically annotated corpus for congestive heart and renal failure from clinical records and the literature,2014,24,10,3,0,38788,noha alnazzawi,Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi),0,"Narrative information in Electronic Health Records (EHRs) and literature articles contains a wealth of clinical information about treatment, diagnosis, medication and family history. This often includes detailed phenotype information for specific diseases, which in turn can help to identify risk factors and thus determine the susceptibility of different patients. Such information can help to improve healthcare applications, including Clinical Decision Support Systems (CDS). Clinical text mining (TM) tools can provide efficient automated means to extract and integrate vital information hidden within the vast volumes of available text. Development or adaptation of TM tools is reliant on the availability of annotated training corpora, although few such corpora exist for the clinical domain. In response, we have created a new annotated corpus (PhenoCHF), focussing on the identification of phenotype information for a specific clinical sub-domain, i.e., congestive heart failure (CHF). The corpus is unique in this domain, in its integration of information from both EHRs (300 discharge summaries) and literature articles (5 full-text papers). The annotation scheme, whose design was guided by a domain expert, includes both entities and relations pertinent to CHF. Two further domain experts performed the annotation, resulting in high quality annotation, with agreement rates up to 0.92 F-Score."
rak-etal-2014-interoperability,Interoperability and Customisation of Annotation Schemata in Argo,2014,14,5,5,1,39325,rafal rak,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The process of annotating text corpora involves establishing annotation schemata which define the scope and depth of an annotation task at hand. We demonstrate this activity in Argo, a Web-based workbench for the analysis of textual resources, which facilitates both automatic and manual annotation. Annotation tasks in the workbench are defined by building workflows consisting of a selection of available elementary analytics developed in compliance with the Unstructured Information Management Architecture specification. The architecture accommodates complex annotation types that may define primitive as well as referential attributes. Argo aids the development of custom annotation schemata and supports their interoperability by featuring a schema editor and specialised analytics for schemata alignment. The schema editor is a self-contained graphical user interface for defining annotation types. Multiple heterogeneous schemata can be aligned by including one of two type mapping analytics currently offered in Argo. One is based on a simple mapping syntax and, although limited in functionality, covers most common use cases. The other utilises a well established graph query language, SPARQL, and is superior to other state-of-the-art solutions in terms of expressiveness. We argue that the customisation of annotation schemata does not need to compromise their interoperability."
mihaila-ananiadou-2014-meta,The Meta-knowledge of Causality in Biomedical Scientific Discourse,2014,25,0,2,1,39487,claudiu mihuailua,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Causality lies at the heart of biomedical knowledge, being involved in diagnosis, pathology or systems biology. Thus, automatic causality recognition can greatly reduce the human workload by suggesting possible causal connections and aiding in the curation of pathway models. For this, we rely on corpora that are annotated with classified, structured representations of important facts and findings contained within text. However, it is impossible to correctly interpret these annotations without additional information, e.g., classification of an event as fact, hypothesis, experimental result or analysis of results, confidence of authors about the validity of their analyses etc. In this study, we analyse and automatically detect this type of information, collectively termed meta-knowledge (MK), in the context of existing discourse causality annotations. Our effort proves the feasibility of identifying such pieces of information, without which the understanding of causal relations is limited."
rehm-etal-2014-strategic,"The Strategic Impact of {META}-{NET} on the Regional, National and International Level",2014,47,2,3,0,60,georg rehm,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article provides an overview of the dissemination work carried out in META-NET from 2010 until early 2014; we describe its impact on the regional, national and international level, mainly with regard to politics and the situation of funding for LT topics. This paper documents the initiativeÂs work throughout Europe in order to boost progress and innovation in our field."
korkontzelos-ananiadou-2014-locating,Locating Requests among Open Source Software Communication Messages,2014,22,1,2,1,39671,ioannis korkontzelos,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"As a first step towards assessing the quality of support offered online for Open Source Software (OSS), we address the task of locating requests, i.e., messages that raise an issue to be addressed by the OSS community, as opposed to any other message. We present a corpus of online communication messages randomly sampled from newsgroups and bug trackers, manually annotated as requests or non-requests. We identify several linguistically shallow, content-based heuristics that correlate with the classification and investigate the extent to which they can serve as independent classification criteria. Then, we train machine-learning classifiers on these heuristics. We experiment with a wide range of settings, such as different learners, excluding some heuristics and adding unigram features of various parts-of-speech and frequency. We conclude that some heuristics can perform well, while their accuracy can be improved further using machine learning, at the cost of obtaining manual annotations."
E14-4022,Using a Random Forest Classifier to Compile Bilingual Dictionaries of Technical Terms from Comparable Corpora,2014,20,13,4,1,33045,georgios kontonatsios,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"We describe a machine learning approach, a Random Forest (RF) classifier, that is used to automatically compile bilingual dictionaries of technical terms from comparable corpora. We evaluate the RF classifier against a popular term alignment method, namely context vectors, and we report an improvement of the translation accuracy. As an application, we use the automatically extracted dictionary in combination with a trained Statistical Machine Translation (SMT) system to more accurately translate unknown terms. The dictionary extraction method described in this paper is freely available 1 ."
D14-1177,Combining String and Context Similarity for Bilingual Term Alignment from Comparable Corpora,2014,40,11,4,1,33045,georgios kontonatsios,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Automatically compiling bilingual dictionaries of technical terms from comparable corpora is a challenging problem, yet with many potential applications. In this paper, we exploit two independent observations about term translations: (a) terms are often formed by corresponding sub-lexical units across languages and (b) a term and its translation tend to appear in similar lexical context. Based on the first observation, we develop a new character n-gram compositional method, a logistic regression classifier, for learning a string similarity measure of term translations. According to the second observation, we use an existing context-based approach. For evaluation, we investigate the performance of compositional and context-based methods on: (a) similar and unrelated languages, (b) corpora of different degree of comparability and (c) the translation of frequent and rare terms. Finally, we combine the two translation clues, namely string and contextual similarity, in a linear model and we show substantial improvements over the two translation signals."
C14-1214,Comparable Study of Event Extraction in Newswire and Biomedical Domains,2014,23,10,4,0.689655,3222,makoto miwa,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Event extraction is a popular research topic in natural language processing. Several event extraction tasks have been defined for both the newswire and biomedical domains. In general, different systems have been developed for the two domains, despite the fact that the tasks in both domains share a number of characteristics. In this paper, we analyse the commonalities and differences between the tasks in the two domains. Based on this analysis, we demonstrate how an event extraction method originally designed for the biomedical domain can be adapted for application to the newswire domain. The performance is state-of-the-art for both domains, with F-scores of 52.7% for the biomedical domain and 52.1% for the newswire domain in terms of their primary evaluation metrics."
W13-2512,Using a Random Forest Classifier to recognise translations of biomedical terms across languages,2013,37,1,3,1,33045,georgios kontonatsios,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"We present a novel method to recognise semantic equivalents of biomedical terms in language pairs. We hypothesise that biomedical term are formed by semantically similar textual units across languages. Based on this hypothesis, we employ a Random Forest (RF) classifier that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples. We apply our method on two language pairs: one that uses the same character set and another with a different script, English-French and EnglishChinese, respectively. We show that English-French pairs of terms are highly transliterated in contrast to the EnglishChinese pairs. Nonetheless, our method performs robustly on both cases. We evaluate RF against a state-of-the-art alignment method, GIZA, and we report a statistically significant improvement. Finally, we compare RF against Support Vector Machines and analyse our results."
W13-2310,Towards a Better Understanding of Discourse: Integrating Multiple Discourse Annotation Perspectives Using {UIMA},2013,28,1,6,1,39487,claudiu mihuailua,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"There exist various different discourse annotation schemes that vary both in the perspectives of discourse structure considered and the granularity of textual units that are annotated. Comparison and integration of multiple schemes have the potential to provide enhanced information. However, the differing formats of corpora and tools that contain or produce such schemes can be a barrier to their integration. U-Compare is a graphical, UIMA-based workflow construction platform for combining interoperable natural language processing (NLP) resources, without the need for programming skills. In this paper, we present an extension of U-Compare that allows the easy comparison, integration and visualisation of resources that contain or output annotations based on multiple discourse annotation schemes. The extension works by allowing the construction of parallel subworkflows for each scheme within a single U-Compare workflow. The different types of discourse annotations produced by each sub-workflow can be either merged or visualised side-by-side for comparison. We demonstrate this new functionality by using it to compare annotations belonging to two different approaches to discourse analysis, namely discourse relations and functional discourse annotations. Integrating these different annotation types within an interoperable environment allows us to study the correlations between different types of discourse and report on the new insights that this allows us to discover."
W13-2311,Making {UIMA} Truly Interoperable with {SPARQL},2013,10,7,2,1,39325,rafal rak,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"Unstructured Information Management Architecture (UIMA) has been gaining popularity in annotating text corpora. The architecture defines common data structures and interfaces to support interoperability of individual processing components working together in a UIMA application. The components exchange data by sharing common type systemsxe2x80x94schemata of data type structuresxe2x80x94which extend a generic, top-level type system built into UIMA. This flexibility in extending type systems has resulted in the development of repositories of components that share one or several type systems; however, components coming from different repositories, and thus not sharing type systems, remain incompatible. Commonly, this problem has been solved programmatically by implementing UIMA components that perform the alignment of two type systems, an arduous task that is impractical with a growing number of type systems. We alleviate this problem by introducing a conversion mechanism based on SPARQL, a query language for the data retrieval and manipulation of RDF graphs. We provide a UIMA component that serialises data coming from a source component into RDF, executes a user-defined, typeconversion query, and deserialises the updated graph into a target component. The proposed solution encourages ad hoc conversions, enables the usage of heterogeneous components, and facilitates highly customised UIMA applications."
W13-2008,Overview of the Cancer Genetics ({CG}) task of {B}io{NLP} Shared Task 2013,2013,33,22,3,0.970083,2607,sampo pyysalo,Proceedings of the {B}io{NLP} Shared Task 2013 Workshop,0,"We present the design, preparation, results and analysis of the Cancer Genetics (CG) event extraction task, a main task of the BioNLP Shared Task (ST) 2013. The CG task is an information extraction task targeting the recognition of events in text, represented as structured n-ary associations of given physical entities. In addition to addressing the cancer domain, the CG task is differentiated from previous event extraction tasks in the BioNLP ST series in addressing a wide range of pathological processes and multiple levels of biological organization, ranging from the molecular through the cellular and organ levels up to whole organisms. Final test set submissions were accepted from six teams. The highest-performing system achieved an Fscore of 55.4%. This level of performance is broadly comparable with the state of the art for established molecular-level extraction tasks, demonstrating that event extraction resources and methods generalize well to higher levels of biological organization and are applicable to the analysis of scientific texts on cancer. The CG task continues as an open challenge to all interested parties, with tools and resources available from http://2013. bionlp-st.org/."
W13-2009,Overview of the Pathway Curation ({PC}) task of {B}io{NLP} Shared Task 2013,2013,47,30,8,0.751899,35319,tomoko ohta,Proceedings of the {B}io{NLP} Shared Task 2013 Workshop,0,"We present the Pathway Curation (PC) task, a main event extraction task of the BioNLP shared task (ST) 2013. The PC task concerns the automatic extraction of biomolecular reactions from text. The task setting, representation and semantics are defined with respect to pathway model standards and ontologies (SBML, BioPAX, SBO) and documents selected by relevance to specific model reactions. Two BioNLP ST 2013 participants successfully completed the PC task. The highest achieved Fscore, 52.8%, indicates that event extraction is a promising approach to supporting pathway curation efforts. The PC task continues as an open challenge with data, resources and tools available from http://2013.bionlp-st.org/"
W13-2012,{N}a{CT}e{M} {E}vent{M}ine for {B}io{NLP} 2013 {CG} and {PC} tasks,2013,17,23,2,0.689655,3222,makoto miwa,Proceedings of the {B}io{NLP} Shared Task 2013 Workshop,0,"This paper describes NaCTeM entries for the Cancer Genetics (CG) and Pathway Curation (PC) tasks in the BioNLP Shared Task 2013. We have applied a state-ofthe-art event extraction system EventMine to the tasks in two different settings: a single-corpus setting for the CG task and a stacking setting for the PC task. EventMine was applicable to the two tasks with simple task specific configuration, and it produced a reasonably high performance, positioning second in the CG task and first in the PC task."
P13-4008,Extending an interoperable platform to facilitate the creation of multilingual and multimodal {NLP} applications,2013,9,4,6,1,33045,georgios kontonatsios,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"U-Compare is a UIMA-based workflow construction platform for building natural language processing (NLP) applications from heterogeneous language resources (LRs), without the need for programming skills. U-Compare has been adopted within the context of the METANET Network of Excellence, and over 40 LRs that process 15 European languages have been added to the U-Compare component library. In line with METANETxe2x80x99s aims of increasing communication between citizens of different European countries, U-Compare has been extended to facilitate the development of a wider range of applications, including both multilingual and multimodal workflows. The enhancements exploit the UIMA Subject of Analysis (Sofa) mechanism, that allows different facets of the input data to be represented. We demonstrate how our customised extensions to U-Compare allow the construction and testing of NLP applications that transform the input data in different ways, e.g., machine translation, automatic summarisation and text-to-speech."
P13-4020,Development and Analysis of {NLP} Pipelines in Argo,2013,8,4,4,1,39325,rafal rak,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"Developing sophisticated NLP pipelines composed of multiple processing tools and components available through different providers may pose a challenge in terms of their interoperability. The Unstructured Information Management Architecture (UIMA) is an industry standard whose aim is to ensure such interoperability by defining common data structures and interfaces. The architecture has been gaining attention from industry and academia alike, resulting in a large volume of UIMA-compliant processing components. In this paper, we demonstrate Argo, a Web-based workbench for the development and processing of NLP pipelines/workflows. The workbench is based upon UIMA, and thus has the potential of using many of the existing UIMA resources. We present features, and show examples, of facilitating the distributed development of components and the analysis of processing results. The latter includes annotation visualisers and editors, as well as serialisation to RDF format, which enables flexible querying in addition to data manipulation thanks to the semantic query language SPARQL. The distributed development feature allows users to seamlessly connect their tools to workflows running in Argo, and thus take advantage of both the available library of components (without the need of installing them locally) and the analytical tools."
P13-3006,What causes a causal relation? Detecting Causal Triggers in Biomedical Scientific Discourse,2013,28,1,2,1,39487,claudiu mihuailua,51st Annual Meeting of the Association for Computational Linguistics Proceedings of the Student Research Workshop,0,"Current domain-specific information extraction systems represent an important resource for biomedical researchers, who need to process vaster amounts of knowledge in short times. Automatic discourse causality recognition can further improve their workload by suggesting possible causal connections and aiding in the curation of pathway models. We here describe an approach to the automatic identification of discourse causality triggers in the biomedical domain using machine learning. We create several baselines and experiment with various parameter settings for three algorithms, i.e., Conditional Random Fields (CRF), Support Vector Machines (SVM) and Random Forests (RF). Also, we evaluate the impact of lexical, syntactic and semantic features on each of the algorithms and look at errors. The best performance of 79.35% F-score is achieved by CRFs when using all three feature types."
W12-4304,Open-domain Anatomical Entity Mention Detection,2012,31,31,4,0.751899,35319,tomoko ohta,Proceedings of the Workshop on Detecting Structure in Scholarly Discourse,0,"Anatomical entities such as kidney, muscle and blood are central to much of biomedical scientific discourse, and the detection of mentions of anatomical entities is thus necessary for the automatic analysis of the structure of domain texts. Although a number of resources and methods addressing aspects of the task have been introduced, there have so far been no annotated corpora for training and evaluating systems for broad-coverage, open-domain anatomical entity mention detection. We introduce the AnEM corpus, a domain- and species-independent resource manually annotated for anatomical entity mentions using a fine-grained classification system. The corpus texts are selected randomly from citation abstracts and full-text papers with the aim of making the corpus representative of the entire available biomedical scientific literature. We demonstrate the use of the corpus through an evaluation of the broad-coverage MetaMap tagger and a CRF-based system trained on the corpus data, considering also a combination of these two methods. The combined system demonstrates a promising level of performance, approaching 80% F-score for mention detection for a relaxed matching criterion. The corpus and other introduced resources are available under open licences from http://www.nactem.ac.uk/anatomy/."
W12-4305,A three-way perspective on scientific discourse annotation for knowledge extraction,2012,48,12,6,0,3114,maria liakata,Proceedings of the Workshop on Detecting Structure in Scholarly Discourse,0,"This paper presents a three-way perspective on the annotation of discourse in scientific literature. We use three different schemes, each of which focusses on different aspects of discourse in scientific articles, to annotate a corpus of three full-text papers, and compare the results. One scheme seeks to identify the core components of scientific investigations at the sentence level, a second annotates meta-knowledge pertaining to bio-events and a third considers how epistemic knowledge is conveyed at the clause level. We present our analysis of the comparison, and a discussion of the contributions of each scheme."
W12-3806,Bridging the Gap Between Scope-based and Event-based Negation/Speculation Annotations: A Bridge Not Too Far,2012,25,3,4,0,4231,pontus stenetorp,Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,0,"We study two approaches to the marking of extra-propositional aspects of statements in text: the task-independent cue-and-scope representation considered in the CoNLL-2010 Shared Task, and the tagged-event representation applied in several recent event extraction tasks. Building on shared task resources and the analyses from state-of-the-art systems representing the two broad lines of research, we identify specific points of mismatch between the two perspectives and propose ways of addressing them. We demonstrate the feasibility of our approach by constructing a method that uses cue-and-scope analyses together with a small set of features motivated by data analysis to predict event negation and speculation. Evaluation on BioNLP Shared Task 2011 data indicates the method to outperform the negation/speculation components of state-of-the-art event extraction systems.n n The system and resources introduced in this work are publicly available for research purposes at: https://github.com/ninjin/eepura"
W12-2410,"{P}ub{M}ed-Scale Event Extraction for Post-Translational Modifications, Epigenetics and Protein Structural Relations",2012,35,10,7,0,28457,jari bjorne,{B}io{NLP}: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,0,"Recent efforts in biomolecular event extraction have mainly focused on core event types involving genes and proteins, such as gene expression, protein-protein interactions, and protein catabolism. The BioNLP'11 Shared Task extended the event extraction approach to sub-protein events and relations in the Epigenetics and Post-translational Modifications (EPI) and Protein Relations (REL) tasks. In this study, we apply the Turku Event Extraction System, the best-performing system for these tasks, to all PubMed abstracts and all available PMC full-text articles, extracting 1.4M EPI events and 2.2M REL relations from 21M abstracts and 372K articles. We introduce several entity normalization algorithms for genes, proteins, protein complexes and protein components, aiming to uniquely identify these biological entities. This normalization effort allows direct mapping of the extracted events and relations with post-translational modifications from UniProt, epigenetics from PubMeth, functional domains from InterPro and macromolecular structures from PDB. The extraction of such detailed protein information provides a unique text mining dataset, offering the opportunity to further deepen the information provided by existing PubMed-scale event extraction efforts. The methods and data introduced in this study are freely available from bionlp.utu.fi."
W12-2412,New Resources and Perspectives for Biomedical Event Extraction,2012,39,1,5,0.970083,2607,sampo pyysalo,{B}io{NLP}: Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,0,"Event extraction is a major focus of recent work in biomedical information extraction. Despite substantial advances, many challenges still remain for reliable automatic extraction of events from text. We introduce a new biomedical event extraction resource consisting of analyses automatically created by systems participating in the recent BioNLP Shared Task (ST) 2011. In providing for the first time the outputs of a broad set of state-of-the-art event extraction systems, this resource opens many new opportunities for studying aspects of event extraction, from the identification of common errors to the study of effective approaches to combining the strengths of systems. We demonstrate these opportunities through a multi-system analysis on three BioNLP ST 2011 main tasks, focusing on events that none of the systems can successfully extract. We further argue for new perspectives to the performance evaluation of domain event extraction systems, considering a document-level, off-the-page representation and evaluation to complement the mention-level evaluations pursued in most recent work."
P12-3021,"Building Trainable Taggers in a Web-based, {UIMA}-Supported {NLP} Workbench",2012,9,1,3,1,39325,rafal rak,Proceedings of the {ACL} 2012 System Demonstrations,0,"Argo is a web-based NLP and text mining workbench with a convenient graphical user interface for designing and executing processing workflows of various complexity. The workbench is intended for specialists and non-technical audiences alike, and provides the ever expanding library of analytics compliant with the Unstructured Information Management Architecture, a widely adopted interoperability framework. We explore the flexibility of this framework by demonstrating workflows involving three processing components capable of performing self-contained machine learning-based tagging. The three components are responsible for the three distinct tasks of 1) generating observations or features, 2) training a statistical model based on the generated features, and 3) tagging unlabelled data with the model. The learning and tagging components are based on an implementation of conditional random fields (CRF); whereas the feature generation component is an analytic capable of extending basic token information to a comprehensive set of features. Users define the features of their choice directly from Argo's graphical interface, without resorting to programming (a commonly used approach to feature engineering). The experimental results performed on two tagging tasks, chunking and named entity recognition, showed that a tagger with a generic set of features built in Argo is capable of competing with task-specific solutions."
wang-etal-2012-biomedical,Biomedical {C}hinese-{E}nglish {CLIR} Using an Extended {CM}e{SH} Resource to Expand Queries,2012,25,4,4,0,42943,xinkai wang,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Cross-lingual information retrieval (CLIR) involving the Chinese language has been thoroughly studied in the general language domain, but rarely in the biomedical domain, due to the lack of suitable linguistic resources and parsing tools. In this paper, we describe a Chinese-English CLIR system for biomedical literature, which exploits a bilingual ontology, the ``eCMeSH Tree''''''''. This is an extension of the Chinese Medical Subject Headings (CMeSH) Tree, based on Medical Subject Headings (MeSH). Using the 2006 and 2007 TREC Genomics track data, we have evaluated the performance of the eCMeSH Tree in expanding queries. We have compared our results to those obtained using two other approaches, i.e. pseudo-relevance feedback (PRF) and document translation (DT). Subsequently, we evaluate the performance of different combinations of these three retrieval methods. Our results show that our method of expanding queries using the eCMeSH Tree can outperform the PRF method. Furthermore, combining this method with PRF and DT helps to smooth the differences in query expansion, and consequently results in the best performance amongst all experiments reported. All experiments compare the use of two different retrieval models, i.e. Okapi BM25 and a query likelihood language model. In general, the former performs slightly better."
nawaz-etal-2012-identification,Identification of Manner in Bio-Events,2012,22,18,3,1,23847,raheel nawaz,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Due to the rapid growth in the volume of biomedical literature, there is an increasing requirement for high-performance semantic search systems, which allow biologists to perform precise searches for events of interest. Such systems are usually trained on corpora of documents that contain manually annotated events. Until recently, these corpora, and hence the event extraction systems trained on them, focussed almost exclusively on the identification and classification of event arguments, without taking into account how the textual context of the events could affect their interpretation. Previously, we designed an annotation scheme to enrich events with several aspects (or dimensions) of interpretation, which we term meta-knowledge, and applied this scheme to the entire GENIA corpus. In this paper, we report on our experiments to automate the assignment of one of these meta-knowledge dimensions, i.e. Manner, to recognised events. Manner is concerned with the rate, strength intensity or level of the event. We distinguish three different values of manner, i.e., High, Low and Neutral. To our knowledge, our work represents the first attempt to classify the manner of events. Using a combination of lexical, syntactic and semantic features, our system achieves an overall accuracy of 99.4{\%}."
rak-etal-2012-collaborative,Collaborative Development and Evaluation of Text-processing Workflows in a {UIMA}-supported Web-based Workbench,2012,10,4,3,1,39325,rafal rak,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Challenges in creating comprehensive text-processing worklows include a lack of the interoperability of individual components coming from different providers and/or a requirement imposed on the end users to know programming techniques to compose such workflows. In this paper we demonstrate Argo, a web-based system that addresses these issues in several ways. It supports the widely adopted Unstructured Information Management Architecture (UIMA), which handles the problem of interoperability; it provides a web browser-based interface for developing workflows by drawing diagrams composed of a selection of available processing components; and it provides novel user-interactive analytics such as the annotation editor which constitutes a bridge between automatic processing and manual correction. These features extend the target audience of Argo to users with a limited or no technical background. Here, we focus specifically on the construction of advanced workflows, involving multiple branching and merging points, to facilitate various comparative evalutions. Together with the use of user-collaboration capabilities supported in Argo, we demonstrate several use cases including visual inspections, comparisions of multiple processing segments or complete solutions against a reference standard, inter-annotator agreement, and shared task mass evaluations. Ultimetely, Argo emerges as a one-stop workbench for defining, processing, editing and evaluating text processing tasks."
black-etal-2012-data,A data and analysis resource for an experiment in text mining a collection of micro-blogs on a political topic.,2012,14,6,4,0,43355,william black,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The analysis of a corpus of micro-blogs on the topic of the 2011 UK referendum about the Alternative Vote has been undertaken as a joint activity by text miners and social scientists. To facilitate the collaboration, the corpus and its analysis is managed in a Web-accessible framework that allows users to upload their own textual data for analysis and to manage their own text annotation resources used for analysis. The framework also allows annotations to be searched, and the analysis to be re-run after amending the analysis resources. The corpus is also doubly human-annotated stating both whether each tweet is overall positive or negative in sentiment and whether it is for or against the proposition of the referendum."
E12-2021,brat: a Web-based Tool for {NLP}-Assisted Text Annotation,2012,20,383,5,0,4231,pontus stenetorp,Proceedings of the Demonstrations at the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We introduce the brat rapid annotation tool (BRAT), an intuitive web-based tool for text annotation supported by Natural Language Processing (NLP) technology. BRAT has been developed for rich structured annotation for a variety of NLP tasks and aims to support manual curation efforts and increase annotator productivity using NLP techniques. We discuss several case studies of real-world annotation projects using pre-release versions of BRAT and present an evaluation of annotation assisted by semantic class disambiguation on a multicategory entity mention annotation task, showing a 15% decrease in total annotation time. BRAT is available under an open-source license from: http://brat.nlplab.org"
W11-3307,Promoting Interoperability of Resources in {META}-{SHARE},2011,17,6,7,1,17111,paul thompson,"Proceedings of the Workshop on Language Resources, Technology and Services in the Sharing Paradigm",0,"META-NET is a Network of Excellencen aiming to improve significantly on the numbern of language technologies that can assistn European citizens, by enabling enhancedn communication and cooperation acrossn languages. A major outcome will be META-n SHARE, a searchable network of repositoriesn that collect resources such as language data,n tools and related web services, covering an large number of European languages. Thesen resources are intended to facilitate then development and evaluation of a wide range ofn new language processing applications andn services. An important aim of META-SHAREn is the promotion of interoperability amongstn resources. In this paper, we describe ourn planned efforts to help to achieve this aim,n through the adoption of the UIMA frameworkn and the integration of the U-Compare systemn within the META-SHARE network. U-n Compare facilitates the rapid construction andn evaluation of NLP applications that make usen of interoperable components, and, as such, cann help to speed up the development of a newn generation of European language technologyn applications."
W11-1804,Overview of the Infectious Diseases ({ID}) task of {B}io{NLP} Shared Task 2011,2011,33,34,9,0.607279,2607,sampo pyysalo,Proceedings of {B}io{NLP} Shared Task 2011 Workshop,0,"This paper presents the preparation, resources, results and analysis of the Infectious Diseases (ID) information extraction task, a main task of the BioNLP Shared Task 2011. The ID task represents an application and extension of the BioNLP'09 shared task event extraction approach to full papers on infectious diseases. Seven teams submitted final results to the task, with the highest-performing system achieving 56% F-score in the full task, comparable to state-of-the-art performance in the established BioNLP'09 task. The results indicate that event extraction methods generalize well to new domains and full-text publications and are applicable to the extraction of events relevant to the molecular mechanisms of infectious diseases."
W11-1507,Enrichment and Structuring of Archival Description Metadata,2011,18,5,4,0,25346,kalliopi zervanou,"Proceedings of the 5th {ACL}-{HLT} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"Cultural heritage institutions are making their digital content available and searchable online. Digital metadata descriptions play an important role in this endeavour. This metadata is mostly manually created and often lacks detailed annotation, consistency and, most importantly, explicit semantic content descriptors which would facilitate online browsing and exploration of available information. This paper proposes the enrichment of existing cultural heritage metadata with automatically generated semantic content descriptors. In particular, it is concerned with metadata encoding archival descriptions (EAD) and proposes to use automatic term recognition and term clustering techniques for knowledge acquisition and content-based document classification purposes."
W11-0210,Building a Coreference-Annotated Corpus from the Domain of Biochemistry,2011,12,13,2,1,10980,riza batistanavarro,Proceedings of {B}io{NLP} 2011 Workshop,0,"One of the reasons for which the resolution of coreferences has remained a challenging information extraction task, especially in the biomedical domain, is the lack of training data in the form of annotated corpora. In order to address this issue, we developed the HANAPIN corpus. It consists of full-text articles from biochemistry literature, covering entities of several semantic types: chemical compounds, drug targets (e.g., proteins, enzymes, cell lines, pathogens), diseases, organisms and drug effects. All of the co-referring expressions pertaining to these semantic types were annotated based on the annotation scheme that we developed. We observed four general types of coreferences in the corpus: sortal, pronominal, abbreviation and numerical. Using the MASI distance metric, we obtained 84% in computing the inter-annotator agreement in terms of Krippendorff's alpha. Consisting of 20 full-text, open-access articles, the corpus will enable other researchers to use it as a resource for their own coreference resolution methodologies."
W10-3112,Evaluating a meta-knowledge annotation scheme for bio-events,2010,32,19,3,1,23847,raheel nawaz,Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,0,"The correct interpretation of biomedical texts by text mining systems requires the recognition of a range of types of high-level information (or meta-knowledge) about the text. Examples include expressions of negation and speculation, as well as pragmatic/rhetorical intent (e.g. whether the information expressed represents a hypothesis, generally accepted knowledge, new experimental knowledge, etc.) Although such types of information have previously been annotated at the text-span level (most commonly sentences), annotation at the level of the event is currently quite sparse. In this paper, we focus on the evaluation of the multi-dimensional annotation scheme that we have developed specifically for enriching bio-events with meta-knowledge information. Our annotation scheme is intended to be general enough to allow integration with different types of bio-event annotation, whilst being detailed enough to capture important subtleties in the nature of the meta-knowledge expressed in the text. To our knowledge, our scheme is unique within the field with regards to the diversity of meta-knowledge aspects annotated for each event, whilst the evaluation results have confirmed its feasibility and soundness."
W10-1919,Towards Event Extraction from Full Texts on Infectious Diseases,2010,17,11,8,0.579771,2607,sampo pyysalo,Proceedings of the 2010 Workshop on Biomedical Natural Language Processing,0,"Event extraction approaches based on expressive structured representations of extracted information have been a significant focus of research in recent biomedical natural language processing studies. However, event extraction efforts have so far been limited to publication abstracts, with most studies further considering only the specific transcription factor-related subdo-main of molecular biology of the GENIA corpus. To establish the broader relevance of the event extraction approach and proposed methods, it is necessary to expand on these constraints. In this study, we propose an adaptation of the event extraction approach to a subdomain related to infectious diseases and present analysis and initial experiments on the feasibility of event extraction from domain full text publications."
ananiadou-etal-2010-evaluating,Evaluating a Text Mining Based Educational Search Portal,2010,7,3,1,1,3223,sophia ananiadou,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we present the main features of a text mining based search engine for the UK Educational Evidence Portal available at the UK National Centre for Text Mining (NaCTeM), together with a user-centred framework for the evaluation of the search engine. The framework is adapted from an existing proposal by the ISLE (EAGLES) Evaluation Working group. We introduce the metrics employed for the evaluation, and explain how these relate to the text mining based search engine. Following this, we describe how we applied the framework to the evaluation of a number of key text mining features of the search engine, namely the automatic clustering of search results, classification of search results according to a taxonomy, and identification of topics and other documents that are related to a chosen document. Finally, we present the results of the evaluation in terms of the strengths, weaknesses and improvements identified for each of these features."
nawaz-etal-2010-meta,Meta-Knowledge Annotation of Bio-Events,2010,28,24,4,1,23847,raheel nawaz,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Biomedical corpora annotated with event-level information provide an important resource for the training of domain-specific information extraction (IE) systems. These corpora concentrate primarily on creating classified, structured representations of important facts and findings contained within the text. However, bio-event annotations often do not take into account additional information (meta-knowledge) that is expressed within the textual context of the bio-event, e.g., the pragmatic/rhetorical intent and the level of certainty ascribed to a particular bio-event by the authors. Such additional information is indispensible for correct interpretation of bio-events. Therefore, an IE system that simply presents a list of ÂbareÂ bio-events, without information concerning their interpretation, is of little practical use. We have addressed this sparseness of meta-knowledge available in existing bio-event corpora by developing a multi-dimensional annotation scheme tailored to bio-events. The scheme is intended to be general enough to allow integration with different types of bio-event annotation, whilst being detailed enough to capture important subtleties in the nature of the meta-knowledge expressed about different bio-events. To our knowledge, our scheme is unique within the field with regards to the diversity of meta-knowledge aspects annotated for each event."
kano-etal-2010-u,{U}-Compare: An Integrated Language Resource Evaluation Platform Including a Comprehensive {UIMA} Resource Library,2010,8,10,4,1,8106,yoshinobu kano,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Language resources, including corpus and tools, are normally required to be combined in order to achieve a userÂs specific task. However, resources tend to be developed independently in different, incompatible formats. In this paper we describe about U-Compare, which consists of the U-Compare component repository and the U-Compare platform. We have been building a highly interoperable resource library, providing the world largest ready-to-use UIMA component repository including wide variety of corpus readers and state-of-the-art language tools. These resources can be deployed as local services or web services, even possible to be hosted in clustered machines to increase the performance, while users do not need to be aware of such differences. In addition to the resource library, an integrated language processing platform is provided, allowing workflow creation, comparison, evaluation and visualization, using the resources in the library or any UIMA component, without any programming via graphical user interfaces, while a command line launcher is also available without GUIs. The evaluation itself is processed in a UIMA component, users can create and plug their own evaluation metrics in addition to the predefined metrics. U-Compare has been successfully used in many projects including BioCreative, Conll and the BioNLP shared task."
C10-2098,Imbalanced Classification Using Dictionary-based Prototypes and Hierarchical Decision Rules for Entity Sense Disambiguation,2010,17,2,4,0,26225,tingting mu,Coling 2010: Posters,0,"Entity sense disambiguation becomes difficult with few or even zero training instances available, which is known as imbalanced learning problem in machine learning. To overcome the problem, we create a new set of reliable training instances from dictionary, called dictionary-based prototypes. A hierarchical classification system with a tree-like structure is designed to learn from both the prototypes and training instances, and three different types of classifiers are employed. In addition, supervised dimensionality reduction is conducted in a similarity-based space. Experimental results show our system outperforms three baseline systems by at least 8.3% as measured by macro F1 score."
W09-1504,Integrated {NLP} Evaluation System for Pluggable Evaluation Metrics with Extensive Interoperable Toolkit,2009,17,7,3,1,8106,yoshinobu kano,"Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing ({SETQA}-{NLP} 2009)",0,"To understand the key characteristics of NLP tools, evaluation and comparison against different tools is important. And as NLP applications tend to consist of multiple semi-independent sub-components, it is not always enough to just evaluate complete systems, a fine grained evaluation of underlying components is also often worthwhile. Standardization of NLP components and resources is not only significant for reusability, but also in that it allows the comparison of individual components in terms of reliability and robustness in a wider range of target domains. But as many evaluation metrics exist in even a single domain, any system seeking to aid inter-domain evaluation needs not just predefined metrics, but must also support pluggable user-defined metrics. Such a system would of course need to be based on an open standard to allow a large number of components to be compared, and would ideally include visualization of the differences between components. We have developed a pluggable evaluation system based on the UIMA framework, which provides visualization useful in error analysis. It is a single integrated system which includes a large ready-to-use, fully interoperable library of NLP tools."
P09-1054,Stochastic Gradient Descent Training for {L}1-regularized Log-linear Models with Cumulative Penalty,2009,26,143,3,1,338,yoshimasa tsuruoka,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion. This learning framework is attractive because it often requires much less training time in practice than batch training algorithms. However, L1-regularization, which is becoming popular in natural language processing because of its ability to produce compact models, cannot be efficiently applied in SGD training, due to the large dimensions of feature vectors and the fluctuations of approximate gradients. We present a simple method to solve these problems by penalizing the weights according to cumulative values for L1 penalty. We evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging. Experimental results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasi-Newton method for L1-regularized loglinear models."
E09-2016,Three {B}io{NLP} Tools Powered by a Biological Lexicon,2009,9,2,4,1,8081,yutaka sasaki,Proceedings of the Demonstrations Session at {EACL} 2009,0,"In this paper, we demonstrate three NLP applications of the BioLexicon, which is a lexical resource tailored to the biology domain. The applications consist of a dictionary-based POS tagger, a syntactic parser, and query processing for biomedical information retrieval. Biological terminology is a major barrier to the accurate processing of literature within biology domain. In order to address this problem, we have constructed the BioLexicon using both manual and semiautomatic methods. We demonstrate the utility of the biology-oriented lexicon within three separate NLP applications."
E09-1090,Fast Full Parsing by Linear-Chain Conditional Random Fields,2009,25,37,3,1,338,yoshimasa tsuruoka,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,This paper presents a chunking-based discriminative approach to full parsing. We convert the task of full parsing into a series of chunking tasks and apply a conditional random field (CRF) model to each level of chunking. The probability of an entire parse tree is computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depth-first search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser.
D09-1157,Classifying Relations for Biomedical Named Entity Disambiguation,2009,36,4,3,0.714286,46478,xinglong wang,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Named entity disambiguation concerns linking a potentially ambiguous mention of named entity in text to an unambiguous identifier in a standard database. One approach to this task is supervised classification. However, the availability of training data is often limited, and the available data sets tend to be imbalanced and, in some cases, heterogeneous. We propose a new method that distinguishes a named entity by finding the informative keywords in its surrounding context, and then trains a model to predict whether each keyword indicates the semantic class of the entity. While maintaining a comparable performance to supervised classification, this method avoids using expensive manually annotated data for each new domain, and thus achieves better portability."
2009.jeptalnrecital-demonstration.5,{ASSIST} : un moteur de recherche sp{\\'e}cialis{\\'e} pour l{'}analyse des cadres d{'}exp{\\'e}riences,2009,-1,-1,3,0,1155,davy weissenbacher,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,"L{'}analyse qualitative des donn{\'e}es demande au sociologue un important travail de s{\'e}lection et d{'}interpr{\'e}tation des documents. Afin de faciliter ce travail, cette communaut{\'e} c{'}est dot{\'e}e d{'}outils informatique mais leur fonctionnalit{\'e}s sont encore limit{\'e}es. Le projet ASSIST est une {\'e}tude exploratoire pour pr{\'e}ciser les modules de traitement automatique des langues (TAL) permettant d{'}assister le sociologue dans son travail d{'}analyse. Nous pr{\'e}sentons le moteur de recherche r{\'e}alis{\'e} et nous justifions le choix des composants de TAL int{\'e}gr{\'e}s au prototype."
W08-0605,Accelerating the Annotation of Sparse Named Entities by Dynamic Sentence Selection,2008,21,18,3,1,338,yoshimasa tsuruoka,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"This paper presents an active learning-like framework for reducing the human effort for making named entity annotations in a corpus. In this framework, the annotation work is performed as an iterative and interactive process between the human annotator and a probabilistic named entity tagger. At each iteration, sentences that are most likely to contain named entities of the target category are selected by the probabilistic tagger and presented to the annotator. This iterative annotation process is repeated until the estimated coverage reaches the desired level. Unlike active learning approaches, our framework produces a named entity corpus that is free from the sampling bias introduced by the active strategy. We evaluated our framework by simulating the annotation process using two named entity corpora and show that our approach could drastically reduce the number of sentences to be annotated when applied to sparse named entities."
W08-0609,How to Make the Most of {NE} Dictionaries in Statistical {NER},2008,29,49,4,1,8081,yutaka sasaki,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"When term ambiguity and variability are very high, dictionary-based Named Entity Recognition (NER) is not an ideal solution even though large-scale terminological resources are available. Many researches on statistical NER have tried to cope with these problems. However, it is not straightforward how to exploit existing and additional Named Entity (NE) dictionaries in statistical NER. Presumably, addition of NEs to an NE dictionary leads to better performance. However, in reality, the retraining of NER models is required to achieve this. We have established a novel way to improve the NER performance by addition of NEs to an NE dictionary without retraining. We chose protein name recognition as a case study because it most suffers the problems related to heavy term variation and ambiguity. In our approach, first, known NEs are identified in parallel with Part-of-Speech (POS) tagging based on a general word dictionary and an NE dictionary. Then, statistical NER is trained on the tagger outputs with correct NE labels attached. We evaluated performance of our NER on the standard JNLPBA-2004 data set. The F-score on the test set has been improved from 73.14 to 73.78 after adding the protein names appearing in the training data to the POS tagger dictionary without any model retraining. The performance further increased to 78.72 after enriching the tagging dictionary with test set protein names. Our approach has demonstrated high performance in protein name recognition, which indicates how to make the most of known NEs in statistical NER."
piao-etal-2008-clustering,Clustering Related Terms with Definitions,2008,10,2,3,0,17869,scott piao,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"It is a challenging task to match similar or related terms/expressions in NLP and Text Mining applications. Two typical areas in need for such work are terminology and ontology constructions, where terms and concepts are extracted and organized into certain structures with various semantic relations. In the EU BOOTSTrep Project we test various techniques for matching terms that can assist human domain experts in building and enriching ontologies. This paper reports on a work in which we evaluated a text comparing and clustering tool for this task. Particularly, we explore the feasibility of matching related terms with their definitions. Ontology terms, such as Gene Ontology terms, are often assigned with detailed definitions, which provide a fundamental information source for detecting relations between terms. Here we focus on the exploitation of term definitions for the term matching task. Our experiment shows that the tool is capable of grouping many related terms using their definitions."
thompson-etal-2008-building,Building a Bio-Event Annotated Corpus for the Acquisition of Semantic Frames from Biomedical Corpora,2008,16,9,4,1,17111,paul thompson,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper reports on the design and construction of a bio-event annotated corpus which was developed with a specific view to the acquisition of semantic frames from biomedical corpora. We describe the adopted annotation scheme and the annotation process, which is supported by a dedicated annotation tool. The annotated corpus contains 677 abstracts of biomedical research articles."
saetre-etal-2008-connecting,Connecting Text Mining and Pathways using the {P}ath{T}ext Resource,2008,10,0,9,0,32381,rune saetre,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Many systems have been developed in the past few years to assist researchers in the discovery of knowledge published as English text, for example in the PubMed database. At the same time, higher level collective knowledge is often published using a graphical notation representing all the entities in a pathway and their interactions. We believe that these pathway visualizations could serve as an effective user interface for knowledge discovery if they can be linked to the text in publications. Since the graphical elements in a Pathway are of a very different nature than their corresponding descriptions in English text, we developed a prototype system called PathText. The goal of PathText is to serve as a bridge between these two different representations. In this paper, we first describe the overall architecture and the interfaces of the PathText system, and then provide some details about the core Text Mining components."
I08-2122,Towards Data and Goal Oriented Analysis: Tool Inter-operability and Combinatorial Comparison,2008,8,2,8,1,8106,yoshinobu kano,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"Recently, NLP researches have advanced using F-scores, precisions, and recalls with gold standard data as evaluation measures. However, such evaluations cannot capture the different behaviors of varying NLP tools or the different behaviors of a NLP tool that depends on the data and domain in which it works. Because an increasing number of tools are available nowadays, it has become increasingly important to grasp these behavioral differences, in order to select a suitable set of tools, which forms a complex workflow for a specific purpose. In order to observe such differences, we need to integrate available combinations of tools into a workflow and to compare the combinatorial results. Although generic frameworks like UIMA (Unstructured Information Management Architecture) provide interoperability to solve this problem, the solution they provide is only partial. In order for truly interoperable toolkits to become a reality, we also need sharable and comparable type systems with an automatic combinatorial comparison generator, which would allow systematic comparisons of available tools. In this paper, we describe such an environment, which we developed based on UIMA, and we show its feasibility through an example of a protein-protein interaction (PPI) extraction system."
I08-1050,Identifying Sections in Scientific Abstracts using Conditional Random Fields,2008,18,99,3,0,48675,kenji hirohata,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"OBJECTIVE: The prior knowledge about the rhetorical structure of scientific abstracts is useful for various text-mining tasks such as information extraction, information retrieval, and automatic summarization. This paper presents a novel approach to categorize sentences in scientific abstracts into four sections, objective, methods, results, and conclusions. METHOD: Formalizing the categorization task as a sequential labeling problem, we employ Conditional Random Fields (CRFs) to annotate section labels into abstract sentences. The training corpus is acquired automatically from Medline abstracts. RESULTS: The proposed method outperformed the previous approaches, achieving 95.5% per-sentence accuracy and 68.8% per-abstract accuracy. CONCLUSION: The experimental results showed that CRFs could model the rhetorical structure of abstracts more suitably."
D08-1047,A Discriminative Candidate Generator for String Transformations,2008,26,27,3,1,4956,naoaki okazaki,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"String transformation, which maps a source string s into its desirable form t*, is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1-regularized logistic regression model. We also propose a procedure to generate negative instances that affect the decision boundary of the model. The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations."
C08-1083,A Discriminative Alignment Model for Abbreviation Recognition,2008,23,13,2,1,4956,naoaki okazaki,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a discriminative alignment model for extracting abbreviations and their full forms appearing in actual text. The task of abbreviation recognition is formalized as a sequential alignment problem, which finds the optimal alignment (origins of abbreviation letters) between two strings (abbreviation and full form). We design a large amount of finegrained features that directly express the events where letters produce or do not produce abbreviations. We obtain the optimal combination of features on an aligned abbreviation corpus by using the maximum entropy framework. The experimental results show the usefulness of the alignment model and corpus for improving abbreviation recognition."
C08-1096,Event Frame Extraction Based on a Gene Regulation Corpus,2008,21,8,5,1,8081,yutaka sasaki,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper describes the supervised acquisition of semantic event frames based on a corpus of biomedical abstracts, in which the biological process of E. coli gene regulation has been linguistically annotated by a group of biologists in the EC research project BOOTStrep. Gene regulation is one of the rapidly advancing areas for which information extraction could boost research. Event frames are an essential linguistic resource for extraction of information from biological literature. This paper presents a specification for linguistic-level annotation of gene regulation events, followed by novel methods of automatic event frame extraction from text. The event frame extraction performance has been evaluated with 10-fold cross validation. The experimental results show that a precision of nearly 50% and a recall of around 20% are achieved. Since the goal of this paper is event frame extraction, rather than event instance extraction, the issue of low recall could be solved by applying the methods to a larger-scale corpus."
W07-1505,An Annotation Type System for a Data-Driven {NLP} Pipeline,2007,19,17,7,0,10102,udo hahn,Proceedings of the Linguistic Annotation Workshop,0,"We introduce an annotation type system for a data-driven NLP core system. The specifications cover formal document structure and document meta information, as well as the linguistic levels of morphology, syntax and semantics. The type system is embedded in the framework of the Unstructured Information Management Architecture (UIMA)."
U07-1001,Text Mining Techniques for Building a Biolexicon,2007,0,0,1,1,3223,sophia ananiadou,Proceedings of the Australasian Language Technology Workshop 2007,0,"My talk will focus on building a biolexicon by leveraging existing bio-resources, combining them within a common, standardized lexical, terminological, conceptual representation framework and employing advanced NL technologies to discover new terms, concepts, relations and linguistic lexical information from text. In particular I will discuss term normalisation techniques, named entity recognition and a smart dictionary look up. This research forms part of the National Centre for Text Mining (www.nactem.ac.uk) and the project BOOTStrep. Proceedings of the Australasian Language Technology Workshop 2007, pages 1-1"
P06-2083,A Term Recognition Approach to Acronym Recognition,2006,19,30,2,1,4956,naoaki okazaki,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We present a term recognition approach to extract acronyms and their definitions from a large text collection. Parenthetical expressions appearing in a text collection are identified as potential acronyms. Assuming terms appearing frequently in the proximity of an acronym to be the expanded forms (definitions) of the acronyms, we apply a term recognition method to enumerate such candidates and to measure the likelihood scores of the expanded forms. Based on the list of the expanded forms and their likelihood scores, the proposed algorithm determines the final acronym-definition pairs. The proposed method combined with a letter matching algorithm achieved 78% precision and 85% recall on an evaluation corpus with 4,212 acronym-definition pairs."
okazaki-ananiadou-2006-clustering,Clustering acronyms in biomedical text for disambiguation,2006,10,22,2,1,4956,naoaki okazaki,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Given the increasing number of neologisms in biomedicine (names of genes, diseases, molecules, etc.), the rate of acronyms used in literature also increases. Existing acronym dictionaries cannot keep up with the rate of new creations. Thus, discovering and disambiguating acronyms and their expanded forms are essential aspects of text mining and terminology management. We present a method for clustering long forms identified by an acronym recognition method. Applying the acronym recognition method to MEDLINE abstracts, we obtained a list of short/long forms. The recognized short/long forms were classified by abiologist to construct an evaluation set for clustering sets of similar long forms. We observed five types of term variation in the evaluation set and defined four similarity measures to gathers the similar longforms (i.e., orthographic, morphological, syntactic, lexico semantic variants, nested abbreviations). The complete-link clustering with the four similarity measures achieved 87.5{\%} precision and 84.9{\%} recall on the evaluation set."
nenadic-etal-2006-towards,Towards a terminological resource for biomedical text mining,2006,23,2,3,1,17963,goran nenadic,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"One of the main challenges in biomedical text mining is the identification of terminology, which is a key factor for accessing and integrating the information stored in literature. Manual creation of biomedical terminologies cannot keep pace with the data that becomes available. Still, many of them have been used in attempts to recognise terms in literature, but their suitability for text mining has been questioned as substantial re-engineering is needed to tailor the resources for automatic processing. Several approaches have been suggested to automatically integrate and map between resources, but the problems of extensive variability of lexical representations and ambiguity have been revealed. In this paper we present a methodology to automatically maintain a biomedical terminological database, which contains automatically extracted terms, their mutual relationships, features and possible annotations that can be useful in text processing. In addition to TermDB, a database used for terminology management and storage, we present the following modules that are used to populate the database: TerMine (recognition, extraction and normalisation of terms from literature), AcroTerMine (extraction and clustering of acronyms and their long forms), AnnoTerm (annotation and classification of terms), and ClusTerm (extraction of term associations and clustering of terms)."
W05-1304,A Machine Learning Approach to Acronym Generation,2005,8,15,2,0.961538,338,yoshimasa tsuruoka,"Proceedings of the {ACL}-{ISMB} Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Semantics",0,"This paper presents a machine learning approach to acronym generation. We formalize the generation process as a sequence labeling problem on the letters in the definition (expanded form) so that a variety of Markov modeling approaches can be applied to this task. To construct the data for training and testing, we extracted acronym-definition pairs from MEDLINE abstracts and manually annotated each pair with positional information about the letters in the acronym. We have built an MEMM-based tagger using this training data set and evaluated the performance of acronym generation. Experimental results show that our machine learning method gives significantly better performance than that achieved by the standard heuristic rule for acronym generation and enables us to obtain multiple candidate acronyms together with their likelihoods represented in probability values."
W04-1812,Design and Implementation of a Terminology-based Literature Mining and Knowledge Structuring System,2004,5,2,2,1,41496,hideki mima,Proceedings of {C}ompu{T}erm 2004: 3rd International Workshop on Computational Terminology,0,"The purpose of the study is to develop an integrated knowledge management system for the domains of genome and nano-technology, in which terminology-based literature mining, knowledge acquisition, knowledge structuring, and knowledge retrieval are combined. The system supports integrating different databases (papers and patents, technologies and innovations) and retrieving different types of knowledge simultaneously. The main objective of the system is to facilitate knowledge acquisition from documents and new knowledge discovery through a terminology-based similarity calculation and a visualization of automatically structured knowledge. Implementation issues of the system are also mentioned."
C04-1087,Enhancing automatic term recognition through recognition of variation,2004,11,55,2,1,17963,goran nenadic,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Terminological variation is an integral part of the linguistic ability to realise a concept in many ways, but it is typically considered an obstacle to automatic term recognition (ATR) and term management. We present a method that integrates term variation in a hybrid ATR approach, in which term candidates are recognised by a set of linguistic filters and termhood assignment is based on joint frequency of occurrence of all term variants. We evaluate the effectiveness of incorporating specific types of term variation by comparing it to the performance of a baseline method that treats term variants as separate terms. We show that ATR precision is enhanced by considering joint termhoods of all term variants, while recall benefits by the introduction of new candidates through consideration of different variation types. On a biomedical test corpus we show that precision can be increased by 20--70% for the top ranked terms, while recall improves generally by 2--25%."
W03-2911,Morpho-syntactic Clues for Terminological Processing in {S}erbian,2003,7,8,3,1,17963,goran nenadic,Proceedings of the 2003 {EACL} Workshop on Morphological Processing of {S}lavic Languages,0,"In this paper we discuss morpho-syntactic clues that can be used to facilitate terminological processing in Serbian. A method (called srCe) for automatic extraction of multiword terms is presented. The approach incorporates a set of generic morpho-syntactic filters for recognition of term candidates, a method for conflation of morphological variants and a module for foreign word recognition. Morpho-syntactic filters describe general term formation patterns, and are implemented as generic regular expressions. The inner structure together with the agreements within term candidates are used as clues to discover the boundaries of nested terms. The results of the terminological processing of a textbook corpus in the domains of mathematics and computer science are presented."
W03-1303,Using Domain-Specific Verbs for Term Classification,2003,10,22,3,1,5891,irena spasic,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"In this paper we present an approach to term classification based on verb complementation patterns. The complementation patterns have been automatically learnt by combining information found in a corpus and an ontology, both belonging to the biomedical domain. The learning process is unsupervised and has been implemented as an iterative reasoning procedure based on a partial order relation induced by the domain-specific ontology. First, term recognition was performed by both looking up the dictionary of terms listed in the ontology and applying the C/NC-value method. Subsequently, domain-specific verbs were automatically identified in the corpus. Finally, the classes of terms typically selected as arguments for the considered verbs were induced from the corpus and the ontology. This information was used to classify newly recognised terms. The precision of the classification method reached 64%."
W03-1316,Selecting Text Features for Gene Name Classification: from Documents to Terms,2003,25,21,4,1,17963,goran nenadic,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"In this paper we discuss the performance of a text-based classification approach by comparing different types of features. We consider the automatic classification of gene names from the molecular biology literature, by using a support-vector machine method. Classification features range from words, lemmas and stems, to automatically extracted terms. Also, simple co-occurrences of genes within documents are considered. The preliminary experiments performed on a set of 3,000 S. cerevisiae gene names and 53,000 Medline abstracts have shown that using domain-specific terms can improve the performance compared to the standard bag-of-words approach, in particular for genes classified with higher confidence, and for under-represented classes."
E03-1077,An Integrated Term-Based Corpus Query System,2003,10,2,4,1,5891,irena spasic,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper we describe the X-TRACT workbench, which enables efficient term-based querying against a domain-specific literature corpus. Its main aim is to aid domain specialists in locating and extracting new knowledge from scientific literature corpora. Before querying, a corpus is automatically terminologically analysed by the ATRACT system, which performs terminology recognition based on the C/NC-value method enhanced by incorporation of term variation handling. The results of terminology processing are annotated in XML, and the produced XML documents are stored in an XML-native database. All corpus retrieval operations are performed against this database using an XML query language. We illustrate the way in which the X-TRACT workbench can be utilised for knowledge discovery, literature mining and conceptual information extraction."
W02-1408,Automatic Discovery of Term Similarities Using Pattern Mining,2002,16,41,3,1,17963,goran nenadic,{COLING}-02: {COMPUTERM} 2002: Second International Workshop on Computational Terminology,0,"Term recognition and clustering are key topics in automatic knowledge acquisition and text mining. In this paper we present a novel approach to the automatic discovery of term similarities, which serves as a basis for both classification and clustering of domain-specific concepts represented by terms. The method is based on automatic extraction of significant patterns in which terms tend to appear. The approach is domain independent: it needs no manual description of domain-specific features and it is based on knowledge-poor processing of specific term features. However, automatically collected patterns are domain specific and identify significant contexts in which terms are used. Beside features that represent contextual patterns, we use lexical and functional similarities between terms to define a combined similarity measure. The approach has been tested and evaluated in the domain of molecular biology, and preliminary results are presented."
spasic-etal-2002-tuning,Tuning Context Features with Genetic Algorithms,2002,11,2,3,1,5891,irena spasic,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In this paper we present an approach to tuning of context features acquired from corpora. The approach is based on the idea of a genetic algorithm (GA). We analyse a whole population of contexts surrounding related linguistic entities in order to find a generic property characteristic of such contexts. Our goal is to tune the context properties so as not to lose any correct feature values, but also to minimise the presence of ambiguous values. The GA implements a crossover operator based on dominant and recessive genes, where a gene corresponds to a context feature. A dominant gene is the one that, when combined with another gene of the same type, is inevitably reflected in the offspring. Dominant genes denote the more suitable context features. In each iteration of the GA, the number of individuals in the population is halved, finally resulting in a single individual that contains context features tuned with respect to the information contained in the training corpus. We illustrate the general method by using a case study concerned with the identification of relationships between verbs and terms complementing them. More precisely, we tune the classes of terms that are typically selected as arguments for the considered verbs in order to acquire their semantic features. * This research is a part of the BioPATH research project coordinated by LION BioScience (http://www.lionbioscience.com) and funded by German Ministry of Research"
nenadic-etal-2002-automatic,Automatic Acronym Acquisition and Term Variation Management within Domain-Specific Texts,2002,11,33,3,1,17963,goran nenadic,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In this paper we present a framework for the effective management of terms and their variants that are automatically acquired from domain-specific texts. In our approach, the term variant recognition is incorporated in the automatic term retrieval process by taking into account orthographical, morphological, syntactic, lexico-semantic and pragmatic term variations. In particular, we address acronyms as a common way of introducing term variants in scientific papers. We describe a method for the automatic acquisition of newly introduced acronyms and the mapping to their xe2x80x98meaningsxe2x80x99, i.e. the corresponding terms. The proposed three-step procedure is based on morpho-syntactic constraints that are commonly used in acronym definitions. First, acronym definitions containing an acronym and the corresponding term are retrieved. These two elements are matched in the second step by performing morphological analysis of words and combining forms constituting the term. The problems of acronym variation and acronym ambiguity are addressed in the third step by establishing classes of term variants that correspond to specific concepts. We present the results of the acronym acquisition in the domain of molecular biology: the precision of the method ranged from 94% to 99% depending on the size of the corpus used for evaluation, whilst the recall was 73%."
C02-1083,A Methodology for Terminology-based Knowledge Acquisition and Integration,2002,10,14,2,1,41496,hideki mima,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"In this paper we propose an integrated knowledge management system in which terminology-based knowledge acquisition, knowledge integration, and XML-based knowledge retrieval are combined using tag information and ontology management tools. The main objective of the system is to facilitate knowledge acquisition through query answering against XML-based documents in the domain of molecular biology. Our system integrates automatic term recognition, term variation management, context-based automatic term clustering, ontology-based inference, and intelligent tag information retrieval. Tag-based retrieval is implemented through interval operations, which prove to be a powerful means for textual mining and knowledge acquisition. The aim is to provide efficient access to heterogeneous biological textual data and databases, enabling users to integrate a wide range of textual and non-textual resources effortlessly."
maynard-ananiadou-2000-creating,Creating and Using Domain-specific Ontologies for Terminological Applications,2000,10,2,2,0,25114,diana maynard,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Huge volumes of scientific databases and text collections are constantly becoming available, but their usefulness is at present hampered by their lack of uniformity and structure. There is therefore an overwhelming need for tools to facilitate the processing and discovery of technical terminology, in order to make processing of these resources more efficient. Both NLP and statistical techniques can provide such tools, but they would benefit greatly from the availability of suitable lexical resources. While information resources do exist in some areas of terminology, these are not designed for linguistic use. In this paper, we investigate how one such resource, the UMLS, is used for terminological acquisition in the TRUCKS system, and how other domain-specific resources might be adapted or created for terminological applications."
C00-1077,Identifying Terms by their Family and {F}riends,2000,11,56,2,0,25114,diana maynard,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"Multi-word terms are traditionally identified using statistical techniques or, more recently, using hybrid techniques combining statistics with shallow linguistic information. Approaches to word sense disambiguation and machine translation have taken advantage of contextual information in a more meaningful way, but terminology has rarely followed suit. We present an approach to term recognition which identifies salient parts of the context and measures their strength of association to relevant candidate terms. The resulting list of ranked terms is shown to improve on that produced by traditional methods, in terms of precision and distribution, while the information acquired in the process can also be used for a variety of other applications, such as disambiguation, lexical tuning and term clustering."
1998.tc-1.15,Machine Translation Trends in {E}urope and {J}apan,1998,-1,-1,1,1,3223,sophia ananiadou,Proceedings of Translating and the Computer 20,0,None
C96-1009,Extracting Nested Collocations,1996,11,113,2,0,55636,katerina frantzi,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"This paper provides an approach to the semi-automatic extraction of collocations from corpora using statistics. The growing availability of large textual corpora, and the increasing number of applications of collocation extraction, has given rise to various approaches on the topic. In this paper, we address the problem of nested collocations; that is, those being part of longer collocations. Most approaches till now, treated substrings of collocations as collocations, only if they appeared frequently enough by themselves in the corpus. These techniques left a lot of collocations unextracted. In this paper, we propose an algorithm for a semi-automatic extraction of nested uninterrupted and interrupted collocations, paying particular attention to nested collocation."
C94-2167,A Methodology for Automatic Term Recognition,1994,6,120,1,1,3223,sophia ananiadou,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
1994.tc-1.2,Terms are not alone: term choice and choice terms,1994,-1,-1,1,1,3223,sophia ananiadou,Proceedings of Translating and the Computer 16,0,None
