2020.acl-main.304,Q17-1010,0,0.0288087,"dings are obtained by average pooling. We feed the token embeddings into the BiLSTM-CRF for decoding. The hidden size of the BiLSTM layer is 256 for the monolingual teacher models and 600 or 800 for the multilingual student model depending on the dataset as larger hidden size for the multilingual model results in better performance in our experiment. The settings of teacher and student models are as follows: • Monolingual Teachers: Each teacher is trained with a dataset of a specific language. We use M-BERT concatenated with languagespecific Flair (Akbik et al., 2018) embeddings and fastText (Bojanowski et al., 2017) word embeddings as token embeddings2 for all the monolingual teacher models. • Multilingual Student: The student model is trained with the datasets of all the languages combined. We only use M-BERT as token embeddings for the multilingual student model. Training For model training, the mini-batch size is set to 2000 tokens. We train all models with SGD optimizer with a learning rate of 0.1 and anneal the learning rate by 0.5 if there is no improvements on the development set for 10 epochs. For all models, we use a single NVIDIA Tesla V100 GPU for training including the student model. We tune"
2020.acl-main.304,P19-1595,0,0.0318413,"9: end for 10: 11: while S < S do 12: S = S + 1. ˆ do 13: for mini-batch (x, y, pˆ) sampled from D 14: Compute the KD loss LKD (x, pˆ). 15: Compute the golden target loss LNLL (x, y). 16: Compute the final loss L = λLKD + (1 − λ)LNLL . 17: Update θ: θ = θ - η ∗ ∂L/∂θ . 18: if λ − τ > 0 do 19: Update interpolation factor λ: λ = λ − τ 20: else 21: Update interpolation factor λ: λ = 0 22: end if 23: end while learns from the gold targets and pseudo targets in training by optimizing the following loss function: LALL = λLKD + (1 − λ)LNLL where λ decreases from 1 to 0 throughout training following Clark et al. (2019), LKD is one of the Eq. 5, 8, 9, 13 or an averaging of Eq. 9, 13. The overall distillation process is summarized in Algorithm 1. 4 4.1 • CoNLL NER: We collect the corpora of 4 languages from the CoNLL 2002 and 2003 shared task (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) • WikiAnn NER (Pan et al., 2017): The dataset contains silver standard NER tags that are annotated automatically on 282 languages that exist in Wikipedia. We select the data of 8 languages from different language families or from different language subgroups of IndoEuropean languages. We randomly choose 5000 sen"
2020.acl-main.304,N19-1383,0,0.0165646,"n be formulated as sequence labeling problems and these tasks can provide extra information to many downstream tasks and products such as searching engine, chat-bot and syntax parsing (Jurafsky and Martin, 2009). Most of the previ∗ Kewei Tu is the corresponding author. This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. ous work on sequence labeling focused on monolingual models, and the work on multilingual sequence labeling mainly focused on cross-lingual transfer learning to improve the performance of low-resource or zero-resource languages (Johnson et al., 2019; Huang et al., 2019a; Rahimi et al., 2019; Huang et al., 2019b; Keung et al., 2019), but their work still trains monolingual models. However, it would be very resource consuming considering if we train monolingual models for all the 7,000+ languages in the world. Besides, there are languages with limited labeled data that are required for training. Therefore it is beneficial to have a single unified multilingual sequence labeling model to handle multiple languages, while less attention is paid to the unified multilingual models due to the significant difference between different languages. Recently, Multilingual"
2020.acl-main.304,D19-1672,0,0.0149242,"n be formulated as sequence labeling problems and these tasks can provide extra information to many downstream tasks and products such as searching engine, chat-bot and syntax parsing (Jurafsky and Martin, 2009). Most of the previ∗ Kewei Tu is the corresponding author. This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. ous work on sequence labeling focused on monolingual models, and the work on multilingual sequence labeling mainly focused on cross-lingual transfer learning to improve the performance of low-resource or zero-resource languages (Johnson et al., 2019; Huang et al., 2019a; Rahimi et al., 2019; Huang et al., 2019b; Keung et al., 2019), but their work still trains monolingual models. However, it would be very resource consuming considering if we train monolingual models for all the 7,000+ languages in the world. Besides, there are languages with limited labeled data that are required for training. Therefore it is beneficial to have a single unified multilingual sequence labeling model to handle multiple languages, while less attention is paid to the unified multilingual models due to the significant difference between different languages. Recently, Multilingual"
2020.acl-main.304,D16-1139,0,0.550108,"model and then trains a weak student model through mimicking the output probabilities (Hinton et al., 2015; Lan et al., 2018; Mirzadeh et al., 2019) or hidden states (Romero 3317 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3317–3330 c July 5 - 10, 2020. 2020 Association for Computational Linguistics et al., 2014; Seunghyun Lee, 2019) of the teacher model. The student model can achieve an accuracy comparable to that of the teacher model and usually has a smaller model size through KD. Inspired by KD applied in neural machine translation (NMT) (Kim and Rush, 2016) and multilingual NMT (Tan et al., 2019), our approach contains a set of monolingual teacher models, one for each language, and a single multilingual student model. Both groups of models are based on BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016), one of the state-of-the-art models in sequence labeling. In BiLSTM-CRF, the CRF layer models the relation between neighbouring labels which leads to better results than simply predicting each label separately based on the BiLSTM outputs. However, the CRF structure models the label sequence globally with the correlations between neighboring label"
2020.acl-main.304,D16-1180,0,0.191514,"Missing"
2020.acl-main.304,N18-1202,0,0.0486673,"Missing"
2020.acl-main.304,P19-1493,0,0.0592419,"Missing"
2020.acl-main.304,D18-1061,0,0.0394979,"Missing"
2020.acl-main.304,P19-1015,0,0.136206,"quence labeling problems and these tasks can provide extra information to many downstream tasks and products such as searching engine, chat-bot and syntax parsing (Jurafsky and Martin, 2009). Most of the previ∗ Kewei Tu is the corresponding author. This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. ous work on sequence labeling focused on monolingual models, and the work on multilingual sequence labeling mainly focused on cross-lingual transfer learning to improve the performance of low-resource or zero-resource languages (Johnson et al., 2019; Huang et al., 2019a; Rahimi et al., 2019; Huang et al., 2019b; Keung et al., 2019), but their work still trains monolingual models. However, it would be very resource consuming considering if we train monolingual models for all the 7,000+ languages in the world. Besides, there are languages with limited labeled data that are required for training. Therefore it is beneficial to have a single unified multilingual sequence labeling model to handle multiple languages, while less attention is paid to the unified multilingual models due to the significant difference between different languages. Recently, Multilingual BERT (M-BERT) (Devlin"
2020.acl-main.304,C18-1327,0,0.0130226,"and multilingual BiLSTM-Softmax model with token-level KD based on Eq. 4 as Softmax and Token for reference. Table 2, 3, and 4 show the effectiveness of our approach on 4 tasks over 25 datasets. In all the tables, we report scores averaged over 5 runs. Observation #0. BiLSTM-Softmax models perform inferior to BiLSTM-CRF models in most cases in the multilingual setting: The results show that the BiLSTM-CRF approach is stronger than the BiLSTM-Softmax approach on three of the four tasks, which are consistent with previous work on sequence labeling (Ma and Hovy, 2016; Reimers and Gurevych, 2017; Yang et al., 2018). The token-level KD approach performs almost the same as the BiLSTM-Softmax baseline in most of the tasks except the Aspect Extraction task. Observation #1. Monolingual teacher models outperform multilingual student models: This is probably because the monolingual teacher models are based on both multilingual embeddings M-BERT and strong monolingual embeddings (Flair/fastText). The monolingual embedding may provide additional information that is not available to the multilingual student models. Furthermore, note that the learning problem faced by a multilingual student model is much more diff"
2020.acl-main.304,W12-1908,0,\N,Missing
2020.acl-main.304,N09-1010,0,\N,Missing
2020.acl-main.304,W03-0419,0,\N,Missing
2020.acl-main.304,N16-1030,0,\N,Missing
2020.acl-main.304,P18-1129,0,\N,Missing
2020.acl-main.304,C18-1139,0,\N,Missing
2020.acl-main.304,W18-6125,0,\N,Missing
2020.acl-main.304,W02-2024,0,\N,Missing
2020.acl-main.304,N19-2023,0,\N,Missing
2020.acl-main.304,N19-1078,0,\N,Missing
2020.acl-main.304,N19-1423,0,\N,Missing
2020.acl-main.304,D19-1441,0,\N,Missing
2020.acl-main.304,D19-1374,0,\N,Missing
2020.acl-main.304,D19-1138,0,\N,Missing
2020.coling-main.227,P10-1131,0,0.00963561,"ohnson (2016) use two large corpora containing more than 700k sentences. Mareˇcek and Straka (2013) utilize a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Han et al. (2017) use a subset of the BLLIP corpus that contains around 180k sentences. With the advancement of computing power and deep neural models, we expect to see more future work on training with big data. 4.5 Unsupervised Multilingual Parsing To tackle the lack of supervision in unsupervised dependency parsing, some previous work considers learning models of multiple languages simultaneously (Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013; Jiang et al., 2019; Han et al., 2019b). Ideally, these models can learn from each other by identifying shared syntactic behaviors of different languages, especially those in the same language family. For example, Berg-Kirkpatrick and Klein (2010) propose to utilize the similarity of different languages defined by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b) propose to learn a unified multilingual parser with language embeddings as input. Jiang et al. (2019) propose to guide the learning process of unsupervised dependency parser from t"
2020.coling-main.227,Q13-1007,0,0.0138753,"e to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity. Berg-Kirkpatrick et al. (2010) use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV. 3.1.2 Inference Given a model parameter"
2020.coling-main.227,D10-1117,0,0.198748,"of the child tokens already generated from a head token. Headden III et al. (2009) propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity. Berg-Kirkpatrick et al. (2010) use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically l"
2020.coling-main.227,D17-1171,1,0.910925,"s is typically employed as the learning objective function for autoencoder models. For a training dataset including N sentences X = {x1 , x2 , ..., xN }, the objective function is as follows: L(Θ) = N X log P(ˆ x(i) |x(i) ; Θ) (5) i=1 where Θ is the model parameter and x ˆ(i) is a copy of x(i) representing the reconstructed sentence1 . In some cases, there is an additional regularization term (e.g., L1) of Θ. 1 In Han et al. (2019a), x is the word sequence, while x ˆ is the POS tag sequence of the same sentence. 2526 The first autoencoder model for unsupervised dependency parsing, proposed by Cai et al. (2017), is based on the conditional random field autoencoder framework (CRFAE). The encoder is a first-order graph-based discriminative dependency parser mapping an input sentence to the space of dependency trees. The decoder independently generates each token of the reconstructed sentence conditioned on the head of the token specified by the dependency tree. Both the encoder and the decoder are arc-factored, meaning that the encoding and decoding probabilities can be factorized by dependency arcs. Coordinate descent is applied to minimize the reconstruction loss and alternately updates the encoder"
2020.coling-main.227,N09-1009,0,0.056112,"Missing"
2020.coling-main.227,N19-1423,0,0.0363274,"nd Smith (2012) 64.3 Tu and Honavar (2012) 71.4 Bisk and Hockenmaier (2012) 71.5 Spitkovsky et al. (2013) 72.0 Jiang et al. (2016) 72.5 Han et al. (2017) 75.1 He et al. (2018)* 60.2 Discriminative Approaches Daum´e III (2009) Le and Zuidema (2015) † 73.2 Cai et al. (2017) 71.7 Li et al. (2019) 54.7 Han et al. (2019a) 75.6 59.1 53.1 57.0 53.3 64.4 57.6 59.5 47.9 45.4 65.8 55.7 37.8 61.4 Table 2: Reported directed dependency accuracies on section 23 of the WSJ corpus, evaluated on sentences of length ≤ 10 and all lengths. *: without gold POS tags. †: with more training data in addition to WSJ. (Devlin et al., 2019) are even more informative, capturing contextual information. However, word embeddings have not been widely used in unsupervised dependency parsing. One concern is that word embeddings are too informative and may make unsupervised models more prone to overfitting. One exception is He et al. (2018), who propose to use invertible neural projections to map word embeddings into a latent space that is more amenable to unsupervised parsing. 4.4 Big Data Although unsupervised parsing does not require syntactically annotated training corpora and can theoretically use almost unlimited raw texts for tra"
2020.coling-main.227,K15-1012,0,0.0224171,"ependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but al"
2020.coling-main.227,N16-1024,0,0.0310519,"follows a distribution, different sampling approaches are used to optimize the objective function (i.e., likelihood) according to different model schema. Three unsupervised dependency parsing models were proposed in recent years based on variational autoencoders (shown in Table 1). There are three probabilities involved in ELBO: the prior probability of the syntactic structure, the probability of generating the sentence from the syntactic structure (the decoder), and the variational posterior (the encoder) from the sentence to the syntactic structure. Recurrent Neural Network Grammars (RNNG) (Dyer et al., 2016) is a transition-based constituent parser, with a discriminative and a generative variant. Discriminative RNNG incrementally constructs the constituency tree of the input sentence through three kinds of operations: generating a non-terminal token, shifting, and reducing. Generative RNNG replaces the shifting operation with a word generation operation and incrementally generates a constituency tree and its corresponding sentence. The probability of each operation is calculated by a neural network. Li et al. (2019) modify RNNG for dependency parsing and use discriminative RNNG and generative RNN"
2020.coling-main.227,P10-2036,0,0.050584,"Missing"
2020.coling-main.227,N12-1069,0,0.0533256,"Missing"
2020.coling-main.227,P15-1133,0,0.0159582,"sampling algorithm. The variational variant of D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector. 2527 3.2.3 Other Discriminative Approaches Apart from the approaches based on autoencoder and variational autoencoder, there are also a few other discriminative approaches based on discriminative clustering (Grave and Elhadad, 2015), self-training (Le and Zuidema, 2015), or searching (Daum´e III, 2009). Because of space limit, below we only introduce the approach based on discriminative clustering called Convex MST (Grave and Elhadad, 2015). Convex MST employs a first-order graph-based discriminative parser. It searches for the parses of all the training sentences and learns the parser simultaneously, with a learning objective that the searched parses are close to the predicted parses by the parser. In other words, the parses should be easily predictable by the parser. The objective function can be relaxed to become conv"
2020.coling-main.227,D17-1176,1,0.909602,"tive approaches all make use of neural networks (Li et al., 2019; Corro and Titov, 2018). 4.3 Lexicalization In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance, Han et al. (2017) use neural networks to predict dependency probabilities that are automatically smoothed. In p"
2020.coling-main.227,P19-1526,1,0.643102,"algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches with more complicated objectives often require more advanced learning algorithms, but many of the algorithms can still be seen as extensions of the EM algorithm that revise either the Estep (e.g., to update Q(z) based on posterior regularization terms) or the M-step (e.g., to optimize the posterior probability that incorporates parameter priors). 2525 Autoencoder Variational Autoencoder CRFAE (Cai et al., 2017) D-NDMV (Han et al., 2019a) Deterministic Variant (Li et al., 2019) D-NDMV (Han et al., 2019a) Variational Variant (Corro and Titov, 2018) Intermediate Representation Z Encoder Decoder P (z|x) P (ˆ x|z) S P (s|x) P (z, x ˆ|s) Z P (z|x) P (z, x) S P (s|x) P (z, x|s) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation. x ˆ is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x. In addition to the EM algorithm,"
2020.coling-main.227,D19-1576,1,0.272781,"algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches with more complicated objectives often require more advanced learning algorithms, but many of the algorithms can still be seen as extensions of the EM algorithm that revise either the Estep (e.g., to update Q(z) based on posterior regularization terms) or the M-step (e.g., to optimize the posterior probability that incorporates parameter priors). 2525 Autoencoder Variational Autoencoder CRFAE (Cai et al., 2017) D-NDMV (Han et al., 2019a) Deterministic Variant (Li et al., 2019) D-NDMV (Han et al., 2019a) Variational Variant (Corro and Titov, 2018) Intermediate Representation Z Encoder Decoder P (z|x) P (ˆ x|z) S P (s|x) P (z, x ˆ|s) Z P (z|x) P (z, x) S P (s|x) P (z, x|s) Z P (z|x) P (x|z) Table 1: Major approaches based on autoencoders and variational autoencoders for unsupervised dependency parsing. Z: dependency tree. S: continuous sentence representation. x ˆ is a copy of x representing the reconstructed sentence. z is the dependency tree. s is the continuous representation of sentence x. In addition to the EM algorithm,"
2020.coling-main.227,D18-1160,0,0.676031,"the neural network in order to compute sentence-specific rule probabilities. Compared with generative approaches, it is more natural for discriminative approaches to use neural networks to score dependencies or parsing actions, so recent discriminative approaches all make use of neural networks (Li et al., 2019; Corro and Titov, 2018). 4.3 Lexicalization In the most common setting of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much large"
2020.coling-main.227,P19-1311,0,0.0174769,"dency parsing can serve as the inspiration for studies of other unsupervised tasks, especially unsupervised structured prediction tasks. A recent example is Nishida and Nakayama (2020), who study unsupervised discourse parsing (inducing discourse structures for a given text) by borrowing techniques from unsupervised parsing such as Viterbi EM and heuristically designed initialization. Unsupervised dependency parsing techniques can also be used as building blocks for transfer learning of parsers. Some of the approaches discussed in this paper have already been applied to cross-lingual parsing (He et al., 2019; Li and Tu, 2020), and more such endeavors are expected in the future. 6.3 Interpretability One prominent problem of deep neural networks is that they act as black boxes and are generally not interpretable. How to improve the interpretability of neural networks is a research topic that gains much attention recently. For natural language texts, their linguistic structures reveal important information of the texts and at the same time can be easily understood by human. It is therefore an interesting direction to integrate techniques of unsupervised parsing into various neural models of NLP task"
2020.coling-main.227,N09-1012,0,0.110825,"Missing"
2020.coling-main.227,D16-1073,1,0.963124,"g, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute generation probabilities from a set of features that conveys syntactic similarity. Berg-Kirkpatrick et al. (2010) use a log-linear model based on manually-designed local morpho-syntactic features (e.g., whether a word is a noun) and Jiang et al. (2016) employ a neural network to automatically learn such features. Both approaches are based on DMV. 3.1.2 Inference Given a model parameterized by Θ and a sentence x, the model predicts the parse z∗ with the highest probability. z∗ = arg max P (x, z; Θ) z∈Z(x) (1) where Z(x) is the set of all valid dependency trees of the sentence x. Due to the independence assumptions made by generative models, the inference problem can be efficiently solved exactly in most cases. For example, chart parsing can be used for DMV. 2524 3.1.3 Learning Objective Log marginal likelihood is typically employed as the ob"
2020.coling-main.227,D17-1177,1,0.84753,"unction can be relaxed to become convex and then can be optimized exactly. 3.2.4 Pros and Cons Discriminative models are capable of accessing global features from the whole input sentence and are typically more expressive than generative models. On the other hand, discriminative approaches are often more complicated and do not admit tractable exact inference. 4 4.1 Recent Trends Combined Approaches Generative approaches and discriminative approaches have different pros and cons. Therefore, a natural idea is to combine the strengths of the two types of approaches to achieve better performance. Jiang et al. (2017) propose to jointly train two state-of-the-art models of unsupervised dependency parsing, the generative LC-DMV (Noji et al., 2016) and the discriminative Convex MST, with the dual decomposition technique that encourages the two models to gradually influence each other during training. 4.2 Neural Parameterization Traditional generative approaches either directly learn or use manually-designed features to compute dependency rule probabilities. Following the recent rise of deep learning in the field of NLP, Jiang et al. (2016) propose to predict dependency rule probabilities using a neural netwo"
2020.coling-main.227,D19-1148,1,0.821377,"than 700k sentences. Mareˇcek and Straka (2013) utilize a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Han et al. (2017) use a subset of the BLLIP corpus that contains around 180k sentences. With the advancement of computing power and deep neural models, we expect to see more future work on training with big data. 4.5 Unsupervised Multilingual Parsing To tackle the lack of supervision in unsupervised dependency parsing, some previous work considers learning models of multiple languages simultaneously (Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013; Jiang et al., 2019; Han et al., 2019b). Ideally, these models can learn from each other by identifying shared syntactic behaviors of different languages, especially those in the same language family. For example, Berg-Kirkpatrick and Klein (2010) propose to utilize the similarity of different languages defined by a phylogenetic tree and learn several dependency parsers jointly. Han et al. (2019b) propose to learn a unified multilingual parser with language embeddings as input. Jiang et al. (2019) propose to guide the learning process of unsupervised dependency parser from the knowledge of another language by us"
2020.coling-main.227,P04-1061,0,0.756548,"d word. In unsupervised dependency parsing, the goal is to obtain a dependency parser without using annotated sentences. Some work requires no training data and derives dependency trees from centrality or saliency information (Søgaard, 2012). We focus on learning a dependency parser from an unannotated dataset that consists of a set of sentences without any parse tree annotation. In many cases, part-of-speech (POS) tags of the words in the training sentences are assumed to be available during training. Two evaluation metrics are widely used in previous work of unsupervised dependency parsing (Klein and Manning, 2004): directed dependency accuracy (DDA) and undirected dependency accuracy (UDA). DDA denotes the percentage of correctly predicted dependency edges, while UDA is similar to DDA but disregards the directions of edges when evaluating their correctness. 2.2 Related Areas Supervised Dependency Parsing Supervised dependency parsing aims to train a dependency parser from training sentences that are manually annotated with their dependency parse trees. Generally, supervised dependency parsing approaches can be divided into graph-based approaches and transition-based approaches. A graph-based dependency"
2020.coling-main.227,N15-1067,0,0.0167662,"D-NDMV (Han et al., 2019a) has the same structure as the deterministic variant described in Section 3.2.1, except that the variational variant probabilistically models the intermediate continuous vector conditioned on the input sentence using a Gaussian distribution. It also specifies a Gaussian prior over the intermediate continuous vector. 2527 3.2.3 Other Discriminative Approaches Apart from the approaches based on autoencoder and variational autoencoder, there are also a few other discriminative approaches based on discriminative clustering (Grave and Elhadad, 2015), self-training (Le and Zuidema, 2015), or searching (Daum´e III, 2009). Because of space limit, below we only introduce the approach based on discriminative clustering called Convex MST (Grave and Elhadad, 2015). Convex MST employs a first-order graph-based discriminative parser. It searches for the parses of all the training sentences and learns the parser simultaneously, with a learning objective that the searched parses are close to the predicted parses by the parser. In other words, the parses should be easily predictable by the parser. The objective function can be relaxed to become convex and then can be optimized exactly."
2020.coling-main.227,2020.findings-emnlp.193,1,0.735047,"serve as the inspiration for studies of other unsupervised tasks, especially unsupervised structured prediction tasks. A recent example is Nishida and Nakayama (2020), who study unsupervised discourse parsing (inducing discourse structures for a given text) by borrowing techniques from unsupervised parsing such as Viterbi EM and heuristically designed initialization. Unsupervised dependency parsing techniques can also be used as building blocks for transfer learning of parsers. Some of the approaches discussed in this paper have already been applied to cross-lingual parsing (He et al., 2019; Li and Tu, 2020), and more such endeavors are expected in the future. 6.3 Interpretability One prominent problem of deep neural networks is that they act as black boxes and are generally not interpretable. How to improve the interpretability of neural networks is a research topic that gains much attention recently. For natural language texts, their linguistic structures reveal important information of the texts and at the same time can be easily understood by human. It is therefore an interesting direction to integrate techniques of unsupervised parsing into various neural models of NLP tasks, such that the n"
2020.coling-main.227,2020.acl-main.300,1,0.770267,"ncy tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce not only edges but also nodes of a tree. Consequently, there have been far more papers in unsupervised dependency parsing than in unsupervised constituency parsing over the past decade. More recently, however, there is a surge in interest in unsupervised constituency parsing and several novel approaches were proposed in the past two years (Li et al., 2020). While we focus on unsupervised dependency parsing in this paper, most of our discussions on the classification of approaches and recent trends apply to unsupervised constituency parsing as well. Latent Tree Models with Downstream Tasks Latent tree models treat the parse tree as a latent variable that is used in downstream tasks such as sentiment classification. While no treebank is used in training, these models rely on the performance of the downstream tasks to guide the learning of the latent parse trees. To enable end-to-end learning, the REINFORCE algorithm and the Gumbel-softmax trick ("
2020.coling-main.227,P13-1105,0,0.0406975,"Missing"
2020.coling-main.227,P14-1126,0,0.0191202,"transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because it has to induce"
2020.coling-main.227,P13-1028,0,0.045653,"Missing"
2020.coling-main.227,D12-1028,0,0.049334,"Missing"
2020.coling-main.227,H05-1066,0,0.398895,"Missing"
2020.coling-main.227,D11-1006,0,0.0357848,"onald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dependency parsing because"
2020.coling-main.227,D10-1120,0,0.243154,"we mentioned earlier, the joint probability of a sentence and its dependency tree can be decomposed into the product of the probabilities of the components in the dependency tree. Apart from the vanilla marginal likelihood, priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibi"
2020.coling-main.227,2020.tacl-1.15,0,0.0780453,"dency parsing and dependency grammar induction) is the most challenging, which aims to obtain a dependency parser without using annotated sentences. Despite its difficulty, unsupervised parsing is an interesting research direction, not only because it would reveal ways to utilize almost unlimited text data without the need for human annotation, but also because it can serve as the basis for studies of transfer and semi-supervised learning of parsers. The techniques developed for unsupervised dependency parsing could also be utilized for other NLP tasks, such as unsupervised discourse parsing (Nishida and Nakayama, 2020). In addition, research in unsupervised parsing inspires and verifies cognitive research of human language acquisition. In this paper, we conduct a survey of unsupervised dependency parsing research. We first introduce the definition and evaluation metrics of unsupervised dependency parsing, and discuss research areas related to it. Then we present in detail two major classes of approaches to unsupervised dependency parsing: generative approaches and discriminative approaches. Finally, we discuss important new techniques and setups of unsupervised dependency parsing that appear in recent years"
2020.coling-main.227,D16-1004,0,0.675614,"Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Learning Algorithm The Expectation-Maximization (EM) algorithm is typically used to optimize log marginal likelihood. For each sentence, the EM algorithm aims to maximize the following lower-bound of the objective function and alternates between the E-step and M-step. log P (x; Θ) − KL(Q(z)kP (z|x, Θ)) (4) where Q(z) is an auxiliary distribution with regard to z. In the E-step, Θ is fixed and Q(z) is set to P (z|x, Θ). A set of so-called expected counts can be derived from Q(z) to faci"
2020.coling-main.227,C16-1003,0,0.0830307,"dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance, Han et al. (2017) use neural networks to predict dependency probabilities that are automatically smoothed. In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similarities between words. Recently"
2020.coling-main.227,N18-1202,0,0.00783051,"pproaches listed in this table may use different training sets and different external 2529 knowledge in their experiments, and one should check the corresponding papers to understand such differences before comparing these accuracies. While the accuracy of unsupervised dependency parsing has increased by over thirty points in the last fifteen years, it is still well below that of supervised models, which leaves much room for improvement and challenges for future research. 6 6.1 Future Directions Utilization of Syntactic Information in Pretrained Language Modeling Pretrained language modeling (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling. However, pretrained langua"
2020.coling-main.227,P07-1049,0,0.0488629,"of unsupervised dependency parsing, the parser is unlexicalized with POS tags being the tokens in the sentences. The POS tags are either human annotated or induced from the training corpus (Spitkovsky et al., 2011a; He et al., 2018). However, words with the same POS tag may have very different syntactic behavior and hence it should be beneficial to introduce lexical information into unsupervised parsers. Headden III et al. (2009), Blunsom and Cohn (2010), and Han et al. (2017) use partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Yuret (1998), Seginer (2007), Pate and Johnson (2016), and Spitkovsky et al. (2013) experiment with full lexicalization. However, because the number of words is huge, a major problem with full lexicalization is that the grammar becomes much larger and thus learning requires more data. To mitigate the negative impact of data scarcity, smoothing techniques can be used. For instance, Han et al. (2017) use neural networks to predict dependency probabilities that are automatically smoothed. In principle, lexicalized approaches could also benefit from pretrained word embeddings, which capture syntactic and semantic similaritie"
2020.coling-main.227,P06-1072,0,0.00842549,", x(2) , ..., x(N ) }: L(Θ) = N X log P(x(i) ; Θ) (2) i=1 where the model parameters are denoted by Θ. The likelihood of each sentence x is as follows: X P (x; Θ) = P (x, z; Θ) (3) z∈Z(x) where Z(x) is the set of all valid dependency trees of sentence x. As we mentioned earlier, the joint probability of a sentence and its dependency tree can be decomposed into the product of the probabilities of the components in the dependency tree. Apart from the vanilla marginal likelihood, priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce"
2020.coling-main.227,N10-1116,0,0.198444,"xiliary distribution with regard to z. In the E-step, Θ is fixed and Q(z) is set to P (z|x, Θ). A set of so-called expected counts can be derived from Q(z) to facilitate the subsequent Mstep and they are typically calculated using the inside-outside algorithm. In the M-step, Θ is optimized based on the expected counts with Q(z) fixed. There are a few variants of the EM algorithm. If Q(z) represents a point-estimation (i.e., the best dependency tree has a probability of 1), the algorithm becomes hard-EM or Viterbi EM, which is found to outperform standard EM in unsupervised dependency parsing (Spitkovsky et al., 2010b). SoftmaxEM (Tu and Honavar, 2012) falls between EM (considering all possible dependency trees) and hard-EM (only considering the best dependency tree), applying a softmax-like transformation to Q(z). During the EM iterations, an annealing schedule (Tu and Honavar, 2012) can be used to gradually shift from hardEM to softmax-EM and finally to the EM algorithm, which leads to better performance than sticking to a single algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches"
2020.coling-main.227,W10-2902,0,0.447046,"xiliary distribution with regard to z. In the E-step, Θ is fixed and Q(z) is set to P (z|x, Θ). A set of so-called expected counts can be derived from Q(z) to facilitate the subsequent Mstep and they are typically calculated using the inside-outside algorithm. In the M-step, Θ is optimized based on the expected counts with Q(z) fixed. There are a few variants of the EM algorithm. If Q(z) represents a point-estimation (i.e., the best dependency tree has a probability of 1), the algorithm becomes hard-EM or Viterbi EM, which is found to outperform standard EM in unsupervised dependency parsing (Spitkovsky et al., 2010b). SoftmaxEM (Tu and Honavar, 2012) falls between EM (considering all possible dependency trees) and hard-EM (only considering the best dependency tree), applying a softmax-like transformation to Q(z). During the EM iterations, an annealing schedule (Tu and Honavar, 2012) can be used to gradually shift from hardEM to softmax-EM and finally to the EM algorithm, which leads to better performance than sticking to a single algorithm. Lateen EM (Spitkovsky et al., 2011c) repeatedly alternates between EM and hard-EM, which is also found to produce better results than both EM and hard-EM. Approaches"
2020.coling-main.227,D11-1118,0,0.371112,"priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Lea"
2020.coling-main.227,W11-0303,0,0.356416,"priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Lea"
2020.coling-main.227,D11-1117,0,0.334101,"priors and regularization terms are often added into the objective function to incorporate various inductive biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Lea"
2020.coling-main.227,D12-1063,0,0.0162904,"odel with Valence (DMV) that generates the sentence and the parse tree simultaneously. Without knowing the dependency tree structure, each head token has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden III et al. (2009) propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. Fo"
2020.coling-main.227,D13-1204,0,0.17345,"addition to the EM algorithm, the learning objective can also be optimized with gradient descent. Yang et al. (2020) recently observe that gradient descent can sometimes significantly outperform EM when learning neural DMV. Better learning results can also be achieved by manipulating the training data. Spitkovsky et al. (2010a) apply curriculum learning to DMV training, which starts with only the shortest sentences and then progresses to increasingly longer sentences. Tu and Honavar (2011) provide a theoretical analysis on the utility of curriculum learning in unsupervised dependency parsing. Spitkovsky et al. (2013) propose to treat different learning algorithms and configurations as modules and connect them to form a network. Some approaches discussed above, such as Lateen EM and curriculum learning, can be seen as special cases of this approach. 3.1.5 Pros and Cons It is often straightforward to incorporate various inductive biases and manually-designed local features into generative approaches. Moreover, generative models can be easily trained via the EM algorithm and its extensions. On the other hand, generative models often have limited expressive power because of the independence assumptions they m"
2020.coling-main.227,D12-1121,1,0.917515,"biases. Smith and Eisner (2006) insert penalty terms into the objective to control dependency lengths and the root number of the parse tree. Cohen and Smith (2008; 2009) leverage logistic-normal prior distributions to encourage correlations between POS tags in DMV. Naseem et al. (2010) design a posterior constraint based on a set of manually-specified universal dependency rules. Gillenwater et al. (2011) add a posterior regularization term to encourage rule sparsity. The approaches of Spitkovsky et al. (2011b) can be seen as adding posterior constraints over parse trees based on punctuation. Tu and Honavar (2012) introduce an entropy term to prevent the model from ˇ becoming too ambiguous. Mareˇcek and Zabokrtsk` y (2012) insert a term that prefers reducible subtrees (i.e., their removal does not break the grammaticality of the sentence) in the parse tree. The same reducibility principle is used by Mareˇcek and Straka (2013) to bias the decision probabilities in DMV. Noji et al. (2016) place a hard constraint in the objective that limits the degree of center-embedding of the parse tree. 3.1.4 Learning Algorithm The Expectation-Maximization (EM) algorithm is typically used to optimize log marginal like"
2020.coling-main.227,2020.coling-main.347,1,0.836207,"has to sample a decision (conditioned on the head token and the dependency direction) of whether to generate a child token or not before actually generating the child token. Besides, the generation of a child token in DMV is additionally conditioned on the valence, defined as the number of the child tokens already generated from a head token. Headden III et al. (2009) propose to also introduce the valence into the condition of decision sampling. Spitkovsky et al. (2012) additionally condition decision and child token generation on sibling words, sentence completeness, and punctuation context. Yang et al. (2020) propose a second-order extension of DMV that incorporates grandparent-child or sibling information. In addition to these generative dependency models, other grammar formalisms have also been used for unsupervised dependency parsing, such as tree substitution grammars (Blunsom and Cohn, 2010) and combinatory categorial grammars (Bisk and Hockenmaier, 2012; Bisk and Hockenmaier, 2013). Similar tokens may have similar syntactic behaviors in a grammar. For example, all the verbs are very likely to generate a noun to the left as the subject. One way to capture this prior knowledge is to compute ge"
2020.coling-main.227,W15-2201,0,0.026376,"of its edges (McDonald et al., 2005). A transition-based dependency parser searches for a sequence of actions that incrementally constructs the parse tree, typically from left to right. While current start-of-the-art approaches have achieved strong results in supervised dependency parsing, their usefulness is limited to resource-rich languages and domains with many annotated datasets. Cross-Domain and Cross-Lingual Parsing One useful approach to handling the lack of treebank resources in the target domain or language is to adapt a learned parser from a resource-rich source domain or language (Yu et al., 2015; McDonald et al., 2011; Ma and Xia, 2014; Duong et al., 2015). This is very related to unsupervised parsing as both approaches do not rely on treebanks in the target domain or language. However, unsupervised parsing is more challenging because it does not have access to any source treebank either. Unsupervised Constituency Parsing Constituency parsing aims to discover a constituency tree of the input sentence in which the leaf nodes are words and the non-leaf nodes (nonterminal nodes) represent phrases. Unsupervised constituency parsing is often considered more difficult than unsupervised dep"
2020.coling-main.227,P19-1230,0,0.0218836,"Utilization of Syntactic Information in Pretrained Language Modeling Pretrained language modeling (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019), as a new NLP paradigm, has been utilized in various areas including question answering, machine translation, grammatical error correction, and so on. Pretrained language models leverage a large-scale corpus for pretraining and then small data sets of specific tasks for finetuning, reducing the difficulty of downstream tasks and boosting their performance. Current state-of-the-art approaches on supervised dependency parsing, such as Zhou and Zhao (2019), adopt the new paradigm and benefit from pretrained language modeling. However, pretrained language models have not been widely used in unsupervised dependency parsing. One major concern is that pretrained language models are too informative and may make unsupervised models more prone to overfitting. Besides, massive syntactic and semantic information is encoded in pretrained language models and how to extract the syntactic part from them is a challenging task. 6.2 Inspiration for Other Tasks Unsupervised dependency parsing is a classic unsupervised learning task. Many techniques developed fo"
2020.coling-main.347,N10-1083,0,0.0991094,"Missing"
2020.coling-main.347,D10-1117,0,0.394079,"Missing"
2020.coling-main.347,D17-1171,1,0.862383,"del. A main disadvantage of DMV and many of its extensions is that they lack expressiveness. The generation of a dependent token is only conditioned on its parent, the relative direction of the token to its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to discriminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, Grave and Elhadad (2015) uses the idea of disrciminative clustering, Cai et al. (2017) uses a discriminative parser in the CRF-autoencoder framework, and Li et al. (2018) uses an encoder-decoder framework that contains a discriminative transitioned-based parser. For DMV, Han et al. (2019) proposes the discriminative neural DMV which uses a global sentence embedding to introduce contextual information into the calculation of grammar rule probabilities. In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information ∗ Corresponding Author Our source code is available at: https://github.com/sustcsongl"
2020.coling-main.347,N09-1009,0,0.352879,"Missing"
2020.coling-main.347,W16-5901,0,0.260255,"ide algorithm. We can use gradient descent to update θ. X ∇θ Q(θ) = e(r, x)∇θ log pθ (r) (6) r∈R 3914 Learning via direct marginal likelihood optimization We can also use gradient descent to maximize log pθ (x) directly. Based on the derivation of Salakhutdinov et al. (2003)2 , we have X pθ (z|x)∇θ log pθ (x, z) ∇θ (log pθ (x)) = z∈T (x) X = pθ (z|x) X c(r, x, z)∇θ log pθ (r) (7) r∈R z∈T (x) = X e(r, x)∇θ log pθ (r) r∈R where e(r, x) is the expected count of grammar rule r in sentence x based on pθ (z|x). Traditionally, we use the inside-outside algorithm to obtain the expected count e(r, x). Eisner (2016) points out that we can use back-propagation to calculate the expected count e(r, x). e(r, x) = ∂ log pθ (x) ∂ log pθ (r) (8) So we only need to use the inside algorithm to calculate log pθ (x) and then use back-propagation to update the parameters directly, without the need for the outside algorithm. Mini-batch gradient descent as online EM In Equation 7, we note that the gradient contains the term e(r, x). If we use mini-batch gradient descent to optimize log pθ (x), it is analogous to the onlineEM algorithm (Liang and Klein, 2009). To compute the gradient for each mini-batch, we first need"
2020.coling-main.347,P15-1133,0,0.727993,"MV) (Klein and Manning, 2004), which is a probabilistic generative model. A main disadvantage of DMV and many of its extensions is that they lack expressiveness. The generation of a dependent token is only conditioned on its parent, the relative direction of the token to its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to discriminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, Grave and Elhadad (2015) uses the idea of disrciminative clustering, Cai et al. (2017) uses a discriminative parser in the CRF-autoencoder framework, and Li et al. (2018) uses an encoder-decoder framework that contains a discriminative transitioned-based parser. For DMV, Han et al. (2019) proposes the discriminative neural DMV which uses a global sentence embedding to introduce contextual information into the calculation of grammar rule probabilities. In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information ∗ Corresponding Author"
2020.coling-main.347,D17-1176,1,0.941554,"he generation steps. 2.2 Neuralized DMV Models Neural DMV One limitation of the DMV model is that it does not consider the correlation between tokens. Jiang et al. (2016) proposed the Neural DMV (NDMV) model, which uses continuous POS embedding to represent discrete POS tags and calculate rule probabilities through neural networks based on the POS embedding. In this way, the model can learn the correlation between POS tags and smooth grammar rule probabilities accordingly. Lexicalized NDMV Neural DMV is still an unlexicalized model which is based on POS tags and does not use word information. Han et al. (2017) proposed the Lexicalized NDMV (L-NDMV) in which each token is a POS/word pair. The neural network that computes rule probabilities takes both the POS embedding and the word embedding as input. To reduce the vocabulary size, they replace low-frequency words with their POS tags. 3912 3 Method 3.1 Second-Order Parsing In our proposed second-order NDMV, we calculate each rule probability based additionally on the information of the sibling or grandparent. We take sibling-NDMV for example to demonstrate the generative story. • We start with the imaginary root token, generating its only child c wit"
2020.coling-main.347,P19-1526,1,0.915404,"its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to discriminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, Grave and Elhadad (2015) uses the idea of disrciminative clustering, Cai et al. (2017) uses a discriminative parser in the CRF-autoencoder framework, and Li et al. (2018) uses an encoder-decoder framework that contains a discriminative transitioned-based parser. For DMV, Han et al. (2019) proposes the discriminative neural DMV which uses a global sentence embedding to introduce contextual information into the calculation of grammar rule probabilities. In the literature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information ∗ Corresponding Author Our source code is available at: https://github.com/sustcsonglin/second-order-neural-dmv This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 1 Licence details: 3911 Proceedings of the 28"
2020.coling-main.347,N09-1012,0,0.339347,"Missing"
2020.coling-main.347,D16-1073,1,0.822588,"decision from the DECISION distribution PDECISION (dec|p, dir, val) to determine whether to generate a new child in direction dir. If dec is CONTINUE, then a new child p is generated from the CHILD distribution PCHILD (c|p, dir, val). If dec is STOP, then p stops generating children in direction dir. The joint probability of the sequence and its corresponding dependency parse tree can be calculated by taking product of the probabilities of all the generation steps. 2.2 Neuralized DMV Models Neural DMV One limitation of the DMV model is that it does not consider the correlation between tokens. Jiang et al. (2016) proposed the Neural DMV (NDMV) model, which uses continuous POS embedding to represent discrete POS tags and calculate rule probabilities through neural networks based on the POS embedding. In this way, the model can learn the correlation between POS tags and smooth grammar rule probabilities accordingly. Lexicalized NDMV Neural DMV is still an unlexicalized model which is based on POS tags and does not use word information. Han et al. (2017) proposed the Lexicalized NDMV (L-NDMV) in which each token is a POS/word pair. The neural network that computes rule probabilities takes both the POS em"
2020.coling-main.347,P19-1228,0,0.123765,"Wp xp es = Ws xs We feed ec , ep , es to the same neural network that consists of three consecutive MLPs. The first and second MLPs are used respectively to insert valence and direction information into the representations, and the last MLP is used to produce final hidden representations hc , hp , hs (see the appendix for the complete formulation). We use different parameters of the first and second MLPs for different values of valence val and direction dir. We add skip-connections to the first and second MLPs because skipconnections have been found very useful in unsupervised neural parsing (Kim et al., 2019). We then follow Wang et al. (2019) and use a decomposed trilinear function to compute the unnormalized rule probability from the three vectors hc , hp , hs . S(p, s, c) = q X op,i × os,i × oc,i i=1 op = Cp hp o c = C c hc os = Cs hs where Cp , Cc , Cs ∈ Rq×d are the parameters of the decomposed trilinear function and × is scalar multiplication. Then we apply a softmax function to produce the final rule probability. eS(p,s,c) PCHILD (c|p, s, dir, val) = P S(p,s,c0 ) e c0 ∈C where C is the vocabulary. 3913 Figure 1: Illustration of our neural architecture. 3.3 Learning The learning objective fu"
2020.coling-main.347,P04-1061,0,0.721677,"sers can reach a very high accuracy (Dozat and Manning, 2017; Zhang et al., 2020). Unfortunately, supervised parsing requires treebanks (annotated parse trees) for training, which are very expensive and time-consuming to build. On the other hand, unsupervised dependency parsing requires only unannotated corpora for training, though the accuracy of unsupervised parsing still lags far behind that of supervised parsing. We focus on unsupervised dependency parsing in this paper. Most methods in the literature of unsupervised dependency parsing are based on the Dependency Model with Valence (DMV) (Klein and Manning, 2004), which is a probabilistic generative model. A main disadvantage of DMV and many of its extensions is that they lack expressiveness. The generation of a dependent token is only conditioned on its parent, the relative direction of the token to its parent, and whether its parent has already generated any child in this direction, hence ignoring other contextual information. To improve model expressiveness, researchers often turn to discriminative methods, which can incorporate more contextual information into the scoring or prediction of dependency arcs. For example, Grave and Elhadad (2015) uses"
2020.coling-main.347,P10-1001,0,0.269076,"ature of supervised graph-based dependency parsing, however, there exists another technique for incorporating contextual information ∗ Corresponding Author Our source code is available at: https://github.com/sustcsonglin/second-order-neural-dmv This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 1 Licence details: 3911 Proceedings of the 28th International Conference on Computational Linguistics, pages 3911–3924 Barcelona, Spain (Online), December 8-13, 2020 and increasing expressiveness, namely high-order parsing (Koo and Collins, 2010; Ma and Hai, 2012). A first-order parser, such as the DMV, only considers local parent-children information. In comparison, a high-order parser takes into account the interaction between multiple dependency arcs. In this work, we propose the second-order neural DMV model, which incorporates second-order information (e.g., sibling or grandparent) into the original (neural) DMV model. To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization (Salakhutdinov et al., 2003; Tran et al., 2016) over th"
2020.coling-main.347,N15-1067,0,0.405157,"Missing"
2020.coling-main.347,N09-1069,0,0.0448775,"se the inside-outside algorithm to obtain the expected count e(r, x). Eisner (2016) points out that we can use back-propagation to calculate the expected count e(r, x). e(r, x) = ∂ log pθ (x) ∂ log pθ (r) (8) So we only need to use the inside algorithm to calculate log pθ (x) and then use back-propagation to update the parameters directly, without the need for the outside algorithm. Mini-batch gradient descent as online EM In Equation 7, we note that the gradient contains the term e(r, x). If we use mini-batch gradient descent to optimize log pθ (x), it is analogous to the onlineEM algorithm (Liang and Klein, 2009). To compute the gradient for each mini-batch, we first need to compute the expected counts from the training sentences in the mini-batch, which is exactly what the online E-step does; we then use the expected counts to compute the gradient and update the model parameters, which is similar to the M-step, except that here we only perform one update step, while in the EM algorithm multiple update steps may be taken based on the same expected counts. According to Liang and Klein (2009), online-EM has a faster convergence speed and can even find a better solution. Empirically, we do find that dire"
2020.coling-main.347,C12-2077,0,0.0321463,"ph-based dependency parsing, however, there exists another technique for incorporating contextual information ∗ Corresponding Author Our source code is available at: https://github.com/sustcsonglin/second-order-neural-dmv This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/. 1 Licence details: 3911 Proceedings of the 28th International Conference on Computational Linguistics, pages 3911–3924 Barcelona, Spain (Online), December 8-13, 2020 and increasing expressiveness, namely high-order parsing (Koo and Collins, 2010; Ma and Hai, 2012). A first-order parser, such as the DMV, only considers local parent-children information. In comparison, a high-order parser takes into account the interaction between multiple dependency arcs. In this work, we propose the second-order neural DMV model, which incorporates second-order information (e.g., sibling or grandparent) into the original (neural) DMV model. To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization (Salakhutdinov et al., 2003; Tran et al., 2016) over the widely used expec"
2020.coling-main.347,D10-1120,0,0.747769,"training, section 22 for validation and section 23 for testing. We use sentences of length ≤ 10 in training and use sentences of length ≤ 10 (WSJ10) and all sentences (WSJ) in testing. Universal Dependency Treebank Following the setting of Li et al. (2018) and Han et al. (2019), we conduct experiments on selected languages from the Universal Dependency Treebank 1.4 (Nivre et al., 2016). We use sentences of length ≤ 15 in training and sentences of length ≤ 15 and ≤ 40 in testing. Setting On the WSJ dataset, for fair comparison, we follow Han et al. (2017) and Han et al. (2019) and use HDP-DEP (Naseem et al., 2010) to initialize our models. Specifically, we train the unsupervised HDP-DEP model on WSJ, use it to parse the training corpus, and then use the predicted parse trees to perform supervised learning of our model for several epochs. On the UD dataset, we use the K&M initialization (Klein and Manning, 2004). We use direct marginal likelihood optimization (DMO) as the training method and use Adam (Kingma and Ba, 2015) as the optimizer with learning rate 0.001. The batch size is set to 64 for WSJ and 100 for UD. The hyperparameters of the neural networks, the setting of L-NDMV and more details can be"
2020.coling-main.347,2020.acl-demos.38,0,0.113497,"hods on log-likelihood for UD (French). 5 10 15 20 iteration 25 30 Figure 5: Comparison of training methods on UAS for UD (French). 3919 Training method separate training joint training UAS (WSJ10 / WSJ) L-NDMV sibling-NDMV 76.6 / 62.7 77.5 / 64.8 79.2 / 65.4 78.7 / 65.6 joint parsing 78.4 / 65.8 79.9 / 67.5 Table 5: The effect of joint training and joint parsing complexity of O(n3 ) of first-order NDMV models, where n is the sentence length. However, transitionbased parsers are hard to batchify, while our model can be parallelized efficiently following the methods introduced by Torch-Struct (Rush, 2020). In practice, our second-order parser runs very fast on GPU, requiring only several minutes to train. 6 Conclusion We propose second-order NDMV models, which incorporate sibling or grandparent information. We find that sibling information is very useful in unsupervised dependency parsing. We use agreement-based learning to combine the benefits of second-order parsing and lexicalization, achieving state-of-the-art results on the WSJ dataset. We also show the effectiveness of our neural parameterization architecture with skip-connections and the direct marginal likelihood optimization method. A"
2020.coling-main.347,D13-1204,0,0.335103,"Missing"
2020.coling-main.347,W16-5907,0,0.0405888,"ing (Koo and Collins, 2010; Ma and Hai, 2012). A first-order parser, such as the DMV, only considers local parent-children information. In comparison, a high-order parser takes into account the interaction between multiple dependency arcs. In this work, we propose the second-order neural DMV model, which incorporates second-order information (e.g., sibling or grandparent) into the original (neural) DMV model. To achieve better learning accuracy, we design a new neural architecture for rule probability computation and promote direct marginal likelihood optimization (Salakhutdinov et al., 2003; Tran et al., 2016) over the widely used expectationmaximization algorithm for training. One particular challenge faced by second-order neural DMVs is that the number of grammar rules grows cubically to the vocabulary size, making it difficult to store and train a lexicalized model containing thousands of words. Therefore, instead of learning a second-order lexicalized model, we propose to jointly learn a second-order unlexicalized model (whose vocabulary consists of POS tags instead of words) and a first-order lexicalized model based on the agreement-based learning framework (Liang et al., 2007). The jointly le"
2020.coling-main.347,D12-1121,1,0.946077,"Missing"
2020.coling-main.347,P19-1454,1,0.816084,"es to the same neural network that consists of three consecutive MLPs. The first and second MLPs are used respectively to insert valence and direction information into the representations, and the last MLP is used to produce final hidden representations hc , hp , hs (see the appendix for the complete formulation). We use different parameters of the first and second MLPs for different values of valence val and direction dir. We add skip-connections to the first and second MLPs because skipconnections have been found very useful in unsupervised neural parsing (Kim et al., 2019). We then follow Wang et al. (2019) and use a decomposed trilinear function to compute the unnormalized rule probability from the three vectors hc , hp , hs . S(p, s, c) = q X op,i × os,i × oc,i i=1 op = Cp hp o c = C c hc os = Cs hs where Cp , Cc , Cs ∈ Rq×d are the parameters of the decomposed trilinear function and × is scalar multiplication. Then we apply a softmax function to produce the final rule probability. eS(p,s,c) PCHILD (c|p, s, dir, val) = P S(p,s,c0 ) e c0 ∈C where C is the vocabulary. 3913 Figure 1: Illustration of our neural architecture. 3.3 Learning The learning objective function L(θ) is the log-likelihood o"
2020.coling-main.347,2020.acl-main.302,0,0.0358361,"Missing"
2020.emnlp-main.182,P18-1152,0,0.0238944,"d C is their parsing accuracy. As we defined in Equation 1, the consensus prediction of parsers B and C is regarded as ground truth, no matter whether the prediction is actually right or wrong. Thus parsers B and C should have high accuracy and also different inductive biases so that they are unlikely to make the same mistake. In addition, B and C should not be too similar to parser A, because otherwise the first two terms in Equation 1 would become hard to optimize. 3.2 Evaluation Criteria for Sentence Quality We consider two aspects of the sentence quality as follows: • Fluency: Inspired by Holtzman et al. (2018); Xu et al. (2018); Pang et al. (2020), we use • Meaning Preservation: Adversarial examples that differ too much from the original sentences are less effective in attacks because humans can easily identify them. We use BERTScore (Zhang et al., 2019b) as another reward in learning to evaluate the similarity between two sentences at the meaning level. We choose to use BERTScore because it correlates better with human judgments than traditional measures such as BLEU (Papineni et al., 2002). sm (x, x ˆ) = BERT Score(x, x ˆ) By maximizing these criteria, we hope to make the adversarial examples loo"
2020.emnlp-main.182,D17-1215,0,0.031583,"ne word from “am” to “fires”. This change makes the perturbed example I fires a writer ungrammatical. Even if the perturbed example is “I fire a writer” that meets the rules of grammar, the true output structure is still different from the input sentence “I am a writer”. More importantly, this true parse is unknown to the attacker, which hinders the next update step. Adversarial examples, which contain perturbations to the input of a model that elicit large changes in the output, have been shown to be an effective way of assessing the robustness of models in natural language processing (NLP) (Jia and Liang, 2017; Belinkov and Bisk, 2018; Hosseini et al., 2017; Samanta and Mehta, 2017; Alzantot et al., † min log P(y =?|ˆ x + r) ∗ Introduction ∗ ˆ1 x ˆ2 x ˆ3 x ˆ4 x x4 2018; Ebrahimi et al., 2018; Michel et al., 2019; Wang et al., 2019). Adversarial training, in which models are trained on adversarial examples, has also been shown to improve the accuracy and robustness of NLP models (Goodfellow et al., 2015; Tram`er et al., 2017; Yasunaga et al., 2018). So far, most existing methods of generating adversarial examples only work for classification tasks (Jia and Liang, 2017; Wang et al., 2019) and are not"
2020.emnlp-main.182,Q16-1023,0,0.0240695,"ion task. 4.1 Data Our model does not need labeled data for training but we need a victim parser and two reference parsers in our experiments. We learn these parsers on an English dataset: Penn Treebank 3.0 (PTB, Marcus et al. (1994)). We also use the same data for training and evaluating our model. 4.2 Parser Selection We choose the Deep Biaffine parser (Dozat and Manning (2017)), one of the state-of-the-art graphbased parsers, as the victim parser A. For the reference parsers, we choose two other well-known dependency parsers: - Parser B: StackPTR from Ma et al. (2018) - Parser C: BiST from Kiperwasser and Goldberg (2016) The three parsers are trained with PTB. All the hyper-parameters of these parsers are the same as reported in their papers. 4.3 Evaluation Metrics Our goal is to generate fluent sentences that are mispredicted by the victim model. Thus, we evaluate the adversarial examples produced by our model from 2 aspects: generation fluency and attacking efficiency (6 metrics). Generation Fluency We use the perplexity on GPT-2 to evaluate the fluency of the generated sentences. Attacking efficiency We evaluate the attacking success rates at the token level and sentence level. The token level attacking su"
2020.emnlp-main.182,N15-1142,0,0.0707263,"Missing"
2020.emnlp-main.182,P16-1101,0,0.0212845,"he sentence level attacking success rate reduces from 72 to 70. We perform significance tests on the attacking success rate. The p-value is calculated by using the one-tailed sign test with bootstrap resampling on 50 samples following Chollampatt, Wang, and Ng (2019). We compare the attacking success rate with and without retraining. The p-values (5.42e20 at the token level and 3.39e-21 at the sentence level) show that the improvement is significant. 5 Experiments on POS Tagging 5.1 Experimental Setup In this section, we apply our method to the part-ofspeech tagging task using the tagger from Ma and Hovy (2016) as the victim model. For the reference taggers, we choose two state-of-the-art taggers: Stanford POS tagger from Toutanova et al. (2003) and Senna tagger from Collobert et al. (2011). All the hyper-parameters of the three taggers are the same as reported in their papers. We conduct the experiments on the PTB dataset. Similar to dependency parsing, the word level approach in section 2.3 is the baseline. For the adversarial example generator, we use the same structure and pretrain strategy as Section 4.4, except that the dimension of hidden state is set to 512. We train the sentence generator u"
2020.emnlp-main.182,P18-1130,0,0.0313215,"arsing, a well-known structured prediction task. 4.1 Data Our model does not need labeled data for training but we need a victim parser and two reference parsers in our experiments. We learn these parsers on an English dataset: Penn Treebank 3.0 (PTB, Marcus et al. (1994)). We also use the same data for training and evaluating our model. 4.2 Parser Selection We choose the Deep Biaffine parser (Dozat and Manning (2017)), one of the state-of-the-art graphbased parsers, as the victim parser A. For the reference parsers, we choose two other well-known dependency parsers: - Parser B: StackPTR from Ma et al. (2018) - Parser C: BiST from Kiperwasser and Goldberg (2016) The three parsers are trained with PTB. All the hyper-parameters of these parsers are the same as reported in their papers. 4.3 Evaluation Metrics Our goal is to generate fluent sentences that are mispredicted by the victim model. Thus, we evaluate the adversarial examples produced by our model from 2 aspects: generation fluency and attacking efficiency (6 metrics). Generation Fluency We use the perplexity on GPT-2 to evaluate the fluency of the generated sentences. Attacking efficiency We evaluate the attacking success rates at the token"
2020.emnlp-main.182,H94-1020,0,0.675587,"ense against Adversarial Attack Following Goodfellow et al. (2015), we use adversarial training to withstand attacks. More specifically, we enhance the victim model by injecting adversarial examples into the training data and retraining the model with the mixed data. 4 Experiments on Dependency Parsing We first perform experiments on dependency parsing, a well-known structured prediction task. 4.1 Data Our model does not need labeled data for training but we need a victim parser and two reference parsers in our experiments. We learn these parsers on an English dataset: Penn Treebank 3.0 (PTB, Marcus et al. (1994)). We also use the same data for training and evaluating our model. 4.2 Parser Selection We choose the Deep Biaffine parser (Dozat and Manning (2017)), one of the state-of-the-art graphbased parsers, as the victim parser A. For the reference parsers, we choose two other well-known dependency parsers: - Parser B: StackPTR from Ma et al. (2018) - Parser C: BiST from Kiperwasser and Goldberg (2016) The three parsers are trained with PTB. All the hyper-parameters of these parsers are the same as reported in their papers. 4.3 Evaluation Metrics Our goal is to generate fluent sentences that are misp"
2020.emnlp-main.182,N19-1314,0,0.181705,"cture is still different from the input sentence “I am a writer”. More importantly, this true parse is unknown to the attacker, which hinders the next update step. Adversarial examples, which contain perturbations to the input of a model that elicit large changes in the output, have been shown to be an effective way of assessing the robustness of models in natural language processing (NLP) (Jia and Liang, 2017; Belinkov and Bisk, 2018; Hosseini et al., 2017; Samanta and Mehta, 2017; Alzantot et al., † min log P(y =?|ˆ x + r) ∗ Introduction ∗ ˆ1 x ˆ2 x ˆ3 x ˆ4 x x4 2018; Ebrahimi et al., 2018; Michel et al., 2019; Wang et al., 2019). Adversarial training, in which models are trained on adversarial examples, has also been shown to improve the accuracy and robustness of NLP models (Goodfellow et al., 2015; Tram`er et al., 2017; Yasunaga et al., 2018). So far, most existing methods of generating adversarial examples only work for classification tasks (Jia and Liang, 2017; Wang et al., 2019) and are not designed for structured prediction tasks. However, since many structured prediction tasks such as partof-speech (POS) tagging and dependency parsing are essential building blocks of many AI systems, it is"
2020.emnlp-main.182,2020.acl-main.333,1,0.720944,"ed in Equation 1, the consensus prediction of parsers B and C is regarded as ground truth, no matter whether the prediction is actually right or wrong. Thus parsers B and C should have high accuracy and also different inductive biases so that they are unlikely to make the same mistake. In addition, B and C should not be too similar to parser A, because otherwise the first two terms in Equation 1 would become hard to optimize. 3.2 Evaluation Criteria for Sentence Quality We consider two aspects of the sentence quality as follows: • Fluency: Inspired by Holtzman et al. (2018); Xu et al. (2018); Pang et al. (2020), we use • Meaning Preservation: Adversarial examples that differ too much from the original sentences are less effective in attacks because humans can easily identify them. We use BERTScore (Zhang et al., 2019b) as another reward in learning to evaluate the similarity between two sentences at the meaning level. We choose to use BERTScore because it correlates better with human judgments than traditional measures such as BLEU (Papineni et al., 2002). sm (x, x ˆ) = BERT Score(x, x ˆ) By maximizing these criteria, we hope to make the adversarial examples look more like human generated sentences"
2020.emnlp-main.182,P02-1040,0,0.106776,"eria for Sentence Quality We consider two aspects of the sentence quality as follows: • Fluency: Inspired by Holtzman et al. (2018); Xu et al. (2018); Pang et al. (2020), we use • Meaning Preservation: Adversarial examples that differ too much from the original sentences are less effective in attacks because humans can easily identify them. We use BERTScore (Zhang et al., 2019b) as another reward in learning to evaluate the similarity between two sentences at the meaning level. We choose to use BERTScore because it correlates better with human judgments than traditional measures such as BLEU (Papineni et al., 2002). sm (x, x ˆ) = BERT Score(x, x ˆ) By maximizing these criteria, we hope to make the adversarial examples look more like human generated sentences and not differ too much from the original sentences in meaning. 3.3 Sentence Generator We propose to use a seq2seq model (Wang et al., 2016) as the adversarial sentence generator, which has been widely used in machine translation, dialogue and many other areas. The seq2seq model specifies P (ˆ x|x; Θ), the conditional probability of generating an adversarial sentence x ˆ given an input sentence x. We train the model by reinforcement learning guided"
2020.emnlp-main.182,N03-1033,0,0.275871,"alue is calculated by using the one-tailed sign test with bootstrap resampling on 50 samples following Chollampatt, Wang, and Ng (2019). We compare the attacking success rate with and without retraining. The p-values (5.42e20 at the token level and 3.39e-21 at the sentence level) show that the improvement is significant. 5 Experiments on POS Tagging 5.1 Experimental Setup In this section, we apply our method to the part-ofspeech tagging task using the tagger from Ma and Hovy (2016) as the victim model. For the reference taggers, we choose two state-of-the-art taggers: Stanford POS tagger from Toutanova et al. (2003) and Senna tagger from Collobert et al. (2011). All the hyper-parameters of the three taggers are the same as reported in their papers. We conduct the experiments on the PTB dataset. Similar to dependency parsing, the word level approach in section 2.3 is the baseline. For the adversarial example generator, we use the same structure and pretrain strategy as Section 4.4, except that the dimension of hidden state is set to 512. We train the sentence generator using reinforcement learning with hyper-parameter α = 1, β = 0.001, γ = 30. Adam(Kingma and Ba, 2014) is used to optimize the parameters w"
2020.emnlp-main.182,D16-1058,0,0.196075,"Missing"
2020.emnlp-main.182,N18-1089,0,0.0187015,"that elicit large changes in the output, have been shown to be an effective way of assessing the robustness of models in natural language processing (NLP) (Jia and Liang, 2017; Belinkov and Bisk, 2018; Hosseini et al., 2017; Samanta and Mehta, 2017; Alzantot et al., † min log P(y =?|ˆ x + r) ∗ Introduction ∗ ˆ1 x ˆ2 x ˆ3 x ˆ4 x x4 2018; Ebrahimi et al., 2018; Michel et al., 2019; Wang et al., 2019). Adversarial training, in which models are trained on adversarial examples, has also been shown to improve the accuracy and robustness of NLP models (Goodfellow et al., 2015; Tram`er et al., 2017; Yasunaga et al., 2018). So far, most existing methods of generating adversarial examples only work for classification tasks (Jia and Liang, 2017; Wang et al., 2019) and are not designed for structured prediction tasks. However, since many structured prediction tasks such as partof-speech (POS) tagging and dependency parsing are essential building blocks of many AI systems, it is important to study adversarial attack (generating adversarial examples) and defense (adversarial training) of structured prediction models. There are multiple challenges that have to be addressed in building an efficient and effective attac"
2020.emnlp-main.182,P19-1559,0,0.165774,"adversarial examples only work for classification tasks (Jia and Liang, 2017; Wang et al., 2019) and are not designed for structured prediction tasks. However, since many structured prediction tasks such as partof-speech (POS) tagging and dependency parsing are essential building blocks of many AI systems, it is important to study adversarial attack (generating adversarial examples) and defense (adversarial training) of structured prediction models. There are multiple challenges that have to be addressed in building an efficient and effective attacker for structured prediction models in NLP. Zhang et al. (2019a) pointed out two major prob2327 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2327–2338, c November 16–20, 2020. 2020 Association for Computational Linguistics lems encountered by attackers of NLP tasks. First, since words are discrete, making small disturbances to words in the gradient direction is difficult. Secondly, there is no guarantee that the generated adversarial examples are fluent. In addition to these two problems, there is a unique challenge faced by attackers of structured prediction tasks. While small perturbations to images or e"
2020.emnlp-main.182,D18-1316,0,0.0347452,"ea of adding continuous perturbations to inputs has been applied to tasks in NLP (Sato et al., 2018; Gong et al., 2018). In order to solve the mapping problem from the modified word vector to the word, Papernot et al. (2016) built a special dictionary to select words to replace the original words. In addition to replacement manipulation, Samanta and Mehta (2017) introduced three modification strategies: removal and addition. Michel et al. (2019) leveraged atomic character-level operation. Some attack strategies to generate adversarial examples have been proposed in the sentence level setting. Zhao et al. (2018) searched adversarial examples in the continuous vector space and then used generative adversarial networks (Goodfellow et al., 2014) to map the fixed-length vectors to data instances. However, these attackers are only designed for classification tasks or generation tasks and can not be easily applied to structured prediction systems. Attack Design on Structured Prediction Model There is also some prior work on attacking structured prediction models. Cisse et al. (2017) proposed to attack structured prediction models in the 2334 Baseline Ours Generation Fluency (Perplexity ↓) 377.36 244.69 Tok"
2020.emnlp-main.485,C18-1139,0,0.0252366,"mation. We use these settings for a better understanding of how the decoders perform on each task when the encoders capture different levels of contextual information. Decoder We use the MaxEnt approach, the traditional CRF approach and AINs with the first-order and factorized second-order CRFs for decoding. We denote these approaches by MaxEnt, CRF, AIN-1O and AIN-F2O respectively. We set the iteration number M to 3 in AINs because we find that more iterations do not result in further improvement in accuracy. Hyper-parameters For the hyper-parameters, we follow the settings of previous work (Akbik et al., 2018). We use Stochastic Gradient Descent for optimization with a fixed learning rate of 0.1 and a batch size of 32. We fix the hidden size of the CNN and BiLSTM layer to 512 and 256 respectively, and the kernel size of CNN to 3. We anneal the learning rate by 0.5 if there is no improvement in the development sets for 10 epochs when training. Evaluation We use F1 score to evaluate the NER, slot filling and chunking tasks and use accuracy to evaluate the POS tagging task. We convert the BIO 6022 format into BIOES format for NERs, slot filling and chunking datasets and use the official release of CoN"
2020.emnlp-main.485,Q17-1010,0,0.0139758,"rsal POS tag annotations with 8 languages for experiments. The list of treebanks is shown in Table 3. We use the standard training/development/test split for experiments. Slot Filling Slot filling is a task that interprets user commands by extracting relevant slots, which can be formulated as a sequence labeling task. We use the Air Travel Information System (ATIS) (Hemphill et al., 1990) dataset for the task. 4 . 3.2 Settings Embeddings For word embeddings in the NER, chunking and slot filling experiments, we use the same word embedding as in Lample et al. (2016) except that we use fastText (Bojanowski et al., 2017) embedding for Dutch which we find significantly improves the accuracy (more than 5 F1 scores on CoNLL NER). We use fastText embeddings for all UD tagging experiments. For character embedding, we use a single layer character CNN with a hidden size of 50, because Yang et al. (2018) empirically showed that it has competitive performance with character LSTM. We concatenate the word embedding and character CNN output for the final word representation. 3 https://lindat.mff.cuni.cz/repository/xmlui/handle/ 11234/1-2837 4 We use the same dataset split as https://github.com/sz128/ slot_filling_and_int"
2020.emnlp-main.485,P17-1178,0,0.0569513,"to a CRF (Lafferty et al., 2001) decoder layer to produce final predictions. The CRF layer is a linear-chain structure that models the relation between neighboring labels. In the traditional CRF approach, exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are applied for training and prediction respectively. In many sequence labeling tasks, the CRF layer leads to better results than the simpler method of predicting each label independently. In practice, we sometimes require very fast sequence labelers for training (e.g., on huge datasets like WikiAnn (Pan et al., 2017)) and prediction (e.g. for low latency online serving). The BiLSTM encoder and the CRF layer both contain sequential computation and require O(n) time over n input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure (Collobert et al., 2011; Strubell et al., 2017), distill larger encoders into smaller ones (Tsai et al., 2019; Mukherjee and Awadallah, 2020) or in other settings (Tu and Gimpel, 2018; Yang et al., 2018; Tu and Gimpel, 2019; Cui and Zhang, 2019). The CRF layer, however, is more difficult to replace"
2020.emnlp-main.485,D17-1283,0,0.0132944,"ctively. In many sequence labeling tasks, the CRF layer leads to better results than the simpler method of predicting each label independently. In practice, we sometimes require very fast sequence labelers for training (e.g., on huge datasets like WikiAnn (Pan et al., 2017)) and prediction (e.g. for low latency online serving). The BiLSTM encoder and the CRF layer both contain sequential computation and require O(n) time over n input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure (Collobert et al., 2011; Strubell et al., 2017), distill larger encoders into smaller ones (Tsai et al., 2019; Mukherjee and Awadallah, 2020) or in other settings (Tu and Gimpel, 2018; Yang et al., 2018; Tu and Gimpel, 2019; Cui and Zhang, 2019). The CRF layer, however, is more difficult to replace because of its superior accuracy compared with faster alternatives in many tasks. In order to achieve sublinear time complexity on the CRF layer, we must parallelize the CRF prediction over the tokens. In this paper, we apply Mean-Field Variational Inference (MFVI) to approximately decode the linear-chain CRF. MFVI iteratively passes messages am"
2020.emnlp-main.485,P19-1454,1,0.646152,"e Viterbi decoding and one iteration of our MFVI inference on the CRF model. Yi is the random variable representing the i-th label with three possible values. The illustrated vectors represent Viterbi scores and Qi distributions respectively. i ˜ has the same shape as U in where the matrix U Eq. 2. The factor graph of our factorized secondorder CRF is shown at the bottom of Figure 1. The update formula is similar to that of our first-order approach but with more neighbors: ′ Qm i (yi |x)∝ exp{ψu (x, yi )+s (i−2, i, m) +s(i−1, i, m)+s(i+1, i, m)+s′ (i+2, i, m)} et al., 2016; Chen et al., 2018; Wang et al., 2019) using the MFVI algorithm for solving intractable problems of densely connected probabilistic models to get better accuracy, we propose to employ the MFVI algorithm to accelerate tractable inference of sequence-structured probabilistic models. As far as we know, this is the first attempt of using approximate inference on tractable models for speedup with GPU parallelization. The time complexity of each iteration of the MFVI algorithm is O(nL2 ), which is on par with the time complexity of the exact probabilistic inference algorithms. However, in each iteration, the update of each distribution"
2020.emnlp-main.485,C18-1327,0,0.119662,"sometimes require very fast sequence labelers for training (e.g., on huge datasets like WikiAnn (Pan et al., 2017)) and prediction (e.g. for low latency online serving). The BiLSTM encoder and the CRF layer both contain sequential computation and require O(n) time over n input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure (Collobert et al., 2011; Strubell et al., 2017), distill larger encoders into smaller ones (Tsai et al., 2019; Mukherjee and Awadallah, 2020) or in other settings (Tu and Gimpel, 2018; Yang et al., 2018; Tu and Gimpel, 2019; Cui and Zhang, 2019). The CRF layer, however, is more difficult to replace because of its superior accuracy compared with faster alternatives in many tasks. In order to achieve sublinear time complexity on the CRF layer, we must parallelize the CRF prediction over the tokens. In this paper, we apply Mean-Field Variational Inference (MFVI) to approximately decode the linear-chain CRF. MFVI iteratively passes messages among neighboring labels to update their distributions locally. Unlike the exact probabilistic inference algorithms, MFVI can be parallelized over different"
2020.findings-emnlp.236,N19-1078,0,0.0199641,"sed quadrilinear potential function based on the vector representations of two neighboring labels and two neighboring words consistently achieves the best performance. 1 BiLSTM Encoder h1 h2 h3 … hn Word Representations x1 x2 x3 … xn Figure 1: Neural architecture for sequence labeling Introduction Sequence labeling is the task of labeling each token of a sequence. It is an important task in natural language processing and has a lot of applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003; Xin et al., 2018), Named Entity Recognition (NER) (Ritter et al., 2011; Akbik et al., 2019), Chunking (Tjong Kim Sang and Buchholz, 2000; Suzuki et al., 2006). The neural CRF model is one of the most widelyused approaches to sequence labeling and can achieve superior performance on many tasks (Collobert et al., 2011; Chen et al., 2015; Ling et al., 2015; Ma and Hovy, 2016; Lample et al., 2016a). It often employs an encoder such as a BiLSTM to compute the contextual vector representation of each word in the input sequence. The potential function at each position of the input sequence in a neural CRF is typically decomposed into an emission function (of the current label and the vecto"
2020.findings-emnlp.236,D15-1141,0,0.026453,"ral architecture for sequence labeling Introduction Sequence labeling is the task of labeling each token of a sequence. It is an important task in natural language processing and has a lot of applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003; Xin et al., 2018), Named Entity Recognition (NER) (Ritter et al., 2011; Akbik et al., 2019), Chunking (Tjong Kim Sang and Buchholz, 2000; Suzuki et al., 2006). The neural CRF model is one of the most widelyused approaches to sequence labeling and can achieve superior performance on many tasks (Collobert et al., 2011; Chen et al., 2015; Ling et al., 2015; Ma and Hovy, 2016; Lample et al., 2016a). It often employs an encoder such as a BiLSTM to compute the contextual vector representation of each word in the input sequence. The potential function at each position of the input sequence in a neural CRF is typically decomposed into an emission function (of the current label and the vector representation of the current word) and a transition function (of the previous and current labels) (Liu et al., 2018; Yang et al., 2018). ∗ Softmax or various CRFs Kewei Tu and Yong Jiang are the corresponding authors. In this paper, we design"
2020.findings-emnlp.236,D19-1422,0,0.136821,"t sequence. The potential function at each position of the input sequence in a neural CRF is typically decomposed into an emission function (of the current label and the vector representation of the current word) and a transition function (of the previous and current labels) (Liu et al., 2018; Yang et al., 2018). ∗ Softmax or various CRFs Kewei Tu and Yong Jiang are the corresponding authors. In this paper, we design a series of increasingly expressive potential functions for neural CRF models. First, we compute the transition function from label embeddings (Ma et al., 2016; Nam et al., 2016; Cui and Zhang, 2019) instead of label identities. Second, we use a single potential function over the current word and the previous and current labels, instead of decomposing it into the emission and transition functions, leading to more expressiveness. We also employ tensor decomposition in order to keep the potential function tractable. Thirdly, we take the representations of additional neighboring words as input to the potential function, instead of solely relying on the BiLSTM to capture contextual information. To empirically evaluate different approaches, we conduct experiments on four well-known sequence la"
2020.findings-emnlp.356,N19-1078,0,0.122839,"Missing"
2020.findings-emnlp.356,C18-1139,0,0.258865,"ettings, does combining different kinds of contextual embeddings result in a better sequence labeler? Are noncontextual embeddings helpful when the models are equipped with contextual embeddings? 2. When we train models in low-resource and cross-domain settings, do the conclusions from the rich-resource settings still hold? 3. Can sequence labelers automatically learn the importance of each kind of embeddings when they are concatenated? 2 2.1 Introduction In recent years, sequence labelers equipped with contextual embeddings have achieved significant accuracy improvement (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2019; Martin et al., 2019) over approaches that use static non-contextual word embeddings (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014). Different types of embeddings have different inductive biases to guide the learning process. However, little work has been done to study how to concatenate these contextual embeddings and non-contextual embeddings to build better sequence labelers in ∗ Yong Jiang and Kewei Tu are the corresponding authors. ‡ : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. Model Architecture Sequen"
2020.findings-emnlp.356,Q17-1010,0,0.0461714,"t use contextual word embeddings such as ELMo (Peters et al., 2018) since Akbik et al. (2018) showed that concatenating Flair embeddings with ELMo embeddings cannot further improve the accuracy. 2 We do not use the pooled version of Flair due to its slower speed in training. Non-contextual Word Embeddings (NWEs) The most common approach to the NWEs is Word2vec (Mikolov et al., 2013), which is a skipgram model learning word representations by predicting neighboring words. Based on this approach, GloVe (Pennington et al., 2014) creates a co-occurrence matrix for global information and fastText (Bojanowski et al., 2017) represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages. Non-contextual Character Embeddings (NCEs) Using character information to represent the embeddings of word is proposed by Santos and Zadrozny (2014) with a lot of following work using a CNN structure to encode character representation (dos Santos and Guimarães, 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016). Lample et al. (2016) utilized BiLSTM on the character sequence of each word. We follow this approach as it usually results in better accuracy (Yang e"
2020.findings-emnlp.356,Q16-1026,0,0.0341466,"esentations by predicting neighboring words. Based on this approach, GloVe (Pennington et al., 2014) creates a co-occurrence matrix for global information and fastText (Bojanowski et al., 2017) represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages. Non-contextual Character Embeddings (NCEs) Using character information to represent the embeddings of word is proposed by Santos and Zadrozny (2014) with a lot of following work using a CNN structure to encode character representation (dos Santos and Guimarães, 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016). Lample et al. (2016) utilized BiLSTM on the character sequence of each word. We follow this approach as it usually results in better accuracy (Yang et al., 2018). 3 Experiments and Results For simplicity, we use M to represent M-BERT embeddings, F to represent Flair embeddings, W to represent fastText embeddings, C to represent non-contextual character embeddings, All to represents the concatenation of all types of embeddings and the operator “+” to represent the concatenation operation. We use the MaxEnt approach for all experiments3 . Due to the space limit, some detail"
2020.findings-emnlp.356,L16-1262,0,0.0773898,"Missing"
2020.findings-emnlp.356,P17-1178,0,0.0164893,"M to represent M-BERT embeddings, F to represent Flair embeddings, W to represent fastText embeddings, C to represent non-contextual character embeddings, All to represents the concatenation of all types of embeddings and the operator “+” to represent the concatenation operation. We use the MaxEnt approach for all experiments3 . Due to the space limit, some detailed experiment settings, extra experiments and discussions are included in the appendix. 3.1 Settings Datasets We use datasets from three multilingual sequence labeling tasks over 8 languages in our experiments: WikiAnn NER datasets (Pan et al., 2017), UD Part-Of-Speech (POS) tagging datasets (Nivre et al., 2016), and CoNLL 2003 chunking datasets (Tjong Kim Sang and De Meulder, 2003). We use language-specific fastText and Flair embeddings depending on the dataset. Embedding Concatenation Since experimenting on all 15 concatenation combinations of the four embeddings is not essential for evaluating the effectiveness of each kind of embeddings, we experiment on the following 7 concatenations: F, F+W, 3 We find that the observations from the MaxEnt experiments do not change in all experiments with the CRF approach. 3993 Relative Scores 10 5 0"
2020.findings-emnlp.356,D14-1162,0,0.0877623,"use the Flair embeddings due to their high accuracy for sequence labeling task2 . 1 We do not use contextual word embeddings such as ELMo (Peters et al., 2018) since Akbik et al. (2018) showed that concatenating Flair embeddings with ELMo embeddings cannot further improve the accuracy. 2 We do not use the pooled version of Flair due to its slower speed in training. Non-contextual Word Embeddings (NWEs) The most common approach to the NWEs is Word2vec (Mikolov et al., 2013), which is a skipgram model learning word representations by predicting neighboring words. Based on this approach, GloVe (Pennington et al., 2014) creates a co-occurrence matrix for global information and fastText (Bojanowski et al., 2017) represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages. Non-contextual Character Embeddings (NCEs) Using character information to represent the embeddings of word is proposed by Santos and Zadrozny (2014) with a lot of following work using a CNN structure to encode character representation (dos Santos and Guimarães, 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016). Lample et al. (2016) utilized BiLSTM on the character se"
2020.findings-emnlp.356,N18-1202,0,0.293936,". In rich-resources settings, does combining different kinds of contextual embeddings result in a better sequence labeler? Are noncontextual embeddings helpful when the models are equipped with contextual embeddings? 2. When we train models in low-resource and cross-domain settings, do the conclusions from the rich-resource settings still hold? 3. Can sequence labelers automatically learn the importance of each kind of embeddings when they are concatenated? 2 2.1 Introduction In recent years, sequence labelers equipped with contextual embeddings have achieved significant accuracy improvement (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2019; Martin et al., 2019) over approaches that use static non-contextual word embeddings (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014). Different types of embeddings have different inductive biases to guide the learning process. However, little work has been done to study how to concatenate these contextual embeddings and non-contextual embeddings to build better sequence labelers in ∗ Yong Jiang and Kewei Tu are the corresponding authors. ‡ : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. Model"
2021.acl-long.142,N19-1078,0,0.0383627,"l together with the input sentence. Finally, we calculate the negative likelihood loss LNLL and LNLL-EXT together with the CL loss (either LCL-L2 or LCL-KL ). ternal contexts: ˜ = [sep_token; x ˆ1; · · · ; x ˆl] x where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, “[SEP]” in BERT). 2.2 NER Model We solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2019). In the sequence labeling model, the input sentence x is fed into a transformer-based pretrained contextual embeddings model to get the token representations {v1 , · · · , vn } by vi =embedi (x). The token representations are fed into a CRF layer to get the conditional probability pθ (y|x): ψ(y 0 , y, vi ) = exp(WyT vi + by0 ,y ) (1) n Q ψ(yi−1 , yi , vi ) i=1 pθ (y|x) = n P Q 0 , y0 , v ) ψ(yi−1 i i y 0 ∈Y(x) i=1 where ψ is the potential function and θ represents the model parameters. Y(x) denotes the set of all possible label sequences given x. y0 is defined to be a special start symbol. WT"
2021.acl-long.142,C18-1139,0,0.337962,"uch to the Supreme Court . Senate Republicans deployed the nuclear option on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump s nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP"
2021.acl-long.142,D19-1539,0,0.0937785,"Missing"
2021.acl-long.142,D19-1195,0,0.0140693,"nslation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual scoring functions to re-rank the retrieved texts. Instead, we use BERTScore to re-rank the retrieved texts instead as BERTScore evaluates semantic correlations between the texts 1807 based on pretrained contextual embeddings. 6 Multi-View Learning Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approaches are semi-super"
2021.acl-long.142,P18-1015,0,0.0136535,"neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual scoring functions to re-rank the retrieved texts. Instead, we use BERTScore to re-rank the retrieved texts instead as BERTScore evaluates semantic correlations between the texts 1807 based on pretrained contextual embeddings. 6 Multi-View Learning Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approa"
2021.acl-long.142,D18-1217,0,0.0495751,"at can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approaches are semi-supervised learning techniques that require two independent views of the data. The model with higher confidence is applied to construct additional labeled data by predicting on unlabeled data. Sun (2013) and Xu et al. (2013) have extensively studied various multiview learning approaches. Hu et al. (2021) shows the effectiveness of multi-view learning on crosslingual structured prediction tasks. Recently, Clark et al. (2018) proposed Cross-View Training (CVT), which trains a unified model instead of multiple models and targets at minimizing the KL divergence between the probability distributions of the model and auxiliary prediction modules. Comparing with CVT, CL targets at improving the accuracy of two kinds of inputs rather than only one of them. We also propose to minimize the distance of token representations between different views in addition to KL-divergence. Besides, CL utilizes the external contexts and therefore we do not need to construct auxiliary prediction modules in the model. Moreover, CVT cannot"
2021.acl-long.142,P19-1082,0,0.0282248,"rength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual scoring functions to re-rank the retrieved texts. Instead, we us"
2021.acl-long.142,D18-1111,0,0.0779912,"Table 3: A comparison of different approaches in transfer learning. The models are trained on the CoNLL-03 dataset. Approach CL-L2 CL-KL CL–L2 +S EMI CL-KL+S EMI SE 59.95 61.79 4.1 Comparison of Re-ranking Approaches Various re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is reranking through a fuzzy match score. The approach has been widely applied in a lot of previous work (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020). The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models (W / C ONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT17 in Table 5. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p < 0.05). The possible reason might be that the tf-idf weighting"
2021.acl-long.142,2020.acl-main.747,0,0.0789365,"(Liu et al., 2019) for token representations which is the default configuration in the code5 of BERTScore (Zhang et al., 2020). For token representations in the NER model, 2 the accuracy of a query counts 1.0 if all the entities in the query are correctly recognized and 0.0 otherwise. 3 If the descriptions are not available, we use the titles of the results instead. 4 We determined that 6 is a reasonable number based on preliminary experiments. 5 https://github.com/Tiiiger/bert_score 1804 we use pretrained Bio-BERT (Lee et al., 2020) for datasets from the biomedical domain and use XLMRoBERTa (Conneau et al., 2020) for datasets from other domains. Training During training, we fine-tune the pretrained contextual embeddings by AdamW (Loshchilov and Hutter, 2018) optimizer with a batch size of 4. We use a learning rate of 5 × 10−6 to update the parameters in the pretrained contextual embeddings. For the CRF layer parameters, we use a learning rate of 0.05. We train the NER models for 10 epochs for the datasets in Social Media and Biomedical domains while we train the NER models for 5 epochs for other datasets for efficiency as these datasets have more training sentences. 3.2 Results We experiment on the fo"
2021.acl-long.142,2021.acl-long.207,1,0.753104,"ased on pretrained contextual embeddings. 6 Multi-View Learning Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approaches are semi-supervised learning techniques that require two independent views of the data. The model with higher confidence is applied to construct additional labeled data by predicting on unlabeled data. Sun (2013) and Xu et al. (2013) have extensively studied various multiview learning approaches. Hu et al. (2021) shows the effectiveness of multi-view learning on crosslingual structured prediction tasks. Recently, Clark et al. (2018) proposed Cross-View Training (CVT), which trains a unified model instead of multiple models and targets at minimizing the KL divergence between the probability distributions of the model and auxiliary prediction modules. Comparing with CVT, CL targets at improving the accuracy of two kinds of inputs rather than only one of them. We also propose to minimize the distance of token representations between different views in addition to KL-divergence. Besides, CL utilizes the e"
2021.acl-long.142,W17-4418,0,0.101114,"Missing"
2021.acl-long.142,N19-1423,0,0.0665368,"ate Republicans deployed the nuclear option on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump s nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/CLNER. ‡ Input Sentence: in wh"
2021.acl-long.142,P19-1236,0,0.15668,"ar dataset for NER. CoNLL++ is a revision of the CoNLL-03 datasets. Wang et al. (2019) fixed annotation errors on the test set by professional annotators and improved the quality of the training data through their CrossWeigh approach. We use the standard dataset split for these datasets. • Biomedical: We use BC5CDR (Li et al., 2016) and NCBI-disease (Do˘gan et al., 2014) datasets, which are two popular biomedical NER datasets. We merge the training and development data as training set following Nooralahzadeh et al. (2019). • Science and Technology: We use CBS SciTech News dataset collected by Jia et al. (2019). The dataset only contains the test set with the same label set as the CoNLL-03 dataset. We use the dataset to evaluate the effectiveness of crossdomain transferability from the news domain. • E-commerce: We collect and annotate an internal dataset from one anonymous E-commerce website. The dataset contains 25 named entity labels for goods in short texts. We also collect 300,000 unlabeled sentences for semi-supervised training. We show the statistics of the datasets in Table 1. Annotations of the E-commerce dataset We manually labeled the user queries through crowdsourcing from www.aliexpress"
2021.acl-long.142,2020.coling-main.207,0,0.0253611,"t work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual sco"
2021.acl-long.142,N16-1030,0,0.0914812,"ds the texts to a transformer-based model together with the input sentence. Finally, we calculate the negative likelihood loss LNLL and LNLL-EXT together with the CL loss (either LCL-L2 or LCL-KL ). ternal contexts: ˜ = [sep_token; x ˆ1; · · · ; x ˆl] x where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, “[SEP]” in BERT). 2.2 NER Model We solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2019). In the sequence labeling model, the input sentence x is fed into a transformer-based pretrained contextual embeddings model to get the token representations {v1 , · · · , vn } by vi =embedi (x). The token representations are fed into a CRF layer to get the conditional probability pθ (y|x): ψ(y 0 , y, vi ) = exp(WyT vi + by0 ,y ) (1) n Q ψ(yi−1 , yi , vi ) i=1 pθ (y|x) = n P Q 0 , y0 , v ) ψ(yi−1 i i y 0 ∈Y(x) i=1 where ψ is the potential function and θ represents the model parameters. Y(x) denotes the set of all possible label sequences given x. y0 is"
2021.acl-long.142,2020.acl-main.45,0,0.0391605,"Missing"
2021.acl-long.142,2021.ccl-1.108,0,0.0533332,"Missing"
2021.acl-long.142,2020.coling-main.78,0,0.312318,"• CL-L2 represents minimizing the L2 distance between token representations (Eq. 5). • CL-KL represents minimizing the KL divergence (Eq. 8) between CRF output distributions. Besides, we also compare our approaches with previous state-of-the-art approaches over entity-level F1 scores6 . During the evaluation, our approaches are evaluated using inputs without external contexts (W / O C ONTEXT) and inputs with them (W / C ONTEXT). We report the results averaged over 5 runs in our experiments. The results are listed in 6 We do not compare the results from previous work such as Yu et al. (2020); Luoma and Pyysalo (2020); Yamada et al. (2020) that utilizes the document-level contexts in CoNLL-03 NER here. We conduct a comparison with these approaches in Appendix A. Table 27 . With the external contexts, our models with CL outperform previous state-of-the-art approaches on most of the datasets. Our approaches significantly outperform the baseline that is trained without external contexts with only one exception. Comparing with LUKE, our approaches and our baseline outperform LUKE in all the cases. The possible reason is that LUKE is pretrained only using long word sequences, which makes the model prone to fail"
2021.acl-long.142,P16-1101,0,0.175555,"nsformer-based model together with the input sentence. Finally, we calculate the negative likelihood loss LNLL and LNLL-EXT together with the CL loss (either LCL-L2 or LCL-KL ). ternal contexts: ˜ = [sep_token; x ˆ1; · · · ; x ˆl] x where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, “[SEP]” in BERT). 2.2 NER Model We solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2019). In the sequence labeling model, the input sentence x is fed into a transformer-based pretrained contextual embeddings model to get the token representations {v1 , · · · , vn } by vi =embedi (x). The token representations are fed into a CRF layer to get the conditional probability pθ (y|x): ψ(y 0 , y, vi ) = exp(WyT vi + by0 ,y ) (1) n Q ψ(yi−1 , yi , vi ) i=1 pθ (y|x) = n P Q 0 , y0 , v ) ψ(yi−1 i i y 0 ∈Y(x) i=1 where ψ is the potential function and θ represents the model parameters. Y(x) denotes the set of all possible label sequences given x. y0 is defined to be a spe"
2021.acl-long.142,2020.emnlp-demos.2,0,0.0268364,"Missing"
2021.acl-long.142,2020.emnlp-main.107,0,0.0631329,"Missing"
2021.acl-long.142,D19-6125,0,0.0628019,"ulder, 2003) dataset and CoNLL++ (Wang et al., 2019) dataset. The CoNLL-03 dataset is the most popular dataset for NER. CoNLL++ is a revision of the CoNLL-03 datasets. Wang et al. (2019) fixed annotation errors on the test set by professional annotators and improved the quality of the training data through their CrossWeigh approach. We use the standard dataset split for these datasets. • Biomedical: We use BC5CDR (Li et al., 2016) and NCBI-disease (Do˘gan et al., 2014) datasets, which are two popular biomedical NER datasets. We merge the training and development data as training set following Nooralahzadeh et al. (2019). • Science and Technology: We use CBS SciTech News dataset collected by Jia et al. (2019). The dataset only contains the test set with the same label set as the CoNLL-03 dataset. We use the dataset to evaluate the effectiveness of crossdomain transferability from the news domain. • E-commerce: We collect and annotate an internal dataset from one anonymous E-commerce website. The dataset contains 25 named entity labels for goods in short texts. We also collect 300,000 unlabeled sentences for semi-supervised training. We show the statistics of the datasets in Table 1. Annotations of the E-comme"
2021.acl-long.142,N18-1202,0,0.0156664,"ibuster and confirm Neil Gorsuch to the Supreme Court . Senate Republicans deployed the nuclear option on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump s nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at htt"
2021.acl-long.142,W16-3919,0,0.0224422,"Missing"
2021.acl-long.142,W02-2024,0,0.403859,"Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018, 2019; Wang et al., 2020b). Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT (Devlin et al., 2019), Flair (Akbik et al., 2018) and LUKE (Yamada et al., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve th"
2021.acl-long.142,2020.acl-main.304,1,0.865963,"(θ) = LCL-L2 (θ) + LCL-KL (θ) in Eq. 9). Results in Table 7 show that the external context can help to improve the accuracy even when the NER model is trained without the contexts. However, when the model is trained with the external contexts, the accuracy of the model Related Work Named Entity Recognition Named Entity Recognition (Sundheim, 1995) has been studied for decades. Most of the work takes NER as a sequence labeling problem and applies the linear-chain CRF (Lafferty et al., 2001) to achieve state-of-the-art accuracy (Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018, 2019; Wang et al., 2020b). Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT (Devlin et al., 2019), Flair (Akbik et al., 2018) and LUKE (Yamada et al., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieva"
2021.acl-long.142,2021.acl-long.206,1,0.769048,"Missing"
2021.acl-long.142,2020.findings-emnlp.356,1,0.898596,"(θ) = LCL-L2 (θ) + LCL-KL (θ) in Eq. 9). Results in Table 7 show that the external context can help to improve the accuracy even when the NER model is trained without the contexts. However, when the model is trained with the external contexts, the accuracy of the model Related Work Named Entity Recognition Named Entity Recognition (Sundheim, 1995) has been studied for decades. Most of the work takes NER as a sequence labeling problem and applies the linear-chain CRF (Lafferty et al., 2001) to achieve state-of-the-art accuracy (Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018, 2019; Wang et al., 2020b). Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT (Devlin et al., 2019), Flair (Akbik et al., 2018) and LUKE (Yamada et al., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieva"
2021.acl-long.142,2021.acl-long.46,1,0.801203,"Missing"
2021.acl-long.142,D19-1519,0,0.105801,"‡ Input Sentence: in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users’ search queries, tweets and short comments in various domains such as social media and E-commerce domains. When professional annotators annotate ambiguous named entities in such cases, they usually rely on domain knowledge for disambiguation. This kind of knowledge can often be found through a search engine. Moreover, when the annotators are not sure about a certain entity, they are usually encouraged to find related knowledge through a search engine (Wang et al., 2019). Therefore, we believe that NER models can benefit from such a process as well. In this paper, we propose to improve NER models by retrieving texts related to the input sentence by an off-the-shelf search engine. We re-rank the retrieved texts according to their semantic relevance to the input sentence and select several top-ranking texts as the external contexts. Consequently, we concatenate the input sentence and external contexts together as a new retrieval-based input view and feed it to the pretrained contextual embedding 1800 Proceedings of the 59th Annual Meeting of the Association for"
2021.acl-long.142,2020.acl-main.144,0,0.18718,"n of different approaches in transfer learning. The models are trained on the CoNLL-03 dataset. Approach CL-L2 CL-KL CL–L2 +S EMI CL-KL+S EMI SE 59.95 61.79 4.1 Comparison of Re-ranking Approaches Various re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is reranking through a fuzzy match score. The approach has been widely applied in a lot of previous work (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020). The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models (W / C ONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT17 in Table 5. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p < 0.05). The possible reason might be that the tf-idf weighting gives high weight"
2021.acl-long.142,2020.emnlp-main.523,0,0.252158,"Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/CLNER. ‡ Input Sentence: in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users’ search queries, tweets and short"
2021.acl-long.142,2020.acl-main.577,0,0.336047,"nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/CLNER. ‡ Input Sentence: in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users’ search quer"
2021.acl-long.142,N18-1120,0,0.146753,"core. BS: BERTScore. Table 3: A comparison of different approaches in transfer learning. The models are trained on the CoNLL-03 dataset. Approach CL-L2 CL-KL CL–L2 +S EMI CL-KL+S EMI SE 59.95 61.79 4.1 Comparison of Re-ranking Approaches Various re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is reranking through a fuzzy match score. The approach has been widely applied in a lot of previous work (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020). The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models (W / C ONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT17 in Table 5. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p < 0.05). The possible reason might be that"
2021.acl-long.142,P19-1336,0,0.0214626,"Missing"
2021.acl-long.142,W18-5713,0,0.0243972,"l., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al.,"
2021.acl-long.206,N19-1078,0,0.0939277,"Missing"
2021.acl-long.206,J88-1003,0,0.685257,"CE can find a strong word representation on a single GPU with only a few GPU-hours for structured prediction tasks. In comparison, a lot of NAS approaches require dozens or even thousands of GPU-hours to search for good neural architectures for their corresponding tasks. Empirical results show that ACE outperforms strong baselines. Furthermore, when ACE is applied to concatenate pretrained contextualized embeddings fine-tuned on specific tasks, we can achieve state-of-the-art accuracy on 6 structured prediction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are t"
2021.acl-long.206,C18-1139,0,0.623884,"rms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.1 1 Introduction Recent developments on pretrained contextualized embeddings have significantly improved the performance of structured prediction tasks in natural ∗ Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/ACE. ‡ language processing. Approaches based on contextualized embeddings, such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-increasing number of embeddi"
2021.acl-long.206,N19-1423,0,0.618458,"hieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.1 1 Introduction Recent developments on pretrained contextualized embeddings have significantly improved the performance of structured prediction tasks in natural ∗ Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/ACE. ‡ language processing. Approaches based on contextualized embeddings, such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-increasing number of embedding learning methods that ope"
2021.acl-long.206,D19-1539,0,0.111357,"Missing"
2021.acl-long.206,Q17-1010,0,0.0604138,"diction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embeddings, ELMo (Peters et al., 2018), a pretrained contextualized word embedding generated with multiple Bidirectional LSTM layers, significantly outperforms previous state-of-the-art approaches on several NLP tasks. Following this idea, Akbik et al. (2018) proposed Flair embeddings, which is a kind of contextualized character embeddings and"
2021.acl-long.206,W06-2920,0,0.0403544,"ur search design can usually lead to better results compared to both of the baselines. 4.3.2 Comparison With State-of-the-Art approaches As we have shown, ACE has an advantage in searching for better embedding concatenations. We further show that ACE is competitive or even stronger than state-of-the-art approaches. We additionally use XLNet (Yang et al., 2019) and RoBERTa as the candidates of ACE. In some tasks, we have several additional settings to better compare with previous work. In NER, we also conduct a comparison on the revised version of German datasets in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). Recent work such as Yu et al. (2020) and Yamada et al. (2020) utilizes document contexts in the datasets. We follow their work and extract document embeddings for the transformer-based embeddings. Specifically, we follow the fine-tune process of Yamada et al. (2020) to fine-tune the transformer-based embeddings over the document except for BERT and M-BERT embeddings. For BERT and M-BERT, we follow the document extraction process of Yu et al. (2020) because we find that the model with such document embeddings is significantly stronger than the model trained with the fine-tuning process of Yam"
2021.acl-long.206,2020.acl-main.777,0,0.0351125,"Missing"
2021.acl-long.206,D18-1217,0,0.10887,"Missing"
2021.acl-long.206,2020.acl-main.607,1,0.770928,"Missing"
2021.acl-long.206,D16-1139,0,0.0528067,"also find that the PTB dataset used by Mrini et al. (2020) is not identical to the dataset in previous work such as Zhang et al. (2020) and Wang and Tu (2020). ‡ : For reference, we confirmed with the authors of He and Choi (2020) that they used a different data pre-processing script with previous work. . ACE No discount (Eq. 5) Simple (Eq. 4) D EV 93.18 92.98 92.89 T EST 90.00 89.90 89.82 embeddings that are not very useful in the concatenation. Moreover, ACE models can be used to guide the training of weaker models through techniques such as knowledge distillation in structured prediction (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a, 2021b), leading to models that are both stronger and faster. Table 5: Comparison of reward functions. NER POS AE CHK All Random ACE All+Weight Ensemble Ensembledev Ensembletest 92.4 92.6 93.0 92.7 92.2 92.2 92.7 90.6 91.3 91.7 90.4 90.6 90.8 91.4 73.2 74.7 75.6 73.7 68.1 70.2 73.9 96.7 96.7 96.8 96.7 96.5 96.7 96.7 DP SDP UAS LAS ID OOD 96.7 95.1 94.3 90.8 96.8 95.2 94.4 90.8 96.9 95.3 94.5 90.9 96.7 95.1 94.3 90.7 96.1 94.3 94.1 90.3 96.8 95.2 94.3 90.7 96.8 95.2 94.4 90.8 7 Table 6: A comparison among All, Random, ACE, All+Weight and Ensemble. CHK:"
2021.acl-long.206,D19-1279,0,0.0134518,"e each edge represents the inputs and outputs between these nodes. In ACE, we represent each 2645 embedding candidate as a node. The input to the nodes is the input sentence x, and the outputs are the embeddings v l . Since we concatenate the embeddings as the word representation of the task model, there is no connection between nodes in our search space. Therefore, the search space can be significantly reduced. For each node, there are a lot of options to extract word features. Taking BERT embeddings as an example, Devlin et al. (2019) concatenated the last four layers as word features while Kondratyuk and Straka (2019) applied a weighted sum of all twelve layers. However, the empirical results (Devlin et al., 2019) do not show a significant difference in accuracy. We follow the typical usage for each embedding to further reduce the search space. As a result, each embedding only has a fixed operation and the resulting search space contains 2L −1 possible combinations of nodes. In NAS, weight sharing (Pham et al., 2018a) shares the weight of structures in training different neural architectures to reduce the training cost. In comparison, we fixed the weight of pretrained embedding candidates in ACE except for"
2021.acl-long.206,D16-1180,0,0.0966408,"PTB dataset used by Mrini et al. (2020) is not identical to the dataset in previous work such as Zhang et al. (2020) and Wang and Tu (2020). ‡ : For reference, we confirmed with the authors of He and Choi (2020) that they used a different data pre-processing script with previous work. . ACE No discount (Eq. 5) Simple (Eq. 4) D EV 93.18 92.98 92.89 T EST 90.00 89.90 89.82 embeddings that are not very useful in the concatenation. Moreover, ACE models can be used to guide the training of weaker models through techniques such as knowledge distillation in structured prediction (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a, 2021b), leading to models that are both stronger and faster. Table 5: Comparison of reward functions. NER POS AE CHK All Random ACE All+Weight Ensemble Ensembledev Ensembletest 92.4 92.6 93.0 92.7 92.2 92.2 92.7 90.6 91.3 91.7 90.4 90.6 90.8 91.4 73.2 74.7 75.6 73.7 68.1 70.2 73.9 96.7 96.7 96.8 96.7 96.5 96.7 96.7 DP SDP UAS LAS ID OOD 96.7 95.1 94.3 90.8 96.8 95.2 94.4 90.8 96.9 95.3 94.5 90.9 96.7 95.1 94.3 90.7 96.1 94.3 94.1 90.3 96.8 95.2 94.3 90.7 96.8 95.2 94.4 90.8 7 Table 6: A comparison among All, Random, ACE, All+Weight and Ensemble. CHK: chunking. performs al"
2021.acl-long.206,N16-1030,0,0.474791,"pendency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embeddings, ELMo (Peters et al., 2018), a pretrained contextualized word embedding generated with multiple Bidirectional LSTM layers, significantly outperforms previous state-of-the-art approaches on several NLP tasks. Following this idea, Akbik et al. (2018) proposed Flair embeddings, which is a kind of contextualized character embeddings and achieved strong performance in sequence labeling tasks. Recently, Devlin et al. (2019) proposed BERT, which encodes contextualized sub-word information by Transformers (Vaswani et al., 2017) and s"
2021.acl-long.206,D19-5505,0,0.0269117,"Missing"
2021.acl-long.206,N18-1088,0,0.0142024,"e hyper-parameter of each structure and decide the input order of each structure. Evolutionary algorithms have been applied to architecture search for many decades (Miller et al., 1989; Angeline et al., 1994; Stanley and Miikkulainen, 2002; Floreano et al., 2008; Jozefowicz et al., 2015). The algorithm repeatedly generates new populations through recombination and mutation operations and selects survivors through competing among the population. Recent work with evolutionary algorithms differ in the method on parent/survivor selection and population generation. For example, Real et al. (2017), Liu et al. (2018a), Wistuba (2018) and Real et al. (2019) applied tournament selection (Goldberg and Deb, 1991) for the parent selection while Xie and Yuille (2017) keeps all parents. Suganuma et al. (2017) and Elsken et al. (2018) chose the best model while Real et al. (2019) chose several latest models as survivors. 3 Given an embedding concatenation generated from the controller, the task model is trained over the task data and returns a reward to the controller. The controller receives the reward to update its parameter and samples a new embedding concatenation for the task model. Figure 1 shows the gener"
2021.acl-long.206,2020.emnlp-demos.2,0,0.0370801,"the controller for 50 steps. Table 5 shows that both the discount factor and the binary vector |at − ai |for the task are helpful in both development and test datasets. 4 Please refer to Appendix for more details about the embeddings. 5 We compare ACE with other fine-tuned embeddings in Appendix. 2649 Baevski et al. (2019) Straková et al. (2019) Yu et al. (2020) Yamada et al. (2020) XLM-R+Fine-tune ACE+Fine-tune de 85.1 86.4 87.7 88.3 de06 90.3 91.4 91.7 NER en 93.5 93.4 93.5 94.3 94.1 94.6 es 88.8 90.3 89.3 95.9 nl 92.7 93.7 95.3 95.7 Owoputi et al. (2013) Gui et al. (2017) Gui et al. (2018) Nguyen et al. (2020) XLM-R+Fine-tune ACE+Fine-tune Ritter 90.4 90.9 91.2 90.1 92.3 93.4 POS ARK 93.2 92.4 94.1 93.7 94.4 TB-v2 94.6 92.8 95.2 95.4 95.8 Table 2: Comparison with state-of-the-art approaches in NER and POS tagging. † : Models are trained on both train and development set. C HUNK CoNLL 2000 Akbik et al. (2018) Clark et al. (2018) Liu et al. (2019b) Chen et al. (2020) XLM-R+Fine-tune ACE+Fine-tune AE 14Lap 14Res 15Res 16Res Xu et al. (2018)† Xu et al. (2019) Wang et al. (2020a) Wei et al. (2020) XLM-R+Fine-tune ACE+Fine-tune 96.7 97.0 97.3 95.5 97.0 97.3 84.2 84.3 82.7 85.9 87.4 84.6 87.1 90.5 92.0 72"
2021.acl-long.206,S15-2153,0,0.0506143,"Missing"
2021.acl-long.206,S14-2008,0,0.0159013,"thousands of GPU-hours to search for good neural architectures for their corresponding tasks. Empirical results show that ACE outperforms strong baselines. Furthermore, when ACE is applied to concatenate pretrained contextualized embeddings fine-tuned on specific tasks, we can achieve state-of-the-art accuracy on 6 structured prediction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embedding"
2021.acl-long.206,N13-1039,0,0.0434432,"Missing"
2021.acl-long.206,D14-1162,0,0.0909253,"-of-the-art accuracy on 6 structured prediction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embeddings, ELMo (Peters et al., 2018), a pretrained contextualized word embedding generated with multiple Bidirectional LSTM layers, significantly outperforms previous state-of-the-art approaches on several NLP tasks. Following this idea, Akbik et al. (2018) proposed Flair embeddings, which is a kind of"
2021.acl-long.206,N18-1202,0,0.0524695,"ow that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.1 1 Introduction Recent developments on pretrained contextualized embeddings have significantly improved the performance of structured prediction tasks in natural ∗ Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/ACE. ‡ language processing. Approaches based on contextualized embeddings, such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-"
2021.acl-long.206,S15-2082,0,0.0548003,"Missing"
2021.acl-long.206,S14-2004,0,0.0702456,"Missing"
2021.acl-long.206,2020.acl-main.304,1,0.783349,", such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-increasing number of embedding learning methods that operate on different granularities (e.g., word, subword, or character level) and with different model architectures, choosing the best embeddings to concatenate for a specific task becomes non-trivial, and exploring all possible concatenations can be prohibitively demanding in computing resources. Neural architecture search (NAS) is an active area of research in deep learning to automatically search for better model architectures, and has achieved state-of-the-art performance on various tasks in computer vision, such as im"
2021.acl-long.207,P19-1299,0,0.110715,"situation where some source languages are not as similar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A). To tackle this challenging problem, most of the previous works do majority voting (Plank and Agi´c, 2018) and truth inference on hard predictions of multiple sources (Rahimi et al., 2019). To better incorporate target language information, some recent works train a new model on the target unlabeled data with hard/soft predictions from multiple source models, such as mixture-of-experts model (Chen et al., 2019) and knowledge distillation (KD) (Wu et al., 2020), and assign weights to multiple sources based on language similarity. However, these similaritybased approaches are heuristic-based, and cannot well learn the confidence level of multiple source models. In this paper, we propose to leverage a small number of labeled target data to selectively transfer the knowledge from multiple source models. 2661 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2661–2674 August 1–6, 2021."
2021.acl-long.207,D18-1217,0,0.335977,"a new task-specific model in the target language. Both the aggregation model and target task-specific model can map the inputs to the structured outputs but there exists a tradeoff. The aggregation model generally has strong cross-lingual ability since source models are firstly well trained1 , but has lower flexibility since source models are usually frozen. Instead, the target taskspecific model tends to be more flexible and has strong capacity but has poor performance since the model is easily over-fitted on the small training sample. Inspired by previous work on multi/cross-view learning (Clark et al., 2018; Jiang et al., 2019; Fei and Li, 2020), we regard the aggregation model (aggregated source view) and the target taskspecific model (target view) as two views since they both can map the input sentence to structured outputs. We propose a novel multi-view framework to achieve a good trade-off between the two views. To capture the diverse strength and weakness of multiple source models, we propose three approaches to obtain the aggregated source view from language/sentence/sub-structure level in a coarse-to-fine manner. By encouraging two views to influence each other, the proposed framework can"
2021.acl-long.207,2020.acl-main.747,0,0.194106,"Missing"
2021.acl-long.207,J88-1003,0,0.0882324,"ews to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data. 1 Introduction Structured prediction is the task of mapping input sentences to structured outputs. It is a fundamental task in natural language processing and has many applications, i.e., sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020). ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. To achieve strong performance, structured prediction models mostly require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019;"
2021.acl-long.207,D17-1005,0,0.0176857,"base model for all approaches. We run each approach five times and report the averaged accuracy for POS tagging, f1-score for NER, and unlabelled attachment score (UAS) and labeled attachment score (LAS) for dependency parsing. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability di"
2021.acl-long.207,D15-1166,0,0.0559339,"el Aggregation We simply introduce a trainable probability vector αlang , which is depicted on the bottom right part of 2663 the Figure 1. The final output distribution of the aggregated source view can be computed as, pS (y|x) = K X (1) (k) αlang k=1 · αsub (xi ) = ] (t) Softmax(hi WKTi ) Then the aggregation distribution becomes, pS (y|x) = n X K Y i=1 k=1 (k) αsub (xi ) · p(k) s (yi |x) In this approach, our target model acts as a selector to dynamically assess the multiple source models on sub-structure level. Sentence-level Aggregation In this section, we leverage an attention mechanism (Luong et al., 2015; Vaswani et al., 2017) to learn the weight of each source model on an input sentence, as shown on the top right part of Figure 1. Firstly, we use the internal states of the [CLS] (t) token as sentence representation. Secondly, h0 from the target model T is used as a query to attend (k) h0 from the k-th source model Sk to produce the probabilities αsent (x) ∈ RK . (1) (t) αsent (x) = Softmax(h0 WKT0 ) where K0 is the concatenation of sentence representations from K source models, and W ∈ Rd×d is the bilinear weight matrix. Then the probabilities are utilized to compute the aggregation distribu"
2021.acl-long.207,D18-1061,0,0.0650358,"Missing"
2021.acl-long.207,P11-2052,0,0.097664,"Missing"
2021.acl-long.207,P19-1015,0,0.453502,"require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones. Existing works can be categorized into two types: single-source transfer and multi-source transfer. The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics, i.e., domain, word order, capitalization, and script style. However, in practice, we are more likely to encounter the situation where some source languages are not as similar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A). To tackle this challenging problem, most of the previous works do majority voting (Plank and Agi´c, 2018) and truth inference on hard predictions of multiple sources (Rahi"
2021.acl-long.207,D17-1038,0,0.0266652,"d weakness of different source models (see Sec.1 for more discussion.). Sub-structure-level Aggregation We further propose a fine-grained aggregation approach on sub-structure level, which is also based on the attention mechanism. As shown in the left part of Figure 1, for token xi in a given sentence x, (t) we use its representation hi as the query to attend the corresponding representation from each source 3 We also try many metrics of measuring the similarity between two probability distributions, e.g., mean squared error (MSE) (Wu et al., 2020), Cosine, and Jensen-Shannon divergence (JS) (Ruder and Plank, 2017), and we find KL perform best. 2664 2. KD assigns equal importance to multiple source models, which can be seen as a fixed uniform vector in our language-level aggregation approach. 3. Besides language-level aggregation, we propose two fine-grained aggregation strategies to dynamically balance the information from source models. 4. To achieve the previously described goal, our approach has trainable parameters in the aggregation component and our multi-view learning framework can jointly learn the parameters of two views. 2.5 Training and Inference Strategies Following previous work on cross-l"
2021.acl-long.207,P18-1096,0,0.552775,"ompare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability distribution of source models over the sub-structure level (DT-max). 3) evaluating each source model and voting on the sub-structure level (DT-vote). We also provide the maximal results of DT on language level (DT-Max(lang)) 6 . Hard-KD The hard knowledge distillation approaches first pred"
2021.acl-long.207,D18-1548,0,0.053178,"Missing"
2021.acl-long.207,W02-2024,0,0.719114,"2020), we conduct the experiments in a leaveone-out setting in which we hold out one language as the target language and the others as the source languages. To simulate the low-resources scenario, for each training set in a specific target language, we randomly select fifty sentences 4 with the gold annotations and discard the annotations of the remaining sentences to construct the training set. We randomly select six languages from Universal Dependencies Treebanks (v2.2)5 for dependency parsing and POS tagging tasks. We use the datasets from CoNLL 2002 and CoNLL 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for NER tasks. We utilize the base cased multilingual BERT (Devlin et al., 2019) as base model for all approaches. We run each approach five times and report the averaged accuracy for POS tagging, f1-score for NER, and unlabelled attachment score (UAS) and labeled attachment score (LAS) for dependency parsing. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-thear"
2021.acl-long.207,Q14-1005,0,0.0892508,"has many applications, i.e., sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020). ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. To achieve strong performance, structured prediction models mostly require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones. Existing works can be categorized into two types: single-source transfer and multi-source transfer. The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics, i.e., domain, word ord"
2021.acl-long.207,2021.acl-long.142,1,0.835839,"Missing"
2021.acl-long.207,2020.acl-main.581,0,0.0860103,"milar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A). To tackle this challenging problem, most of the previous works do majority voting (Plank and Agi´c, 2018) and truth inference on hard predictions of multiple sources (Rahimi et al., 2019). To better incorporate target language information, some recent works train a new model on the target unlabeled data with hard/soft predictions from multiple source models, such as mixture-of-experts model (Chen et al., 2019) and knowledge distillation (KD) (Wu et al., 2020), and assign weights to multiple sources based on language similarity. However, these similaritybased approaches are heuristic-based, and cannot well learn the confidence level of multiple source models. In this paper, we propose to leverage a small number of labeled target data to selectively transfer the knowledge from multiple source models. 2661 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2661–2674 August 1–6, 2021. ©2021 Association for Computational Linguistics In"
2021.acl-long.207,D19-1077,0,0.0368048,"Missing"
2021.acl-long.207,2020.repl4nlp-1.16,0,0.0169926,"o it, we focus on the cross-lingual scenario and our two views are a target task-specific model and the aggregation of multiple pre-trained source models. Contextual Multilingual Language Model Trained on massive unlabeled data of hundreds of monolingual corpus, the contextual multilingual models (Devlin et al., 2019; Conneau et al., 2020) learn common representations for multiple languages. Though cross-lingual transfer learning significantly benefits from these models (Pires et al., 2019; Wu and Dredze, 2019b), large gaps still remain between low and high-resources setups (Hu et al., 2020a; Wu and Dredze, 2020). 6 Conclusion We propose a novel multi-view framework to selectively transfer knowledge from multiple sources by utilizing a small amount of labeled dataset. Experimental results show that our approaches achieve state-of-the-art performances on all tasks. Moreover, even compared to approaches with extra resources like source language data, our substructure-level approach still shows significant improvements. Acknowledgement This work was supported by the National Natural Science Foundation of China (61976139) and by Alibaba Group through Alibaba Innovative Research Program. We thank Yuting Zh"
2021.acl-long.207,P95-1026,0,0.702103,"g. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability distribution of source models over the sub-structure level (DT-max). 3) evaluating each source model and voting on the sub-structure level (DT-vote). We also provide the maximal results of DT on language level (DT-Max(lang))"
2021.acl-long.207,N01-1026,0,0.292596,"language processing and has many applications, i.e., sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020). ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. To achieve strong performance, structured prediction models mostly require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones. Existing works can be categorized into two types: single-source transfer and multi-source transfer. The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics"
2021.acl-long.207,N18-1089,0,0.0259681,"hment score (UAS) and labeled attachment score (LAS) for dependency parsing. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability distribution of source models over the sub-structure level (DT-max). 3) evaluating each source model and voting on the sub-structure level (DT-vote). We also"
2021.acl-long.269,I17-1099,0,0.0176651,"s shows the Joint model can mimic human supporters’ behaviors in strategy utilization. We believe our work will facilitate research on more data-driven approaches to build dialog systems capable of providing effective emotional support. 2 2.1 Related Work Emotional & Empathetic Conversation Figure 2 intuitively shows the relationships among In this paper, we define the task of Emotional ESC, emotional conversation, and empathetic conSupport Conversation (ESC), aiming to provide versation. Emotion has been shown to be impor3470 tant for building more engaging dialog systems (Zhou et al., 2018; Li et al., 2017; Zhou and Wang, 2018; Huber et al., 2018; Huang et al., 2020). As a notable work of emotional conversation, Zhou et al. (2018) propose Emotional Chatting Machine (ECM) to generate emotional responses given a pre-specified emotion. This task is required to accurately express (designated or not) emotions in generated responses. While ES may include expressing emotions, such as happiness or sadness, it has a broader aim of reducing the user’s emotional distress through the utilization of proper support skills, which is fundamentally different from emotional chatting. Emotional chatting is merely"
2021.acl-long.269,W04-1013,0,0.0229567,"Missing"
2021.acl-long.269,2020.acl-demos.30,0,0.167107,"en used to provide effective ES (Hill, 2009). Figure 2 illustrates the relationship between the three tasks and we provide further discussion in Section 2.1. Second, people are not naturally good at being supportive, so guidelines have been developed to train humans how to be more supportive. Without trained individuals, existing online conversation datasets(Sharma et al., 2020a; Rashkin et al., 2019; Zhong et al., 2020; Sun et al., 2021) do not naturally exhibit examples or elements of supportive conversations. As a result, data-driven models that leverage such corpora (Radford et al., 2019; Zhang et al., 2020; Roller et al., 2020) are limited in their ability to explicitly learn how to utilize support skills and thus provide effective ES. Empathetic Responding Emotional Chatting Accurately express emotions in responses Understand users&apos; feelings and reply accordingly Emotional Support Conversation Reduce users&apos; emotional distress and help them work through the challenges Figure 2: Emotional support conversations (our work) can include elements of emotional chatting (Zhou et al., 2018) and empathetic responding(Rashkin et al., 2019). support through social interactions (like the interactions betwee"
2021.acl-long.269,2021.findings-acl.72,1,0.739112,"responses. While ES may include expressing emotions, such as happiness or sadness, it has a broader aim of reducing the user’s emotional distress through the utilization of proper support skills, which is fundamentally different from emotional chatting. Emotional chatting is merely a basic quality of dialog systems, while ES is a more high-level and complex ability that dialog systems are expected to be equipped with. Another related task is empathetic responding (Rashkin et al., 2019; Lin et al., 2019; Majumder et al., 2020; Zandie and Mahoor, 2020; Sharma et al., 2020a; Zhong et al., 2020; Zheng et al., 2021), which aims at understanding users’ feelings and then replying accordingly. For instance, Rashkin et al. (2019) argued that dialog models can generate more empathetic responses by recognizing the interlocutor’s feelings. Effective ES naturally requires expressing empathy according to the help-seeker’s experiences and feelings, as shown in our proposed Emotional Support Framework (Section 3.2, Figure 3). Hence, empathetic responding is only one of the necessary components of emotional support. In addition to empathetic responding, an emotional support conversation needs to explore the users’ p"
2021.acl-long.269,2020.emnlp-main.531,0,0.274036,"pathetic responding (Rashkin et al., 2019) return messages that are examples of emotion or empathy and are thus limited in functionality, as they are not capable of many other skills that are often used to provide effective ES (Hill, 2009). Figure 2 illustrates the relationship between the three tasks and we provide further discussion in Section 2.1. Second, people are not naturally good at being supportive, so guidelines have been developed to train humans how to be more supportive. Without trained individuals, existing online conversation datasets(Sharma et al., 2020a; Rashkin et al., 2019; Zhong et al., 2020; Sun et al., 2021) do not naturally exhibit examples or elements of supportive conversations. As a result, data-driven models that leverage such corpora (Radford et al., 2019; Zhang et al., 2020; Roller et al., 2020) are limited in their ability to explicitly learn how to utilize support skills and thus provide effective ES. Empathetic Responding Emotional Chatting Accurately express emotions in responses Understand users&apos; feelings and reply accordingly Emotional Support Conversation Reduce users&apos; emotional distress and help them work through the challenges Figure 2: Emotional support convers"
2021.acl-long.269,2020.cl-1.2,0,0.051659,"Missing"
2021.acl-long.269,P18-1104,0,0.0205011,"model can mimic human supporters’ behaviors in strategy utilization. We believe our work will facilitate research on more data-driven approaches to build dialog systems capable of providing effective emotional support. 2 2.1 Related Work Emotional & Empathetic Conversation Figure 2 intuitively shows the relationships among In this paper, we define the task of Emotional ESC, emotional conversation, and empathetic conSupport Conversation (ESC), aiming to provide versation. Emotion has been shown to be impor3470 tant for building more engaging dialog systems (Zhou et al., 2018; Li et al., 2017; Zhou and Wang, 2018; Huber et al., 2018; Huang et al., 2020). As a notable work of emotional conversation, Zhou et al. (2018) propose Emotional Chatting Machine (ECM) to generate emotional responses given a pre-specified emotion. This task is required to accurately express (designated or not) emotions in generated responses. While ES may include expressing emotions, such as happiness or sadness, it has a broader aim of reducing the user’s emotional distress through the utilization of proper support skills, which is fundamentally different from emotional chatting. Emotional chatting is merely a basic quality of d"
2021.acl-long.272,N19-1125,0,0.0367524,"Missing"
2021.acl-long.272,2020.findings-emnlp.352,0,0.0437804,"Missing"
2021.acl-long.272,2020.acl-main.185,0,0.0563013,"Missing"
2021.acl-long.272,D16-1139,0,0.0622703,"Missing"
2021.acl-long.272,P17-4012,0,0.081538,"Missing"
2021.acl-long.272,W19-8609,0,0.0396998,"Missing"
2021.acl-long.272,N16-1014,0,0.0681581,"Missing"
2021.acl-long.272,2020.acl-main.428,0,0.0704239,"Missing"
2021.acl-long.272,I17-1099,0,0.039754,"Missing"
2021.acl-long.272,2020.findings-emnlp.22,0,0.0755207,"Missing"
2021.acl-long.272,P17-1061,0,0.0321126,"Missing"
2021.acl-long.380,N10-1083,0,0.12678,"Missing"
2021.acl-long.380,P19-1299,0,0.0180284,"asible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the"
2021.acl-long.380,2020.acl-main.747,0,0.0968285,"Missing"
2021.acl-long.380,J88-1003,0,0.0958873,"dels and the true labels. By making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions fro"
2021.acl-long.380,N19-1423,0,0.125683,"gorithm, which alternates between updating a posterior distribution and optimizing model parameters. To empirically evaluate our proposed approaches, we extensively conduct experiments on four sequence labeling tasks of twenty-one datasets. Our two proposed approaches, especially the latent variable model, outperform several strong baselines. 2 Background 2.1 Sequence Labeling Given a sentence x = x1 , . . . , xn , its word representations are extracted from the pre-trained embeddings and passed into a sentence encoder such as BiLSTM, Convolutional Neural Networks (CNN) and multilingual BERT (Devlin et al., 2019) to obtain a sequence of contextual features. Without considering the dependencies between predicted labels, the Softmax layer computes the conditional probability as follows, Pθ (y|x) = n Y Pθ (yi |x) i=1 Given the gold sequence y∗ = y1∗ , . . . , yn∗ , the general training objective is to minimize the negative log-likelihood of the sequence, J (θθ ) = − log Pθ (y∗ |x) = − n X J (θθ ) = − log Pθ (ˆ y|x) = − Cross-Lingual/Domain Transfer Supervised models fail when labeled data are absent. Learning from imperfect predictions from rich-resource sources is a viable approach to tackle the problem"
2021.acl-long.380,P19-1266,0,0.0448562,"Missing"
2021.acl-long.380,D18-1498,0,0.0522747,"Missing"
2021.acl-long.380,N06-2015,0,0.188956,"tagging task, we use Universal Dependencies treebanks (UD) v2.44 and randomly select five anguages together with the English dataset. The whole datasets are English (En), Catalan (Ca), Indonesian (Id), Hindi (Hi), Finnish (Fi), and Russian (Ru). For the Aspect Extraction task, we select the restaurant domain over subtask 1 in the SemEval-2016 shared task (Pontiki et al., 2016). For the NER task, we evaluate our models on the CoNLL 2002 and 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Cross-Domain Sequence Labeling We use English portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb). More details can be found in the Appendix A.1. 4.2 Approaches Single-source Setup The following approaches are applicable for single-source setup, • DT: we use the pre-trained source model to directly predict the pseudo labels on the target unlabeled data. • Hard: we use the pseudo labels from DT on the target unlabeled data to train a new model. Inference For inference, we use Q(y) to obtain ypred 4 lowing Wu et al. (2020), the source model are previously trained on its"
2021.acl-long.380,2020.findings-emnlp.236,1,0.851087,"glish portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb). More details can be found in the Appendix A.1. 4.2 Approaches Single-source Setup The following approaches are applicable for single-source setup, • DT: we use the pre-trained source model to directly predict the pseudo labels on the target unlabeled data. • Hard: we use the pseudo labels from DT on the target unlabeled data to train a new model. Inference For inference, we use Q(y) to obtain ypred 4 lowing Wu et al. (2020), the source model are previously trained on its corresponding training data. We use the BIO scheme for CoNLL and OntoNotes NER tasks and Aspect Extraction. We run each model three times and report the average accuracy for the POS tagging task and F1-score for the other tasks. 2, Multi-source Setup The following approaches are applicable for multi-source setup, Pφ (ˆ y(u) |y, u) u=1 Experiments We use the multilingual BERT (mBERT) as our word representations3 as the sentence encoder. Fol2 Another choice is to use Pθ (y|x), however, we found that utilizing Q(y) generally achieves better perform"
2021.acl-long.380,2021.acl-long.207,1,0.894793,"2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source lan"
2021.acl-long.380,N19-1383,0,0.0949712,", 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled"
2021.acl-long.380,D17-1302,0,0.0180989,"s widely studied (Steedman et al., 6 The CoNLL NER datasets have 11 labels (9 entity labels, a padding label and an ending label). 4916 F1 De Nl Es 74.1 80.6 76.1 74.0 80.5 75.9 73.9 80.4 75.8 73.7 80.2 75.6 73.6 2 3 4 10 80.1 2 3 4 τ 10 τ 75.4 2 3 4 10 τ Figure 3: The performance of MRT approach in single-source setup with soft predictions on three NER datasets by varying different τ . 2003). Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks. Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the strong cross-lingual ability of the contextual multilingual embeddings. 7 Conclus"
2021.acl-long.380,N16-1030,0,0.0408353,"latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang"
2021.acl-long.380,2020.acl-main.193,0,0.46218,"guages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches. We start by introducing a novel approach based on the minimum risk training framework. We design a new decomposable risk function parameterized by a fixed matrix that models the relations between the noisy predictions from the source models and the true labels. We then make the matrix trainable, which leads to further expressiveness and connects minimum risk training to learning latent 4909 Proceedings of the 59th Annual Meeting of the Associ"
2021.acl-long.380,P16-1101,0,0.0435505,"l learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et"
2021.acl-long.380,P17-1135,0,0.0379881,"Missing"
2021.acl-long.380,P19-1493,0,0.0143402,"Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks. Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the strong cross-lingual ability of the contextual multilingual embeddings. 7 Conclusion In this paper, we propose two approaches to the zero-shot sequence labeling problem. Our MRT approach uses a fixed matrix to model the relations between the predicted labels from the source models and the true labels. Our LVM approach uses trainable matrices to model these label relations. We extensively verify the effectiveness of our approaches on both single-source and multisource transfer over both cross-lingual and crossdomain sequence labeling problems. Experiments show that MRT and LVM general"
2021.acl-long.380,P19-1015,0,0.109868,"ed data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches. We start by introducing a novel approach based on the minimum risk training framework. We design a new decomposable risk function parameterized by a fixed matrix that models the relations between the noisy predictions from the source models and the true labels. We then make the matrix trainable, which leads to further expressiveness and connects minimum risk training to learning latent 4909 Proceedings of the 59th Annual Me"
2021.acl-long.380,W09-1119,0,0.0197473,"connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsk"
2021.acl-long.380,D11-1141,0,0.00798819,"mum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo"
2021.acl-long.380,P18-1096,0,0.0210148,"pose a multi-view framework to selectively transfer knowledge from multiple sources by utilizing a small amount of labeled dataset. Crossdomain adaption is widely studied (Steedman et al., 6 The CoNLL NER datasets have 11 labels (9 entity labels, a padding label and an ending label). 4916 F1 De Nl Es 74.1 80.6 76.1 74.0 80.5 75.9 73.9 80.4 75.8 73.7 80.2 75.6 73.6 2 3 4 10 80.1 2 3 4 τ 10 τ 75.4 2 3 4 10 τ Figure 3: The performance of MRT approach in single-source setup with soft predictions on three NER datasets by varying different τ . 2003). Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP task"
2021.acl-long.380,P16-1159,0,0.0343896,"hard predictions and the target model’s soft predictions, Methodology Minimum Risk Training In supervised learning, minimum risk training aims to minimize the expected error (risk) concerning the conditional probability, X J (θθ ) = Pθ (y|x)R(y∗ , y) y∈Y(x) where R(y∗ , y) is the risk function that measures the distance between the gold sequence y∗ and the candidate sequence y, and Y(x) denotes the collection of all the possible label sequences given the sentence x. The risk function can be defined in many ways depending on specific applications, such as the BLEU score in machine translation (Shen et al., 2016). However, in our setting, there are no gold labels to compute R(y∗ , y). Instead, we assume there are multiple pretrained source models which can be used to predict hard labels, and we define the risk function as R(ˆ y, y) to measure the difference between pseudo label sequence 4910 ˆ predicted by source models and the candidate y sequence y. The objective function becomes, x y∈Y(x) Conventional minimum risk training is intractable which is mainly due to the combination of two reasons: first, the set of candidate label sequences Y(x) is exponential in size and intractable to enumerate; second"
2021.acl-long.380,N03-1031,0,0.424391,"Missing"
2021.acl-long.380,N12-1052,0,0.0777677,"Missing"
2021.acl-long.380,W02-2024,0,0.400509,"hree tasks to conduct the cross-lingual sequence labeling task, which are POS tagging, NER, and Aspect Extraction. For the POS tagging task, we use Universal Dependencies treebanks (UD) v2.44 and randomly select five anguages together with the English dataset. The whole datasets are English (En), Catalan (Ca), Indonesian (Id), Hindi (Hi), Finnish (Fi), and Russian (Ru). For the Aspect Extraction task, we select the restaurant domain over subtask 1 in the SemEval-2016 shared task (Pontiki et al., 2016). For the NER task, we evaluate our models on the CoNLL 2002 and 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Cross-Domain Sequence Labeling We use English portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb). More details can be found in the Appendix A.1. 4.2 Approaches Single-source Setup The following approaches are applicable for single-source setup, • DT: we use the pre-trained source model to directly predict the pseudo labels on the target unlabeled data. • Hard: we use the pseudo labels from DT on the target unlabeled data to train a new"
2021.acl-long.380,N03-1033,0,0.314546,"rue labels. By making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sou"
2021.acl-long.380,Q14-1005,0,0.0759383,"nd Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the im"
2021.acl-long.380,2020.emnlp-main.639,0,0.0335027,"from multiple sources by utilizing a small amount of labeled dataset. Crossdomain adaption is widely studied (Steedman et al., 6 The CoNLL NER datasets have 11 labels (9 entity labels, a padding label and an ending label). 4916 F1 De Nl Es 74.1 80.6 76.1 74.0 80.5 75.9 73.9 80.4 75.8 73.7 80.2 75.6 73.6 2 3 4 10 80.1 2 3 4 τ 10 τ 75.4 2 3 4 10 τ Figure 3: The performance of MRT approach in single-source setup with soft predictions on three NER datasets by varying different τ . 2003). Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks. Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the"
2021.acl-long.380,2020.acl-main.581,0,0.0664042,"t languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches. We start by introducing a novel approach based on the minimum risk training framework. We design a new decomposable risk function parameterized by a fixed matrix that models th"
2021.acl-long.380,D19-1077,0,0.0498863,"trained on its corresponding training data. We use the BIO scheme for CoNLL and OntoNotes NER tasks and Aspect Extraction. We run each model three times and report the average accuracy for the POS tagging task and F1-score for the other tasks. 2, Multi-source Setup The following approaches are applicable for multi-source setup, Pφ (ˆ y(u) |y, u) u=1 Experiments We use the multilingual BERT (mBERT) as our word representations3 as the sentence encoder. Fol2 Another choice is to use Pθ (y|x), however, we found that utilizing Q(y) generally achieves better performance. 3 Following previous work (Wu and Dredze, 2019; Wu et al., 2020), we fine-tune mBERT’s parameters. 4913 • Hard-Cat: we apply DT with all the source languages/domains, mix the resulting pseudo labels from all the sources on the unlabeled target data, and train a new model. • Hard-Vote: we do majority voting at the token level on the pseudo labels from DT with each source and train a new model. 4 https://universaldependencies.org/ C O NLL NER English German Dutch Spanish Avg. A SPECT E XTRACTION English Spanish Dutch Russian Turkish Avg. S INGLE - SOURCE : The following approaches have access to hard predictions: — 72.17 79.54 75.13 75.61 D"
2021.acl-long.380,N15-1069,0,0.0277639,"rce problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced b"
2021.acl-long.46,C18-1139,0,0.441402,"ank (PTB) 3.0 and follow the same pre-processing pipeline as in Ma et al. (2018). For unlabeled data, we sample sentences that belong to the same languages of the labeled data from the WikiAnn datasets for Case 1a, 2a and 4 and we sample sentences from the target languages of WikiAnn datasets for Case 3. We use the BLLIP corpus3 as the unlabeled data for Case 1b and 2b. Models For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation. For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Zadrozny, 2014) as the word representations for stronger monolingual teacher models (Wang et al., 2020c). For Case 3, we use MBERT embeddings for the teacher. Also for Case 3, we fine-tune the teacher model on the training set of the four Indo-European languages from the WikiAnn dataset and train student models on the four additional languages. For the teacher models in Case 1b and 2b, we simply use the same embeddings as the student because there is already huge performance gap between the teacher and student in these settings and hen"
2021.acl-long.46,2020.iwpt-1.2,0,0.0868451,"Missing"
2021.acl-long.46,Q17-1010,0,0.0229249,"or English and 5000 sentences for each of the other languages. We split the datasets by 3:1:1 for training/development/test. For Case 1b and 2b, we use Penn Treebank (PTB) 3.0 and follow the same pre-processing pipeline as in Ma et al. (2018). For unlabeled data, we sample sentences that belong to the same languages of the labeled data from the WikiAnn datasets for Case 1a, 2a and 4 and we sample sentences from the target languages of WikiAnn datasets for Case 3. We use the BLLIP corpus3 as the unlabeled data for Case 1b and 2b. Models For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation. For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Zadrozny, 2014) as the word representations for stronger monolingual teacher models (Wang et al., 2020c). For Case 3, we use MBERT embeddings for the teacher. Also for Case 3, we fine-tune the teacher model on the training set of the four Indo-European languages from the WikiAnn dataset and train student models on the four additional languages. For the teacher models in Case 1b and"
2021.acl-long.46,P19-1595,0,0.0149707,"of a large teacher model. The typical KD objective function is the cross-entropy between the output distributions predicted by the teacher model and the student model: X LKD = − Pt (y|x) log Ps (y|x) (2) y∈Y(x) where Pt and Ps are the teacher’s and the student’s distributions respectively. During training, the student jointly learns from the gold targets and the distributions predicted by the teacher by optimizing the following objective function: Lstudent = λLKD + (1 − λ)Ltarget where λ is an interpolation coefficient between the target loss Ltarget and the structural KD loss LKD . Following Clark et al. (2019); Wang et al. (2020a), one may apply teacher annealing in training by decreasing λ linearly from 1 to 0. Because KD does not require gold labels, unlabeled data can also be used in the KD loss. 551 3 Structural Knowledge Distillation When performing knowledge distillation on structured prediction, a major challenge is that the structured output space is exponential in size, leading to intractable computation of the KD objective in Eq. 2. However, if the scoring function of the student model can be factorized into scores of substructures (Eq. 1), then we can derive the following factorized form"
2021.acl-long.46,2020.acl-main.747,0,0.135739,"Missing"
2021.acl-long.46,N19-1423,0,0.273421,"This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/StructuralKD. ♠ as online serving. An interesting and viable solution to this problem is knowledge distillation (KD) (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015), which can be used to transfer the knowledge of a large model (the teacher) to a smaller model (the student). In the field of natural language processing (NLP), for example, KD has been successfully applied to compress massive pretrained language models such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) into much smaller and faster models without significant loss in accuracy (Tang et al., 2019; Sanh et al., 2019; Tsai et al., 2019; Mukherjee and Hassan Awadallah, 2020). A typical approach to KD is letting the student mimic the teacher model’s output probability distributions on the training data by using the cross-entropy objective. For structured prediction problems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label se"
2021.acl-long.46,K17-3002,0,0.015241,"l for named entity recognition (NER) that is based on large pretrained contextualized embeddings to a smaller CRF model with static embeddings that is more suitable for fast online serving. For a CRF student model described in section 2.1, if we absorb the emission score Se (yi , x) into the transition score St ((yi−1 , yi ), x) at each position i, then the substructure space Us (x) contains every two adjacent labels {(yi−1 , yi )} for i=1, . . . , n, with n beCase 1b: Graph-based Dependency Parsing ⇒ Dependency Parsing as Sequence Labeling In this case, we use the biaffine parser proposed by Dozat et al. (2017) as the teacher and the sequence labeling approach proposed by Strzyz et al. (2019) as the student for the dependency parsing task. The biaffine parser is one of the state-of-the-art models, while the sequence labeling parser provides a good speed-accuracy tradeoff. There is a big gap in accuracy between the two models and therefore KD can be used to improve the accuracy of the sequence labeling parser. Here we follow the head-selection formulation of dependency parsing without the tree constraint. The dependency parse tree y is represented by hy1 , . . . , yn i, where n is the sentence length"
2021.acl-long.46,P19-1266,0,0.0536502,"Missing"
2021.acl-long.46,D16-1139,0,0.172232,"py objective. For structured prediction problems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics margi"
2021.acl-long.46,D16-1180,0,0.12717,"ructured prediction problems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics marginal distributions over"
2021.acl-long.46,P19-1233,0,0.0774946,"ictions from marginal distributions of the CRF teacher still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that second-order CRF parsers achieve sign"
2021.acl-long.46,P16-1101,0,0.0363014,"ons between adjacent labels. While predictions from marginal distributions of the CRF teacher still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that"
2021.acl-long.46,P18-1130,0,0.017162,"s-lingual transfer in Case 3, we use the four Indo-European languages as the source for the teacher model and additionally select four languages from different language families as the target for the student models.2 We use the standard training/development/test split for the CoNLL datasets. For WikiAnn, we follow the sampling of Wang et al. (2020a) with 12000 sentences for English and 5000 sentences for each of the other languages. We split the datasets by 3:1:1 for training/development/test. For Case 1b and 2b, we use Penn Treebank (PTB) 3.0 and follow the same pre-processing pipeline as in Ma et al. (2018). For unlabeled data, we sample sentences that belong to the same languages of the labeled data from the WikiAnn datasets for Case 1a, 2a and 4 and we sample sentences from the target languages of WikiAnn datasets for Case 3. We use the BLLIP corpus3 as the unlabeled data for Case 1b and 2b. Models For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation. For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Za"
2021.acl-long.46,2020.acl-main.202,0,0.036762,"Missing"
2021.acl-long.46,P17-1178,0,0.0170604,"e i-th word as its head and with its length larger than 1. It is intractable to compute such marginal probabilities by enumerating all the output structures, but we can tractably compute them using dynamic programming. See supplementary material for a detailed description of our dynamic programming method. 4 Experiments We evaluate our approaches described in Section 3 on NER (Case 1a, 2a, 3, 4) and dependency parsing (Case 1b, 2b). 4.1 Settings Datasets We use CoNLL 2002/2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for Case 1a, 2a and 4, and use WikiAnn datasets (Pan et al., 2017) for Case 1a, 2a, 3, and 4. The CoNLL datasets contain the corpora of four Indo-European languages. We use the same four languages from the WikiAnn datasets. For cross-lingual transfer in Case 3, we use the four Indo-European languages as the source for the teacher model and additionally select four languages from different language families as the target for the student models.2 We use the standard training/development/test split for the CoNLL datasets. For WikiAnn, we follow the sampling of Wang et al. (2020a) with 12000 sentences for English and 5000 sentences for each of the other language"
2021.acl-long.46,P19-1493,0,0.0162243,"re consecutive labels {(yi−1 , yi )}. In contrast, a MaxEnt model predicts the label probability distribution Ps (yi |x) of each token independently and hence the substructure space Us (x) consists of every individual label {yi }. To calculate the substructure marginal of the teacher Pt (yi |x), we can again utilize the forwardbackward algorithm: Teacher Factorization Produces More Fine-grained Substructures than Student Factorization Case 3: MaxEnt ⇒ Linear-Chain CRF Here we consider KD in the opposite direction of Case 2a. An example application is zero-shot crosslingual NER. Previous work (Pires et al., 2019; Wu and Dredze, 2019) has shown that multilingual BERT (M-BERT) has strong zero-shot crosslingual transferability in NER tasks. Many such models employ a MaxEnt decoder. In scenarios requiring fast speed and low computation cost, however, we may want to distill knowledge from such models to a model with much cheaper static monolingual embeddings while compensating the performance loss with a linear-chain CRF decoder. As described in Case 1a, the substructures of a linear-chain CRF model are consecutive labels {(yi−1 , yi )}. Because of the label independence and local normalization in the Max"
2021.acl-long.46,N19-1335,0,0.0202894,"le head-selection first-order approach (Dozat and Manning, 2017). Such speed-accuracy tradeoff as seen in sequence labeling and dependency parsing also occurs in many other structured prediction tasks. This makes KD an interesting and very useful technique that can be used to circumvent this tradeoff to some extent. 6.2 Knowledge Distillation in Structured Prediction KD has been applied in many structured prediction tasks in the fields of NLP, speech recognition and computer vision, with applications such as neural machine translation (Kim and Rush, 2016; Tan et al., 2019), sequence labeling (Tu and Gimpel, 2019; Wang et al., 2020a), connectionist temporal classification (Huang et al., 2018), image semantic segmentation (Liu et al., 2019a) and so on. In KD for structured prediction tasks, how to handle the exponential number of structured outputs is a main challenge. To address this difficult problem, recent work resorts to approximation of the KD objective. Kim and Rush (2016) proposed sequence-level distillation through predicting K-best sequences of the teacher in neural machine translation. Kuncoro et al. (2016) proposed to use multiple greedy parsers as teachers and generate the probability dist"
2021.acl-long.46,P19-1454,1,0.926723,"to the following form without the need for calculating the student partition function Zs (x). LKD = − X Pt (u|x) × logPs (u|x) (6) u∈Us (x) In all the cases except Case 1a and Case 3, the student model is locally normalized and hence we can follow this form of objective. 3.2 parse tree independently. A second-order dependency parser scores pairs of dependency arcs with a shared token. The substructures of second-order parsing are therefore all the dependency arc pairs with a shared token. It has been found that secondorder extensions of the biaffine parser often have higher parsing accuracy (Wang et al., 2019; Zhang et al., 2020; Wang et al., 2020d; Wang and Tu, 2020). Therefore, we may take a second-order dependency parser as the teacher to improve a sequence labeling parser. Here we consider the second-order dependency parser of Wang and Tu (2020). It employs mean field variational inference to estimate the probabilities of arc existence Pt (hi |x) and uses a first-order biaffine model to estimate the probabilities of arc labels Pt (li |x). Therefore, the substructure marginal can be calculated in the same way as Eq. 5. 3.3 Student Factorization Produces More Fine-grained Substructures than Teac"
2021.acl-long.46,2020.acl-main.304,1,0.125515,"oblems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics marginal distributions over these substructure"
2021.acl-long.46,2020.emnlp-main.485,1,0.0750727,"oblems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics marginal distributions over these substructure"
2021.acl-long.46,2021.acl-long.206,1,0.538582,"still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that second-order CRF parsers achieve significantly higher accuracy than first-order parsers (Wan"
2021.acl-long.46,2021.acl-long.142,1,0.721594,"still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that second-order CRF parsers achieve significantly higher accuracy than first-order parsers (Wan"
2021.eacl-tutorials.1,D16-1073,1,0.841115,"d and third parts, we will introduce in detail two major classes of approaches to unsupervised parsing, generative and discriminative approaches, and discuss their pros and cons. The second part will cover generative approaches, which model the joint probability of the sentence and the corresponding parse tree. Most of the existing generative approaches are based on generative grammars, in particular context-free grammars and dependency models with valence (Klein and Manning, 2004). There are also featurized and neural extensions of generative grammars, such as Berg-Kirkpatrick et al. (2010); Jiang et al. (2016). We will divide our discussion of learning generative grammars into two parts: structure learning and parameter learning. Structure learning concerns finding the optimal set of grammar rules. We will introduce both probabilistic methods such as Stolcke and Omohundro (1994) and heuristic methods such as Clark (2007). Parameter learning concerns learning the probabilities or weights of a pre-specified set of grammar rules. We will discuss a variety of priors and regularizations designed to improve parameter learning, such as Cohen and Smith (2010), Tu and Honavar (2012), Noji et al. (2016), and"
2021.eacl-tutorials.1,D18-1292,0,0.0118435,"We will divide our discussion of learning generative grammars into two parts: structure learning and parameter learning. Structure learning concerns finding the optimal set of grammar rules. We will introduce both probabilistic methods such as Stolcke and Omohundro (1994) and heuristic methods such as Clark (2007). Parameter learning concerns learning the probabilities or weights of a pre-specified set of grammar rules. We will discuss a variety of priors and regularizations designed to improve parameter learning, such as Cohen and Smith (2010), Tu and Honavar (2012), Noji et al. (2016), and (Jin et al., 2018). 3 Outline Part 1. Introduction [20 min] 2 5 • Problem definition Reading List Klein and Manning (2004) – An influential generative approach to unsupervised dependency parsing that is the basis for many subsequent papers. • Motivations and applications • Evaluation • Overview of approaches Jiang et al. (2016) – A neural extension of Klein and Manning (2004). One of the first modern neural approaches to unsupervised parsing. Part 2. Generative Approaches [60 min] • Overview Stolcke and Omohundro (1994) – One of the first structure learning approaches of context-free grammars for unsupervised c"
2021.eacl-tutorials.1,N07-1018,0,0.098062,"67) and algorithmic and empirical studies in 1970s (Baker, 1979). Although deemed an interesting topic by the NLP community, unsupervised parsing had received much less attention than supervised parsing over the past few decades. 1 Proceedings of EACL: Tutorials, pages 1–5 April 19 - 20, 2021. ©2020 Association for Computational Linguistics ing and apply or extend them to other NLP tasks that can potentially benefit from implicitly learned linguistic structures. 2 We will also discuss parameter learning algorithms such as expectation-maximization (Baker, 1979; Spitkovsky et al., 2010b), MCMC (Johnson et al., 2007) and curriculum learning (Spitkovsky et al., 2010a). After introducing approaches based on generative grammars, we will discuss recent approaches that are instead based on neural language models (Shen et al., 2018, 2019). The third part will cover discriminative approaches, which model the conditional probability or score of the parse tree given the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to reconstruct the sentence. Their"
2021.eacl-tutorials.1,N10-1083,0,0.131694,"Missing"
2021.eacl-tutorials.1,N19-1114,0,0.0768135,"ent approaches that are instead based on neural language models (Shen et al., 2018, 2019). The third part will cover discriminative approaches, which model the conditional probability or score of the parse tree given the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to reconstruct the sentence. Their training objective is typically the reconstruction probability. We will then introduce variational autoencoder approaches such as Kim et al. (2019), which has a similar model structure to autoencoder approaches but uses the evidence lower bound as the training objective. Finally, we will briefly discuss other discriminative approaches such as Grave and Elhadad (2015). In the fourth part, we will focus on several special topics. First, while most of the previous approaches to unsupervised parsing are unlexicalized, we will discuss the impact of partial and full lexicalization (e.g., the work by Pate and Johnson (2016); Han et al. (2017)). Second, we will discuss whether and how big training data could benefit unsupervised parsing (Han et"
2021.eacl-tutorials.1,P10-1131,0,0.0114646,"approaches to unsupervised parsing are unlexicalized, we will discuss the impact of partial and full lexicalization (e.g., the work by Pate and Johnson (2016); Han et al. (2017)). Second, we will discuss whether and how big training data could benefit unsupervised parsing (Han et al., 2017). Third, we will introduce recent attempts to induce syntactic parses from pretrained language models such as BERT (Rosa and Mareˇcek, 2019; Wu et al., 2020). Fourth, we will cover unsupervised multilingual parsing, the task of performing unsupervised parsing jointly on multiple languages (e.g., the work by Berg-Kirkpatrick and Klein (2010); Han et al. (2019)). Fifth, we will introduce visually grounded unsupervised parsing, which tries to improve unsupervised parsing with the help from visual data (Shi et al., 2019). Finally, we will discuss latent tree models trained with feedback from downstream tasks, which are related to unsupervised parsing (Yogatama et al., 2016; Choi et al., 2018). In the last part, we will summarize the tutorial and discuss potential future research directions of unsupervised parsing. Overview This will be a three-hour tutorial divided into five parts. In the first part, we will introduce the unsupervis"
2021.eacl-tutorials.1,D17-1171,1,0.879655,"arned linguistic structures. 2 We will also discuss parameter learning algorithms such as expectation-maximization (Baker, 1979; Spitkovsky et al., 2010b), MCMC (Johnson et al., 2007) and curriculum learning (Spitkovsky et al., 2010a). After introducing approaches based on generative grammars, we will discuss recent approaches that are instead based on neural language models (Shen et al., 2018, 2019). The third part will cover discriminative approaches, which model the conditional probability or score of the parse tree given the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to reconstruct the sentence. Their training objective is typically the reconstruction probability. We will then introduce variational autoencoder approaches such as Kim et al. (2019), which has a similar model structure to autoencoder approaches but uses the evidence lower bound as the training objective. Finally, we will briefly discuss other discriminative approaches such as Grave and Elhadad (2015). In the fourth part, we will focus on several special topics. F"
2021.eacl-tutorials.1,P04-1061,0,0.27348,"al., 2020). Finally, we will give an overview of unsupervised parsing approaches to be discussed in the rest of the tutorial. In the second and third parts, we will introduce in detail two major classes of approaches to unsupervised parsing, generative and discriminative approaches, and discuss their pros and cons. The second part will cover generative approaches, which model the joint probability of the sentence and the corresponding parse tree. Most of the existing generative approaches are based on generative grammars, in particular context-free grammars and dependency models with valence (Klein and Manning, 2004). There are also featurized and neural extensions of generative grammars, such as Berg-Kirkpatrick et al. (2010); Jiang et al. (2016). We will divide our discussion of learning generative grammars into two parts: structure learning and parameter learning. Structure learning concerns finding the optimal set of grammar rules. We will introduce both probabilistic methods such as Stolcke and Omohundro (1994) and heuristic methods such as Clark (2007). Parameter learning concerns learning the probabilities or weights of a pre-specified set of grammar rules. We will discuss a variety of priors and r"
2021.eacl-tutorials.1,2020.acl-main.300,1,0.80309,"motivations and applications of unsupervised parsing. For example, we will show that unsupervised parsing approaches can be extended for semi-supervised parsing (Jia et al., 2020) and cross-lingual syntactic transfer (He et al., 2019), and we will also show applications of unsupervised parsing approaches beyond syntactic parsing (e.g., in computer vision (Tu et al., 2013)). We will then discuss how to evaluate unsupervised parsing, including the evaluation metrics and typical experimental setups. We will promote standardized setups to enable meaningful empirical comparison between approaches (Li et al., 2020). Finally, we will give an overview of unsupervised parsing approaches to be discussed in the rest of the tutorial. In the second and third parts, we will introduce in detail two major classes of approaches to unsupervised parsing, generative and discriminative approaches, and discuss their pros and cons. The second part will cover generative approaches, which model the joint probability of the sentence and the corresponding parse tree. Most of the existing generative approaches are based on generative grammars, in particular context-free grammars and dependency models with valence (Klein and"
2021.eacl-tutorials.1,D16-1004,0,0.0209946,"0); Jiang et al. (2016). We will divide our discussion of learning generative grammars into two parts: structure learning and parameter learning. Structure learning concerns finding the optimal set of grammar rules. We will introduce both probabilistic methods such as Stolcke and Omohundro (1994) and heuristic methods such as Clark (2007). Parameter learning concerns learning the probabilities or weights of a pre-specified set of grammar rules. We will discuss a variety of priors and regularizations designed to improve parameter learning, such as Cohen and Smith (2010), Tu and Honavar (2012), Noji et al. (2016), and (Jin et al., 2018). 3 Outline Part 1. Introduction [20 min] 2 5 • Problem definition Reading List Klein and Manning (2004) – An influential generative approach to unsupervised dependency parsing that is the basis for many subsequent papers. • Motivations and applications • Evaluation • Overview of approaches Jiang et al. (2016) – A neural extension of Klein and Manning (2004). One of the first modern neural approaches to unsupervised parsing. Part 2. Generative Approaches [60 min] • Overview Stolcke and Omohundro (1994) – One of the first structure learning approaches of context-free gra"
2021.eacl-tutorials.1,C16-1003,0,0.0194621,"raining objective is typically the reconstruction probability. We will then introduce variational autoencoder approaches such as Kim et al. (2019), which has a similar model structure to autoencoder approaches but uses the evidence lower bound as the training objective. Finally, we will briefly discuss other discriminative approaches such as Grave and Elhadad (2015). In the fourth part, we will focus on several special topics. First, while most of the previous approaches to unsupervised parsing are unlexicalized, we will discuss the impact of partial and full lexicalization (e.g., the work by Pate and Johnson (2016); Han et al. (2017)). Second, we will discuss whether and how big training data could benefit unsupervised parsing (Han et al., 2017). Third, we will introduce recent attempts to induce syntactic parses from pretrained language models such as BERT (Rosa and Mareˇcek, 2019; Wu et al., 2020). Fourth, we will cover unsupervised multilingual parsing, the task of performing unsupervised parsing jointly on multiple languages (e.g., the work by Berg-Kirkpatrick and Klein (2010); Han et al. (2019)). Fifth, we will introduce visually grounded unsupervised parsing, which tries to improve unsupervised pa"
2021.eacl-tutorials.1,P15-1133,0,0.0164102,"n the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to reconstruct the sentence. Their training objective is typically the reconstruction probability. We will then introduce variational autoencoder approaches such as Kim et al. (2019), which has a similar model structure to autoencoder approaches but uses the evidence lower bound as the training objective. Finally, we will briefly discuss other discriminative approaches such as Grave and Elhadad (2015). In the fourth part, we will focus on several special topics. First, while most of the previous approaches to unsupervised parsing are unlexicalized, we will discuss the impact of partial and full lexicalization (e.g., the work by Pate and Johnson (2016); Han et al. (2017)). Second, we will discuss whether and how big training data could benefit unsupervised parsing (Han et al., 2017). Third, we will introduce recent attempts to induce syntactic parses from pretrained language models such as BERT (Rosa and Mareˇcek, 2019; Wu et al., 2020). Fourth, we will cover unsupervised multilingual parsi"
2021.eacl-tutorials.1,P19-1180,0,0.111693,"hool of Information Science and Technology, ShanghaiTech University 2 Alibaba DAMO Academy, Alibaba Group 3 School of Computing, National University of Singapore, Singapore 4 ILCC, University of Edinburgh tukw@shanghaitech.edu.cn yongjiang.jy@alibaba-inc.com dcshanw@nus.edu.sg yanp.zhao@ed.ac.uk Introduction More recently, however, there has been a resurgence of interest in unsupervised parsing, with more than ten papers on unsupervised parsing published in top NLP and AI venues over the past two years, including a best paper at ICLR 2019 (Shen et al., 2019), a best paper nominee at ACL 2019 (Shi et al., 2019), and a best paper nominee at EMNLP 2020 (Zhao and Titov, 2020). This renewed interest in unsupervised parsing can be attributed to the combination of two recent trends. First, there is a general trend in deep learning towards unsupervised training or pre-training. Second, there is an emerging trend in the NLP community towards finding or modeling linguistic structures in neural models. The research on unsupervised parsing fits these two trends perfectly. Because of the renewed attention on unsupervised parsing and its relevance to the recent trends in the NLP community, we believe a tutorial"
2021.eacl-tutorials.1,N10-1116,0,0.0401605,"etical studies in 1960s (Gold, 1967) and algorithmic and empirical studies in 1970s (Baker, 1979). Although deemed an interesting topic by the NLP community, unsupervised parsing had received much less attention than supervised parsing over the past few decades. 1 Proceedings of EACL: Tutorials, pages 1–5 April 19 - 20, 2021. ©2020 Association for Computational Linguistics ing and apply or extend them to other NLP tasks that can potentially benefit from implicitly learned linguistic structures. 2 We will also discuss parameter learning algorithms such as expectation-maximization (Baker, 1979; Spitkovsky et al., 2010b), MCMC (Johnson et al., 2007) and curriculum learning (Spitkovsky et al., 2010a). After introducing approaches based on generative grammars, we will discuss recent approaches that are instead based on neural language models (Shen et al., 2018, 2019). The third part will cover discriminative approaches, which model the conditional probability or score of the parse tree given the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to"
2021.eacl-tutorials.1,W10-2902,0,0.0423179,"etical studies in 1960s (Gold, 1967) and algorithmic and empirical studies in 1970s (Baker, 1979). Although deemed an interesting topic by the NLP community, unsupervised parsing had received much less attention than supervised parsing over the past few decades. 1 Proceedings of EACL: Tutorials, pages 1–5 April 19 - 20, 2021. ©2020 Association for Computational Linguistics ing and apply or extend them to other NLP tasks that can potentially benefit from implicitly learned linguistic structures. 2 We will also discuss parameter learning algorithms such as expectation-maximization (Baker, 1979; Spitkovsky et al., 2010b), MCMC (Johnson et al., 2007) and curriculum learning (Spitkovsky et al., 2010a). After introducing approaches based on generative grammars, we will discuss recent approaches that are instead based on neural language models (Shen et al., 2018, 2019). The third part will cover discriminative approaches, which model the conditional probability or score of the parse tree given the sentence. We will first introduce autoencoder approaches such as Cai et al. (2017), which contain an encoder that maps the sentence to an intermediate representation (such as a parse tree) and a decoder that tries to"
2021.eacl-tutorials.1,D12-1121,1,0.786319,"Kirkpatrick et al. (2010); Jiang et al. (2016). We will divide our discussion of learning generative grammars into two parts: structure learning and parameter learning. Structure learning concerns finding the optimal set of grammar rules. We will introduce both probabilistic methods such as Stolcke and Omohundro (1994) and heuristic methods such as Clark (2007). Parameter learning concerns learning the probabilities or weights of a pre-specified set of grammar rules. We will discuss a variety of priors and regularizations designed to improve parameter learning, such as Cohen and Smith (2010), Tu and Honavar (2012), Noji et al. (2016), and (Jin et al., 2018). 3 Outline Part 1. Introduction [20 min] 2 5 • Problem definition Reading List Klein and Manning (2004) – An influential generative approach to unsupervised dependency parsing that is the basis for many subsequent papers. • Motivations and applications • Evaluation • Overview of approaches Jiang et al. (2016) – A neural extension of Klein and Manning (2004). One of the first modern neural approaches to unsupervised parsing. Part 2. Generative Approaches [60 min] • Overview Stolcke and Omohundro (1994) – One of the first structure learning approaches"
2021.eacl-tutorials.1,2020.acl-main.383,0,0.0146244,"iscuss other discriminative approaches such as Grave and Elhadad (2015). In the fourth part, we will focus on several special topics. First, while most of the previous approaches to unsupervised parsing are unlexicalized, we will discuss the impact of partial and full lexicalization (e.g., the work by Pate and Johnson (2016); Han et al. (2017)). Second, we will discuss whether and how big training data could benefit unsupervised parsing (Han et al., 2017). Third, we will introduce recent attempts to induce syntactic parses from pretrained language models such as BERT (Rosa and Mareˇcek, 2019; Wu et al., 2020). Fourth, we will cover unsupervised multilingual parsing, the task of performing unsupervised parsing jointly on multiple languages (e.g., the work by Berg-Kirkpatrick and Klein (2010); Han et al. (2019)). Fifth, we will introduce visually grounded unsupervised parsing, which tries to improve unsupervised parsing with the help from visual data (Shi et al., 2019). Finally, we will discuss latent tree models trained with feedback from downstream tasks, which are related to unsupervised parsing (Yogatama et al., 2016; Choi et al., 2018). In the last part, we will summarize the tutorial and discu"
2021.eacl-tutorials.1,2020.emnlp-main.354,1,0.825072,"Missing"
2021.emnlp-main.205,P19-1335,0,0.0609584,"Missing"
2021.emnlp-main.205,2020.findings-emnlp.228,0,0.0213909,"enting each entity with a fixed-sized vector has been a common approach in Entity Linking. Ganea and Hofmann (2017) defines a word-entity conditional distribution and samples positive words 2 https://github.com/facebookresearch/ BLINK from it. The representations of those positive words aim to approximate the entity embeddings compared with random words. Yamada et al. (2016) models the relatedness between entities into entity representations. NTEE (Yamada et al., 2017) trains entity representations by predicting the relevant entities for a given context in DBPedia abstract corpus. Ling et al. (2020) and Yamada et al. (2020) pre-train variants of the transformer-based model by maximizing the consistency between the context of the mentions and the corresponding entities. Those entity representations suffer from a cold-start problem that they cannot link mentions to unseen entities. Another line of work is to generate entity representations using entity textual information, such as entity descriptions. Logeswaran et al. (2019) introduces an EL dataset in the zero-shot scenario to place more emphasis on reading entity descriptions. BLINK (Wu et al., 2020) proposes a bi-encoder to encode the"
2021.emnlp-main.205,2021.naacl-main.86,0,0.0194072,"fer from a cold-start problem that they cannot link mentions to unseen entities. Another line of work is to generate entity representations using entity textual information, such as entity descriptions. Logeswaran et al. (2019) introduces an EL dataset in the zero-shot scenario to place more emphasis on reading entity descriptions. BLINK (Wu et al., 2020) proposes a bi-encoder to encode the descriptions and enhance the bi-encoder by distilling the knowledge from the cross-encoder. Yao et al. (2020) repeats the position embedding to solve the long-range modeling problem in entity descriptions. Zhang and Stratos (2021) demonstrates that hard negatives can enhance the contrast when training an EL model. 5 Conclusion In this work, we propose a novel approach to construct multi-view representations from descriptions, which shows promising results on four EL datasets. Extensive results demonstrate the effectiveness of multi-view representations and the heuristic search strategy. In the future, we will explore more reliable and efficient approaches to construct views. Acknowledgement This work was supported by Alibaba Group through Alibaba Innovative Research Program and was also supported by the Key Research an"
2021.emnlp-main.205,2020.emnlp-main.519,0,0.403904,"probEntity retrieval, which aims at disambiguating abilities, e.g., p(entity|mention) (Le and Titov, mentions to canonical entities from massive 2018). Ganea and Hofmann (2017) and Yamada KBs, is essential for many tasks in natural lanet al. (2016) build entity embedding from the local guage processing. Recent progress in entity retrieval shows that the dual-encoder structure context of hyperlinks in entity pages or entity-entity is a powerful and efficient framework to nomico-occurrences. Those embedding-based methods nate candidates if entities are only identified by were extended by BLINK (Wu et al., 2020) and descriptions. However, they ignore the propDEER (Gillick et al., 2019) to two-tower dualerty that meanings of entity mentions diverge encoders (Khattab and Zaharia, 2020), which enin different contexts and are related to varicode mentions and descriptions of entities into highous portions of descriptions, which are treated dimensional vectors respectively. Candidates are equally in previous works. In this work, we retrieved by nearest neighbor search (Andoni and propose Multi-View Entity Representations (MuVER), a novel approach for entity retrieval Indyk, 2008; Johnson et al., 2019) for"
2021.emnlp-main.205,2020.emnlp-main.523,0,0.0338565,"entity with a fixed-sized vector has been a common approach in Entity Linking. Ganea and Hofmann (2017) defines a word-entity conditional distribution and samples positive words 2 https://github.com/facebookresearch/ BLINK from it. The representations of those positive words aim to approximate the entity embeddings compared with random words. Yamada et al. (2016) models the relatedness between entities into entity representations. NTEE (Yamada et al., 2017) trains entity representations by predicting the relevant entities for a given context in DBPedia abstract corpus. Ling et al. (2020) and Yamada et al. (2020) pre-train variants of the transformer-based model by maximizing the consistency between the context of the mentions and the corresponding entities. Those entity representations suffer from a cold-start problem that they cannot link mentions to unseen entities. Another line of work is to generate entity representations using entity textual information, such as entity descriptions. Logeswaran et al. (2019) introduces an EL dataset in the zero-shot scenario to place more emphasis on reading entity descriptions. BLINK (Wu et al., 2020) proposes a bi-encoder to encode the descriptions and enhance"
2021.emnlp-main.205,K16-1025,0,0.0210626,"2. BLINK performance on these datasets is reported in its official Github repository2 . We report the In-KB accuracy in Table 2 and observe that MuVER out-performs BLINK on all datasets except the recall@100 on WNED-WIKI. 4 Related Work Representing each entity with a fixed-sized vector has been a common approach in Entity Linking. Ganea and Hofmann (2017) defines a word-entity conditional distribution and samples positive words 2 https://github.com/facebookresearch/ BLINK from it. The representations of those positive words aim to approximate the entity embeddings compared with random words. Yamada et al. (2016) models the relatedness between entities into entity representations. NTEE (Yamada et al., 2017) trains entity representations by predicting the relevant entities for a given context in DBPedia abstract corpus. Ling et al. (2020) and Yamada et al. (2020) pre-train variants of the transformer-based model by maximizing the consistency between the context of the mentions and the corresponding entities. Those entity representations suffer from a cold-start problem that they cannot link mentions to unseen entities. Another line of work is to generate entity representations using entity textual info"
2021.emnlp-main.205,Q17-1028,0,0.0220008,"t the In-KB accuracy in Table 2 and observe that MuVER out-performs BLINK on all datasets except the recall@100 on WNED-WIKI. 4 Related Work Representing each entity with a fixed-sized vector has been a common approach in Entity Linking. Ganea and Hofmann (2017) defines a word-entity conditional distribution and samples positive words 2 https://github.com/facebookresearch/ BLINK from it. The representations of those positive words aim to approximate the entity embeddings compared with random words. Yamada et al. (2016) models the relatedness between entities into entity representations. NTEE (Yamada et al., 2017) trains entity representations by predicting the relevant entities for a given context in DBPedia abstract corpus. Ling et al. (2020) and Yamada et al. (2020) pre-train variants of the transformer-based model by maximizing the consistency between the context of the mentions and the corresponding entities. Those entity representations suffer from a cold-start problem that they cannot link mentions to unseen entities. Another line of work is to generate entity representations using entity textual information, such as entity descriptions. Logeswaran et al. (2019) introduces an EL dataset in the z"
2021.emnlp-main.205,N18-1071,0,0.0548116,"Missing"
2021.emnlp-main.338,N19-1423,0,0.185995,"gnment put and output spaces over languages. For example, tools (Tiedemann et al., 2014; Zhang et al., 2019) the universal dependency project (McDonald et al., (thanks to the universal word forms). 2013) constructs a universal output space for cross• we don’t really need to perform the reordering lingual dependency parsers, and cross-lingual word action. Instead, the correct order can be implicrepresentation learning algorithms helps aligning itly encoded by multi-task learning: word order word forms of different languages (Conneau et al., information accesses the model as a supervision 2017; Devlin et al., 2019). signal. Beyond word form, word order is another imThe separation of reordering module and structured portant factor in cross-lingual structured prediction prediction module provides a new way to both ex∗ This work was conducted when Tao Ji was interning at plore and transfer order information. Alibaba DAMO Academy. 1 We suggest a distillation framework (Hinton https://github.com/AntNLP/zero-shot-structuredprediction. et al., 2015) for learning the reordering module. 4109 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4109–4120 c November 7–11, 2"
2021.emnlp-main.338,D11-1006,0,0.116698,"Missing"
2021.emnlp-main.338,P18-2077,0,0.0542468,"Missing"
2021.emnlp-main.338,P19-1311,0,0.0114392,"rom a task-generic, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the in"
2021.emnlp-main.338,D17-1302,0,0.0291786,"c, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the intersection with the"
2021.emnlp-main.338,P13-2017,0,0.0181681,"Missing"
2021.emnlp-main.338,D15-1039,0,0.0427418,"Missing"
2021.emnlp-main.338,N19-1385,0,0.0512246,"ur experiments, we will select one of the re- ing blocks are directly obtained from order objec−→ ←− ordering signals to train the model and compare it tive (e.g., M and M ). Since we jointly perform with each other. In addition, we use single-head reordering and structured prediction. The data for self-attention to reduce computation because pre- learning word order is constrained by the corpus liminary experiments show that multiple heads are size of structured prediction task (e.g., treebanks). not helpful for reordering blocks. Michel et al. On the other hand, there are massive unlabelled (2019) has also shown that replacing multi-head sentences which can help build a more powerful with single-head does not hurt performance. reordering module. To use those unlabelled data, We cross stack reordering blocks with original one challenge is that directly feeding them into the Transformer blocks to build the complete encoder joint learning model could be problematic since the (Figure 3). The reordering blocks estimate prob- severe imbalance between structured prediction sigability p(os |ωs ) by learning linear word order on nals and reordering signals would make the model the source langua"
2021.emnlp-main.338,P15-2040,0,0.0128549,"al performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the intersection with the target language’s and Yi Gao for their comments on writing. This word order. Tiedemann and Agic (2016); Wang and Eisner (2016, 2018a) create a"
2021.emnlp-main.338,K17-3009,0,0.0255242,"and three structured prediction models on the train set of Universal Dependencies (UD) English-EWT treebank (v2.2) (Nivre et al., 2018). We use the development set and test set of the UD English-EWT treebank to validate source language performance. Following Ahmad et al. (2019a)’s setup, we take 30 other languages as target languages, and use the development set and test set of their treebanks to evaluate target languages performance. For the reordering model, a Base train set is UD English, and an Extra set is automatically annotated raw texts (Ginter et al., 2017) generated by UDPipe v2.0 (Straka and Straková, 2017) from CommonCrawl and Wikipedia. Each sentence is automatic tokenization and syntactic annotations (include UPOS). The hyperparameters we used in word reordering task and downstream tasks are summarized in Appendix B. The statistics of the UD treebanks are summarized in Appendix C. 6.1 Performances of the Reordering Model Our Models and Baselines We explore the input features’ influence, order representations , and unlabeled data size to the reordering model. For input features, we utilize MUSE, mBERT, and optional upos features. For order representations, we utilize −→ ←− an undirected (M ) o"
2021.emnlp-main.338,W14-1614,0,0.0271603,"Missing"
2021.emnlp-main.338,Q16-1035,0,0.0206985,"t al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the intersection with the target language’s and Yi Gao for their comments on writing. This word order. Tiedemann and Agic (2016); Wang and Eisner (2016, 2018a) create a high-quality syn- research is funded by the NSFC (62076097) and the 2020 East China Normal University Future Scithetic treebank to increase source data. But data entists and Outstanding Scholars Incubation Proaugmentation requires expert knowledge to build gramme (WLKXJ2020). The corresponding autreebank and extra train time. It does not apply to thors are Tao Ji, Yuanbin Wu and Xiaoling Wang. a larger number of target languages. Annotation projection relies on cross-language annotation mapping using parallel corpus and automatic alignment References (Rasooli and Collins, 201"
2021.emnlp-main.338,Q18-1046,0,0.0122369,"with a bag of words input. The reordering module is distilled from a task-generic, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpfu"
2021.emnlp-main.338,D18-1163,0,0.0139835,"with a bag of words input. The reordering module is distilled from a task-generic, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpfu"
2021.emnlp-main.338,D19-1575,0,0.132284,"rders (e.g., SVO or SOV). To share annotations tured prediction. Current sentence encoders among them, we need to handle word order dis(e.g., RNN, Transformer with position emcrepancies carefully: if a model learned on the beddings) are usually word order sensitive. source language is tightly coupled with the source Even with uniform word form representations language word order, performances on target lan(MUSE, mBERT), word order discrepancies guages could be hurt as their word order could be may hurt the adaptation of models. This paper builds structured prediction models with incompatible (Wang et al., 2019). On the other side, bag-of-words inputs. It introduces a new reif one completely drops word order (e.g., bag-ofordering module to organize words following words), the source language (and target languages) the source language order, which learns taskperformances might be poor as order-sensitive feaspecific reordering strategies from a generaltures could be essential. Trade-offs have been made purpose order predictor model. Experiments by using weak word order information (e.g., relon zero-shot cross-lingual dependency parsing, ative positions instead of absolute positions (AhPOS tagging, and"
2021.emnlp-main.338,D19-1092,0,0.0263429,"Missing"
2021.emnlp-main.339,D16-1211,0,0.0132871,"so we can pack them into the batch dimension to obtain an O(1) training complexity. Hence, USE can uniformly extract full structure features efficiently. Comparing to Previous Encoders We divide previous work into three encoding methods: top-k, stack-LSTM, and binary vector. Top-k methods (Chen and Manning, 2014; Weiss et al., 2015) capture the conjunction of only few 1∼3 in-structure items. It extracts only partial structural information. Since the feature template is fixed, it is easy (l−1) Since ct contains the complete structural infor- to batchify. Stack-LSTM methods (Dyer et al., 2015; Ballesteros et al., 2016) can efficiently repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree which cannot be treated we obtain a high layer configuration representation as a stack. Besides, Che et al. (2019) point out that by combining these output vectors: its batch computation is very inefficient. Binary (l) (l) (l) (l) (l) (l) ct = MLP(oσ,t ⊕ oβ,t ⊕ oα,t ⊕ oTarc ,t ⊕ oTrel ,t )."
2021.emnlp-main.339,K19-2007,0,0.0213531,"information. Since the feature template is fixed, it is easy (l−1) Since ct contains the complete structural infor- to batchify. Stack-LSTM methods (Dyer et al., 2015; Ballesteros et al., 2016) can efficiently repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree which cannot be treated we obtain a high layer configuration representation as a stack. Besides, Che et al. (2019) point out that by combining these output vectors: its batch computation is very inefficient. Binary (l) (l) (l) (l) (l) (l) ct = MLP(oσ,t ⊕ oβ,t ⊕ oα,t ⊕ oTarc ,t ⊕ oTrel ,t ). Vector methods (Zhang et al., 2017) use two binary vectors to model whether each element is in a σ or 2 Note that we use action embedding matrix A instead of a β. It can efficiently encode some outside parts of X when encoding action list α. 3 stack and buffer but loss the information of inside Similar to Equation 2, we give the formulation for the other data structures in Appendix B. position. 4124 We compare existing"
2021.emnlp-main.339,D14-1082,0,0.0793963,"ng transition states with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods. 1 1 Introduction configuration input classifier output buffer stack s-invariant subtree action s-dependent update action action Figure 1: An overview of our transition-based parser. efficiently. However, challenges appear when we try to have the cake and eat it. For example, traditional template-based methods (Chen and Manning, 2014) are fast, but only encode partial information of structures (e.g., few top items on stacks and buffers). Structure-based networks (e.g., StackRNN (Dyer et al., 2015)) rely on carefully designed network architecture to get a full encoding of structures (actually, they still miss some off-structure information, see our discussions in Section 4.2), but they are usually slow (e.g., not easy to batch). Furthermore, different structures have different ways of update (stacks are first-in-last-serve, buffers are first-in-first-serve), it also takes efforts to design different encoders and ways of fus"
2021.emnlp-main.339,P18-1130,0,0.197097,"ffer, and action list, which means that augmenting their information is helpful. Considering the performance gain and computational cost of adding heads, we finally use a total of 8 structural heads. The parser double attent to the stack, buffer and action list. Lexical Encoder UAS Dev LAS Glove + BiLSTM + Xformer 95.72 95.81 95.84 93.79 93.87 93.93 95.71 95.93 95.99 94.05 94.21 94.28 Bert + finetune M&H20 95.90 95.97 95.78 93.97 94.02 93.74 96.21 96.28 96.11 94.56 94.60 94.33 UAS Test LAS Table 2: Lexical encoder comparison on PTB. M&H20: Mohammadshahi and Henderson (2020). Speed Parser Type Ma et al. (2018) Dozat and Manning (2017) Ji et al. (2019) Zhang et al. (2020) T G G‡ G‡ 183 496 403 466 Our arc-hybrid parser T 918 Table 3: Parsing speed comparison on PTB test set. The ‡ indicates high-order graph-based parsers. independent Glove embeddings (Pennington et al., 2014) in the arc-hybrid system. We learn the context via BiLSTM or Transformer encoder. The results show that encoding context can further improve performance and the Transformer encoder is better than BiLSTM. The second part reports the use of contextual Bert networks (Devlin et al., 2019). The introduction of Bert networks and in p"
2021.emnlp-main.339,2020.findings-emnlp.294,0,0.272388,"d (UAS) In a fair comparison, our three unified structure and labeled attachment scores (LAS). For evaluencoding (USE) parsers all achieve significant imations on PTB, five punctuation symbols (“ ” : , provements on PTB. This demonstrates the benefit .) are excluded, while on UD, we use the official of complete structural information by our unified evaluation script. encoding. Hyper-parameters For structure-invariant part, Secondly, we compare with strong graph-based we directly adopt most parameter settings of Ji parsers. The second part of Table 1 contains two et al. (2019) and Zhang et al. (2020), including first-order parsers and two high-order parsers (in pretrained embeddings, BiLSTM, and CharCNN. the red cell). Our USE parsers beat the first-order For structure-dependent part, we use a total of 8 methods, but underperform the high-order methstructural heads, allocating two each for the stack, ods which capture high-order features by graph buffer and action list, one for the subtree’s edges neural networks and TreeCRF. However, speed exand one for the edges’ labels. Our pre-experiments periments show that USE is about 2 times faster show that stacking 6 layers of USE yields the bes"
2021.emnlp-main.339,W03-3017,0,0.0665993,"Missing"
2021.emnlp-main.339,W04-0308,0,0.0423912,"Missing"
2021.emnlp-main.339,J08-4003,0,0.0344169,"contain its lexical form and part-of-speech tag, while its structure-dependent view indicates that w is now sitting on the buffer and its distance to the buffer head is p. When w is detached from buffer and attached to the stack, its structure-dependent ∗ This work was conducted when Tao Ji was interning at view will switch to “sitting on the stack” while its Alibaba DAMO Academy. 1 https://github.com/AntNLP/trans-dep-parser. structure-invariant view stay unchanged. A unified 4121 Transition systems have been successfully applied in many fields of NLP, especially parsing (dependency parsing (Nivre, 2008), constituent parsing (Watanabe and Sumita, 2015), and semantic parsing (Yin and Neubig, 2018)). Basically, a transition system takes a series of actions which attach or detach some items (e.g., sentence words, intermediate outputs) to or from some structures (e.g., stacks, buffers, partial trees). Given a set of action series, a classifier is trained to predict the next action given a current configuration of structures in the transition system. The performances of the final system strongly depend on how well the classifier encodes those transition system configurations. Ideally, a good confi"
2021.emnlp-main.339,D19-1145,0,0.124754,"ing completeness and efficiency, we find that with structure indicators, it is relatively easy to encode a structure completely: one only needs to decompose the structure into identifiable subparts. In fact, we can use them to track some parts of structures which are not revealed in previous work (e.g., words have been popped out from stacks). It runs in the same manner as templated-based models, thus the decoding efficiency is guaranteed. We also note that using structure indicator is different from existing ways to include structure information into neural network models (Shaw et al., 2018; Wang et al., 2019; Shiv and Quirk, 2019): it encodes dynamical structures (changing with transition system running) rather than static structures (e.g., fixed parse trees). We can easily implement the unified structure encoding with existing multi-head attention networks (MHA, (Vaswani et al., 2017)). It is also easy to fuse encodings of different structures with multilayer MHA. We conduct experiments on the English Penn Treebank 3.0 and Universal Dependencies v2.2, show that the unified structure encoder is able to help us achieving state-of-the-art transitionbased parser (even competitive to the best graphba"
2021.emnlp-main.339,P15-1113,0,0.0211638,"of-speech tag, while its structure-dependent view indicates that w is now sitting on the buffer and its distance to the buffer head is p. When w is detached from buffer and attached to the stack, its structure-dependent ∗ This work was conducted when Tao Ji was interning at view will switch to “sitting on the stack” while its Alibaba DAMO Academy. 1 https://github.com/AntNLP/trans-dep-parser. structure-invariant view stay unchanged. A unified 4121 Transition systems have been successfully applied in many fields of NLP, especially parsing (dependency parsing (Nivre, 2008), constituent parsing (Watanabe and Sumita, 2015), and semantic parsing (Yin and Neubig, 2018)). Basically, a transition system takes a series of actions which attach or detach some items (e.g., sentence words, intermediate outputs) to or from some structures (e.g., stacks, buffers, partial trees). Given a set of action series, a classifier is trained to predict the next action given a current configuration of structures in the transition system. The performances of the final system strongly depend on how well the classifier encodes those transition system configurations. Ideally, a good configuration encoder should encode transition system"
2021.emnlp-main.339,P15-1032,0,0.0233944,"tructure). Vaswani et al. (2017) noted that a multi-head attention layer has a constant number (O(1)) of sequentially executed operations, which means that efficient GPU-based computing is possible. In training, the USE calculations at different moments are independent of each other, so we can pack them into the batch dimension to obtain an O(1) training complexity. Hence, USE can uniformly extract full structure features efficiently. Comparing to Previous Encoders We divide previous work into three encoding methods: top-k, stack-LSTM, and binary vector. Top-k methods (Chen and Manning, 2014; Weiss et al., 2015) capture the conjunction of only few 1∼3 in-structure items. It extracts only partial structural information. Since the feature template is fixed, it is easy (l−1) Since ct contains the complete structural infor- to batchify. Stack-LSTM methods (Dyer et al., 2015; Ballesteros et al., 2016) can efficiently repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree w"
2021.emnlp-main.339,D17-1175,0,0.01276,"repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree which cannot be treated we obtain a high layer configuration representation as a stack. Besides, Che et al. (2019) point out that by combining these output vectors: its batch computation is very inefficient. Binary (l) (l) (l) (l) (l) (l) ct = MLP(oσ,t ⊕ oβ,t ⊕ oα,t ⊕ oTarc ,t ⊕ oTrel ,t ). Vector methods (Zhang et al., 2017) use two binary vectors to model whether each element is in a σ or 2 Note that we use action embedding matrix A instead of a β. It can efficiently encode some outside parts of X when encoding action list α. 3 stack and buffer but loss the information of inside Similar to Equation 2, we give the formulation for the other data structures in Appendix B. position. 4124 We compare existing work with our USE encoder in terms of the coverage of structure features and GPU computing friendly (in Figure 4). Overall, USE does not lose any structural information and more efficient than previous feature ex"
2021.findings-acl.442,N18-3011,0,0.046304,"Missing"
2021.findings-acl.442,W13-2322,0,0.0466126,"he original method without supervision on attention. As the decoder applies multi-head attention, we design the SA approach, in which the attention distributions of all heads are averaged to compute the attention loss. In this way, we consider the multi-head attention as a supervised attention channel. The SMA approach is designed as in Section 4.3, in which only the first head is a supervised attention channel. In SCE and CE approaches, we used SCE and CE loss function to supervise the attention, respectively. 5.2 AMR-to-text Generation Task and Model : Abstract meaning representation (AMR) (Banarescu et al., 2013) is a semantic graph representation that is independent of the syntactic realization of a sentence. In the graph, nodes represent concepts and edges represent semantic relations between the concepts. AMR-to-text generation is to generate sentences from AMR graphs. We use the AMR dataset LDC2015E86, which contains 16,833 training samples, 1368 development samples, and 1371 test samples. We use the model2 of Mager et al. (2020) on this task, which is a GPT-2 (Radford et al., 2019) model with fine-tuning. Aligner: We apply lemma matching to build the attention supervision as shown in Figure 1. Th"
2021.findings-acl.442,J93-2003,0,0.101517,"ning ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the other hand, it is much easier to obtain a candidate set containing both (8) and (10) and be rather confident that the ideal alignment is in the set. For different tasks, both EM-based algorithms (Brown et al., 1993; Pourdamghani et al., 2014) or rule-based methods (Flanigan et al., 2014) can be used to obtain such ambiguous alignments. However, little work has discussed making use of ambiguous labels for supervised attention. We investigate the generalized supervised attention (GSA), where the supervision signal aligns a target word to multiple possible source items (named the quasi alignment), although only a subset of the items are the true alignment targets. The multiple source items are named candidate set of the quasi alignment. A generalized supervised attention framework is built for various text"
2021.findings-acl.442,P16-1184,0,0.0197889,"ion supervision and design the Summation Cross-Entropy to deal with the ambiguity in quasi alignments. Learning with ambiguous labels has been widely studied, in which the true label is not precisely annotated but in a candidate label set. In crosslingual Part-of-Speech, annotations are derived for low resource languages from cross-language projection, which results in partial or uncertain labels. To solve this problem, T¨ackstr¨om et al. (2013) proposed a partially observed conditional random field (CRF) (Lafferty et al., 2001) method, Wisniewski et al. (2014) made a history-based model, and Buys and Botha (2016) proposed an HMM-based model. SCE is designed for training attention weights using ambiguous labels. Xu et al. (2020) also study learning from ambiguous labels (called partial label learning) in classification tasks. Their method is based on constructing similar and dissimilar pairs of samples. However, supervised attention is not a traditional classification problem. The label spaces are various in different samples, making it difficult to construct similar pairs. Thus, the method is not suitable for GSA. 3 3.1 Basic Model Encoder-decoder Model with Attention Encoder-decoder models, including"
2021.findings-acl.442,D14-1179,0,0.0473474,"Missing"
2021.findings-acl.442,P19-1042,0,0.018163,"Comparison of the test attention of SCE structure. least two alignments. For data-to-text and AMR-to-text generation, SMA outperforms SA. On the other hand, SA performs better than SMA for text summarization. One possible reason is that the summarization dataset has much higher alignment coverage and multialignment coverage and the alignment accuracy may also be higher; consequently, supervised attention works so well that automatic attention becomes unnecessary or even distracting. 5.5 lated using the one-tailed sign test with bootstrap resampling on the test set of all three tasks following Chollampatt et al. (2019): • For data-to-text, we compare the Rouge-L score of SMA-SCE to the result of SA-CE. • For AMR-to-text, we compare the BLEU score of SMA-SCE to the result of SA-CE. • For summarization, we compare the Rouge-L score of SA-SCE to the result of SA-CE. Significance Test To assess the evidence of significance, we perform significance tests on GSA. The p-value is calcuThe p-value results are shown in Table 4, which show that the improvements are significant. 4997 Task P-value Data-to-Text 6.5489e-12 Amr-to-Text 5.5795e-10 Summarization 3.925e-5 Generated Matching 1 Matching 2 Matching 3 Matching 4"
2021.findings-acl.442,P14-1134,0,0.0254297,"tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the other hand, it is much easier to obtain a candidate set containing both (8) and (10) and be rather confident that the ideal alignment is in the set. For different tasks, both EM-based algorithms (Brown et al., 1993; Pourdamghani et al., 2014) or rule-based methods (Flanigan et al., 2014) can be used to obtain such ambiguous alignments. However, little work has discussed making use of ambiguous labels for supervised attention. We investigate the generalized supervised attention (GSA), where the supervision signal aligns a target word to multiple possible source items (named the quasi alignment), although only a subset of the items are the true alignment targets. The multiple source items are named candidate set of the quasi alignment. A generalized supervised attention framework is built for various text generation tasks with alignment relationships between target words and so"
2021.findings-acl.442,N19-1357,0,0.01962,"captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention was considered (Liu et al., 2016; Mi et al., 2016; Kamigaito et al., 2017; Nguyen et al., 2018; Nguyen and Nguyen, 2018), which shows that human knowledge is helpful for guiding the learning process of attention models. Previous work on supervised attention assumes access to ideal alignments. Unfortunately, obtaining ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment co"
2021.findings-acl.442,I17-2002,0,0.0507966,"., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention was considered (Liu et al., 2016; Mi et al., 2016; Kamigaito et al., 2017; Nguyen et al., 2018; Nguyen and Nguyen, 2018), which shows that human knowledge is helpful for guiding the learning process of attention models. Previous work on supervised attention assumes access to ideal alignments. Unfortunately, obtaining ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same,"
2021.findings-acl.442,N19-1238,0,0.015392,"est attention of SCE structure. least two alignments. For data-to-text and AMR-to-text generation, SMA outperforms SA. On the other hand, SA performs better than SMA for text summarization. One possible reason is that the summarization dataset has much higher alignment coverage and multialignment coverage and the alignment accuracy may also be higher; consequently, supervised attention works so well that automatic attention becomes unnecessary or even distracting. 5.5 lated using the one-tailed sign test with bootstrap resampling on the test set of all three tasks following Chollampatt et al. (2019): • For data-to-text, we compare the Rouge-L score of SMA-SCE to the result of SA-CE. • For AMR-to-text, we compare the BLEU score of SMA-SCE to the result of SA-CE. • For summarization, we compare the Rouge-L score of SA-SCE to the result of SA-CE. Significance Test To assess the evidence of significance, we perform significance tests on GSA. The p-value is calcuThe p-value results are shown in Table 4, which show that the improvements are significant. 4997 Task P-value Data-to-Text 6.5489e-12 Amr-to-Text 5.5795e-10 Summarization 3.925e-5 Generated Matching 1 Matching 2 Matching 3 Matching 4"
2021.findings-acl.442,P18-2027,0,0.0126983,"arget contain the same information. We report the details of model structures and hyper-parameters in the appendix. 5.1 Data-to-text Generation Task and Model : We consider the Abstract GENeration DAtaset (AGENDA) (Ammar et al., 2018), which contains pairs of a literature abstract and a knowledge graph extracted from the abstract. The nodes in the knowledge graphs are entity types, such as “Task” and “Method”. The edges are the relations between different entities, including “COMPARE”, “PART-OF”, and so on. We use the training, development, and test splits of 38,720/1000/1000, as Ammar et al. (2018) does. We use GraphWriter1 (Koncel-Kedziorski et al., 2019) on this task. The encoder of this model is a graph transformer and the decoder is an RNN decoder with attention and copying mechanism. More detail is introduced in the appendix. Aligner: The source items of this task include entities and relations, as shown in Figure 3. We use our string matching aligner to extract the alignments from target words to the source entities and extend our aligner for the alignments of relations, such as aligning target words “use” and “apply” to source relation “USED-FOR”. For the details of 1 https://git"
2021.findings-acl.442,C16-1291,0,0.276334,"lignments, which specify candidate sets of alignments and are much easier to obtain than ideal alignments. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the"
2021.findings-acl.442,P18-1138,0,0.0212933,"to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention"
2021.findings-acl.442,D19-1387,0,0.0234071,"s. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments"
2021.findings-acl.442,D15-1166,0,0.432645,"ed Attention method (GSA) based on quasi alignments, which specify candidate sets of alignments and are much easier to obtain than ideal alignments. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of"
2021.findings-acl.442,2020.acl-main.167,0,0.0305511,"Missing"
2021.findings-acl.442,D16-1249,0,0.0571723,"and design the Summation Cross-Entropy to deal with the ambiguity in quasi alignments. Learning with ambiguous labels has been widely studied, in which the true label is not precisely annotated but in a candidate label set. In crosslingual Part-of-Speech, annotations are derived for low resource languages from cross-language projection, which results in partial or uncertain labels. To solve this problem, T¨ackstr¨om et al. (2013) proposed a partially observed conditional random field (CRF) (Lafferty et al., 2001) method, Wisniewski et al. (2014) made a history-based model, and Buys and Botha (2016) proposed an HMM-based model. SCE is designed for training attention weights using ambiguous labels. Xu et al. (2020) also study learning from ambiguous labels (called partial label learning) in classification tasks. Their method is based on constructing similar and dissimilar pairs of samples. However, supervised attention is not a traditional classification problem. The label spaces are various in different samples, making it difficult to construct similar pairs. Thus, the method is not suitable for GSA. 3 3.1 Basic Model Encoder-decoder Model with Attention Encoder-decoder models, including"
2021.findings-acl.442,K16-1028,0,0.203015,"and design the Summation Cross-Entropy to deal with the ambiguity in quasi alignments. Learning with ambiguous labels has been widely studied, in which the true label is not precisely annotated but in a candidate label set. In crosslingual Part-of-Speech, annotations are derived for low resource languages from cross-language projection, which results in partial or uncertain labels. To solve this problem, T¨ackstr¨om et al. (2013) proposed a partially observed conditional random field (CRF) (Lafferty et al., 2001) method, Wisniewski et al. (2014) made a history-based model, and Buys and Botha (2016) proposed an HMM-based model. SCE is designed for training attention weights using ambiguous labels. Xu et al. (2020) also study learning from ambiguous labels (called partial label learning) in classification tasks. Their method is based on constructing similar and dissimilar pairs of samples. However, supervised attention is not a traditional classification problem. The label spaces are various in different samples, making it difficult to construct similar pairs. Thus, the method is not suitable for GSA. 3 3.1 Basic Model Encoder-decoder Model with Attention Encoder-decoder models, including"
2021.findings-acl.442,C18-1193,0,0.0855731,"ework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention was considered (Liu et al., 2016; Mi et al., 2016; Kamigaito et al., 2017; Nguyen et al., 2018; Nguyen and Nguyen, 2018), which shows that human knowledge is helpful for guiding the learning process of attention models. Previous work on supervised attention assumes access to ideal alignments. Unfortunately, obtaining ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the ot"
2021.findings-acl.442,D14-1048,0,0.0173769,"s is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the other hand, it is much easier to obtain a candidate set containing both (8) and (10) and be rather confident that the ideal alignment is in the set. For different tasks, both EM-based algorithms (Brown et al., 1993; Pourdamghani et al., 2014) or rule-based methods (Flanigan et al., 2014) can be used to obtain such ambiguous alignments. However, little work has discussed making use of ambiguous labels for supervised attention. We investigate the generalized supervised attention (GSA), where the supervision signal aligns a target word to multiple possible source items (named the quasi alignment), although only a subset of the items are the true alignment targets. The multiple source items are named candidate set of the quasi alignment. A generalized supervised attention framework is built for various text generation tasks with align"
2021.findings-acl.442,P19-1195,0,0.0153126,"est attention of SCE structure. least two alignments. For data-to-text and AMR-to-text generation, SMA outperforms SA. On the other hand, SA performs better than SMA for text summarization. One possible reason is that the summarization dataset has much higher alignment coverage and multialignment coverage and the alignment accuracy may also be higher; consequently, supervised attention works so well that automatic attention becomes unnecessary or even distracting. 5.5 lated using the one-tailed sign test with bootstrap resampling on the test set of all three tasks following Chollampatt et al. (2019): • For data-to-text, we compare the Rouge-L score of SMA-SCE to the result of SA-CE. • For AMR-to-text, we compare the BLEU score of SMA-SCE to the result of SA-CE. • For summarization, we compare the Rouge-L score of SA-SCE to the result of SA-CE. Significance Test To assess the evidence of significance, we perform significance tests on GSA. The p-value is calcuThe p-value results are shown in Table 4, which show that the improvements are significant. 4997 Task P-value Data-to-Text 6.5489e-12 Amr-to-Text 5.5795e-10 Summarization 3.925e-5 Generated Matching 1 Matching 2 Matching 3 Matching 4"
2021.findings-acl.442,Q13-1001,0,0.0722298,"Missing"
2021.findings-acl.442,D17-1239,0,0.0353923,"Missing"
2021.findings-acl.442,D14-1187,0,0.0468567,"Missing"
D16-1073,D10-1117,0,0.505202,"Missing"
D16-1073,D14-1082,0,0.0285852,"ReLU activation function. We have two versions of weight matrix Wdir for the direction dir being left and right respectively. Outputs (CHILD or DECISION) Softmax Layer: … p = Sof tmax(W f ) W Hidden Layer: f = ReLU (Wdir [vh ; vval ]) Wdir Continous Representation: [vh ; vval ] Inputs: Valency Head POS Tag f (h, dir, val) = ReLU(Wdir [vh ; vval ]) Figure 1: Structure of the neural network. Both CHILD and DECISION use the same architecture for the calculation of distributions. based dependency parsing. Stenetorp (2013) applied recursive neural networks to transitional based dependency parsing. Chen and Manning (2014) built a neural network based parser with dense features instead of sparse indicator features. Dyer et al. (2015) proposed a stack long short-term memory approach to supervised dependency parsing. To our knowledge, our work is the first attempt to incorporate neural networks into a generative grammar for unsupervised dependency parsing. 3 Neural DMV In this section, we introduce our neural based grammar induction approach. We describe the model in section 3.1 and the learning method in section 3.2. 3.1 Model Our model is based on the DMV model (section 2.1), except that the CHILD and DECISION"
D16-1073,N09-1009,0,0.69464,"ce and Technology ShanghaiTech University, Shanghai, China Abstract Previous work on unsupervised dependency parsing is mainly based on the dependency model with valence (DMV) (Klein and Manning, 2004) and its extension (Headden III et al., 2009; Gillenwater et al., 2010). To effectively learn the DMV model for better parsing accuracy, a variety of inductive biases and handcrafted features have been proposed to incorporate prior information into learning. One useful type of prior information is that there exist correlations between the parameters of grammar rules involving different POS tags. Cohen and Smith (2009; 2010) employed special prior distributions to encourage learning of correlations between POS tags. Berg-Kirkpatrick et al. (2010) encoded the relations between POS tags using manually designed features. Unsupervised dependency parsing aims to learn a dependency grammar from text annotated with only POS tags. Various features and inductive biases are often used to incorporate prior knowledge into learning. One useful type of prior information is that there exist correlations between the parameters of grammar rules involving different POS tags. Previous work employed manually designed features"
D16-1073,P15-1033,0,0.0225246,"tively. Outputs (CHILD or DECISION) Softmax Layer: … p = Sof tmax(W f ) W Hidden Layer: f = ReLU (Wdir [vh ; vval ]) Wdir Continous Representation: [vh ; vval ] Inputs: Valency Head POS Tag f (h, dir, val) = ReLU(Wdir [vh ; vval ]) Figure 1: Structure of the neural network. Both CHILD and DECISION use the same architecture for the calculation of distributions. based dependency parsing. Stenetorp (2013) applied recursive neural networks to transitional based dependency parsing. Chen and Manning (2014) built a neural network based parser with dense features instead of sparse indicator features. Dyer et al. (2015) proposed a stack long short-term memory approach to supervised dependency parsing. To our knowledge, our work is the first attempt to incorporate neural networks into a generative grammar for unsupervised dependency parsing. 3 Neural DMV In this section, we introduce our neural based grammar induction approach. We describe the model in section 3.1 and the learning method in section 3.2. 3.1 Model Our model is based on the DMV model (section 2.1), except that the CHILD and DECISION probabilities are calculated through two neural networks. We do not compute the ROOT probabilities using a neural"
D16-1073,P11-2003,0,0.0203824,"II (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. Rasooli and Faili (2012) proposed a transition based unsupervised dependency parser together with ""baby-step"" training (Spitkovsky et al., 2010) to improve parsing accuracy. Le and Zuidema (2015) proposed a complicated reranking based unsupervised dependency parsing system and achieved the state-of-the-art performance on the Penn Treebank dataset. 2.4 Neural based Supervised Dependency Parser There exist several previous approaches on using neural networks for supervised dependency parsing. Garg and Henderson (2011) proposed a Temporal Restricted Boltzmann Machine to do transition The full architecture of the neural network is shown in Figure 1. First, we represent each head tag h as a d dimensional vector vh ∈ Rd , represent each value of valence val as a d0 dimensional vector 0 vval ∈ Rd . We concatenate vh and vval as the input embedding vector. Then we map the input layer to a hidden layer with weight matrix Wdir through a ReLU activation function. We have two versions of weight matrix Wdir for the direction dir being left and right respectively. Outputs (CHILD or DECISION) Softmax Layer: … p = Sof t"
D16-1073,W12-1909,0,0.18872,"Missing"
D16-1073,P10-2036,0,0.131163,"ons shows the results. It can be seen that our approach with Viterbi EM significantly outperforms the EM and viterbi EM baselines and also outperforms the two previous approaches. 4.3 WSJ 53.1 53.3 57.0 52.5 57.6 55.7 64.4 65.8 Table 2: Comparison of recent unsupervised dependency parsing systems. Basic setup means learning from POS tags with sentences of length ≤ 10 and punctuation stripped off. Extra information may contain punctuations, longer sentences, lexiResults on the extended DMV model We directly apply our neural approach to learning the extended DMV model (Headden III et al., 2009; Gillenwater et al., 2010) (with the maximum valence value set to 2 for both CHILD and DECISION rules). As shown in Table 2, we achieve comparable accuracy with recent state-of-the-art systems. If we initialize our model with the grammar learned by Tu and Honavar (2012), the accuracy of our approach can be further improved. Most of the recent state-of-the-art systems employ more complicated models and learning algorithms. For example, Spitkovsky et al. (2013) take several grammar induction techniques as modules and connect them in various ways; Le and Zuidema (2015) use a neural-based supervised parser and reranker tha"
D16-1073,N09-1012,0,0.631817,"Missing"
D16-1073,P04-1061,0,0.920814,"outperforms the previous approaches that also utilize POS tag correla763 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 763–771, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics tions, and achieves a comparable result with recent state-of-the-art grammar induction systems. On the datasets of eight additional languages, our approach is able to achieve better performance than the baseline methods without any parameter tuning. 2 2.1 Related work Dependency Model with Valence The dependency model with valence (DMV) (Klein and Manning, 2004) is the first model to outperform the left-branching baseline in unsupervised dependency parsing of English. The DMV model is a generative model of a sentence and its parse tree. It generates a dependency parse from the root in a recursive top-down manner. At each step, a decision is first made as to whether a new child POS tag shall be generated from the current head tag; if the decision is yes, then a new child POS tag is sampled; otherwise, the existing child tags are recursively visited. There are three types of grammar rules in the model: CHILD, DECISION and ROOT, each with a set of multi"
D16-1073,N15-1067,0,0.73857,"In the work of Tu and Honavar (2012), unambiguity of parse trees is incorporated into the training objective function of DMV to obtain a better performance. 2.3 Other Approaches to Unsupervised Dependency Parsing There are many other approaches to unsupervised dependency parsing that are not based on DMV. Daumé III (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. Rasooli and Faili (2012) proposed a transition based unsupervised dependency parser together with ""baby-step"" training (Spitkovsky et al., 2010) to improve parsing accuracy. Le and Zuidema (2015) proposed a complicated reranking based unsupervised dependency parsing system and achieved the state-of-the-art performance on the Penn Treebank dataset. 2.4 Neural based Supervised Dependency Parser There exist several previous approaches on using neural networks for supervised dependency parsing. Garg and Henderson (2011) proposed a Temporal Restricted Boltzmann Machine to do transition The full architecture of the neural network is shown in Figure 1. First, we represent each head tag h as a d dimensional vector vh ∈ Rd , represent each value of valence val as a d0 dimensional vector 0 vval"
D16-1073,W12-0701,0,0.0157412,"nd CHILD parameters. These two approaches both utilize the correlations between POS tags to obtain better probability estimation of grammar rules involving such correlated POS tags. In the work of Tu and Honavar (2012), unambiguity of parse trees is incorporated into the training objective function of DMV to obtain a better performance. 2.3 Other Approaches to Unsupervised Dependency Parsing There are many other approaches to unsupervised dependency parsing that are not based on DMV. Daumé III (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. Rasooli and Faili (2012) proposed a transition based unsupervised dependency parser together with ""baby-step"" training (Spitkovsky et al., 2010) to improve parsing accuracy. Le and Zuidema (2015) proposed a complicated reranking based unsupervised dependency parsing system and achieved the state-of-the-art performance on the Penn Treebank dataset. 2.4 Neural based Supervised Dependency Parser There exist several previous approaches on using neural networks for supervised dependency parsing. Garg and Henderson (2011) proposed a Temporal Restricted Boltzmann Machine to do transition The full architecture of the neural"
D16-1073,N10-1116,0,0.0289921,"timation of grammar rules involving such correlated POS tags. In the work of Tu and Honavar (2012), unambiguity of parse trees is incorporated into the training objective function of DMV to obtain a better performance. 2.3 Other Approaches to Unsupervised Dependency Parsing There are many other approaches to unsupervised dependency parsing that are not based on DMV. Daumé III (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. Rasooli and Faili (2012) proposed a transition based unsupervised dependency parser together with ""baby-step"" training (Spitkovsky et al., 2010) to improve parsing accuracy. Le and Zuidema (2015) proposed a complicated reranking based unsupervised dependency parsing system and achieved the state-of-the-art performance on the Penn Treebank dataset. 2.4 Neural based Supervised Dependency Parser There exist several previous approaches on using neural networks for supervised dependency parsing. Garg and Henderson (2011) proposed a Temporal Restricted Boltzmann Machine to do transition The full architecture of the neural network is shown in Figure 1. First, we represent each head tag h as a d dimensional vector vh ∈ Rd , represent each val"
D16-1073,D13-1204,0,0.185237,"ions, longer sentences, lexiResults on the extended DMV model We directly apply our neural approach to learning the extended DMV model (Headden III et al., 2009; Gillenwater et al., 2010) (with the maximum valence value set to 2 for both CHILD and DECISION rules). As shown in Table 2, we achieve comparable accuracy with recent state-of-the-art systems. If we initialize our model with the grammar learned by Tu and Honavar (2012), the accuracy of our approach can be further improved. Most of the recent state-of-the-art systems employ more complicated models and learning algorithms. For example, Spitkovsky et al. (2013) take several grammar induction techniques as modules and connect them in various ways; Le and Zuidema (2015) use a neural-based supervised parser and reranker that make use of high-order features and lexical information. We expect that the performance of our approach can be further improved when these more advanced techniques are incorporated. 4.4 WSJ10 Systems with Basic Setup EVG (Headden III et al., 2009) 65.0 TSG-DMV (Blunsom and Cohn, 2010) 65.9 PR-S (Gillenwater et al., 2010) 64.3 UR-A E-DMV (Tu and Honavar, 2012) 71.4 Neural E-DMV 69.7 Neural E-DMV (Good Init) 72.5 Systems Using Extra"
D16-1073,D12-1121,1,0.909992,"grammar rules. There have been many more advanced learning algorithms of the DMV model beyond the basic EM algorithm. In the work of Cohen and Smith (2008), a logistic normal prior was used in the DMV model to capture the similarity between POS tags. In the work of Berg-Kirkpatrick et al. (2010), features that group various morphological variants of nouns and verbs are used to predict the DECISION and CHILD parameters. These two approaches both utilize the correlations between POS tags to obtain better probability estimation of grammar rules involving such correlated POS tags. In the work of Tu and Honavar (2012), unambiguity of parse trees is incorporated into the training objective function of DMV to obtain a better performance. 2.3 Other Approaches to Unsupervised Dependency Parsing There are many other approaches to unsupervised dependency parsing that are not based on DMV. Daumé III (2009) proposed a stochastic search based method to do unsupervised Shift-Reduce transition parsing. Rasooli and Faili (2012) proposed a transition based unsupervised dependency parser together with ""baby-step"" training (Spitkovsky et al., 2010) to improve parsing accuracy. Le and Zuidema (2015) proposed a complicated"
D16-1073,N10-1083,0,\N,Missing
D17-1171,N10-1083,0,0.125147,"Missing"
D17-1171,D10-1117,0,0.267404,"Missing"
D17-1171,C96-1058,0,0.43038,"Missing"
D17-1171,W12-1909,0,0.0462715,"Missing"
D17-1171,P15-1133,0,0.263494,"in sentences from unlabeled data, is a very challenging task in natural language processing. Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by Klein and Manning (2004). Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al., 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016). Besides generative approaches, Grave and Elhadad (2015) proposed an unsupervised discrim∗ This work was supported by the National Natural Science Foundation of China (61503248). Conditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction. There are two components of this model: an encoder and a decoder. The encoder is a globally normalized feature-rich CRF model predicting the conditional distribution of the latent structure given the observed structured input. The decoder of the model is a generative model generating a transformation of the structured input from the latent structure. Ammar e"
D17-1171,D16-1073,1,0.909968,"parsing, which aims to discover syntactic structures in sentences from unlabeled data, is a very challenging task in natural language processing. Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by Klein and Manning (2004). Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al., 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016). Besides generative approaches, Grave and Elhadad (2015) proposed an unsupervised discrim∗ This work was supported by the National Natural Science Foundation of China (61503248). Conditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction. There are two components of this model: an encoder and a decoder. The encoder is a globally normalized feature-rich CRF model predicting the conditional distribution of the latent structure given the observed structured input. The decoder of the model is a generative model generating a transformation o"
D17-1171,P04-1061,0,0.9492,"tic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches. 1 Introduction Unsupervised dependency parsing, which aims to discover syntactic structures in sentences from unlabeled data, is a very challenging task in natural language processing. Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence (DMV) introduced by Klein and Manning (2004). Many approaches have been proposed to enhance these generative models, for example, by designing advanced Bayesian priors (Cohen et al., 2008), representing dependencies with features (Berg-Kirkpatrick et al., 2010), and representing discrete tokens with continuous vectors (Jiang et al., 2016). Besides generative approaches, Grave and Elhadad (2015) proposed an unsupervised discrim∗ This work was supported by the National Natural Science Foundation of China (61503248). Conditional random field autoencoder (Ammar et al., 2014) is a new framework for unsupervised structured prediction. There a"
D17-1171,D07-1015,0,0.0737862,"Missing"
D17-1171,N15-1067,0,0.551122,"Missing"
D17-1171,P05-1012,0,0.364621,"oughout this paper, we ˆ = x. set x The encoder in our model is a log-linear model represented by a first-order dependency parser. The score of a dependency tree can be factorized as the sum of scores of its dependencies. For each dependency arc (x, i, j), where i and j are the indices of the head and child of the dependency, a feature vector f (x, i, j) is specified. The score of a dependency is defined as the inner product of the feature vector and a weight vector w, n Y i=1 θxˆi |ti ˆ , y given x is The conditional distribution of x ˆ |x) = P (y|x)P (ˆ P (y, x x|y) 2.1.1 Features Following McDonald et al. (2005) and Grave et al. (2015), we define the feature vector of a dependency based on the part-of-speech tags (POS) of the head, child and context words, the direction, and the distance between the head and child of the dependency. The feature template used in our parser is shown in Table 1. 2.1.2 Parsing Given parameters w and θ, we can parse a sentence x by searching for a dependency tree y which has the highest probability P (ˆ x, y|x). y∗ = arg max log P (ˆ x, y|x) y∈Y(x) = arg max T φ(x, i, j) = w f (x, i, j) y∈Y(x) 1639 n X i=1 φ(x, hi , i) + log θxˆi |ti  POSi × dis × dir POSj × dis × dir PO"
D17-1171,D10-1120,0,0.740682,"formance of unsupervised dependency parsing in comparison with EM. Therefore, instead of using negative conditional log likelihood as our objective function, we choose to use negative conditional Viterbi log likelihood, − N X  log  max P (ˆ xi , y|xi ) + λΩ(w) (1) y∈Y(xi ) i=1 where Ω(w) is a L1 regularization term of the encoder parameter w and λ is a hyper-parameter controlling the strength of regularization. To encourage learning of dependency relations that satisfy universal linguistic knowledge, we add a soft constraint on the parse tree based on the universal syntactic rules following Naseem et al. (2010) and Grave et al. (2015). Hence our objective function becomes − N X i=1  log α  max P (ˆ xi , y|xi )Q (xi , y) +λΩ(w) Table 2: Universal linguistic rules taken into account. Q(x, y) = exp X ! 1[(ti → xi ) ∈ R] i where 1[(ti → xi ) ∈ R] is an indicator function of whether dependency ti → xi satisfies one of the universal linguistic rules in R. The universal linguistic rules that we use are shown in Table 2 (Naseem et al., 2010). 2.2.2 Algorithm We apply coordinate descent to minimize the objective function, which alternately updates w and θ. In each optimization step of w, we run two epochs"
D17-1171,D13-1204,0,0.449817,"Missing"
D17-1171,W10-2902,0,0.202628,"is × dir POSi × POSj × POSj+1 × dis × dir VERB → VERB VERB → NOUN VERB → PRON VERB → ADV VERB → ADP ADJ → ADV Table 1: Feature template of a dependency, where i is the index of the head, j is the index of the child, dis = |i − j|, and dir is the direction of the dependency. For projective dependency parsing, we can use Eisners algorithm (1996) to find the best parse in O(n3 ) time. For non-projective dependency parsing, we can use the Chu-Liu/Edmond algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977) to find the best parse in O(n2 ) time. 2.2 Parameter Learning 2.2.1 Objective Function Spitkovsky et al. (2010) shows that Viterbi EM can improve the performance of unsupervised dependency parsing in comparison with EM. Therefore, instead of using negative conditional log likelihood as our objective function, we choose to use negative conditional Viterbi log likelihood, − N X  log  max P (ˆ xi , y|xi ) + λΩ(w) (1) y∈Y(xi ) i=1 where Ω(w) is a L1 regularization term of the encoder parameter w and λ is a hyper-parameter controlling the strength of regularization. To encourage learning of dependency relations that satisfy universal linguistic knowledge, we add a soft constraint on the parse tree based o"
D17-1171,D12-1121,1,0.840344,"Missing"
D17-1176,D10-1117,0,0.508976,"0; Tu and Honavar, 2012). Lexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy. The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013). A major problem with ∗ This work was supported by the National Natural Science Foundation of China (61503248). full lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corpora containing more than 700k sentences; Marecek and Straka (2013) utilized a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Finally, smoothing techniques can be used to reduce the negative impact of data scarcity. One example is Neural DMV (NDMV) (Ji"
D17-1176,P10-2036,0,0.223101,"Missing"
D17-1176,N09-1012,0,0.767629,"Missing"
D17-1176,D16-1073,1,0.826404,"Grammar Induction with Neural Lexicalization and Big Training Data∗ Wenjuan Han, Yong Jiang and Kewei Tu {hanwj, jiangyong ,tukw}@shanghaitech.edu.cn School of Information Science and Technology ShanghaiTech University, Shanghai, China Abstract We study the impact of big models (in terms of the degree of lexicalization) and big data (in terms of the training corpus size) on dependency grammar induction. We experimented with L-DMV, a lexicalized version of Dependency Model with Valence (Klein and Manning, 2004) and L-NDMV, our lexicalized extension of the Neural Dependency Model with Valence (Jiang et al., 2016). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art. 1 Introduction Grammar induction is the task of learning a grammar from a set of unannotated sentences. In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpu"
D17-1176,P04-1061,0,0.910121,"us based on Wikipedia in learning an unlexicalized dependency grammar. Finally, smoothing techniques can be used to reduce the negative impact of data scarcity. One example is Neural DMV (NDMV) (Jiang et al., 2016) which incorporates neural networks into DMV and can automatically smooth correlated grammar rule probabilities. Inspired by this background, we conduct a systematic study regarding the impact of the degree of lexicalization and the training data size on the accuracy of grammar induction approaches. We experimented with a lexicalized version of Dependency Model with Valence (L-DMV) (Klein and Manning, 2004) and our lexicalized extension of NDMV (L-NDMV). We find that L-DMV only benefits from very small degrees of lexicalization and moderate sizes of training corpora. In comparison, L-NDMV can benefit from big training data and lexicalization of greater degrees, especially when it is enhanced with good model initialization. The performance of L-NDMV is competitive with the current state-of-the-art. 2 2.1 Methods Lexicalized DMV We choose to lexicalize an extended version of DMV (Gillenwater et al., 2010). We adopt a sim1683 Proceedings of the 2017 Conference on Empirical Methods in Natural Langua"
D17-1176,N15-1067,0,0.662554,"s. Second, we train the same neural network across EM iterations without resetting. More details can be found in the supplementary material. Our algorithm can be seen as an extension of online EM (Liang and Klein, 2009) to accommodate neural network training. 2.3 Model Initialization It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al., 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016). We tested both KM initialization and the following initialization method: we first learn 1684 an unlexicalized DMV using the grammar induction method of Naseem et al. (2010) and use it to parse the training corpus; then, from the parse trees we run maximum likelihood estimation to produce the initial lexicalized model. 3 Experimental Setup For English, we used the BLLIP corpus1 in addition to the regular WSJ corpus in our experiments. Note that the BLLIP corpus is collected from the same news article source as the WSJ corpus, so it is in-domain and is ideal for training"
D17-1176,N09-1069,0,0.034026,"Learning Algorithm: The original NDMV learning method is based on hard-EM and is very time-consuming when applied to L-NDMV with a large training corpus. We propose two improvements to achieve significant speedup. First, at each EM iteration we collect grammar rule counts from a different batch of sentences instead of from the whole training corpus and train the neural network using only these counts. Second, we train the same neural network across EM iterations without resetting. More details can be found in the supplementary material. Our algorithm can be seen as an extension of online EM (Liang and Klein, 2009) to accommodate neural network training. 2.3 Model Initialization It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al., 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016). We tested both KM initialization and the following initialization method: we first learn 1684 an unlexicalized DMV using the grammar induction method of Naseem et al. (2010) and use i"
D17-1176,P13-1028,0,0.0603737,"pported by the National Natural Science Foundation of China (61503248). full lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corpora containing more than 700k sentences; Marecek and Straka (2013) utilized a very large corpus based on Wikipedia in learning an unlexicalized dependency grammar. Finally, smoothing techniques can be used to reduce the negative impact of data scarcity. One example is Neural DMV (NDMV) (Jiang et al., 2016) which incorporates neural networks into DMV and can automatically smooth correlated grammar rule probabilities. Inspired by this background, we conduct a systematic study regarding the impact of the degree of lexicalization and the training data size on the accuracy of grammar induction approaches. We experimented with a lexicalized version of Dependency M"
D17-1176,D10-1120,0,0.0494955,"ine EM (Liang and Klein, 2009) to accommodate neural network training. 2.3 Model Initialization It was previously shown that the heuristic KM initialization method by Klein and Manning (2004) does not work well for lexicalized grammar induction (Headden III et al., 2009; Pate and Johnson, 2016) and it is very helpful to initialize learning with a model learned by a different grammar induction method (Le and Zuidema, 2015; Jiang et al., 2016). We tested both KM initialization and the following initialization method: we first learn 1684 an unlexicalized DMV using the grammar induction method of Naseem et al. (2010) and use it to parse the training corpus; then, from the parse trees we run maximum likelihood estimation to produce the initial lexicalized model. 3 Experimental Setup For English, we used the BLLIP corpus1 in addition to the regular WSJ corpus in our experiments. Note that the BLLIP corpus is collected from the same news article source as the WSJ corpus, so it is in-domain and is ideal for training grammars to be evaluated on the WSJ test set. In order to solve the compatibility issue as well as improve the POS tagging accuracy, we used the Stanford tagger (Toutanova et al., 2003) to retag t"
D17-1176,C16-1003,0,0.488272,"ed sentences. In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012). Lexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy. The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013). A major problem with ∗ This work was supported by the National Natural Science Foundation of China (61503248). full lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson"
D17-1176,D13-1204,0,0.714584,"t common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012). Lexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy. The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013). A major problem with ∗ This work was supported by the National Natural Science Foundation of China (61503248). full lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010) used partial lexicalization in which infrequent words are replaced by special symbols or their POS tags. Another straightforward way to mitigate the data scarcity problem of lexicalization is to use training corpora larger than the standard WSJ corpus. For example, Pate and Johnson (2016) used two large corp"
D17-1176,N03-1033,0,0.0149554,"ion method of Naseem et al. (2010) and use it to parse the training corpus; then, from the parse trees we run maximum likelihood estimation to produce the initial lexicalized model. 3 Experimental Setup For English, we used the BLLIP corpus1 in addition to the regular WSJ corpus in our experiments. Note that the BLLIP corpus is collected from the same news article source as the WSJ corpus, so it is in-domain and is ideal for training grammars to be evaluated on the WSJ test set. In order to solve the compatibility issue as well as improve the POS tagging accuracy, we used the Stanford tagger (Toutanova et al., 2003) to retag the BLLIP corpus and selected the sentences for which the new tags are consistent with the original tags, which resulted in 182244 sentences with length less than or equal to 10 after removing punctuations. We used this subset of BLLIP and section 2-21 of WSJ10 for training, section 22 of WSJ for validation and section 23 of WSJ for testing. We used training sets of four different sizes: WSJ10 only (5779 sentences) and 20k, 50k, and all sentences from the BLLIP subset. For Chinese, we obtained 4762 sentences for training from Chinese Treebank 6.0 (CTB) after converting data to depend"
D17-1176,D12-1121,1,0.898434,"g training data and lexicalization of greater degrees, especially when enhanced with good model initialization, and it achieves a result that is competitive with the current state-of-the-art. 1 Introduction Grammar induction is the task of learning a grammar from a set of unannotated sentences. In the most common setting, the grammar is unlexicalized with POS tags being the tokens, and the training data is the WSJ10 corpus (the Wall Street Journal corpus with sentences no longer than 10 words) containing no more than 6,000 training sentences (Cohen et al., 2008; Berg-Kirkpatrick et al., 2010; Tu and Honavar, 2012). Lexicalized grammar induction aims to incorporate lexical information into the learned grammar to increase its representational power and improve the learning accuracy. The most straightforward approach to encoding lexical information is full lexicalization (Pate and Johnson, 2016; Spitkovsky et al., 2013). A major problem with ∗ This work was supported by the National Natural Science Foundation of China (61503248). full lexicalization is that the grammar becomes much larger and thus learning is more data demanding. To mitigate this problem, Headden et al. (2009) and Blunsom and Cohn (2010)"
D17-1176,N10-1083,0,\N,Missing
D17-1177,P15-1133,0,0.345211,"ouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al., 2016), can be incorporated into generative models to achieve better parsing accuracy. However, due to the strong independence assumption in most generative models, it is difficult for these models to utilize context information that has been shown to benefit supervised parsing. Recently, a feature-rich discriminative model for unsupervised parsing is proposed that captures the global context information of sentences (Grave and Elhadad, 2015). Inspired by discriminative clustering, learning of the model is formulated as convex optimization of both the model parameters and the parses of training sentences. By utilizing language-independent rules between pairs of POS tags to guide learning, the model achieves state-ofthe-art performance on the UD treebank dataset. In this paper we propose to jointly train two state-of-the-art models of unsupervised dependency parsing: a generative model called LC-DMV (Noji et al., 2016) and a discriminative model called Convex-MST (Grave and Elhadad, 2015). We employ a learning algorithm based on th"
D17-1177,P04-1061,0,0.757129,"ies dependencies between words in a sentence, which have been shown to benefit other tasks such as semantic role labeling (Lei et al., 2015) and sentence classification (Ma et al., 2015). Supervised learning of a dependency parser requires annotation of a training corpus by linguistic experts, which can be time and resource consuming. Unsupervised dependency parsing eliminates the need for dependency annotation by directly learning from unparsed text. Previous work on unsupervised dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012). Generative models have ∗ This work was supported by the National Natural Science Foundation of China (61503248). many advantages. For example, the learning objective function can be defined as the marginal likelihood of the training data, which is typically easy to compute in a generative model. In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010;"
D17-1177,D10-1125,0,0.034125,"is a feature representation f (xα , i, j) of an edge in the dependency graph of sentence xα , v represents whether each dependency arc in yα satisfies a set of prespecified linguistic rules, and λ and µ are hyperparameters. The Frank-Wolfe algorithm is employed to optimize the objective function. 2.3 Dual Decomposition Dual decomposition (Dantzig and Wolfe, 1960), a special case of Lagrangian relaxation, is an optimization method that decomposes a hard problem into several small sub-problems. It has been widely used in machine learning (Komodakis et al., 2007) and natural language processing (Koo et al., 2010; Rush and Collins, 2012). Komodakis et al. (2007) proposed using dual decomposition to do MAP inference for Markov random fields. Koo et al. (2010) proposed a new dependency parser based on dual decomposition by combining a graph based dependency model and a non-projective head automata. In the work of Rush et al. (2010), they showed that dual decomposition can effectively integrate two lexicalized parsing models or two correlated tasks. 2.4 Agreement based Learning Liang et al. (2008) proposed agreement based learning that trains several tractable generative models jointly and encourages the"
D17-1177,N15-1121,0,0.0494344,"Missing"
D17-1177,P15-2029,0,0.0325205,"at learns a generative model and a discriminative model jointly based on the dual decomposition method. Our method is simple and general, yet effective to capture the advantages of both models and improve their learning results. We tested our method on the UD treebank and achieved a state-ofthe-art performance on thirty languages. 1 Introduction Dependency parsing is an important task in natural language processing. It identifies dependencies between words in a sentence, which have been shown to benefit other tasks such as semantic role labeling (Lei et al., 2015) and sentence classification (Ma et al., 2015). Supervised learning of a dependency parser requires annotation of a training corpus by linguistic experts, which can be time and resource consuming. Unsupervised dependency parsing eliminates the need for dependency annotation by directly learning from unparsed text. Previous work on unsupervised dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012). Generative models have ∗ This work was supported by the National Natural Science Foundati"
D17-1177,P05-1012,0,0.243032,"parse tree is the probability product of all the rules used in the generation process. To learn the parameters (rule probabilities) of DMV, the expectation maximization algorithm is often used. Noji et al. (2016) exploited two universal syntactic biases in learning DMV: restricting the center-embedding depth and encouraging short dependencies. They achieved a comparable performance with state-of-the-art approaches. 2.2 Convex-MST Convex-MST (Grave and Elhadad, 2015) is a discriminative model for unsupervised dependency parsing based on the first-order maximum spanning tree dependency parser (McDonald et al., 2005). Given a sentence, whether each possible dependency exists or not is predicted based on a set of handcrafted features and a valid parse tree closest to the prediction is identified by the minimum spanning tree algorithm. For each sentence x, a first-order dependency graph is built over the words of the sentence. The weight of each edge is calculated by wT f (x, i, j), where w is the parameters and f (x, i, j) is the handcrafted feature vector of the dependency from the i-th word to the j-th word in sentence x. For sentence x of length n, we can represent it as matrix X where each raw is a fea"
D17-1177,D16-1004,0,0.185812,"Missing"
D17-1177,D10-1001,0,0.0345256,"Dual decomposition (Dantzig and Wolfe, 1960), a special case of Lagrangian relaxation, is an optimization method that decomposes a hard problem into several small sub-problems. It has been widely used in machine learning (Komodakis et al., 2007) and natural language processing (Koo et al., 2010; Rush and Collins, 2012). Komodakis et al. (2007) proposed using dual decomposition to do MAP inference for Markov random fields. Koo et al. (2010) proposed a new dependency parser based on dual decomposition by combining a graph based dependency model and a non-projective head automata. In the work of Rush et al. (2010), they showed that dual decomposition can effectively integrate two lexicalized parsing models or two correlated tasks. 2.4 Agreement based Learning Liang et al. (2008) proposed agreement based learning that trains several tractable generative models jointly and encourages them to agree on certain latent variables. To effectively train the system, a product EM algorithm was used. They showed that the joint model can perform better than each independent model on the accuracy or convergence speed. They also showed that the objective function of the work of Klein and Manning (2004) is a special c"
D17-1177,P06-1072,0,0.408224,"ed dependency parsing mainly focuses on learning generative models, such as the dependency model with valence (DMV) (Klein and Manning, 2004) and combinatory categorial grammars (CCG) (Bisk and Hockenmaier, 2012). Generative models have ∗ This work was supported by the National Natural Science Foundation of China (61503248). many advantages. For example, the learning objective function can be defined as the marginal likelihood of the training data, which is typically easy to compute in a generative model. In addition, many types of inductive bias, such as those favoring short dependency arcs (Smith and Eisner, 2006), encouraging correlations between POS tags (Cohen et al., 2008; Cohen and Smith, 2009; Berg-Kirkpatrick et al., 2010; Jiang et al., 2016), and limiting center embedding (Noji et al., 2016), can be incorporated into generative models to achieve better parsing accuracy. However, due to the strong independence assumption in most generative models, it is difficult for these models to utilize context information that has been shown to benefit supervised parsing. Recently, a feature-rich discriminative model for unsupervised parsing is proposed that captures the global context information of senten"
D17-1177,N10-1083,0,\N,Missing
D17-1177,N09-1009,0,\N,Missing
D17-1179,P16-1231,0,0.124857,"their parameters. Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios. 1 Introduction The recent renaissance of deep learning has led to significant strides forward in several AI fields. In Natural Language Processing (NLP), characterized by highly structured tasks, promising results were obtained by models that combine deep learning methods with traditional structured learning algorithms (Chen and Manning, 2014; Durrett and Klein, 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lam"
D17-1179,N16-1181,0,0.0256035,"r constituent parsing by replacing the forwardbackward algorithm with the inside-outside algorithm. All of these tasks can benefit from semisupervised learning algorithms.1 1 Our code and experimental set up will be available at https://github.com/cosmozhang/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni e"
D17-1179,D14-1082,0,0.0605891,"der and the decoder simultaneously by decoupling their parameters. Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios. 1 Introduction The recent renaissance of deep learning has led to significant strides forward in several AI fields. In Natural Language Processing (NLP), characterized by highly structured tasks, promising results were obtained by models that combine deep learning methods with traditional structured learning algorithms (Chen and Manning, 2014; Durrett and Klein, 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (A"
D17-1179,P15-1030,0,0.117236,"ltaneously by decoupling their parameters. Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios. 1 Introduction The recent renaissance of deep learning has led to significant strides forward in several AI fields. In Natural Language Processing (NLP), characterized by highly structured tasks, promising results were obtained by models that combine deep learning methods with traditional structured learning algorithms (Chen and Manning, 2014; Durrett and Klein, 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng a"
D17-1179,P82-1020,0,0.71631,"Missing"
D17-1179,P16-1087,0,0.0298882,"g/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient s"
D17-1179,D14-1181,0,0.00281176,"tasks such as dependency parsing or constituent parsing by replacing the forwardbackward algorithm with the inside-outside algorithm. All of these tasks can benefit from semisupervised learning algorithms.1 1 Our code and experimental set up will be available at https://github.com/cosmozhang/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the H"
D17-1179,D16-1116,0,0.114307,"Missing"
D17-1179,N16-1030,0,0.366426,"016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lample et al., 2016; Ma and Hovy, 2016; Durrett and Klein, 2015). Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult. Neural networks are notoriously susceptible to over-fitting unless large amounts of training data are available. This problem is exacerbated in the structured settings, as accounting for the dependencies between decisions requires even more data. Providing it through manual annotation is often a difficult labor-intensive task. In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-"
D17-1179,N15-1144,0,0.36216,"as ˆ ), where Y a generative model, describing P (X|Y is the label. In our model, illustrated in Figure 1b, the encoder is a CRF model with neural networks 1701 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1701–1711 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics as its potential extractors, while the decoder is a generative model, trying to reconstruct the input. Our model carries the merit of autoencoders, which can exploit valuable information from unlabeled data. Recent works (Ammar et al., 2014; Lin et al., 2015) suggested using an autoencoder with a CRF model as an encoder in an unsupervised setting. We significantly expand on these works and suggest the following contributions: 1. We propose a unified model seamlessly accommodating both unlabeled and labeled data. While past work focused on unsupervised structured prediction, neglecting the discriminative power of such models, our model easily supports learning in both fully supervised and semisupervised settings. We developed a variation of the Expectation-Maximization (EM) algorithm, used for optimizing the encoder and the decoder of our model sim"
D17-1179,P16-1101,0,0.255363,", 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lample et al., 2016; Ma and Hovy, 2016; Durrett and Klein, 2015). Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult. Neural networks are notoriously susceptible to over-fitting unless large amounts of training data are available. This problem is exacerbated in the structured settings, as accounting for the dependencies between decisions requires even more data. Providing it through manual annotation is often a difficult labor-intensive task. In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning"
D17-1179,D16-1028,0,0.0307705,"013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the original input X, reconstru"
D17-1179,N06-1020,0,0.0692806,"2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unif"
D17-1179,D16-1031,0,0.0417379,"(Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the original input X, reconstruct the input X ˆ aiming to maximize the log probability P (X|X) without knowing the latent variable Y explicitly. Since we focus"
D17-1179,P16-2025,0,0.0234859,", 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lample et al., 2016; Ma and Hovy, 2016; Durrett and Klein, 2015). Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult. Neural networks are notoriously susceptible to over-fitting unless large amounts of training data are available. This problem is exacerbated in the structured settings, as accounting for the dependencies between decisions requires even more data. Providing it through manual annotation is often a difficult labor-intensive task. In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCR"
D17-1179,D16-1137,0,0.0510587,"Missing"
D17-1179,P13-1045,0,0.0184407,"eration (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Mar"
D17-1179,N10-1116,0,0.0132529,"16; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalize"
D17-1179,W15-1511,0,0.0137207,"pproaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the origi"
D17-1179,D10-1017,0,0.0806109,"or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a)"
D17-1179,W16-5907,0,0.0258514,"2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the original input X, reconstruct the input X ˆ aim"
D17-1179,N16-1027,0,0.0315798,"//github.com/cosmozhang/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficult"
D19-1148,P10-1131,0,0.559008,"sing. Supervised parsing requires manual labeling of gold parse trees, which is a very labor-intensive task. On the other hand, unsupervised parsing (a.k.a. grammar induction) does not require labeled data and can make use of large amounts of unlabeled data that are freely available. However, grammar induction is very challenging and its accuracy is still far below that of supervised parsing. To compensate the lack of supervision in grammar induction, some previous work considers multilingual grammar induction, i.e., simultaneously learning grammars of multiple languages (Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013). Existing multilingual approaches require external resources such as parallel corpora, word alignments, and linguistic phylogenetic trees. ∗ Yong Jiang contributed to this work when at ShanghaiTech University. Kewei Tu is the corresponding author. In this paper, we aim at bilingual grammar induction without external resource. We are motivated by our observation that learning the unsupervised Convex-MST model (Grave and Elhadad, 2015) on the English corpus and then directly applying it to parse other languages produces surprisingly good results (Table 1). From the table, we"
D19-1148,D17-1171,1,0.927176,"et of all possible dependency tree, w is the model parameter, X is the unlabeled training corpus, D is the measurement between the parse y and model prediction on sentence x, R(w) is the P regularization term of parameter w, O ∈ {min, } is an operator. Table 2 shows the choices of O, D and R for several widely used models. 2.2 Graph based Dependency Parsing In this paper, we focus on graph based dependency parsers, though we believe that our approaches can be generalized to other types of parsers. Previous work on unsupervised graph based dependency parsing utilizes the autoencoder structure (Cai et al., 2017) or the discriminative clustering techniques (Grave and Elhadad, 2015). Following (McDonald et al., 2005), we can use a discriminative model for dependency parsing with first order factorization such that the score of a dependency tree y is the sum of the scores of its dependency edges. The score of an edge from word h to word m , sw (x, h, m), can be computed as the inner product of a feature vector f (x, h, m) and a parameter vector w. The optimal dependency tree for sentence x be discovered in polynomial time (Eisner, 1996; McDonald et al., 2005). 3 Bilingual Knowledge Sharing Given non-par"
D19-1148,C96-1058,0,0.118802,"ph based dependency parsing utilizes the autoencoder structure (Cai et al., 2017) or the discriminative clustering techniques (Grave and Elhadad, 2015). Following (McDonald et al., 2005), we can use a discriminative model for dependency parsing with first order factorization such that the score of a dependency tree y is the sum of the scores of its dependency edges. The score of an edge from word h to word m , sw (x, h, m), can be computed as the inner product of a feature vector f (x, h, m) and a parameter vector w. The optimal dependency tree for sentence x be discovered in polynomial time (Eisner, 1996; McDonald et al., 2005). 3 Bilingual Knowledge Sharing Given non-parallel corpora of two languages Xs and Xt , our goal is to learn two models with parameters ws and wt for the two languages. The simplest learning objective function is, J(ws , wt ; Xs , Xt ) = J(ws ; Xs ) + J(wt ; Xt ) which contains no interaction between the two models. As suggested by our empirical observation in Table 1, the model of one language may provide a useful inductive bias in learning the model of another language. Note that given a sentence, a graph-based dependency parser has three levels of representations: th"
D19-1148,P15-1133,0,0.577695,"ious work considers multilingual grammar induction, i.e., simultaneously learning grammars of multiple languages (Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013). Existing multilingual approaches require external resources such as parallel corpora, word alignments, and linguistic phylogenetic trees. ∗ Yong Jiang contributed to this work when at ShanghaiTech University. Kewei Tu is the corresponding author. In this paper, we aim at bilingual grammar induction without external resource. We are motivated by our observation that learning the unsupervised Convex-MST model (Grave and Elhadad, 2015) on the English corpus and then directly applying it to parse other languages produces surprisingly good results (Table 1). From the table, we can see that even with this simplistic method (which we call direct transfer), the dependency accuracy on each language is often very close to the accuracy of the model specifically trained on the corpus of that language. For the Swedish language, the accuracy of direct transfer is even better than that of the specifically trained model. This surprising result suggests that grammars of different languages, even those from different language families (e."
D19-1148,P15-1119,0,0.0241976,"een previous work aiming at solving an unsupervised learning task of a target domain with the help of knowledge learned from a source domain (Dai et al., 2008; Wang et al., 2008; Pan and Yang, 2010). There is no labeled data in both the source and the target domains during training. Our transfer grammar induction setting can be seen as an instance of unsupervised transfer learning. Cross-lingual Supervised Dependency Parsing This task focuses on learning a parser with unlabeled training data and additional labeled training data of a second language (McDonald et al., 2011; Naseem et al., 2012; Guo et al., 2015). The main difference between our approach and theirs is that our approach is fully unsupervised. and do not utilize external information like word alignments or cross-lingual word embeddings. Other Approaches to Multilingual Grammar Induction To the best of our knowledge, this task is first proposed by Kuhn (2004). They assume that the syntax trees induced from parallel sentences share structured regularities and utilize the word alignments to guide parsing. From then on, many approaches are proposed on both constituency grammar induction and dependency grammar induction (Snyder et al., 2009;"
D19-1148,P19-1526,1,0.798037,"Missing"
D19-1148,D16-1073,1,0.89992,"Missing"
D19-1148,D17-1177,1,0.737516,"lhadad, 2015). Our three objective functions can be optimized with coordinate descent in a similar way to Convex-MST. In each iteration, we first fix parse Experiments To enable direct comparison with the ConvexMST model, we use the dataset used in their paper (Grave and Elhadad, 2015), the universal treebanks version 2.01 , introduced by McDonald et al. (2013). The dataset contains ten different languages, which belong to five diverse families. In additional, we test our methods on twelve languages from the more recent UD Treebank 1.42 , which is also used in previous grammar induction work (Jiang et al., 2017; Li et al., 2019). Following previous work, we train all the models on the gold POS tags of sentences no longer than ten. We tune hyper-parameters on the development dataset and report the DDA on sentences no longer than ten and all the sentences in the test dataset. As our goal is to investigate the benefits of our regularization methods, the two hyper-parameters µ and β 1 https://github.com/ryanmcd/ uni-dep-tb. The version is not consistent with recent releases of UD Treebanks. 2 http://universaldependencies.org/ 1425 CODE DE ES FR ID IT JA KO PTBR SV Avg Avg-All UD 1.4∗ C-MST D-T RAN W-R E"
D19-1148,D16-1139,0,0.0206843,"wT f (eat ! with) each sentence, which can be seen as a soft version of weight regularization. J(ws , wt ; Xs , Xt ) = J(ws ; Xs ) + J(wt ; Xt )+ X X ||sws (x, h, m) − swt (x, h, m)||22 λ y I x∈X 0 (h,m)∈G(x) eat sushi with Mary where X 0 = Xs ∪ Xt . G(x) is the weighted dependency graph of sentence x. Figure 1: Three levels of representations of the parser: the parameter w, the edge score s, and the parse y. Regularization on Parse Trees (T-Reg) Another alternative is to encourage similarity between the parse trees predicted by the two models. Motivated by the idea of knowledge distillation (Kim and Rush, 2016), in the learning objective of each model, we add a fourth term to encourage the parse tree to be close to the prediction of the other model. Below we show the objective function for ws . y for each training sentence and update parameters ws and wt by stochastic gradient descent; then we fix ws and wt and update y of each sentence by the Frank-Wolfe algorithm. While our three methods are applicable to any pair of languages, intuitively one may use weight regularization only for similar languages, and use edge regularization and tree regularization for an arbitrary language pair. 4 0 J (ws , wt"
D19-1148,P04-1061,0,0.481788,"Missing"
D19-1148,P04-1060,0,0.156059,"Missing"
D19-1148,D11-1006,0,0.0641094,"s Unsupervised Transfer Learning There has been previous work aiming at solving an unsupervised learning task of a target domain with the help of knowledge learned from a source domain (Dai et al., 2008; Wang et al., 2008; Pan and Yang, 2010). There is no labeled data in both the source and the target domains during training. Our transfer grammar induction setting can be seen as an instance of unsupervised transfer learning. Cross-lingual Supervised Dependency Parsing This task focuses on learning a parser with unlabeled training data and additional labeled training data of a second language (McDonald et al., 2011; Naseem et al., 2012; Guo et al., 2015). The main difference between our approach and theirs is that our approach is fully unsupervised. and do not utilize external information like word alignments or cross-lingual word embeddings. Other Approaches to Multilingual Grammar Induction To the best of our knowledge, this task is first proposed by Kuhn (2004). They assume that the syntax trees induced from parallel sentences share structured regularities and utilize the word alignments to guide parsing. From then on, many approaches are proposed on both constituency grammar induction and dependency"
D19-1148,P12-1066,0,0.0240409,"Learning There has been previous work aiming at solving an unsupervised learning task of a target domain with the help of knowledge learned from a source domain (Dai et al., 2008; Wang et al., 2008; Pan and Yang, 2010). There is no labeled data in both the source and the target domains during training. Our transfer grammar induction setting can be seen as an instance of unsupervised transfer learning. Cross-lingual Supervised Dependency Parsing This task focuses on learning a parser with unlabeled training data and additional labeled training data of a second language (McDonald et al., 2011; Naseem et al., 2012; Guo et al., 2015). The main difference between our approach and theirs is that our approach is fully unsupervised. and do not utilize external information like word alignments or cross-lingual word embeddings. Other Approaches to Multilingual Grammar Induction To the best of our knowledge, this task is first proposed by Kuhn (2004). They assume that the syntax trees induced from parallel sentences share structured regularities and utilize the word alignments to guide parsing. From then on, many approaches are proposed on both constituency grammar induction and dependency grammar induction (S"
D19-1148,D16-1004,0,0.262561,"Missing"
D19-1148,P09-1009,0,0.0271399,"tural language processing. Supervised parsing requires manual labeling of gold parse trees, which is a very labor-intensive task. On the other hand, unsupervised parsing (a.k.a. grammar induction) does not require labeled data and can make use of large amounts of unlabeled data that are freely available. However, grammar induction is very challenging and its accuracy is still far below that of supervised parsing. To compensate the lack of supervision in grammar induction, some previous work considers multilingual grammar induction, i.e., simultaneously learning grammars of multiple languages (Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010; Liu et al., 2013). Existing multilingual approaches require external resources such as parallel corpora, word alignments, and linguistic phylogenetic trees. ∗ Yong Jiang contributed to this work when at ShanghaiTech University. Kewei Tu is the corresponding author. In this paper, we aim at bilingual grammar induction without external resource. We are motivated by our observation that learning the unsupervised Convex-MST model (Grave and Elhadad, 2015) on the English corpus and then directly applying it to parse other languages produces surprisingly good resu"
D19-1148,P13-1105,0,0.301961,"Missing"
D19-1148,P05-1012,0,0.0385333,"is the measurement between the parse y and model prediction on sentence x, R(w) is the P regularization term of parameter w, O ∈ {min, } is an operator. Table 2 shows the choices of O, D and R for several widely used models. 2.2 Graph based Dependency Parsing In this paper, we focus on graph based dependency parsers, though we believe that our approaches can be generalized to other types of parsers. Previous work on unsupervised graph based dependency parsing utilizes the autoencoder structure (Cai et al., 2017) or the discriminative clustering techniques (Grave and Elhadad, 2015). Following (McDonald et al., 2005), we can use a discriminative model for dependency parsing with first order factorization such that the score of a dependency tree y is the sum of the scores of its dependency edges. The score of an edge from word h to word m , sw (x, h, m), can be computed as the inner product of a feature vector f (x, h, m) and a parameter vector w. The optimal dependency tree for sentence x be discovered in polynomial time (Eisner, 1996; McDonald et al., 2005). 3 Bilingual Knowledge Sharing Given non-parallel corpora of two languages Xs and Xt , our goal is to learn two models with parameters ws and wt for"
D19-1576,P10-1131,0,0.252007,". Intuitively, one can couple grammar parameters of different languages with similar typology and learn them simultaneously. However, the lacking of measures of language similarity prevents this idea from being further exploited in practice. Previous work in multilingual grammar induction either does not consider language similarity measures (Iwata et al., 2010) or models lan∗ The first and second authors contributed equally. The third author contributed to this work when at ShanghaiTech University. The fourth author is the corresponding author. guage similarity based on linguistic phylogeny (Berg-Kirkpatrick and Klein, 2010). The phylogenetic knowledge, however, could be misleading in measuring language similarity. For example, English and German are both Germanic languages, but English exhibits dominant SubjectVerb-Object (SVO) word order while German does not. In this paper, we propose a novel approach to multilingual grammar induction. Our induction model represents language identities as continuous vectors (i.e., language embeddings) and employs a neural network to predict the grammar parameters of each language based on its embedding. The neural network parameters are universally shared across languages, whi"
D19-1576,Q13-1007,0,0.493381,"monolingual and multilingual settings. For the monolingual setting we trained the baseline models on each language independently. For the multilingual setting we trained them on the combined training data of all the 15 languages and tested on one of the languages. Table 2 shows the experimental results. It can be seen that our multilingual grammar model (G) performs better on average than all the baselines. The improvement be2 We re-implemented the DMV and the NDMV. We set the ATTACH valence and DECISION valence to 2 and used root constraints, similar to previous work (Gimpel and Smith, 2012; Bisk and Hockenmaier, 2013; Noji et al., 2016). 5730 comes more significant when our model is jointly trained with the auxiliary language identification task (G+I). Note that our approach performs worse than the monolingual baseline on some languages, and we speculate that it is partly caused by data imbalance. In particular, the worst-performing Hindi language has only 4997 training sentences, much smaller than the average 7532. It would be interesting to make training more balanced by assigning weights to training samples of different languages, which we leave for future work. To measure the statistical significance"
D19-1576,N09-1009,0,0.14331,"the DECISION and ATTACH distributions PDECISION (dec|h, dir, val) and PATTACH (c|h, dir), where dir is a binary variable representing the direction of generation (left or right), val is a binary variable representing whether the current head token already has a child in the direction dir or not, dec is a binary variable deciding whether to continue generation in the current direction, c is the child token and h is the head token. Almost all previous methods of multilingual grammar induction are based on DMV. Their focus is on designing various priors to couple DMV parameters across languages: Cohen and Smith (2009) propose a logistic normal prior while BergKirkpatrick and Klein (2010) design a hierarchical Gaussian prior according to linguistic phylogeny. The usage of continuous language embeddings has been explored in other tasks. For example, Ammar et al. (2016) and de Lhoneux et al. (2018) apply language embeddings in supervised multilingual dependency parsing. 3 Approach We perform unlexicalized grammar induction in which a sentence is represented as a sequence of part-of-speech (POS) tags. We assume that all the languages share the same set of POS tags. 3.1 Multilingual Grammar Model Our multilingu"
D19-1576,N12-1069,0,0.616168,"re experimented in both monolingual and multilingual settings. For the monolingual setting we trained the baseline models on each language independently. For the multilingual setting we trained them on the combined training data of all the 15 languages and tested on one of the languages. Table 2 shows the experimental results. It can be seen that our multilingual grammar model (G) performs better on average than all the baselines. The improvement be2 We re-implemented the DMV and the NDMV. We set the ATTACH valence and DECISION valence to 2 and used root constraints, similar to previous work (Gimpel and Smith, 2012; Bisk and Hockenmaier, 2013; Noji et al., 2016). 5730 comes more significant when our model is jointly trained with the auxiliary language identification task (G+I). Note that our approach performs worse than the monolingual baseline on some languages, and we speculate that it is partly caused by data imbalance. In particular, the worst-performing Hindi language has only 4997 training sentences, much smaller than the average 7532. It would be interesting to make training more balanced by assigning weights to training samples of different languages, which we leave for future work. To measure t"
D19-1576,P15-1133,0,0.473457,"nt languages, which we leave for future work. To measure the statistical significance of the advantage of our method, we performed the nonparametric Friedman’s test to support/reject the claim (null hypothesis): there is no difference between the G+I model and the NDMV model in a multilingual setting. Based on the above sample data, the P-value 7.8911 × 10−4 would result in rejection of the claim at the 0.05 significance level, thus showing the significance in our performance gain. In Table 3 we compare our method with recent state-of-the-art approaches on the UD Treebank dataset: Convex-MST (Grave and Elhadad, 2015), LC-DMV (Noji et al., 2016) and D-J (Jiang et al., 2017). For the three approaches we use the results reported by Jiang et al. (2017). Our G+I model performs better than Convex-MST and LC-DMV on average, even though additional priors and delicate biases are integrated into the two methods (e.g, the universal linguistic prior for ConvexMST and the limited center-embedding for LCDMV). Our method also slightly outperforms D-J on average, even though D-J combines ConvexMST and LC-DMV and therefore utilizes even more linguistic prior knowledge. 5 Analysis 5.1 Visualization of Language Embeddings O"
D19-1576,P10-2034,0,0.0228726,"syntactic level in spite of their diversity on the surface, as many studies have revealed (Greenberg, 1963; Hawkins, 2014). This fact provides the basis for multilingual grammar induction which tries to simultaneously induce grammars of multiple languages. Intuitively, one can couple grammar parameters of different languages with similar typology and learn them simultaneously. However, the lacking of measures of language similarity prevents this idea from being further exploited in practice. Previous work in multilingual grammar induction either does not consider language similarity measures (Iwata et al., 2010) or models lan∗ The first and second authors contributed equally. The third author contributed to this work when at ShanghaiTech University. The fourth author is the corresponding author. guage similarity based on linguistic phylogeny (Berg-Kirkpatrick and Klein, 2010). The phylogenetic knowledge, however, could be misleading in measuring language similarity. For example, English and German are both Germanic languages, but English exhibits dominant SubjectVerb-Object (SVO) word order while German does not. In this paper, we propose a novel approach to multilingual grammar induction. Our induct"
D19-1576,D16-1073,1,0.883965,"hile BergKirkpatrick and Klein (2010) design a hierarchical Gaussian prior according to linguistic phylogeny. The usage of continuous language embeddings has been explored in other tasks. For example, Ammar et al. (2016) and de Lhoneux et al. (2018) apply language embeddings in supervised multilingual dependency parsing. 3 Approach We perform unlexicalized grammar induction in which a sentence is represented as a sequence of part-of-speech (POS) tags. We assume that all the languages share the same set of POS tags. 3.1 Multilingual Grammar Model Our multilingual grammar model adopts the NDMV (Jiang et al., 2016), a monolingual model, as the basic component. NDMV predicts grammar rule probabilities using neural networks. In our model, we add a continuous vector representation of the language identity l (i.e., a language embedding) as an additional input to the neural networks in NDMV. Specifically, to predict an ATTACH rule probability PATTACH (c|h, dir, val, l), we use a multilayer neural network that takes the embeddings of the head token h, valence val and language identity l as input, uses a weight matrix Wdir specific to the direction dir in the first layer, and uses a weight matrix Wc consisting"
D19-1576,D17-1177,1,0.889934,"del. 4 C ODE ET FI NL EN DE NO GRC HI JA FR IT LA BG SL EU Avg 4.1 Setup 1 Our code is available at https://github.com/ WinnieHAN/mndmv.git. Language Family Finnic Finnic Germanic Germanic Germanic Germanic Hellenic Indo-Irian Janponic Romance Romance Romance Slavonic Slavonic Vasconic Corpus Size 11404 9648 8783 7674 7447 10017 9387 4997 7441 4976 6492 10136 6507 3800 4271 Table 1: Languages and treebanks used in our experiments. Experiment We selected 15 languages across 8 language families and subfamilies to ensure diversity. To enable comparisons with previous state-of-the-art approaches (Jiang et al., 2017; Li et al., 2019), we conducted our experiments on UD Treebank 1.4. For each language, we show its language family and the training corpus size in Table 1. We trained our method on the training sentences with length ≤ 15 and tested our method on the testing sentences with length ≤ 40 after removing all punctuations. Since we are doing unsupervised learning, gold dependency trees were not used during training. We use the directed dependency accuracy (DDA, the percentage of words in the testing dataset which are assigned the correct head, same to the unlabeled attachment score normally used in"
D19-1576,P04-1061,0,0.354887,"parameters are trained with a standard grammar induction objective without any guidance from prior linguistic phylogenetic knowledge. We also introduce an auxiliary language identification task in which we predict the language identities of input sentences using the language embeddings. We evaluate our approach on corpora of 15 languages across 8 language families and subfamilies. We observe that our approach achieves substantial performance gain on average over monolingual and multilingual baselines. 2 Dependency Model with Valence and Other Related Works Dependency Model with Valence (DMV) (Klein and Manning, 2004) is the best known generative model for dependency grammar induction. The DMV generates a sentence and its dependency tree following three types of grammar rules (ATTACH, DECISION and ROOT). It firstly samples a token c from the ROOT distribution PROOT (c) 5728 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5728–5733, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Auxiliary Language Identiﬁcation Model (I) 0.6 and then recursively deci"
D19-1576,D18-1543,0,0.0658073,"Missing"
D19-1576,D16-1004,0,0.404094,"Missing"
P19-1526,N10-1083,0,0.149791,"Missing"
P19-1526,D10-1117,0,0.480621,"Missing"
P19-1526,N09-1009,0,0.622546,"language processing. The dependency relations identified by dependency parsing convey syntactic information useful in subsequent applications such as semantic parsing, information extraction, and question answering. In this paper, we ∗ Corresponding author focus on unsupervised dependency parsing, which aims to induce a dependency parser from training sentences without gold parse annotation. Most previous approaches to unsupervised dependency parsing are based on probabilistic generative models, for example, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) and its extensions (Cohen and Smith, 2009; Headden III et al., 2009; Cohen and Smith, 2010; BergKirkpatrick et al., 2010; Gillenwater et al., 2010; Jiang et al., 2016). A disadvantage of such approaches comes from the context-freeness of dependency grammars, a strong independence assumption that limits the information available in determining how likely a dependency is between two words in a sentence. In DMV, the probability of a dependency is computed from only the head and child tokens, the dependency direction, and the number of dependencies already connected from the head token. Additional information used for computing dependenc"
P19-1526,W12-1909,0,0.0607523,"Missing"
P19-1526,P10-2036,0,0.206164,"Missing"
P19-1526,P15-1133,0,0.783648,"In DMV, the probability of a dependency is computed from only the head and child tokens, the dependency direction, and the number of dependencies already connected from the head token. Additional information used for computing dependency probabilities in later work is also limited to local morpho-syntactic features such as word forms, lemmas and categories (Berg-Kirkpatrick et al., 2010), which does not break the context-free assumption. More recently, researchers have started to utilize discriminative methods in unsupervised dependency parsing based on the idea of discriminative clustering (Grave and Elhadad, 2015), the CRFAE framework (Cai et al., 2017) or the neural variational transition-based parser (Li et al., 2019). By conditioning dependency prediction on the whole input sentence, discriminative methods are capable of utilizing not only local information, but also global and contextual information of a dependency in determining its strength. Specifically, both Grave and Elhadad (2015) and Cai et al. (2017) include in the feature set of a dependency the information of the tokens around the head or child token of the dependency. In this way, 5315 Proceedings of the 57th Annual Meeting of the Associ"
P19-1526,D17-1176,1,0.760772,"the-art approaches on the specific dataset. Setup Following previous work, we conducted experiments under the unlexicalized setting where a sentence is represented as a sequence of gold part-of-speech tags with punctuations removed. The embedding length was set to 10 for the head and child tokens and the valence. The sentence embedding length was also set to 10. We trained the neural networks using stochastic gradient descent with batch size 10 and learning rate 0.01. We used the change of the loss on the validation set as the stop criteria. For our methods in the WSJ experiments, we followed Han et al. (2017) and initialized our model using the pre-trained model of Naseem et al. (2010), which significantly increased the accuracy and decreased the variance. For the other experiments, we used a pre-trained NDMV model to initialize our method. We ran our model for 5 times and report the average DDA. 5319 M ETHODS WSJ10 WSJ NDMV Systems in Basic Setup DMV (Klein and Manning, 2004) 58.3 39.4 59.4 40.5 LN (Cohen et al., 2008) Convex-MST (Grave and Elhadad, 2015) 60.8 48.6 61.3 41.4 Shared LN (Cohen and Smith, 2009) Feature DMV (Berg-Kirkpatrick et al., 2010) 63.0 64.3 53.3 PR-S (Gillenwater et al., 2010"
P19-1526,N09-1012,0,0.413013,"Missing"
P19-1526,D16-1073,1,0.648327,"applications such as semantic parsing, information extraction, and question answering. In this paper, we ∗ Corresponding author focus on unsupervised dependency parsing, which aims to induce a dependency parser from training sentences without gold parse annotation. Most previous approaches to unsupervised dependency parsing are based on probabilistic generative models, for example, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) and its extensions (Cohen and Smith, 2009; Headden III et al., 2009; Cohen and Smith, 2010; BergKirkpatrick et al., 2010; Gillenwater et al., 2010; Jiang et al., 2016). A disadvantage of such approaches comes from the context-freeness of dependency grammars, a strong independence assumption that limits the information available in determining how likely a dependency is between two words in a sentence. In DMV, the probability of a dependency is computed from only the head and child tokens, the dependency direction, and the number of dependencies already connected from the head token. Additional information used for computing dependency probabilities in later work is also limited to local morpho-syntactic features such as word forms, lemmas and categories (Be"
P19-1526,D17-1177,1,0.840711,"l=1 z∈Z(x) X = log pΘ (x, z|s(1) ) (10) 4.1 Dataset and Setup English Penn Treebank We conducted experiments on the Wall Street Journal corpus (WSJ) with section 2-21 for training, section 22 for validation and section 23 for testing. We trained our model with training sentences of length ≤ 10, tuned the hyer-parameters on validation sentences of length ≤ 10 the and evaluated on testing sentences of length ≤ 10 (WSJ10) and all sentences (WSJ). We reported the directed dependency accuracy (DDA) of the learned grammars on the test sentences. Universal Dependency Treebank Following the setup of Jiang et al. (2017); Li et al. (2019), we conducted experiments on selected eight languages from the Universal Dependency Treebank 1.4 (Nivre et al., 2016). We trained our model on training sentences of length ≤ 15 and report the DDA on testing sentences of length ≤ 15 and ≤ 40. Datasets from PASCAL Challenge on Grammar Induction We conducted experiments on corpora of eight languages from the PASCAL Challenge on Grammar Induction (Gelling et al., 2012). We trained our model with training sentences of length ≤ 10 and evaluated on testing sentences of length ≤ 10 and all sentences. Note that on the UD Treebanks an"
P19-1526,P04-1061,0,0.505064,"parsing is a very important task in natural language processing. The dependency relations identified by dependency parsing convey syntactic information useful in subsequent applications such as semantic parsing, information extraction, and question answering. In this paper, we ∗ Corresponding author focus on unsupervised dependency parsing, which aims to induce a dependency parser from training sentences without gold parse annotation. Most previous approaches to unsupervised dependency parsing are based on probabilistic generative models, for example, the Dependency Model with Valence (DMV) (Klein and Manning, 2004) and its extensions (Cohen and Smith, 2009; Headden III et al., 2009; Cohen and Smith, 2010; BergKirkpatrick et al., 2010; Gillenwater et al., 2010; Jiang et al., 2016). A disadvantage of such approaches comes from the context-freeness of dependency grammars, a strong independence assumption that limits the information available in determining how likely a dependency is between two words in a sentence. In DMV, the probability of a dependency is computed from only the head and child tokens, the dependency direction, and the number of dependencies already connected from the head token. Additiona"
P19-1526,N15-1067,0,0.349887,"Missing"
P19-1526,D10-1120,0,0.321188,"we conducted experiments under the unlexicalized setting where a sentence is represented as a sequence of gold part-of-speech tags with punctuations removed. The embedding length was set to 10 for the head and child tokens and the valence. The sentence embedding length was also set to 10. We trained the neural networks using stochastic gradient descent with batch size 10 and learning rate 0.01. We used the change of the loss on the validation set as the stop criteria. For our methods in the WSJ experiments, we followed Han et al. (2017) and initialized our model using the pre-trained model of Naseem et al. (2010), which significantly increased the accuracy and decreased the variance. For the other experiments, we used a pre-trained NDMV model to initialize our method. We ran our model for 5 times and report the average DDA. 5319 M ETHODS WSJ10 WSJ NDMV Systems in Basic Setup DMV (Klein and Manning, 2004) 58.3 39.4 59.4 40.5 LN (Cohen et al., 2008) Convex-MST (Grave and Elhadad, 2015) 60.8 48.6 61.3 41.4 Shared LN (Cohen and Smith, 2009) Feature DMV (Berg-Kirkpatrick et al., 2010) 63.0 64.3 53.3 PR-S (Gillenwater et al., 2010) 65.0 E-DMV (Headden III et al., 2009) 65.9 53.1 TSG-DMV (Blunsom and Cohn, 2"
P19-1526,D16-1004,0,0.369401,"Missing"
P19-1526,D13-1204,0,0.370709,"Missing"
P19-1526,W10-2902,0,0.0934217,"propagate the objective Q(Θ, Φ) into the parameters of the neural networks. We initialize the model either heuristically (Klein and Manning, 2004) or using a pre-trained unsupervised parser (Jiang et al., 2016); then we alternate between E-steps and M-steps until convergence. Note that if we require q(z) to be a delta function, then the algorithm becomes hard-EM, which computes the best parse of each training sentence in the E-step and set the expected count to 1 if the rule is used in the parse and 0 otherwise. It has been found that hard-EM outperforms EM in unsupervised dependency parsing (Spitkovsky et al., 2010; Tu and Honavar, 2012), so we use hard-EM in our experiments. 3.2 Variational Variant for D-NDMV Motivated by (Bowman et al., 2016), we propose to model the global representation s as drawing from a prior distribution, generally a standard 5318 Gaussian distribution. We also propose a variational posterior distribution qΦ (s|x) to approximate this prior distribution. In this way, we formalize it into a variational inference framework. We call this model variational variant and illustrate its graphical model in Figure 2 (right). It can be seen from Figure 2 (right) that the variational variant"
P19-1526,D12-1121,1,0.954062,"(Θ, Φ) into the parameters of the neural networks. We initialize the model either heuristically (Klein and Manning, 2004) or using a pre-trained unsupervised parser (Jiang et al., 2016); then we alternate between E-steps and M-steps until convergence. Note that if we require q(z) to be a delta function, then the algorithm becomes hard-EM, which computes the best parse of each training sentence in the E-step and set the expected count to 1 if the rule is used in the parse and 0 otherwise. It has been found that hard-EM outperforms EM in unsupervised dependency parsing (Spitkovsky et al., 2010; Tu and Honavar, 2012), so we use hard-EM in our experiments. 3.2 Variational Variant for D-NDMV Motivated by (Bowman et al., 2016), we propose to model the global representation s as drawing from a prior distribution, generally a standard 5318 Gaussian distribution. We also propose a variational posterior distribution qΦ (s|x) to approximate this prior distribution. In this way, we formalize it into a variational inference framework. We call this model variational variant and illustrate its graphical model in Figure 2 (right). It can be seen from Figure 2 (right) that the variational variant shares the same formul"
W19-2302,D15-1166,0,0.0254017,"e Gθqr with hq, ri 12: Update Gθrq by ∇θrq Gθrq in Eq. 5. 13: Teacher Forcing: update Gθrq with hq, ri 14: end for 15: end while ∇θ Gθ = ∇θ MLE + λdual · ∇θ Υ, where ∇θ MLE is the guidance from teacher forcing and ∇θ Υ is the guidance from the duality constraint. In DAL-DuAd, the guidance of each generator can be formulated as ∇θ Gθ = ∇θ MLE + λdual · ∇θ Υ + λgan · ∇θ J(θ), where ∇θ J(θ) is the adversarial signal. Experimental Settings A Sina Weibo dataset (Zhou et al., 2017) is employed to train the models. We treat each query-response pair as a single-turn conversation. Attention mechanism (Luong et al., 2015) is applied in all the methods to enhance the performance. All the methods are implemented based on the open source tools Pytorch(Paszke et al., 2017) and OpenNMT (Klein et al., 2017). 1,000,565 query-response pairs are employed as the training data, 3,000 pairs as the validation data. The test data is another unique 10,000 query-response pairs. The length of all the dialogue utterances in the training corpus ranges from 5 to 50. Batch size is set to 64. The vocabulary size is set to 50,000. The dimension of word embedding is set to 500. All the methods adopt a beam size of 5 in the decoding p"
W19-2302,C16-1316,0,0.0611217,"for generation task. A GAN usually contains two neural networks: a generator G and a discriminator D. G generates samples while D is trained to distinguish generated samples from true samples. By regarding the sequence generation as an action-taking problem in reinforcement learning, Li et al. (2017) proposed to apply GAN to dialogue generation, in which the output of the discriminator is used as the reward for the generator’s optimization. Work on the Safe Response Problem There is some existing work on the safe response problem. The first kind of approach is to introduce specific keywords (Mou et al., 2016) or topic information (Xing et al., 2017) into the generated responses. These methods help to increase the dialogue coherence (Peng et al., 2019) by keywords introduction. However, these methods shift the difficulty from diverse response generation to keyword or topic prediction, which are also challenging tasks. The second kind of approach takes the reverse dependency (the query generation task given the responses) into consideration. Li et al. (2016) considered the reverse dependency and proposed Maximum Mutual Information (MMI) method, which is empirically plagued by ungrammatical responses"
W19-2302,P02-1040,0,0.104382,"he guidance of discriminators Dφqr and Dφrq , the generator Gθrq is able to influence the generator Gθqr to produce more diverse responses. We do notice that DAL-Dual achieves slightly better performance than DAL-DuAd on diversity. The reason is that sometimes adversarial methods tend to generate some short but quality responses such as “Let’s go!” for given queries such as “We can have dinner together tonight. ” or “There is an exhibition at the National Museum.”. However, this short but natural response would harm diversity. Response Quality Since the word overlapbased metrics such as BLEU (Papineni et al., 2002) and embedding-based metrics are inappropriate for response quality evaluation due to their low correlation with human judgment (Liu et al., 2016; Mou et al., 2016), we resort to human annotators to evaluate the overall quality of the generated responses. We employ 3 annotators to evaluate the quality of 200 responses generated from each of the aforementioned methods. 2: the response is natural, relevant and informative. 1: the response is appropriate for the given query but may not be very informative. 0: the response is completely irrelevant, incoherent or contains syntactic errors. The fina"
W19-2302,D11-1054,0,0.0340125,"s. Experimental results demonstrate that DAL effectively improves both diversity and overall quality of the generated responses. DAL outperforms stateof-the-art methods regarding automatic metrics and human evaluations. 1 Introduction In recent years, open-domain dialogue systems are gaining much attention owing to their great potential in applications such as educational robots, emotional companion, and chitchat. The existing approaches for open-domain dialogue systems can be divided into two categories: retrieval-based approaches (Hu et al., 2014; Ji et al., 2014) and generative approaches (Ritter et al., 2011; Shang et al., 2015). The retrieval-based approaches are based on conventional information retrieval techniques and strongly rely on the underlying corpus (Wang et al., 2013; Lu and Li, 2013). Since the capability of retrieval-based approaches is strongly limited by corpus, generative approaches are attracting more attention in the field of open-domain dialogue research. The de facto backbone of generative approaches is the Seq2Seq model (Bahdanau 1 We use query and response to denote the first and second utterances in a single-turn dialogue. 11 Proceedings of the Workshop on Methods for Opti"
W19-2302,P17-4012,0,0.0291205,"dance from teacher forcing and ∇θ Υ is the guidance from the duality constraint. In DAL-DuAd, the guidance of each generator can be formulated as ∇θ Gθ = ∇θ MLE + λdual · ∇θ Υ + λgan · ∇θ J(θ), where ∇θ J(θ) is the adversarial signal. Experimental Settings A Sina Weibo dataset (Zhou et al., 2017) is employed to train the models. We treat each query-response pair as a single-turn conversation. Attention mechanism (Luong et al., 2015) is applied in all the methods to enhance the performance. All the methods are implemented based on the open source tools Pytorch(Paszke et al., 2017) and OpenNMT (Klein et al., 2017). 1,000,565 query-response pairs are employed as the training data, 3,000 pairs as the validation data. The test data is another unique 10,000 query-response pairs. The length of all the dialogue utterances in the training corpus ranges from 5 to 50. Batch size is set to 64. The vocabulary size is set to 50,000. The dimension of word embedding is set to 500. All the methods adopt a beam size of 5 in the decoding phase. The maximum length of the target sequence is set to 50. Gradient clipping strategy is adopted when the norm exceeds a threshold of 5. There are 2 fully-connected layers (1000*50"
W19-2302,P15-1152,0,0.0911232,"Missing"
W19-2302,N15-1020,0,0.103277,"Missing"
W19-2302,N16-1014,0,0.646396,"nd response to denote the first and second utterances in a single-turn dialogue. 11 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 11–20 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Association for Computational Linguistics viate unnatural responses. We compare DAL with state-of-the-art methods through extensive experiments, and DAL demonstrates superior performance regarding automatic metrics, human evaluations, and efficiency. There are crucial differences between our dual approach and Maximum Mutual Information (MMI) (Li et al., 2016) though both utilize the reverse dependency to improve the diversity of the generated responses. Due to the challenging mutual information objective, the distribution p(r|q) is same as that in vanilla Seq2Seq in MMI. More specifically, p(r|q) in MMI is trained only by maximum likelihood estimation (MLE) objective at training time (we use p(r|q) to denote the probability distribution of predicting the response r given the query q). The mutual information in MMI is utilized only at inference time, and the inference process is not only time-consuming but also inaccurate in MMI. However, p(r|q) in"
W19-2302,D17-1230,0,0.4764,"e duality constraint derived from the joint probability P (q, r). The adversarial signal from discriminators, Rqr , Rrq , are passed to the corresponding generators as the reward through policy gradient. ing (Goodfellow et al., 2014), or Generative Adversarial Networks (GAN), has proven to be a promising approach for generation task. A GAN usually contains two neural networks: a generator G and a discriminator D. G generates samples while D is trained to distinguish generated samples from true samples. By regarding the sequence generation as an action-taking problem in reinforcement learning, Li et al. (2017) proposed to apply GAN to dialogue generation, in which the output of the discriminator is used as the reward for the generator’s optimization. Work on the Safe Response Problem There is some existing work on the safe response problem. The first kind of approach is to introduce specific keywords (Mou et al., 2016) or topic information (Xing et al., 2017) into the generated responses. These methods help to increase the dialogue coherence (Peng et al., 2019) by keywords introduction. However, these methods shift the difficulty from diverse response generation to keyword or topic prediction, whic"
W19-2302,D16-1230,0,0.179039,"Missing"
W19-2302,D17-1090,0,0.0825438,"Missing"
W19-2302,D13-1096,0,0.0251729,"ng automatic metrics and human evaluations. 1 Introduction In recent years, open-domain dialogue systems are gaining much attention owing to their great potential in applications such as educational robots, emotional companion, and chitchat. The existing approaches for open-domain dialogue systems can be divided into two categories: retrieval-based approaches (Hu et al., 2014; Ji et al., 2014) and generative approaches (Ritter et al., 2011; Shang et al., 2015). The retrieval-based approaches are based on conventional information retrieval techniques and strongly rely on the underlying corpus (Wang et al., 2013; Lu and Li, 2013). Since the capability of retrieval-based approaches is strongly limited by corpus, generative approaches are attracting more attention in the field of open-domain dialogue research. The de facto backbone of generative approaches is the Seq2Seq model (Bahdanau 1 We use query and response to denote the first and second utterances in a single-turn dialogue. 11 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 11–20 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Association for Computational Linguistics viate un"
W19-2302,D17-1065,0,0.0272082,"Missing"
