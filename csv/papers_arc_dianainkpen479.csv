2021.findings-acl.208,{P}-Stance: A Large Dataset for Stance Detection in Political Domain,2021,-1,-1,5,0,3707,yingjie li,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.adaptnlp-1.3,Conditional Adversarial Networks for Multi-Domain Text Classification,2021,-1,-1,2,0,12369,yuan wu,Proceedings of the Second Workshop on Domain Adaptation for NLP,0,"In this paper, we propose conditional adversarial networks (CANs), a framework that explores the relationship between the shared features and the label predictions to impose stronger discriminability to the learned features, for multi-domain text classification (MDTC). The proposed CAN introduces a conditional domain discriminator to model the domain variance in both the shared feature representations and the class-aware information simultaneously, and adopts entropy conditioning to guarantee the transferability of the shared features. We provide theoretical analysis for the CAN framework, showing that CAN{'}s objective is equivalent to minimizing the total divergence among multiple joint distributions of shared features and label predictions. Therefore, CAN is a theoretically sound adversarial network that discriminates over multiple distributions. Evaluation results on two MDTC benchmarks show that CAN outperforms prior methods. Further experiments demonstrate that CAN has a good ability to generalize learned knowledge to unseen domains."
W19-2511,Semantics and Homothetic Clustering of Hafez Poetry,2019,0,0,2,0,24676,arya rahgozar,"Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"We have created two sets of labels for Hafez (1315-1390) poems, using unsupervised learning. Our labels are the only semantic clustering alternative to the previously existing, hand-labeled, gold-standard classification of Hafez poems, to be used for literary research. We have cross-referenced, measured and analyzed the agreements of our clustering labels with Houman{'}s chronological classes. Our features are based on topic modeling and word embeddings. We also introduced a similarity of similarities{'} features, we called homothetic clustering approach that proved effective, in case of Hafez{'}s small corpus of ghazals2. Although all our experiments showed different clusters when compared with Houman{'}s classes, we think they were valid in their own right to have provided further insights, and have proved useful as a contrasting alternative to Houman{'}s classes. Our homothetic clusterer and its feature design and engineering framework can be used for further semantic analysis of Hafez{'}s poetry and other similar literary research."
D19-6208,"Multi-Task, Multi-Channel, Multi-Input Learning for Mental Illness Detection using Social Media Text",2019,0,0,2,0,26426,prasadith gamaarachchige,Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019),0,"We investigate the impact of using emotional patterns identified by the clinical practitioners and computational linguists to enhance the prediction capabilities of a mental illness detection (in our case depression and post-traumatic stress disorder) model built using a deep neural network architecture. Over the years, deep learning methods have been successfully used in natural language processing tasks, including a few in the domain of mental illness and suicide ideation detection. We illustrate the effectiveness of using multi-task learning with a multi-channel convolutional neural network as the shared representation and use additional inputs identified by researchers as indicatives in detecting mental disorders to enhance the model predictability. Given the limited amount of unstructured data available for training, we managed to obtain a task-specific AUC higher than 0.90. In comparison to methods such as multi-class classification, we identified multi-task learning with multi-channel convolution neural network and multiple-inputs to be effective in detecting mental disorders."
W18-4405,Cyberbullying Intervention Based on Convolutional Neural Networks,2018,0,1,2,0,28152,qianjia huang,"Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying ({TRAC}-2018)",0,"This paper describes the process of building a cyberbullying intervention interface driven by a machine-learning based text-classification service. We make two main contributions. First, we show that cyberbullying can be identified in real-time before it takes place, with available machine learning and natural language processing tools. Second, we present a mechanism that provides individuals with early feedback about how other people would feel about wording choices in their messages before they are sent out. This interface not only gives a chance for the user to revise the text, but also provides a system-level flagging/intervention in a situation related to cyberbullying."
W18-4419,Cyber-aggression Detection using Cross Segment-and-Concatenate Multi-Task Learning from Text,2018,0,3,4,0,28175,ahmed orabi,"Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying ({TRAC}-2018)",0,"In this paper, we propose a novel deep-learning architecture for text classification, named cross segment-and-concatenate multi-task learning (CSC-MTL). We use CSC-MTL to improve the performance of cyber-aggression detection from text. Our approach provides a robust shared feature representation for multi-task learning by detecting contrasts and similarities among polarity and neutral classes. We participated in the cyber-aggression shared task under the team name uOttawa. We report 59.74{\%} F1 performance for the Facebook test set and 56.9{\%} for the Twitter test set, for detecting aggression from text."
W18-0609,Deep Learning for Depression Detection of {T}witter Users,2018,-1,-1,4,0,28175,ahmed orabi,Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic,0,"Mental illness detection in social media can be considered a complex task, mainly due to the complicated nature of mental disorders. In recent years, this research area has started to evolve with the continuous increase in popularity of social media platforms that became an integral part of people{'}s life. This close relationship between social media platforms and their users has made these platforms to reflect the users{'} personal life with different limitations. In such an environment, researchers are presented with a wealth of information regarding one{'}s life. In addition to the level of complexity in identifying mental illnesses through social media platforms, adopting supervised machine learning approaches such as deep neural networks have not been widely accepted due to the difficulties in obtaining sufficient amounts of annotated training data. Due to these reasons, we try to identify the most effective deep neural network architecture among a few of selected architectures that were successfully used in natural language processing tasks. The chosen architectures are used to detect users with signs of mental illnesses (depression in our case) given limited unstructured text data extracted from the Twitter social media platform."
S18-1027,u{O}ttawa at {S}em{E}val-2018 Task 1: Self-Attentive Hybrid {GRU}-Based Network,2018,0,0,3,0,28175,ahmed orabi,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"We propose a novel attentive hybrid GRU-based network (SAHGN), which we used at SemEval-2018 Task 1: Affect in Tweets. Our network has two main characteristics, 1) has the ability to internally optimize its feature representation using attention mechanisms, and 2) provides a hybrid representation using a character level Convolutional Neural Network (CNN), as well as a self-attentive word-level encoder. The key advantage of our model is its ability to signify the relevant and important information that enables self-optimization. Results are reported on the valence intensity regression task."
P18-1224,Neural Natural Language Inference Models Enhanced with External Knowledge,2018,32,70,4,1,11735,qian chen,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets."
J18-4006,Introduction to the Special Issue on Language in Social Media: Exploiting Discourse and Other Contextual Information,2018,51,4,2,0,7013,farah benamara,Computational Linguistics,0,"Social media content is changing the way people interact with each other and share information, personal messages, and opinions about situations, objects, and past experiences. Most social media texts are short online conversational posts or comments that do not contain enough information for natural language processing (NLP) tools, as they are often accompanied by non-linguistic contextual information, including meta-data (e.g., the user{'}s profile, the social network of the user, and their interactions with other users). Exploiting such different types of context and their interactions makes the automatic processing of social media texts a challenging research task. Indeed, simply applying traditional text mining tools is clearly sub-optimal, as, typically, these tools take into account neither the interactive dimension nor the particular nature of this data, which shares properties with both spoken and written language. This special issue contributes to a deeper understanding of the role of these interactions to process social media data from a new perspective in discourse interpretation. This introduction first provides the necessary background to understand what context is from both the linguistic and computational linguistic perspectives, then presents the most recent context-based approaches to NLP for social media. We conclude with an overview of the papers accepted in this special issue, highlighting what we believe are the future directions in processing social media texts."
C18-1033,Authorship Identification for Literary Book Recommendations,2018,0,3,2,0,30754,haifa alharthi,Proceedings of the 27th International Conference on Computational Linguistics,0,"Book recommender systems can help promote the practice of reading for pleasure, which has been declining in recent years. One factor that influences reading preferences is writing style. We propose a system that recommends books after learning their authors{'} style. To our knowledge, this is the first work that applies the information learned by an author-identification model to book recommendations. We evaluated the system according to a top-k recommendation scenario. Our system gives better accuracy when compared with many state-of-the-art methods. We also conducted a qualitative analysis by checking if similar books/authors were annotated similarly by experts."
W17-5307,Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference,2017,10,30,6,1,11735,qian chen,Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP},0,"The RepEval 2017 Shared Task aims to evaluate natural language understanding models for sentence representation, in which a sentence is represented as a fixed-length vector with neural networks and the quality of the representation is tested with a natural language inference task. This paper describes our system (alpha) that is ranked among the top in the Shared Task, on both the in-domain test set (obtaining a 74.9{\%} accuracy) and on the cross-domain test set (also attaining a 74.9{\%} accuracy), demonstrating that the model generalizes well to the cross-domain data. Our model is equipped with intra-sentence gated-attention composition which helps achieve a better performance. In addition to submitting our model to the Shared Task, we have also tested it on the Stanford Natural Language Inference (SNLI) dataset. We obtain an accuracy of 85.5{\%}, which is the best reported result on SNLI when cross-sentence attention is not allowed, the same condition enforced in RepEval 2017."
W17-3104,Monitoring Tweets for Depression to Detect At-risk Users,2017,9,10,2,0,31802,zunaira jamil,Proceedings of the Fourth Workshop on Computational Linguistics and Clinical Psychology {---} From Linguistic Signal to Clinical Reality,0,"We propose an automated system that can identify at-risk users from their public social media activity, more specifically, from Twitter. The data that we collected is from the {\#}BellLetsTalk campaign, which is a wide-reaching, multi-year program designed to break the silence around mental illness and support mental health across Canada. To achieve our goal, we trained a user-level classifier that can detect at-risk users that achieves a reasonable precision and recall. We also trained a tweet-level classifier that predicts if a tweet indicates depression. This task was much more difficult due to the imbalanced data. In the dataset that we labeled, we came across 5{\%} depression tweets and 95{\%} non-depression tweets. To handle this class imbalance, we used undersampling methods. The resulting classifier had high recall, but low precision. Therefore, we only use this classifier to compute the estimated percentage of depressed tweets and to add this value as a feature for the user-level classifier."
W17-2201,Metaphor Detection in a Poetry Corpus,2017,-1,-1,2,0,31981,vaibhav kesarwani,"Proceedings of the Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"Metaphor is indispensable in poetry. It showcases the poet{'}s creativity, and contributes to the overall emotional pertinence of the poem while honing its specific rhetorical impact. Previous work on metaphor detection relies on either rule-based or statistical models, none of them applied to poetry. Our method focuses on metaphor detection in a poetry corpus. It combines rule-based and statistical models (word embeddings) to develop a new classification system. Our system has achieved a precision of 0.759 and a recall of 0.804 in identifying one type of metaphor in poetry."
P17-1152,Enhanced {LSTM} for Natural Language Inference,2017,29,298,6,1,11735,qian chen,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6{\%} on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result{---}it further improves the performance even when added to the already very strong model."
E17-2088,A Dataset for Multi-Target Stance Detection,2017,23,17,2,0.740741,914,parinaz sobhani,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Current models for stance classification often treat each target independently, but in many applications, there exist natural dependencies among targets, e.g., stance towards two or more politicians in an election or towards several brands of the same product. In this paper, we focus on the problem of multi-target stance detection. We present a new dataset that we built for this task. Furthermore, We experiment with several neural models on the dataset and show that they are more effective in jointly modeling the overall position towards two related targets compared to independent predictions and other models of joint learning, such as cascading classification. We make the new dataset publicly available, in order to facilitate further research in multi-target stance classification."
W16-4702,Local-Global Vectors to Improve Unigram Terminology Extraction,2016,19,3,2,0,33559,ehsan amjadian,Proceedings of the 5th International Workshop on Computational Terminology (Computerm2016),0,The present paper explores a novel method that integrates efficient distributed representations with terminology extraction. We show that the information from a small number of observed instances can be combined with local and global word embeddings to remarkably improve the term extraction results on unigram terms. To do so we pass the terms extracted by other tools to a filter made of the local-global embeddings and a classifier which in turn decides whether or not a term candidate is a term. The filter can also be used as a hub to merge different term extraction tools into a single higher-performing system. We compare filters that use the skip-gram architecture and filters that employ the CBOW architecture for the task at hand.
W16-0207,Bilingual Chronological Classification of Hafez{'}s Poems,2016,20,0,2,0,24676,arya rahgozar,Proceedings of the Fifth Workshop on Computational Linguistics for Literature,0,"We present a novel task: the chronological classification of Hafezxe2x80x99s poems (ghazals). We compiled a bilingual corpus in digital form, with consistent idiosyncratic properties. We have used Hoomanxe2x80x99s labeled ghazals in order to train automatic classifiers to classify the remaining ghazals. Our classification framework uses a Support Vector Machine (SVM) classifier with similarity features based on Latent Dirichlet Allocation (LDA). In our analysis of the results we use the LDA topicsxe2x80x99 main terms that are passed on to a Principal Component Analysis (PCA) module."
W15-2916,How much does word sense disambiguation help in sentiment analysis of micropost data?,2015,25,10,2,0,36880,chiraag sumanth,"Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"This short paper describes a sentiment analysis system for micro-post data that includes analysis of tweets from Twitter and Short Messaging Service (SMS) text messages. We discuss our system that makes use of Word Sense Disambiguation techniques in sentiment analysis at the message level, where the entire tweet or SMS text was analysed to determine its dominant sentiment. Previous work done in the area of Word Sense Disambiguation does not throw light on its influence on the analysis of social-media text and micropost data, which is what our work aims to achieve. Our experiments show that the use of Word Sense Disambiguation alone has resulted in an improved sentiment analysis system that outperforms systems built without incorporating Word Sense Disambiguation."
W15-1527,Estimating User Location in Social Media with Stacked Denoising Auto-encoders,2015,31,12,2,0,7776,ji liu,Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,0,"Only very few users disclose their physical locations, which may be valuable and useful in applications such as marketing and security monitoring; in order to automatically detect their locations, many approaches have been proposed using various types of information, including the tweets posted by the users. It is not easy to infer the original locations from textual data, because text tends to be noisy, particularly in social media. Recently, deep learning techniques have been shown to reduce the error rate of many machine learning tasks, due to their ability to learn meaningful representations of input data. We investigate the potential of building a deep-learning architecture to infer the location of Twitter users based merely on their tweets. We find that stacked denoising auto-encoders are well suited for this task, with results comparable to state-of-the-art models."
W15-0509,From Argumentation Mining to Stance Classification,2015,32,34,2,0.740741,914,parinaz sobhani,Proceedings of the 2nd Workshop on Argumentation Mining,0,"Argumentation mining and stance classification were recently introduced as interesting tasks in text mining. In this paper, a novel framework for argument tagging based on topic modeling is proposed. Unlike other machine learning approaches for argument tagging which often require large set of labeled data, the proposed model is minimally supervised and merely a one-to-one mapping between the pre-defined argument set and the extracted topics is required. These extracted arguments are subsequently exploited for stance classification. Additionally, a manuallyannotated corpus for stance classification and argument tagging of online news comments is introduced and made available. Experiments on our collected corpus demonstrate the benefits of using topic-modeling for argument tagging. We show that using Non-Negative Matrix Factorization instead of Latent Dirichlet Allocation achieves better results for argument classification, close to the results of a supervised classifier. Furthermore, the statistical model that leverages automatically-extracted arguments as features for stance classification shows promising results."
D15-2006,Applications of Social Media Text Analysis,2015,-1,-1,2,0,37743,atefeh farzindar,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts,0,"Analyzing social media texts is a complex problem that becomes difficult to address using traditional Natural Language Processing (NLP) methods. Our tutorial focuses on presenting new methods for NLP tasks and applications that work on noisy and informal texts, such as the ones from social media.Automatic processing of large collections of social media texts is important because they contain a lot of useful information, due to the in-creasing popularity of all types of social media. Use of social media and messaging apps grew 203 percent year-on-year in 2013, with overall app use rising 115 percent over the same period, as reported by Statista, citing data from Flurry Analytics. This growth means that 1.61 billion people are now active in social media around the world and this is expected to advance to 2 billion users in 2016, led by India. The research shows that consumers are now spending daily 5.6 hours on digital media including social media and mo-bile internet usage.At the heart of this interest is the ability for users to create and share content via a variety of platforms such as blogs, micro-blogs, collaborative wikis, multimedia sharing sites, social net-working sites. The unprecedented volume and variety of user-generated content, as well as the user interaction network constitute new opportunities for understanding social behavior and building socially intelligent systems. Therefore it is important to investigate methods for knowledge extraction from social media data. Furthermore, we can use this information to detect and retrieve more related content about events, such as photos and video clips that have caption texts."
S13-2062,u{O}ttawa: System description for {S}em{E}val 2013 Task 2 Sentiment Analysis in {T}witter,2013,6,8,3,0,40563,hamid poursepanj,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"We present two systems developed at the University of Ottawa for the SemEval 2013 Task 2. The first system (for Task A) classifies the polarity / sentiment orientation of one target word in a Twitter message. The second system (for Task B) classifies the polarity of whole Twitter messages. Our two systems are very simple, based on supervised classifiers with bag-ofwords feature representation, enriched with information from several sources. We present a few additional results, besides results of the submitted runs."
R13-1003,Opinion Learning from Medical Forums,2013,16,2,4,0,41291,tanveer ali,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"Our study focuses on opinion mining of several medical forums dedicated to Hearing Loss (HL). Surgeries related to HL are the most common surgeries in North America; thus, they affect many patients and their families. We have extracted the opinions of people from these forums related to stigma of HL, consequences of HL surgeries, living with HL, failures of HL loss treatments, etc. We performed a manual annotation first with two annotators and have 93% overall agreement with kappa 0.78 and then applied Machine Learning methods to classify the data into opinionated and non-opinionated messages. Using our feature set, we achieved best F-score 0.577 and 0.585 with SVM and logistic-R classifier respectively."
I13-1077,Can {I} Hear You? Sentiment Analysis on Medical Forums,2013,13,23,4,0,41291,tanveer ali,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Text mining studies have started to investigae relations between positive and negative opinions and patientsxe2x80x99 physical health. Several studies linked the personal lexicon with health and the health-related behavior of the individual. However, few text mining studies were performed to analyze opinions expressed in a large volume of user-written Web content. Our current study focused on performing sentiment analysis on several medical forums dedicated to Hearing Loss (HL). We categorized messages posted on the forums as positive, negative and neutral. Our study had two stages: first, we applied manual annotation of the posts with two annotators and have 82.01% overall agreement with kappa 0.65 and then we applied Machine Learning techniques to classify the posts."
W12-3711,Prior versus Contextual Emotion of a Word in a Sentence,2012,30,8,2,0,4776,diman ghazi,Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis,0,"A set of words labelled with their prior emotion is an obvious place to start on the automatic discovery of the emotion of a sentence, but it is clear that context must also be considered. No simple function of the labels on the individual words may capture the overall emotion of the sentence; words are interrelated and they mutually influence their affect-related interpretation. We present a method which enables us to take the contextual emotion of a word and the syntactic structure of the sentence into account to classify sentences by emotion classes. We show that this promising method outperforms both a method based on a Bag-of-Words representation and a system based only on the prior emotions of words. The goal of this work is to distinguish automatically between prior and contextual emotion, with a focus on exploring features important for this task."
N12-1016,Segmentation Similarity and Agreement,2012,22,13,2,0,41086,chris fournier,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a new segmentation evaluation metric, called segmentation similarity (S), that quantifies the similarity between two segmentations as the proportion of boundaries that are not transformed when comparing them using edit distance, essentially using edit distance as a penalty function and scaling penalties by segmentation size. We propose several adapted inter-annotator agreement coefficients which use S that are suitable for segmentation. We show that S is configurable enough to suit a wide variety of segmentation evaluations, and is an improvement upon the state of the art. We also propose using inter-annotator agreement coefficients to evaluate automatic segmenters in terms of human performance."
N12-1038,Getting More from Segmentation Evaluation,2012,9,15,2,0,42813,martin scaiano,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce a new segmentation evaluation measure, WinPR, which resolves some of the limitations of WindowDiff. WinPR distinguishes between false positive and false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses."
W11-2826,Generation of Formal and Informal Sentences,2011,20,8,2,0,44156,fadi sheikha,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"This paper addresses the task of using natural language generation (NLG) techniques to generate sentences with formal and with informal style. We studied the main characteristics of each style, which helped us to choose parameters that can produce sentences in one of the two styles. We collected some ready-made parallel list of formal and informal words and phrases, from different sources. In addition, we added two more parallel lists: one that contains most of the contractions in English (short forms) and their full forms, and another one that consists in some common abbreviations and their full forms. These parallel lists might help to generate sentences in the preferred style, by changing words or expressions for that style. Our NLG system is built on top of the SimpleNLG package (Gatt and Reiter, 2009). We used templates from which we generated valid English texts with formal or informal style. In order to evaluate the quality of the generated sentences and their level of formality, we used human judges. The evaluation results show that our system can generate formal and informal style successfully, with high accuracy. The main contribution of our work consists in designing a set of parameters that led to good results for the task of generating texts with different formality levels."
R11-1093,Finding Negative Key Phrases for {I}nternet Advertising Campaigns using {W}ikipedia,2011,5,3,2,0,42813,martin scaiano,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"In internet advertising, negative key phrases are used in order to exclude the display of an advertisement to non-target audience. We describe a method for automatically identifying negative key phrases. We use Wikipedia as our sense inventory and as an annotated corpus from which we create context vectors and determine negative phrases, which correlate with negative senses of a positive key phrase."
W10-1912,Extraction of Disease-Treatment Semantic Relations from Biomedical Sentences,2010,10,19,2,1,19354,oana frunza,Proceedings of the 2010 Workshop on Biomedical Natural Language Processing,0,"This paper describes our study on identifying semantic relations that exist between diseases and treatments in biomedical sentences. We focus on three semantic relations: Cure, Prevent, and Side Effect. The contributions of this paper consists in the fact that better results are obtained compared to previous studies and the fact that our research settings allow the integration of biomedical and medical knowledge. We obtain 98.55% F-measure for the Cure relation, 100% F-measure for the Prevent relation, and 88.89% F-measure for the Side Effect relation."
W10-0205,A Corpus-based Method for Extracting Paraphrases of Emotion Terms,2010,20,30,2,0,45568,fazel keshtkar,Proceedings of the {NAACL} {HLT} 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,0,"Since paraphrasing is one of the crucial tasks in natural language understanding and generation, this paper introduces a novel technique to extract paraphrases for emotion terms, from non-parallel corpora. We present a bootstrapping technique for identifying paraphrases, starting with a small number of seeds. WordNet Affect emotion words are used as seeds. The bootstrapping approach learns extraction patterns for six classes of emotions. We use annotated blogs and other datasets as texts from which to extract paraphrases, based on the highest-scoring extraction patterns. The results include lexical and morpho-syntactic paraphrases, that we evaluate with human judges."
W10-0217,Hierarchical versus Flat Classification of Emotions in Text,2010,17,47,2,0,4776,diman ghazi,Proceedings of the {NAACL} {HLT} 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,0,"We explore the task of automatic classification of texts by the emotions expressed. Our novel method arranges neutrality, polarity and emotions hierarchically. We test the method on two datasets and show that it outperforms the corresponding flat approach, which does not take into account the hierarchical information. The highly imbalanced structure of most of the datasets in this area, particularly the two datasets with which we worked, has a dramatic effect on the performance of classification. The hierarchical approach helps alleviate the effect."
mihaila-etal-2010-romanian,{R}omanian Zero Pronoun Distribution: A Comparative Study,2010,11,2,3,0,39487,claudiu mihuailua,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Anaphora resolution is still a challenging research field in natural language processing, lacking a algorithm that correctly resolves anaphoric pronouns. Anaphoric zero pronouns pose an even greater challenge, since this category is not lexically realised. Thus, their resolution is conditioned by their prior identification stage. This paper reports on the distribution of zero pronouns in Romanian in various genres: encyclopaedic, legal, literary, and news-wire texts. For this purpose, the RoZP corpus has been created, containing almost 50000 tokens and 800 zero pronouns which are manually annotated. The distribution patterns are compared across genres, and exceptional cases are presented in order to facilitate the methodological process of developing a future zero pronoun identification and resolution algorithm. The evaluation results emphasise that zero pronouns appear frequently in Romanian, and their distribution depends largely on the genre. Additionally, possible features are revealed for their identification, and a search scope for the antecedent has been determined, increasing the chances of correct resolution."
C10-2035,Building Systematic Reviews Using Automatic Text Classification Techniques,2010,8,27,2,1,19354,oana frunza,Coling 2010: Posters,0,"The amount of information in medical publications continues to increase at a tremendous rate. Systematic reviews help to process this growing body of information. They are fundamental tools for evidence-based medicine. In this paper, we show that automatic text classification can be useful in building systematic reviews for medical topics to speed up the reviewing process. We propose a per-question classification method that uses an ensemble of classifiers that exploit the particular protocol of a systematic review. We also show that when integrating the classifier in the human workflow of building a review the per-question method is superior to the global method. We test several evaluation measures on a real dataset."
W09-2811,Visual Development Process for Automatic Generation of Digital Games Narrative Content,2009,5,3,2,0,46944,maria caropreso,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"Users of Natural Language Generation systems are required to have sophisticated linguistic and sometimes even programming knowledge, which has hindered the adoption of this technology by individuals outside the computational linguistics research community. We have designed and implemented a visual environment for creating and modifying NLG templates which requires no programming ability and minimum linguistic knowledge. It allows specifying templates with any number of variables and dependencies between them. Internally, it uses SimpleNLG to provide the linguistic background knowledge. We tested the performance of our system in the context of an interactive simulation game. We describe the templates used for testing and show examples of sentences that our system generates from these templates."
D09-1129,Real-Word Spelling Correction using {G}oogle {W}eb 1{T} 3-grams,2009,11,87,2,0,34291,aminul islam,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"We present a method for detecting and correcting multiple real-word spelling errors using the Google Web IT 3-gram data set and a normalized and modified version of the Longest Common Subsequence (LCS) string matching algorithm. Our method is focused mainly on how to improve the detection recall (the fraction of errors correctly detected) and the correction recall (the fraction of errors correctly amended), while keeping the respective precisions (the fraction of detections or amendments that are correct) as high as possible. Evaluation results on a standard data set show that our method outperforms two other methods on the same task."
2009.mtsummit-papers.21,Inducing translations from officially published materials in {C}anadian government websites,2009,-1,-1,2,0,47497,qibo zhu,Proceedings of Machine Translation Summit XII: Papers,0,None
W08-0623,Textual Information for Predicting Functional Properties of the Genes,2008,3,2,2,1,19354,oana frunza,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,This paper is focused on determining which proteins affect the activity of Aryl Hydrocarbon Receptor (AHR) system when learning a model that can accurately predict its activity when single genes are knocked out. Experiments with results are presented when models are trained on a single source of information: abstracts from Medline (http://medline.cos.com/) that talk about the genes involved in the experiments. The results suggest that AdaBoost classifier with a binary bag-of-words representation obtains significantly better results.
alzghool-inkpen-2008-combining,Combining Multiple Models for Speech Information Retrieval,2008,11,0,2,0,47978,muath alzghool,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included."
spracklin-etal-2008-using,Using the Complexity of the Distribution of Lexical Elements as a Feature in Authorship Attribution,2008,11,8,2,0,47999,leanne spracklin,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Traditional Authorship Attribution models extract normalized counts of lexical elements such as nouns, common words and punctuation and use these normalized counts or ratios as features for author fingerprinting. The text is viewed as a Âbag-of-wordsÂ and the order of words and their position relative to other words is largely ignored. We propose a new method of feature extraction which quantifies the distribution of lexical elements within the text using Kolmogorov complexity estimates. Testing carried out on blog corpora indicates that such measures outperform ratios when used as features in an SVM authorship attribution model. Moreover, by adding complexity estimates to a model using ratios, we were able to increase the F-measure by 5.2-11.8{\%}"
N07-1045,Near-Synonym Choice in an Intelligent Thesaurus,2007,17,14,1,1,8015,diana inkpen,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"An intelligent thesaurus assists a writer with alternative choices of words and orders them by their suitability in the writing context. In this paper we focus on methods for automatically choosing nearsynonyms by their semantic coherence with the context. Our statistical method uses the Web as a corpus to compute mutual information scores. Evaluation experiments show that this method performs better than a previous method on the same task. We also propose and evaluate two more methods, one that uses anticollocations, and one that uses supervised learning. To asses the difficulty of the task, we present results obtained by human judges."
2007.jeptalnrecital-long.8,A tool for detecting {F}rench-{E}nglish cognates and false friends,2007,-1,-1,2,1,19354,oana frunza,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cognates are pairs of words in different languages similar in spelling and meaning. They can help a second-language learner on the tasks of vocabulary expansion and reading comprehension. False friends are pairs of words that have similar spelling but different meanings. Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. In this article we present a method to automatically classify a pair of words as cognates or false friends, by using several measures of orthographic similarity as features for classification. We use this method to create complete lists of cognates and false friends between two languages. We also disambiguate partial cognates in context. We applied all our methods to French and English, but they can be applied to other pairs of languages as well. We built a tool that takes the produced lists and annotates a French text with equivalent English cognates or false friends, in order to help second-language learners improve their reading comprehension skills and retention rate."
P06-1056,Semi-Supervised Learning of Partial Cognates Using Bilingual Bootstrapping,2006,13,6,2,1,19354,oana frunza,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. Detecting the actual meaning of a partial cognate in context can be useful for Machine Translation tools and for Computer-Assisted Language Learning tools. In this paper we propose a supervised and a semi-supervised method to disambiguate partial cognates between two languages: French and English. The methods use only automatically-labeled data; therefore they can be applied for other pairs of languages as well. We also show that our methods perform well when using corpora from different domains."
N06-2016,Investigating Cross-Language Speech Retrieval for a Spontaneous Conversational Speech Collection,2006,6,7,1,1,8015,diana inkpen,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,Cross-language retrieval of spontaneous speech combines the challenges of working with noisy automated transcription and language translation. The CLEF 2005 Cross-Language Speech Retrieval (CL-SR) task provides a standard test collection to investigate these challenges. We show that we can improve retrieval performance: by careful selection of the term weighting scheme; by decomposing automated transcripts into phonetic substrings to help ameliorate transcription errors; and by combining automatic transcriptions with manually-assigned metadata. We further show that topic translation with online machine translation resources yields effective CL-SR.
islam-inkpen-2006-second,Second Order Co-occurrence {PMI} for Determining the Semantic Similarity of Words,2006,31,82,2,0,942,md islam,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents a new corpus-based method for calculating the semantic similarity of two target words. Our method, called Second Order Co-occurrencePMI (SOC-PMI), uses Pointwise Mutual Information to sort lists of important neighbor words of the two target words. Then we consider the words which are common in both lists and aggregate their PMI values (from the opposite list) to calculate the relative semantic similarity. Our method was empirically evaluated using Miller and CharlerÂs (1991) 30 noun pair subset, Ruben-stein and GoodenoughÂs (1965) 65 noun pairs, 80 synonym test questions from the Test of English as a Foreign Language (TOEFL), and 50 synonym test questions from a collection of English as a Second Language (ESL) tests. Evaluation results show that our method outperforms several competing corpus-based methods."
J06-2003,Building and Using a Lexical Knowledge Base of Near-Synonym Differences,2006,62,60,1,1,8015,diana inkpen,Computational Linguistics,0,"The initial knowledge base is later enriched with information from other machine-readable dictionaries. Information about the collocational behavior of the near-synonyms is acquired from free text. The knowledge base is used by Xenon, a natural language generation system that shows how the new lexical resource can be used to choose the best near-synonym in specific situations."
H05-1007,Semantic Similarity for Detecting Recognition Errors in Automatic Speech Transcripts,2005,27,39,1,1,8015,diana inkpen,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Browsing through large volumes of spoken audio is known to be a challenging task for end users. One way to alleviate this problem is to allow users to gist a spoken audio document by glancing over a transcript generated through Automatic Speech Recognition. Unfortunately, such transcripts typically contain many recognition errors which are highly distracting and make gisting more difficult. In this paper we present an approach that detects recognition errors by identifying words which are semantic outliers with respect to other words in the transcript. We describe several variants of this approach. We investigate a wide range of evaluation measures and we show that we can significantly reduce the number of errors in content words, with the trade-off of losing some good content words."
W02-0909,Acquiring Collocations for Lexical Choice between Near-Synonyms,2002,13,43,1,1,8015,diana inkpen,Proceedings of the {ACL}-02 Workshop on Unsupervised Lexical Acquisition,0,We extend a lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour. This type of knowledge is useful in the process of lexical choice between near-synonyms. We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech). For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster. For this task we use a much larger corpus (the Web). We also look at associations (longer-distance co-occurrences) as a possible source of learning more about nuances that the near-synonyms may carry.
