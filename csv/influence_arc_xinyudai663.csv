2020.acl-main.319,P19-1425,0,0.0599296,"Missing"
2020.acl-main.319,P18-1163,0,0.0402783,"Missing"
2020.acl-main.319,C18-1055,0,0.042899,"Missing"
2020.acl-main.319,D19-5506,0,0.0890598,"em, especially when applied to less decent real-world inputs compared to training data (Belinkov and Bisk, 2017). For example, typos may severely deteriorate system outputs (Table 1). Moreover, recent studies show that a neural machine translation system can also be broken by noisy synthetic inputs (Belinkov and Bisk, 2017; Lee et al., 2018). Due to the black-box nature of a neural system, it has been a challenge to fathom when and how the system tends to fail. Intuitively, researchers seek to apprehend such failures by the analysis of handcrafted error indicating features (Zhao et al., 2018; Karpukhin et al., 2019). This strategy is costly because it requires expert knowledge for both linguistics and the target neural architecture. Such features are also less applicable because some common errors in deep learning systems are hard to formulate, or very specific to certain architectures. Instead of designing error features, recent researchers adopt ideas from adversarial learning (Goodfellow et al., 2014) to generate adversarial examples for mining pitfalls of NLP systems (Cheng et al., 2018a; Ebrahimi et al., 2018; Zhao et al., 2017). Adversarial examples are minor perturbed inputs that keep the semantic"
2020.acl-main.319,D15-1166,0,0.162662,"Missing"
2020.acl-main.319,N19-1314,0,0.345133,"f of the batch and perturb its source using the current agent as negative samples. During D’s updates, we randomly generate a new batch of pairs from parallel data likewise to test its accuracy. D is updated at most stepD epochs, or until its test accuracy reaches acc bound. Env only1 yields -1 as overall terminal rewards when all sequences in SRC are intermediately terminated. For samples classified as negative during survival, their follow-up rewards and actions are masked as 0. If the agent survives until the end, Env yields additional averaged rd as final rewards for an episode. We follow Michel et al. (2019) to adopt relative degradation: rd = score(y, ref s) − score(y 0 , ref s) score(y, ref s) (10) where y and y 0 denote original and perturbed output, ref s are references, and score is a translation metric. If score(y, ref s) is zero, we return zero as rd . To calculate score we retokenize perturbed SRC by victim models vocabulary and tokenizer before translation. 1 It is commonly accepted that frequent negative rewards result in agents’ tendency to regard zero-reward as optimum and fail exploration, which further leads to training failure. 3489 3.2 Agent As it is shown in Figure 1 (c), the age"
2020.acl-main.319,W18-6319,0,0.0202891,"Missing"
2020.acl-main.319,P19-1020,0,0.0189011,", Ladv is determined by the goal of the attack. However, currently effective adversarial generation for NLP is to search by maximizing a surrogate gradientbased loss: argmax Ladv (x0 , x1 , ...x0i ...xn ) (7) 1≤i≤n,x0 ∈vocab where Ladv is a differentiable function indicating the adversarial object. Due to its formidable search space, this paradigm simply perturbs on a small ratio of token positions and greedy search by brute force among candidates. Note that adversarial example generation is fundamentally different from noised hidden representation in adversarial training (Cheng et al., 2019; Sano et al., 2019), which is not to be concerned in this work. 3 Approach In this section, we will describe our reinforced learning and generation of adversarial examples (Figure 1) in detail. Overall, the victim model is a part of the environment (denoted as Env), which yields rewards indicating overall degradation based on modified inputs. A reinforced agent 3488 learns to modify every source position from left to right sequentially. Meanwhile, a discriminator in Env provides every-step survival signals by determining whether SRC is ill-perturbed. 3.1 Environment We encapsulate the victim translation model wi"
2020.acl-main.319,D18-1397,0,0.0258153,"uto-regressive generation of each yi until the end of sequence symbol (EOS) is generated: P (yi |y&lt;i , X) = softmax(fdec (yi−1 , st , ct ; θdec )) (1) where ct is the attentive result for current decoder state st given H. 2.2 Actor-Critic for Reinforcement Learning Reinforcement learning (Sutton and Barto, 2018, RL) is a widely used machine learning technique following the paradigm of explore and exploit, which is apt for unsupervised policy learning in many challenging tasks (e.g., games (Mnih et al., 2015)). It is also used for direct optimization for non-differentiable learning objectives (Wu et al., 2018; Bahdanau et al., 2016) in NLP. Actor-critic (Konda and Tsitsiklis, 2000) is one of the most popular RL architectures where the agent consists of a separate policy and value networks called actor and critic. They both take in environment state st at each time step as input, while 3487 } TGT N V(st ) softmax softmax critic linear actor linear & victim NMT survival/terminal signals 2 at Y × { } ✓ 3 discriminator SRC 4 { Environment linear+dropout final degradation linear+dropout Mean Mean src bi-GRU tgt bi-GRU Mean Sum src bi-GRU Tokens embedding st agent actor rt 5 Xemb Yemb X Y 1 critic (a) O"
2020.acl-main.319,D18-1036,0,0.0199899,"al translation system, especially when applied to less decent real-world inputs compared to training data (Belinkov and Bisk, 2017). For example, typos may severely deteriorate system outputs (Table 1). Moreover, recent studies show that a neural machine translation system can also be broken by noisy synthetic inputs (Belinkov and Bisk, 2017; Lee et al., 2018). Due to the black-box nature of a neural system, it has been a challenge to fathom when and how the system tends to fail. Intuitively, researchers seek to apprehend such failures by the analysis of handcrafted error indicating features (Zhao et al., 2018; Karpukhin et al., 2019). This strategy is costly because it requires expert knowledge for both linguistics and the target neural architecture. Such features are also less applicable because some common errors in deep learning systems are hard to formulate, or very specific to certain architectures. Instead of designing error features, recent researchers adopt ideas from adversarial learning (Goodfellow et al., 2014) to generate adversarial examples for mining pitfalls of NLP systems (Cheng et al., 2018a; Ebrahimi et al., 2018; Zhao et al., 2017). Adversarial examples are minor perturbed inpu"
2020.acl-main.5,D18-1547,0,0.0447236,"Missing"
2020.acl-main.5,W19-5932,0,0.0508391,"asets. 1 Source Slot – – .. . attraction-area restaurant-food restaurant-area .. . restaurant-area – – .. . U5 : I also need a taxi to commute and need it to arrive at the restaurant. S5 : I have booked a cab to take you to the restaurant when you leave All Saint’s church. The booked car type is a yellow volkswagen. taxi-departure taxi-destination attraction-area restaurant-food restaurant-area .. . attraction-name restaurant-name restaurant-area – – .. . ontology(Henderson et al., 2014a; Mrkˇsi´c et al., 2017; Zhong et al., 2018). Open vocabulary approaches (Xu and Hu, 2018; Wu et al., 2019; Gao et al., 2019; Ren et al., 2019) break the assumption of predefined ontologies, turning to generate values only given target slots. Wu et al. (2019) propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users’ goals, dialogue state tracking (DST) is employed to estimate dialogue states at each turn. Dialogue states consist of constraints and"
2020.acl-main.5,D14-1162,0,0.0820616,"Missing"
2020.acl-main.5,D17-1206,0,0.0765432,"Missing"
2020.acl-main.5,D19-1196,0,0.116686,"Missing"
2020.acl-main.5,W14-4340,0,0.5986,"fits of explicit slot connection modeling, and our model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets. 1 Source Slot – – .. . attraction-area restaurant-food restaurant-area .. . restaurant-area – – .. . U5 : I also need a taxi to commute and need it to arrive at the restaurant. S5 : I have booked a cab to take you to the restaurant when you leave All Saint’s church. The booked car type is a yellow volkswagen. taxi-departure taxi-destination attraction-area restaurant-food restaurant-area .. . attraction-name restaurant-name restaurant-area – – .. . ontology(Henderson et al., 2014a; Mrkˇsi´c et al., 2017; Zhong et al., 2018). Open vocabulary approaches (Xu and Hu, 2018; Wu et al., 2019; Gao et al., 2019; Ren et al., 2019) break the assumption of predefined ontologies, turning to generate values only given target slots. Wu et al. (2019) propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users’ goals, di"
2020.acl-main.5,D18-1299,0,0.0248015,"Missing"
2020.acl-main.5,W14-4343,0,0.0757215,"Missing"
2020.acl-main.5,P19-1078,0,0.606512,"MultiWOZ 2.1 datasets. 1 Source Slot – – .. . attraction-area restaurant-food restaurant-area .. . restaurant-area – – .. . U5 : I also need a taxi to commute and need it to arrive at the restaurant. S5 : I have booked a cab to take you to the restaurant when you leave All Saint’s church. The booked car type is a yellow volkswagen. taxi-departure taxi-destination attraction-area restaurant-food restaurant-area .. . attraction-name restaurant-name restaurant-area – – .. . ontology(Henderson et al., 2014a; Mrkˇsi´c et al., 2017; Zhong et al., 2018). Open vocabulary approaches (Xu and Hu, 2018; Wu et al., 2019; Gao et al., 2019; Ren et al., 2019) break the assumption of predefined ontologies, turning to generate values only given target slots. Wu et al. (2019) propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users’ goals, dialogue state tracking (DST) is employed to estimate dialogue states at each turn. Dialogue states consist o"
2020.acl-main.5,P18-1133,0,0.0410285,"2 Word Copying i:wi =w In this section, we will describe DST-SC model in detail. DST-SC is an open vocabulary model based on the encoder-decoder architecture. As shown in Figure 1, there are three components that contribute to obtain the target slot value: (1) word generation from the vocabulary; (2) word copying from the dialogue history; (3) value copying from the source slot. To reduce the burden on the decoder, DST-SC also equips with a slot gate (Wu et al., 2019) to predict for slot values of none and dontcare. hi = GRU(φemb (wi )). (4) The copy mechanism is shown to be effective in DST (Lei et al., 2018; Xu and Hu, 2018; Wu et al., 2019). Here, we follow Wu et al. (2019) to augment the vanilla attention-based decoder with pointergenerator copying, enabling it to capture slot values that explicitly occur in the dialogue history. X Pwc (yt = w) = ati . (5) • We demonstrate that DST-SC is more effective for handling the related-slot problem and outperforms state-of-the-art baselines. 2.1 (3) i=1 • To the best of our knowledge, this work is the first one to discuss the related-slot problem in multi-domain DST and address it by explicitly modeling slot connections across domains. 2 (2) (1) Word G"
2020.acl-main.5,P18-1134,0,0.405076,"MultiWOZ 2.0 and MultiWOZ 2.1 datasets. 1 Source Slot – – .. . attraction-area restaurant-food restaurant-area .. . restaurant-area – – .. . U5 : I also need a taxi to commute and need it to arrive at the restaurant. S5 : I have booked a cab to take you to the restaurant when you leave All Saint’s church. The booked car type is a yellow volkswagen. taxi-departure taxi-destination attraction-area restaurant-food restaurant-area .. . attraction-name restaurant-name restaurant-area – – .. . ontology(Henderson et al., 2014a; Mrkˇsi´c et al., 2017; Zhong et al., 2018). Open vocabulary approaches (Xu and Hu, 2018; Wu et al., 2019; Gao et al., 2019; Ren et al., 2019) break the assumption of predefined ontologies, turning to generate values only given target slots. Wu et al. (2019) propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users’ goals, dialogue state tracking (DST) is employed to estimate dialogue states at each turn. Dialogue"
2020.acl-main.5,P15-2130,0,0.0603261,"Missing"
2020.acl-main.5,P18-1135,0,0.17312,"ur model achieves state-of-the-art performance on MultiWOZ 2.0 and MultiWOZ 2.1 datasets. 1 Source Slot – – .. . attraction-area restaurant-food restaurant-area .. . restaurant-area – – .. . U5 : I also need a taxi to commute and need it to arrive at the restaurant. S5 : I have booked a cab to take you to the restaurant when you leave All Saint’s church. The booked car type is a yellow volkswagen. taxi-departure taxi-destination attraction-area restaurant-food restaurant-area .. . attraction-name restaurant-name restaurant-area – – .. . ontology(Henderson et al., 2014a; Mrkˇsi´c et al., 2017; Zhong et al., 2018). Open vocabulary approaches (Xu and Hu, 2018; Wu et al., 2019; Gao et al., 2019; Ren et al., 2019) break the assumption of predefined ontologies, turning to generate values only given target slots. Wu et al. (2019) propose a copy-augmented encoder-decoder model to track dialogue states, which outperforms fixed vocabulary models and achieves the state-of-the-art result in multi-domain DST. Task-oriented dialogue systems assist users to achieve their certain goals, such as making a restaurant reservation or booking a taxi. To fulfill users’ goals, dialogue state tracking (DST) is employed to es"
2020.acl-main.5,P17-1163,0,0.0762936,"Missing"
2020.acl-main.65,W14-3348,0,0.231421,"ce on the validation set and are used across all the experiments. https://wordnet.princeton.edu/ https://en.oxforddictionaries.com/ 712 3. Pip-sem is our intuitive pipeline that consists of a sememe predictor and a definition generator. The sememe predictor is trained on HowNet and is responsible for annotating words in definition generation datasets. The definition generator is used to generate definitions given the word, context, and pseudo annotations of sememes. Metrics We adopt two several automatic metrics that are often used in generation tasks: BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014). BLEU considers the exact match between generation results and references and is the most common metric used to evaluate generation systems. Following previous work, we compute Model I-Attention (Gadetsky et al., 2018) LOG-CaD (Ishiwatari et al., 2019) *LOG-CaD † Pip-sem ESD-def † ESD-sem WordNet BLEU METEOR 23.77 / 24.79 / 24.70 8.66 25.52 11.33 25.75 11.52 26.48 12.45 Oxford BLEU METEOR 17.25 / 18.53 / 18.24 8.43 19.89 11.10 19.98 10.79 20.86 11.86 Table 2: BLEU and Meteor scores on WordNet and Oxford dataset. ‘† ’ indicates models that incorporate external sememe annotations while training"
2020.acl-main.65,P18-2043,0,0.275237,"s an important role in natural language understanding for human. It is a common practice for human to consult a dictionary when encountering unfamiliar words (Fraser, 1999). However, it is often the case that we cannot find satisfying definitions for words that are rarely used or newly created. To assist dictionary compilation and help human readers understand unfamiliar texts, generating definitions automatically is of practical significance. Noraset et al. (2017) first propose definition modeling, which is the task of generating the dictionary definition for a given word with its embedding. Gadetsky et al. (2018) extend the work by incorporating word sense disambiguation to generate context-aware word definitions.Both methods adopt a variant of encoder-decoder architecture, ∗ † Equal contribution Corresponding author captain the person in charge of a ship the person who is a member of a ship where the word to be defined is mapped to a lowdimension semantic vector by an encoder, and the decoder is responsible for generating the definition given the semantic vector. Although the existing encoder-decoder architecture (Gadetsky et al., 2018; Ishiwatari et al., 2019; Washio et al., 2019) yields reasonable"
2020.acl-main.65,N19-1350,0,0.205989,"tly modeling the “components” of definitions, leading to under-specific generation results. In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition generation, which explicitly decomposes meaning of words into semantic components, and models them with discrete latent variables for definition generation. Experimental results show that ESD achieves substantial improvements on WordNet and Oxford benchmarks over strong previous baselines. 1 Table 1: An example of the definitions of word “captain”. Reference is from Oxford dictionary and Generated is from the method of Ishiwatari et al. (2019). Introduction Dictionary definition, which provides explanatory sentences for word senses, plays an important role in natural language understanding for human. It is a common practice for human to consult a dictionary when encountering unfamiliar words (Fraser, 1999). However, it is often the case that we cannot find satisfying definitions for words that are rarely used or newly created. To assist dictionary compilation and help human readers understand unfamiliar texts, generating definitions automatically is of practical significance. Noraset et al. (2017) first propose definition modeling,"
2020.acl-main.65,P19-1430,0,0.0448,"Missing"
2020.acl-main.65,Q17-1010,0,0.0410491,"mantic components z, the word representation r∗ and the context representation H. Definition Encoder &&quot; &# &$ ℎ( 2&quot; 2# … 23 +&quot; +# … +1 /&quot; /# … /1 3.2.1 ⊕ )4 &lt;s&gt; )&quot; )# &&quot; Same as Ishiwatari et al. (2019), our encoder consists of two parts, namely word encoder and context encoder. )$ &# Definition Decoder Word Encoder The word encoder is responsible for mapping the word w∗ to a low-dimensional vector r∗ , and consists of a word embedding and a character level encoder. The word embedding is initialized by large-scale pretrained word embeddings such as GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017), and is kept fixed at the training time. Previous works (Noraset et al., 2017; Ishiwatari et al., 2019) also show that morphological information can be helpful for definition generation. We employ a convolutional neural network (Krizhevsky et al., 2012) to encode the character sequence of the word. We concatenate the word embedding and the character encoding to get the word representation r∗ . Figure 1: Neural architecture of ESD, including the word encoder, context encoder, the decoder and the definition encoder for the posterior networks. However, it is generally computationally intractable"
2020.acl-main.65,K16-1002,0,0.0421507,"architecture for a brief understanding. Following the common practice of context-aware definition models (Gadetsky et al., 2018; Ishiwatari et al., 2019), we first encode the source word w∗ 3.2.2 Semantic Components Predictor For the proposed ESD, we need to model both the semantic components posterior qφ (z|w∗ , C, D) and the prior pθ (z|w∗ , C). Semantic Components Posterior Approximator Exactly modeling the true posterior qφ (z|w∗ , C, D) is usually intractable. Therefore, we adopt an approximation method to simplify the posterior inference (Zhang et al., 2016) Following the spirit of VAE (Bowman et al., 2016), we use neural networks for better approximation in this paper. Specifically, we first compute the representation 0 HD =h1:T of the definition D = d1:T with a bidirectional LSTM network. We then obtain the representation of definition D and context C with 710 max-pooling operation. 0 hD = max-pooling(h1:T ) (5) hC = max-pooling(h1:|C |) (6) Finally, we adopt a GRU-like (Cho et al., 2014) gate mechanism to allow the decoder to dynamically fuse information from the word representation r∗ , context vector ct , and semantic context vector ot , which can be calculated as follows: With these repres"
2020.acl-main.65,I17-2070,0,0.188941,"ent codes. 5.3 Case Studies Examples of learned latent codes In Table 6, we show some examples of learned latent codes on WordNet dataset. We can see that our model does learn informative codes, i.e. words with similar meanings are assigned with similar latent codes, and codes of words with different meanings tend to differ. Related Work Definition Generation Definition modeling was firstly proposed by Noraset et al. (2017). They take a word embedding as input and generate a definition of the word. An obvious drawback is that their model cannot handle polysemous words. Recently several works (Ni and Wang, 2017; Gadetsky et al., 2018; Ishiwatari et al., 2019) consider the context-aware definition generation task, where the context is introduced to disambiguate senses of words. They all adopt a encoder-decoder architecture, and rely heavily on the decoder to extract semantic components of the word semantic, thus leading to under-specific definitions. In contrast, we introduce a group of discrete latent variables to model these semantic components explicitly. Examples of generated definitions We also list several generation samples in Table 5. We can see that the definitions generated by our method ar"
2020.acl-main.65,P17-1187,0,0.0537413,"Missing"
2020.acl-main.65,P02-1040,0,0.107702,"s are chosen based on the performance on the validation set and are used across all the experiments. https://wordnet.princeton.edu/ https://en.oxforddictionaries.com/ 712 3. Pip-sem is our intuitive pipeline that consists of a sememe predictor and a definition generator. The sememe predictor is trained on HowNet and is responsible for annotating words in definition generation datasets. The definition generator is used to generate definitions given the word, context, and pseudo annotations of sememes. Metrics We adopt two several automatic metrics that are often used in generation tasks: BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014). BLEU considers the exact match between generation results and references and is the most common metric used to evaluate generation systems. Following previous work, we compute Model I-Attention (Gadetsky et al., 2018) LOG-CaD (Ishiwatari et al., 2019) *LOG-CaD † Pip-sem ESD-def † ESD-sem WordNet BLEU METEOR 23.77 / 24.79 / 24.70 8.66 25.52 11.33 25.75 11.52 26.48 12.45 Oxford BLEU METEOR 17.25 / 18.53 / 18.24 8.43 19.89 11.10 19.98 10.79 20.86 11.86 Table 2: BLEU and Meteor scores on WordNet and Oxford dataset. ‘† ’ indicates models that incorporate ext"
2020.acl-main.65,D14-1162,0,0.0870726,"ates the target definition from the semantic components z, the word representation r∗ and the context representation H. Definition Encoder &&quot; &# &$ ℎ( 2&quot; 2# … 23 +&quot; +# … +1 /&quot; /# … /1 3.2.1 ⊕ )4 &lt;s&gt; )&quot; )# &&quot; Same as Ishiwatari et al. (2019), our encoder consists of two parts, namely word encoder and context encoder. )$ &# Definition Decoder Word Encoder The word encoder is responsible for mapping the word w∗ to a low-dimensional vector r∗ , and consists of a word embedding and a character level encoder. The word embedding is initialized by large-scale pretrained word embeddings such as GloVe (Pennington et al., 2014) or FastText (Bojanowski et al., 2017), and is kept fixed at the training time. Previous works (Noraset et al., 2017; Ishiwatari et al., 2019) also show that morphological information can be helpful for definition generation. We employ a convolutional neural network (Krizhevsky et al., 2012) to encode the character sequence of the word. We concatenate the word embedding and the character encoding to get the word representation r∗ . Figure 1: Neural architecture of ESD, including the word encoder, context encoder, the decoder and the definition encoder for the posterior networks. However, it is"
2020.acl-main.65,D19-1357,0,0.298103,"h its embedding. Gadetsky et al. (2018) extend the work by incorporating word sense disambiguation to generate context-aware word definitions.Both methods adopt a variant of encoder-decoder architecture, ∗ † Equal contribution Corresponding author captain the person in charge of a ship the person who is a member of a ship where the word to be defined is mapped to a lowdimension semantic vector by an encoder, and the decoder is responsible for generating the definition given the semantic vector. Although the existing encoder-decoder architecture (Gadetsky et al., 2018; Ishiwatari et al., 2019; Washio et al., 2019) yields reasonable generation results, it relies heavily on the decoder to extract thorough semantic components of the word, leading to under-specific definition generation results, i.e. missing some semantic components. As illustrated in Table 1, to generate a precise definition of the word “captain”, one needs to know that “captain” refers to a person, “captain” is related to ship, and “captain” manages or is in charge of the ship, where person, ship, manage are three semantic components of word “captain”. However, due to the lack of explicitly modeling of these semantic components, the mode"
2020.acl-main.65,D17-1013,1,0.830622,"finition. We first propose to leverage sememe annotations of HowNet (Dong and Dong, 2003) as an external signal to guide the learning of latent variables. As we mentioned in Section 2.3, sememes are also known to be helpful for definition generation (Yang et al., 2019). Previously, Xie et al. (2017) show that it is possible to predict sememes of words from large scale pretrained distributional representations. Suppose the set of sememes in HowNet are denoted by S = {s1 , s2 , · · · , sn }, and each word w in HowNet is annotated by a small subset of S, denoted by Sw = {si |si ∈ S}. Inspired by Weng et al. (2017), we adopt a bag-of-word loss to ensure that z is informative enough to be predictive about sememe annotations Sw : X (sem) Lcom = −log p(si |z) (8) i si ∈Sw 711 Our next motivation is that the sememes annotation is still expensive, while definitions of words are off-the-shelf when training. Inspired by Bao et al. (2019) and John et al. (2019), we enforce the model to predict every words in the target definition D=d1:T to ensure that z is informative enough: (def) Lcom = −log T X p(di |z) (9) i=1 Semantic Diversity Objective To achieve the goal of decomposing semantics, it is crucial that ther"
2020.acl-main.65,D16-1050,0,0.0206668,"detailing each component of ESD, we overview the architecture for a brief understanding. Following the common practice of context-aware definition models (Gadetsky et al., 2018; Ishiwatari et al., 2019), we first encode the source word w∗ 3.2.2 Semantic Components Predictor For the proposed ESD, we need to model both the semantic components posterior qφ (z|w∗ , C, D) and the prior pθ (z|w∗ , C). Semantic Components Posterior Approximator Exactly modeling the true posterior qφ (z|w∗ , C, D) is usually intractable. Therefore, we adopt an approximation method to simplify the posterior inference (Zhang et al., 2016) Following the spirit of VAE (Bowman et al., 2016), we use neural networks for better approximation in this paper. Specifically, we first compute the representation 0 HD =h1:T of the definition D = d1:T with a bidirectional LSTM network. We then obtain the representation of definition D and context C with 710 max-pooling operation. 0 hD = max-pooling(h1:T ) (5) hC = max-pooling(h1:|C |) (6) Finally, we adopt a GRU-like (Cho et al., 2014) gate mechanism to allow the decoder to dynamically fuse information from the word representation r∗ , context vector ct , and semantic context vector ot , whi"
2020.coling-main.329,D12-1050,0,0.0105208,"ntly. These methods are likely not sufficient to learn good enough idiom representations (Bahdanau et al., 2017); Second, since most Chinese idioms consist of four Chinese characters, another direct approach is to treat each Chinese idiom as a regular multi-word expression. In this category, most studies combine the distributional word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b) to learn representations of multi-word expressions. Additionally, some methods try to enhance the representations by employing a more powerful compositionality function (Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Socher et al., 2013), integrating combination rules (Kober et al., 2016; Weir et al., 2016), or incorporating the general linguistic knowledge (Qi et al., 2019). By combining the word embeddings, 1 Our code is available at https://github.com/njunlp/SKER * Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3684 Proceedings of the 28th International Conference on Computational Linguistics, pages 3684–3695 Barcelona, Spain (Online), December 8-13, 2020 持之以恒 愚公移山 to pursue unremi"
2020.coling-main.329,P16-1223,0,0.0248351,"graph according to the annotations from a high-quality synonym dictionary or the cosine similarity between the pre-trained idiom embeddings and then incorporate the graph attention network and gate mechanism to encode the graph. Experimental results on ChID, a large-scale Chinese idiom reading comprehension dataset, show that our model achieves state-of-the-art performance.1 1 Introduction Machine reading comprehension is the task that asks a machine to answer questions based on a given context. Although various advanced neural models have been proposed in recent years (Hermann et al., 2015; Chen et al., 2016). Chinese reading comprehension is still a challenging task (He et al., 2018; Sun et al., 2020). One of the reasons is the existence of Chinese idioms, also known as “chengyu”, whose non-literal and non-compositional characteristics make them difficult for machines to understand (Shao et al., 2018; Zheng et al., 2019). Recently, some studies (Jiang et al., 2018; Zheng et al., 2019) claimed that good idiom representations are essential for idiom comprehension. Meanwhile, many methods have been proposed to obtain idiom representations, which can be divided into two categories: First, a straightf"
2020.coling-main.329,D19-1600,0,0.342458,"ctionary. As we mainly focus on the enhancement of candidate idioms, the relationship among neighboring synonyms is ignored. However, building a high-quality dictionary is labor-intensive, and Mikolov et al. (2013a) claimed that the semantic relevance between two words can be measured by the cosine similarity of their embeddings. Therefore, we also construct the graph with pre-trained embeddings. Similarly, for node vi and vj , an undirected edge eij is added to the graph when the cosine similarity of their corresponding idiom embeddings is higher than a pre-defined threshold. As Zheng et al. (2019) claimed that there is a substantial probability that two idioms are near-synonyms when their cosine similarity is higher than 0.65 in ChID, which is the dataset we used in this work, the value of the threshold is set as 0.65. 3.3 Synonym Knowledge Enhanced Reader Overview Figure 3 gives an overview of our proposed model. The key challenge for better idiom representations and answer predictions is to utilize the synonym graph while reducing the potential noise effectively. Facing the challenge, we propose our model, which is composed of an encoding module, an aggregating module, and a predicti"
2020.coling-main.329,N19-1423,0,0.0107159,"65. 3.3 Synonym Knowledge Enhanced Reader Overview Figure 3 gives an overview of our proposed model. The key challenge for better idiom representations and answer predictions is to utilize the synonym graph while reducing the potential noise effectively. Facing the challenge, we propose our model, which is composed of an encoding module, an aggregating module, and a predicting module. Specifically, in the aggregating module, we adopt the graph attention network to fully utilize the synonym graph and meanwhile filter the potential noise with the gate mechanism. Encoding Module We utilize BERT (Devlin et al., 2019), which has shown marvelous performance in various NLP tasks, to encode passages. Concretely, we first tokenize a passage with the WordPiece vocabulary, which is used in the original paper of BERT, and then generate the input sequence by concatenating the [CLS] token, the tokenized passage, and the [SEP] token, where [CLS] and [SEP] are two special tokens used in BERT for leading and ending the sequence. The blank in the passage is replaced by the [MASK] token. Through BERT, the passage is encoded into a list of word representations. Here, we omit a detailed description of BERT and refer inter"
2020.coling-main.329,N19-1246,0,0.0168961,"These idioms can help model comprehend “顺手牵羊” more efficiently. However, in head 2, model pays more attention to “偷梁换柱” (literal meaning: replace the beams and pillars (with inferior ones)), which is a metaphorical idiom for describing adulterate. Our model can reduce its impact with the following gate mechanism. 6 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) tasks require a machine reader to answer questions with given contexts. Recently, many high-quality and large-scale reading comprehension datasets for English (Rajpurkar et al., 2016; Yang et al., 2018; Dua et al., 2019) have been constructed to facilitate research on this topic. In turn, many neural models have been proposed to tackle these MRC problems, which approach or even surpass humans (Seo et al., 2017; Yu et al., 2018; Huang et al., 2018; Hu et al., 2019). Similarly, MRC datasets for Chinese have also been constructed, such as CMRC2018 (Cui et al., 2019b), DuReader (He et al., 2018), C3 (Sun et al., 2020), etc. Among them, ChID, proposed by Zheng et al. (2019), is a cloze-style multiple-choice MRC dataset. It aims at Chinese idioms, also known as “chengyu”, whose non-literal and non-compositional sem"
2020.coling-main.329,W18-2501,0,0.0277863,"is obtained through a layer of feedforward neural network; in def, we crawl the definitions of each idiom from the Internet, encode the definitions through BERT, and then feed the average of the encoded results into a feedforward neural network, and the outputs are used as the final representations of Chinese idioms. 4.3 Implemented Details For LM, AR and SAR, we mostly follow the same architecture, optimization, and data pre-processing used in Zheng et al. (2019). For models based on BERT, we follow Cui et al. (2019a) to use LTP for Chinese word segmentation and implement them with AllenNLP (Gardner et al., 2018). Instead of using the official BERT (Chinese), we employ an open source BERT with whole word masking5 as the pretrained encoder, which is more effective in handing Chinese texts. We use the embeddings from Song et al. (2018) to construct the Chinese idiom synonym graph. The random idiom embeddings are initialized according to Glorot and Bengio (2010) and the size of embeddings is 200. For BERT-based models, there is an extra matrix that can convert the above idiom embeddings into 768-dimensional vectors, which is the hidden size of BERT. Adam (Kingma and Ba, 2015) is used to optimize all the"
2020.coling-main.329,D11-1129,0,0.0301257,"Chinese idioms appear less frequently. These methods are likely not sufficient to learn good enough idiom representations (Bahdanau et al., 2017); Second, since most Chinese idioms consist of four Chinese characters, another direct approach is to treat each Chinese idiom as a regular multi-word expression. In this category, most studies combine the distributional word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b) to learn representations of multi-word expressions. Additionally, some methods try to enhance the representations by employing a more powerful compositionality function (Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Socher et al., 2013), integrating combination rules (Kober et al., 2016; Weir et al., 2016), or incorporating the general linguistic knowledge (Qi et al., 2019). By combining the word embeddings, 1 Our code is available at https://github.com/njunlp/SKER * Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3684 Proceedings of the 28th International Conference on Computational Linguistics, pages 3684–3695 Barcelona, Spain (Online), December 8-13, 2020 持"
2020.coling-main.329,W18-2605,0,0.153841,"he cosine similarity between the pre-trained idiom embeddings and then incorporate the graph attention network and gate mechanism to encode the graph. Experimental results on ChID, a large-scale Chinese idiom reading comprehension dataset, show that our model achieves state-of-the-art performance.1 1 Introduction Machine reading comprehension is the task that asks a machine to answer questions based on a given context. Although various advanced neural models have been proposed in recent years (Hermann et al., 2015; Chen et al., 2016). Chinese reading comprehension is still a challenging task (He et al., 2018; Sun et al., 2020). One of the reasons is the existence of Chinese idioms, also known as “chengyu”, whose non-literal and non-compositional characteristics make them difficult for machines to understand (Shao et al., 2018; Zheng et al., 2019). Recently, some studies (Jiang et al., 2018; Zheng et al., 2019) claimed that good idiom representations are essential for idiom comprehension. Meanwhile, many methods have been proposed to obtain idiom representations, which can be divided into two categories: First, a straightforward way is to treat each Chinese idiom as a single token and obtain its r"
2020.coling-main.329,D19-1170,0,0.0131585,"Our model can reduce its impact with the following gate mechanism. 6 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) tasks require a machine reader to answer questions with given contexts. Recently, many high-quality and large-scale reading comprehension datasets for English (Rajpurkar et al., 2016; Yang et al., 2018; Dua et al., 2019) have been constructed to facilitate research on this topic. In turn, many neural models have been proposed to tackle these MRC problems, which approach or even surpass humans (Seo et al., 2017; Yu et al., 2018; Huang et al., 2018; Hu et al., 2019). Similarly, MRC datasets for Chinese have also been constructed, such as CMRC2018 (Cui et al., 2019b), DuReader (He et al., 2018), C3 (Sun et al., 2020), etc. Among them, ChID, proposed by Zheng et al. (2019), is a cloze-style multiple-choice MRC dataset. It aims at Chinese idioms, also known as “chengyu”, whose non-literal and non-compositional semantic characteristics pose unique challenges for machines to understand. Idiom Representation Good idiom representations are crucial for natural language understanding (Liu et al., 2019; Zheng et al., 2019). Recent works made noticeable efforts in"
2020.coling-main.329,W18-0516,0,0.093614,"t performance.1 1 Introduction Machine reading comprehension is the task that asks a machine to answer questions based on a given context. Although various advanced neural models have been proposed in recent years (Hermann et al., 2015; Chen et al., 2016). Chinese reading comprehension is still a challenging task (He et al., 2018; Sun et al., 2020). One of the reasons is the existence of Chinese idioms, also known as “chengyu”, whose non-literal and non-compositional characteristics make them difficult for machines to understand (Shao et al., 2018; Zheng et al., 2019). Recently, some studies (Jiang et al., 2018; Zheng et al., 2019) claimed that good idiom representations are essential for idiom comprehension. Meanwhile, many methods have been proposed to obtain idiom representations, which can be divided into two categories: First, a straightforward way is to treat each Chinese idiom as a single token and obtain its representation by training on a large-scale corpus (Song et al., 2018; Zheng et al., 2019). However, compared to common words, Chinese idioms appear less frequently. These methods are likely not sufficient to learn good enough idiom representations (Bahdanau et al., 2017); Second, since"
2020.coling-main.329,D16-1175,0,0.0206647,"sentations (Bahdanau et al., 2017); Second, since most Chinese idioms consist of four Chinese characters, another direct approach is to treat each Chinese idiom as a regular multi-word expression. In this category, most studies combine the distributional word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b) to learn representations of multi-word expressions. Additionally, some methods try to enhance the representations by employing a more powerful compositionality function (Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Socher et al., 2013), integrating combination rules (Kober et al., 2016; Weir et al., 2016), or incorporating the general linguistic knowledge (Qi et al., 2019). By combining the word embeddings, 1 Our code is available at https://github.com/njunlp/SKER * Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3684 Proceedings of the 28th International Conference on Computational Linguistics, pages 3684–3695 Barcelona, Spain (Online), December 8-13, 2020 持之以恒 愚公移山 to pursue unremittingly the old man moves mountains synonymic relationship persevering in"
2020.coling-main.329,P19-1552,0,0.0284443,"humans (Seo et al., 2017; Yu et al., 2018; Huang et al., 2018; Hu et al., 2019). Similarly, MRC datasets for Chinese have also been constructed, such as CMRC2018 (Cui et al., 2019b), DuReader (He et al., 2018), C3 (Sun et al., 2020), etc. Among them, ChID, proposed by Zheng et al. (2019), is a cloze-style multiple-choice MRC dataset. It aims at Chinese idioms, also known as “chengyu”, whose non-literal and non-compositional semantic characteristics pose unique challenges for machines to understand. Idiom Representation Good idiom representations are crucial for natural language understanding (Liu et al., 2019; Zheng et al., 2019). Recent works made noticeable efforts in improving the quality of idiom representations by either treating each idiom as a single token (Song et al., 2018; Zheng et al., 2019) or regarding idioms as regular multi-word expressions (Socher et al., 2013; Weir et al., 2016; Qi et al., 2019). However, the former approaches require large-scale corpora, while the latter approaches demonstrate the essentiality of semantic compositionality (Pelletier, 1994), which many idioms do not have. Notably, both approaches treat idioms separately and ignore the relationship among them. Diff"
2020.coling-main.329,P19-1571,0,0.0546377,"se characters, another direct approach is to treat each Chinese idiom as a regular multi-word expression. In this category, most studies combine the distributional word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b) to learn representations of multi-word expressions. Additionally, some methods try to enhance the representations by employing a more powerful compositionality function (Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Socher et al., 2013), integrating combination rules (Kober et al., 2016; Weir et al., 2016), or incorporating the general linguistic knowledge (Qi et al., 2019). By combining the word embeddings, 1 Our code is available at https://github.com/njunlp/SKER * Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3684 Proceedings of the 28th International Conference on Computational Linguistics, pages 3684–3695 Barcelona, Spain (Online), December 8-13, 2020 持之以恒 愚公移山 to pursue unremittingly the old man moves mountains synonymic relationship persevering in doing something ?! 水滴石穿 坚持不懈 dripping water penetrates the stone to persevere unremittin"
2020.coling-main.329,L18-1005,0,0.575519,"prehension dataset, show that our model achieves state-of-the-art performance.1 1 Introduction Machine reading comprehension is the task that asks a machine to answer questions based on a given context. Although various advanced neural models have been proposed in recent years (Hermann et al., 2015; Chen et al., 2016). Chinese reading comprehension is still a challenging task (He et al., 2018; Sun et al., 2020). One of the reasons is the existence of Chinese idioms, also known as “chengyu”, whose non-literal and non-compositional characteristics make them difficult for machines to understand (Shao et al., 2018; Zheng et al., 2019). Recently, some studies (Jiang et al., 2018; Zheng et al., 2019) claimed that good idiom representations are essential for idiom comprehension. Meanwhile, many methods have been proposed to obtain idiom representations, which can be divided into two categories: First, a straightforward way is to treat each Chinese idiom as a single token and obtain its representation by training on a large-scale corpus (Song et al., 2018; Zheng et al., 2019). However, compared to common words, Chinese idioms appear less frequently. These methods are likely not sufficient to learn good eno"
2020.coling-main.329,D13-1170,0,0.0196736,"ikely not sufficient to learn good enough idiom representations (Bahdanau et al., 2017); Second, since most Chinese idioms consist of four Chinese characters, another direct approach is to treat each Chinese idiom as a regular multi-word expression. In this category, most studies combine the distributional word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b) to learn representations of multi-word expressions. Additionally, some methods try to enhance the representations by employing a more powerful compositionality function (Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Socher et al., 2013), integrating combination rules (Kober et al., 2016; Weir et al., 2016), or incorporating the general linguistic knowledge (Qi et al., 2019). By combining the word embeddings, 1 Our code is available at https://github.com/njunlp/SKER * Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3684 Proceedings of the 28th International Conference on Computational Linguistics, pages 3684–3695 Barcelona, Spain (Online), December 8-13, 2020 持之以恒 愚公移山 to pursue unremittingly the old man mo"
2020.coling-main.329,N18-2028,0,0.37355,"tence of Chinese idioms, also known as “chengyu”, whose non-literal and non-compositional characteristics make them difficult for machines to understand (Shao et al., 2018; Zheng et al., 2019). Recently, some studies (Jiang et al., 2018; Zheng et al., 2019) claimed that good idiom representations are essential for idiom comprehension. Meanwhile, many methods have been proposed to obtain idiom representations, which can be divided into two categories: First, a straightforward way is to treat each Chinese idiom as a single token and obtain its representation by training on a large-scale corpus (Song et al., 2018; Zheng et al., 2019). However, compared to common words, Chinese idioms appear less frequently. These methods are likely not sufficient to learn good enough idiom representations (Bahdanau et al., 2017); Second, since most Chinese idioms consist of four Chinese characters, another direct approach is to treat each Chinese idiom as a regular multi-word expression. In this category, most studies combine the distributional word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b) to learn representations of multi-word expressions. Additionally, some methods try to enhance the representations"
2020.coling-main.329,2020.tacl-1.10,0,0.0859776,"ity between the pre-trained idiom embeddings and then incorporate the graph attention network and gate mechanism to encode the graph. Experimental results on ChID, a large-scale Chinese idiom reading comprehension dataset, show that our model achieves state-of-the-art performance.1 1 Introduction Machine reading comprehension is the task that asks a machine to answer questions based on a given context. Although various advanced neural models have been proposed in recent years (Hermann et al., 2015; Chen et al., 2016). Chinese reading comprehension is still a challenging task (He et al., 2018; Sun et al., 2020). One of the reasons is the existence of Chinese idioms, also known as “chengyu”, whose non-literal and non-compositional characteristics make them difficult for machines to understand (Shao et al., 2018; Zheng et al., 2019). Recently, some studies (Jiang et al., 2018; Zheng et al., 2019) claimed that good idiom representations are essential for idiom comprehension. Meanwhile, many methods have been proposed to obtain idiom representations, which can be divided into two categories: First, a straightforward way is to treat each Chinese idiom as a single token and obtain its representation by tr"
2020.coling-main.329,J16-4006,0,0.0924756,"et al., 2017); Second, since most Chinese idioms consist of four Chinese characters, another direct approach is to treat each Chinese idiom as a regular multi-word expression. In this category, most studies combine the distributional word embeddings (Mikolov et al., 2013a; Mikolov et al., 2013b) to learn representations of multi-word expressions. Additionally, some methods try to enhance the representations by employing a more powerful compositionality function (Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Socher et al., 2013), integrating combination rules (Kober et al., 2016; Weir et al., 2016), or incorporating the general linguistic knowledge (Qi et al., 2019). By combining the word embeddings, 1 Our code is available at https://github.com/njunlp/SKER * Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3684 Proceedings of the 28th International Conference on Computational Linguistics, pages 3684–3695 Barcelona, Spain (Online), December 8-13, 2020 持之以恒 愚公移山 to pursue unremittingly the old man moves mountains synonymic relationship persevering in doing something ?!"
2020.coling-main.329,D18-1259,0,0.0303575,"ning and semantics. These idioms can help model comprehend “顺手牵羊” more efficiently. However, in head 2, model pays more attention to “偷梁换柱” (literal meaning: replace the beams and pillars (with inferior ones)), which is a metaphorical idiom for describing adulterate. Our model can reduce its impact with the following gate mechanism. 6 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) tasks require a machine reader to answer questions with given contexts. Recently, many high-quality and large-scale reading comprehension datasets for English (Rajpurkar et al., 2016; Yang et al., 2018; Dua et al., 2019) have been constructed to facilitate research on this topic. In turn, many neural models have been proposed to tackle these MRC problems, which approach or even surpass humans (Seo et al., 2017; Yu et al., 2018; Huang et al., 2018; Hu et al., 2019). Similarly, MRC datasets for Chinese have also been constructed, such as CMRC2018 (Cui et al., 2019b), DuReader (He et al., 2018), C3 (Sun et al., 2020), etc. Among them, ChID, proposed by Zheng et al. (2019), is a cloze-style multiple-choice MRC dataset. It aims at Chinese idioms, also known as “chengyu”, whose non-literal and no"
2020.coling-main.329,P19-1075,0,0.105182,"show that our model achieves state-of-the-art performance.1 1 Introduction Machine reading comprehension is the task that asks a machine to answer questions based on a given context. Although various advanced neural models have been proposed in recent years (Hermann et al., 2015; Chen et al., 2016). Chinese reading comprehension is still a challenging task (He et al., 2018; Sun et al., 2020). One of the reasons is the existence of Chinese idioms, also known as “chengyu”, whose non-literal and non-compositional characteristics make them difficult for machines to understand (Shao et al., 2018; Zheng et al., 2019). Recently, some studies (Jiang et al., 2018; Zheng et al., 2019) claimed that good idiom representations are essential for idiom comprehension. Meanwhile, many methods have been proposed to obtain idiom representations, which can be divided into two categories: First, a straightforward way is to treat each Chinese idiom as a single token and obtain its representation by training on a large-scale corpus (Song et al., 2018; Zheng et al., 2019). However, compared to common words, Chinese idioms appear less frequently. These methods are likely not sufficient to learn good enough idiom representat"
2020.coling-main.70,P19-1052,0,0.203345,"he lack of ASC data, enormous labeled data of document-level sentiment classification (DSC) are available at online review sites such as Amazon and Yelp. These reviews contain substantial sentiment knowledge and semantic patterns. Therefore, one meaningful but challenging research question is how to leverage resource-rich DSC data to improve the low-resource task ASC. For this purpose, He et al. (2018) design the PRET+MULT framework to transfer sentiment knowledge from DSC data to ASC task through sharing shallow embedding and LSTM layer. Inspired by the capsule network (Sabour et al., 2017), Chen and Qian (2019) propose TransCap to share bottom three capsule layers, then separate two tasks only in the last ClassCap layer. Fundamentally, PRET+MULT and Transcap improve ASC by sharing parameters and multi-task learning, but they cannot accurately control and interpret what knowledge to be transferred. In this work, we directly focus on the aforementioned attention issue in the ASC task and propose a novel framework, Attention Transfer Network (ATN), to explicitly transfer attention knowledge from the DSC task for improving the attention capability of the ASC task. Compared with PRET+MULT and Transcap, o"
2020.coling-main.70,D17-1047,0,0.316817,"ny feature engineering, and achieve competitive results on the ASC task (Tang et al., 2016a). From the above example, we can see that a sentence sometimes refers to several opinion targets and they may express different sentiment polarities, thus one main challenge of ASC is to separate different opinion contexts for different targets. To this end, abundant state-of-the-art works employ attention mechanism (Bahdanau et al., 2014) to capture sentiment words related to the given target, and then aggregate them to make sentiment prediction (Wang et al., 2016; Tang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Majumder et al., 2018; Fan et al., 2018). Despite the effectiveness of attention mechanism, we argue that it fails to reach the full potential due to the limited ASC labeled data. It is well-known that the promising results of deep learning heavily rely on sufficient training data. However, the annotation ∗ Authors contributed equally. Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. † Licence details: http:// 811 Proceedings of the 28th International Conference on Computational Linguistics, page"
2020.coling-main.70,P18-1235,0,0.049408,"Missing"
2020.coling-main.70,D18-1380,0,0.16751,"itive results on the ASC task (Tang et al., 2016a). From the above example, we can see that a sentence sometimes refers to several opinion targets and they may express different sentiment polarities, thus one main challenge of ASC is to separate different opinion contexts for different targets. To this end, abundant state-of-the-art works employ attention mechanism (Bahdanau et al., 2014) to capture sentiment words related to the given target, and then aggregate them to make sentiment prediction (Wang et al., 2016; Tang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Majumder et al., 2018; Fan et al., 2018). Despite the effectiveness of attention mechanism, we argue that it fails to reach the full potential due to the limited ASC labeled data. It is well-known that the promising results of deep learning heavily rely on sufficient training data. However, the annotation ∗ Authors contributed equally. Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. † Licence details: http:// 811 Proceedings of the 28th International Conference on Computational Linguistics, pages 811–821 Barcelona, Spain (Online), Decem"
2020.coling-main.70,P18-2092,0,0.362064,"esponding sentiment polarity. The difficulty of annotation leads to that existing public aspect-level datasets are all relatively small-scale, which finally limits the potential of attention mechanism. Despite the lack of ASC data, enormous labeled data of document-level sentiment classification (DSC) are available at online review sites such as Amazon and Yelp. These reviews contain substantial sentiment knowledge and semantic patterns. Therefore, one meaningful but challenging research question is how to leverage resource-rich DSC data to improve the low-resource task ASC. For this purpose, He et al. (2018) design the PRET+MULT framework to transfer sentiment knowledge from DSC data to ASC task through sharing shallow embedding and LSTM layer. Inspired by the capsule network (Sabour et al., 2017), Chen and Qian (2019) propose TransCap to share bottom three capsule layers, then separate two tasks only in the last ClassCap layer. Fundamentally, PRET+MULT and Transcap improve ASC by sharing parameters and multi-task learning, but they cannot accurately control and interpret what knowledge to be transferred. In this work, we directly focus on the aforementioned attention issue in the ASC task and pr"
2020.coling-main.70,P11-1016,0,0.0594452,"positive, neutral, negative) of a given opinion target in a review sentence. An opinion target, also known as aspect term, refers to a word or a phrase in review describing an aspect of an entity. For example, the sentence “The tastes are great, but the service is dreadful” consists of two opinion targets, namely “tastes” and “service”. User’s sentiment towards the opinion target “tastes” is positive while negative in terms of target “service”. Traditional methods usually focus on designing a set of features such as bag-of-words or sentiment lexicon to train a classifier (e.g., SVM) for ASC (Jiang et al., 2011; Kiritchenko et al., 2014). Motivated by the great success of deep learning in computer vision (Krizhevsky et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2003), recent works use neural networks to learn low-dimensional and continuous text representations without any feature engineering, and achieve competitive results on the ASC task (Tang et al., 2016a). From the above example, we can see that a sentence sometimes refers to several opinion targets and they may express different sentiment polarities, thus one main challenge of ASC is to s"
2020.coling-main.70,S14-2076,0,0.414816,"negative) of a given opinion target in a review sentence. An opinion target, also known as aspect term, refers to a word or a phrase in review describing an aspect of an entity. For example, the sentence “The tastes are great, but the service is dreadful” consists of two opinion targets, namely “tastes” and “service”. User’s sentiment towards the opinion target “tastes” is positive while negative in terms of target “service”. Traditional methods usually focus on designing a set of features such as bag-of-words or sentiment lexicon to train a classifier (e.g., SVM) for ASC (Jiang et al., 2011; Kiritchenko et al., 2014). Motivated by the great success of deep learning in computer vision (Krizhevsky et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2003), recent works use neural networks to learn low-dimensional and continuous text representations without any feature engineering, and achieve competitive results on the ASC task (Tang et al., 2016a). From the above example, we can see that a sentence sometimes refers to several opinion targets and they may express different sentiment polarities, thus one main challenge of ASC is to separate different opinion c"
2020.coling-main.70,N18-1169,0,0.15738,"target-dependent sentence representation rs for sentiment prediction. 3 3.1 Experiments Datasets and Metrics We evaluate our model on two ASC benchmark datasets from SemEval 2014 Task 4 (Pontiki et al., 2014). They respectively contain reviews from Restaurant and Laptop domains. Following previous studies (Tang et al., 2016b; Chen et al., 2017; He et al., 2018), we remove samples with conflicting polarities in all datasets. The statistics of the ASC datasets are shown in Table 1. To pre-train the DSC module, we employ two larget-scale DSC datasets, respectively Yelp Review and Amazon Review (Li et al., 2018a). The DSC dataset Yelp Review is applied to transfer attention knowledge for the ASC dataset Restaurant. The Amazon Review is used for the dataset Laptop. Table 2 shows their statistics. In this work, we adopt Accuracy and Macro-F1 score as the metrics to evaluate the performance of different methods on the ASC task. 3.2 Experimental Settings In our experiments, word embeddings are initialized by 300-dimension GloVe (Pennington et al., 2014). After initialization, the word vectors are fixed and not fine-tuned during the training stage. All the weight matrices and biases are given the initial"
2020.coling-main.70,P18-1087,0,0.109958,"target-dependent sentence representation rs for sentiment prediction. 3 3.1 Experiments Datasets and Metrics We evaluate our model on two ASC benchmark datasets from SemEval 2014 Task 4 (Pontiki et al., 2014). They respectively contain reviews from Restaurant and Laptop domains. Following previous studies (Tang et al., 2016b; Chen et al., 2017; He et al., 2018), we remove samples with conflicting polarities in all datasets. The statistics of the ASC datasets are shown in Table 1. To pre-train the DSC module, we employ two larget-scale DSC datasets, respectively Yelp Review and Amazon Review (Li et al., 2018a). The DSC dataset Yelp Review is applied to transfer attention knowledge for the ASC dataset Restaurant. The Amazon Review is used for the dataset Laptop. Table 2 shows their statistics. In this work, we adopt Accuracy and Macro-F1 score as the metrics to evaluate the performance of different methods on the ASC task. 3.2 Experimental Settings In our experiments, word embeddings are initialized by 300-dimension GloVe (Pennington et al., 2014). After initialization, the word vectors are fixed and not fine-tuned during the training stage. All the weight matrices and biases are given the initial"
2020.coling-main.70,P16-1200,0,0.0383541,"ocument d. We pre-train the DSC module by minimizing the cross-entropy loss between the predicted sentiment distribution and the ground truth. After pre-training is finished, all parameters in the DSC module are fixed. 2.3 Base ASC Module As shown in the left part of Figure 1, the base ASC module has a similar architecture to the DSC module. The difference is that the ASC task needs to model opinion target information. To obtain target-aware context representations, we additionally employ position embedding besides word embedding, which is an effective method of modeling position information (Lin et al., 2016; Gehring et al., 2016). Therefore, the base ASC module is an attention-based BiLSTM network enhanced with position embedding. Specifically, given a sentence s = {w1 , w2 , ..., wn } and an opinion target t = {wl , wl+1 , ..., wr } in s, we first map each word wi into its word embedding representation wi by using the word embedding table. 813 To incorporate opinion target information with position embedding, we calculate the relative distance li of each word wi to the opinion target t:   l − i if i &lt; l, li = 0 (4) if l ≤ i ≤ r,   i − r otherwise . The distance index li is mapped into the"
2020.coling-main.70,D18-1377,0,0.0249678,"Missing"
2020.coling-main.70,D16-1046,0,0.024206,"icient labeled data finally limits the effectiveness of attention mechanism for the ASC task. Different from the above methods, we improve the attention capacity of the ASC model in this work, by transferring substantial attention knowledge from the DSC model pre-trained with resourcerich document-level sentiment classification data. 4.2 Transfer Learning Transfer learning aims to extract knowledge from one or more source tasks and then apply them to a target task. Neural transfer learning has proven effective for image recognition (Donahue et al., 2014) and natural language processing tasks (Mou et al., 2016; Dong and De Melo, 2018; Wu et al., 2020). He et al. (2018) are the first to transfer knowledge from document-level review data to improve the ASC task through sharing embedding and LSTM layers. Chen and Qian (2019) employ capsule network to share bottom features between the ASC task and DSC task. In this work, we aim to transfer attention knowledge from the DSC model explicitly to improve the effectiveness of attention mechanism for the ASC task. In contrast to the two existing works, our proposed approaches show better performance and good interpretability. 5 Conclusion Insufficient labeled"
2020.coling-main.70,D14-1162,0,0.0840275,"stics of the ASC datasets are shown in Table 1. To pre-train the DSC module, we employ two larget-scale DSC datasets, respectively Yelp Review and Amazon Review (Li et al., 2018a). The DSC dataset Yelp Review is applied to transfer attention knowledge for the ASC dataset Restaurant. The Amazon Review is used for the dataset Laptop. Table 2 shows their statistics. In this work, we adopt Accuracy and Macro-F1 score as the metrics to evaluate the performance of different methods on the ASC task. 3.2 Experimental Settings In our experiments, word embeddings are initialized by 300-dimension GloVe (Pennington et al., 2014). After initialization, the word vectors are fixed and not fine-tuned during the training stage. All the weight matrices and biases are given the initial value by sampling from the uniform distribution U (−0.1, 0.1). The dimension of LSTM cell hidden states is set to 300. We employ stochastic gradient descent (SGD) 815 Dataset Restaurant-Train Restaurant-Test Laptop-Train Laptop-Test #Pos 2164 728 994 341 #Neg 807 196 870 128 #Neu 637 196 464 169 #Total 3608 1120 2328 638 Datasets Yelp Review Amazon Review #Pos 266k 277k #Neg 177k 277k #Total 443k 554k Table 2: Statistics of the DSC datasets."
2020.coling-main.70,S14-2004,0,0.285918,"datasets to improve the attention capability of the aspect-level sentiment classification task. In the ATN model, we design two different methods to transfer attention knowledge and conduct experiments on two ASC benchmark datasets. Extensive experimental results show that our methods consistently outperform state-of-the-art works. Further analysis also validates the effectiveness of ATN. Our code and dataset are available at https://github.com/1429904852/ATN. 1 Introduction Aspect-level sentiment classification (ASC) is a fundamental task in sentiment analysis (Pang et al., 2008; Liu, 2012; Pontiki et al., 2014), which aims to infer the sentiment polarity (e.g. positive, neutral, negative) of a given opinion target in a review sentence. An opinion target, also known as aspect term, refers to a word or a phrase in review describing an aspect of an entity. For example, the sentence “The tastes are great, but the service is dreadful” consists of two opinion targets, namely “tastes” and “service”. User’s sentiment towards the opinion target “tastes” is positive while negative in terms of target “service”. Traditional methods usually focus on designing a set of features such as bag-of-words or sentiment l"
2020.coling-main.70,C16-1311,0,0.411824,"negative in terms of target “service”. Traditional methods usually focus on designing a set of features such as bag-of-words or sentiment lexicon to train a classifier (e.g., SVM) for ASC (Jiang et al., 2011; Kiritchenko et al., 2014). Motivated by the great success of deep learning in computer vision (Krizhevsky et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2003), recent works use neural networks to learn low-dimensional and continuous text representations without any feature engineering, and achieve competitive results on the ASC task (Tang et al., 2016a). From the above example, we can see that a sentence sometimes refers to several opinion targets and they may express different sentiment polarities, thus one main challenge of ASC is to separate different opinion contexts for different targets. To this end, abundant state-of-the-art works employ attention mechanism (Bahdanau et al., 2014) to capture sentiment words related to the given target, and then aggregate them to make sentiment prediction (Wang et al., 2016; Tang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Majumder et al., 2018; Fan et al., 2018). Despite the effectiveness of"
2020.coling-main.70,D16-1021,0,0.347723,"negative in terms of target “service”. Traditional methods usually focus on designing a set of features such as bag-of-words or sentiment lexicon to train a classifier (e.g., SVM) for ASC (Jiang et al., 2011; Kiritchenko et al., 2014). Motivated by the great success of deep learning in computer vision (Krizhevsky et al., 2012), speech recognition (Dahl et al., 2012) and natural language processing (Bengio et al., 2003), recent works use neural networks to learn low-dimensional and continuous text representations without any feature engineering, and achieve competitive results on the ASC task (Tang et al., 2016a). From the above example, we can see that a sentence sometimes refers to several opinion targets and they may express different sentiment polarities, thus one main challenge of ASC is to separate different opinion contexts for different targets. To this end, abundant state-of-the-art works employ attention mechanism (Bahdanau et al., 2014) to capture sentiment words related to the given target, and then aggregate them to make sentiment prediction (Wang et al., 2016; Tang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Majumder et al., 2018; Fan et al., 2018). Despite the effectiveness of"
2020.coling-main.70,D16-1058,0,0.427627,"imensional and continuous text representations without any feature engineering, and achieve competitive results on the ASC task (Tang et al., 2016a). From the above example, we can see that a sentence sometimes refers to several opinion targets and they may express different sentiment polarities, thus one main challenge of ASC is to separate different opinion contexts for different targets. To this end, abundant state-of-the-art works employ attention mechanism (Bahdanau et al., 2014) to capture sentiment words related to the given target, and then aggregate them to make sentiment prediction (Wang et al., 2016; Tang et al., 2016b; Ma et al., 2017; Chen et al., 2017; Majumder et al., 2018; Fan et al., 2018). Despite the effectiveness of attention mechanism, we argue that it fails to reach the full potential due to the limited ASC labeled data. It is well-known that the promising results of deep learning heavily rely on sufficient training data. However, the annotation ∗ Authors contributed equally. Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. † Licence details: http:// 811 Proceedings of the 28th Int"
2020.coling-main.70,P18-1234,0,0.0133811,"prediction. MemNet (Tang et al., 2016b) uses multi-hops attention on the word embeddings to generate the targetdependent sentence representation. RAM (Chen et al., 2017) works similar to the method MemNet. It employs BiLSTM to build memory and applies GRU-based multi-hops attention. IARM (Majumder et al., 2018) incoporates the neighboring targets-related information for ASC by using memory networks. MGAN (Fan et al., 2018) proposes a fine-grained attention mechanism to capture the word-level interaction between target and context, then combines it with coarse-grained attention for ASC. GCAE (Xue and Li, 2018) uses a convolutional neural network (CNN) with gating mechanisms to perform the ASC task. TNet (Li et al., 2018b) proposes target specific transformation component to integrate target information into the word representation. (II). Besides, we also compare two existing methods using transferred knowledge from large-scale DSC data to facilitate the ASC task: PRET+MULT (He et al., 2018) shares shadow embedding and LSTM layers between the ASC model and the DSC model through multi-task learning. TransCap (Chen and Qian, 2019) employs capsule network to share the bottom features between the ASC ta"
2020.findings-emnlp.234,P19-1048,0,0.238944,"hand-crafted features, including Imperatively Defined Factor graph (IDF) (Klinger and Cimiano, 2013a), joint inference based on IDF (Klinger and Cimiano, 2013b), and Integer Linear Programming (ILP) (Yang and Cardie, 2013). However, these methods heavily depend on the quality of handcrafted features and sometimes perform worse than pipeline methods (Klinger and Cimiano, 2013b). The opinion triplet extraction is a new aspectoriented fine-grained opinion extraction task (Peng et al., 2019). Inspired by extracting (aspect term, sentiment) pair in a joint model (Li et al., 2019; Luo et al., 2019; He et al., 2019), Peng et al. (2019) propose a two-stage framework to extract opinion triplets. In the first stage, they first use a neural model to extract the pair (aspect term, sentiment) and unpaired opinion terms, then detect the pair relation between aspect term and opinion terms in the second stage. We can see that the key opinon pair extraction of aspect term and opinion term is still accomplished in pipeline and their approach also suffers from error propagation. 6 4 5 59.53 57.83 56.45 56.29 71.60 70.58 68.59 68.01 Conclusions Aspect-oriented fine-grained opinion extraction (AFOE), including opinion"
2020.findings-emnlp.234,P82-1020,0,0.816073,"Missing"
2020.findings-emnlp.234,Q17-1010,0,0.00864571,"evaluate the performance of different methods, we use precision, recall, and F1-score as the evaluation metrics. The extracted aspect terms and opinion terms are regarded as correct only if predicted and ground truth spans are exactly matched. 4.2 Experimental Settings Following the design of DE-CNN (Xu et al., 2018), we use double embeddings to initialize the word vectors of GTS-CNN and GTS-BiLSTM, which contains a domain-general embedding from 300-dimension GloVe (Pennington et al., 2014) pre-trained with 840 billion tokens and a 100dimension domain-specific embedding trained with fastText (Bojanowski et al., 2017). The CNN kernel size on domain-specific embedding is 3 and others are 5. In GTS-BiLSTM, the dimension of LSTM cell is set to 50. We adopt Adam optimizer (Kingma and Ba, 2015) to optimize networks and the initial learning rate is 0.001. The dropout (Srivastava et al., 2014) is applied after embedding layer with probability 0.5. As for GTS-BERT, we use uncased BERTBASE version2 and set the learn2 https://github.com/google-research/bert ing rate to 5e-5. The mini-batch size is set to 32. The development set is used for early stopping. We run each model five times and report the average result of"
2020.findings-emnlp.234,P19-1520,0,0.289331,"services. Opinion term refers to the term in a sentence † ∗ Corresponding author. hot dogs, coffee top notch, average (hot dogs, top notch), (coffee, average) (hot dogs, top notch, positive), (coffee, average, neutral) used to express attitudes or opinions explicitly. For example, in the sentence of Figure 1, “hot dogs” and “coffee” are two aspect terms, “top notch” and “average” are two opinion terms. To obtain the above two opinion factors, many works devote to the co-extraction of aspect term and opinion term in a joint framework (Wang et al., 2016, 2017; Li and Lam, 2017; Yu et al., 2019; Dai and Song, 2019). However, the extracted results of these works are two separate sets of aspect term and opinion term, and they neglect the pair relation between them, which is crucial for downstream sentiment analysis tasks and has many potential applications, such as providing sentiment clues for aspect level sentiment classification (Pontiki et al., 2014), generating fine-grained opinion summarization (Zhuang et al., 2006) or analyzing in-depth opinions (Kobayashi et al., 2007), etc. Opinion pair extraction (OPE) is to extract all opinion pairs from a sentence in the form of (aspect term, opinion term). An"
2020.findings-emnlp.234,N19-1423,0,0.0150602,"n Xu et al. (2018). BiLSTM. BiLSTM employs a standard forward Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and a backward LSTM to encode the sentence, then concatenate the hidden states in two LSTMs as the representation hi of each word wi . BERT. BERT adopts subwords embedding, position embedding and segment embedding as the representation of subword, then employs a multilayer bidirectional Transformer (Vaswani et al., 2017) to generate the contextual represenations {h1 , h2 , · · · , hn } of the given sentence s. For a more comprehensive description, readers can refer to Devlin et al. (2019). To obtain a robust representation for word-pair (wi , wj ), we additionally employ an attention layer to enhance the connection between wi and wj . The 2579 details are as follows: uij = v> (Wa1 hi + Wa2 hj + ba ), exp(uij ) αij = Pn , k=1 exp(uik ) n X e hi = hi + αij hj , (1) p0ij = softmax(Ws rij + bs ), (2) (3) j=1 where Wa1 and Wa2 are weight matrices, and ba is the bias. Note that, the above attention is not applied on the representations of BERT, because BERT itself contains multiple self-attention layers. Finally, we concatenate the enhanced representations of wi and wj to represent"
2020.findings-emnlp.234,N19-1259,1,0.93962,"oriented fine-grained opinion extraction, including OPE and OTE, with one unified tagging task instead of pipelines. Besides, this new scheme is easily extended to other pair/triplet extraction tasks from text. By reviewing the aspect-based sentiment analysis (ABSA) (Pontiki et al., 2014) research, we can summarize two types of state-of-the-art pipeline approaches to extract opinion pairs: (I). Co-extraction (Wang et al., 2017; Dai and Song, 2019)+Pair relation Detection (PD) (Xu et al., 2018); (II). Aspect term Extraction (AE) (Xu et al., 2018)+Aspect-oriented Opinion Term Extraction (AOTE) (Fan et al., 2019). Nevertheless, pipeline approaches easily suffer from error propagation and inconvenience in real-world scenarios. To address the above issues and facilitate the research of AFOE, we propose a novel tagging scheme, Grid Tagging Scheme (GTS), which transforms opinion pair extraction into one unified grid tagging task. In this grid tagging task, we tag all word-pair relations and then decode all opinion pairs simultaneously with our proposed decoding method. Accordingly, GTS can extract all opinion factors of OPE in one step, instead of pipelines. Furthermore, different opinion factors are mutu"
2020.findings-emnlp.234,D17-1310,0,0.036617,"g feature or entity of products or services. Opinion term refers to the term in a sentence † ∗ Corresponding author. hot dogs, coffee top notch, average (hot dogs, top notch), (coffee, average) (hot dogs, top notch, positive), (coffee, average, neutral) used to express attitudes or opinions explicitly. For example, in the sentence of Figure 1, “hot dogs” and “coffee” are two aspect terms, “top notch” and “average” are two opinion terms. To obtain the above two opinion factors, many works devote to the co-extraction of aspect term and opinion term in a joint framework (Wang et al., 2016, 2017; Li and Lam, 2017; Yu et al., 2019; Dai and Song, 2019). However, the extracted results of these works are two separate sets of aspect term and opinion term, and they neglect the pair relation between them, which is crucial for downstream sentiment analysis tasks and has many potential applications, such as providing sentiment clues for aspect level sentiment classification (Pontiki et al., 2014), generating fine-grained opinion summarization (Zhuang et al., 2006) or analyzing in-depth opinions (Kobayashi et al., 2007), etc. Opinion pair extraction (OPE) is to extract all opinion pairs from a sentence in the f"
2020.findings-emnlp.234,P19-1056,0,0.0597472,"ng algorithms and hand-crafted features, including Imperatively Defined Factor graph (IDF) (Klinger and Cimiano, 2013a), joint inference based on IDF (Klinger and Cimiano, 2013b), and Integer Linear Programming (ILP) (Yang and Cardie, 2013). However, these methods heavily depend on the quality of handcrafted features and sometimes perform worse than pipeline methods (Klinger and Cimiano, 2013b). The opinion triplet extraction is a new aspectoriented fine-grained opinion extraction task (Peng et al., 2019). Inspired by extracting (aspect term, sentiment) pair in a joint model (Li et al., 2019; Luo et al., 2019; He et al., 2019), Peng et al. (2019) propose a two-stage framework to extract opinion triplets. In the first stage, they first use a neural model to extract the pair (aspect term, sentiment) and unpaired opinion terms, then detect the pair relation between aspect term and opinion terms in the second stage. We can see that the key opinon pair extraction of aspect term and opinion term is still accomplished in pipeline and their approach also suffers from error propagation. 6 4 5 59.53 57.83 56.45 56.29 71.60 70.58 68.59 68.01 Conclusions Aspect-oriented fine-grained opinion extraction (AFOE),"
2020.findings-emnlp.234,D14-1162,0,0.0860488,"Missing"
2020.findings-emnlp.234,S15-2082,0,0.388553,"Missing"
2020.findings-emnlp.234,S14-2004,0,0.888109,"aspect terms, “top notch” and “average” are two opinion terms. To obtain the above two opinion factors, many works devote to the co-extraction of aspect term and opinion term in a joint framework (Wang et al., 2016, 2017; Li and Lam, 2017; Yu et al., 2019; Dai and Song, 2019). However, the extracted results of these works are two separate sets of aspect term and opinion term, and they neglect the pair relation between them, which is crucial for downstream sentiment analysis tasks and has many potential applications, such as providing sentiment clues for aspect level sentiment classification (Pontiki et al., 2014), generating fine-grained opinion summarization (Zhuang et al., 2006) or analyzing in-depth opinions (Kobayashi et al., 2007), etc. Opinion pair extraction (OPE) is to extract all opinion pairs from a sentence in the form of (aspect term, opinion term). An opinion pair consists of an aspect term and a corresponding opinion term. This task needs to extract three opinion factors, i.e., aspect terms, opinion terms, and the pair relation between them. Figure 1 shows an example. We can see that the sentence “the hot dogs are top notch and great coffee!” contains two opinion pairs, respectively (hot"
2020.findings-emnlp.234,D16-1059,0,0.0375123,"in a sentence representing feature or entity of products or services. Opinion term refers to the term in a sentence † ∗ Corresponding author. hot dogs, coffee top notch, average (hot dogs, top notch), (coffee, average) (hot dogs, top notch, positive), (coffee, average, neutral) used to express attitudes or opinions explicitly. For example, in the sentence of Figure 1, “hot dogs” and “coffee” are two aspect terms, “top notch” and “average” are two opinion terms. To obtain the above two opinion factors, many works devote to the co-extraction of aspect term and opinion term in a joint framework (Wang et al., 2016, 2017; Li and Lam, 2017; Yu et al., 2019; Dai and Song, 2019). However, the extracted results of these works are two separate sets of aspect term and opinion term, and they neglect the pair relation between them, which is crucial for downstream sentiment analysis tasks and has many potential applications, such as providing sentiment clues for aspect level sentiment classification (Pontiki et al., 2014), generating fine-grained opinion summarization (Zhuang et al., 2006) or analyzing in-depth opinions (Kobayashi et al., 2007), etc. Opinion pair extraction (OPE) is to extract all opinion pairs"
2020.findings-emnlp.234,P18-2094,0,0.314111,"scheme, Grid Tagging Scheme (GTS). To the best of our knowledge, GTS is the first work to address the complete aspect-oriented fine-grained opinion extraction, including OPE and OTE, with one unified tagging task instead of pipelines. Besides, this new scheme is easily extended to other pair/triplet extraction tasks from text. By reviewing the aspect-based sentiment analysis (ABSA) (Pontiki et al., 2014) research, we can summarize two types of state-of-the-art pipeline approaches to extract opinion pairs: (I). Co-extraction (Wang et al., 2017; Dai and Song, 2019)+Pair relation Detection (PD) (Xu et al., 2018); (II). Aspect term Extraction (AE) (Xu et al., 2018)+Aspect-oriented Opinion Term Extraction (AOTE) (Fan et al., 2019). Nevertheless, pipeline approaches easily suffer from error propagation and inconvenience in real-world scenarios. To address the above issues and facilitate the research of AFOE, we propose a novel tagging scheme, Grid Tagging Scheme (GTS), which transforms opinion pair extraction into one unified grid tagging task. In this grid tagging task, we tag all word-pair relations and then decode all opinion pairs simultaneously with our proposed decoding method. Accordingly, GTS ca"
2020.findings-emnlp.234,P13-1161,0,0.0988575,"ogs, top notch) and (coffee, average) (the former is the aspect term, and latter 2576 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2576–2585 c November 16 - 20, 2020. 2020 Association for Computational Linguistics represents the corresponding opinion term). OPE sometimes could be complicated because an aspect term may correspond to several opinion terms and vice versa. Despite the great importance of OPE, it is still under-investigated, and only a few early works mentioned or explored this task (Hu and Liu, 2004; Zhuang et al., 2006; Klinger and Cimiano, 2013b; Yang and Cardie, 2013). only with one unified grid tagging task. The main contributions of this work can be summarized as follows: • We propose a novel tagging scheme, Grid Tagging Scheme (GTS). To the best of our knowledge, GTS is the first work to address the complete aspect-oriented fine-grained opinion extraction, including OPE and OTE, with one unified tagging task instead of pipelines. Besides, this new scheme is easily extended to other pair/triplet extraction tasks from text. By reviewing the aspect-based sentiment analysis (ABSA) (Pontiki et al., 2014) research, we can summarize two types of state-of-the-a"
2020.findings-emnlp.234,D18-1244,0,0.0546896,"Missing"
2020.nlptea-1.7,P06-1032,0,0.0298971,"te matcher and probability fusion mechanism. 2 3 Related Work Grammatical error diagnosis models appeared as early as the 1980s. Early grammatical error diagnosis models used rule-based methods to check and correct grammatical errors (Naber D, 2003). However, because the design of matching rules requires rich linguistic knowledge, it has become more and more difficult as well as time-consuming to design rules for such models. In order to deal with more complex error types, a series of grammatical error detection and correction models based on machine translation technology have been proposed. Brockett et al. (2006) proposed a model that uses Statistical Machine Tranalation (SMT) techniques to detect and correct grammatical errors, which deal with mass/count noun confusions by translating the incorrect phrases as a whole. Felice et al. (2014) proposed a model for grammatical error diagnosis which combines rule-based and SMT systems in a pipeline. The model first uses rules to detect errors and generate candidates. After the candidates are roughly screened by the n-gram language model, they are sent to the SMT model for further screening. In the 3.1 Methodology Baseline Model Similar to most previous mode"
2020.nlptea-1.7,I17-4006,0,0.214173,"olve the CGED2018 shared task, Hu et al. (2018) proposed a sequence-to-sequence network to model the problem, and used a semi-supervised method to generate pseudo-grammatical error data for training the model. Models based on machine translation require a large-scale training corpus to train the model. Inspired by the powerful capabilities of Neural Machine Translation (NMT) in grammatical error diagnosis, Zheng et al. (2016) regarded CGED as a sequence labeling problem, and used the powerful feature learning ability of an LSTM network to model the input sequence, and achieved better results. Yang et al. (2017) incorporated more grammatical features into the model based on the BiLSTM-CRF framework. Based on the LSTMCRF error detection model, Li et al. (2018) combined three error correction models: a rule-based model, an NMT GEC model, and an SMT GEC model. The three GEC models aid the BiLSTMCRF model in marking possible error locations during the detection phase. Fu et al. (2018) designed a model that incorporates richer features and added a template matcher and probability fusion mechanism. 2 3 Related Work Grammatical error diagnosis models appeared as early as the 1980s. Early grammatical error d"
2020.nlptea-1.7,2020.acl-main.82,0,0.0210696,"ords. It should be noted that we also merge the character position information in the vocabulary into these feature items. Figure 1: The base model of the BiLSTM-CRF framework used by BSGED 3.2 BERT-Encoder and Gating mechanism Unlike previous models based on the BiLSTMCRF architecture, BSGED does not utilize overly complex feature engineering, but uses the novel BERT model to obtain a token embedding representation of the input sequence. As a pre-trained language model, BERT has been successfully applied to many natural language understanding tasks, such as Chinese spelling error correction (Zhang et al. 2020). Due to its powerful semantic extraction capabilities, we utilize BERT as a semantic feature extractor, converting characters into vector representations. In order to preserve the long-term dependencies on the input sequence better, BSGED takes the final layer output of the BERT model as part of the BiLSTM input, instead of concatenating it with the output results of the other features through the BiLSTM network. Experiments verify that this operation can further improve the overall performance of BSGED. 1 Figure 2: Schematic diagram of the features used in BSGED We propose a novel fusion mec"
2020.nlptea-1.7,W13-3601,0,0.0263261,"tic Intelligence and Knowledge Engineering Research, Nanjing, China {caoyc, heliang, robertr}@smail.nju.edu.cn daixinyu@nju.edu.cn Abstract Compared with English grammatical error diagnosis, Chinese grammatical error correction has received limited interest in the research community. English grammar error detection models began being developed as early as the 1980s, such as the early Writer’s Workbench system (Macdonald NH, 1983) for detecting punctuation errors and style errors. Later, a series of tasks for English grammatical error detection and correction were proposed, such as CoNLL-2013 (Ng et al., 2013) and CoNLL-2014 (Ng et al., 2014). With the release of the CGED task in the NLPTEA workshop in recent years, grammar diagnosis models for Chinese have also begun to be developed. The goal of the CGED task is to use natural language processing techniques to diagnose grammatical errors in Chinese sentences written by learners who use Chinese as a second language. The CGED task allows researchers to exchange experiences and ultimately promote the development of this shared task. It defines four types of Chinese grammatical errors, which are: redundant words (denoted as a capital ""R""), missing wor"
2020.nlptea-1.7,W18-3708,0,0.0354036,"Missing"
2021.acl-short.69,D18-1461,0,0.0772069,"urface form. Despite their empirical effectiveness, subword algorithms may produce improper segmentation due to their data-dependent nature. NMT models ∗ † are typically robust to such errors when trained on large corpora or the target language is regular in morphological changes, like French or German. However, the problem will arise when such conditions are not met, i.e. there is not enough data for learning compact composition rules or the target language is morphologically rich and complex. An alternative segmentation choice is to use fully character-level (CHAR) models (Lee et al., 2017; Cherry et al., 2018; Gupta et al., 2019; Gao et al., 2020; Banar et al., 2020), which has the potential to alleviate above issues. CHAR does not need to learn any segmentation rules and keeps all available information in the surface form, avoiding the risk of information loss due to improper segmentation. What is more, the main pain point of CHAR that it takes too long to train is less obvious in above settings since there is not as much data as in the rich resource setting. However, there has not been a comprehensive study in these settings. In this paper, we conduct a systematic comparison between CHAR and oth"
2021.acl-short.69,W02-0603,0,0.290502,"ing with low resource, and adapting to unseen domains. Experimental results show strong competitiveness of character-based models. Further analyses show that compared to subword-based models, character-based models are better at handling morphological phenomena, generating rare and unknown words, and more suitable for transferring to unseen domains. 1 Introduction Neural machine translation (NMT) has achieved great success in recent years. Modern NMT systems typically operate on subword level, using segmentation algorithms such as byte pair encoding (BPE) (Sennrich et al., 2016) or Morfessor (Creutz and Lagus, 2002). Compared to word-level models, subword segmentation helps overcome the out-ofvocabulary (OOV) problem and make better use of morphological information in the surface form. Despite their empirical effectiveness, subword algorithms may produce improper segmentation due to their data-dependent nature. NMT models ∗ † are typically robust to such errors when trained on large corpora or the target language is regular in morphological changes, like French or German. However, the problem will arise when such conditions are not met, i.e. there is not enough data for learning compact composition rules"
2021.acl-short.69,N19-1154,0,0.0176604,"T (Cherry et al., 2018; Banar et al., 2020) mainly focus on reducing computation cost of them. Cherry et al. (2018) show that by employing source sequence compression techniques, the quality and efficiency of character-based models can be properly balanced. Banar et al. (2020) share the same idea as Cherry et al. (2018) but build their models using Transformer architecture. Our work differs from theirs in that we aim to analyze the performance of existing models instead of exploring novel architectures. There are also several researches on comparison between CHAR and other subword algorithms (Durrani et al., 2019; Gupta et al., 2019). Durrani et al. (2019) compare character-based models and subword-based models in terms of representation quality, and find that representation learned by the former are more suitable for modeling morphology, and more robust to noisy input. Gupta et al. (2019) investigate the performance of different segmentation algorithms when using Transformer architecture, and find that character-based models can achieve better performance when translating noisy text or text from a different domain. Our finds are consistent with them, yet we conduct a more large-scale and in-depth ana"
2021.acl-short.69,2020.acl-main.145,0,0.0611721,"ctiveness, subword algorithms may produce improper segmentation due to their data-dependent nature. NMT models ∗ † are typically robust to such errors when trained on large corpora or the target language is regular in morphological changes, like French or German. However, the problem will arise when such conditions are not met, i.e. there is not enough data for learning compact composition rules or the target language is morphologically rich and complex. An alternative segmentation choice is to use fully character-level (CHAR) models (Lee et al., 2017; Cherry et al., 2018; Gupta et al., 2019; Gao et al., 2020; Banar et al., 2020), which has the potential to alleviate above issues. CHAR does not need to learn any segmentation rules and keeps all available information in the surface form, avoiding the risk of information loss due to improper segmentation. What is more, the main pain point of CHAR that it takes too long to train is less obvious in above settings since there is not as much data as in the rich resource setting. However, there has not been a comprehensive study in these settings. In this paper, we conduct a systematic comparison between CHAR and other subword algorithms, e.g. BPE and Mo"
2021.acl-short.69,W17-3204,0,0.158226,"d unknown words. 4 Translation Across Distant Domains Domain robustness (M¨uller et al., 2020), which refers to models’ generalization ability on unseen domains, is important for NMT applications. However, subword algorithms need to learn segmentation rules from a given corpus, which may be domain-specific. When applied to a new domain, they may improperly segment target-domain specific words, hurting the domain robustness. In contrast, CHAR does not suffer from the issue. In this section, we investigate how different segmentation algorithms affect NMT models’ domain robustness. 4.1 Following Koehn and Knowles (2017), each time we train a source domain model on one of four subsets and report results on test sets of the other three domain. We experiment in two settings: No Adapt and Finetune. The first one involves no target domain data, while the latter uses randomly sampled 100k sentence pairs from target domain data to finetune the source domain model. 4.2 Results We report the average out-of-domain (OOD) BLEU scores of NMT systems based on different segmentation algorithms in Figure 3a and Figure 3b. As can be seen from the figure, CHAR surpasses other algorithms in almost all settings, except when fin"
2021.acl-short.69,P18-1007,0,0.0186422,"ect translation of domain-specific and OOV words, which may be segmented improperly by subword algorithms. No adapting Finetune Word Char BPE Morf. BPE-D 11.03 30.26 12.46 40.53 9.02 39.53 9.74 38.49 11.11 40.26 Table 3: Average OOD BLEU of models based on different subword algorithms when adapting from Law to other domains. BPE-D: BPE-dropout (Provilkov et al., 2020) 4.4 Comparison with Advanced Segmentation Algorithms Although we focus on deterministic segmentation algorithms in this paper, there are more advanced ones such as BPE-dropout (Provilkov et al., 2020) and subword regularization (Kudo, 2018), which produce multiple segmentation candidates when training and show improved performance. Therefore, we also conduct experiments comparing CHAR with BPE-dropout in terms of domain adaptation performance. We take the setting of adapting from Law to other domains and report results in Table 3. As can be seen, although BPEdropout surpasses BPE by a large margin, CHAR still achieves the best performance, which again shows the superiority of CHAR. 5 Related Work Character-level neural machine translation has received growing attention in recent years. Lee et al. (2017) first propose a fully cha"
2021.acl-short.69,Q17-1026,0,0.113687,"formation in the surface form. Despite their empirical effectiveness, subword algorithms may produce improper segmentation due to their data-dependent nature. NMT models ∗ † are typically robust to such errors when trained on large corpora or the target language is regular in morphological changes, like French or German. However, the problem will arise when such conditions are not met, i.e. there is not enough data for learning compact composition rules or the target language is morphologically rich and complex. An alternative segmentation choice is to use fully character-level (CHAR) models (Lee et al., 2017; Cherry et al., 2018; Gupta et al., 2019; Gao et al., 2020; Banar et al., 2020), which has the potential to alleviate above issues. CHAR does not need to learn any segmentation rules and keeps all available information in the surface form, avoiding the risk of information loss due to improper segmentation. What is more, the main pain point of CHAR that it takes too long to train is less obvious in above settings since there is not as much data as in the rich resource setting. However, there has not been a comprehensive study in these settings. In this paper, we conduct a systematic comparison"
2021.acl-short.69,2020.emnlp-main.203,0,0.064111,"Missing"
2021.acl-short.69,2020.amta-research.14,0,0.0522654,"Missing"
2021.acl-short.69,I17-2050,0,0.0184904,"ree of four morphological phenomena on which CHAR falls behind are so-called stability features (Burlot et al., 2018), which are expressed differently in the source language but should be expressed identically in the target language2 . The disadvantage of CHAR in this kind of phenomena shows CHAR-based model may be less robust to lexical changes to source-side changes, and the reason needs to be further researched. 3 Translation with Low Resource of different resource conditions. For validation and test, we use the original development and test split. Previous works (Sennrich and Zhang, 2019; Nguyen and Chiang, 2017) show that in low resource settings the evaluation results can be sensitive to model size (e.g. hidden dimension, layer number) and the number of BPE merges k, so we run an additional search of hidden dimension, layer number and k, and report the best results in this section. See Appendix A for details. 3.2 We evaluate models with BLEU and chrF3. The results are showed in Figure 1. In general, the performances of CHAR and BPE are on par, and are better than Word and Morfessor. In different data conditions, the results varies. Subword algorithms help alleviate the OOV problem. However, most of"
2021.acl-short.69,P02-1040,0,0.109432,"introflexive, and Vietnamese (Vi) and Malaysian (Ml) for isolating. We use OPUS-100 corpus1 (Tiedemann, 2012), which consists of 1M parallel sentences for each language pair. Model and Hyperparameters We use the Transformer architecture (Vaswani et al., 2017) throughout all experiments. To ensure results’ reliability , we run an exhaustive search of hyperparameters including batch size and learning rate. Detailed hyperparameters can be found in Appendix A. 2.2 Results The results are listed in Table 1. We can see that CHAR outperforms other algorithms in 7 out of 8 languages in terms of BLEU (Papineni et al., 2002) and chrF3 (Popovi´c, 2015), showing strong competitiveness of CHAR’s ability across languages. The only exception is the En-Fr language pair, which are known to be quite similar and is beneficial for BPE to learn a joint segmentation model. It is intuitive that BPE and Morfessor cannot outperform CHAR on introflexive languages (Hi, Ar). Introflexive languages follows non-concatenative morphology (McCarthy, 1981), i.e. grammatical 1 http://data.statmt.org/opus-100-corpus/v1.0/supervised/ Word Char BPE Morf. 55.6 49.6 60.6 36.6 73.6 96.6 33.8 51.4 83.2 74.6 48.8 38.4 9.2 65.4 70.8 83.0 67.0 61."
2021.acl-short.69,W15-3049,0,0.0500774,"Missing"
2021.acl-short.69,2020.acl-main.170,0,0.027137,"nt with findings in Section 3. While performances of CHAR and subword-based algorithms are on par on common words, CHAR outperforms the others by a large margin on domainspecific words. This suggests that the advantage of CHAR mainly comes from the correct translation of domain-specific and OOV words, which may be segmented improperly by subword algorithms. No adapting Finetune Word Char BPE Morf. BPE-D 11.03 30.26 12.46 40.53 9.02 39.53 9.74 38.49 11.11 40.26 Table 3: Average OOD BLEU of models based on different subword algorithms when adapting from Law to other domains. BPE-D: BPE-dropout (Provilkov et al., 2020) 4.4 Comparison with Advanced Segmentation Algorithms Although we focus on deterministic segmentation algorithms in this paper, there are more advanced ones such as BPE-dropout (Provilkov et al., 2020) and subword regularization (Kudo, 2018), which produce multiple segmentation candidates when training and show improved performance. Therefore, we also conduct experiments comparing CHAR with BPE-dropout in terms of domain adaptation performance. We take the setting of adapting from Law to other domains and report results in Table 3. As can be seen, although BPEdropout surpasses BPE by a large m"
2021.acl-short.69,P16-1162,0,0.0601632,"ypologically diverse languages, training with low resource, and adapting to unseen domains. Experimental results show strong competitiveness of character-based models. Further analyses show that compared to subword-based models, character-based models are better at handling morphological phenomena, generating rare and unknown words, and more suitable for transferring to unseen domains. 1 Introduction Neural machine translation (NMT) has achieved great success in recent years. Modern NMT systems typically operate on subword level, using segmentation algorithms such as byte pair encoding (BPE) (Sennrich et al., 2016) or Morfessor (Creutz and Lagus, 2002). Compared to word-level models, subword segmentation helps overcome the out-ofvocabulary (OOV) problem and make better use of morphological information in the surface form. Despite their empirical effectiveness, subword algorithms may produce improper segmentation due to their data-dependent nature. NMT models ∗ † are typically robust to such errors when trained on large corpora or the target language is regular in morphological changes, like French or German. However, the problem will arise when such conditions are not met, i.e. there is not enough data"
2021.acl-short.69,P19-1021,0,0.0175049,"nguages. Interestingly, three of four morphological phenomena on which CHAR falls behind are so-called stability features (Burlot et al., 2018), which are expressed differently in the source language but should be expressed identically in the target language2 . The disadvantage of CHAR in this kind of phenomena shows CHAR-based model may be less robust to lexical changes to source-side changes, and the reason needs to be further researched. 3 Translation with Low Resource of different resource conditions. For validation and test, we use the original development and test split. Previous works (Sennrich and Zhang, 2019; Nguyen and Chiang, 2017) show that in low resource settings the evaluation results can be sensitive to model size (e.g. hidden dimension, layer number) and the number of BPE merges k, so we run an additional search of hidden dimension, layer number and k, and report the best results in this section. See Appendix A for details. 3.2 We evaluate models with BLEU and chrF3. The results are showed in Figure 1. In general, the performances of CHAR and BPE are on par, and are better than Word and Morfessor. In different data conditions, the results varies. Subword algorithms help alleviate the OOV"
2021.acl-short.69,tiedemann-2012-parallel,0,0.0102544,"only focus on performances of characterlevel models when translating to fusional and agglutinative languages (Gupta et al., 2019; Libovick´y and Fraser, 2020), we conduct a comprehensive study covering all four morphological categories. 2.1 Experiment Setup Dataset We consider the translation from English to eight target languages representing four morphological categories, i.e. French (Fr) and Romanian (Ro) for fusional, Finnish (Fi) and Turkish (Tr) for agglutinative, Hebrew (He) and Arabic (Ar) for introflexive, and Vietnamese (Vi) and Malaysian (Ml) for isolating. We use OPUS-100 corpus1 (Tiedemann, 2012), which consists of 1M parallel sentences for each language pair. Model and Hyperparameters We use the Transformer architecture (Vaswani et al., 2017) throughout all experiments. To ensure results’ reliability , we run an exhaustive search of hyperparameters including batch size and learning rate. Detailed hyperparameters can be found in Appendix A. 2.2 Results The results are listed in Table 1. We can see that CHAR outperforms other algorithms in 7 out of 8 languages in terms of BLEU (Papineni et al., 2002) and chrF3 (Popovi´c, 2015), showing strong competitiveness of CHAR’s ability across la"
2021.acl-short.69,N19-1097,0,0.0435671,"Missing"
2021.emnlp-main.679,2020.emnlp-main.38,0,0.211355,"), for the first time, we propose to investigate the problem of few/zero-shot labels in LMTC tasks from a meta-learning perspective. Illustrated in Fig. 1, we simulate some few/zero-shot scenarios, which are faithful to the LMTC task and thereby provide chances for models to learn how to adapt fast and efficiently with a limited amount of data. However, most meta-learning algorithms are designed for multi-class classification under the fewshot setting (Vinyals et al., 2016), and it is critical for meta-learned models’ generalization to construct faithful and diverse tasks (Snell et al., 2017; Bansal et al., 2020). We argue that the simple extension of these approaches to multi-label classification is sub-optimal for the LMTC tasks in that (1) LMTC tasks need to cope with few- and zero-shot scenarios, while existing methods only consider few-shot ones, which is not faithful to the LMTC tasks. (2) LMTC tasks often face the challenge of long-tailed data distribution. However, these algorithms are not designed for specific data distribution and thereby makes the rare labels in the training set less involved in the meta-learning process, which reduces the diversity of the tasks. • Our method outperforms th"
2021.emnlp-main.679,D16-1076,0,0.0629727,"Missing"
2021.emnlp-main.679,P19-3015,0,0.0281833,"t al. (2019) and Rios and Kavuluru (2018), we also report the harmonic average across all R@K and all nDCG@K scores for methods that can predict zero-shot labels. 5.3 Baselines Following Lu et al. (2020), we compare the following baselines. CNN (Kim, 2014) uses convolutional neural networks with max-pooling to extract text features, which are then used to make the predictions for the labels. RCNN (Lai et al., 2015) uses recurrent neural networks with a convolution layer to consider both long-distance and local dependencies. It achieves best the performances across competitive text encoders in Liu et al. (2019). CAML (Mullenbach et al., 2018) is a model designed for clinical notes and text documents. It more improvement with a smaller threshold. Thus, we believe that a smaller threshold would not affect the conclusions of our experiments. 8637 MIMIC-III Frequent R@10 nDCG@10 Few-shot R@10 nDCG@10 Zero-shot R@10 nDCG@10 Harmonic Average R@10 nDCG@10 CNN RCNN CAML 34.6 43.9 41.2 44.2 56.0 53.3 5.5 14.2 5.9 2.9 9.8 3.9 - - - - ZAGRU + SIMPLE - EXT + META - LMTC 49.0 49.2 49.7* 61.3 61.8 62.6* 26.9 27.2 29.1* 17.7 17.8 20.2* 34.7 35.4 38.8* 22.2 23.5 24.1 34.7 35.2 37.4* 25.5 26.1 28.0* ZAGGRU + SIMPLE"
2021.emnlp-main.679,2020.emnlp-main.235,0,0.323521,"Extensive experiments show that META - LMTC achieves state-of-the-art permation. Specifically, Rios and Kavuluru (2018) formance against strong baselines and can still utilizes label textual descriptors to generate a feaenhance powerful BERTlike models. ture vector for each label. Also, it employs a 2layer graph convolutional neural network (Kipf and 1 Introduction Welling, 2017) to take advantage of the structured Large-scale multi-label text classification (LMTC) knowledge of label spaces to enhance label repreis a fundamental and practical task in natural lan- sentations. Apart from that, Lu et al. (2020) finds guage processing (Tsoumakas et al., 2010). LMTC that label similarity graphs based on pre-trained can be found in several domains, such as organiz- word embeddings and co-occurrence frequency are ing documents in Wikipedia articles (Partalas et al., also beneficial. 2015), annotating medical records with diagnostic Nonetheless, these approaches neglect the potenand procedure labels (Yan et al., 2010; Rios and tial meta-knowledge contained in the dataset that Kavuluru, 2018), assigning legislation with rele- can guide the models to learn with only a small vant legal concepts (Chalkidis e"
2021.emnlp-main.679,N18-1100,0,0.0284997,"Kavuluru (2018), we also report the harmonic average across all R@K and all nDCG@K scores for methods that can predict zero-shot labels. 5.3 Baselines Following Lu et al. (2020), we compare the following baselines. CNN (Kim, 2014) uses convolutional neural networks with max-pooling to extract text features, which are then used to make the predictions for the labels. RCNN (Lai et al., 2015) uses recurrent neural networks with a convolution layer to consider both long-distance and local dependencies. It achieves best the performances across competitive text encoders in Liu et al. (2019). CAML (Mullenbach et al., 2018) is a model designed for clinical notes and text documents. It more improvement with a smaller threshold. Thus, we believe that a smaller threshold would not affect the conclusions of our experiments. 8637 MIMIC-III Frequent R@10 nDCG@10 Few-shot R@10 nDCG@10 Zero-shot R@10 nDCG@10 Harmonic Average R@10 nDCG@10 CNN RCNN CAML 34.6 43.9 41.2 44.2 56.0 53.3 5.5 14.2 5.9 2.9 9.8 3.9 - - - - ZAGRU + SIMPLE - EXT + META - LMTC 49.0 49.2 49.7* 61.3 61.8 62.6* 26.9 27.2 29.1* 17.7 17.8 20.2* 34.7 35.4 38.8* 22.2 23.5 24.1 34.7 35.2 37.4* 25.5 26.1 28.0* ZAGGRU + SIMPLE - EXT + META - LMTC 49.1 49.4 49"
2021.emnlp-main.679,D14-1162,0,0.0840534,"Missing"
2021.findings-acl.142,P17-1038,0,0.027152,"Missing"
2021.findings-acl.142,P15-1017,0,0.0780424,"n Introduction Event Detection (ED) is an important task in Information Extraction (IE) that seeks to identify instances of speciﬁed types of events in text (Ji and Grishman, 2008; Li et al., 2013). For example, for the input sentence shown in Figure 1, the ED model aims to predict three event types expressed by this sentence, each of which consists of an event type label and its subtype label, according to the ACE2005 Guidelines. * Corresponding author. ED is an actively studied task in IE where deep learning models have been the dominant approach to deliver the state-of-the-art performance (Chen et al., 2015; Nguyen et al., 2016; Sha et al., 2018; Chen et al., 2018). The last few years witness the success of graph convolutional neural networks for ED (Nguyen and Grishman, 2018; Liu et al., 2018; Yan et al., 2019; Lai et al., 2020) where the dependency trees are employed to boost the performance. Also, another line of research focused on exploiting external knowledge to improve classiﬁcation (Lu et al., 2019; Liu et al., 2019a; Tong et al., 2020). Nevertheless, most previous work typically treats ED as the identiﬁcation and classiﬁcation of trigger words, focusing on using various syntactic depend"
2021.findings-acl.142,D18-1158,0,0.148686,"n Information Extraction (IE) that seeks to identify instances of speciﬁed types of events in text (Ji and Grishman, 2008; Li et al., 2013). For example, for the input sentence shown in Figure 1, the ED model aims to predict three event types expressed by this sentence, each of which consists of an event type label and its subtype label, according to the ACE2005 Guidelines. * Corresponding author. ED is an actively studied task in IE where deep learning models have been the dominant approach to deliver the state-of-the-art performance (Chen et al., 2015; Nguyen et al., 2016; Sha et al., 2018; Chen et al., 2018). The last few years witness the success of graph convolutional neural networks for ED (Nguyen and Grishman, 2018; Liu et al., 2018; Yan et al., 2019; Lai et al., 2020) where the dependency trees are employed to boost the performance. Also, another line of research focused on exploiting external knowledge to improve classiﬁcation (Lu et al., 2019; Liu et al., 2019a; Tong et al., 2020). Nevertheless, most previous work typically treats ED as the identiﬁcation and classiﬁcation of trigger words, focusing on using various syntactic dependency structure or external knowledge to boost classiﬁcation"
2021.findings-acl.142,2020.findings-emnlp.211,0,0.0657705,"Missing"
2021.findings-acl.142,N19-1423,0,0.00881892,"or ED. (Tong et al., 2020) proposes an enrichment knowledge distillation model to leverage external open-domain trigger knowledge to address the long-tail issue. Unlike the existing ED models based on trigger classiﬁcation, we formulate ED as a novel graph parsing problem, therefore it can explicitly model the multiple event correlations and incorporate the rich information regarding the event types. 6.2 Prertained Seq2seq Models Pre-training a universal model and then ﬁne-tuning the model on a downstream task have recently become a popular strategy in the ﬁeld of natural language processing (Devlin et al., 2019). Recent studies also propose approaches to pre-training seq2seq models, such as MASS (Song et al., 2019), PoDA (Wang et al., 2019), PEGASUS (Zhang et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2019) In this paper, our experiments only examine BART. We leave explorations of these models for future work. 7 Conclusion This paper presents the ﬁrst work to formulate ED as a graph parsing task, and to introduce a novel generation-based method to predict event graph by using a pretrained seq2seq model. Our approach is conceptually simple and does not use syntactic dependency infor"
2021.findings-acl.142,I17-1036,0,0.0174985,"f correctly predicted events in total predicted events. Recall: the proportion of correctly predicted events in total gold events of the dataset. ∗R F1-measure: 2∗P P +R 4.2 Hyperparameters The hyperparameters are tuned on the validation set. For all the experiments, we use the same model hyOverall Performance In this section, we comprehensively compare our performance with the following state-of-the-art methods: JRNN proposes a joint event extraction model based on recurrent neural network to improve ED (Nguyen et al., 2016). DLRNN exploits document information via recurrent neural networks (Duan et al., 2017). TBNNAM is the ﬁrst work on detecting events without triggers (Liu et al., 2019b). GCN-ED uses an argument pooling mechanism for event detection based on GCN (Nguyen and Grishman, 2018). JMEE uses GCN with highway network and selfattention (Liu et al., 2018). MOGANED is an advanced graph neural network (GNN) model. It proposes a multi-order graph attention network to effectively model the multiorder syntactic relations in dependency trees and improve ED (Yan et al., 2019). EE-GCN simultaneously exploits syntactic structure and typed dependency label information to perform ED (Cui et al., 2020"
2021.findings-acl.142,P11-1113,0,0.0168823,"new beam.push({Y, score}); 4.3 else last token = Y[i-1]; if last token==EOS then new beam.push({Y, score}); continue; else if last token in type set then for vi in constrained subtype set do Y = Y ∪ vi , score+ = P (vi ); new beam.push({Y, score}); else set = type set + {EOS}; for vi in set do Y = Y ∪ vi , score+ = P (vi ); new beam.push({Y, score}); beam = new beam.topK(); {Y, score} ← beam.topK(k = 1); 4 4.1 Experiments Dataset and Evaluation Metrics We utilized the ACE 2005 corpus as our dataset. For comparison, as the same as previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), we used the same test set with 40 newswire articles and the same development set with 30 other documents randomly selected from different genres and the rest 529 documents are used for training. Also, following previous work (Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2019a; Tong et al., 2020), we use the following criteria to evaluate the results: Precision: the proportion of correctly predicted events in total predicted events. Recall: the proportion of correctly predicted events in total gold events of the dataset. ∗R F1-measure: 2∗P P +R 4."
2021.findings-acl.142,P08-1030,0,0.0570815,"vi in type set do Y = Y ∪ vi , score+ = P (vi ); new beam.push({Y, score}); 4.3 else last token = Y[i-1]; if last token==EOS then new beam.push({Y, score}); continue; else if last token in type set then for vi in constrained subtype set do Y = Y ∪ vi , score+ = P (vi ); new beam.push({Y, score}); else set = type set + {EOS}; for vi in set do Y = Y ∪ vi , score+ = P (vi ); new beam.push({Y, score}); beam = new beam.topK(); {Y, score} ← beam.topK(k = 1); 4 4.1 Experiments Dataset and Evaluation Metrics We utilized the ACE 2005 corpus as our dataset. For comparison, as the same as previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), we used the same test set with 40 newswire articles and the same development set with 30 other documents randomly selected from different genres and the rest 529 documents are used for training. Also, following previous work (Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2019a; Tong et al., 2020), we use the following criteria to evaluate the results: Precision: the proportion of correctly predicted events in total predicted events. Recall: the proportion of correctly predicted events in total gold event"
2021.findings-acl.142,2020.emnlp-main.435,0,0.179083,"entence shown in Figure 1, the ED model aims to predict three event types expressed by this sentence, each of which consists of an event type label and its subtype label, according to the ACE2005 Guidelines. * Corresponding author. ED is an actively studied task in IE where deep learning models have been the dominant approach to deliver the state-of-the-art performance (Chen et al., 2015; Nguyen et al., 2016; Sha et al., 2018; Chen et al., 2018). The last few years witness the success of graph convolutional neural networks for ED (Nguyen and Grishman, 2018; Liu et al., 2018; Yan et al., 2019; Lai et al., 2020) where the dependency trees are employed to boost the performance. Also, another line of research focused on exploiting external knowledge to improve classiﬁcation (Lu et al., 2019; Liu et al., 2019a; Tong et al., 2020). Nevertheless, most previous work typically treats ED as the identiﬁcation and classiﬁcation of trigger words, focusing on using various syntactic dependency structure or external knowledge to boost classiﬁcation performance. Methodologically speaking, this type of trigger-based ED models suffer from the following inherent drawbacks. Firstly, most previous approaches depend hea"
2021.findings-acl.142,2020.acl-main.703,0,0.208127,"espectively predict the trigger label. However, note that modeling the associations between triggers is not equivalent to modeling the correlations between events. That is to say, the current models cannot explicitly model the correlations between multiple events. 2. We further propose a novel generation-based approach to predict the event graph via a seq2seq model. The proposed transducer can directly derive the events from the global contextual information in the input sentences, without being limited to the representations of trigger words. Particularly, we employ a pretrained model, BART (Lewis et al., 2020), to generate a linearized event graph, thereby addressing the data sparsity. Lastly, the existing approaches cannot leverage the hierarchical structure information of event type and subtype. The two kinds of event type labels contain the information with different granularity. Intuitively, the event type-level information can be used to guide the subtype-level classiﬁcation. Furthermore, the event type or subtype label itself also conveys explicit semantic information that may be conducive to event prediction. However, the rich information is neglected by the existing approaches. To address t"
2021.findings-acl.142,P13-1008,0,0.0125574,"score}); 4.3 else last token = Y[i-1]; if last token==EOS then new beam.push({Y, score}); continue; else if last token in type set then for vi in constrained subtype set do Y = Y ∪ vi , score+ = P (vi ); new beam.push({Y, score}); else set = type set + {EOS}; for vi in set do Y = Y ∪ vi , score+ = P (vi ); new beam.push({Y, score}); beam = new beam.topK(); {Y, score} ← beam.topK(k = 1); 4 4.1 Experiments Dataset and Evaluation Metrics We utilized the ACE 2005 corpus as our dataset. For comparison, as the same as previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), we used the same test set with 40 newswire articles and the same development set with 30 other documents randomly selected from different genres and the rest 529 documents are used for training. Also, following previous work (Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2019a; Tong et al., 2020), we use the following criteria to evaluate the results: Precision: the proportion of correctly predicted events in total predicted events. Recall: the proportion of correctly predicted events in total gold events of the dataset. ∗R F1-measure: 2∗P P +R 4.2 Hyperparameters"
2021.findings-acl.142,P10-1081,0,0.0423729,"∪ vi , score+ = P (vi ); new beam.push({Y, score}); 4.3 else last token = Y[i-1]; if last token==EOS then new beam.push({Y, score}); continue; else if last token in type set then for vi in constrained subtype set do Y = Y ∪ vi , score+ = P (vi ); new beam.push({Y, score}); else set = type set + {EOS}; for vi in set do Y = Y ∪ vi , score+ = P (vi ); new beam.push({Y, score}); beam = new beam.topK(); {Y, score} ← beam.topK(k = 1); 4 4.1 Experiments Dataset and Evaluation Metrics We utilized the ACE 2005 corpus as our dataset. For comparison, as the same as previous work (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2013), we used the same test set with 40 newswire articles and the same development set with 30 other documents randomly selected from different genres and the rest 529 documents are used for training. Also, following previous work (Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2019a; Tong et al., 2020), we use the following criteria to evaluate the results: Precision: the proportion of correctly predicted events in total predicted events. Recall: the proportion of correctly predicted events in total gold events of the dataset. ∗R F1-m"
2021.findings-acl.142,N19-1080,0,0.0243363,"Missing"
2021.findings-acl.142,D18-1156,0,0.0248547,"Missing"
2021.findings-acl.142,P19-1429,0,0.0270924,"Missing"
2021.findings-acl.142,N16-1034,0,0.276823,"t Detection (ED) is an important task in Information Extraction (IE) that seeks to identify instances of speciﬁed types of events in text (Ji and Grishman, 2008; Li et al., 2013). For example, for the input sentence shown in Figure 1, the ED model aims to predict three event types expressed by this sentence, each of which consists of an event type label and its subtype label, according to the ACE2005 Guidelines. * Corresponding author. ED is an actively studied task in IE where deep learning models have been the dominant approach to deliver the state-of-the-art performance (Chen et al., 2015; Nguyen et al., 2016; Sha et al., 2018; Chen et al., 2018). The last few years witness the success of graph convolutional neural networks for ED (Nguyen and Grishman, 2018; Liu et al., 2018; Yan et al., 2019; Lai et al., 2020) where the dependency trees are employed to boost the performance. Also, another line of research focused on exploiting external knowledge to improve classiﬁcation (Lu et al., 2019; Liu et al., 2019a; Tong et al., 2020). Nevertheless, most previous work typically treats ED as the identiﬁcation and classiﬁcation of trigger words, focusing on using various syntactic dependency structure or ext"
2021.findings-acl.142,P17-1099,0,0.0243751,"ger classiﬁcation based models, we ﬁrst predict the event types from the input text, and then output the corresponding triggers for previously predicted event types. 3 Event Graph Parsing via a Pretrained Seq2seq Model Under our graph parsing formulation, the ED task is to transduce an input sentence into an event graph, as illustrated in Section 2. To achieve this, we choose to predict nodes sequentially rather than simultaneously, because (1) we believe the previous node generation is informative to the current node prediction; (2) variants of efﬁcient seq2seq models (Bahdanau et al., 2014; See et al., 2017) can be employed to model this process. Theoretically, the advantages of applying a seq2seq model to event graph parsing are two-fold. First, there is no need to use trigger words for event detection. Second, when predicting an event type node during decoding, the global contextual information in the input sentence can be taken into consideration by the cross-attention mechanism between the decoder and the encoder. However, in the preliminary research experiments, the generic seq2seq event detection approaches did not obtain satisfactory performance. The main reason may be that the seq2seq-bas"
2021.findings-acl.142,2020.acl-main.522,0,0.132755,"nding author. ED is an actively studied task in IE where deep learning models have been the dominant approach to deliver the state-of-the-art performance (Chen et al., 2015; Nguyen et al., 2016; Sha et al., 2018; Chen et al., 2018). The last few years witness the success of graph convolutional neural networks for ED (Nguyen and Grishman, 2018; Liu et al., 2018; Yan et al., 2019; Lai et al., 2020) where the dependency trees are employed to boost the performance. Also, another line of research focused on exploiting external knowledge to improve classiﬁcation (Lu et al., 2019; Liu et al., 2019a; Tong et al., 2020). Nevertheless, most previous work typically treats ED as the identiﬁcation and classiﬁcation of trigger words, focusing on using various syntactic dependency structure or external knowledge to boost classiﬁcation performance. Methodologically speaking, this type of trigger-based ED models suffer from the following inherent drawbacks. Firstly, most previous approaches depend heavily on the trigger word. On the one hand, triggers are nonessential to event detection (Liu et al., 2019b); On the other hand, the identiﬁcation and classiﬁcation of trigger words may, to some extent, hinder the accura"
2021.findings-acl.142,D19-1412,0,0.0135565,"address the long-tail issue. Unlike the existing ED models based on trigger classiﬁcation, we formulate ED as a novel graph parsing problem, therefore it can explicitly model the multiple event correlations and incorporate the rich information regarding the event types. 6.2 Prertained Seq2seq Models Pre-training a universal model and then ﬁne-tuning the model on a downstream task have recently become a popular strategy in the ﬁeld of natural language processing (Devlin et al., 2019). Recent studies also propose approaches to pre-training seq2seq models, such as MASS (Song et al., 2019), PoDA (Wang et al., 2019), PEGASUS (Zhang et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2019) In this paper, our experiments only examine BART. We leave explorations of these models for future work. 7 Conclusion This paper presents the ﬁrst work to formulate ED as a graph parsing task, and to introduce a novel generation-based method to predict event graph by using a pretrained seq2seq model. Our approach is conceptually simple and does not use syntactic dependency information and any other extra knowledge; however, it signiﬁcantly outperforms the traditional trigger classiﬁcation-based encoderonly"
2021.findings-acl.142,D19-1582,0,0.151333,"e, for the input sentence shown in Figure 1, the ED model aims to predict three event types expressed by this sentence, each of which consists of an event type label and its subtype label, according to the ACE2005 Guidelines. * Corresponding author. ED is an actively studied task in IE where deep learning models have been the dominant approach to deliver the state-of-the-art performance (Chen et al., 2015; Nguyen et al., 2016; Sha et al., 2018; Chen et al., 2018). The last few years witness the success of graph convolutional neural networks for ED (Nguyen and Grishman, 2018; Liu et al., 2018; Yan et al., 2019; Lai et al., 2020) where the dependency trees are employed to boost the performance. Also, another line of research focused on exploiting external knowledge to improve classiﬁcation (Lu et al., 2019; Liu et al., 2019a; Tong et al., 2020). Nevertheless, most previous work typically treats ED as the identiﬁcation and classiﬁcation of trigger words, focusing on using various syntactic dependency structure or external knowledge to boost classiﬁcation performance. Methodologically speaking, this type of trigger-based ED models suffer from the following inherent drawbacks. Firstly, most previous ap"
2021.findings-acl.252,2020.acl-main.564,0,0.0274512,"Generative Adversarial Network (GAN) (Goodfellow et al., 2014), and Zheng et al. (2019) explore this method for unknown intent detection. However, there are two major distinctions between our study and these works. First, they generate OOD utterances according to continuous latent variables, which cannot be easily interpreted. In contrast, our framework generates utterances by performing local replacements to IND utterances, 2853 which is more interpretable to human. Second, our framework additionally contains a weighting module to reform the generated utterances. Our work is also inspired by Cai et al. (2020), which proposes a framework to augment the IND data, while our framework aims to generate OOD data. 3 Preliminary In this section, we formalize unknown intent detection task. Then we introduce the energy score, and its superiority and limitations for this task. 3.1 exp[fy (u)/T ] , y 0 exp[fy 0 (u)/T ] p(y|u) = P y0 Energy-based OOD Detection An energy-based model (LeCun et al., 2006) builds an energy function E(u) that maps an input u to a scalar called energy score (i.e., E : RD → R). Using the energy function, probability density p(u) can be expressed as: p(u) = exp(−E(u)/T ) , Z (1) R whe"
2021.findings-acl.252,N16-1061,0,0.0273089,"es and reweight them. We demonstrate that GOT can further improve the performance of the energy score by explicitly shaping the energy gap and achieves state-of-the-art results. • We show the generality of GOT by applying generated weighted OOD utterances to fine-tune the softmax-based detector, and the fine-tuned softmax-based detector can also yield significant improvements. 2 Related Work Lane et al., 2006, Manevitz and Yousef, 2007 and Dai et al., 2007 address OOD detection for the text-mining task. Recently, this problem has attracted growing attention from researchers (Tur et al., 2014; Fei and Liu, 2016; Fei et al., 2016; Ryu et al., 2017; Shu et al., 2017). Hendrycks and Gimpel (2017) present a simple baseline that utilizes the softmax confidence score to detect OOD inputs. Shu et al. (2017) create a binary classifier and calculate the confidence threshold for each class. Some distance-based methods (Oh et al., 2018; Lin and Xu, 2019; Yan et al., 2020) are also used to detect unknown intents as OOD utterances highly deviate from IND utterances in their local neighborhood. Simultaneously, with the advancement of deep generative models, learning such a model to approximate the distribution of"
2021.findings-acl.252,D19-1131,0,0.0395448,"Missing"
2021.findings-acl.252,P19-1548,0,0.134778,"can also yield significant improvements. 2 Related Work Lane et al., 2006, Manevitz and Yousef, 2007 and Dai et al., 2007 address OOD detection for the text-mining task. Recently, this problem has attracted growing attention from researchers (Tur et al., 2014; Fei and Liu, 2016; Fei et al., 2016; Ryu et al., 2017; Shu et al., 2017). Hendrycks and Gimpel (2017) present a simple baseline that utilizes the softmax confidence score to detect OOD inputs. Shu et al. (2017) create a binary classifier and calculate the confidence threshold for each class. Some distance-based methods (Oh et al., 2018; Lin and Xu, 2019; Yan et al., 2020) are also used to detect unknown intents as OOD utterances highly deviate from IND utterances in their local neighborhood. Simultaneously, with the advancement of deep generative models, learning such a model to approximate the distribution of training data is possible. However, Ren et al. (2019) find that likelihood scores derived from these models can be confounded by background components, and propose a likelihood ratio method to alleviate this issue. Gangal et al. (2019) reformulate and apply this method to unknown intent detection. Different from these methods, we intro"
2021.findings-acl.252,D17-1314,0,0.0804,"er improve the performance of the energy score by explicitly shaping the energy gap and achieves state-of-the-art results. • We show the generality of GOT by applying generated weighted OOD utterances to fine-tune the softmax-based detector, and the fine-tuned softmax-based detector can also yield significant improvements. 2 Related Work Lane et al., 2006, Manevitz and Yousef, 2007 and Dai et al., 2007 address OOD detection for the text-mining task. Recently, this problem has attracted growing attention from researchers (Tur et al., 2014; Fei and Liu, 2016; Fei et al., 2016; Ryu et al., 2017; Shu et al., 2017). Hendrycks and Gimpel (2017) present a simple baseline that utilizes the softmax confidence score to detect OOD inputs. Shu et al. (2017) create a binary classifier and calculate the confidence threshold for each class. Some distance-based methods (Oh et al., 2018; Lin and Xu, 2019; Yan et al., 2020) are also used to detect unknown intents as OOD utterances highly deviate from IND utterances in their local neighborhood. Simultaneously, with the advancement of deep generative models, learning such a model to approximate the distribution of training data is possible. However, Ren et al. (2019)"
2021.naacl-main.302,P19-1580,0,0.0207815,"a larger FD-1 means a larger risk of losing some critical information from sequence positions. To alleviate this potential risk, we add dropout to query, key, and value before the calculation of attention. Transformer decoder, otherwise the last layer of the Transformer encoder. The positions of each feature dropout applied in Transformer1 are shown in Figure 1b. 3.2 Structure Dropout There are three structure dropouts, respectively LayerDrop (Fan et al., 2020a), DropHead (Zhou et al., 2020) and HeadMask (Sun et al., 2020), which are specifically designed for Transformer. Some recent studies (Voita et al., 2019; Michel et al., 2019) show multi-head attention mechanism is dominated by a small portion of attention heads. To prevent domination and excessive coadaptation between different attention heads, Zhou et al. (2020) and Sun et al. (2020) respectively propose structured DropHead and HeadMask that drop certain entire heads during training. In contrast, LayerDrop (Fan et al., 2020a) is a higher-level and coarser-grained structure dropout. It drops some entire layers at training time and directly reduces the Transformer model size. In this work, we adopt LayerDrop as the structure dropout to incorpo"
2021.naacl-main.302,2020.findings-emnlp.178,0,0.220175,"methods such as weight decay (Krogh and Hertz, 1992), data augmentation (Sennrich et al., 2016a), dropout (Srivastava et al., 2014), parameter sharing (Dehghani et al., 2018; Xia et al., 2019) are all widely adopted to address overfitting. Among these regularization approaches, dropout (Srivastava et al., 2014), which randomly drops out some hidden units during training, is the most popular one and various dropout techniques have been proposed for Transformer. For example, Fan et al. (2020a) propose LayerDrop, a random structured dropout, to drop certain layers of Transformer during training. Zhou et al. (2020) alternatively propose DropHead as a structured dropout method for regularizing the multi-head attention mechanism. Both of them achieved promising performances. One great advantage of dropout is that it is free of additional computational costs and resource requirements. Hence we ask one question: can we achieve stronger or even state-of-the-art (SOTA) results only relying on various dropout techniques instead of extra model architecture design or knowledge enhancement? Introduction To this end, in this paper, we propose UniDrop to integrate three different-level dropout techniques In recent"
2021.naacl-main.458,D19-1633,0,0.553828,"y. Experiment results show Roy et al., 2018) of latent codes for discrete latent that our model achieves comparable or better performance in machine translation tasks than spaces, which may hurt the translation efficiency— several strong baselines. the essential goal of non-autoregressive decoding. Akoury et al. (2019) introduce syntactic labels 1 Introduction as a proxy to the learned discrete latent space and Non-autoregressive Transformer (NAT, Gu et al., improve the NATs’ performance. The syntactic 2018; Wang et al., 2019; Lee et al., 2018; label greatly reduces the search space of latent Ghazvininejad et al., 2019) is a promising text gen- codes, leading to a better performance in both quality and speed. However, it needs an external syneration model for machine translation. It introduces tactic parser to produce the reference syntactic tree, the conditional independent assumption among the target language outputs and simultaneously gener- which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency ates the whole sentence, bringing in a remarkable efficiency improvement (more than 10× speed-up) between latent variables for non-autoregressive decoding efficient"
2021.naacl-main.458,P07-2045,0,0.0151311,"Missing"
2021.naacl-main.458,D18-1149,0,0.555263,"arge number (more than 215 , Kaiser et al., 2018; the model capacity. Experiment results show Roy et al., 2018) of latent codes for discrete latent that our model achieves comparable or better performance in machine translation tasks than spaces, which may hurt the translation efficiency— several strong baselines. the essential goal of non-autoregressive decoding. Akoury et al. (2019) introduce syntactic labels 1 Introduction as a proxy to the learned discrete latent space and Non-autoregressive Transformer (NAT, Gu et al., improve the NATs’ performance. The syntactic 2018; Wang et al., 2019; Lee et al., 2018; label greatly reduces the search space of latent Ghazvininejad et al., 2019) is a promising text gen- codes, leading to a better performance in both quality and speed. However, it needs an external syneration model for machine translation. It introduces tactic parser to produce the reference syntactic tree, the conditional independent assumption among the target language outputs and simultaneously gener- which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency ates the whole sentence, bringing in a remarkable efficiency improvement (more than 10"
2021.naacl-main.458,D19-1573,0,0.253454,"Missing"
2021.naacl-main.458,D19-1437,0,0.449018,"to the lack of dependencies modeling for To learn these codes in an unsupervised way, we the target outputs, making it harder to model the use each latent code to represent a fuzzy target generation of the target side translation. category instead of a chunk as the previous reA promising way is to model the dependencies search (Akoury et al., 2019). More specifically, of the target language by the latent variables. A line we first employ vector quantization (Roy et al., of research works (Kaiser et al., 2018; Roy et al., 2018) to discretize the target language to the la2018; Shu et al., 2019; Ma et al., 2019) introduce tent space with a smaller number (less than 128) latent variable modeling to the non-autoregressive of latent variables, which can serve as the fuzzy Transformer and improves translation quality. The word-class information each target language word. latent variables could be regarded as the spring- We then model the latent variables with conditional board to bridge the modeling gap, introducing random fields (CRF, Lafferty et al., 2001; Sun et al., more informative decoder inputs than the previ- 2019). To avoid the mismatch of the training and 5749 Proceedings of the 2021 Conference"
2021.naacl-main.458,N19-4009,0,0.0191697,"parated subword embeddings for the IWSLT14 dataset. Model Setting. In the case of IWSLT14 task, we use a small setting (dmodel = 256, dhidden = 512, pdropout = 0.1, nlayer = 5 and nhead = 4) for Transformer and NAT models. For the WMT tasks, we use the Transformer-base setting (dmodel = 512, dhidden = 512, pdropout = 0.3, nhead = 8 and nlayer = 6) of the Vaswani et al. (2017). We set the hyperparameter α used in Eq. 15 and λ in Eq. 7-8 to 1.0 and 0.999, respectively. The categorical number K is set to 64 in our experiments. We implement our model based on the open-source framework of fairseq (Ott et al., 2019). Optimization. We optimize the parameter with the Adam (Kingma and Ba, 2015) with β = (0.9, 0.98). We use inverse square root learning rate scheduling (Vaswani et al., 2017) for the WMT tasks and linear annealing schedule (Lee et al., 2018) from 3 × 10−4 to 1 × 10−5 for the IWSLT14 task. Each mini-batch consists of 2048 tokens for IWSLT14 and 32K tokens for WMT tasks. WMT14 EN-DE DE-EN IWSLT14 DE-EN LV-NAR AXE CMLM SynST Flowseq 11.80 20.40 20.74 20.85 / 24.90 25.50 25.40 / / 23.82 24.75 NAT (ours) CNAT (ours) 9.80 21.30 11.02 25.73 17.77 29.81 Table 1: Results of the NAT models with argmax d"
2021.naacl-main.458,P02-1040,0,0.111587,"uces tactic parser to produce the reference syntactic tree, the conditional independent assumption among the target language outputs and simultaneously gener- which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency ates the whole sentence, bringing in a remarkable efficiency improvement (more than 10× speed-up) between latent variables for non-autoregressive decoding efficiently. versus the autoregressive model. However, the NAT models still lay behind the autoregressive models in In this paper, we propose to learn a set of latent terms of BLEU (Papineni et al., 2002) for machine codes that can act like the syntactic label, which is translation. We attribute the low-quality of NAT learned without using the explicit syntactic trees. models to the lack of dependencies modeling for To learn these codes in an unsupervised way, we the target outputs, making it harder to model the use each latent code to represent a fuzzy target generation of the target side translation. category instead of a chunk as the previous reA promising way is to model the dependencies search (Akoury et al., 2019). More specifically, of the target language by the latent variables. A line"
2021.naacl-main.458,D07-1043,0,0.0071131,"e a sharp distribution for each latent variable, showing that our learned fuzzy classes are meaningful. 5 Related Work Non-autoregressive Machine Translation. Gu et al. (2018) first develop a non-autoregressive Transformer (NAT) for machine translation, which produces the outputs in parallel, and the inference speed is thus significantly boosted. Due to the missing of dependencies among the target outputs, the translation quality is largely sacrificed. A line of work proposes to mitigate such perQuantitative Results. We first compute the Vformance degradation by enhancing the decoder Measure (Rosenberg and Hirschberg, 2007) score between the latent categories to POS tags and sub- inputs. Lee et al. (2018) propose a method of itwords frequencies. The results are listed in Table 7. erative refinement based on the previous outputs. Overall, the “w/ POS tags” achieves a higher V- Guo et al. (2019) enhance decoder input by introducing the phrase table in statistical machine transMeasure score, indicating that the latent codes are more related to the POS tags than sub-words fre- lation and embedding transformation. There are quencies. The homogeneity score (H-score) evalu- also some work focuses on improving the decod"
2021.naacl-main.458,P16-1162,0,0.0910943,"arg max p(y|z ∗ , x; θ), y y∗ where identifying only requires independently One potential issue is that the mismatch of the maximizing the local probability for each output training and inference stage for the used categorical position. 5752 4 Experiments Model Datasets. We conduct the experiments on the most widely used machine translation benchmarks: WMT14 English-German (WMT14 EN-DE, 4.5M pairs)1 and IWSLT14 German-English (IWSLT14, 160K pairs)2 . The datasets are processed with the Moses script (Koehn et al., 2007), and the words are segmented into subword units using byte-pair encoding (Sennrich et al., 2016, BPE). We use the shared subword embeddings between the source language and target language for the WMT datasets and the separated subword embeddings for the IWSLT14 dataset. Model Setting. In the case of IWSLT14 task, we use a small setting (dmodel = 256, dhidden = 512, pdropout = 0.1, nlayer = 5 and nhead = 4) for Transformer and NAT models. For the WMT tasks, we use the Transformer-base setting (dmodel = 512, dhidden = 512, pdropout = 0.3, nhead = 8 and nlayer = 6) of the Vaswani et al. (2017). We set the hyperparameter α used in Eq. 15 and λ in Eq. 7-8 to 1.0 and 0.999, respectively. The"
2021.naacl-main.458,P19-1125,0,0.679393,"le (a) AT (b)Non-Autoregressive NATDecoding Decoding (c) LT Experiment results on WMT14 and IWSLT14 show that CNAT achieves the new state-of-the- Figure 1: Different inference process of different Transart performance without knowledge distillation. former models. With the sequence-level knowledge distillation and y x reranking techniques, the CNAT is comparable to • Inputs Initialization: With the target sethe current state-of-the-art iterative-based model quence length m, we can compute the dewhile keeping a competitive decoding speedup. coder inputs h = h1:m with Softcopy (Li et al., 2019; Wei et al., 2019) as: 2 Background hj = Neural machine translation (NMT) is formulated as a conditional probability model p(y|x), which models a sentence y = {y1 , y2 , · · · , ym } in the target language given the input x = {x1 , x2 , · · · , xn } from the source language. 2.1 Non-Autoregressive Neural Machine Translation Gu et al. (2018) proposes Non-Autoregressive Transformer (NAT) for machine translation, breaking the dependency among target tokens, thus achieving simultaneous decoding for all tokens. For a source sentence, a non-autoregressive decoder factorizes the probability of its target sentence as:"
2021.naacl-main.458,D19-1072,0,0.0211239,": Approach In this section, we present our proposed CNAT, an extension to the Transformer incorporated with non-autoregressive decoding for target tokens and autoregressive decoding for latent sequences. In brief, CNAT follows the architecture of Latent Transformer (Kaiser et al., 2018), except for the latent variable modeling (in § 3.1 and § 3.2) and inputs initialization (in § 3.3). 3.1 Modeling Target Categorical Information by Vector Quantization Categorical information has achieved great success in neural machine translation, such as partof-speech (POS) tag in autoregressive translation (Yang et al., 2019) and syntactic label in nonautoregressive translation (Akoury et al., 2019). Inspired by the broad application of categorical information, we propose to model the implicit categorical information of target words in a nonautoregressive Transformer. Each target sequence y = y1:m will be assigned to a discrete latent variable sequence z = z1:m . We assume that each zi will capture the fuzzy category of its token yi . Then, the conditional probability p(y|x) is factorized with respect to the categorical latent variable: p(y|x) = X zi = k, qi = Qk , and k = arg min ||repr(yi ) − Qj ||2 , j∈[K] repr"
D17-1013,D17-1151,0,0.0303533,"ide. Later, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b). The source side context is computed at each time-step of decoding, based on the attention weights between the source side representations and the current hidden state of the decoder. However, the hidden states in the recurrent decoder still originate from the single fixed-length representation (Luong et al., 2015b), or the average of the bi-directional representations (Bahdanau et al., 2014). Here we refer to the representation as initial state. Interestingly, Britz et al. (2017) find that the value of initial state does not affect the translation performance, and prefer to set the initial state to be a zero vector. On the contrary, we argue that initial state still plays an important role of translation, which is currently neglected. We notice that beside the end-to-end error back propagation for the initial and transition parameters, there is no direct control of the initial state in the current NMT architectures. Due to the large number of parameters, it may be difficult for the NMT system to learn the proper sentence representation as the initial state. Thus, the"
D17-1013,D14-1179,0,0.0372327,"Missing"
D17-1013,P15-1166,0,0.0572008,"the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder. Zhang and Zong (2016) propose to use multitask learning for incorporating source side monolingual data. Different from these attempts, our method focuses solely on the current translation task, and does not require any extra data or annotation. In the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using en"
D17-1013,P15-1001,0,0.0765902,"he length of x and y, respectively. In the encoding stage, a bi-directional recurrent neural network is used (Bahdanau et al., 2014) to encode x into a sequence of vectors (h1 , h2 , · · · , h|x |). For each xi , the representation hi is: − → ← − hi = [ hi ; hi ] (1) Related Work Many previous works have noticed the problem of training an NMT system with lots of parameters. Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captio"
D17-1013,2010.amta-papers.33,0,0.0594987,"ns mechanism could be used as a training method and brings no extra computing cost during decoding. Experiments on the Chinese-English and German-English translation tasks show that both the constraining of the initial state and the decoder hidden states bring significant improvement over the baseline systems. Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality. 2 tion (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al., 2016; L’Hostis et al., 2016). In these research, word prediction mechanisms are employed to decide the selection of words or constrain the target vocabulary, while in this paper, we use word prediction as a control mechanism for neural model training. 3 Notations and Backgrounds We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al., 2015b), based on which we propose our word prediction mechanism. Denote a source-target sentence pair as {x, y} from the t"
D17-1013,D14-1175,0,0.0505949,"Missing"
D17-1013,P16-1100,0,0.0293479,"y, respectively. In the encoding stage, a bi-directional recurrent neural network is used (Bahdanau et al., 2014) to encode x into a sequence of vectors (h1 , h2 , · · · , h|x |). For each xi , the representation hi is: − → ← − hi = [ hi ; hi ] (1) Related Work Many previous works have noticed the problem of training an NMT system with lots of parameters. Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared e"
D17-1013,D15-1166,0,0.0629789,"technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016). Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016). Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder. Zhang and Zong (2016) propose to use multitask learning for incorporating source side monolingual data. Different from these attempts, our method focuses solely on the current translation task, and does not require any extra data or annotation. In the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using en"
D17-1013,D16-1160,0,0.031291,"ould bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments. The way we add the word prediction is similar to the research of multi-task learning. Dong et al. (2015) propose to share an encoder between different translation tasks. Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder. Zhang and Zong (2016) propose to use multitask learning for incorporating source side monolingual data. Different from these attempts, our method focuses solely on the current translation task, and does not require any extra data or annotation. In the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using encoder information. However, the purpose and the way of our mechanism are different from them. The word prediction technique has been applied in the research of both statistical machine translawhere [·; ·] denotes the concatenation of column − → ← − vectors; hi"
D17-1013,D09-1022,0,0.0606512,"Missing"
D17-1013,C16-1205,0,0.143249,"Missing"
D17-1013,P16-2021,0,0.0277493,"and brings no extra computing cost during decoding. Experiments on the Chinese-English and German-English translation tasks show that both the constraining of the initial state and the decoder hidden states bring significant improvement over the baseline systems. Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality. 2 tion (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al., 2016; L’Hostis et al., 2016). In these research, word prediction mechanisms are employed to decide the selection of words or constrain the target vocabulary, while in this paper, we use word prediction as a control mechanism for neural model training. 3 Notations and Backgrounds We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al., 2015b), based on which we propose our word prediction mechanism. Denote a source-target sentence pair as {x, y} from the training set, where x is the source word seque"
D17-1013,P02-1040,0,0.104925,"97 and 1082 source sentences, respectively, with 4 references for each sentence. For the DE-EN, the experiments trained on the standard benchmark WMT14, and it has about 4.5 million sentence pairs. We use newstest 2013 (NST13) as validation set, and newstest 2014(NST14) as test set. These sets have 3000 and 2737 source sentences, respectively, with 1 reference for each sentence. Sentences were encoded using byte-pair encoding (BPE) (Britz et al., 2017). 5.2 5.4 Translation Experiments To see the effect of word predictions in translation, we evaluate these systems in case-insensitive IBM-BLEU (Papineni et al., 2002) on both CH-EN and DE-EN tasks. The detailed results are show in the Table 1 and Table 2. Compared to the baseNMT system, all of our models achieve significant improvements. On the CH-EN experiments, simply adding word predictions to the initial state (WPE ) already brings considerable improvements. The average improvement on test set is 2.53 BLEU, showing that constraining the initial state does lead to a higher translation quality. Adding word predicSystems and Techniques We implement a baseline system with the bidirectional encoder (Bahdanau et al., 2014) and the attention mechanism (Luong"
D17-1013,P07-1020,0,\N,Missing
D17-1079,P14-2131,0,0.0170647,"Missing"
D17-1079,P16-1039,0,0.348057,"ing self-training and tri-training methods for leveraging auto-segmented data. Neural parsers have benefited from automatically labeled data via dependencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarow"
D17-1079,D13-1129,1,0.896381,"Missing"
D17-1079,C14-1078,1,0.843812,"Missing"
D17-1079,P15-1168,0,0.622783,"cter embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra cla"
D17-1079,D14-1093,1,0.462459,"Missing"
D17-1079,D15-1141,0,0.602486,"Missing"
D17-1079,I05-3025,0,0.0362245,"Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith character in the sent"
D17-1079,P15-1167,0,0.0923753,"estigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014)"
D17-1079,D16-1257,0,0.0260578,"Missing"
D17-1079,N06-1020,0,0.0153109,"their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empir"
D17-1079,D10-1002,0,0.0159819,"f feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empirical Methods in Natu"
D17-1079,N16-1118,0,0.0378788,"Missing"
D17-1079,P14-2050,0,0.0361789,"unigram wi and character bigram wi−1 wi , respectively. A forward word representation efi is calculated as follows: ri = concat2 (rilstm−f , rilstm−b ) = tanh(W2 [rilstm−f ; rilstm−b ]) Given the representation ri , we use a scoring unit to score for each potential segment label. Given ri , the score of segment label M is: i fM = WM h, where h = concat3 (ri , eM ), = tanh(W3 [ri ; eM ]) WM is the score matrix for label M, and eM is the label embedding for label M. 3 Word-Context Character Embeddings Our model structure is a derivation from the skipgram model (Mikolov et al., 2013), similar to Levy and Goldberg (2014). Given a sentence with length n: {w1 , w2 , w3 , · · · wn } and its corresponding segment labels: {l1 , l2 , l3 , · · · ln }, the pre-training context of current character wt is the around characters in the windows with size c, together with their corresponding segment labels (Figure 2). Characters wi and labels li in the context are represented by vectors ecwi ∈ Rd and ecli ∈ Rd , respectively, where d is the embedding dimensionality. The word-context embedding of character wt is represented as ewt ∈ Rd , which is trained by predicting the surrounding context representations ecw′ efi = conca"
D17-1079,P14-1043,0,0.01322,"d Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Sco"
D17-1079,D16-1046,0,0.0609597,"Missing"
D17-1079,N15-1142,0,0.0336542,"dicting the surrounding context representations ecw′ efi = concat1 (ewi , ewi−1 wi ), = tanh(W1 [ewi ; ewi−1 wi ]) A backward representation ebi can be obtained in the same way. Then efi and ebi are fed into forward and backward LSTM units at current position, obtaining the corresponding forward and backward LSTM representations rilstm−f and rilstm−b , respectively. In the scoring layer, we first obtain a linear combination of rilstm−f and rilstm−b , which is the final 761 and ecli , parameterizing the labeled segmentation information in the embedding parameters. To capture order information (Ling et al., 2015), we use different embedding matrices for context embedding in different context positions, training different embeddings for the same word when they reside on different locations as the context word. In particular, our context window size is five. As a result, each word has four different versions of ec , namely ec−1 , ec−2 , ec+1 , and ec+2 , each taking a distinct embedding matrix. Given the context window [w−2 , w−1 , w, w+1 , w+2 ], w−1 is the left first context word of the focus word w, ec−1,wi will be selected from embedding matrix E−1 , and w+1 is the right first word of w, ec+1,wi wil"
D17-1079,P14-1028,0,0.295876,"embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li,"
D17-1079,N09-1007,0,0.0471648,"Missing"
D17-1079,I05-3027,0,0.631704,"Missing"
D17-1079,W06-0127,0,0.253287,"in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith character in the sentence, and N is the s"
D17-1079,D13-1061,0,0.12946,"endencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-trai"
D17-1079,P13-1043,1,0.546747,"Missing"
D17-1079,P15-1032,0,0.00999897,"Missing"
D17-1079,P16-2092,0,0.103199,"Missing"
D17-1079,O03-4002,0,0.708664,"nference on Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith cha"
D17-1079,P95-1026,0,0.657468,"2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2"
D17-1079,D13-1031,0,0.0880344,"Missing"
D17-1079,E14-1062,1,0.919559,"Missing"
D17-1079,P16-1040,1,0.632749,"espectively, significantly out-performing self-training and tri-training methods for leveraging auto-segmented data. Neural parsers have benefited from automatically labeled data via dependencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling"
D17-1079,P07-1106,1,0.78255,"Missing"
D19-1086,D18-1048,0,0.07512,"4. 938 studies mainly use capsule network for information aggregation, where the capsules could have a less interpretable meaning. In contrast, our model learns what we expect by the aid of auxiliary learning signals, which endows our model with better interpretability. RNN layers, nor compatible with the state-of-theart Transformer for the additional recurrences prevent Transformer decoder from being parallelized. Another direction is to introduce global representations. Lin et al. (2018) model a global source representation by deconvolution networks. Xia et al. (2017); Zhang et al. (2018); Geng et al. (2018) propose to provide a holistic view of target sentence by multi-pass decoding. Zhou et al. (2019) improve Zhang et al. (2018) to a synchronous bidirectional decoding fashion. Similarly, Weng et al. (2019) deploy bidirectional decoding in interactive translation setting. Different from these work aiming at providing static global information in the whole translation process, our approach models a dynamically global (holistic) context by using capsules network to separate source contents at every decoding steps. 6 Conclusion In this paper, we propose to recognize the translated PAST and untransl"
D19-1086,D16-1096,0,0.044511,"sentences become longer, which are commonly thought hard to translate. We attribute this to the less number of under-translation cases in our model, meaning that our model learns better translation quality and adequacy, especially for long sentences. Related Work Inadequate translation problem is a widely known weakness of NMT models, especially when translating long sentences (Kong et al., 2019; Tu et al., 2016; Lei et al., 2019). To alleviate this problem, one direction is to recognize the translated and untranslated contents, and pay more attention to untranslated parts. Tu et al. (2016), Mi et al. (2016) and Li et al. (2018) employ coverage vector or coverage ratio to indicate the lexical-level coverage of source words. Meng et al. (2018) influence the attentive vectors by translated/untranslated information. Our work mainly follows the path of Zheng et al. (2018), which introduce two extra recurrent layers in the decoder to maintain the representations of the past and future translation contents. However, it may be not easy to show the direct correspondence between the source contents and learned representations in the past/future Does guided dynamic routing really matter? Despite the promis"
D19-1086,C18-1232,0,0.0294723,"cally, an encoder first maps the source sentence into a sequence of encoded representations: Figure 1: An example of separation of PAST and F U TURE in machine translation. When generating the current translation “his”, the source tokens “hBOSi”, “布什(Bush)” and phrase “为...辩护(defend)” are the translated contents (PAST), while the remaining tokens are untranslated contents (F UTURE). sule Network (Hinton et al., 2011) with routingby-agreement mechanism (Sabour et al., 2017), which has demonstrated its appealing strength of solving the problem of parts-to-wholes assignment (Hinton et al., 2018; Gong et al., 2018; Dou et al., 2019; Li et al., 2019), to model the separation of the PAST and F UTURE: 1. We first cast the PAST and F UTURE source contents as two groups of capsules. 2. We then design a novel variant of the routingby-agreement mechanism, called Guided Dynamic Routing (G DR), which is guided by the current translating status at each decoding step to assign each source word to its associated capsules by assignment probabilities for several routing iterations. 3. Finally, the PAST and F UTURE capsules accumulate their expected contents from representations, and are fed into the decoder to provi"
D19-1086,W17-4123,0,0.0805954,"Missing"
D19-1086,P16-1008,1,0.931586,"et al., 2015). Like human translators, NMT systems should have the ability to know the relevant source-side context for the current word (P RESENT), as well as recognize what parts in the source contents have been translated (PAST) and what parts have not (F UTURE), at each decoding step. Accordingly, the PAST, P RESENT and F U TURE are three dynamically changing states during the whole translation process. Previous studies have shown that NMT models are likely to face the illness of inadequate translation (Kong et al., 2019), which is usually embodied in over- and under-translation problems (Tu et al., 2016, 2017). This issue may be attributed to the poor ability of NMT of recognizing the dynamic translated and untranslated contents. To remedy this, Zheng et al. (2018) first demonstrate that explicitly tracking PAST and F UTURE contents helps NMT models alleviate this issue and generate better translation. In their work, the running PAST and F UTURE contents are modeled as recurrent states. However, the recurrent process is still non-trivial to determine which parts of the source words are the PAST and which are the F U TURE , and to what extent the recurrent states represent them respectively,"
D19-1086,D19-1074,0,0.035318,"Missing"
D19-1086,D19-1087,0,0.0241637,"dynamic PAST and F UTURE. (a) Translation length v.s source length (b) BLEU v.s source length Figure 5: Comparison regarding source length. 5 gets a larger improvement when the input sentences become longer, which are commonly thought hard to translate. We attribute this to the less number of under-translation cases in our model, meaning that our model learns better translation quality and adequacy, especially for long sentences. Related Work Inadequate translation problem is a widely known weakness of NMT models, especially when translating long sentences (Kong et al., 2019; Tu et al., 2016; Lei et al., 2019). To alleviate this problem, one direction is to recognize the translated and untranslated contents, and pay more attention to untranslated parts. Tu et al. (2016), Mi et al. (2016) and Li et al. (2018) employ coverage vector or coverage ratio to indicate the lexical-level coverage of source words. Meng et al. (2018) influence the attentive vectors by translated/untranslated information. Our work mainly follows the path of Zheng et al. (2018), which introduce two extra recurrent layers in the decoder to maintain the representations of the past and future translation contents. However, it may b"
D19-1086,D17-1013,1,0.941176,"the encoder leverages N stacked identical layers to map the sentence into contextual representations: The NMT model is now able to employ the dynamic holistic context for better generation. 3.3 Learning PAST and F UTURE as Expected h(l) = EncoderLayer(h(l−1) ), Auxiliary Guided Losses To ensure that the dynamic routing process runs as expected, we introduce the following auxiliary guided signals to assist the learning process. where the superscript l indicates layer depth. Based on the encoded source representations hN , a decoder generates translation word by word. The Bag-of-Word Constraint Weng et al. (2017) propose a multitasking scheme to boost NMT by predicting the bag-of-words of target sentence using the Word Predictions approach. Inspired by 934 trained by minimizing the loss L(θ), where θ is the set of all the parameter of the proposed model: this work, we introduce a B OW constraint to encourage the PAST and F UTURE capsules to be predictive of the preceding and subsequent bag-ofwords regarding each decoding step respectively: L B OW M L(θ) = T 1X = − log pPRE (y≤t |ΩPt ) T t=0  − log pSUB (y≥t |ΩFt ) , where λ1 and λ2 are hyper-parameters. 4 where ppre (y≤t |ΩPt ) and psub (y≥t |ΩFt ) a"
D19-1086,N19-1359,1,0.823548,"ce sentence into a sequence of encoded representations: Figure 1: An example of separation of PAST and F U TURE in machine translation. When generating the current translation “his”, the source tokens “hBOSi”, “布什(Bush)” and phrase “为...辩护(defend)” are the translated contents (PAST), while the remaining tokens are untranslated contents (F UTURE). sule Network (Hinton et al., 2011) with routingby-agreement mechanism (Sabour et al., 2017), which has demonstrated its appealing strength of solving the problem of parts-to-wholes assignment (Hinton et al., 2018; Gong et al., 2018; Dou et al., 2019; Li et al., 2019), to model the separation of the PAST and F UTURE: 1. We first cast the PAST and F UTURE source contents as two groups of capsules. 2. We then design a novel variant of the routingby-agreement mechanism, called Guided Dynamic Routing (G DR), which is guided by the current translating status at each decoding step to assign each source word to its associated capsules by assignment probabilities for several routing iterations. 3. Finally, the PAST and F UTURE capsules accumulate their expected contents from representations, and are fed into the decoder to provide a time-dependent holistic view of"
D19-1086,P18-2047,0,0.01658,"ger, which are commonly thought hard to translate. We attribute this to the less number of under-translation cases in our model, meaning that our model learns better translation quality and adequacy, especially for long sentences. Related Work Inadequate translation problem is a widely known weakness of NMT models, especially when translating long sentences (Kong et al., 2019; Tu et al., 2016; Lei et al., 2019). To alleviate this problem, one direction is to recognize the translated and untranslated contents, and pay more attention to untranslated parts. Tu et al. (2016), Mi et al. (2016) and Li et al. (2018) employ coverage vector or coverage ratio to indicate the lexical-level coverage of source words. Meng et al. (2018) influence the attentive vectors by translated/untranslated information. Our work mainly follows the path of Zheng et al. (2018), which introduce two extra recurrent layers in the decoder to maintain the representations of the past and future translation contents. However, it may be not easy to show the direct correspondence between the source contents and learned representations in the past/future Does guided dynamic routing really matter? Despite the promising numbers of the G"
D19-1086,1983.tc-1.13,0,0.706531,"Missing"
D19-1086,C18-1276,0,0.0187648,"milar to Equation 6. The PAST and F UTURE representations are computed by weighted summation, which is similar to Equation 4. 938 studies mainly use capsule network for information aggregation, where the capsules could have a less interpretable meaning. In contrast, our model learns what we expect by the aid of auxiliary learning signals, which endows our model with better interpretability. RNN layers, nor compatible with the state-of-theart Transformer for the additional recurrences prevent Transformer decoder from being parallelized. Another direction is to introduce global representations. Lin et al. (2018) model a global source representation by deconvolution networks. Xia et al. (2017); Zhang et al. (2018); Geng et al. (2018) propose to provide a holistic view of target sentence by multi-pass decoding. Zhou et al. (2019) improve Zhang et al. (2018) to a synchronous bidirectional decoding fashion. Similarly, Weng et al. (2019) deploy bidirectional decoding in interactive translation setting. Different from these work aiming at providing static global information in the whole translation process, our approach models a dynamically global (holistic) context by using capsules network to separate so"
D19-1086,D15-1166,0,0.0656013,"urce contents in the Section 3.3. Note that we employ G DR at every decoding step t to obtain the time-dependent PAST and F UTURE and omit the subscript t for simplicity. In the dynamic routing process, each vector output of capsule j is calculated with a non-linear (7) where Wb ∈ Rd+dc ∗2 and w ∈ Rdc are learnable parameters. Instead of using simple scalar prod> Ω (Sabour et al., 2017), which uct, i.e., bij = vij j could not consider the current decoding state as a condition signal, we resort to the MLP to take zi into account inspired by MLP-based attention mechanism (Bahdanau et al., 2015; Luong et al., 2015). That is why we call it “guided” dynamic routing. 2 Note that unlike Sabour et al. (2017), where each pair of input capsule i and output capsule j has a distinct transformation matrix Wij as their numbers are predefined (I × J transformation matrices in total), here we share the transformation matrix Wj of output capsule j among all the input capsules due to the varied amount of the source words. So there are J transformation matrices in our model. 933 Output probabilities Softmax Algorithm 1 Guided Dynamic Routing (G DR) Past Capsules Input: Encoder hidden state h, current decoding hidden Ou"
D19-1086,1981.tc-1.7,0,0.677566,"Missing"
D19-1086,D18-1350,0,0.0406832,"Missing"
D19-1086,Q18-1011,1,0.439939,"ecognize what parts in the source contents have been translated (PAST) and what parts have not (F UTURE), at each decoding step. Accordingly, the PAST, P RESENT and F U TURE are three dynamically changing states during the whole translation process. Previous studies have shown that NMT models are likely to face the illness of inadequate translation (Kong et al., 2019), which is usually embodied in over- and under-translation problems (Tu et al., 2016, 2017). This issue may be attributed to the poor ability of NMT of recognizing the dynamic translated and untranslated contents. To remedy this, Zheng et al. (2018) first demonstrate that explicitly tracking PAST and F UTURE contents helps NMT models alleviate this issue and generate better translation. In their work, the running PAST and F UTURE contents are modeled as recurrent states. However, the recurrent process is still non-trivial to determine which parts of the source words are the PAST and which are the F U TURE , and to what extent the recurrent states represent them respectively, this less interpretable nature is probably not the best way to model and exploit the dynamic PAST and F UTURE. We argue that an explicit separation of the source wor"
D19-1086,P02-1040,0,\N,Missing
D19-1086,W04-1013,0,\N,Missing
D19-1086,Q19-1006,0,\N,Missing
D19-1086,P16-1162,0,\N,Missing
D19-1086,D15-1229,0,\N,Missing
D19-1429,W17-6002,0,0.016438,"fusion model (with the blue α, see §4), where the green part belongs to the source model, the orange part belongs to the target model and the white part is common. Better viewed in color. eling schemes to model both the sample-level and element-level domain relevance. • Empirical evidences and analyses are provided on three different tasks in two different languages, which verify the effectiveness of our method. 2 Knowledge Distillation for Adaptation Knowledge distillation (KD), which distills the knowledge from a sophisticated model to a simple model, has been employed in domain adaptation (Bao et al., 2017; Meng et al., 2018). Recently, online knowledge distillation(Furlanello et al., 2018; Zhou et al., 2018) is shown to be more effective, which shares lower layers between the two models and trains them simultaneously. For sequence labeling domain adaptation, we utilize the online knowledge distillation method to distill knowledge from the source model to improve the target model, denoted as basicKD, which is depicted in Figure 2. We use the BiLSTM-CRF architecture (Huang et al., 2015), for both the source model and the target model, and share the embedding layer between them. Notations For the"
D19-1429,P07-1056,0,0.158191,"such as Chinese word segmentation (CWS), POS tagging (POS) and named entity recognition (NER), are fundamental tasks in natural language processing. Recently, with the development of deep learning, neural sequence labeling approaches have achieved pretty high accuracy (Chen et al., 2017; Zhang and Yang, 2018), relying on large-scale annotated corpora. However, most of the standard annotated corpora belong to the news domain, and models trained on these corpora will get sharp declines in performance when applied to other domains like social media, forum, literature or patents (Daume III, 2007; Blitzer et al., 2007), which limits their application in the real world. Domain adaptation 1 Our code is available at https://github.com/yhy1117/ FGKF-DA. aims to exploit the abundant information of wellstudied source domains to improve the performance in target domains (Pan and Yang, 2010), which is suitable to handle this issue. Following Daume III (2007), we focus on the supervised domain adaptation setting, which utilizes large-scale annotated data from the source domain and smallscale annotated data from the target domain. For sequence labeling tasks, each sample is usually a sentence, which consists of a seq"
D19-1429,P17-1110,0,0.252815,"are rising soooo fast! Alas as time goes by, hair’s gone. Rock to 204 Section next week! Table 1: Tweets from the social media domain have different degrees of relevance to the source domain (news). Within each case, the bold part is strongly relevant and the italic part is weakly relevant. Introduction Sequence labeling tasks, such as Chinese word segmentation (CWS), POS tagging (POS) and named entity recognition (NER), are fundamental tasks in natural language processing. Recently, with the development of deep learning, neural sequence labeling approaches have achieved pretty high accuracy (Chen et al., 2017; Zhang and Yang, 2018), relying on large-scale annotated corpora. However, most of the standard annotated corpora belong to the news domain, and models trained on these corpora will get sharp declines in performance when applied to other domains like social media, forum, literature or patents (Daume III, 2007; Blitzer et al., 2007), which limits their application in the real world. Domain adaptation 1 Our code is available at https://github.com/yhy1117/ FGKF-DA. aims to exploit the abundant information of wellstudied source domains to improve the performance in target domains (Pan and Yang, 2"
D19-1429,P07-1033,0,0.344686,"Missing"
D19-1429,P07-1034,0,0.10637,"to unsupervised or semi-supervised domain adaptation. However, we focus on supervised sequence labeling domain adaptation, where huge improvement can be achieved by utilizing only small-scale annotated data from the target domain. Previous works in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses on the supervised single source domain adaptation, which devote to implementing the knowledge fusion between the source domain and the target domain, not within multiple source domains. Moreover, considering the important ch"
D19-1429,P17-1060,0,0.0254509,"n. Previous works in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses on the supervised single source domain adaptation, which devote to implementing the knowledge fusion between the source domain and the target domain, not within multiple source domains. Moreover, considering the important characteristics of sequence labeling tasks, we put more attention to the finer-grained adaptation, considering the domain relevance in sample level and element level. 8 Figure 5: Results of CWS target test set with varying tar"
D19-1429,W06-0115,0,0.0878241,"Missing"
D19-1429,D18-1226,0,0.208759,"style, which only appear in the social media domain (i.e. weakly relevant). The phenomenon can be more complicated for the cases where the whole sample is strongly relevant while contains some target domain specific elements, or vice versa, showing the diversity of relevance at the element-level. In the rest of this paper, we use ‘domain relevance’ to refer to the domain relevance to the source domain, unless specified otherwise. Conventional neural sequence labeling domain adaptation methods (Liu and Zhang, 2012; Liu et al., 2014; Zhang et al., 2014; Chen et al., 2017; Peng and Dredze, 2017; Lin and Lu, 2018) mainly 4197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4197–4206, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics target source target source source source target source equally equally transfer transfer sample element sample element strongly relevant high domain relevance weakly relevant low domain relevance transfer moreless transfer more transfer less transfer (a) Previous methods source !# target &quot; source! target source target"
D19-1429,C12-2073,0,0.029265,"ets similar to the news domain (i.e. strongly relevant). But there are also some tweets of their own style, which only appear in the social media domain (i.e. weakly relevant). The phenomenon can be more complicated for the cases where the whole sample is strongly relevant while contains some target domain specific elements, or vice versa, showing the diversity of relevance at the element-level. In the rest of this paper, we use ‘domain relevance’ to refer to the domain relevance to the source domain, unless specified otherwise. Conventional neural sequence labeling domain adaptation methods (Liu and Zhang, 2012; Liu et al., 2014; Zhang et al., 2014; Chen et al., 2017; Peng and Dredze, 2017; Lin and Lu, 2018) mainly 4197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4197–4206, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics target source target source source source target source equally equally transfer transfer sample element sample element strongly relevant high domain relevance weakly relevant low domain relevance transfer moreless transfe"
D19-1429,D14-1093,0,0.0232295,"ws domain (i.e. strongly relevant). But there are also some tweets of their own style, which only appear in the social media domain (i.e. weakly relevant). The phenomenon can be more complicated for the cases where the whole sample is strongly relevant while contains some target domain specific elements, or vice versa, showing the diversity of relevance at the element-level. In the rest of this paper, we use ‘domain relevance’ to refer to the domain relevance to the source domain, unless specified otherwise. Conventional neural sequence labeling domain adaptation methods (Liu and Zhang, 2012; Liu et al., 2014; Zhang et al., 2014; Chen et al., 2017; Peng and Dredze, 2017; Lin and Lu, 2018) mainly 4197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4197–4206, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics target source target source source source target source equally equally transfer transfer sample element sample element strongly relevant high domain relevance weakly relevant low domain relevance transfer moreless transfer more transfer le"
D19-1429,J93-2004,0,0.0649324,"Missing"
D19-1429,D15-1064,0,0.0877344,"Missing"
D19-1429,W17-2612,0,0.109955,"me tweets of their own style, which only appear in the social media domain (i.e. weakly relevant). The phenomenon can be more complicated for the cases where the whole sample is strongly relevant while contains some target domain specific elements, or vice versa, showing the diversity of relevance at the element-level. In the rest of this paper, we use ‘domain relevance’ to refer to the domain relevance to the source domain, unless specified otherwise. Conventional neural sequence labeling domain adaptation methods (Liu and Zhang, 2012; Liu et al., 2014; Zhang et al., 2014; Chen et al., 2017; Peng and Dredze, 2017; Lin and Lu, 2018) mainly 4197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4197–4206, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics target source target source source source target source equally equally transfer transfer sample element sample element strongly relevant high domain relevance weakly relevant low domain relevance transfer moreless transfer more transfer less transfer (a) Previous methods source !# target &quot; source! ta"
D19-1429,C18-1232,0,0.0299762,"tion. Element-level Relevance To acquire the element-level relevance, we employ the domain representation q ∈ R2dh (dh is the dimension of the Bi-LSTM) and calculate the similarity between the element representation and the domain representation. We incorporate two methods to get q: (1) Domain-q: q is a trainable domain specific vector, where every element within a domain share the same q; (2) Sample-q: q is the domain relevant feature extracted from each sample, where every element within a sample share the same q. Because of the superiority of the capsule network modeling abstract features (Gong et al., 2018; Yang et al., 2018), we use it to capture the domain relevant features within a sample. We incorporate the same bottom-up aggregation process as Gong et al. (2018) and the encoded vector is regarded as q: q = Capsule(h) (5) where h is the hidden state matrix of a sample. The similarity calculation formula is the matrix dot 2 : wjelem = q&gt; Bhj (6) where hj is the hidden states of the j th element and wjelem is the relevance weight of it. B ∈ R2dh ×2dh is a trainable matrix. 2 We also try dot and MLP, while matrix dot get better performance with fewer parameters. 4199 3.2 Sample-level Relevance"
D19-1429,D18-1498,0,0.0203295,"m the target domain. Previous works in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses on the supervised single source domain adaptation, which devote to implementing the knowledge fusion between the source domain and the target domain, not within multiple source domains. Moreover, considering the important characteristics of sequence labeling tasks, we put more attention to the finer-grained adaptation, considering the domain relevance in sample level and element level. 8 Figure 5: Results of CWS target test se"
D19-1429,D11-1141,0,0.0158297,"Missing"
D19-1429,D17-1038,0,0.021029,"the target domain lexicons (Liu et al., 2014; Zhang et al., 2014), unlabeled (Liu and Zhang, 2012) or partial-labeled target domain data (Liu et al., 2014) to boost the sequence labeling adaptation performance, which belong to unsupervised or semi-supervised domain adaptation. However, we focus on supervised sequence labeling domain adaptation, where huge improvement can be achieved by utilizing only small-scale annotated data from the target domain. Previous works in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses o"
D19-1429,D18-1350,0,0.0332048,"Relevance To acquire the element-level relevance, we employ the domain representation q ∈ R2dh (dh is the dimension of the Bi-LSTM) and calculate the similarity between the element representation and the domain representation. We incorporate two methods to get q: (1) Domain-q: q is a trainable domain specific vector, where every element within a domain share the same q; (2) Sample-q: q is the domain relevant feature extracted from each sample, where every element within a sample share the same q. Because of the superiority of the capsule network modeling abstract features (Gong et al., 2018; Yang et al., 2018), we use it to capture the domain relevant features within a sample. We incorporate the same bottom-up aggregation process as Gong et al. (2018) and the encoded vector is regarded as q: q = Capsule(h) (5) where h is the hidden state matrix of a sample. The similarity calculation formula is the matrix dot 2 : wjelem = q&gt; Bhj (6) where hj is the hidden states of the j th element and wjelem is the relevance weight of it. B ∈ R2dh ×2dh is a trainable matrix. 2 We also try dot and MLP, while matrix dot get better performance with fewer parameters. 4199 3.2 Sample-level Relevance To acquire the samp"
D19-1429,D18-1041,0,0.0265729,"in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses on the supervised single source domain adaptation, which devote to implementing the knowledge fusion between the source domain and the target domain, not within multiple source domains. Moreover, considering the important characteristics of sequence labeling tasks, we put more attention to the finer-grained adaptation, considering the domain relevance in sample level and element level. 8 Figure 5: Results of CWS target test set with varying target training data si"
D19-1429,E14-1062,0,0.146862,"rongly relevant). But there are also some tweets of their own style, which only appear in the social media domain (i.e. weakly relevant). The phenomenon can be more complicated for the cases where the whole sample is strongly relevant while contains some target domain specific elements, or vice versa, showing the diversity of relevance at the element-level. In the rest of this paper, we use ‘domain relevance’ to refer to the domain relevance to the source domain, unless specified otherwise. Conventional neural sequence labeling domain adaptation methods (Liu and Zhang, 2012; Liu et al., 2014; Zhang et al., 2014; Chen et al., 2017; Peng and Dredze, 2017; Lin and Lu, 2018) mainly 4197 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4197–4206, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics target source target source source source target source equally equally transfer transfer sample element sample element strongly relevant high domain relevance weakly relevant low domain relevance transfer moreless transfer more transfer less transfer (a) Prev"
D19-1429,C18-1269,0,0.0312881,"i-supervised domain adaptation. However, we focus on supervised sequence labeling domain adaptation, where huge improvement can be achieved by utilizing only small-scale annotated data from the target domain. Previous works in domain adaptation often try to find a subset of source domain data to align with the target domain data (Chopra et al., 2013; Ruder and Plank, 2017) which realizes a kind of source data sample or construct a common feature space, while those methods may wash out informative characteristics of target domain samples. Instance-based domain adaptation (Jiang and Zhai, 2007; Zhang and Xiong, 2018) implement the source sample weighting by assigning higher weights to source domain samples which are more similar to the target domain. There are also some methods (Guo et al., 2018; Kim et al., 2017; Zeng et al., 2018) explicitly weighting multiple source domain models for target samples in multi-source domain adaptation. However, our work focuses on the supervised single source domain adaptation, which devote to implementing the knowledge fusion between the source domain and the target domain, not within multiple source domains. Moreover, considering the important characteristics of sequenc"
D19-1429,P18-1144,0,0.0412787,"st! Alas as time goes by, hair’s gone. Rock to 204 Section next week! Table 1: Tweets from the social media domain have different degrees of relevance to the source domain (news). Within each case, the bold part is strongly relevant and the italic part is weakly relevant. Introduction Sequence labeling tasks, such as Chinese word segmentation (CWS), POS tagging (POS) and named entity recognition (NER), are fundamental tasks in natural language processing. Recently, with the development of deep learning, neural sequence labeling approaches have achieved pretty high accuracy (Chen et al., 2017; Zhang and Yang, 2018), relying on large-scale annotated corpora. However, most of the standard annotated corpora belong to the news domain, and models trained on these corpora will get sharp declines in performance when applied to other domains like social media, forum, literature or patents (Daume III, 2007; Blitzer et al., 2007), which limits their application in the real world. Domain adaptation 1 Our code is available at https://github.com/yhy1117/ FGKF-DA. aims to exploit the abundant information of wellstudied source domains to improve the performance in target domains (Pan and Yang, 2010), which is suitable"
D19-1429,D17-1079,1,0.870722,"Missing"
D19-1597,P17-1152,0,0.0248281,"mbination of the scenario text, the question, and the option as a query, to retrieve the top-ranked sentence from a corpus. We use the ranking score of this sentence as the score of the option. Finally, we choose the option with the highest score as the answer. In PMI (Clark et al., 2016), for each option, we calculate the Pointwise Mutual Information (PMI) between the question and the option as 5868 the score of the option. Finally, we choose the option with the highest score as the answer. Probabilities in PMI are estimated based on a corpus. We tested four textual entailment methods: ESIM (Chen et al., 2017), DIIN (Gong et al., 2018), BERTN LI (Devlin et al., 2018), and BiMPM (Wang et al., 2017). The first three methods were trained on the XNLI dataset (Conneau et al., 2018). The last method was trained on the LCQMC dataset (Liu et al., 2018). For each option, a textual entailment method retrieves six topranked sentences from a corpus to form the entailing text. Retrieval follows the procedure described in the above-mentioned IR method. The scenario text and diagram annotations may or may not be included in the entailing text, depending on the configuration. A combination of the question and the"
D19-1597,D18-1269,0,0.0336256,"s the score of the option. Finally, we choose the option with the highest score as the answer. In PMI (Clark et al., 2016), for each option, we calculate the Pointwise Mutual Information (PMI) between the question and the option as 5868 the score of the option. Finally, we choose the option with the highest score as the answer. Probabilities in PMI are estimated based on a corpus. We tested four textual entailment methods: ESIM (Chen et al., 2017), DIIN (Gong et al., 2018), BERTN LI (Devlin et al., 2018), and BiMPM (Wang et al., 2017). The first three methods were trained on the XNLI dataset (Conneau et al., 2018). The last method was trained on the LCQMC dataset (Liu et al., 2018). For each option, a textual entailment method retrieves six topranked sentences from a corpus to form the entailing text. Retrieval follows the procedure described in the above-mentioned IR method. The scenario text and diagram annotations may or may not be included in the entailing text, depending on the configuration. A combination of the question and the option form the entailed text. Finally, we choose the option with the highest entailment score as the answer. We tested one reading comprehension method: BERTRC (Devlin e"
D19-1597,W18-2605,0,0.0239168,"u et al., 2018). For each option, a textual entailment method retrieves six topranked sentences from a corpus to form the entailing text. Retrieval follows the procedure described in the above-mentioned IR method. The scenario text and diagram annotations may or may not be included in the entailing text, depending on the configuration. A combination of the question and the option form the entailed text. Finally, we choose the option with the highest entailment score as the answer. We tested one reading comprehension method: BERTRC (Devlin et al., 2018). It was trained on the DuReader dataset (He et al., 2018). For each option, a reading comprehension method retrieves six top-ranked sentences from a corpus as part of the passage for reading comprehension. Retrieval follows the procedure described in the above-mentioned IR method. The scenario text and diagram annotations may or may not be included in the passage, depending on the configuration. Finally, the reading comprehension method extracts a text span from the passage. We choose the option that is the most similar to the extracted text span as the answer. Similarity is computed by the cosine similarity between two bags of words. 4.3 Results Th"
D19-1597,D17-1082,0,0.466365,"for question answering, textual entailment, and reading comprehension demonstrate the unique challenges presented by SQA for future research. 1 Introduction Scenario-based question answering (SQA) is an emerging application of NLP (Lally et al., 2017). Different from traditional QA, a question in SQA is accompanied by a scenario, e.g., a patient summary in the medical domain asking for diagnosis or treatment. A scenario differs from a document given in the reading comprehension task where the answer can be extracted or abstracted from the document (Rajpurkar et al., 2016; Nguyen et al., 2016; Lai et al., 2017). SQA requires retrieving and integrating knowledge from multiple sources, and applying general knowledge to a specific case described by the scenario. SQA has found application in many fields, especially in the legal domain (Ye et al., 2018; Luo et al., 2017; Zhong et al., 2018) and in high-school geography exams (Ding et al., 2018; Zhang et al., 2018). The latter is particularly challenging because a geographical scenario consists of both text and diagrams (e.g., maps, charts). Questions include city planning, climates, agriculture planning, transportation, etc. An example of a scenario and"
D19-1597,C18-1166,0,0.0317348,"correct answer) D. Thermal power plant Diagram annotations --- templated text: M is to the north of the river. M is in the northwest of the satellite city. Diagram annotations --- free-form text: The river flows north. Figure 1: An example of a scenario, a question, and diagram annotations. scribes a scenario to be decided (Ye et al., 2018; Luo et al., 2017; Zhong et al., 2018). For some domains, reasoning with domain knowledge is essential to SQA. Therefore, such questions often appear in exams like China’s version of the SAT called Gaokao. For example, for the geography domain, Ding et al. (2018) and Zhang et al. (2018) construct a knowledge graph to support answering scenario-based geography questions at high school level. 2.2 Related Datasets There are many datasets for traditional QA, such as WebQuestions (Berant et al., 2013) and WikiQA (Yang et al., 2015). A closely related task is reading comprehension, where the answer to a question is extracted or abstracted from a given document (Rajpurkar et al., 2016; Nguyen et al., 2016; Lai et al., 2017). By comparison, SQA is arguably more difficult because a scenario is present and contextualizes a question, but no direct answer can be"
D19-1597,D17-1289,0,0.160162,"nt from traditional QA, a question in SQA is accompanied by a scenario, e.g., a patient summary in the medical domain asking for diagnosis or treatment. A scenario differs from a document given in the reading comprehension task where the answer can be extracted or abstracted from the document (Rajpurkar et al., 2016; Nguyen et al., 2016; Lai et al., 2017). SQA requires retrieving and integrating knowledge from multiple sources, and applying general knowledge to a specific case described by the scenario. SQA has found application in many fields, especially in the legal domain (Ye et al., 2018; Luo et al., 2017; Zhong et al., 2018) and in high-school geography exams (Ding et al., 2018; Zhang et al., 2018). The latter is particularly challenging because a geographical scenario consists of both text and diagrams (e.g., maps, charts). Questions include city planning, climates, agriculture planning, transportation, etc. An example of a scenario and a question is presented in Figure 1. Geographical SQA has posed great challenges to NLP and related research, ranging from scenario understanding to cross-modal knowledge integration and reasoning. However, there is a lack of large datasets and benchmarking e"
D19-1597,D16-1264,0,0.392841,"lts on a variety of state-of-the-art methods for question answering, textual entailment, and reading comprehension demonstrate the unique challenges presented by SQA for future research. 1 Introduction Scenario-based question answering (SQA) is an emerging application of NLP (Lally et al., 2017). Different from traditional QA, a question in SQA is accompanied by a scenario, e.g., a patient summary in the medical domain asking for diagnosis or treatment. A scenario differs from a document given in the reading comprehension task where the answer can be extracted or abstracted from the document (Rajpurkar et al., 2016; Nguyen et al., 2016; Lai et al., 2017). SQA requires retrieving and integrating knowledge from multiple sources, and applying general knowledge to a specific case described by the scenario. SQA has found application in many fields, especially in the legal domain (Ye et al., 2018; Luo et al., 2017; Zhong et al., 2018) and in high-school geography exams (Ding et al., 2018; Zhang et al., 2018). The latter is particularly challenging because a geographical scenario consists of both text and diagrams (e.g., maps, charts). Questions include city planning, climates, agriculture planning, transporta"
D19-1597,D15-1237,0,0.092689,"Missing"
D19-1597,N18-1168,0,0.102134,"., 2017). Different from traditional QA, a question in SQA is accompanied by a scenario, e.g., a patient summary in the medical domain asking for diagnosis or treatment. A scenario differs from a document given in the reading comprehension task where the answer can be extracted or abstracted from the document (Rajpurkar et al., 2016; Nguyen et al., 2016; Lai et al., 2017). SQA requires retrieving and integrating knowledge from multiple sources, and applying general knowledge to a specific case described by the scenario. SQA has found application in many fields, especially in the legal domain (Ye et al., 2018; Luo et al., 2017; Zhong et al., 2018) and in high-school geography exams (Ding et al., 2018; Zhang et al., 2018). The latter is particularly challenging because a geographical scenario consists of both text and diagrams (e.g., maps, charts). Questions include city planning, climates, agriculture planning, transportation, etc. An example of a scenario and a question is presented in Figure 1. Geographical SQA has posed great challenges to NLP and related research, ranging from scenario understanding to cross-modal knowledge integration and reasoning. However, there is a lack of large datasets"
D19-1597,D18-1390,0,0.0663687,"l QA, a question in SQA is accompanied by a scenario, e.g., a patient summary in the medical domain asking for diagnosis or treatment. A scenario differs from a document given in the reading comprehension task where the answer can be extracted or abstracted from the document (Rajpurkar et al., 2016; Nguyen et al., 2016; Lai et al., 2017). SQA requires retrieving and integrating knowledge from multiple sources, and applying general knowledge to a specific case described by the scenario. SQA has found application in many fields, especially in the legal domain (Ye et al., 2018; Luo et al., 2017; Zhong et al., 2018) and in high-school geography exams (Ding et al., 2018; Zhang et al., 2018). The latter is particularly challenging because a geographical scenario consists of both text and diagrams (e.g., maps, charts). Questions include city planning, climates, agriculture planning, transportation, etc. An example of a scenario and a question is presented in Figure 1. Geographical SQA has posed great challenges to NLP and related research, ranging from scenario understanding to cross-modal knowledge integration and reasoning. However, there is a lack of large datasets and benchmarking efforts for this task."
D19-1597,D13-1160,0,\N,Missing
D19-1597,N19-1423,0,\N,Missing
I05-3035,J95-4004,0,0.0888271,"organization name, we collected these words to formed a “forbidden words” lexicon. Based on the consideration given in preceding section, we constructed a set of atomic feature patterns, listed in table 2. Additionally, we defined a set of conjunctive feature patterns, which could form effective feature conjunctions to express complicated contextual information. 2.3 Error-driven learning As a method based on statistics, no matter how well a CRFs model is constructed, some obviously errors always occurred because of the sparseness of training data. For this reson, error-driven learning method (Brill, 1995) is adopted to refine the segmentation result in this bakeoff in three steps: 1) Based on CRFs model, we segment the training data which has been removed all the space between words. Based on the comparison of the segmentation result with the original training data, the difference between them will be extracted. If a difference occurs more than one time, an error-driven rule will be constructed. The rule is described as: ǩψ ZZZZZZis the segmentation of ǩin training data. We named this rule set constructed by this step CRF-Ruleset. 2) Based on FMM&BMM, we segment the training data whi"
I05-3035,W03-1721,0,0.0194815,", through error-driven learning, we can segment the string “ 䙧ᇣᑇ⧚䆎 ” as “ 䙧ᇣᑇ⧚䆎 ” while this string is always segmented wrong as “䙧ᇣᑇ⧚䆎” segmented by CRFs model. In other words, error-driven learning can always can be seen as a consistency check. It assures the consistency of the segmentation of the training data and testing data when some strings such as “䙧ᇣᑇ⧚䆎” occur in both. 2.4 New word detection CRFs segmentation model can gives good performance on OOV words identification. But there are still some new words that have not been recognized. So an additive new words recognizer is adopted (Chen, 2003). In-word probability of each character is used for new word detection. The in-word probability of a character is a probability that the character occurs as a part of a word of two or more characters. And the in-word probability of a character is trained from the training data and is calculated as follows: Pinword (C ) Number of C Occurrence in words . Number of C Occurrence The consecutive single characters are combined into a new word if the in-word probability of each single character is over a threshold. Obviously, the value of the threshold is the key to the performance of this new words"
I05-3035,W04-3236,0,\N,Missing
I05-3035,C04-1081,0,\N,Missing
K17-1011,P02-1040,0,\N,Missing
K17-1011,D08-1024,1,\N,Missing
K17-1011,P13-1126,0,\N,Missing
K17-1011,P01-1067,0,\N,Missing
K17-1011,C04-1072,0,\N,Missing
K17-1011,P11-2031,0,\N,Missing
K17-1011,P05-1033,1,\N,Missing
K17-1011,J03-1002,0,\N,Missing
K17-1011,K15-1007,0,\N,Missing
K17-1011,2013.mtsummit-papers.7,0,\N,Missing
K17-1011,N12-1026,0,\N,Missing
L16-1104,C12-1018,0,0.0325284,"Missing"
L16-1104,D12-1133,0,0.0180185,"inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentence"
L16-1104,W08-2102,0,0.068385,"Missing"
L16-1104,P05-1022,0,0.122212,"Missing"
L16-1104,A00-2018,0,0.56498,"Missing"
L16-1104,D14-1082,0,0.266842,"al Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging task compared with neural dependency parsing (Table 1). First,"
L16-1104,Q13-1033,0,0.0129105,"). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging task compared with neural d"
L16-1104,P04-1013,0,0.114156,"Missing"
L16-1104,P08-1067,0,0.0719478,"Missing"
L16-1104,J93-2004,0,0.0583985,"on in the action layer, and lj is the jth label in the label layer. We adopt a greedy decoding strategy in the hierarchical parsing process. In each parsing step, the action type ai with the highest probability is first selected, and then the constituent label lj with the highest probability is selected given the optimal action type ai . As in the baseline parser, we adopt the cross-entropy loss as our training objective: L(θ) = − X log p(yi,j |x, Acts) + yi,j ∈A 5. 5.1. λ k θ k2 2 (10) Experiments Set-up We conduct our experiments on the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993). Following the standard splits of WSJ, sections 2–21 are used as the labeled training data, section 24 is used as the development data and section 23 is used as the evaluation data. Ten-fold jackknifing (Collins, 2000) is used to automatically assign POS tags to the training data. The SVMTool is used as the POS-tagger1 . 5.2. Parameters We carry out a development experiment to measure the correlation between hidden layer size and constituent parsing accuracies. From Table 3, we can see that both the baseline neural parser and the hierarchical neural parser achieve higher parsing accuracies wi"
L16-1104,N06-1020,0,0.105368,"Missing"
L16-1104,J08-4003,0,0.0283774,"89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging"
L16-1104,N07-1051,0,0.151924,"Missing"
L16-1104,W05-1513,0,0.402545,"parser by using a hierarchical output layer, inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of"
L16-1104,P12-1046,0,0.0380018,"Missing"
L16-1104,P13-1045,0,0.0608923,"Missing"
L16-1104,P07-1080,0,0.167269,"mapping matrix from hidden layer to the action layer and da is the number of action types. W3i ∈ Rdlabel ×dh is the mapping matrix from the hidden layer to the label layer. The probability of a labeled action yi,j , given its history Acts and input x, is computed as: p(yi,j |x, Acts) = p(ai |x, Acts) × p(lj |x, Acts, ai ) (7) Neural where Model Collins (1999) Charniak (2000) Charniak and Johnson (2005)‡ Huang (2008)‡ McClosky et al. (2006)‡ Shindo et al. (2012) Sagae and Lavie (2005) Petrov and Klein (2007) Carreras et al. (2008) Zhu et al. (2013) Zhu et al. (2013) + padding Henderson (2004)‡ Titov and Henderson (2007) Collobert (2011) Billingsley and Curran (2012) Socher et al. (2013)‡ Legrand and Collobert (2014) Watanabe and Sumita (2015) F1 88.2 89.6 91.1 91.7 92.1 92.4 86.0 90.1 91.1 89.9 90.4 90.1 90.0 87.9 84.9 90.4 88.3 90.7 Speed 3.5 5.7 This Work This Work + hierarchical 89.13 89.06 133.6 320.2 3.7 6.2 100.7 89.5 31.7 6.1 22.0 1.8 i p(ai |x, Acts) = eoact P k eoact (8) ak ∈GEN(Acts) j p(lj |x, Acts, ai ) = eolabel (ai ) P k eolabel (ai ) (9) Table 4: Comparisons with previous work. ‡: reranking model. Speed: sentences per second. lk ∈GEN(Acts) Here ai is the ith action in the action layer, and lj"
L16-1104,W09-3825,1,0.909876,"ing node with constituent label X whose child is s0 ; push the new node back onto the stack. • LEFT/RIGHT-X: pop the top two nodes s1 , s0 off the stack; generate a binary-branching node with constituent label X whose left child is s1 and right child is s0 , with the left (LEFT)/right (RIGHT) child as its head; push the new node back to the stack. The shift-reduce actions only build binarized trees. As a result, a binarization process is necessary to convert the Penn Treebank into binarized trees. During this process, temporary nodes are constructed. To accommodate for binarization, we follow Zhang and Clark (2009), adding counterparts to LEFT/RIGHT-X for temporary nodes, namely LEFT/RIGHT-TEMP-X. 3. h = (W1 x + b1 )3 (3) a∈A Here A is the set of all gold labeled actions in the training data. Mini-bached AdaGrad (Duchi et al., 2011) and dropout (Srivastava et al., 2014) are used for optimization. Hierarchical Output Neural Network Due to its large hidden and output layer sizes, the vanilla neural constituent parser is much slower than the dependency counterpart. The main computation cost is the mapping from hidden layer to output layer. Motivated by the hierarchical neural language model (Mnih and Hinto"
L16-1104,P11-2033,1,0.811029,"archical output layer, inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency par"
L16-1104,P15-1117,1,0.88504,"Missing"
L16-1104,P13-1043,1,0.834198,"Missing"
L16-1104,P15-1113,0,\N,Missing
L18-1145,D14-1179,0,0.03576,"Missing"
L18-1145,C12-1059,0,0.0335631,"Missing"
L18-1145,Q13-1033,0,0.0129566,"the experimental results indicate that our method can achieve +1.06 BLEU improvements. 2. Related Work To the best of our knowledge, Goldberg et. al. (2012) first define the concept of dynamic oracle and propose an online algorithm for parsing problems, , which provides a set of optimal transitions for every valid parser configuration. For configurations which are not part of a gold derivation, their dynamic oracle permits all transitions that can lead to a tree with minimum loss compared to the gold tree. Based on their approach, several other methods using dynamic oracle have been proposed (Goldberg and Nivre, 2013) (G´omezRodrıguez et al., 2014). However, their work in the field of parsing cannot be directly applied in neural machine translation. To mitigate the discrepancy between training and inference, Daume et al. (2009) introduce SEARN, which aims to tackle the problems that training examples might be different from actual test examples. They show that structured prediction can be mapped into a search setting usProposed Methods In this section, we first give a brief introduction of neural machine translation. And then we present the general framework for our algorithms. At last, we describe our two"
L18-1145,D14-1099,0,0.0274225,"Missing"
L18-1145,D13-1176,0,0.0203397,"setting usProposed Methods In this section, we first give a brief introduction of neural machine translation. And then we present the general framework for our algorithms. At last, we describe our two methods respectively, namely language model guided scheduled sampling and pre-trained model guided scheduled sampling. 3.1. Neural Machine Translation Neural machine translation aims to directly model the conditional probability p(Y |X) of translating a source sentence, x1 , ..., xn , to a target sentence, y1 , ..., ym . Generally, it accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013). Basically, the encoder generates a context vector for each source sentence and then the decoder outputs a translation, one target word at a time. During training when we are decoding, we always provide the model with true previous token at every time step. Minibatch stochastic gradient descent is applied to look for a set of parameters θ∗ that maximizes the log likelihood of producing the correct target sentence. Specifically, given a batch of training pairs {(X i , Y i )}, we aim to find θ∗ which satisfies: X log p(Y i |X i ; θ) (1) θ∗ = arg max θ (X i ,Y i ) Whereas during inference the mo"
L18-1145,P02-1040,0,0.102773,"Missing"
L18-1145,C14-1008,0,0.0422661,"ever, there are certain limitations in Scheduled Sampling and we propose two dynamic oracle-based methods to improve it. We manage to mitigate the discrepancy by changing the training process towards a less guided scheme and meanwhile aggregating the oracle’s demonstrations. Experimental results show that the proposed approaches improve translation quality over standard NMT system. Keywords: machine translation, dynamic oracle, language model 1. Introduction Neural networks have been widely used contemporarily and have achieved great performance on a variety of fields like sentiment analysis (Santos and Gattit, 2014) and visual object recognition (Ciregan et al., 2012). For sequential problems, recurrent neural networks can be applied to process sequences. To address issues like long term dependencies in the data (Bengio et al., 1994), the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or Gated Recurrent Unit (GRU) can be used to tackle the problem (Cho et al., 2014). A straightforward application of the LSTM and GRU architecture have already shown impressive performance in several difficult tasks, including machine translation (Sutskever et al., 2014), and image captioning (Vinyals et a"
N16-1148,E14-2007,0,0.09776,"e actions, the translation quality could be significantly improved. Greater gains could be achieved by iteratively performing both actions. 1 Typical IMT systems usually use a left-toright sentence completing framework pioneered by Langlais et al (2000), in which the users process the translation from the beginning of the sentence and interact with the system at the left-most error. By assuming the translation from the beginning to the modified part (called ""prefix"") to be correct, the system generates new translations after the given prefix (Koehn, 2009; Barrachina et al., 2009; Ortiz, 2011; Alabau et al., 2014). Introduction To obtain high quality translations, human translators usually have to modify the results generated by a machine translation (MT) system (called post editing, PE). In many cases, PE needs a lot of modifications, which is time-consuming (Plitt and Masselot, 2010). To speed up the process, interactive machine translation (IMT) is proposed which instantly update the translation result after every human action (Langlais et al., 2000; Foster et al., 2002; Barrachina et al., 2009; Koehn, 2009; González-Rubio et al., 2013; Alabau et al., 2014). Because the translation quality could be"
N16-1148,W02-1020,0,0.713831,""") to be correct, the system generates new translations after the given prefix (Koehn, 2009; Barrachina et al., 2009; Ortiz, 2011; Alabau et al., 2014). Introduction To obtain high quality translations, human translators usually have to modify the results generated by a machine translation (MT) system (called post editing, PE). In many cases, PE needs a lot of modifications, which is time-consuming (Plitt and Masselot, 2010). To speed up the process, interactive machine translation (IMT) is proposed which instantly update the translation result after every human action (Langlais et al., 2000; Foster et al., 2002; Barrachina et al., 2009; Koehn, 2009; González-Rubio et al., 2013; Alabau et al., 2014). Because the translation quality could be improved after every update, Despite the success of this left-to-right framework, one potential weakness is that it is difficult to modify critical translation errors at the end of a sentence. Critical translation errors are those errors that has large impact on the translation of other words or phrases. When a translation ambiguity occurs at the end of a sentence while it causes translation errors at the beginning, modifying this critical errors first may bring g"
N16-1148,2014.amta-workshop.3,0,0.0144391,"the algorithm in a typical phrase-based machine translation (Koehn et al., 2003). The only exception is that it makes an extra comparison between each translation option and previous PR pairs, which ignores all the phrases that overlap with the source side of a PRP. As a result, a lot of translation options are ignored, which makes the search space much smaller than standard decoding. In this way, we could guarantee that all the PRPs are correctly translated and the whole process can be carried out in real-time. The system could collect all PRPs and adapt the models using methods described in Germann (2014) or Marie (2015). In our current implementation, we mainly focus on the picking and revising step and leave model adaptation as future work. 3 Automatic Suggestion Models To further reduce the human actions, we propose to use automatic suggestion models for the picking and revising step, respectively. Such models can offer suggestions to users in both picking and revising steps. Because both picking and revising actions are performing selections from multiple candidates, we use classifier-based approaches to model these two steps. In the following subsections, we will introduce how we define t"
N16-1148,D13-1025,0,0.0170353,"er the given prefix (Koehn, 2009; Barrachina et al., 2009; Ortiz, 2011; Alabau et al., 2014). Introduction To obtain high quality translations, human translators usually have to modify the results generated by a machine translation (MT) system (called post editing, PE). In many cases, PE needs a lot of modifications, which is time-consuming (Plitt and Masselot, 2010). To speed up the process, interactive machine translation (IMT) is proposed which instantly update the translation result after every human action (Langlais et al., 2000; Foster et al., 2002; Barrachina et al., 2009; Koehn, 2009; González-Rubio et al., 2013; Alabau et al., 2014). Because the translation quality could be improved after every update, Despite the success of this left-to-right framework, one potential weakness is that it is difficult to modify critical translation errors at the end of a sentence. Critical translation errors are those errors that has large impact on the translation of other words or phrases. When a translation ambiguity occurs at the end of a sentence while it causes translation errors at the beginning, modifying this critical errors first may bring great positive effects on previous parts of the translation, which m"
N16-1148,D14-1130,0,0.069783,"ight have a large influence to the translation of their context. To make the picking step easier to be integrated into MT system, we limit the selection of translation errors to be those phrases in the previous PRcycle output. If it's the first PR-cycle, then those errors come from phrases used to generate the baseline translation. For more convenient user interactions, in our PRIMT system, critical errors can be picked from both the source and target side by simply a mouse click on it. The correspondence/alignment between source and target phrases are visualized for easier human observation. Green et al. (2014) demonstrated that performing post-editing, i.e. directly editing the translation errors, could get acceptable translations faster than performing left-to-right IMT. Such result also indicates that identifying critical translation errors is not a difficult task for human to perform. 2.3 Revising In the revising step, the users revise the translation of sji by selecting the correct translation t′ from the translation table, or manually add one if there is no 1 j si is the phrase that covers the source words from index i to j, and translated into t. 1242 A pick-revise pair (PRP), (sji , t′ ), is"
N16-1148,N03-1017,0,0.133198,"for human to perform. 2.3 Revising In the revising step, the users revise the translation of sji by selecting the correct translation t′ from the translation table, or manually add one if there is no 1 j si is the phrase that covers the source words from index i to j, and translated into t. 1242 A pick-revise pair (PRP), (sji , t′ ), is obtained after a PR cycle for a source sentence. We use a constrained decoder to search for the best translation with the previous PRPs as constraints. The constrained search algorithm is similar to the algorithm in a typical phrase-based machine translation (Koehn et al., 2003). The only exception is that it makes an extra comparison between each translation option and previous PR pairs, which ignores all the phrases that overlap with the source side of a PRP. As a result, a lot of translation options are ignored, which makes the search space much smaller than standard decoding. In this way, we could guarantee that all the PRPs are correctly translated and the whole process can be carried out in real-time. The system could collect all PRPs and adapt the models using methods described in Germann (2014) or Marie (2015). In our current implementation, we mainly focus o"
N16-1148,P09-4005,0,0.0210441,"trate that by interactions through either one of the actions, the translation quality could be significantly improved. Greater gains could be achieved by iteratively performing both actions. 1 Typical IMT systems usually use a left-toright sentence completing framework pioneered by Langlais et al (2000), in which the users process the translation from the beginning of the sentence and interact with the system at the left-most error. By assuming the translation from the beginning to the modified part (called ""prefix"") to be correct, the system generates new translations after the given prefix (Koehn, 2009; Barrachina et al., 2009; Ortiz, 2011; Alabau et al., 2014). Introduction To obtain high quality translations, human translators usually have to modify the results generated by a machine translation (MT) system (called post editing, PE). In many cases, PE needs a lot of modifications, which is time-consuming (Plitt and Masselot, 2010). To speed up the process, interactive machine translation (IMT) is proposed which instantly update the translation result after every human action (Langlais et al., 2000; Foster et al., 2002; Barrachina et al., 2009; Koehn, 2009; González-Rubio et al., 2013; Ala"
N16-1148,W00-0507,0,0.332552,"ation error (Pick) and revising the translation (Revise). The picked phrase could be at any position of the sentence, which improves the efficiency of human computer interaction. We also propose automatic suggestion models for the two actions to further reduce the cost of human interaction. Experiment results demonstrate that by interactions through either one of the actions, the translation quality could be significantly improved. Greater gains could be achieved by iteratively performing both actions. 1 Typical IMT systems usually use a left-toright sentence completing framework pioneered by Langlais et al (2000), in which the users process the translation from the beginning of the sentence and interact with the system at the left-most error. By assuming the translation from the beginning to the modified part (called ""prefix"") to be correct, the system generates new translations after the given prefix (Koehn, 2009; Barrachina et al., 2009; Ortiz, 2011; Alabau et al., 2014). Introduction To obtain high quality translations, human translators usually have to modify the results generated by a machine translation (MT) system (called post editing, PE). In many cases, PE needs a lot of modifications, which"
N16-1148,D15-1120,0,0.0256354,"Missing"
N16-1148,W07-0737,0,0.0270569,"The users could also type a new translation through a separated input area. Model Adaptation (sji ,t′ ) (sji ,t′ ) no Revising (sji ,t) Picking yes Stop Figure 1: An overview of PRIMT framework. 2.4 Decoder and Model Adaptation 2.2 Picking In the picking step, the users pick the wronglytranslated phrase, (sji ,t)1 , to be revised. The picking process aims at finding critical errors in the translation, caused by errors in the translation table or inherent translation ambiguities. The more critical the error is, the larger translation quality improvement can be achieved by correcting the error (Mohit and Hwa, 2007). Critical errors might have a large influence to the translation of their context. To make the picking step easier to be integrated into MT system, we limit the selection of translation errors to be those phrases in the previous PRcycle output. If it's the first PR-cycle, then those errors come from phrases used to generate the baseline translation. For more convenient user interactions, in our PRIMT system, critical errors can be picked from both the source and target side by simply a mouse click on it. The correspondence/alignment between source and target phrases are visualized for easier"
N16-1148,J03-1002,0,0.00572591,"select all correct translation options as positive instances for the revising step, and randomly sample the same number of wrong translation options to be negative instances. Specifically, translation options that are used by the baseline system are included as negative instances. 3.2.2 RSM Features The features used for RSM are showed in Table 3. For translations of a given source phrase, there is no need to compare their source-side information because these translation options share the same source phrase and context. So these features mainly focus 2 We trained word alignments with Giza++(Och and Ney, 2003) on estimating the translation quality of a given translation option. As a result, features for RSM only including the scores for TM, LM and LRM, etc, which are simpler compared to PSM. Category TM LM LRM count Lexical Description TM scores of current translation option LM score of current translation option LM score of each target word LRM scores of current translation option Target word count Target words 4.2 Methodology Table 3: Features for the RSM 4 Experiments 4.1 4.1.1 Experiment Settings Translation Settings Through out the experiments, we use an in-house implementation of the phrase-b"
N16-1148,P02-1040,0,0.110377,"corporate our PRIMT framework into the translation system. The parallel data for training the translation model includes 8.2 million sentences pairs from LDC2002E18, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T10, LDC2007T09. A 5gram language model is trained with MKN smoothing (Chen and Goodman, 1999) on Xinhua portion of Gigaword which contains 14.6 million sentences. We use a combination of NIST02 and NIST03 to tune the MT system parameters and train the suggestion models. We test the system on NIST04 and NIST05 data. The translation results are evaluated with case insensitive 4-gram BLEU (Papineni et al., 2002). Our baseline phrase-based MT system has comparable performance with the open source toolkit Moses (Koehn et al., 2003). 4.1.2 network has one hidden layer of 80 nodes, with sigmoid function as the activation function. We use one-hot representation for the source and target word features when using the maximum entropy and SVM model, and use pre-trained word embeddings (Mikolov et al., 2013) for the neural model. Classification Settings We use three classification models to model the automatic suggestion models: the maximum entropy model, the SVM model and the neural network model. We use a ma"
N18-1116,P16-1100,0,0.233867,"oth word-level and character-level information can be helpful for generating better representations, current research which tries to exploit both word-level and character-level information only composed the word-level representation by character embeddings with the word boundary information (Ling et al., 2015b; Costa-juss`a and 1284 Proceedings of NAACL-HLT 2018, pages 1284–1293 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Fonollosa, 2016) or replaces the word representation with its inside characters when encountering the out-of-vocabulary words (Luong and Manning, 2016; Wu et al., 2016). In this paper, we propose a novel encoder-decoder model that makes use of both character and word information. More specifically, we augment the standard encoder to attend to individual characters to generate better source word representations (§3.1). We also augment the decoder with a second attention that attends to the source-side characters to generate better translations (§3.2). To demonstrate the effectiveness of the proposed model, we carry out experiments on three translation tasks: Chinese-English, EnglishChinese and English-German. Our experiments show that: (1) t"
N18-1116,W17-4712,0,0.0236227,"improve the model by incorporating multiple levels of granularity. Specifically, we propose (1) an encoder with character attention which augments the (sub)word-level representation with character-level information; (2) a decoder with multiple attentions that enable the representations from different levels of granularity to control the translation cooperatively. Experiments on three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model and a strong characterbased model. 1 Introduction Neural machine translation (NMT) models (Britz et al., 2017) learn to map from source language sentences to target language sentences via continuous-space intermediate representations. Since word is usually thought of as the basic unit of language communication (Jackendoff, 1992), early NMT systems built these representations starting from the word level (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Weng et al., 2017). Later systems tried using smaller units such as subwords to address the problem of out-of-vocabulary (OOV) words (Sennrich et al., 2016; Wu et al., 2016). Although they obtain reasonable results, these word or sub-wor"
N18-1116,P16-2058,0,0.0380798,"Missing"
N18-1116,W17-4739,0,0.0442815,"Missing"
N18-1116,P16-1162,0,0.204501,"trong characterbased model. 1 Introduction Neural machine translation (NMT) models (Britz et al., 2017) learn to map from source language sentences to target language sentences via continuous-space intermediate representations. Since word is usually thought of as the basic unit of language communication (Jackendoff, 1992), early NMT systems built these representations starting from the word level (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Weng et al., 2017). Later systems tried using smaller units such as subwords to address the problem of out-of-vocabulary (OOV) words (Sennrich et al., 2016; Wu et al., 2016). Although they obtain reasonable results, these word or sub-word methods still have some potential weaknesses. First, the learned representations ∗ Corresponding author. of (sub)words are based purely on their contexts, but the potentially rich information inside the unit itself is seldom explored. Taking the Chinese word 被打伤 (bei-da-shang) as an example, the three characters in this word are a passive voice marker, “hit” and “wound”, respectively. The meaning of the whole word, “to be wounded”, is fairly compositional. But this compositionality is ignored if the whole word"
N18-1116,D17-1013,1,0.828696,"three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model and a strong characterbased model. 1 Introduction Neural machine translation (NMT) models (Britz et al., 2017) learn to map from source language sentences to target language sentences via continuous-space intermediate representations. Since word is usually thought of as the basic unit of language communication (Jackendoff, 1992), early NMT systems built these representations starting from the word level (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Weng et al., 2017). Later systems tried using smaller units such as subwords to address the problem of out-of-vocabulary (OOV) words (Sennrich et al., 2016; Wu et al., 2016). Although they obtain reasonable results, these word or sub-word methods still have some potential weaknesses. First, the learned representations ∗ Corresponding author. of (sub)words are based purely on their contexts, but the potentially rich information inside the unit itself is seldom explored. Taking the Chinese word 被打伤 (bei-da-shang) as an example, the three characters in this word are a passive voice marker, “hit” and “wound”, respe"
N18-1116,Q17-1026,0,0.151135,"e word or sub-word boundaries can be non-trivial. For languages like Chinese and Japanese, a word segmentation step is needed, which must usually be trained on labeled data. For languages like English and German, word boundaries are easy to detect, but subword boundaries need to be learned by methods like BPE. In both cases, the segmentation model is trained only in monolingual data, which may result in units that are not suitable for translation. On the other hand, there have been multiple efforts to build models operating purely at the character level (Ling et al., 2015a; Yang et al., 2016; Lee et al., 2017). But splitting this finely can increase potential ambiguities. For example, the Chinese word 红茶 (hong-cha) means “black tea,” but the two characters means “red” and “tea,” respectively. It shows that modeling the character sequence alone may not be able to fully utilize the information at the word or sub-word level, which may also lead to an inaccurate representation. A further problem is that character sequences are longer, making them more costly to process with a recurrent neural network model (RNN). While both word-level and character-level information can be helpful for generating better"
N18-1116,C16-1288,0,0.0317945,"Missing"
N19-1192,D17-1156,0,0.131636,"oints, guiding the training process to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against over-fitting. Furthermore, our method leads to an improvement on a machine reading experiment as well. 1 Introduction Neural Machine Translation (NMT) (Cho et al., 2014; Sutskever et al., 2014) has been rapidly developed during the past several years. For further performance improvement, deeper and more expressive structures (Johnson et al., 2017; Barone et al., 2017b; Gehring et al., 2017; Vaswani et al., 2017) have been exploited. However, all of these models have more than hundreds of millions of parameters, which makes the training process more challenging. During the training of NMT models, we notice the following two problematic phenomena: First, the training process is unstable. This is evidenced by the decreasing of training loss with ∗ Corresponding Author. fluctuate performance on the validation set. Second, the performance on validation set usually begins to worsen after several epochs, while the training loss keeps decreasing, which suggests t"
N19-1192,W17-4710,0,0.107698,"oints, guiding the training process to obtain better performance. Experiments on several datasets and language pairs show steady improvement over a strong self-attention-based baseline system. We also provide analysis on data-limited setting against over-fitting. Furthermore, our method leads to an improvement on a machine reading experiment as well. 1 Introduction Neural Machine Translation (NMT) (Cho et al., 2014; Sutskever et al., 2014) has been rapidly developed during the past several years. For further performance improvement, deeper and more expressive structures (Johnson et al., 2017; Barone et al., 2017b; Gehring et al., 2017; Vaswani et al., 2017) have been exploited. However, all of these models have more than hundreds of millions of parameters, which makes the training process more challenging. During the training of NMT models, we notice the following two problematic phenomena: First, the training process is unstable. This is evidenced by the decreasing of training loss with ∗ Corresponding Author. fluctuate performance on the validation set. Second, the performance on validation set usually begins to worsen after several epochs, while the training loss keeps decreasing, which suggests t"
N19-1192,Q17-1024,0,0.530165,"0; T0 = −1, Tˆ0 = −1; θT = ∅; θ00 = θ0 ; L0 (θ) = L(θ) while not reach stopping criteria do repeat if Tˆk = Tk then Lt (θ) = L(θ) else Lt (θ) = L(θ) + LW-KD (θ) 20 minimize Lt (θ) and update θt ; 0 0 θt = αθt−1 + (1 − α)θt ; t=t+1; until t mod ∆T == 0; Tk+1 = t; evaluate on validation set; if get better checkpoint then Tˆk+1 = t; if use EMA as teacher then 0 θT = θt ; else θT = θt ; 21 else 9 10 11 12 13 14 15 16 17 18 19 Tˆk+1 = Tˆk ; 22 k = k + 1; 23 Integrated with Mean Teacher Knowledge distillation usually works better when teacher models have better performance. As Tarvainen and Valpola (2017) proposed in their work, averaging model parameters over training steps tends to produce a more accurate model that using final parameters directly. They called this method as Mean Teacher. Following Tarvainen and Valpola (2017), besides updating parameters, we maintain the exponential moving average (EMA) of the model parameters as: 0 0 θt = αθt−1 + (1 − α)θt , (9) where t is the update step, θ is the parameters of the 0 training model and θ the parameters of EMA. α is the decay weight which is close to 1.0, and typically in multiple-nines range, i.e., 0.999, 0.9999. By doing so, at each time"
N19-1192,W18-2705,0,0.0477999,"n NMT Regularization has broad applications in training NMT models to improve performance and avoid over-fitting. There are some common regularization techniques, such as L2 normalization and dropout (Srivastava et al., 2014). These methods are simple and easy to implement but need carefully tuning on the validation set. These methods are also orthogonal to our method. There are also some works to exploit regularization techniques in fine tuning of NMT model. Barone et al. (2017a) proposed a tuneout method which randomly replaces columns of weight matrices of out-of-domain parameter matrices. Khayrallah et al. (2018) shared similar training object with us, as they computed the KL divergence between out-of-domain and in-domain model. Both of their works request a pre-trained teacher model, while we are work on a more general training problem which does not require such kind of model. 6.2 Online Knowledge Distillation While traditional knowledge distillation requires a static, pre-trained teacher model, online knowledge distillation tends to overcome this problem by selecting or generating a teacher dynamically from scratch. To the best of our knowledge, Zhang et al. (2017) is the first trial to replace the"
N19-1192,D16-1139,0,0.131576,"on, the student model learns to match the predictions of the teacher model. Concretely, assuming that we learn a classification model (parameterized by θ) on a set of training samples in the form of (x, y) with |V| classes. Instead of minimizing the cross-entropy loss between one-hot label y and model’s output probability p(y|x; θ), knowledge distillation uses the teacher model’s distribution q(·|x) as “soft targets” and optimizes the loss: LKD (θ) = − |V| X q(y = k|x; θT ) (5) k=1 log p(y = k|x; θ), where θT parameterizes the teacher model and p(·|x) is the distribution of the student model. Kim and Rush (2016) proposed that, as the loss of NMT model (Equation 4) can be factored into minimizing cross-entropy loss between the target 1933 validation score update teacher models (darker means better) knowledge distillation training steps direction Checkpoints: Teacher Models: ?? ′ ?? ′ +1 ?? ′ +2 ?? ′′ ?? ′′ +1 Figure 1: Illustration of online distillation from checkpoints(ODC). Darker color means better performance on validation data. In validation step Tk0 , ODC selects the current best checkpoints as the teacher model; while in the next validation step Tk0 +1 , the training generates a better checkpo"
N19-1192,P84-1044,0,0.354513,"Missing"
N19-1192,2015.iwslt-evaluation.11,0,0.177643,"Missing"
N19-1192,W17-4739,0,0.0208349,"anning (2015) 5 , which has 133K sentence pairs, with 2.70M English words and 3.31M Vietnamese words. We use the released validation and test set, which has 1553 and 1268 sentences respectively. Following the settings in Huang et al. (2017), the Vietnamese and English vocabulary size are 7,709 and 17,191, respectively. For WMT17 English-Turkish translation task, We use the pre-processed data released by WMT176 . It has 207K sentence pairs, with 5.21M English words and 4.63 Turkish words. We use newstest2016 as our validation set and newstest2017 as the test set. We use joint BPE segmentation (Sennrich et al., 2017) to process the whole training data. The merge operations are 16K. Implementation Details Without specific statement, we follow the transformer base v1 hyperparameters settings 7 , with 6 layers in both encoder and decoder, 512 hidden units and 8 attention heads in multi-head attention mechanism and 2048 hidden units in feed-forward layers. Parameters are optimized using Adam(Kingma and Ba, 2014). The initial learning rate is set as 0.1 and scheduled according to the method proposed in Vaswani et al. (2017), with warm-up steps as 4000. We periodically evaluate the training model on the validat"
N19-1192,W16-2323,0,0.135357,"worsen after several epochs, while the training loss keeps decreasing, which suggests the model being at risk of over-fitting. In order to alleviate these issues, the common practice is to periodically evaluate models on a held-out set (with each evaluated model saved as a checkpoint). Training is terminated when m consecutive checkpoints show no improvement and select the checkpoint with best evaluation score as the final model. Further improvement can be achieved by utilizing more checkpoints, by smoothing, which averages these checkpoints’ parameters to generate more desirable parameters (Sennrich et al., 2016a); or by ensemble, which averages these checkpoints’ output probabilities at every step during inference (Chen et al., 2017). However, we notice that all of these methods have a limitation. Once the training process gets parameters with poor performance, selecting, smoothing or ensemble from the checkpoints in this process may have limited generalization performance as well. We impute the limitation to the “offline” property of these methods. In other words, only employing checkpoints after training cannot affect the original training process. In this paper, we propose to utilize checkpoints"
N19-1192,P16-1162,0,0.382409,"worsen after several epochs, while the training loss keeps decreasing, which suggests the model being at risk of over-fitting. In order to alleviate these issues, the common practice is to periodically evaluate models on a held-out set (with each evaluated model saved as a checkpoint). Training is terminated when m consecutive checkpoints show no improvement and select the checkpoint with best evaluation score as the final model. Further improvement can be achieved by utilizing more checkpoints, by smoothing, which averages these checkpoints’ parameters to generate more desirable parameters (Sennrich et al., 2016a); or by ensemble, which averages these checkpoints’ output probabilities at every step during inference (Chen et al., 2017). However, we notice that all of these methods have a limitation. Once the training process gets parameters with poor performance, selecting, smoothing or ensemble from the checkpoints in this process may have limited generalization performance as well. We impute the limitation to the “offline” property of these methods. In other words, only employing checkpoints after training cannot affect the original training process. In this paper, we propose to utilize checkpoints"
N19-1192,N18-1122,0,0.0274376,"former.py 8 We set k = 5 in this experiments. 1936 as the final model. In this case, checkpoints may have better performance but higher variance which could be harmful to parameters averaging. SYSTEM newsdev2017 newstest2017 21.96 22.24 23.01 23.37 24.22 Zhang et al. (2018c) baseline ODC • best-k-ensemble: Do ensemble inference (average the output probabilities) with the best k checkpoints (Chen et al., 2017). Table 2: Case-sensitive BLEU scores on WMT17 Chinese-English Translation As shown in Table 1, our baseline is comparable to the other two recent published results (Zhang et al. (2018b), Yang et al. (2018)). In consistent with Chen et al. (2017), using checkpoints for smoothing or ensemble does improve the baseline system. Using EMA parameters also improve the baseline system as well, which is in consist with (Tarvainen and Valpola, 2017). Compared to the baseline, our approach ODC brings translation improvement across different test sets and achieves 42.48 BLEU scores on average(+1.09 BLEU v.s. baseline). This result confirms that using best checkpoint as teacher indeed helps improving the performance of the translation model. Besides, ODC is comparable to the best results among smoothing and"
N19-1192,P18-1166,0,0.08734,"k checkpoints, instead of the last k, 4 http://data.statmt.org/wmt18/translationtask/preprocessed/zh-en/ 5 https://github.com/tefan-it/nmt-en-vi 6 http://data.statmt.org/wmt17/translationtask/preprocessed/tr-en/ Evaluation on Chinese-English Translation Tasks 7 https://github.com/tensorflow/tensor2tensor/blob/v1.3.0/ tensor2tensor/models/transformer.py 8 We set k = 5 in this experiments. 1936 as the final model. In this case, checkpoints may have better performance but higher variance which could be harmful to parameters averaging. SYSTEM newsdev2017 newstest2017 21.96 22.24 23.01 23.37 24.22 Zhang et al. (2018c) baseline ODC • best-k-ensemble: Do ensemble inference (average the output probabilities) with the best k checkpoints (Chen et al., 2017). Table 2: Case-sensitive BLEU scores on WMT17 Chinese-English Translation As shown in Table 1, our baseline is comparable to the other two recent published results (Zhang et al. (2018b), Yang et al. (2018)). In consistent with Chen et al. (2017), using checkpoints for smoothing or ensemble does improve the baseline system. Using EMA parameters also improve the baseline system as well, which is in consist with (Tarvainen and Valpola, 2017). Compared to the"
N19-1259,D10-1101,0,0.082814,"ons for different targets in the same review. • We build four datasets from different domains serving as a benchmark for future works. We conduct extensive experiments on these datasets, and the results show that our model could significantly exceed a variety of baselines. We release the datasets and our source code at https://github.com/NJUNLP/TOWE 2 Related works A lot of works have been carried out for Opinion Targets Extraction. Traditional methods can be categorized into unsupervised/semi-supervised methods (Hu and Liu, 2004; Zhuang et al., 2006; Qiu et al., 2011) and supervised methods (Jakob and Gurevych, 2010; Shu et al., 2017). Recently, deep learning methods have also made progress in this task. Liu et al. (2015) apply a recurrent neural network with pre-trained word emebddings to solve this task. Yin et al. (2016) exploit a CRF with dependency-paths enhanced word embeddings for aspect term extraction. Poria et al. (2016) use a deep convolutional neural network (CNN) and Xu et al. (2018) propose a CNN model with double embeddings. Some works extract the targets and opinion words jointly as a co-extraction strategy. Qiu et al. (2011) propose double propagation to expand opinion targets and opinio"
N19-1259,D17-1310,0,0.619841,"s “limited” and “excellent” are opinion words. More examples can be found in the upper part of Figure 1. Recently, a great number of works based on neural networks have been done on these two subtasks (Liu et al., 2015; Poria et al., 2016; Xu et al., 2018). Furthermore, some works also integrate the two subtasks into a multi-task learning architecture to extract them jointly, which achieves great progress on both subtasks (Wang et al., 2016, 2017; 2509 Proceedings of NAACL-HLT 2019, pages 2509–2518 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Li and Lam, 2017). However, the extracted opinion targets and opinion words are not in pairs and the correspondence is not extracted. For instance, in the example sentence, hmenu:limitedi and hdishes:excellenti are two opinion pairs. Obviously, extracting them as pairs is significant for ABSA. Additionally, in Figure 1, the list of pairs extracted from the example review can be considered to be an extractive pair-wise opinion summarization. Considering the significance of the pairs in reviews and promising results of targets extraction in previous works, in this paper, we propose a new subtask for ABSA named T"
N19-1259,D15-1168,0,0.233023,"Missing"
N19-1259,D14-1162,0,0.0808095,"Missing"
N19-1259,S15-2082,0,0.507654,"Missing"
N19-1259,S14-2004,0,0.832884,"] • restaurant : [excellent] Figure 1: The upper part is a restaurant review and the lower part shows the pairs of extracted opinion targets (in red) and opinion words (in blue). Introduction Sentiment analysis, also known as opinion mining (Pang and Lee, 2007; Liu, 2012), has drawn increasing attention of researchers and industries in recent years. It can provide valuable information from user-generated reviews. However, sentiment analysis at sentence level or document level sometimes cannot provide more detailed information, thus a finer-grained task, Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014), is proposed to identify the opinions of a specific target or aspect ∗ Corresponding author. in reviews. ABSA consists of multiple subtasks including aspect category detection, opinion target extraction, aspect level sentiment classification etc. Opinion target extraction (OTE) and opinion words extraction (OWE) are two such fundamental subtasks. Opinion targets, sometimes called aspect terms, are the words or phrases in the sentence representing features or entities towards which users show attitude. Opinion words (or opinion terms) refer to those terms used to express attitude explicitly. F"
N19-1259,J11-1002,0,0.338747,"erate target-specific context representations for different targets in the same review. • We build four datasets from different domains serving as a benchmark for future works. We conduct extensive experiments on these datasets, and the results show that our model could significantly exceed a variety of baselines. We release the datasets and our source code at https://github.com/NJUNLP/TOWE 2 Related works A lot of works have been carried out for Opinion Targets Extraction. Traditional methods can be categorized into unsupervised/semi-supervised methods (Hu and Liu, 2004; Zhuang et al., 2006; Qiu et al., 2011) and supervised methods (Jakob and Gurevych, 2010; Shu et al., 2017). Recently, deep learning methods have also made progress in this task. Liu et al. (2015) apply a recurrent neural network with pre-trained word emebddings to solve this task. Yin et al. (2016) exploit a CRF with dependency-paths enhanced word embeddings for aspect term extraction. Poria et al. (2016) use a deep convolutional neural network (CNN) and Xu et al. (2018) propose a CNN model with double embeddings. Some works extract the targets and opinion words jointly as a co-extraction strategy. Qiu et al. (2011) propose double"
N19-1259,W95-0107,0,0.253098,"handcrafted templates. Moreover, in these two methods, the process of detecting opinion words and the process of discovering correspondence is separated into two tasks, which suffers from error propagation. Our model for TOWE aims at detecting the corresponding opinion words in one step with sequence labeling. 3 3.1 Our Methods Task Formulation Given a sentence s = {w1 , w2 , . . . , wi , . . . , wn } consisting of n words, and a opinion target t in the sentence, the task is to make sequence labelling on the sentence to extract the target-oriented opinion words. We use the BIO tagging scheme (Ramshaw and Marcus, 1995) on this task. For each word wi in the sentence s, it should be tagged as yi ∈ {B, I, O} (B: Beginning, I: Inside, O: Others). For example, for different opinion targets, the sentence “Waiters are very friendly and the pasta is out of this world .” is tagged in wi /yi style as follows: 1. Waiters/O are/O very/O [friendly/B] and/O the/O pasta/O is/O out/O of/O this/O world/O ./O (Given opinion target: waiter, extract “friendly” as corresponding opinion word). 2. Waiters/O are/O very/O friendly/O and/O the/O pasta/O is/O [out/B of/I this/I world/I] ./O (Given Opinion target: pasta, extract “out"
N19-1259,P17-2023,0,0.0320155,"in the same review. • We build four datasets from different domains serving as a benchmark for future works. We conduct extensive experiments on these datasets, and the results show that our model could significantly exceed a variety of baselines. We release the datasets and our source code at https://github.com/NJUNLP/TOWE 2 Related works A lot of works have been carried out for Opinion Targets Extraction. Traditional methods can be categorized into unsupervised/semi-supervised methods (Hu and Liu, 2004; Zhuang et al., 2006; Qiu et al., 2011) and supervised methods (Jakob and Gurevych, 2010; Shu et al., 2017). Recently, deep learning methods have also made progress in this task. Liu et al. (2015) apply a recurrent neural network with pre-trained word emebddings to solve this task. Yin et al. (2016) exploit a CRF with dependency-paths enhanced word embeddings for aspect term extraction. Poria et al. (2016) use a deep convolutional neural network (CNN) and Xu et al. (2018) propose a CNN model with double embeddings. Some works extract the targets and opinion words jointly as a co-extraction strategy. Qiu et al. (2011) propose double propagation to expand opinion targets and opinion words lists in a"
N19-1259,C16-1311,0,0.176544,"So, we first split the sentence into three segments: left context {w1 , w2 , · · · , wl }, target term {wl+1 , · · · , wr−1 } and right context {wr , · · · , wn } and left and right contexts are targetspecific.We use a left LSTM to model the left context plus target and a right LSTM to model the target plus right context respectively. In this way the target-specific contexts could generate targetspecific context representations. However, the direction of the two LSTMs is a crucial problem. 3.3.1 Inward-LSTM We can use a simple strategy called Inward-LSTM, which follows the design of TD-LSTM (Tang et al., 2016). As Figure 2 shows, Inward-LSTM runs the two LSTMs from the two ends of the sentence to the middle target respectively. It runs the left LSTM from the first word to opinion target as a forward-LSTM and a right LSTM from the last word to the opinion target as a backward-LSTM, so we call it as Inward. This is a process of passing the context to target. We obtain left context representations HL and right context representations HR as follows: −−−−→ hLi = LSTM(hLi−1 , ei ), ∀i ∈ [1, · · · , r − 1] , (1) ←−−−− hRi = LSTM(hRi+1 , ei ), ∀i ∈ [l + 1, · · · , n] . (2) It is obvious that the words of o"
N19-1259,D16-1059,0,0.293386,"plicitly. For example, in the sentence “The menu is limited but almost all of the dishes are excellent.”, the words “menu” and “dishes” are two opinion targets, and the words “limited” and “excellent” are opinion words. More examples can be found in the upper part of Figure 1. Recently, a great number of works based on neural networks have been done on these two subtasks (Liu et al., 2015; Poria et al., 2016; Xu et al., 2018). Furthermore, some works also integrate the two subtasks into a multi-task learning architecture to extract them jointly, which achieves great progress on both subtasks (Wang et al., 2016, 2017; 2509 Proceedings of NAACL-HLT 2019, pages 2509–2518 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Li and Lam, 2017). However, the extracted opinion targets and opinion words are not in pairs and the correspondence is not extracted. For instance, in the example sentence, hmenu:limitedi and hdishes:excellenti are two opinion pairs. Obviously, extracting them as pairs is significant for ABSA. Additionally, in Figure 1, the list of pairs extracted from the example review can be considered to be an extractive pair-wise opinion summarization."
N19-1259,P18-2094,0,0.718266,"the words or phrases in the sentence representing features or entities towards which users show attitude. Opinion words (or opinion terms) refer to those terms used to express attitude explicitly. For example, in the sentence “The menu is limited but almost all of the dishes are excellent.”, the words “menu” and “dishes” are two opinion targets, and the words “limited” and “excellent” are opinion words. More examples can be found in the upper part of Figure 1. Recently, a great number of works based on neural networks have been done on these two subtasks (Liu et al., 2015; Poria et al., 2016; Xu et al., 2018). Furthermore, some works also integrate the two subtasks into a multi-task learning architecture to extract them jointly, which achieves great progress on both subtasks (Wang et al., 2016, 2017; 2509 Proceedings of NAACL-HLT 2019, pages 2509–2518 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Li and Lam, 2017). However, the extracted opinion targets and opinion words are not in pairs and the correspondence is not extracted. For instance, in the example sentence, hmenu:limitedi and hdishes:excellenti are two opinion pairs. Obviously, extracting"
N19-1325,H05-1091,0,0.554559,"re heuristically labeled as LivedIn by DS, but neither of them mention the relation while S2 mentions the BornIn relation. S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For example, if a relational fact <Apple, founder, Steve Jobs> exists in KB, distant supervision will assign founder as the label of all sentences that contain “Apple” and “Steve Jobs” together. However, it suffers from n"
N19-1325,P05-1045,0,0.0141379,"n a robust relation classifier which explicitly learns from correctly labeled data and correct incorrectly labeled data implicitly. The training procedure for relation classifier is summarized in Algorithm 2. 4 4.1 Batch size bs Word Dimension dw Position Dimension dp Convolution Filter Dimension dc Convolution Window Size l Latent Variable Dimension dz Dropout p Regulator λ, β 160 50 5×2 230 3 100 0.5 100, 2 Table 2: Hyperparameter settings Freebase with New York Times corpus(NYT)2 and developed by (Riedel et al., 2010). Entity mentions are recognized by the Stanford named entity recognizer (Finkel et al., 2005). The relation facts in Freebase are divided into two parts for training and testing respectively. The sentences from the corpus of the years 2005-2006 are used as the training instances, and sentences from 2007 are used as the testing instances. There are 52 positive relations and a special relation NA. Following previous works, we evaluate our model on the held-out evaluation, which compares relation facts extracted from the test corpus with those in Freebase. We adopt aggregated precision/recall curves and precision@N (P@N) to illustrate the performance of our model. 4.2 Parameter Settings"
N19-1325,D18-1247,0,0.187379,"tperforms the state-of-the-art models. 1 DS LivedIn Gold NA LivedIn BornIn NA LivedIn Table 1: Examples of noisy labeling problem in distant supervision relation classification. S1 and S2 are heuristically labeled as LivedIn by DS, but neither of them mention the relation while S2 mentions the BornIn relation. S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For example, if a relati"
N19-1325,P11-1055,0,0.674899,"pple” and “Steve Jobs” together. However, it suffers from noisy labeling problem due to the irrelevance of aligned text and incompleteness of KB, which consists of false positives and false negatives. The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2"
N19-1325,P16-1200,0,0.479563,"abeling problem due to the irrelevance of aligned text and incompleteness of KB, which consists of false positives and false negatives. The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics DS true"
N19-1325,D17-1189,0,0.442433,"ashed line) than without consideration of false positive instances. Nevertheless, there are still a lot of false negative instances expressing similar semantic information with positive data. These instances also provide evidence for the target relation. The incorrect labels will weaken the discriminative capability of available features and confuse the model if they stay the same. However, when we remedy the label correctly, we indeed possess the optimal decision boundary (red solid line). • There lacks an effective method to fully utilize noisy data of distant supervision. (Xu et al., 2013; Liu et al., 2017) apply methods such as pseudo-labels to directly correct the label of noisy data and Luo et al. (2017) design a dynamic transition matrix to model noise patterns. They still suffer from the drawback of error propagation during training. To tackle the above challenges, we propose a novel framework exploiting noisy data to enhance distant supervision relation classification. We design an instance discriminator with reinforcement learning to recognize both false positive and false negative instances simultaneously, and further split the noisy dataset into two sets, representing correctly labeled"
N19-1325,P17-1040,0,0.0565925,"t of false negative instances expressing similar semantic information with positive data. These instances also provide evidence for the target relation. The incorrect labels will weaken the discriminative capability of available features and confuse the model if they stay the same. However, when we remedy the label correctly, we indeed possess the optimal decision boundary (red solid line). • There lacks an effective method to fully utilize noisy data of distant supervision. (Xu et al., 2013; Liu et al., 2017) apply methods such as pseudo-labels to directly correct the label of noisy data and Luo et al. (2017) design a dynamic transition matrix to model noise patterns. They still suffer from the drawback of error propagation during training. To tackle the above challenges, we propose a novel framework exploiting noisy data to enhance distant supervision relation classification. We design an instance discriminator with reinforcement learning to recognize both false positive and false negative instances simultaneously, and further split the noisy dataset into two sets, representing correctly labeled and incorrectly labeled data respectively. Additionally, we learn a robust relation classifier applyin"
N19-1325,P09-1113,0,0.924258,"S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For example, if a relational fact <Apple, founder, Steve Jobs> exists in KB, distant supervision will assign founder as the label of all sentences that contain “Apple” and “Steve Jobs” together. However, it suffers from noisy labeling problem due to the irrelevance of aligned text and incompleteness of KB, which consists of false positiv"
N19-1325,P18-1046,0,0.212871,"The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics DS true positive data DS false positive data side effect of incorrectly labeled data by recognizing them and treating them as unlabeled data. On"
N19-1325,P18-1199,0,0.45276,"The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics DS true positive data DS false positive data side effect of incorrectly labeled data by recognizing them and treating them as unlabeled data. On"
N19-1325,P05-1053,0,0.511935,"s LivedIn by DS, but neither of them mention the relation while S2 mentions the BornIn relation. S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For example, if a relational fact <Apple, founder, Steve Jobs> exists in KB, distant supervision will assign founder as the label of all sentences that contain “Apple” and “Steve Jobs” together. However, it suffers from noisy labeling proble"
N19-1325,D12-1042,0,0.619739,"together. However, it suffers from noisy labeling problem due to the irrelevance of aligned text and incompleteness of KB, which consists of false positives and false negatives. The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association f"
N19-1325,P16-1220,0,0.0253473,"hat our method outperforms the state-of-the-art models. 1 DS LivedIn Gold NA LivedIn BornIn NA LivedIn Table 1: Examples of noisy labeling problem in distant supervision relation classification. S1 and S2 are heuristically labeled as LivedIn by DS, but neither of them mention the relation while S2 mentions the BornIn relation. S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For ex"
N19-1325,P13-2117,0,0.0520394,"Missing"
N19-1325,W02-1010,0,0.405364,"ification. S1 and S2 are heuristically labeled as LivedIn by DS, but neither of them mention the relation while S2 mentions the BornIn relation. S3 expresses the LivedIn relation but it is mislabeled as NA since no relation of the entity pair exist in KB. Introduction Relation classification plays a crucial role in natural language processing (NLP) tasks, such as question answering and knowledge base completion (Xu et al., 2016; Han et al., 2018a). The goal of relation classification is to predict relations of the target entity pair given a plain text. Traditional supervised learning methods (Zelenko et al., 2002; Bunescu and Mooney, 2005; Zhou et al., 2005) heavily rely on large scale annotated data which is time and labor consuming. Mintz et al. (2009) proposed distant supervision (DS) to automatically generate training data for relation classification based on the assumption that if two target entities have a relation in knowledge base (KB), sentences containing this entity pair might express the relation. For example, if a relational fact <Apple, founder, Steve Jobs> exists in KB, distant supervision will assign founder as the label of all sentences that contain “Apple” and “Steve Jobs” together."
N19-1325,D15-1203,0,0.720479,"uffers from noisy labeling problem due to the irrelevance of aligned text and incompleteness of KB, which consists of false positives and false negatives. The false positives means that not all sentences containing two entities mention the relation in KB, such as S1 and S2 in Table 1. And the false negatives are sentences are mislabeled as no relation (NA) due to the absence of relational fact in KB even though they express the target relation, such as S3 in Table 1. In order to reduce the impact of noisy data, previous works (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018b) adopt MultiInstance Learning (MIL) for relation classification. Recent studies (Feng et al., 2018; Qin et al., 2018b,a) introduce reinforcement learning (RL) and adversarial learning to filter out incorrectly labeled sentences and achieve significant improvements. However, there are two remaining challenges of noisy labeling problem. • Most of these approaches focus on solving the false positives but overlook false nega3216 Proceedings of NAACL-HLT 2019, pages 3216–3225 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Li"
N19-1325,C14-1220,0,0.0373944,"data. To address the issue of data sparsity, Mintz et al. (2009) propose distant supervision to automatically annotate large scale training data, which inevitably results in noisy labeling problem. To tolerate noisy instances in positive examples, most early approaches employ multiinstance learning framework, including multiinstance single-label learning (Riedel et al., 2010) and multi-instance multi-label learning (Hoffmann et al., 2011; Surdeanu et al., 2012). Recently, deep learning has also been introduced to propose an end-to-end convolutional neural network for relation classification (Zeng et al., 2014). In the sentences bag of one entity pair, Zeng et al. (2015) select the most reliable sentence, and Lin et al. (2016) propose attention schemes to de-emphasize unreliable sentences. Han et al. (2018b) incorporate hierarchical information of relations to enhance the attention scheme. But they fail to handle the issue where all sentences in one bag are mislabeled. Feng et al. (2018); Qin et al. (2018b,a) further achieve improvement by using reinforcement 3217 Relation Classifier with Semi-Supervised Learning Instance Discriminator with Reinforcement Learning reward Classifier DPOS DNA Instance"
P12-2056,J93-2003,0,0.0410874,"Missing"
P12-2056,W08-0336,0,0.32287,"Missing"
P12-2056,D09-1075,0,0.180957,"e it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to"
P12-2056,P10-1016,0,0.111291,"ic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to conventional word a"
P12-2056,P08-1115,0,0.0415467,"“bilingual motivated word (BS)” respectively. Both methods iteratively learn word segmentation and alignment alternatively, with the former starting from word-based corpus and the latter starting from characters-based corpus. Therefore, PW can be experimented on all segmentations. Table 6 lists their results in small288 scale task, we see that both PW and BS underperformed our approach. This may be attributed to the low recall of the learned BS or PW in their approaches. BS underperformed both two baselines, one reason is that Ma and Way (2009) also employed word lattice decoding techniques (Dyer et al., 2008) to tackle the low recall of BS, which was removed from our experiments for fair comparison. Interestingly, we found that using character as WSA and BS as WSR (Char+BS), a moderate gain (+0.43 point) was achieved compared with fully BS-based system; and using character as WSA and PW as WSR (Char+PW), significant gains were achieved compared with fully PW-based system, the result of CTB segmentation in this setting even outperformed our proposed approach (+0.42 point). This observation indicated that in our framework, better combinations of WSA and WSR can be found to achieve better translation"
P12-2056,N07-2007,0,0.449632,"0), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (Elming and Habash, 2007) to convert the character alignment to conventional word alignment for translation rule induction. In the 2 We hereafter use “word segmentation” for short. Interestingly, word is also a basic token in syntax-based rules. 285 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 285–290, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics experiment, our approach consistently outperformed two baselines with three different word segmenters: fully word-based system (using word for both alignment and translation) and fu"
P12-2056,J07-3002,0,0.0638484,"Missing"
P12-2056,P09-1104,0,0.0171604,"ent combination only, no translation results were reported. 287 4 Experiments 4.1 Setup FBIS corpus (LDC2003E14) (210K sentence pairs) was used for small-scale task. A large bilingual corpus of our lab (1.9M sentence pairs) was used for large-scale task. The NIST’06 and NIST’08 test sets were used as the development set and test set respectively. The Chinese portions of all these data were preprocessed by character segmenter (CHAR), ICTCLAS word segmenter 5 (ICT) and Stanford word segmenters with CTB and PKU specifications6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as ESChar, which is converted to three ESWords based on three segmentations respectively. These ESWords were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evalu"
P12-2056,P07-2045,0,0.00620301,"s6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as ESChar, which is converted to three ESWords based on three segmentations respectively. These ESWords were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the"
P12-2056,W04-3250,0,0.0723423,"Missing"
P12-2056,2005.mtsummit-papers.11,0,0.00520031,"f Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the vocabulary size and translation performance without explicitly distinguishing WSA and WSR. We then evaluated the translation performance. The baselines are fully word-based MT systems (WordSys), i.e. using word as both WSA and WSR, and fully character-based systems (CharSys). Table 5 6 http://www.ictclas.org/ http://nlp.stanford.edu/software/segmenter.shtml S L CTB PKU ICT CTB PKU ICT Word alignment P R F 76.0 81.9 78.9 76.1 82.0 79.0 75.2 80.8 78.0 79.6 85.6 82.5 80.0 85.4 82.6 80.0 85.0 82.4 Character alignment P R F 78.2 85.2 8"
P12-2056,P07-1039,0,0.0187328,"and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, vi"
P12-2056,E09-1063,0,0.743425,"f the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use"
P12-2056,P03-1021,0,0.0209669,"aghighi et al. (2009) were hand-aligned as ESChar, which is converted to three ESWords based on three segmentations respectively. These ESWords were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) with all segmentations. This gain can be attributed to the small vocabulary size (sparsity) for character alignment. The observation is consistent with Koehn (2005) which claimed that there is a negative correlation between the vocabulary size and translation performance without explicitly di"
P12-2056,J03-1002,0,0.0161365,"he development set and test set respectively. The Chinese portions of all these data were preprocessed by character segmenter (CHAR), ICTCLAS word segmenter 5 (ICT) and Stanford word segmenters with CTB and PKU specifications6 respectively. The first 100 sentence pairs of the hand-aligned set in Haghighi et al. (2009) were hand-aligned as ESChar, which is converted to three ESWords based on three segmentations respectively. These ESWords were appended to training corpus with the corresponding word segmentation for evaluation purpose. Both character and word alignment were performed by GIZA++ (Och and Ney, 2003) enhanced with gdf heuristics to combine bidirectional alignments (Koehn et al., 2003). A 5-gram language model was trained from the Xinhua portion of Gigaword corpus. A phrase-based MT decoder similar to (Koehn et al., 2007) was used with the decoding weights optimized by MERT (Och, 2003). 4.2 Evaluation We first evaluate the alignment quality. The method discussed in section 3 was used to compare character and word alignment. As can be seen from Table 3, the systems using character as WSA outperformed the ones using word as WSA in both small-scale (row 3-5) and large-scale task (row 6-8) wit"
P12-2056,W10-1760,0,0.520071,"us as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WSR respectively, as shown Figure 1(b). We investigate a solution in this framework: first, we use Chinese character as the basic unit for alignment, viz. character alignment; second, we use a simple method (El"
P12-2056,2009.eamt-1.3,0,0.0377949,"Missing"
P12-2056,W07-0705,0,0.0608566,"Missing"
P12-2056,C10-1135,0,0.0705294,"o the word segmentation of the bilingual corpus as word segmentation for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specifications as WSA and WS"
P12-2056,P11-3001,1,0.883752,"Missing"
P12-2056,W08-0335,0,0.346113,"ity, we will refer to the word segmentation of the bilingual corpus as word segmentation for alignment (WSA for short), because it determines the basic tokens for alignment; and refer to the word segmentation of the aligned corpus as word segmentation for rules (WSR for short), because it determines the basic tokens of translation 1 WSA Bilingual Corpus rules2, which also determines how the translation rules would be matched by the source sentences. It is widely accepted that word segmentation with a higher F-score will not necessarily yield better translation performance (Chang et al., 2008; Zhang et al., 2008; Xiao et al., 2010). Therefore, many approaches have been proposed to learn word segmentation suitable for SMT. These approaches were either complicated (Ma et al., 2007; Chang et al., 2008; Ma and Way, 2009; Paul et al., 2010), or of high computational complexity (Chung and Gildea 2009; Duan et al., 2010). Moreover, they implicitly assumed that WSA and WSR should be equal. This requirement may lead to a suboptimal problem that word segmentation better for alignment is not necessarily better for translation. To tackle this, we propose a framework that uses different word segmentation specific"
P12-2056,J03-4003,0,\N,Missing
P12-2056,D08-1064,0,\N,Missing
P15-1080,D13-1106,0,0.0259952,"eatures. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Section 3) Employing a neural network for SMT modeling has two issues to be tackled. The first issue"
P15-1080,J93-2003,0,0.0319196,"lation probability feature selects phrases that occurs more frequently in the training corpus, which sometimes is long with a lower translation probability, as in translating named entities or idioms; Introduction One of the core problems in the research of statistical machine translation is the modeling of translation hypotheses. Each modeling method defines a score of a target sentence e = e1 e2 ...ei ...eI , given a source sentence f = f1 f2 ...fj ...fJ , where each ei is the ith target word and fj is the jth source word. The well-known modeling method starts from the Source-Channel model (Brown et al., 1993)(Equation 1). The scoring of e decomposes to the calculation of a translation model and a language model. P r(e|f ) = P r(e)P r(f |e)/P r(f ) (2) (1) 825 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 825–835, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sometimes is short but with a high translation probability, as in translating verbs or pronouns. These three features jointly decide the choice of translations. Simply use the weighted"
P15-1080,P05-1033,0,0.676095,"m hm (e|f )] =∑ ∑M ′ e′ exp[ m=1 λm hm (e |f )] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL , is actually a linear combination of all features, as shown in Equation 3. sL (e) = M ∑ λm hm (e|f ) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation probability feature"
P15-1080,P11-2031,0,0.033909,"e hypothesis pairs from n-best set of current iteration only presented in Section 4.3. tem is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005). We set the beam size to 20. We train a 5-gram language model on the monolingual data with MKN smoothing(Chen and Goodman, 1998). For each parameter tuning experiments, we ran the same training procedure 3 times and present the average results. The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002). Significant test is performed using bootstrap re-sampling implemented by Clark et al. (2011). We employ a two-layer neural network with 11 input layer nodes, corresponding to features listed in Section 3.2 and 1 output layer node. The number of nodes in the hidden layer varies in different settings. The sigmoid function is used as the activation function for each node in the hidden layer. For the output layer we use a linear activation function. We try different λ for the L1 norm from 0.01 to 0.00001 and use the one with best performance on the development set. We solve the optimization problem with ALGLIB package4 . Although the system performance on the dev set varies, the performa"
P15-1080,N03-1017,0,0.229596,"|f ) 1 ∑M exp[ m=1 λm hm (e|f )] =∑ ∑M ′ e′ exp[ m=1 λm hm (e |f )] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL , is actually a linear combination of all features, as shown in Equation 3. sL (e) = M ∑ λm hm (e|f ) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation proba"
P15-1080,Q14-1031,0,0.0710243,"L (e) = M ∑ λm hm (e|f ) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation probability feature selects phrases that occurs more frequently in the training corpus, which sometimes is long with a lower translation probability, as in translating named entities or idioms; Introduction One of the core problems in the research of statistical machine translation is the modeling of translatio"
P15-1080,P14-1129,0,0.0270603,"ocal minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Section 3) Employing a neural network for SMT modeling has two issues to be tackled. The first issue is the parameter learn"
P15-1080,P08-2010,0,0.028251,"ndicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder. These work focus on the explicit representation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality. The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. Duh and Kirchhoff (2008) used the boosting method to combine several results of MERT and achieved improvement in a re-ranking setting. Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/compon"
P15-1080,P06-1077,0,0.0917127,"∑ ∑M ′ e′ exp[ m=1 λm hm (e |f )] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL , is actually a linear combination of all features, as shown in Equation 3. sL (e) = M ∑ λm hm (e|f ) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation probability feature selects phrases th"
P15-1080,P13-1078,0,0.0194805,"presentation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality. The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. Duh and Kirchhoff (2008) used the boosting method to combine several results of MERT and achieved improvement in a re-ranking setting. Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; A"
P15-1080,P14-1012,0,0.0138411,"linear models. (Section 6) 2 Related work Many research has been attempting to bring nonlinearity into the training of SMT. These efforts could be roughly divided into the following three categories. The first line of research attempted to reinterpret original features via feature transformation or additional learning. For example, Maskey and Zhou (2012) use a deep belief network to learn representations of the phrase translation and lexical translation probability features. Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder. These work focus on the explicit representation of the features and usually employ extra learning procedure. Our proposed method only takes the original features, with no transformation, as the input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality. The second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. Duh and Kirchhoff (2008) used the b"
P15-1080,P14-1066,0,0.0186922,"ch employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Section 3) Employing a neural network for"
P15-1080,P02-1038,0,0.445544,"Missing"
P15-1080,P03-1021,0,0.706452,"arget sides of the translation In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Section 3) Employing a neural network for SMT modeling has two issues to be tackled. The first issue is the parameter learning. Log-linear models rely on minimum error rate training (MERT) (Och, 2003) to achieve best performance. When the scoring function become non-linear, the intersection points of these non-linear functions could not be effectively calculated and enumerated. Thus MERT is no longer suitable for learning the parameters. To solve the problem, we present a framework for effective training including several criteria to transform the training problem into a binary classification task, a unified objective function and an iterative training algorithm. (Section 4) The second issue is the structure of neural network. Single layer neural networks are equivalent to linear models; t"
P15-1080,D11-1125,0,0.438376,"ric as eval(·) 1 . Note that, in linear cases, s is a linear function as in Equation 3, while in the non-linear case described in this paper, s is the scoring function in Equation 4. Ideally, the training objective is to select a scoring function sˆ, from all functions S, that scores the ˆ, higher than correct translation (or references) e any other hypotheses (Equation 5). sˆ = {s ∈ S|s(ˆ e) &gt; s(e) ∀e ∈ C} Pairwise (PW) To score the better hypothesis in sampled hypothesis pairs higher than the worse one in the same pair. This objective is adapted from the Pairwise Ranking Optimization (PRO) (Hopkins and May, 2011), which tries to ranking all the hypotheses instead of selecting the best one. We use the same sampling strategy as their original paper. Note that each of the above criteria transforms the original problem of selecting best hypotheses from an exponential space to a certain pairwise comparison problem, which could be easily trained using binary classifiers. 4.2 Training Objective For the binary classification task, we use a hinge loss following Watanabe (2012). Because the network has a lot of parameters compared with the linear model, we use a L1 norm instead of L2 norm as the regularization"
P15-1080,P02-1040,0,0.0946696,"best and worst hypothesis in Cnbest as the ”hope” and ”fear” hypothesis, respectively, in order to avoid multi-pass decoding. 4.1 Training Criteria The task of machine translation is a complex problem with structural output space. Decoding algorithms search for the translation hypothesis with the highest score, according to a given scoring function, from an exponentially large set of candidate hypotheses. The purpose of training is to select the scoring function, so that the function score the hypotheses ”correctly”. The correctness is often introduced by some extrinsic metrics, such as BLEU (Papineni et al., 2002).  or We denote the scoring function as s(f , e; θ),  denote the simply s, which is parameterized by θ; set of all translation hypotheses as C; denote the extrinsic metric as eval(·) 1 . Note that, in linear cases, s is a linear function as in Equation 3, while in the non-linear case described in this paper, s is the scoring function in Equation 4. Ideally, the training objective is to select a scoring function sˆ, from all functions S, that scores the ˆ, higher than correct translation (or references) e any other hypotheses (Equation 5). sˆ = {s ∈ S|s(ˆ e) &gt; s(e) ∀e ∈ C} Pairwise (PW) To sc"
P15-1080,D13-1140,0,0.0221914,"ting. Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models. The third line of research attempted to add non-linear features/components into the log-linear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target sides of the translation In this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional l"
P15-1080,N12-1026,0,0.0146753,"thesis pairs higher than the worse one in the same pair. This objective is adapted from the Pairwise Ranking Optimization (PRO) (Hopkins and May, 2011), which tries to ranking all the hypotheses instead of selecting the best one. We use the same sampling strategy as their original paper. Note that each of the above criteria transforms the original problem of selecting best hypotheses from an exponential space to a certain pairwise comparison problem, which could be easily trained using binary classifiers. 4.2 Training Objective For the binary classification task, we use a hinge loss following Watanabe (2012). Because the network has a lot of parameters compared with the linear model, we use a L1 norm instead of L2 norm as the regularization term, to favor sparse solutions. We define our training objective function in Equation 6. (5) In practice, the candidate set C is exponentially large and hard to enumerate; the correct translation ˆ may not even exist in the current search space for e various reasons, e.g. unknown source word. As a result, we use the n-best set Cnbest to approximate C, use the extrinsic metric eval(·) to evaluate the quality of hypotheses in Cnbest and use the following three"
P15-1080,P01-1067,0,0.651429,"del. 1 P r(e|f ) = pλM (e|f ) 1 ∑M exp[ m=1 λm hm (e|f )] =∑ ∑M ′ e′ exp[ m=1 λm hm (e |f )] Because the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL , is actually a linear combination of all features, as shown in Equation 3. sL (e) = M ∑ λm hm (e|f ) (3) m=1 The log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). It is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014). Taking common features in a typical phrasebased (Koehn et al., 2003) or hierarchical phrasebased (Chiang, 2005) machine translation system as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phra"
P15-1080,P14-2023,0,\N,Missing
P15-1102,P07-1056,0,0.286073,"Missing"
P15-1102,W03-0407,0,0.12415,"ous views form two different views of one review text, it is natural to employ the co-training algorithm, which requires two views for semi-supervised classification. Co-training is a typical bootstrapping algorithm that first learns a separate classifier for each view using the labeled data. The most confident predictions of each classifier on the unlabeled data are then used to construct additional labeled training data iteratively. Co-training has been extensively used in NLP, including statistical parsing (Sarkar , 2001), reference resolution (Ng and Cardie, 2003), part-of-speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004), and sentiment classification (Wan , 2009; Li et al., 2010a). But it should be noted that the dual views in our approach are different from traditional views. One important property of our approach is that two views are opposite and therefore associated with opposite class labels. Figure 2 illustrates the process of dual-view co-training. (1) Dual-view training For each instance in the initial labeled set, we construct the dual-view representations. Let xlo and xla denote the bags of words in the original view and the antonymous view, respectively."
P15-1102,P10-1043,0,0.684428,"classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the na¨ıve Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with per"
P15-1102,C10-1072,0,0.291112,"classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the na¨ıve Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with per"
P15-1102,I13-1052,0,0.0220069,"ed with the initial labeled data only; • Expectation Maximization (EM), with the na¨ıve Bayes model proposed by Nigam et al. (2000); • Label Propagation (LP), a graph-based semi-supervised learning method proposed by Zhu and Ghahramani (2002); • Transductive SVM (T-SVM), an extension of SVM so that it can exploit unlabeled data in semi-supervised learning ( Joachims, 1999); • Self-Training, a bootstrapping model that first trains a classifier, uses it to classify the unlabeled data, and adds the most confident data to the labeled set; • Self-Reserved, a variation of self-training proposed in (Liu et al., 2013),with a reserved procedure to incorporate some less confident examples; • Co-Static, the co-training algorithm by using two static partitions of feature set as two views (Blum and Mitchell, 1998); • Co-Dynamic, a variation of co-training that uses dynamic feature space in each loop. It was reported in (Li et al., 2011) that the CoDynamic significantly outperforms Co-Static significantly; • Co-PI, another variation of co-training proposed by (Li et al., 2010a), by using personal 1059 900 and impersonal views for co-training. 4.3 We implement the following nine systems and compare them with our"
P15-1102,W04-2405,0,0.257398,"text, it is natural to employ the co-training algorithm, which requires two views for semi-supervised classification. Co-training is a typical bootstrapping algorithm that first learns a separate classifier for each view using the labeled data. The most confident predictions of each classifier on the unlabeled data are then used to construct additional labeled training data iteratively. Co-training has been extensively used in NLP, including statistical parsing (Sarkar , 2001), reference resolution (Ng and Cardie, 2003), part-of-speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004), and sentiment classification (Wan , 2009; Li et al., 2010a). But it should be noted that the dual views in our approach are different from traditional views. One important property of our approach is that two views are opposite and therefore associated with opposite class labels. Figure 2 illustrates the process of dual-view co-training. (1) Dual-view training For each instance in the initial labeled set, we construct the dual-view representations. Let xlo and xla denote the bags of words in the original view and the antonymous view, respectively. Note that the class labels in two views are"
P15-1102,W06-3808,0,0.284929,"s. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the na¨ıve Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (2011) explored the use of label propagation (Zhu and Ghahramani, 2002). As pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment patterns. However, to the best knowledge, such investigations are very scarce in t"
P15-1102,W02-1011,0,0.0196424,"unlabeled reviews. The dominating text representation method in both supervised and semi-supervised sentiment classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the na¨ıve Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-tr"
P15-1102,I08-1039,0,0.0285942,"supervised sentiment classification is known as the bag-of-words (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the na¨ıve Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed co"
P15-1102,Y11-1044,0,0.0701477,"et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the na¨ıve Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (2011) explored the use of label propagation (Zhu and Ghahramani, 2002). As pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment patterns. However, to the best knowledge, such investigations are very scarce in the research of semiProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1054–1063, c Beijing, China, July 26-31, 2015. 2015 Association for Comp"
P15-1102,N01-1023,0,0.187828,"Missing"
P15-1102,N03-1023,0,0.037582,"Approach Since the original and antonymous views form two different views of one review text, it is natural to employ the co-training algorithm, which requires two views for semi-supervised classification. Co-training is a typical bootstrapping algorithm that first learns a separate classifier for each view using the labeled data. The most confident predictions of each classifier on the unlabeled data are then used to construct additional labeled training data iteratively. Co-training has been extensively used in NLP, including statistical parsing (Sarkar , 2001), reference resolution (Ng and Cardie, 2003), part-of-speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004), and sentiment classification (Wan , 2009; Li et al., 2010a). But it should be noted that the dual views in our approach are different from traditional views. One important property of our approach is that two views are opposite and therefore associated with opposite class labels. Figure 2 illustrates the process of dual-view co-training. (1) Dual-view training For each instance in the initial labeled set, we construct the dual-view representations. Let xlo and xla denote the bags of words in the original"
P15-1102,P09-1027,0,0.430652,"problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the na¨ıve Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (2011) explored the use of label propagation (Zhu and Ghahramani, 2002). As pointed by (Goldberg and Zhu, 2006): it is necessary to investigate better review text representations and similarity measures based on linguistic knowledge, as well as reviews’ sentiment patterns. However, to the best knowledge, such investigations are very scarce in the research of semiProceedings of the 53rd Annual Meeting of the Association for Computatio"
P15-1102,P13-2093,1,0.828907,"ords (BOW) model, which is difficult to meet the requirements for understanding the review text and dealing with complex linguistic structures such as negation. For example, the BOW representations of two opposite reviews “It works well” and “It doesn’t work well” are considered to be very similar by most statistical learning algorithms. In supervised sentiment classification, many approaches have been proposed in addressing the negation problem (Pang et al., 2002; Na et al., 2004; Polanyi and Zaenen , 2004; Kennedy and Inkpen, 2006; Ikeda et al., 2008; Li et al., 2010b; Orimaye et al., 2012; Xia et al., 2013). Nevertheless, in semi-supervised sentiment classification, most of the current approaches directly apply standard semi-supervised learning algorithms, without paying attention to appropriate representation for review texts. For example, Aue and Gamon (2005) applied the na¨ıve Bayes EM algorithm (Nigam et al., 2000). Goldberg and Zhu (2006) applied a graph-based semi-supervised learning algorithm by (Zhu et al., 2003). Wan (2009) employed a co-training approach for cross-language sentiment classification. Li et al. (2010a) employed cotraining with personal and impersonal views. Ren et al. (20"
P15-1102,C10-2173,0,0.0170348,"ntiment analysis task of rating inference. Dasgupta and Ng (2009) proposed a semi-supervised approach to mine the unambiguous reviews at first and then exploiting them to classify the ambiguous reviews, via a combination of active learning, transductive learning and ensemble learning. Ren et al. (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised sentiment classifier, and compared their results with Transductive SVMs(T-SVM). LP and T-SVM are 1055 transductive learning methods where the test data should participate in the training process. Zhou et al. (2010) proposed a deep learning approach called active deep networks to address semi-supervised sentiment classification with active learning. Socher et al. (2012) introduced a deep learning framework called semi-supervised recursive autoencoders for predicting sentencelevel sentiment distributions. The limitation of deep learning approaches might be their dependence on a considerable amount of unlabeled data to learn the representations and the inability to explicitly model the negation problem. One line of semi-supervised learning research is to bootstrap class labels using techniques like self-tr"
P15-1102,P09-1079,0,\N,Missing
P15-1102,D11-1014,0,\N,Missing
P16-1132,D15-1041,0,0.0171462,"speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been use"
P16-1132,P05-1022,0,0.125056,"of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the"
P16-1132,D14-1082,0,0.694626,"s0 ], j is the head of the queue (i.e. [ q0 = wj , q1 = wj+1 · · · ]), and L is a set of dependency arcs that has been built. At each step, the parser chooses one of the following actions: • S HIFT (S): move the front word wj from the queue onto the stacks. • L EFT-l (L): add an arc with label l between the top two trees on the stack (s1 ← s0 ), and remove s1 from the stack. • R IGHT-l (R): add an arc with label l between the top two trees on the stack (s1 → s0 ), and remove s0 from the stack. Given the sentence “John loves Mary”, the gold standard action sequence is S, S, L, S, R. 2.1 Model Chen and Manning (2014) proposed a deterministic neural dependency parser, which rely on dense embeddings to predict the optimal actions at each step. We propose a variation of Chen and Manning 1394 (2014), which splits the output layer into two hierarchical layers: the action layer and dependency label layer. The hierarchical parser determines a action in two steps, first deciding the action type, and then the dependency label (Figure 2). At each step of deterministic parsing, the neural model extracts n atomic features from the parsing state. We adopt the feature templates of Chen and Manning (2014). Every atomic"
P16-1132,D15-1215,0,0.0311275,"Missing"
P16-1132,W02-1001,0,0.0460083,"ee with the best heuristic reranking score yˆi0 . J(Θh ) = 1 |Dh | X (xi ,yi0 ,ˆ yi0 )∈Dh ri (Θh ) = ri (Θh ) + λ ||Θh || 2 max(0, st (xi , yˆi0 , Θh )) − st (xi , yi0 , Θh ) (20) (21) The detailed training algorithm is given by Algorithm 1. AdaGrad (Duchi et al., 2011) updating with subgradient (Ratliff et al., 2007) and minibatch is adopted for optimization. 5 5.1 section 23 for final testing. Following prior work on reranking, we use Penn2Malt1 to convert constituent trees to dependency trees. Ten-fold POS jackknifing is used in the training of the baseline parser. We use the POS-tagger of Collins (2002) to assign POS automatically. Because our reranking model is a dynamic reranking model, which generates training instances during search, we train 10 baseline parsing models on the 10-fold jackknifing data, and load the baseline parser model dynamically for reranking training . We follow Chen and Manning (2014), using the set of pre-trained word embeddings with a dictionary size of 13,0002 from Collobert et al. (2011). The word embeddings were trained on the entire English Wikipedia, which contains about 631 million words. 5.2 There are two different networks in our system, namely a hierarchic"
P16-1132,P15-1033,0,0.0578806,"curacies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural n"
P16-1132,P13-2111,0,0.0549975,"Missing"
P16-1132,J08-4003,0,0.0982509,"Missing"
P16-1132,D11-1137,0,0.0521305,"Missing"
P16-1132,Q13-1012,0,0.383871,"the best tree output. Denote b(i) as the beam at i-th step of search, k-best candidates in the beam of i + 1 step is: b(i + 1) = arg K (st (x, c, Θh ) + sb (x, c)), (14) c∈c(i) where c(i) denotes the set of newly constructed trees by revising trees in b(i), sb (x, c) is the baseline model score and arg K leaves the k best candidate trees to the next beam. Finally, the output tree yi of reranking is selected from all searched trees C in the revising process yi = arg max(st (x, c, Θc ) + sb (x, c)) c∈C (15) Interpolated Reranker In testing, we also adopt the popular mixture reranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which obtains better reranking performance by a linear combination of the reranking score and the baseline model score. yi = arg max (β(st (xi , y, Θc ) + st (x, y, Θh )) y∈τ (xi ) + (1 − β)sb (xi , y)) (16) Here yi is the final output tree for a sentence xi ; τ (xi ) returns all the trees candidates of the dynamic reranking; β ∈[0, 1] is a hyper-parameter. 4.4 Training As k-best neural rerankers (Socher et al., 2013; Zhu et al., 2015), we use the max-margin criterion to train our model in a stage-wise manner (Doppa et al., 2013). Given training data Dc = (xi , yi , yˆ"
P16-1132,W05-1506,0,0.0964815,"rerankers are capable of capturing global syntax features across the tree. In contrast, the most non-local neural parser with LSTM (Dyer et al., 2015) cannot exploit global features. Different to previous neural rerankers, our work in this paper contributes on integrating search and learning for reranking, instead of proposing a new neural model. Forest Reranking Forest reranking (Huang, 2008; Hayashi et al., 2013) offers a different way to extend the coverage of reranking candidates, with computing the reranking score in the trees forests by decomposing non-local features with cube-pruning (Huang and Chiang, 2005). In contrast, the neural reranking score encodes the whole dependency tree, which cannot be decomposed for forest reranking efficiently and accurately. HC-Search Doppa et al. (2013) proposed a structured prediction model with HC-Search strategy and imitation learning, which is closely related to our work in spirit. They used the complete space search (Doppa et al., 2012) for sequence labeling tasks, and the whole search process halts after a specific time bound. Different from them, we propose a dynamic parsing reranking model based on the action revising process, which is a multi-step proces"
P16-1132,P08-1067,0,0.601351,"local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association fo"
P16-1132,D14-1081,0,0.0457333,"literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have rat"
P16-1132,P15-1031,0,0.0450277,"Missing"
P16-1132,P13-1045,0,0.206611,"ve accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016."
P16-1132,P15-1032,0,0.0169178,"by replacing the SVM classifier at the transition-based MaltParser (Nivre et al., 2007) with a feed-forward neural network, achieving significantly higher accuracies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively"
P16-1132,P15-1117,1,0.854947,"classifier at the transition-based MaltParser (Nivre et al., 2007) with a feed-forward neural network, achieving significantly higher accuracies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another"
P16-1132,P15-1112,0,0.250311,"S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have rather weak feature lo"
P16-1132,J93-2004,0,0.0549301,"Missing"
P19-1602,P97-1064,0,0.0341154,"onal auto-encoders (VAEs) is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et a"
P19-1602,P17-1177,1,0.860672,"om/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and manipulation, towards the syntax of a sen"
P19-1602,W01-0713,0,0.0302084,"ed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John e"
P19-1602,W18-6503,0,0.0200293,"AE could graft the designed syntax to another sentence under certain circumstances. 2 Related Work The variational auto-encoders (VAEs) is proposed by Kingma and Welling (2014) for image generation. Bowman et al. (2016) successfully applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating samplin"
P19-1602,N16-1024,0,0.03024,"lly applied VAE in the NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John et al., 2018). In previous work, the style is usually defined by categorical features such"
P19-1602,P16-1078,0,0.0219963,"release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely"
P19-1602,P02-1040,0,0.104047,"9.79 11.09 syntax-VAE BLEU↑ 7.26 7.41 8.19 8.98 9.07 9.26 9.36 Forward PPL↓ 34.01 35.00 36.53 42.44 44.11 48.70 49.73 VAE 12 DSS-VAE 11 BLEU KL-Weight 1.3 1.2 1.0 0.7 0.5 0.3 0.1 10 9 8 Table 1: BLEU and Forward PPL of VAE with varying KL weights on the PTB test set. The larger↑ (or lower↓ ), the better. 7 31 36 41 46 51 Forward-PPL 1. Reconstruction BLEU. The reconstruction task aims to generate the input sentence itself. In the task, both syntactic and semantic vectors are chosen as the predicted mean of the encoded distribution. We evaluate the reconstruction performance by the BLEU score (Papineni et al., 2002) with input as the reference.3 It reflects how well the model could preserve input information, and is crucial for representation learning and “goal-oriented” text generation. 2. Forward PPL. We then perform unconditioned generation, where both syntactic and semantic vectors are sampled from prior. Forward perplexity (PPL) (Zhao et al., 2018) is the generated sentences’ perplexity score predicted by a pertained language model.4 It shows the fluency of generated sentences from VAE’s prior. We computed Forward PPL based on 100K sampled sentences. 3. Reverse PPL. Unconditioned generation is furth"
P19-1602,D17-1066,0,0.0159685,"ee sequence, leading to better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntaxtransfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work. ‡ 1 Introduction Variational auto-encoders (VAEs, Kingma and Welling, 2014) are widely used in language generation tasks (Serban et al., 2017; Kusner et al., 2017; Semeniuta et al., 2017; Li et al., 2018b). VAE encodes a sentence into a probabilistic latent space, from which it learns to decode the same sentence. In addition to traditional reconstruction loss of an autoencoder, VAE employs an extra regularization term, penalizing the Kullback– Leibler (KL) divergence between the encoded posterior distribution and its prior. This property enables us to sample and generate sentences from the continuous latent space. Additionally, we can ∗ Equal contributions. Corresponding author. ‡ We release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually"
P19-1602,E17-1117,0,0.0226876,"NLP domain, showing that VAE improves recurrent neural network (RNN)-based language modeling (RNN-LM, Mikolov et al., 2010); that VAE allows sentence sampling and sentence interpolation in the continuous latent space. Later, VAE is widely used in various natural language generation tasks (Gupta et al., 2018; Kusner et al., 2017; Hu et al., 2017; Deriu and Cieliebak, 2018). Syntactic language modeling, to the best of our knowledge, could be dated back to Chelba (1997). Charniak (2001) and Clark (2001) propose to utilize a top-down parsing mechanism for language modeling. Dyer et al. (2016) and Kuncoro et al. (2017) introduce the neural network to this direction. The Parsing-Reading-Predict Network (PRPN, Shen et al., 2017), which reports a state-of-the-art results on syntactic language modeling, learns a latent syntax by training with a language modeling objective. Different from their work, our approach models syntax in a continuous space, facilitating sampling and manipulation of syntax. Our work is also related to style-transfer text generation (Fu et al., 2018; Li et al., 2018a; John et al., 2018). In previous work, the style is usually defined by categorical features such as sentiment. We move one"
P19-1602,N18-1169,0,0.105168,"Missing"
P19-1602,P17-1064,0,0.0150872,"https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and manipulation, towards"
P19-1602,D18-1423,0,0.108519,"better performance of language generation. Additionally, the advantage of sampling in the disentangled syntactic and semantic latent spaces enables us to perform novel applications, such as the unsupervised paraphrase generation and syntaxtransfer generation. Experimental results show that our proposed model achieves similar or better performance in various tasks, compared with state-of-the-art related work. ‡ 1 Introduction Variational auto-encoders (VAEs, Kingma and Welling, 2014) are widely used in language generation tasks (Serban et al., 2017; Kusner et al., 2017; Semeniuta et al., 2017; Li et al., 2018b). VAE encodes a sentence into a probabilistic latent space, from which it learns to decode the same sentence. In addition to traditional reconstruction loss of an autoencoder, VAE employs an extra regularization term, penalizing the Kullback– Leibler (KL) divergence between the encoded posterior distribution and its prior. This property enables us to sample and generate sentences from the continuous latent space. Additionally, we can ∗ Equal contributions. Corresponding author. ‡ We release the implementation and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the l"
P19-1602,D17-1013,1,0.827385,"uth training signals; in testing, we do not need external syntactic trees. We build an RNN 1 https://www.sutd.edu.sg/cmsresource/faculty/yuezhang/ zpar.html 6010 with softmax, whose objective is the cross-entropy loss against the groundtruth distribution t, given by: X tw log p(w|zsem ) (3) L(mul) sem = − This is ... S NP .. . /S This is This is ... w∈V ... S NP .. . /S This is ... Figure 2: Overview of our DSS-VAE. Forward dashed arrows are multi-task losses; backward dashed arrows are adversarial losses. where p(w|zsyn ) is the predicted distribution. BoW has been explored by previous work (Weng et al., 2017; John et al., 2018), showing good ability of preserving semantics. For the syntactic space, the multi-task loss trains a model to predict syntax on zsyn . Due to our proposal in §3.2.1, we could build a dedicated RNN, predicting the tokens in the linearized parse tree sequence, whose loss is: Xn log p(si |s1 · · · si−1 , zsyn ) (4) L(mul) syn = − i=1 (independent of the VAE’s decoder) to predict such linearized parse trees, where each parsing token is represented by an embedding (similar to a traditional RNN decoder). Notice that, a node and its backtracking, e.g., NP and /NP, have different"
P19-1602,P18-2053,0,0.0136272,"ters are not updated. 3.2.3 Adversarial Reconstruction Loss Our next intuition is that syntax and semantics are more interwoven to each other than other information such as style and content. Suppose, for example, the syntax and semantics have been perfectly separated by the losses in 6011 §3.2.2, where zsem could predict BoW well, but does not contain any information about the syntactic tree. Even in this ideal case, the decoder can reconstruct the original sentence from zsem by simply learning to re-order words (as zsem does contain BoW). Such word re-ordering knowledge is indeed learnable (Ma et al., 2018), and does not necessarily contain the syntactic information. Therefore, the multi-task and adversarial losses for syntax and semantics do not suffice to regularize DSS-VAE. We now propose an adversarial reconstruction loss to discourage the sentence being predicted by a single subspace zsyn or zsem . When combined, however, they should provide a holistic view of the entire sentence. Formally, let zs be a latent variable (zs = zsyn or zsem ). A decoding adversary is trained to predict the sentence based on zs , denoted by prec (xi |x1 · · · xi−1 , zs ). Then, the adversarial reconstruction los"
P19-1602,J93-2004,0,0.0651646,"er during training. 4 Experiments We evaluate our method on reconstruction and unconditional language generation (§4.1). Then, we apply it two applications, namely, unsupervised paraphrase generation (§4.2) and syntax-transfer generation (§4.3). 4.1 Reconstruction and Unconditional Language Generation First, we compare our model in reconstruction and unconditional language generation with a traditional VAE and a syntactic language model (PRPN, Shen et al., 2017). Dataset We followed previous work (Bowman et al., 2016) and used a standard benchmark, the WSJ sections in the Penn Treebank (PTB) (Marcus et al., 1993). We also followed the standard split: Sections 2–21 for training, Section 24 for validation, and Section 23 for test. Settings We trained VAE and DSS-VAE, both with 100-dimensional RNN states. For the vocabulary, we chose 30k most frequent words. We trained PRPN with the default parameter in the code base.2 Evaluation We evaluate model performance with the following metrics: 6012 2 https://github.com/yikangshen/PRPN 47.33 8.98 45.6 9.6 49.73 9.36 49.79 11.09 syntax-VAE BLEU↑ 7.26 7.41 8.19 8.98 9.07 9.26 9.36 Forward PPL↓ 34.01 35.00 36.53 42.44 44.11 48.70 49.73 VAE 12 DSS-VAE 11 BLEU KL-Wei"
P19-1602,P17-2092,1,0.84797,"tion and models at https:// github.com/baoy-nlp/DSS-VAE † even manually manipulate the latent space, inspiring various applications such as sentence interpolation (Bowman et al., 2016) and text style transfer (Hu et al., 2017). However, the continuous latent space of VAE blends syntactic and semantic information together, without modeling the syntax explicitly. We argue that it may be not necessarily the best in the text generation scenario. Recently, researchers have shown that explicitly syntactic modeling improves the generation quality in sequence-tosequence models (Eriguchi et al., 2016; Zhou et al., 2017; Li et al., 2017; Chen et al., 2017). It is straightforward to adopt such idea in the VAE setting, since a vanilla VAE does not explicitly model the syntax. A line of studies (Kusner et al., 2017; G´omez-Bombarelli et al., 2018; Dai et al., 2018) propose to impose context-free grammars (CFGs) as hard constraints in the VAE decoder, so that they could generate syntactically valid outputs of programs, molecules, etc. However, the above approaches cannot be applied to syntactic modeling in VAE’s continuous latent space, and thus, we do not enjoy the two benefits of VAE, namely, sampling and mani"
P19-1602,P01-1017,0,\N,Missing
P19-1602,D14-1179,0,\N,Missing
P19-1602,K16-1002,0,\N,Missing
Q18-1011,D17-1151,0,0.0132202,"aseline. Comparison with Other Work. (Rows 9-11). We also conduct experiments with multi-layer decoders (Wu et al., 2016) to see whether the NMT system can automatically model the translated and untranslated contents with additional decoder lay152 ers (Rows 9-10). However, we find that the performance is not improved using a two-layer decoder (Row 9), until a deeper version (three-layer decoder, Row 10) is used. This indicates that enhancing performance by simply adding more RNN layers into the decoder without any explicit instruction is nontrivial, which is consistent with the observation of Britz et al. (2017). Our model also outperforms the word-level C OVERAGE (Tu et al., 2016), which considers the coverage information of the source words independently. Our proposed model can be regarded as a high-level coverage model, which captures higher level coverage information, and gives more specific signals for the decision of attention and target prediction. Our model is more deeply involved in generating target words, by being fed not only to the attention model as in Tu et al. (2016), but also to the decoder state. 5.1.2 Subjective Evaluation Following Tu et al. (2016), we conduct subjective evaluatio"
Q18-1011,W17-3203,0,0.0258306,",000. We use the total BPE vocabulary for each side. We tie the weights of the target-side embeddings and the output weight matrix (Press and Wolf, 2017) for De-En. All out-of-vocabulary words are mapped to a special token UNK. We train each model with sentences lengths of up to 50 words in the training data. The dimension of word embeddings is 512, and all hidden sizes are 1024. In training, we set the batch size to 80 for ZhEn, and 64 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does not decrease. For the proposed model, we use the same setting as the baseline model. The F UTURE and PAST layer sizes are 1024. We employ a two-pass strategy for training the proposed model, which has proven useful to ease training difficulty when the model is relatively complicated (Shen et al., 2016; Wang et al., 2017; Wang et al., 2018). Model parameters shared with the baseline are initialized by the baseline model. 5.1 Results on Chinese-English We first evaluate"
Q18-1011,W17-4725,0,0.0491104,"Missing"
Q18-1011,D13-1176,0,0.0776419,"TURE contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed model outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.† 1 Introduction Neural machine translation (NMT) generally adopts an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), where the encoder summarizes the source sentence into a source context vector, and the decoder generates the target sentence word-by-word based on the given source. During translation, the decoder implicitly serves several functionalities at the same time: * Equal contributions. Our code can be downloaded from https://github. com/zhengzx-nlp/past-and-future-nmt. † 1. Building a language model over the target sentence for translation fluency (L M). 2. Acquiring the most relevant source-side information to generate the current target word (P RESENT)."
Q18-1011,D15-1166,0,0.161208,"de can be downloaded from https://github. com/zhengzx-nlp/past-and-future-nmt. † 1. Building a language model over the target sentence for translation fluency (L M). 2. Acquiring the most relevant source-side information to generate the current target word (P RESENT). 3. Maintaining what parts in the source have been translated (PAST) and what parts have not (F UTURE). However, it may be difficult for a single recurrent neural network (RNN) decoder to accomplish these functionalities simultaneously. A recent successful extension of NMT models is the attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), which makes a soft selection over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state. In this sense, the attention mechanism separates the P RESENT functionality from the decoder RNN, achieving significant performance improvement. In addition to P RESENT, we address the importance of modeling PAST and F UTURE contents in machine translation. The PAST contents indicate translated information, whereas the F UTURE contents indicate untranslated information, both being crucial to NMT models, especially to avoid undertranslation"
Q18-1011,C16-1205,0,0.043448,"del both translated (with PAST-RNN) and untranslated (with F UTURERNN) instead of using a single coverage vector to indicate translated source words. The difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed not only to the attention mechanism but also the decoder’s states. In the context of semantic-level coverage, Wang et al. (2016) propose a memory-enhanced decoder s0 s0F Decoder Layer st s1F sP0 source summarization s1 sPt-1 c1 ct stF Future Layer sPt Past Layer Attention (Present) Layer Figure 2: NMT decoder augmented with PAST and F UTURE layers. and Meng et al. (2016) propose a memory-enhanced attention model. Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents. However, their models lack an explicit objective to guide such intuition, which is one of the key ingredients for the success in this work. In addition, we use two separate layers to explicitly model translated and untranslated contents, which is another distinguishing feature of the proposed approach. Future Modeling. Standard neural sequence decoders"
Q18-1011,D16-1096,0,0.0876841,"ly helps the prediction at the beginning of the sentence. We attribute the vanishing of such signals to the overloaded use of decoder states (e.g., L M, PAST, and F UTURE functionalities), and hence we propose to explicitly model the holistic source summarization by PAST and F UTURE contents at each decoding step. 3 Related Work Our research is built upon an attention-based sequence-to-sequence model (Bahdanau et al., 2015), but is also related to coverage modeling, future modeling, and functionality separation. We discuss these topics in the following. Coverage Modeling. Tu et al. (2016) and Mi et al. (2016) maintain a coverage vector to indicate which source words have been translated and which source words have not. These vectors are updated by accumulating attention probabilities at each decoding step, which provides an opportunity for the attention model to distinguish translated source words from untranslated ones. Viewing coverage vectors as a (soft) indicator of translated source contents, following this idea, we take one step further. We model translated and untranslated source contents by directly manipulating the attention vector (i.e., the source contents that are being translated) ins"
Q18-1011,D16-1147,0,0.0206683,"o predict the target words that remain untranslated. Along the direction of future modeling, we introduce a F UTURE layer to maintain the untranslated source contents, which is updated at each decoding step by subtracting the source content being translated (i.e., attention vector) from the last state (i.e., the untranslated source content so far). 148 Functionality Separation. Recent work has revealed that the overloaded use of representations makes model training difficult, and such problems can be alleviated by explicitly separating these functions (Reed and Freitas, 2015; Ba et al., 2016; Miller et al., 2016; Gulcehre et al., 2016; Rockt¨aschel et al., 2017). For example, Miller et al. (2016) separate the functionality of look-up keys and memory contents in memory networks (Sukhbaatar et al., 2015). Rockt¨aschel et al. (2017) propose a keyvalue-predict attention model, which outputs three vectors at each step: the first is used to predict the next-word distribution; the second serves as the key for decoding; and the third is used for the attention mechanism. In this work, we further separate PAST and F UTURE functionalities from the decoder’s hidden representations. 4 Modeling PAST and F UTURE fo"
Q18-1011,J03-1002,0,0.0224634,") . | {z } PAST loss Dataset. We conduct experiments on ChineseEnglish (Zh-En), German-English (De-En), and English-German (En-De) translation tasks. For Zh-En, the training set consists of 1.6m sentence pairs, which are extracted from the LDC corpora3 . The NIST 2003 (MT03) dataset is our development set; the NIST 2002 (MT02), 2004 (MT04), 2005 (MT05), 2006 (MT06) datasets are test sets. We also evaluate the alignment performance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards port"
Q18-1011,P02-1040,0,0.100819,"rformance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For Zh-En, we limit the vocabulary size to 30K. For De-En and En-De, the number of join"
Q18-1011,E17-2025,0,0.0188295,"14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For Zh-En, we limit the vocabulary size to 30K. For De-En and En-De, the number of joint BPE operations is 90,000. We use the total BPE vocabulary for each side. We tie the weights of the target-side embeddings and the output weight matrix (Press and Wolf, 2017) for De-En. All out-of-vocabulary words are mapped to a special token UNK. We train each model with sentences lengths of up to 50 words in the training data. The dimension of word embeddings is 512, and all hidden sizes are 1024. In training, we set the batch size to 80 for ZhEn, and 64 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does"
Q18-1011,W17-4738,0,0.0132824,"uishes the PAST and F UTURE directly, which is a higher level coverage mechanism than the word coverage model. 153 ∆ – -1.00 -3.83 Table 4: Evaluation of the alignment quality. The lower the score, the better the alignment quality. 5.2 Results on German-English We also evaluate our model on the WMT17 benchmarks for both De-En and En-De. As shown in Table 5, our baseline gives comparable BLEU scores to the state-of-the-art NMT systems of WMT17. Our proposed model improves the strong baseline on both De-En and En-De. This shows that our proposed model works well across different language pairs. Rikters et al. (2017) and Sennrich et al. (2017a) obtain higher BLEU scores than our model, because they use additional large scale synthetic data (about 10M) for training. It maybe unfair to compare our model to theirs directly. 5.3 Alignment Quality AER 39.73 38.73 35.90 Analysis We conduct analyses on Zh-En, to better understand our model from different perspectives. Parameters and Speeds. As shown in Table 6, the baseline model (BASE) has 80M parameters. A single F UTURE or PAST layer introduces 15M to 17M parameters, and the corresponding objective introduces 18M parameters. In this work, the most complex mod"
Q18-1011,P16-1162,0,0.121344,"2005 (MT05), 2006 (MT06) datasets are test sets. We also evaluate the alignment performance on the standard benchmark of Liu and Sun (2015), which contains 900 manually aligned sentence pairs. We measure the alignment quality with the alignment error rate (Och and Ney, 2003). For De-En and En-De, we conduct experiments on the WMT17 (Bojar et al., 2017) corpus. The dataset consists of 5.6M sentence pairs. We use newstest2016 as our development set, and newstest2017 as our testset. We follow Sennrich et al. (2017a) to segment both German and English words into subwords using byte-pair encoding (Sennrich et al., 2016, BPE). We measure the translation quality with BLEU scores (Papineni et al., 2002). We use the multi-bleu script for Zh-En4 , and the multi-bleu-detok script for De-En and En-De5 . 3 The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/generic/ multi-bleu.perl 5 https://github.com/EdinburghNLP/ nematus/blob/master/data/ multi-bleu-detok.perl 151 Training Details. We use the Nematus6 (Sennrich et al., 2017b), implementing a baseline translation system, RNNS EARCH. For"
Q18-1011,E17-3017,0,0.0569003,"Missing"
Q18-1011,P16-1159,0,0.0842396,"4 for De-En and En-De. We set the beam size to 12 in testing. We shuffle the training corpus after each epoch. We use Adam (Kingma and Ba, 2014) with annealing (Denkowski and Neubig, 2017) as our optimization algorithm. We set the initial learning rate as 0.0005, which halves when the validation crossentropy does not decrease. For the proposed model, we use the same setting as the baseline model. The F UTURE and PAST layer sizes are 1024. We employ a two-pass strategy for training the proposed model, which has proven useful to ease training difficulty when the model is relatively complicated (Shen et al., 2016; Wang et al., 2017; Wang et al., 2018). Model parameters shared with the baseline are initialized by the baseline model. 5.1 Results on Chinese-English We first evaluate the proposed model on the Chinese-English translation and alignment tasks. 5.1.1 Translation Quality Table 2 shows the translation performances on Chinese-English. Clearly the proposed approach significantly improves the translation quality in all cases, although there are still considerable differences among different variants. F UTURE Layer. (Rows 1-4). All the activation functions for the F UTURE layer obtain BLEU score im"
Q18-1011,P16-1008,1,0.87499,"election over source words and yields an attentive vector to represent the most relevant source parts for the current decoding state. In this sense, the attention mechanism separates the P RESENT functionality from the decoder RNN, achieving significant performance improvement. In addition to P RESENT, we address the importance of modeling PAST and F UTURE contents in machine translation. The PAST contents indicate translated information, whereas the F UTURE contents indicate untranslated information, both being crucial to NMT models, especially to avoid undertranslation and over-translation (Tu et al., 2016). Ideally, PAST grows and F UTURE declines during the translation process. However, it may be difficult for a single RNN to explicitly model the above processes. In this paper, we propose a novel neural machine translation system that explicitly models PAST and F UTURE contents with two additional RNN layers. The RNN modeling the PAST contents (called PAST layer) starts from scratch and accumulates the in145 Transactions of the Association for Computational Linguistics, vol. 6, pp. 145–157, 2018. Action Editor: Philipp Koehn. Submission batch: 6/2017; Revision batch: 9/2017; Published 3/2018."
Q18-1011,D16-1027,0,0.0173536,"nslated source contents by directly manipulating the attention vector (i.e., the source contents that are being translated) instead of attention probability (i.e., the probability of a source word being translated). In addition, we explicitly model both translated (with PAST-RNN) and untranslated (with F UTURERNN) instead of using a single coverage vector to indicate translated source words. The difference with Tu et al. (2016) is that the PAST and FUTURE contents in our model are fed not only to the attention mechanism but also the decoder’s states. In the context of semantic-level coverage, Wang et al. (2016) propose a memory-enhanced decoder s0 s0F Decoder Layer st s1F sP0 source summarization s1 sPt-1 c1 ct stF Future Layer sPt Past Layer Attention (Present) Layer Figure 2: NMT decoder augmented with PAST and F UTURE layers. and Meng et al. (2016) propose a memory-enhanced attention model. Both implement the memory with a Neural Turing Machine (Graves et al., 2014), in which the reading and writing operations are expected to erase translated contents and highlight untranslated contents. However, their models lack an explicit objective to guide such intuition, which is one of the key ingredients"
Q18-1011,D17-1013,1,0.785512,"g feature of the proposed approach. Future Modeling. Standard neural sequence decoders generate target sentences from left to right, thus failing to estimate some desired properties in the future (e.g., the length of target sentence). To address this problem, actor-critic algorithms are employed to predict future properties (Li et al., 2017; Bahdanau et al., 2017), in their models, an interpolation of the actor (the standard generation policy) and the critic (a value function that estimates the future values) is used for decision making. Concerning the future generation at each decoding step, Weng et al. (2017) guide the decoder’s hidden states to not only generate the current target word, but also predict the target words that remain untranslated. Along the direction of future modeling, we introduce a F UTURE layer to maintain the untranslated source contents, which is updated at each decoding step by subtracting the source content being translated (i.e., attention vector) from the last state (i.e., the untranslated source content so far). 148 Functionality Separation. Recent work has revealed that the overloaded use of representations makes model training difficult, and such problems can be allevi"
S12-1057,P94-1019,0,0.0613811,"Missing"
S12-1057,N10-1047,0,0.0280231,"Missing"
S12-1057,W05-1203,0,\N,Missing
S12-1057,O97-1002,0,\N,Missing
S12-1074,C96-1058,0,0.113243,"splitting sentence by punctuations and extracting last character of word as lemma. The experiments show that, with a combination of the two proposed methods, our system can improve LAS about one percent and finally get the second prize out of nine participating systems. We also try to handle the multilevel labels, but with no improvement. 1 Introduction Task 5 of SemEval-2012 tries to find approaches to improve Chinese sematic dependency parsing (SDP). SDP is a kind of dependency parsing. Currently, there are many dependency parsers available, such as Eisner’s probabilistic dependency parser (Eisner, 1996), McDonald’s MSTParser (McDonald et al. 2005a; McDonald et al. 2005b) and Nivre’s MaltParser (Nivre, 2006). Despite of elaborate models, lots of problems still exist in dependency parsing. For example, sentence length has been proved to show great impact on the parsing performance. (Li et al., 2010) used a two-stage approach based on sentence fragment for high-order graph-based dependency parsing. Lacking of linguistic knowledge is also blamed. Three methods are promoted in this paper trying to improve the performance: splitting sentence by commas and semicolons, extracting last character of w"
S12-1074,P05-1012,0,0.167616,"Missing"
S12-1074,H05-1066,0,0.0450379,"Missing"
W06-0140,W03-1721,0,0.0700406,"Missing"
W06-0140,W03-0430,0,0.113027,"Missing"
W06-0140,W04-3236,0,0.0684225,"Missing"
W06-0140,C04-1081,0,0.0424837,"s of the graphical model are linked by edges in a linear chain, CRFs make a first-order Markov independence assumption among output nodes, and thus correspond to finite state machines (FSMs). CRFs define the conditional probability of a state sequence given an input sequence as 1 ⎛ T K ⎞ exp⎜ ∑∑ λ k f k ( st −1 , st , o, t ) ⎟ Zo ⎝ t =1 k =1 ⎠ Where f k ( st −1 , s t , o, t ) is an arbitrary feature PΑ ( s |o) = function over its arguments, and λk is a learned weight for each feature function. Based on CRFs model, we cast the segmentation problem as a sequence tagging problem. Different from (Peng et al., 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. The basic segmentation is a process of labeling each hanzi with a tag given the features derived from its surrounding context. The features used in our experiment can be broken into two categories: character features and word features. The character features are instantiations of the following templates, similar to those described in (Ng and Jin, 2004"
W10-2917,N06-1013,0,0.217513,"and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier, which is trained using labeled data. This method is highly efficient in training because it only makes decisions on alignment links from existing models and avoids searching the entire alignment space. In this paper, we follow Ayan and Dorr (2006)’s idea of combining multiple alignment results. And we use more features, such as bi-lexical features, w"
W10-2917,J93-2003,0,0.0107643,"od, namely Tri-training, to train classifiers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The final translation quality of a phrase-based translation system is slightly improved, as well. 1 Introduction Word alignment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et 135 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135–143, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics alignment links in AI are correct, which means alignment links in A"
W10-2917,P04-1023,0,0.0523745,"Missing"
W10-2917,P06-2014,0,0.0161952,"spite the large improvement in F1 score, our two Tri-training models only get slightly better score than the well-known Model4GDF. This kind of inconsistence between AER or F1 scores and 5 Related work Previous work mainly focuses on supervised learning of word alignment. Liu et al. (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Moore (2005) proposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the"
W10-2917,P06-1097,0,0.0567277,"Missing"
W10-2917,J07-3002,0,0.0167083,"plotted in Figure 2a. Basically, both accuracy and recall increase with the size of labeled data. However, we also find that the increase of all the scores gets slower when the 140 ModelName Model4C2E Model4E2C BerkeleyAl. Model4GDF Supervised Tri-Bootstrap Tri-Divide Dev04 24.54 26.54 26.19 26.75 27.07 26.88 27.04 Test05 17.10 19.00 20.08 20.67 20.00 20.49 20.96 Test06 17.52 20.18 19.65 20.58 19.47 20.76 20.79 Test08 14.59 16.56 16.70 17.05 16.13 17.31 17.18 Table 4: Experiments on machine translation (BLEU4 scores in percentage) BLEU scores is a known issue in machine translation community (Fraser and Marcu, 2007). One possible explanation is that both AER or F1 are 0-1 loss functions, which means missing one link and adding one redundant link will get the same penalty. And more importantly, every wrong link receives the same penalty under these metrics. However, these different errors may have different effects on the machine translation quality. Thus, improving alignment quality according to AER or F1 may not directly lead to an increase of BLEU scores. The relationship among these metrics are still under investigation. Note that, both experiments on data size show some unsteadiness during the learni"
W10-2917,N03-1017,0,0.00834837,"027). we treat all alignment links as sure links. AER = 1 − |A ∩ P |+ |A ∩ S| |A |+ |S| (3) We also define a F1 score to be the harmonic mean of classifier’s accuracy and recall of correct decisions (Formula 4). F1 = 2 ∗ accuracy ∗ recall accuracy + recall (4) We also evaluate the machine translation quality using unlabeled data (in Table 1) and these alignment results as aligned training data. We use multi-references data sets from NIST Open MT Evaluation as development and test data. The English side of the parallel corpus is trained into a language model using SRILM (Stolcke, 2002). Moses (Koehn et al., 2003) is used for decoding. Translation quality is measured by BLEU4 score ignoring the case. 4.3 Experiments of Semi-supervised Models We present our experiment results on semisupervised models in Table 3. The two strategies of generating initial classifiers are compared. TriBootstrap is the model using the original bootstrap sampling initialization; and Tri-Divide is the model using the dividing initialization as described in Section 3.2. Items with superscripts 0 indicate models before the first iteration, i.e. initial models. The scores of BerkeleyAligner and the supervised model are also inclu"
W10-2917,N06-1014,0,0.0238493,"sed model are also included for comparison. In general, all supervised and semi-supervised models achieve better results than the best submodel, which proves the effectiveness of labeled training data. It is also reasonable that initial models are not as good as the supervised model, because they only use part of the labeled data for training. After the iterative training, both the two 4.2 Experiments of Sub-models We use the following three sub-models: bidirectional results of Giza++ (Och and Ney, 2003) Model4, namely Model4C2E and Model4E2C, and the joint training result of BerkeleyAligner (Liang et al., 2006) (BerkeleyAl.). To evaluate AER, all three data sets listed in Table 1 are combined and used for the unsupervised training of each sub-model. Table 2 presents the alignment quality of those sub-models, as well as a supervised ensemble of 139 0.79 0.78 0.8 0.77 F−1 scores Scores(F−1, Accuracy, Recall) 0.9 0.7 0.6 0.76 0.75 F−1 Recall Accuracy 0.5 0.4 0 1000 2000 3000 4000 5000 Training Instances Number Tri−Divide Supervised Tri−Bootstrap 0.74 0.73 0 6000 (a) 0.5 1 1.5 2 Number of sentences 2.5 3 5 x 10 (b) Figure 2: (a) Experiments on the Size of Labeled Training Data in Supervised Training; (b"
W10-2917,P05-1057,0,0.131229,"understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier, which is trained using labeled data. This method is highly efficient in training because it only makes decisions on alignment links from existing models and avoids searching the entire alignment space. In this paper, we follow Ayan and Dorr (2006)’s"
W10-2917,H05-1011,0,0.263803,"093, P.R.China richardlkx@126.com Abstract al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier, which is trained using labeled data. This method is highly efficient in training because it only makes decisions on alignm"
W10-2917,J03-1002,0,0.0868239,"fiers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The final translation quality of a phrase-based translation system is slightly improved, as well. 1 Introduction Word alignment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et 135 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135–143, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics alignment links in AI are correct, which means alignment links in AI are always included in the combination"
W10-2917,H05-1010,0,0.350294,"erformance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier, which is trained using labeled data. This method is highly efficient in training because it only makes decisions on alignment links from existing models and avoids searching the entire alignment space. In this paper, we follow Ayan and Dorr (2006)’s idea of combining multipl"
W10-2917,C96-2141,0,0.178455,"ing, to train classifiers using both labeled and unlabeled data collaboratively and further improve the result. Experimental results show that our methods can substantially improve the quality of word alignment. The final translation quality of a phrase-based translation system is slightly improved, as well. 1 Introduction Word alignment is the process of learning bilingual word correspondences. Conventional word alignment process is treated as an unsupervised learning task, which automatically learns the correspondences between bilingual words using an EM style algorithm (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recently, supervised learning methods have been used to improve the performance. They firstly re-formalize word alignment as some kind of classification task. Then the labeled data is used to train the classification model, which is finally used to classify unseen test data (Liu et al., 2005; Taskar et 135 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 135–143, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics alignment links in AI are correct, which means alignment links in AI are always include"
W10-2917,2005.mtsummit-papers.41,0,0.0235786,"nly get slightly better score than the well-known Model4GDF. This kind of inconsistence between AER or F1 scores and 5 Related work Previous work mainly focuses on supervised learning of word alignment. Liu et al. (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Moore (2005) proposes a similar framework, but with more features and a different search method. Other models such as SVM and CRF are also used (Taskar et al., 2005; Cherry and Lin, 2006; Haghighi et al., 2009). For alignment ensemble, Wu and Wang (2005) introduce a boosting approach, in which the labeled data is used to calculate the weight of each sub-model. These researches all focus on the modeling of alignment structure and employ some strategy to search for the optimal alignment. Our main contribution here is the use Co-training style semisupervised methods to assist the ensemble learning framework of Ayan and Dorr (2006). Although we use a maximum entropy model in our experiment, other models like SVM and CRF can also be incorporated into our learning framework. In the area of semi-supervised learning of word alignment, Callison-Burch"
W10-2917,P06-2117,0,0.0643865,"sity Nanjing 210093, P.R.China {huangsj,daixy,chenjj}@nlp.nju.edu.cn 2 School of Foreign Studies, Nanjing University Nanjing 210093, P.R.China richardlkx@126.com Abstract al., 2005; Moore, 2005; Cherry and Lin, 2006; Haghighi et al., 2009). It is well understood that the performance of supervised learning relies heavily on the feature set. As more and more features are added into the model, more data is needed for training. However, due to the expensive cost of labeling, we usually cannot get as much labeled word alignment data as we want. This may limit the performance of supervised methods (Wu et al., 2006). One possible alternative is to use features learnt in some unsupervised manner to help the task. For example, Moore (2005) uses statistics like log-likelihood-ratio and conditionallikelihood-probability to measure word associations; Liu et al. (2005) and Taskar et al. (2005) use results from IBM Model 3 and Model 4, respectively. Ayan and Dorr (2006) propose another way of incorporating unlabeled data. They first train some existing alignment models, e.g. IBM Model4 and Hidden Markov Model, using unlabeled data. The results of these models are then combined using a maximum entropy classifier"
W10-2917,P09-1104,0,\N,Missing
W12-3303,W06-1615,0,0.194359,"Missing"
W12-3303,P07-1056,0,0.298356,"Missing"
W12-3303,P07-1034,0,0.185697,"stributions are significantly different between source and target domain, ALDA would perform poorly. ALDA doesn’t discuss the negative transfer problem and gets hurts when it happens, while AVR actively avoids it by its projection and reweighting strategy. Liao et al. (2005) proposed a method M-Logit, utilizing auxiliary data to help train LR. They also proposed actively sampling target domain instances using Fisher Information Matrix (Fedorov, 1972; Mackay, 1992). Besides, instance weighting was used to mitigate distribution difference between source and target domain in (Huang et al., 2006; Jiang and Zhai, 2007; Sugiyama et al., 2008). These can work as a module in our framework. 3 AVR: Active Vector Rotation Without loss of generalization, we will constrain the discussion of AVR to binary classification tasks. But in fact, AVR can also be applied to multi-class classification and regression. Given training set , | 1, … , , , 1, 1 , traditional supervised learning tries to optimize (Fan et al., 2008; Lin et al., 2008): ∑ min | ; , , (1) where the penalty parameter 0, controls the importance ratio between loss function ; , and regularization parameter |. Loss function’s definition is diverse for diff"
W12-6312,C10-2139,0,0.0228617,"if ci is numeric character and ni = 0 otherwise, ai = 1 if ci is English letter and ai = 0 otherwise. The character-based features template associated with each character type are listed in Table 2. http://crfpp.googlecode.com/svn/trunk/doc/index.html 64 Type surface form number punctuation English letter Template c−1 , c0 , c1 , c−1 c0 , c0 c1 , c−1 c1 n−1 , n0 , n1 , n−1 n0 , n0 n1 , n−1 n1 p−1 , p0 , p1 a−1 , a0 , a1 , a−1 a0 , a0 a1 , a−1 a1 Table 2: Character-based feature template. 2.2.2 Word-based Features Combining word-based features and characterbased features has been suggested by (Sun 2010; Sun and Xu, 2011), based on the observation that word-based features capture a relatively larger context than character-based features. We define c[i:j] as a string that starts at the i-th character and ends at the j-th character, and then define D[i:j] = 1 if c[i:j] matches a word in a pre-defined dictionary, and 0 otherwise. The word-based feature templates are listed in Table 3. Accessor Variety (AV) is firstly proposed by Feng et al. (2004) in the task of identifying meaningful Chinese words from an unlabelled corpus. The basic idea of this approach is when a string appears under differe"
W12-6312,D11-1090,0,0.0502279,"umeric character and ni = 0 otherwise, ai = 1 if ci is English letter and ai = 0 otherwise. The character-based features template associated with each character type are listed in Table 2. http://crfpp.googlecode.com/svn/trunk/doc/index.html 64 Type surface form number punctuation English letter Template c−1 , c0 , c1 , c−1 c0 , c0 c1 , c−1 c1 n−1 , n0 , n1 , n−1 n0 , n0 n1 , n−1 n1 p−1 , p0 , p1 a−1 , a0 , a1 , a−1 a0 , a0 a1 , a−1 a1 Table 2: Character-based feature template. 2.2.2 Word-based Features Combining word-based features and characterbased features has been suggested by (Sun 2010; Sun and Xu, 2011), based on the observation that word-based features capture a relatively larger context than character-based features. We define c[i:j] as a string that starts at the i-th character and ends at the j-th character, and then define D[i:j] = 1 if c[i:j] matches a word in a pre-defined dictionary, and 0 otherwise. The word-based feature templates are listed in Table 3. Accessor Variety (AV) is firstly proposed by Feng et al. (2004) in the task of identifying meaningful Chinese words from an unlabelled corpus. The basic idea of this approach is when a string appears under different linguistic conte"
W12-6312,J04-1004,0,0.0396815,", a−1 a1 Table 2: Character-based feature template. 2.2.2 Word-based Features Combining word-based features and characterbased features has been suggested by (Sun 2010; Sun and Xu, 2011), based on the observation that word-based features capture a relatively larger context than character-based features. We define c[i:j] as a string that starts at the i-th character and ends at the j-th character, and then define D[i:j] = 1 if c[i:j] matches a word in a pre-defined dictionary, and 0 otherwise. The word-based feature templates are listed in Table 3. Accessor Variety (AV) is firstly proposed by Feng et al. (2004) in the task of identifying meaningful Chinese words from an unlabelled corpus. The basic idea of this approach is when a string appears under different linguistic contexts, it may carry a meaning. The more contexts a string appears in, the more likely it is a independent word. Given a string s, we define the left accessor variety of s as the number of distinct characters that precede s in the corpus, denoted by LAV (s). The higher value LAV (s) is, the more likely that s can be separated at its start position. Similarly, right accessor variety of s is defined as the number of distinct charact"
W12-6312,J09-4006,0,0.027947,"Missing"
W12-6312,O03-4002,0,0.619544,"exist in micro-blog text, and we call it rule-based adaptation. Experimentally, using both adaptation strategies, our system achieved 92.46 points of F-score, compared with 88.73 points of F-score of the unadapted CRF word segmenter on the pre-released development data. Our system achieved 92.51 points of F-score on the final test data. 1 Introduction Recent years have witnessed the great development of Chinese word segmentation (CWS) techniques. Among various approaches, character labelling via Conditional Random Field (CRF) modelling has become a prevailing technique (Lafferty et al., 2001; Xue, 2003; Zhao et al., 2006), due to its good performance in OOV words recognition and low development cost. Given a large-scale corpus with human annotation, the only issue the developer need to focus on is to design an expressive set of feature templates which 63 Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 63–68, Tianjin, China, 20-21 DEC. 2012 characters in emoticons, name entities and special punctuation patterns which extensively exist in micro-blog text. Experimentally, using both adaptation strategies, our system achieved 92.46 points of F-score,"
W12-6312,W06-0127,0,\N,Missing
W12-6312,W10-4132,0,\N,Missing
